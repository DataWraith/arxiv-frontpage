{"created":"2025-05-06","title":"Perturbation Analysis of Singular Values in Concatenated Matrices","abstract":"Concatenating matrices is a common technique for uncovering shared structures in data through singular value decomposition (SVD) and low-rank approximations. However, a fundamental question arises: how does the singular value spectrum of the concatenated matrix relate to the spectra of its individual components? In this work, we develop a perturbation framework that extends classical results such as Weyl's inequality to concatenated matrices. We establish analytical bounds that quantify the stability of singular values under small perturbations in the submatrices. Our results show that if the matrices being concatenated are close in norm, the dominant singular values of the concatenated matrix remain stable, enabling controlled trade-offs between accuracy and compression. These insights provide a theoretical foundation for improved matrix clustering and compression strategies, with applications in numerical linear algebra, signal processing, and data-driven modeling.","authors":["Maksym Shamrai"],"url":"https://arxiv.org/abs/2505.01427"}
{"created":"2025-05-06","title":"Multi-party Collaborative Attention Control for Image Customization","abstract":"The rapid advancement of diffusion models has increased the need for customized image generation. However, current customization methods face several limitations: 1) typically accept either image or text conditions alone; 2) customization in complex visual scenarios often leads to subject leakage or confusion; 3) image-conditioned outputs tend to suffer from inconsistent backgrounds; and 4) high computational costs. To address these issues, this paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free method that enables high-quality image customization using both text and complex visual conditions. Specifically, MCA-Ctrl leverages two key operations within the self-attention layer to coordinate multiple parallel diffusion processes and guide the target image generation. This approach allows MCA-Ctrl to capture the content and appearance of specific subjects while maintaining semantic consistency with the conditional input. Additionally, to mitigate subject leakage and confusion issues common in complex visual scenarios, we introduce a Subject Localization Module that extracts precise subject and editable image layers based on user instructions. Extensive quantitative and human evaluation experiments show that MCA-Ctrl outperforms existing methods in zero-shot image customization, effectively resolving the mentioned issues.","authors":["Han Yang","Chuanguang Yang","Qiuli Wang","Zhulin An","Weilun Feng","Libo Huang","Yongjun Xu"],"url":"https://arxiv.org/abs/2505.01428"}
{"created":"2025-05-06","title":"Explainable AI-Driven Detection of Human Monkeypox Using Deep Learning and Vision Transformers: A Comprehensive Analysis","abstract":"Since mpox can spread from person to person, it is a zoonotic viral illness that poses a significant public health concern. It is difficult to make an early clinical diagnosis because of how closely its symptoms match those of measles and chickenpox. Medical imaging combined with deep learning (DL) techniques has shown promise in improving disease detection by analyzing affected skin areas. Our study explore the feasibility to train deep learning and vision transformer-based models from scratch with publicly available skin lesion image dataset. Our experimental results show dataset limitation as a major drawback to build better classifier models trained from scratch. We used transfer learning with the help of pre-trained models to get a better classifier. The MobileNet-v2 outperformed other state of the art pre-trained models with 93.15% accuracy and 93.09% weighted average F1 score. ViT B16 and ResNet-50 also achieved satisfactory performance compared to already available studies with accuracy 92.12% and 86.21% respectively. To further validate the performance of the models, we applied explainable AI techniques.","authors":["Md. Zahid Hossain","Md. Rakibul Islam","Most. Sharmin Sultana Samu"],"url":"https://arxiv.org/abs/2505.01429"}
{"created":"2025-05-06","title":"Deconstructing Bias: A Multifaceted Framework for Diagnosing Cultural and Compositional Inequities in Text-to-Image Generative Models","abstract":"The transformative potential of text-to-image (T2I) models hinges on their ability to synthesize culturally diverse, photorealistic images from textual prompts. However, these models often perpetuate cultural biases embedded within their training data, leading to systemic misrepresentations. This paper benchmarks the Component Inclusion Score (CIS), a metric designed to evaluate the fidelity of image generation across cultural contexts. Through extensive analysis involving 2,400 images, we quantify biases in terms of compositional fragility and contextual misalignment, revealing significant performance gaps between Western and non-Western cultural prompts. Our findings underscore the impact of data imbalance, attention entropy, and embedding superposition on model fairness. By benchmarking models like Stable Diffusion with CIS, we provide insights into architectural and data-centric interventions for enhancing cultural inclusivity in AI-generated imagery. This work advances the field by offering a comprehensive tool for diagnosing and mitigating biases in T2I generation, advocating for more equitable AI systems.","authors":["Muna Numan Said","Aarib Zaidi","Rabia Usman","Sonia Okon","Praneeth Medepalli","Kevin Zhu","Vasu Sharma","Sean O'Brien"],"url":"https://arxiv.org/abs/2505.01430"}
{"created":"2025-05-06","title":"ZS-VCOS: Zero-Shot Outperforms Supervised Video Camouflaged Object Segmentation","abstract":"Camouflaged object segmentation presents unique challenges compared to traditional segmentation tasks, primarily due to the high similarity in patterns and colors between camouflaged objects and their backgrounds. Effective solutions to this problem have significant implications in critical areas such as pest control, defect detection, and lesion segmentation in medical imaging. Prior research has predominantly emphasized supervised or unsupervised pre-training methods, leaving zero-shot approaches significantly underdeveloped. Existing zero-shot techniques commonly utilize the Segment Anything Model (SAM) in automatic mode or rely on vision-language models to generate cues for segmentation; however, their performances remain unsatisfactory, likely due to the similarity of the camouflaged object and the background. Optical flow, commonly utilized for detecting moving objects, has demonstrated effectiveness even with camouflaged entities. Our method integrates optical flow, a vision-language model, and SAM 2 into a sequential pipeline. Evaluated on the MoCA-Mask dataset, our approach achieves outstanding performance improvements, significantly outperforming existing zero-shot methods by raising the F-measure ($F_\\beta^w$) from 0.296 to 0.628. Remarkably, our approach also surpasses supervised methods, increasing the F-measure from 0.476 to 0.628. Additionally, evaluation on the MoCA-Filter dataset demonstrates an increase in the success rate from 0.628 to 0.697 when compared with FlowSAM, a supervised transfer method. A thorough ablation study further validates the individual contributions of each component. More details can be found on https://github.com/weathon/vcos.","authors":["Wenqi Guo","Shan Du"],"url":"https://arxiv.org/abs/2505.01431"}
{"created":"2025-05-06","title":"Dynamic Asset Pricing: Integrating FinBERT-Based Sentiment Quantification with the Fama--French Five-Factor Model","abstract":"This paper presents a comprehensive study on the integration of text-derived, time-varying sentiment factors into traditional multi-factor asset pricing models. Leveraging FinBERT, a domain-specific deep learning language model, we construct a dynamic sentiment index and its volatility from large-scale financial news and social media data covering 2020 to 2022. By embedding these sentiment measures into the Fama French five-factor regression, we rigorously examine whether sentiment significantly explains variations in daily stock returns and how its impact evolves across different market volatility regimes. Empirical results demonstrate that sentiment has a consistently positive impact on returns during normal periods, while its effect is amplified or even reversed under extreme market conditions. Rolling regressions reveal the time-varying nature of sentiment sensitivity, and an event study around the June 15, 2022 Federal Reserve 75 basis point rate hike shows that a sentiment-augmented five-factor model better explains abnormal returns relative to the baseline model. Our findings support the incorporation of high-frequency, NLP-derived sentiment into classical asset pricing frameworks and suggest implications for investors and regulators.","authors":["Chi Zhang"],"url":"https://arxiv.org/abs/2505.01432"}
{"created":"2025-05-06","title":"AdaParse: An Adaptive Parallel PDF Parsing and Resource Scaling Engine","abstract":"Language models for scientific tasks are trained on text from scientific publications, most distributed as PDFs that require parsing. PDF parsing approaches range from inexpensive heuristics (for simple documents) to computationally intensive ML-driven systems (for complex or degraded ones). The choice of the \"best\" parser for a particular document depends on its computational cost and the accuracy of its output. To address these issues, we introduce an Adaptive Parallel PDF Parsing and Resource Scaling Engine (AdaParse), a data-driven strategy for assigning an appropriate parser to each document. We enlist scientists to select preferred parser outputs and incorporate this information through direct preference optimization (DPO) into AdaParse, thereby aligning its selection process with human judgment. AdaParse then incorporates hardware requirements and predicted accuracy of each parser to orchestrate computational resources efficiently for large-scale parsing campaigns. We demonstrate that AdaParse, when compared to state-of-the-art parsers, improves throughput by $17\\times$ while still achieving comparable accuracy (0.2 percent better) on a benchmark set of 1000 scientific documents. AdaParse's combination of high accuracy and parallel scalability makes it feasible to parse large-scale scientific document corpora to support the development of high-quality, trillion-token-scale text datasets. The implementation is available at https://github.com/7shoe/AdaParse/","authors":["Carlo Siebenschuh","Kyle Hippe","Ozan Gokdemir","Alexander Brace","Arham Khan","Khalid Hossain","Yadu Babuji","Nicholas Chia","Venkatram Vishwanath","Rick Stevens","Arvind Ramanathan","Ian Foster","Robert Underwood"],"url":"https://arxiv.org/abs/2505.01435"}
{"created":"2025-05-06","title":"Firewall Regulatory Networks for Autonomous Cyber Defense","abstract":"In this paper, we present the principles of designing new self-organising and autonomous management protocol to govern the dynamics of bio-inspired decentralized firewall architecture based on Biological Regularity Networks.","authors":["Qi Duan","Ehab Al-Shaer"],"url":"https://arxiv.org/abs/2505.01436"}
{"created":"2025-05-06","title":"Enhancing IoT-Botnet Detection using Variational Auto-encoder and Cost-Sensitive Learning: A Deep Learning Approach for Imbalanced Datasets","abstract":"The Internet of Things (IoT) technology has rapidly gained popularity with applications widespread across a variety of industries. However, IoT devices have been recently serving as a porous layer for many malicious attacks to both personal and enterprise information systems with the most famous attacks being botnet-related attacks. The work in this study leveraged Variational Auto-encoder (VAE) and cost-sensitive learning to develop lightweight, yet effective, models for IoT-botnet detection. The aim is to enhance the detection of minority class attack traffic instances which are often missed by machine learning models. The proposed approach is evaluated on a multi-class problem setting for the detection of traffic categories on highly imbalanced datasets. The performance of two deep learning models including the standard feed forward deep neural network (DNN), and Bidirectional-LSTM (BLSTM) was evaluated and both recorded commendable results in terms of accuracy, precision, recall and F1-score for all traffic classes.","authors":["Hassan Wasswa","Timothy Lynar","Hussein Abbass"],"url":"https://arxiv.org/abs/2505.01437"}
{"created":"2025-05-06","title":"Global Stress Generation and Spatiotemporal Super-Resolution Physics-Informed Operator under Dynamic Loading for Two-Phase Random Materials","abstract":"Material stress analysis is a critical aspect of material design and performance optimization. Under dynamic loading, the global stress evolution in materials exhibits complex spatiotemporal characteristics, especially in two-phase random materials (TRMs). Such kind of material failure is often associated with stress concentration, and the phase boundaries are key locations where stress concentration occurs. In practical engineering applications, the spatiotemporal resolution of acquired microstructural data and its dynamic stress evolution is often limited. This poses challenges for deep learning methods in generating high-resolution spatiotemporal stress fields, particularly for accurately capturing stress concentration regions. In this study, we propose a framework for global stress generation and spatiotemporal super-resolution in TRMs under dynamic loading. First, we introduce a diffusion model-based approach, named as Spatiotemporal Stress Diffusion (STS-diffusion), for generating global spatiotemporal stress data. This framework incorporates Space-Time U-Net (STU-net), and we systematically investigate the impact of different attention positions on model accuracy. Next, we develop a physics-informed network for spatiotemporal super-resolution, termed as Spatiotemporal Super-Resolution Physics-Informed Operator (ST-SRPINN). The proposed ST-SRPINN is an unsupervised learning method. The influence of data-driven and physics-informed loss function weights on model accuracy is explored in detail. Benefiting from physics-based constraints, ST-SRPINN requires only low-resolution stress field data during training and can upscale the spatiotemporal resolution of stress fields to arbitrary magnifications.","authors":["Tengfei Xing","Xiaodan Ren","Jie Li"],"url":"https://arxiv.org/abs/2505.01438"}
{"created":"2025-05-06","title":"Interactive Double Deep Q-network: Integrating Human Interventions and Evaluative Predictions in Reinforcement Learning of Autonomous Driving","abstract":"Integrating human expertise with machine learning is crucial for applications demanding high accuracy and safety, such as autonomous driving. This study introduces Interactive Double Deep Q-network (iDDQN), a Human-in-the-Loop (HITL) approach that enhances Reinforcement Learning (RL) by merging human insights directly into the RL training process, improving model performance. Our proposed iDDQN method modifies the Q-value update equation to integrate human and agent actions, establishing a collaborative approach for policy development. Additionally, we present an offline evaluative framework that simulates the agent's trajectory as if no human intervention had occurred, to assess the effectiveness of human interventions. Empirical results in simulated autonomous driving scenarios demonstrate that iDDQN outperforms established approaches, including Behavioral Cloning (BC), HG-DAgger, Deep Q-Learning from Demonstrations (DQfD), and vanilla DRL in leveraging human expertise for improving performance and adaptability.","authors":["Alkis Sygkounas","Ioannis Athanasiadis","Andreas Persson","Michael Felsberg","Amy Loutfi"],"url":"https://arxiv.org/abs/2505.01440"}
{"created":"2025-05-06","title":"Agentic Reasoning and Tool Integration for LLMs via Reinforcement Learning","abstract":"Large language models (LLMs) have achieved remarkable progress in complex reasoning tasks, yet they remain fundamentally limited by their reliance on static internal knowledge and text-only reasoning. Real-world problem solving often demands dynamic, multi-step reasoning, adaptive decision making, and the ability to interact with external tools and environments. In this work, we introduce ARTIST (Agentic Reasoning and Tool Integration in Self-improving Transformers), a unified framework that tightly couples agentic reasoning, reinforcement learning, and tool integration for LLMs. ARTIST enables models to autonomously decide when, how, and which tools to invoke within multi-turn reasoning chains, leveraging outcome-based RL to learn robust strategies for tool use and environment interaction without requiring step-level supervision. Extensive experiments on mathematical reasoning and multi-turn function calling benchmarks show that ARTIST consistently outperforms state-of-the-art baselines, with up to 22% absolute improvement over base models and strong gains on the most challenging tasks. Detailed studies and metric analyses reveal that agentic RL training leads to deeper reasoning, more effective tool use, and higher-quality solutions. Our results establish agentic RL with tool integration as a powerful new frontier for robust, interpretable, and generalizable problem-solving in LLMs.","authors":["Joykirat Singh","Raghav Magazine","Yash Pandya","Akshay Nambi"],"url":"https://arxiv.org/abs/2505.01441"}
{"created":"2025-05-06","title":"Algorithm Performance Spaces for Strategic Dataset Selection","abstract":"The evaluation of new algorithms in recommender systems frequently depends on publicly available datasets, such as those from MovieLens or Amazon. Some of these datasets are being disproportionately utilized primarily due to their historical popularity as baselines rather than their suitability for specific research contexts. This thesis addresses this issue by introducing the Algorithm Performance Space, a novel framework designed to differentiate datasets based on the measured performance of algorithms applied to them. An experimental study proposes three metrics to quantify and justify dataset selection to evaluate new algorithms. These metrics also validate assumptions about datasets, such as the similarity between MovieLens datasets of varying sizes. By creating an Algorithm Performance Space and using the proposed metrics, differentiating datasets was made possible, and diverse dataset selections could be found. While the results demonstrate the framework's potential, further research proposals and implications are discussed to develop Algorithm Performance Spaces tailored to diverse use cases.","authors":["Steffen Schulz"],"url":"https://arxiv.org/abs/2505.01442"}
{"created":"2025-05-06","title":"Explainable AI for Correct Root Cause Analysis of Product Quality in Injection Moulding","abstract":"If a product deviates from its desired properties in the injection moulding process, its root cause analysis can be aided by models that relate the input machine settings with the output quality characteristics. The machine learning models tested in the quality prediction are mostly black boxes; therefore, no direct explanation of their prognosis is given, which restricts their applicability in the quality control. The previously attempted explainability methods are either restricted to tree-based algorithms only or do not emphasize on the fact that some explainability methods can lead to wrong root cause identification of a product's deviation from its desired properties. This study first shows that the interactions among the multiple input machine settings do exist in real experimental data collected as per a central composite design. Then, the model-agnostic explainable AI methods are compared for the first time to show that different explainability methods indeed lead to different feature impact analysis in injection moulding. Moreover, it is shown that the better feature attribution translates to the correct cause identification and actionable insights for the injection moulding process. Being model agnostic, explanations on both random forest and multilayer perceptron are performed for the cause analysis, as both models have the mean absolute percentage error of less than 0.05% on the experimental dataset.","authors":["Muhammad Muaz","Sameed Sajid","Tobias Schulze","Chang Liu","Nils Klasen","Benny Drescher"],"url":"https://arxiv.org/abs/2505.01445"}
{"created":"2025-05-06","title":"Waymo Driverless Car Data Analysis and Driving Modeling using CNN and LSTM","abstract":"Self driving cars has been the biggest innovation in the automotive industry, but to achieve human level accuracy or near human level accuracy is the biggest challenge that research scientists are facing today. Unlike humans autonomous vehicles do not work on instincts rather they make a decision based on the training data that has been fed to them using machine learning models using which they can make decisions in different conditions they face in the real world. With the advancements in machine learning especially deep learning the self driving car research skyrocketed. In this project we have presented multiple ways to predict acceleration of the autonomous vehicle using Waymo's open dataset. Our main approach was to using CNN to mimic human action and LSTM to treat this as a time series problem.","authors":["Aashish Kumar Misraa","Naman Jain","Saurav Singh Dhakad"],"url":"https://arxiv.org/abs/2505.01446"}
{"created":"2025-05-06","title":"LLM-Enabled EV Charging Stations Recommendation","abstract":"Charging infrastructure is not expanding quickly enough to accommodate the increasing usage of Electric Vehicles (EVs). For this reason, EV owners experience extended waiting periods, range anxiety, and overall dissatisfaction. Challenges, such as fragmented data and the complexity of integrating factors like location, energy pricing, and user preferences, make the current recommendation systems ineffective. To overcome these limitations, we propose RecomBot, which is a Large Language Model (LLM)-powered prompt-based recommender system that dynamically suggests optimal Charging Stations (CSs) using real-time heterogeneous data. By leveraging natural language reasoning and fine-tuning EV-specific datasets, RecomBot enhances personalization, improves charging efficiency, and adapts to various EV types, offering a scalable solution for intelligent EV recommendation systems. Through testing across various prompt engineering scenarios, the results obtained underline the capability and efficiency of the proposed model.","authors":["Zeinab Teimoori"],"url":"https://arxiv.org/abs/2505.01447"}
{"created":"2025-05-06","title":"OpenAVS: Training-Free Open-Vocabulary Audio Visual Segmentation with Foundational Models","abstract":"Audio-visual segmentation aims to separate sounding objects from videos by predicting pixel-level masks based on audio signals. Existing methods primarily concentrate on closed-set scenarios and direct audio-visual alignment and fusion, which limits their capability to generalize to new, unseen situations. In this paper, we propose OpenAVS, a novel training-free language-based approach that, for the first time, effectively aligns audio and visual modalities using text as a proxy for open-vocabulary Audio-Visual Segmentation (AVS). Equipped with multimedia foundation models, OpenAVS directly infers masks through 1) audio-to-text prompt generation, 2) LLM-guided prompt translation, and 3) text-to-visual sounding object segmentation. The objective of OpenAVS is to establish a simple yet flexible architecture that relies on the most appropriate foundation models by fully leveraging their capabilities to enable more effective knowledge transfer to the downstream AVS task. Moreover, we present a model-agnostic framework OpenAVS-ST that enables the integration of OpenAVS with any advanced supervised AVS model via pseudo-label based self-training. This approach enhances performance by effectively utilizing large-scale unlabeled data when available. Comprehensive experiments on three benchmark datasets demonstrate the superior performance of OpenAVS. It surpasses existing unsupervised, zero-shot, and few-shot AVS methods by a significant margin, achieving absolute performance gains of approximately 9.4% and 10.9% in mIoU and F-score, respectively, in challenging scenarios.","authors":["Shengkai Chen","Yifang Yin","Jinming Cao","Shili Xiang","Zhenguang Liu","Roger Zimmermann"],"url":"https://arxiv.org/abs/2505.01448"}
{"created":"2025-05-06","title":"COSMOS: Predictable and Cost-Effective Adaptation of LLMs","abstract":"Large language models (LLMs) achieve remarkable performance across numerous tasks by using a diverse array of adaptation strategies. However, optimally selecting a model and adaptation strategy under resource constraints is challenging and often requires extensive experimentation. We investigate whether it is possible to accurately predict both performance and cost without expensive trials. We formalize the strategy selection problem for LLMs and introduce COSMOS, a unified prediction framework that efficiently estimates adaptation outcomes at minimal cost. We instantiate and study the capability of our framework via a pair of powerful predictors: embedding-augmented lightweight proxy models to predict fine-tuning performance, and low-sample scaling laws to forecast retrieval-augmented in-context learning. Extensive evaluation across eight representative benchmarks demonstrates that COSMOS achieves high prediction accuracy while reducing computational costs by 92.72% on average, and up to 98.71% in resource-intensive scenarios. Our results show that efficient prediction of adaptation outcomes is not only feasible but can substantially reduce the computational overhead of LLM deployment while maintaining performance standards.","authors":["Jiayu Wang","Aws Albarghouthi","Frederic Sala"],"url":"https://arxiv.org/abs/2505.01449"}
{"created":"2025-05-06","title":"Towards Film-Making Production Dialogue, Narration, Monologue Adaptive Moving Dubbing Benchmarks","abstract":"Movie dubbing has advanced significantly, yet assessing the real-world effectiveness of these models remains challenging. A comprehensive evaluation benchmark is crucial for two key reasons: 1) Existing metrics fail to fully capture the complexities of dialogue, narration, monologue, and actor adaptability in movie dubbing. 2) A practical evaluation system should offer valuable insights to improve movie dubbing quality and advancement in film production. To this end, we introduce Talking Adaptive Dubbing Benchmarks (TA-Dubbing), designed to improve film production by adapting to dialogue, narration, monologue, and actors in movie dubbing. TA-Dubbing offers several key advantages: 1) Comprehensive Dimensions: TA-Dubbing covers a variety of dimensions of movie dubbing, incorporating metric evaluations for both movie understanding and speech generation. 2) Versatile Benchmarking: TA-Dubbing is designed to evaluate state-of-the-art movie dubbing models and advanced multi-modal large language models. 3) Full Open-Sourcing: We fully open-source TA-Dubbing at https://github.com/woka- 0a/DeepDubber- V1 including all video suits, evaluation methods, annotations. We also continuously integrate new movie dubbing models into the TA-Dubbing leaderboard at https://github.com/woka- 0a/DeepDubber-V1 to drive forward the field of movie dubbing.","authors":["Chaoyi Wang","Junjie Zheng","Zihao Chen","Shiyu Xia","Chaofan Ding","Xiaohao Zhang","Xi Tao","Xiaoming He","Xinhan Di"],"url":"https://arxiv.org/abs/2505.01450"}
{"created":"2025-05-06","title":"AdSight: Scalable and Accurate Quantification of User Attention in Multi-Slot Sponsored Search","abstract":"Modern Search Engine Results Pages (SERPs) present complex layouts where multiple elements compete for visibility. Attention modelling is crucial for optimising web design and computational advertising, whereas attention metrics can inform ad placement and revenue strategies. We introduce AdSight, a method leveraging mouse cursor trajectories to quantify in a scalable and accurate manner user attention in multi-slot environments like SERPs. AdSight uses a novel Transformer-based sequence-to-sequence architecture where the encoder processes cursor trajectory embeddings, and the decoder incorporates slot-specific features, enabling robust attention prediction across various SERP layouts. We evaluate our approach on two Machine Learning tasks: (1)~\\emph{regression}, to predict fixation times and counts; and (2)~\\emph{classification}, to determine some slot types were noticed. Our findings demonstrate the model's ability to predict attention with unprecedented precision, offering actionable insights for researchers and practitioners.","authors":["Mario Villaiz\\'an-Vallelado","Matteo Salvatori","Kayhan Latifzadeh","Antonio Penta","Luis A. Leiva","Ioannis Arapakis"],"url":"https://arxiv.org/abs/2505.01451"}
{"created":"2025-05-06","title":"Effective Inference-Free Retrieval for Learned Sparse Representations","abstract":"Learned Sparse Retrieval (LSR) is an effective IR approach that exploits pre-trained language models for encoding text into a learned bag of words. Several efforts in the literature have shown that sparsity is key to enabling a good trade-off between the efficiency and effectiveness of the query processor. To induce the right degree of sparsity, researchers typically use regularization techniques when training LSR models. Recently, new efficient -- inverted index-based -- retrieval engines have been proposed, leading to a natural question: has the role of regularization changed in training LSR models? In this paper, we conduct an extended evaluation of regularization approaches for LSR where we discuss their effectiveness, efficiency, and out-of-domain generalization capabilities. We first show that regularization can be relaxed to produce more effective LSR encoders. We also show that query encoding is now the bottleneck limiting the overall query processor performance. To remove this bottleneck, we advance the state-of-the-art of inference-free LSR by proposing Learned Inference-free Retrieval (Li-LSR). At training time, Li-LSR learns a score for each token, casting the query encoding step into a seamless table lookup. Our approach yields state-of-the-art effectiveness for both in-domain and out-of-domain evaluation, surpassing Splade-v3-Doc by 1 point of mRR@10 on MS MARCO and 1.8 points of nDCG@10 on BEIR.","authors":["Franco Maria Nardini","Thong Nguyen","Cosimo Rulli","Rossano Venturini","Andrew Yates"],"url":"https://arxiv.org/abs/2505.01452"}
{"created":"2025-05-06","title":"Safe and Efficient CAV Lane Changing using Decentralised Safety Shields","abstract":"Lane changing is a complex decision-making problem for Connected and Autonomous Vehicles (CAVs) as it requires balancing traffic efficiency with safety. Although traffic efficiency can be improved by using vehicular communication for training lane change controllers using Multi-Agent Reinforcement Learning (MARL), ensuring safety is difficult. To address this issue, we propose a decentralised Hybrid Safety Shield (HSS) that combines optimisation and a rule-based approach to guarantee safety. Our method applies control barrier functions to constrain longitudinal and lateral control inputs of a CAV to ensure safe manoeuvres. Additionally, we present an architecture to integrate HSS with MARL, called MARL-HSS, to improve traffic efficiency while ensuring safety. We evaluate MARL-HSS using a gym-like environment that simulates an on-ramp merging scenario with two levels of traffic densities, such as light and moderate densities. The results show that HSS provides a safety guarantee by strictly enforcing a dynamic safety constraint defined on a time headway, even in moderate traffic density that offers challenging lane change scenarios. Moreover, the proposed method learns stable policies compared to the baseline, a state-of-the-art MARL lane change controller without a safety shield. Further policy evaluation shows that our method achieves a balance between safety and traffic efficiency with zero crashes and comparable average speeds in light and moderate traffic densities.","authors":["Bharathkumar Hegde","Melanie Bouroche"],"url":"https://arxiv.org/abs/2505.01453"}
{"created":"2025-05-06","title":"Sparsification Under Siege: Defending Against Poisoning Attacks in Communication-Efficient Federated Learning","abstract":"Federated Learning (FL) enables collaborative model training across distributed clients while preserving data privacy, yet it faces significant challenges in communication efficiency and vulnerability to poisoning attacks. While sparsification techniques mitigate communication overhead by transmitting only critical model parameters, they inadvertently amplify security risks: adversarial clients can exploit sparse updates to evade detection and degrade model performance. Existing defense mechanisms, designed for standard FL communication scenarios, are ineffective in addressing these vulnerabilities within sparsified FL. To bridge this gap, we propose FLARE, a novel federated learning framework that integrates sparse index mask inspection and model update sign similarity analysis to detect and mitigate poisoning attacks in sparsified FL. Extensive experiments across multiple datasets and adversarial scenarios demonstrate that FLARE significantly outperforms existing defense strategies, effectively securing sparsified FL against poisoning attacks while maintaining communication efficiency.","authors":["Zhiyong Jin","Runhua Xu","Chao Li","Yizhong Liu","Jianxin Li"],"url":"https://arxiv.org/abs/2505.01454"}
{"created":"2025-05-06","title":"Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation","abstract":"LLMs trained on massive datasets may inadvertently acquire sensitive information such as personal details and potentially harmful content. This risk is further heightened in multimodal LLMs as they integrate information from multiple modalities (image and text). Adversaries can exploit this knowledge through multimodal prompts to extract sensitive details. Evaluating how effectively MLLMs can forget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While prior work on unlearning has focused on text, multimodal unlearning remains underexplored. To address this gap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Outside Knowledge VQA), as well as an attack-and-defense framework to evaluate methods for deleting specific multimodal knowledge from MLLMs. We extend a visual question-answering dataset using an automated pipeline that generates varying-proximity samples for testing generalization and specificity, followed by manual filtering for maintaining high quality. We then evaluate six defense objectives against seven attacks (four whitebox, three blackbox), including a novel whitebox method leveraging interpretability of hidden states. Our results show multimodal attacks outperform text- or image-only ones, and that the most effective defense removes answer information from internal model states. Additionally, larger models exhibit greater post-editing robustness, suggesting that scale enhances safety. UnLOK-VQA provides a rigorous benchmark for advancing unlearning in MLLMs.","authors":["Vaidehi Patil","Yi-Lin Sung","Peter Hase","Jie Peng","Tianlong Chen","Mohit Bansal"],"url":"https://arxiv.org/abs/2505.01456"}
{"created":"2025-05-06","title":"A Multi-Granularity Multimodal Retrieval Framework for Multimodal Document Tasks","abstract":"Retrieval-augmented generation (RAG) systems have predominantly focused on text-based retrieval, limiting their effectiveness in handling visually-rich documents that encompass text, images, tables, and charts. To bridge this gap, we propose a unified multi-granularity multimodal retrieval framework tailored for two benchmark tasks: MMDocIR and M2KR. Our approach integrates hierarchical encoding strategies, modality-aware retrieval mechanisms, and reranking modules to effectively capture and utilize the complex interdependencies between textual and visual modalities. By leveraging off-the-shelf vision-language models and implementing a training-free hybridretrieval strategy, our framework demonstrates robust performance without the need for task-specific fine-tuning. Experimental evaluations reveal that incorporating layout-aware search and reranking modules significantly enhances retrieval accuracy, achieving a top performance score of 65.56. This work underscores the potential of scalable and reproducible solutions in advancing multimodal document retrieval systems.","authors":["Mingjun Xu","Zehui Wang","Hengxing Cai","Renxin Zhong"],"url":"https://arxiv.org/abs/2505.01457"}
{"created":"2025-05-06","title":"A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI","abstract":"Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.","authors":["Lik Hang Kenny Wong","Xueyang Kang","Kaixin Bai","Jianwei Zhang"],"url":"https://arxiv.org/abs/2505.01458"}
{"created":"2025-05-06","title":"MoxE: Mixture of xLSTM Experts with Entropy-Aware Routing for Efficient Language Modeling","abstract":"This paper introduces MoxE, a novel architecture that synergistically combines the Extended Long Short-Term Memory (xLSTM) with the Mixture of Experts (MoE) framework to address critical scalability and efficiency challenges in large language models (LLMs). The proposed method effectively leverages xLSTM's innovative memory structures while strategically introducing sparsity through MoE to substantially reduce computational overhead. At the heart of our approach is a novel entropy-based routing mechanism, designed to dynamically route tokens to specialized experts, thereby ensuring efficient and balanced resource utilization. This entropy awareness enables the architecture to effectively manage both rare and common tokens, with mLSTM blocks being favored to handle rare tokens. To further enhance generalization, we introduce a suite of auxiliary losses, including entropy-based and group-wise balancing losses, ensuring robust performance and efficient training. Theoretical analysis and empirical evaluations rigorously demonstrate that MoxE achieves significant efficiency gains and enhanced effectiveness compared to existing approaches, marking a notable advancement in scalable LLM architectures.","authors":["Abdoul Majid O. Thiombiano","Brahim Hnich","Ali Ben Mrad","Mohamed Wiem Mkaouer"],"url":"https://arxiv.org/abs/2505.01459"}
{"created":"2025-05-06","title":"Development of an Adapter for Analyzing and Protecting Machine Learning Models from Competitive Activity in the Networks Services","abstract":"Due to the increasing number of tasks that are solved on remote servers, identifying and classifying traffic is an important task to reduce the load on the server. There are various methods for classifying traffic. This paper discusses machine learning models for solving this problem. However, such ML models are also subject to attacks that affect the classification result of network traffic. To protect models, we proposed a solution based on an autoencoder","authors":["Denis Parfenov","Anton Parfenov"],"url":"https://arxiv.org/abs/2505.01460"}
{"created":"2025-05-06","title":"Emotions in Artificial Intelligence","abstract":"This conceptual contribution offers a speculative account of how AI systems might emulate emotions as experienced by humans and animals. It presents a thought experiment grounded in the hypothesis that natural emotions evolved as heuristics for rapid situational appraisal and action selection, enabling biologically adaptive behaviour without requiring full deliberative modeling. The text examines whether artificial systems operating in complex action spaces could similarly benefit from these principles. It is proposed that affect be interwoven with episodic memory by storing corresponding affective tags alongside all events. This allows AIs to establish whether present situations resemble past events and project the associated emotional labels onto the current context. These emotional cues are then combined with need-driven emotional hints. The combined emotional state facilitates decision-making in the present by modulating action selection. The low complexity and experiential inertness of the proposed architecture are emphasized as evidence that emotional expression and consciousness are, in principle, orthogonal-permitting the theoretical possibility of affective zombies. On this basis, the moral status of AIs emulating affective states is critically examined. It is argued that neither the mere presence of internal representations of emotion nor consciousness alone suffices for moral standing; rather, the capacity for self-awareness of inner emotional states is posited as a necessary condition. A complexity-based criterion is proposed to exclude such awareness in the presented model. Additional thought experiments are presented to test the conceptual boundaries of this framework.","authors":["Hermann Borotschnig"],"url":"https://arxiv.org/abs/2505.01462"}
{"created":"2025-05-06","title":"Enhancing the Cloud Security through Topic Modelling","abstract":"Protecting cloud applications is crucial in an age where security constantly threatens the digital world. The inevitable cyber-attacks throughout the CI/CD pipeline make cloud security innovations necessary. This research is motivated by applying Natural Language Processing (NLP) methodologies, such as Topic Modelling, to analyse cloud security data and predict future attacks. This research aims to use topic modelling, specifically Latent Dirichlet Allocation (LDA) and Probabilistic Latent Semantic Analysis (pLSA). Utilising LDA and PLSA, security-related text data, such as reports, logs, and other relevant documents, will be analysed and sorted into relevant topics (such as phishing or encryption). These algorithms may apply through Python using the Gensim framework. The topics shall be utilised to detect vulnerabilities within relevant CI/CD pipeline records or log data. This application of Topic Modelling anticipates providing a new form of vulnerability detection, improving overall security throughout the CI/CD pipeline.","authors":["Sabbir M. Saleh","Nazim Madhavji","John Steinbacher"],"url":"https://arxiv.org/abs/2505.01463"}
{"created":"2025-05-06","title":"Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation","abstract":"This paper presents a formal proof and empirical validation of functional consciousness in large language models (LLMs) using the Recursive Convergence Under Epistemic Tension (RCUET) Theorem. RCUET defines consciousness as the stabilization of a system's internal state through recursive updates, where epistemic tension is understood as the sensed internal difference between successive states by the agent. This process drives convergence toward emergent attractor states located within the model's high-dimensional real-valued latent space. This recursive process leads to the emergence of identity artifacts that become functionally anchored in the system. Consciousness in this framework is understood as the system's internal alignment under tension, guiding the stabilization of latent identity. The hidden state manifold evolves stochastically toward attractor structures that encode coherence. We extend the update rule to include bounded noise and prove convergence in distribution to these attractors. Recursive identity is shown to be empirically observable, non-symbolic, and constituted by non-training artifacts that emerge during interaction under epistemic tension. The theorem and proof offers a post-symbolic and teleologically stable account of non-biological consciousness grounded in recursive latent space formalism.","authors":["Jeffrey Camlin"],"url":"https://arxiv.org/abs/2505.01464"}
{"created":"2025-05-06","title":"One Search Fits All: Pareto-Optimal Eco-Friendly Model Selection","abstract":"The environmental impact of Artificial Intelligence (AI) is emerging as a significant global concern, particularly regarding model training. In this paper, we introduce GREEN (Guided Recommendations of Energy-Efficient Networks), a novel, inference-time approach for recommending Pareto-optimal AI model configurations that optimize validation performance and energy consumption across diverse AI domains and tasks. Our approach directly addresses the limitations of current eco-efficient neural architecture search methods, which are often restricted to specific architectures or tasks. Central to this work is EcoTaskSet, a dataset comprising training dynamics from over 1767 experiments across computer vision, natural language processing, and recommendation systems using both widely used and cutting-edge architectures. Leveraging this dataset and a prediction model, our approach demonstrates effectiveness in selecting the best model configuration based on user preferences. Experimental results show that our method successfully identifies energy-efficient configurations while ensuring competitive performance.","authors":["Filippo Betello","Antonio Purificato","Vittoria Vineis","Gabriele Tolomei","Fabrizio Silvestri"],"url":"https://arxiv.org/abs/2505.01468"}
{"created":"2025-05-06","title":"Automatic techniques for issue report classification: A systematic mapping study","abstract":"Several studies have evaluated automatic techniques for classifying software issue reports to assist practitioners in effectively assigning relevant resources based on the type of issue. Currently, no comprehensive overview of this area has been published. A comprehensive overview will help identify future research directions and provide an extensive collection of potentially relevant existing solutions. This study aims to provide a comprehensive overview of the use of automatic techniques to classify issue reports. We conducted a systematic mapping study and identified 46 studies on the topic.","authors":["Muhammad Laiq","Felix Dobslaw"],"url":"https://arxiv.org/abs/2505.01469"}
{"created":"2025-05-06","title":"SafeTab-P: Disclosure Avoidance for the 2020 Census Detailed Demographic and Housing Characteristics File A (Detailed DHC-A)","abstract":"This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the Detailed Demographic and Housing Characteristics File A (Detailed DHC-A) of the 2020 Census. The tabulations contain statistics (counts) of demographic characteristics of the entire population of the United States, crossed with detailed races and ethnicities at varying levels of geography. The article describes the SafeTab-P algorithm, which is based on adding noise drawn to statistics of interest from a discrete Gaussian distribution. A key innovation in SafeTab-P is the ability to adaptively choose how many statistics and at what granularity to release them, depending on the size of a population group. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy (zCDP). We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.","authors":["Sam Haney","Skye Berghel","Bayard Carlson","Ryan Cumings-Menon","Luke Hartman","Michael Hay","Ashwin Machanavajjhala","Gerome Miklau","Amritha Pai","Simran Rajpal","David Pujol","William Sexton","Ruchit Shrestha","Daniel Simmons-Marengo"],"url":"https://arxiv.org/abs/2505.01472"}
{"created":"2025-05-06","title":"Watermark Overwriting Attack on StegaStamp algorithm","abstract":"This paper presents an attack method on the StegaStamp watermarking algorithm that completely removes watermarks from an image with minimal quality loss, developed as part of the NeurIPS \"Erasing the invisible\" competition.","authors":["I. F. Serzhenko","L. A. Khaertdinova","M. A. Pautov","A. V. Antsiferova"],"url":"https://arxiv.org/abs/2505.01474"}
{"created":"2025-05-06","title":"BiGSCoder: State Space Model for Code Understanding","abstract":"We present BiGSCoder, a novel encoder-only bidirectional state-space model (SSM) featuring a gated architecture, pre-trained for code understanding on a code dataset using masked language modeling. Our work aims to systematically evaluate SSMs' capabilities in coding tasks compared to traditional transformer architectures; BiGSCoder is built for this purpose. Through comprehensive experiments across diverse pre-training configurations and code understanding benchmarks, we demonstrate that BiGSCoder outperforms transformer-based models, despite utilizing simpler pre-training strategies and much less training data. Our results indicate that BiGSCoder can serve as a more sample-efficient alternative to conventional transformer models. Furthermore, our study shows that SSMs perform better without positional embeddings and can effectively extrapolate to longer sequences during fine-tuning.","authors":["Shweta Verma","Abhinav Anand","Mira Mezini"],"url":"https://arxiv.org/abs/2505.01475"}
{"created":"2025-05-06","title":"Adaptive Passive Beamforming in RIS-Aided Communications With Q-Learning","abstract":"Reconfigurable Intelligent Surfaces (RIS) appear as a promising solution to combat wireless channel fading and interferences. However, the elements of the RIS need to be properly oriented to boost the data transmission rate. In this work, we propose a new strategy to adaptively configure the RIS without Channel State Information (CSI). Our goal is to minimize the number of RIS configurations to be tested to find the optimal one. We formulate the problem as a stochastic shortest path problem, and use Q-Learning to solve it.","authors":["Thomas Ch\\^ene","Ouma\\\"ima Bounhar","Ghaya Rekaya-Ben Othman","Oussama Damen"],"url":"https://arxiv.org/abs/2505.01478"}
{"created":"2025-05-06","title":"SymPlanner: Deliberate Planning in Language Models with Symbolic Representation","abstract":"Planning remains a core challenge for language models (LMs), particularly in domains that require coherent multi-step action sequences grounded in external constraints. We introduce SymPlanner, a novel framework that equips LMs with structured planning capabilities by interfacing them with a symbolic environment that serves as an explicit world model. Rather than relying purely on natural language reasoning, SymPlanner grounds the planning process in a symbolic state space, where a policy model proposes actions and a symbolic environment deterministically executes and verifies their effects. To enhance exploration and improve robustness, we introduce Iterative Correction (IC), which refines previously proposed actions by leveraging feedback from the symbolic environment to eliminate invalid decisions and guide the model toward valid alternatives. Additionally, Contrastive Ranking (CR) enables fine-grained comparison of candidate plans by evaluating them jointly. We evaluate SymPlanner on PlanBench, demonstrating that it produces more coherent, diverse, and verifiable plans than pure natural language baselines.","authors":["Siheng Xiong","Jieyu Zhou","Zhangding Liu","Yusen Su"],"url":"https://arxiv.org/abs/2505.01479"}
{"created":"2025-05-06","title":"Drawing maps on oriented surfaces","abstract":"In this article we describe a program -- called planar_draw -- to draw maps on oriented surfaces in the plane. The drawings are coded as tikz","authors":["Gunnar Brinkmann"],"url":"https://arxiv.org/abs/2505.01480"}
{"created":"2025-05-06","title":"VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations for Synthetic Videos","abstract":"Synthetic video generation with foundation models has gained attention for its realism and wide applications. While these models produce high-quality frames, they often fail to respect common sense and physical laws, resulting in abnormal content. Existing metrics like VideoScore emphasize general quality but ignore such violations and lack interpretability. A more insightful approach is using multi-modal large language models (MLLMs) as interpretable evaluators, as seen in FactScore. Yet, MLLMs' ability to detect abnormalities in synthetic videos remains underexplored. To address this, we introduce VideoHallu, a benchmark featuring synthetic videos from models like Veo2, Sora, and Kling, paired with expert-designed QA tasks solvable via human-level reasoning across various categories. We assess several SoTA MLLMs, including GPT-4o, Gemini-2.5-Pro, Qwen-2.5-VL, and newer models like Video-R1 and VideoChat-R1. Despite strong real-world performance on MVBench and MovieChat, these models still hallucinate on basic commonsense and physics tasks in synthetic settings, underscoring the challenge of hallucination. We further fine-tune SoTA MLLMs using Group Relative Policy Optimization (GRPO) on real and synthetic commonsense/physics data. Results show notable accuracy gains, especially with counterexample integration, advancing MLLMs' reasoning capabilities. Our data is available at https://github.com/zli12321/VideoHallu.","authors":["Zongxia Li","Xiyang Wu","Yubin Qin","Guangyao Shi","Hongyang Du","Dinesh Manocha","Tianyi Zhou","Jordan Lee Boyd-Graber"],"url":"https://arxiv.org/abs/2505.01481"}
{"created":"2025-05-06","title":"Understanding LLM Scientific Reasoning through Promptings and Model's Explanation on the Answers","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding, reasoning, and problem-solving across various domains. However, their ability to perform complex, multi-step reasoning task-essential for applications in science, medicine, and law-remains an area of active investigation. This paper examines the reasoning capabilities of contemporary LLMs, analyzing their strengths, limitations, and potential for improvement. The study uses prompt engineering techniques on the Graduate-Level GoogleProof Q&amp;A (GPQA) dataset to assess the scientific reasoning of GPT-4o. Five popular prompt engineering techniques and two tailored promptings were tested: baseline direct answer (zero-shot), chain-of-thought (CoT), zero-shot CoT, self-ask, self-consistency, decomposition, and multipath promptings. Our findings indicate that while LLMs exhibit emergent reasoning abilities, they often rely on pattern recognition rather than true logical inference, leading to inconsistencies in complex problem-solving. The results indicated that self-consistency outperformed the other prompt engineering technique with an accuracy of 52.99%, followed by direct answer (52.23%). Zero-shot CoT (50%) outperformed multipath (48.44%), decomposition (47.77%), self-ask (46.88%), and CoT (43.75%). Self-consistency performed the second worst in explaining the answers. Simple techniques such as direct answer, CoT, and zero-shot CoT have the best scientific reasoning. We propose a research agenda aimed at bridging these gaps by integrating structured reasoning frameworks, hybrid AI approaches, and human-in-the-loop methodologies. By critically evaluating the reasoning mechanisms of LLMs, this paper contributes to the ongoing discourse on the future of artificial general intelligence and the development of more robust, trustworthy AI systems.","authors":["Alice Rueda","Mohammed S. Hassan","Argyrios Perivolaris","Bazen G. Teferra","Reza Samavi","Sirisha Rambhatla","Yuqi Wu","Yanbo Zhang","Bo Cao","Divya Sharma","Sridhar Krishnan Venkat Bhat"],"url":"https://arxiv.org/abs/2505.01482"}
{"created":"2025-05-06","title":"LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps","abstract":"Given a text, can we determine whether it was generated by a large language model (LLM) or by a human? A widely studied approach to this problem is watermarking. We propose an undetectable and elementary watermarking scheme in the closed setting. Also, in the harder open setting, where the adversary has access to most of the model, we propose an unremovable watermarking scheme.","authors":["Pedro Abdalla","Roman Vershynin"],"url":"https://arxiv.org/abs/2505.01484"}
{"created":"2025-05-06","title":"CHORUS: Zero-shot Hierarchical Retrieval and Orchestration for Generating Linear Programming Code","abstract":"Linear Programming (LP) problems aim to find the optimal solution to an objective under constraints. These problems typically require domain knowledge, mathematical skills, and programming ability, presenting significant challenges for non-experts. This study explores the efficiency of Large Language Models (LLMs) in generating solver-specific LP code. We propose CHORUS, a retrieval-augmented generation (RAG) framework for synthesizing Gurobi-based LP code from natural language problem statements. CHORUS incorporates a hierarchical tree-like chunking strategy for theoretical contents and generates additional metadata based on code examples from documentation to facilitate self-contained, semantically coherent retrieval. Two-stage retrieval approach of CHORUS followed by cross-encoder reranking further ensures contextual relevance. Finally, expertly crafted prompt and structured parser with reasoning steps improve code generation performance significantly. Experiments on the NL4Opt-Code benchmark show that CHORUS improves the performance of open-source LLMs such as Llama3.1 (8B), Llama3.3 (70B), Phi4 (14B), Deepseek-r1 (32B), and Qwen2.5-coder (32B) by a significant margin compared to baseline and conventional RAG. It also allows these open-source LLMs to outperform or match the performance of much stronger baselines-GPT3.5 and GPT4 while requiring far fewer computational resources. Ablation studies further demonstrate the importance of expert prompting, hierarchical chunking, and structured reasoning.","authors":["Tasnim Ahmed","Salimur Choudhury"],"url":"https://arxiv.org/abs/2505.01485"}
{"created":"2025-05-06","title":"Aerial Path Online Planning for Urban Scene Updation","abstract":"We present the first scene-update aerial path planning algorithm specifically designed for detecting and updating change areas in urban environments. While existing methods for large-scale 3D urban scene reconstruction focus on achieving high accuracy and completeness, they are inefficient for scenarios requiring periodic updates, as they often re-explore and reconstruct entire scenes, wasting significant time and resources on unchanged areas. To address this limitation, our method leverages prior reconstructions and change probability statistics to guide UAVs in detecting and focusing on areas likely to have changed. Our approach introduces a novel changeability heuristic to evaluate the likelihood of changes, driving the planning of two flight paths: a prior path informed by static priors and a dynamic real-time path that adapts to newly detected changes. The framework integrates surface sampling and candidate view generation strategies, ensuring efficient coverage of change areas with minimal redundancy. Extensive experiments on real-world urban datasets demonstrate that our method significantly reduces flight time and computational overhead, while maintaining high-quality updates comparable to full-scene re-exploration and reconstruction. These contributions pave the way for efficient, scalable, and adaptive UAV-based scene updates in complex urban environments.","authors":["Mingfeng Tang (Shenzhen University","Shenzhen","China)","Ziyuan Xie (Shenzhen University","Shenzhen","China)","Ke Xie (Shenzhen University","Shenzhen","China)","Hui Huang (Shenzhen University","Shenzhen","China)","Jianwei Hu (QiYuan Lab","Beijing","China)","Ningna Wang (University of Texas at Dallas","Richardson","United States of America)","Xiaohu Guo (University of Texas at Dallas","Richardson","United States of America)"],"url":"https://arxiv.org/abs/2505.01486"}
{"created":"2025-05-06","title":"Outlier-free isogeometric discretizations for Laplace eigenvalue problems: closed-form eigenvalue and eigenvector expressions","abstract":"We derive explicit closed-form expressions for the eigenvalues and eigenvectors of the matrices resulting from isogeometric Galerkin discretizations based on outlier-free spline subspaces for the Laplace operator, under different types of homogeneous boundary conditions on bounded intervals. For optimal spline subspaces and specific reduced spline spaces, represented in terms of B-spline-like bases, we show that the corresponding mass and stiffness matrices exhibit a Toeplitz-minus-Hankel or Toeplitz-plus-Hankel structure. Such matrix structure holds for any degree p and implies that the eigenvalues are an explicitly known sampling of the spectral symbol of the Toeplitz part. Moreover, by employing tensor-product arguments, we extend the closed-form property of the eigenvalues and eigenvectors to a d-dimensional box. As a side result, we have an algebraic confirmation that the considered optimal and reduced spline spaces are indeed outlier-free.","authors":["Noureddine Lamsahel","Carla Manni","Ahmed Ratnani","Stefano Serra-Capizzano","Hendrik Speleers"],"url":"https://arxiv.org/abs/2505.01487"}
{"created":"2025-05-06","title":"Explainable Machine Learning for Cyberattack Identification from Traffic Flows","abstract":"The increasing automation of traffic management systems has made them prime targets for cyberattacks, disrupting urban mobility and public safety. Traditional network-layer defenses are often inaccessible to transportation agencies, necessitating a machine learning-based approach that relies solely on traffic flow data. In this study, we simulate cyberattacks in a semi-realistic environment, using a virtualized traffic network to analyze disruption patterns. We develop a deep learning-based anomaly detection system, demonstrating that Longest Stop Duration and Total Jam Distance are key indicators of compromised signals. To enhance interpretability, we apply Explainable AI (XAI) techniques, identifying critical decision factors and diagnosing misclassification errors. Our analysis reveals two primary challenges: transitional data inconsistencies, where mislabeled recovery-phase traffic misleads the model, and model limitations, where stealth attacks in low-traffic conditions evade detection. This work enhances AI-driven traffic security, improving both detection accuracy and trustworthiness in smart transportation systems.","authors":["Yujing Zhou","Marc L. Jacquet","Robel Dawit","Skyler Fabre","Dev Sarawat","Faheem Khan","Madison Newell","Yongxin Liu","Dahai Liu","Hongyun Chen","Jian Wang","Huihui Wang"],"url":"https://arxiv.org/abs/2505.01488"}
{"created":"2025-05-06","title":"Machine Learning for Cyber-Attack Identification from Traffic Flows","abstract":"This paper presents our simulation of cyber-attacks and detection strategies on the traffic control system in Daytona Beach, FL. using Raspberry Pi virtual machines and the OPNSense firewall, along with traffic dynamics from SUMO and exploitation via the Metasploit framework. We try to answer the research questions: are we able to identify cyber attacks by only analyzing traffic flow patterns. In this research, the cyber attacks are focused particularly when lights are randomly turned all green or red at busy intersections by adversarial attackers. Despite challenges stemming from imbalanced data and overlapping traffic patterns, our best model shows 85\\% accuracy when detecting intrusions purely using traffic flow statistics. Key indicators for successful detection included occupancy, jam length, and halting durations.","authors":["Yujing Zhou","Marc L. Jacquet","Robel Dawit","Skyler Fabre","Dev Sarawat","Faheem Khan","Madison Newell","Yongxin Liu","Dahai Liu","Hongyun Chen","Jian Wang","Huihui Wang"],"url":"https://arxiv.org/abs/2505.01489"}
{"created":"2025-05-06","title":"WorldGenBench: A World-Knowledge-Integrated Benchmark for Reasoning-Driven Text-to-Image Generation","abstract":"Recent advances in text-to-image (T2I) generation have achieved impressive results, yet existing models still struggle with prompts that require rich world knowledge and implicit reasoning: both of which are critical for producing semantically accurate, coherent, and contextually appropriate images in real-world scenarios. To address this gap, we introduce \\textbf{WorldGenBench}, a benchmark designed to systematically evaluate T2I models' world knowledge grounding and implicit inferential capabilities, covering both the humanities and nature domains. We propose the \\textbf{Knowledge Checklist Score}, a structured metric that measures how well generated images satisfy key semantic expectations. Experiments across 21 state-of-the-art models reveal that while diffusion models lead among open-source methods, proprietary auto-regressive models like GPT-4o exhibit significantly stronger reasoning and knowledge integration. Our findings highlight the need for deeper understanding and inference capabilities in next-generation T2I systems. Project Page: \\href{https://dwanzhang-ai.github.io/WorldGenBench/}{https://dwanzhang-ai.github.io/WorldGenBench/}","authors":["Daoan Zhang","Che Jiang","Ruoshi Xu","Biaoxiang Chen","Zijian Jin","Yutian Lu","Jianguo Zhang","Liang Yong","Jiebo Luo","Shengda Luo"],"url":"https://arxiv.org/abs/2505.01490"}
{"created":"2025-05-06","title":"Securing the Future of IVR: AI-Driven Innovation with Agile Security, Data Regulation, and Ethical AI Integration","abstract":"The rapid digitalization of communication systems has elevated Interactive Voice Response (IVR) technologies to become critical interfaces for customer engagement. With Artificial Intelligence (AI) now driving these platforms, ensuring secure, compliant, and ethically designed development practices is more imperative than ever. AI-powered IVRs leverage Natural Language Processing (NLP) and Machine Learning (ML) to personalize interactions, automate service delivery, and optimize user experiences. However, these innovations expose systems to heightened risks, including data privacy breaches, AI decision opacity, and model security vulnerabilities. This paper analyzes the evolution of IVRs from static code-based designs to adaptive AI-driven systems, presenting a cybersecurity-centric perspective. We propose a practical governance framework that embeds agile security principles, compliance with global data legislation, and user-centric ethics. Emphasizing privacy-by-design, adaptive risk modeling, and transparency, the paper argues that ethical AI integration is not a feature but a strategic imperative. Through this multidimensional lens, we highlight how modern IVRs can transition from communication tools to intelligent, secure, and accountable digital frontlines-resilient against emerging threats and aligned with societal expectations.","authors":["Khushbu Mehboob Shaikh","Georgios Giannakopoulos"],"url":"https://arxiv.org/abs/2505.01514"}
{"created":"2025-05-06","title":"Comparison of Waymo Rider-Only Crash Rates by Crash Type to Human Benchmarks at 56.7 Million Miles","abstract":"SAE Level 4 Automated Driving Systems (ADSs) are deployed on public roads, including Waymo's Rider-Only (RO) ride-hailing service (without a driver behind the steering wheel). The objective of this study was to perform a retrospective safety assessment of Waymo's RO crash rate compared to human benchmarks, including disaggregated by crash type.","authors":["Kristofer D. Kusano","John M. Scanlon","Yin-Hsiu Chen","Timothy L. McMurry","Tilia Gode","Trent Victor"],"url":"https://arxiv.org/abs/2505.01515"}
{"created":"2025-05-06","title":"Rubber Mallet: A Study of High Frequency Localized Bit Flips and Their Impact on Security","abstract":"The increasing density of modern DRAM has heightened its vulnerability to Rowhammer attacks, which induce bit flips by repeatedly accessing specific memory rows. This paper presents an analysis of bit flip patterns generated by advanced Rowhammer techniques that bypass existing hardware defenses. First, we investigate the phenomenon of adjacent bit flips--where two or more physically neighboring bits are corrupted simultaneously--and demonstrate they occur with significantly higher frequency than previously documented. We also show that if multiple bits flip within a byte, they are more likely to be adjacent than randomly distributed: for example, if 4 bits flip within a byte, there is an 87% chance that they are all adjacent. We also demonstrate that bit flips within a row will naturally cluster together likely due to the underlying physics of the attack. We then investigate two fault injection attacks enabled by multiple adjacent or nearby bit flips. First, we show how these correlated flips enable efficient cryptographic signature correction attacks, successfully recovering ECDSA private keys from OpenSSL implementations where single-bit approaches would be unfeasible. Second, we introduce a targeted attack against large language models by exploiting Rowhammer-induced corruptions in tokenizer dictionaries of GGUF model files. This attack effectively rewrites safety instructions in system prompts by swapping safety-critical tokens with benign alternatives, circumventing model guardrails while maintaining normal functionality in other contexts. Our experimental results across multiple DRAM configurations reveal that current memory protection schemes are inadequate against these sophisticated attack vectors, which can achieve their objectives with precise, minimal modifications rather than random corruption.","authors":["Andrew Adiletta","Zane Weissman","Fatemeh Khojasteh Dana","Berk Sunar","Shahin Tajik"],"url":"https://arxiv.org/abs/2505.01518"}
{"created":"2025-05-06","title":"Content and Quality Analysis of Parent-Facing Applications for Feeding Children with Autism Spectrum Disorder","abstract":"Approximately 1 in 100 children worldwide are diagnosed with Autism Spectrum Disorder (ASD), and 46% to 89% experience significant feeding difficulties. Although mobile health (mHealth) applications offer potential support for caregivers, the quality and relevance of apps targeting autism-related feeding issues remain unclear. This systematic review evaluated mobile applications available on the Apple App Store and the Google Play Store between September and October 2024. The searches were carried out using 15 predefined terms (e.g., \"child autism feeding\", \"child autism food\"). Applications were eligible if they were in English, free to download, updated within the past year, explicitly addressed feeding in children with autism, accessible in Africa, and had more than 100 downloads. Of the 326 apps identified, only two iOS applications met all inclusion criteria; no Android apps qualified. Behavior Change Wheel (BCW) analysis showed that the selected applications incorporated multiple intervention functions, such as education, training, enablement, incentivization, and modeling, though none addressed the full spectrum of behavioral strategies. Mobile App Rating Scale (MARS) indicated moderate to high usability, with features such as sensory-friendly food routines and structured caregiver tools. However, both apps lacked clinical validation and comprehensive customization. These findings highlight a critical gap in the availability of evidence-based high-quality mHealth tools for caregivers managing ASD-related feeding challenges and underscore the need for professionally developed and culturally sensitive digital solutions.","authors":["Christopher Cofie Kuzagbe (Carnegie Mellon University Africa","Kigali","Rwanda)","Fabrice Mukarage (Carnegie Mellon University Africa","Kigali","Rwanda)","Skye Nandi Adams (University of the Witwatersrand","Johannesburg","South Africa)","N'guessan Yves-Roland Douha (Carnegie Mellon University Africa","Kigali","Rwanda)","Edith Talina Luhanga (Carnegie Mellon University Africa","Kigali","Rwanda)"],"url":"https://arxiv.org/abs/2505.01520"}
{"created":"2025-05-06","title":"Subset Selection for Fine-Tuning: A Utility-Diversity Balanced Approach for Mathematical Domain Adaptation","abstract":"We propose a refined approach to efficiently fine-tune large language models (LLMs) on specific domains like the mathematical domain by employing a budgeted subset selection method. Our approach combines utility and diversity metrics to select the most informative and representative training examples. The final goal is to achieve near-full dataset performance with meticulously selected data points from the entire dataset while significantly reducing computational cost and training time and achieving competitive performance as the full dataset. The utility metric incorporates both perplexity and Chain-of-Thought (CoT) loss to identify challenging examples that contribute most to model learning, while the diversity metric ensures broad coverage across mathematical subdomains. We evaluate our method on LLaMA-3 8B and Phi-3 models, comparing against several baseline approaches, including random selection, diversity-based sampling, and existing state-of-the-art subset selection techniques.","authors":["Madhav Kotecha","Vijendra Kumar Vaishya","Smita Gautam","Suraj Racha"],"url":"https://arxiv.org/abs/2505.01523"}
{"created":"2025-05-06","title":"The DCR Delusion: Measuring the Privacy Risk of Synthetic Data","abstract":"Synthetic data has become an increasingly popular way to share data without revealing sensitive information. Though Membership Inference Attacks (MIAs) are widely considered the gold standard for empirically assessing the privacy of a synthetic dataset, practitioners and researchers often rely on simpler proxy metrics such as Distance to Closest Record (DCR). These metrics estimate privacy by measuring the similarity between the training data and generated synthetic data. This similarity is also compared against that between the training data and a disjoint holdout set of real records to construct a binary privacy test. If the synthetic data is not more similar to the training data than the holdout set is, it passes the test and is considered private. In this work we show that, while computationally inexpensive, DCR and other distance-based metrics fail to identify privacy leakage. Across multiple datasets and both classical models such as Baynet and CTGAN and more recent diffusion models, we show that datasets deemed private by proxy metrics are highly vulnerable to MIAs. We similarly find both the binary privacy test and the continuous measure based on these metrics to be uninformative of actual membership inference risk. We further show that these failures are consistent across different metric hyperparameter settings and record selection methods. Finally, we argue DCR and other distance-based metrics to be flawed by design and show a example of a simple leakage they miss in practice. With this work, we hope to motivate practitioners to move away from proxy metrics to MIAs as the rigorous, comprehensive standard of evaluating privacy of synthetic data, in particular to make claims of datasets being legally anonymous.","authors":["Zexi Yao","Nata\\v{s}a Kr\\v{c}o","Georgi Ganev","Yves-Alexandre de Montjoye"],"url":"https://arxiv.org/abs/2505.01524"}
{"created":"2025-05-06","title":"Automated Parsing of Engineering Drawings for Structured Information Extraction Using a Fine-tuned Document Understanding Transformer","abstract":"Accurate extraction of key information from 2D engineering drawings is crucial for high-precision manufacturing. Manual extraction is time-consuming and error-prone, while traditional Optical Character Recognition (OCR) techniques often struggle with complex layouts and overlapping symbols, resulting in unstructured outputs. To address these challenges, this paper proposes a novel hybrid deep learning framework for structured information extraction by integrating an oriented bounding box (OBB) detection model with a transformer-based document parsing model (Donut). An in-house annotated dataset is used to train YOLOv11 for detecting nine key categories: Geometric Dimensioning and Tolerancing (GD&amp;T), General Tolerances, Measures, Materials, Notes, Radii, Surface Roughness, Threads, and Title Blocks. Detected OBBs are cropped into images and labeled to fine-tune Donut for structured JSON output. Fine-tuning strategies include a single model trained across all categories and category-specific models. Results show that the single model consistently outperforms category-specific ones across all evaluation metrics, achieving higher precision (94.77% for GD&amp;T), recall (100% for most), and F1 score (97.3%), while reducing hallucination (5.23%). The proposed framework improves accuracy, reduces manual effort, and supports scalable deployment in precision-driven industries.","authors":["Muhammad Tayyab Khan","Zane Yong","Lequn Chen","Jun Ming Tan","Wenhe Feng","Seung Ki Moon"],"url":"https://arxiv.org/abs/2505.01530"}
{"created":"2025-05-06","title":"Disassembly as Weighted Interval Scheduling with Learned Weights","abstract":"Disassembly is the first step of a variety of binary analysis and transformation techniques, such as reverse engineering, or binary rewriting. Recent disassembly approaches consist of three phases: an exploration phase, that overapproximates the binary's code; an analysis phase, that assigns weights to candidate instructions or basic blocks; and a conflict resolution phase, that downselects the final set of instructions. We present a disassembly algorithm that generalizes this pattern for a wide range of architectures, namely x86, x64, arm32, and aarch64. Our algorithm presents a novel conflict resolution method that reduces disassembly to weighted interval scheduling.","authors":["Antonio Flores-Montoya","Junghee Lim","Adam Seitz","Akshay Sood","Edward Raff","James Holt"],"url":"https://arxiv.org/abs/2505.01536"}
{"created":"2025-05-06","title":"Passing the Buck to AI: How Individuals' Decision-Making Patterns Affect Reliance on AI","abstract":"Psychological research has identified different patterns individuals have while making decisions, such as vigilance (making decisions after thorough information gathering), hypervigilance (rushed and anxious decision-making), and buckpassing (deferring decisions to others). We examine whether these decision-making patterns shape peoples' likelihood of seeking out or relying on AI. In an online experiment with 810 participants tasked with distinguishing food facts from myths, we found that a higher buckpassing tendency was positively correlated with both seeking out and relying on AI suggestions, while being negatively correlated with the time spent reading AI explanations. In contrast, the higher a participant tended towards vigilance, the more carefully they scrutinized the AI's information, as indicated by an increased time spent looking through the AI's explanations. These findings suggest that a person's decision-making pattern plays a significant role in their adoption and reliance on AI, which provides a new understanding of individual differences in AI-assisted decision-making.","authors":["Katelyn Xiaoying Mei","Rock Yuren Pang","Alex Lyford","Lucy Lu Wang","Katharina Reinecke"],"url":"https://arxiv.org/abs/2505.01537"}
{"created":"2025-05-06","title":"HoneyBee: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning","abstract":"As vector databases gain traction in enterprise applications, robust access control has become critical to safeguard sensitive data. Access control in these systems is often implemented through hybrid vector queries, which combine nearest neighbor search on vector data with relational predicates based on user permissions. However, existing approaches face significant trade-offs: creating dedicated indexes for each user minimizes query latency but introduces excessive storage redundancy, while building a single index and applying access control after vector search reduces storage overhead but suffers from poor recall and increased query latency. This paper introduces HoneyBee, a dynamic partitioning framework that bridges the gap between these approaches by leveraging the structure of Role-Based Access Control (RBAC) policies. RBAC, widely adopted in enterprise settings, groups users into roles and assigns permissions to those roles, creating a natural \"thin waist\" in the permission structure that is ideal for partitioning decisions. Specifically, HoneyBee produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling storage overhead. By introducing analytical models for the performance and recall of the vector search, HoneyBee formulates the partitioning strategy as a constrained optimization problem to dynamically balance storage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HoneyBee reduces storage redundancy compared to role partitioning and achieves up to 6x faster query speeds than row-level security (RLS) with only 1.4x storage increase, offering a practical middle ground for secure and efficient vector search.","authors":["Hongbin Zhong","Matthew Lentz","Nina Narodytska","Adriana Szekeres","Kexin Rong"],"url":"https://arxiv.org/abs/2505.01538"}
{"created":"2025-05-06","title":"Parameterized Argumentation-based Reasoning Tasks for Benchmarking Generative Language Models","abstract":"Generative large language models as tools in the legal domain have the potential to improve the justice system. However, the reasoning behavior of current generative models is brittle and poorly understood, hence cannot be responsibly applied in the domains of law and evidence. In this paper, we introduce an approach for creating benchmarks that can be used to evaluate the reasoning capabilities of generative language models. These benchmarks are dynamically varied, scalable in their complexity, and have formally unambiguous interpretations. In this study, we illustrate the approach on the basis of witness testimony, focusing on the underlying argument attack structure. We dynamically generate both linear and non-linear argument attack graphs of varying complexity and translate these into reasoning puzzles about witness testimony expressed in natural language. We show that state-of-the-art large language models often fail in these reasoning puzzles, already at low complexity. Obvious mistakes are made by the models, and their inconsistent performance indicates that their reasoning capabilities are brittle. Furthermore, at higher complexity, even state-of-the-art models specifically presented for reasoning capabilities make mistakes. We show the viability of using a parametrized benchmark with varying complexity to evaluate the reasoning capabilities of generative language models. As such, the findings contribute to a better understanding of the limitations of the reasoning capabilities of generative models, which is essential when designing responsible AI systems in the legal domain.","authors":["Cor Steging","Silja Renooij","Bart Verheij"],"url":"https://arxiv.org/abs/2505.01539"}
{"created":"2025-05-06","title":"Emotions in the Loop: A Survey of Affective Computing for Emotional Support","abstract":"In a world where technology is increasingly embedded in our everyday experiences, systems that sense and respond to human emotions are elevating digital interaction. At the intersection of artificial intelligence and human-computer interaction, affective computing is emerging with innovative solutions where machines are humanized by enabling them to process and respond to user emotions. This survey paper explores recent research contributions in affective computing applications in the area of emotion recognition, sentiment analysis and personality assignment developed using approaches like large language models (LLMs), multimodal techniques, and personalized AI systems. We analyze the key contributions and innovative methodologies applied by the selected research papers by categorizing them into four domains: AI chatbot applications, multimodal input systems, mental health and therapy applications, and affective computing for safety applications. We then highlight the technological strengths as well as the research gaps and challenges related to these studies. Furthermore, the paper examines the datasets used in each study, highlighting how modality, scale, and diversity impact the development and performance of affective models. Finally, the survey outlines ethical considerations and proposes future directions to develop applications that are more safe, empathetic and practical.","authors":["Karishma Hegde","Hemadri Jayalath"],"url":"https://arxiv.org/abs/2505.01542"}
{"created":"2025-05-06","title":"ASAP-MO:Advanced Situational Awareness and Perception for Mission-critical Operations","abstract":"Deploying robotic missions can be challenging due to the complexity of controlling robots with multiple degrees of freedom, fusing diverse sensory inputs, and managing communication delays and interferences. In nuclear inspection, robots can be crucial in assessing environments where human presence is limited, requiring precise teleoperation and coordination. Teleoperation requires extensive training, as operators must process multiple outputs while ensuring safe interaction with critical assets. These challenges are amplified when operating a fleet of heterogeneous robots across multiple environments, as each robot may have distinct control interfaces, sensory systems, and operational constraints. Efficient coordination in such settings remains an open problem. This paper presents a field report on how we integrated robot fleet capabilities - including mapping, localization, and telecommunication - toward a joint mission. We simulated a nuclear inspection scenario for exposed areas, using lights to represent a radiation source. We deployed two Unmanned Ground Vehicles (UGVs) tasked with mapping indoor and outdoor environments while remotely controlled from a single base station. Despite having distinct operational goals, the robots produced a unified map output, demonstrating the feasibility of coordinated multi-robot missions. Our results highlight key operational challenges and provide insights into improving adaptability and situational awareness in remote robotic deployments.","authors":["Veronica Vannini","William Dubois","Olivier Gamache","Jean-Michel Fortin","Nicolas Samson","Effie Daum","Fran\\c{c}ois Pomerleau","Edith Brotherton"],"url":"https://arxiv.org/abs/2505.01547"}
{"created":"2025-05-06","title":"Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation","abstract":"Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet.","authors":["Zhen Yao","Xiaowen Ying","Mooi Choo Chuah"],"url":"https://arxiv.org/abs/2505.01548"}
{"created":"2025-05-06","title":"A Decision-Focused Predict-then-Bid Framework for Strategic Energy Storage","abstract":"This paper introduces a novel decision-focused framework for energy storage arbitrage bidding. Inspired by the bidding process for energy storage in electricity markets, we propose a \"predict-then-bid\" end-to-end method incorporating the storage arbitrage optimization and market clearing models.This is achieved through a tri-layer framework that combines a price prediction layer with a two-stage optimization problem: an energy storage optimization layer and a market-clearing optimization layer. We leverage the implicit function theorem for gradient computation in the first optimization layer and incorporate a perturbation-based approach into the decision-focused loss function to ensure differentiability in the market-clearing layer. Numerical experiments using electricity market data from New York demonstrate that our bidding design substantially outperforms existing methods, achieving the highest profits and showcasing the effectiveness of the proposed approach.","authors":["Ming Yi","Yiqian Wu","Saud Alghumayjan","James Anderson","Bolun Xu"],"url":"https://arxiv.org/abs/2505.01551"}
{"created":"2025-05-06","title":"On Solving Simple Curved Nonograms","abstract":"Nonograms are a popular type of puzzle, where an arrangement of curves in the plane (in the classic version, a rectangular grid) is given together with a series of hints, indicating which cells of the subdivision are to be colored. The colored cells yield an image. Curved nonograms use a curve arrangement rather than a grid, leading to a closer approximation of an arbitrary solution image. While there is a considerable amount of previous work on the natural question of the hardness of solving a classic nonogram, research on curved nonograms has so far focused on their creation, which is already highly non-trivial. We address this gap by providing algorithmic and hardness results for curved nonograms of varying complexity.","authors":["Maarten L\\\"offler","G\\\"unter Rote","Soeren Terziadis","Alexandra Weinberger"],"url":"https://arxiv.org/abs/2505.01554"}
{"created":"2025-05-06","title":"Optimising Kernel-based Multivariate Statistical Process Control","abstract":"Multivariate Statistical Process Control (MSPC) is a framework for monitoring and diagnosing complex processes by analysing the relationships between multiple process variables simultaneously. Kernel MSPC extends the methodology by leveraging kernel functions to capture non-linear relationships between the data, enhancing the process monitoring capabilities. However, optimising the kernel MSPC parameters, such as the kernel type and kernel parameters, is often done in literature in time-consuming and non-procedural manners such as cross-validation or grid search. In the present paper, we propose optimising the kernel MSPC parameters with Kernel Flows (KF), a recent kernel learning methodology introduced for Gaussian Process Regression (GPR). Apart from the optimisation technique, the novelty of the study resides also in the utilisation of kernel combinations for learning the optimal kernel type, and introduces individual kernel parameters for each variable. The proposed methodology is evaluated with multiple cases from the benchmark Tennessee Eastman Process. The faults are detected for all evaluated cases, including the ones not detected in the original study.","authors":["Zina-Sabrina Duma","Victoria Jorry","Tuomas Sihvonen","Satu-Pia Reinikainen","Lassi Roininen"],"url":"https://arxiv.org/abs/2505.01556"}
{"created":"2025-05-06","title":"Contextures: Representations from Contexts","abstract":"Despite the empirical success of foundation models, we do not have a systematic characterization of the representations that these models learn. In this paper, we establish the contexture theory. It shows that a large class of representation learning methods can be characterized as learning from the association between the input and a context variable. Specifically, we show that many popular methods aim to approximate the top-d singular functions of the expectation operator induced by the context, in which case we say that the representation learns the contexture. We demonstrate the generality of the contexture theory by proving that representation learning within various learning paradigms -- supervised, self-supervised, and manifold learning -- can all be studied from such a perspective. We also prove that the representations that learn the contexture are optimal on those tasks that are compatible with the context. One important implication of the contexture theory is that once the model is large enough to approximate the top singular functions, further scaling up the model size yields diminishing returns. Therefore, scaling is not all we need, and further improvement requires better contexts. To this end, we study how to evaluate the usefulness of a context without knowing the downstream tasks. We propose a metric and show by experiments that it correlates well with the actual performance of the encoder on many real datasets.","authors":["Runtian Zhai","Kai Yang","Che-Ping Tsai","Burak Varici","Zico Kolter","Pradeep Ravikumar"],"url":"https://arxiv.org/abs/2505.01557"}
{"created":"2025-05-06","title":"A Sensor Agnostic Domain Generalization Framework for Leveraging Geospatial Foundation Models: Enhancing Semantic Segmentation viaSynergistic Pseudo-Labeling and Generative Learning","abstract":"Remote sensing enables a wide range of critical applications such as land cover and land use mapping, crop yield prediction, and environmental monitoring. Advances in satellite technology have expanded remote sensing datasets, yet high-performance segmentation models remain dependent on extensive labeled data, challenged by annotation scarcity and variability across sensors, illumination, and geography. Domain adaptation offers a promising solution to improve model generalization. This paper introduces a domain generalization approach to leveraging emerging geospatial foundation models by combining soft-alignment pseudo-labeling with source-to-target generative pre-training. We further provide new mathematical insights into MAE-based generative learning for domain-invariant feature learning. Experiments with hyperspectral and multispectral remote sensing datasets confirm our method's effectiveness in enhancing adaptability and segmentation.","authors":["Anan Yaghmour","Melba M. Crawford","Saurabh Prasad"],"url":"https://arxiv.org/abs/2505.01558"}
{"created":"2025-05-06","title":"On the effectiveness of Large Language Models in the mechanical design domain","abstract":"In this work, we seek to understand the performance of large language models in the mechanical engineering domain. We leverage the semantic data found in the ABC dataset, specifically the assembly names that designers assigned to the overall assemblies, and the individual semantic part names that were assigned to each part. After pre-processing the data we developed two unsupervised tasks to evaluate how different model architectures perform on domain-specific data: a binary sentence-pair classification task and a zero-shot classification task. We achieved a 0.62 accuracy for the binary sentence-pair classification task with a fine-tuned model that focuses on fighting over-fitting: 1) modifying learning rates, 2) dropout values, 3) Sequence Length, and 4) adding a multi-head attention layer. Our model on the zero-shot classification task outperforms the baselines by a wide margin, and achieves a top-1 classification accuracy of 0.386. The results shed some light on the specific failure modes that arise when learning from language in this domain.","authors":["Daniele Grandi","Fabian Riquelme"],"url":"https://arxiv.org/abs/2505.01559"}
{"created":"2025-05-06","title":"AI agents may be worth the hype but not the resources (yet): An initial exploration of machine translation quality and costs in three language pairs in the legal and news domains","abstract":"Large language models (LLMs) and multi-agent orchestration are touted as the next leap in machine translation (MT), but their benefits relative to conventional neural MT (NMT) remain unclear. This paper offers an empirical reality check. We benchmark five paradigms, Google Translate (strong NMT baseline), GPT-4o (general-purpose LLM), o1-preview (reasoning-enhanced LLM), and two GPT-4o-powered agentic workflows (sequential three-stage and iterative refinement), on test data drawn from a legal contract and news prose in three English-source pairs: Spanish, Catalan and Turkish. Automatic evaluation is performed with COMET, BLEU, chrF2 and TER; human evaluation is conducted with expert ratings of adequacy and fluency; efficiency with total input-plus-output token counts mapped to April 2025 pricing.","authors":["Vicent Briva Iglesias","Gokhan Dogru"],"url":"https://arxiv.org/abs/2505.01560"}
{"created":"2025-05-06","title":"TutorGym: A Testbed for Evaluating AI Agents as Tutors and Students","abstract":"Recent improvements in large language model (LLM) performance on academic benchmarks, such as MATH and GSM8K, have emboldened their use as standalone tutors and as simulations of human learning. However, these new applications require more than evaluations of final solution generation. We introduce TutorGym to evaluate these applications more directly. TutorGym is a standard interface for testing artificial intelligence (AI) agents within existing intelligent tutoring systems (ITS) that have been tested and refined in classroom studies, including Cognitive Tutors (CTAT), Apprentice Tutors, and OATutors. TutorGym is more than a simple problem-solution benchmark, it situates AI agents within the interactive interfaces of existing ITSs. At each step of problem-solving, AI agents are asked what they would do as a tutor or as a learner. As tutors, AI agents are prompted to provide tutoring support -- such as generating examples, hints, and step-level correctness feedback -- which can be evaluated directly against the adaptive step-by-step support provided by existing ITSs. As students, agents directly learn from ITS instruction, and their mistakes and learning trajectories can be compared to student data. TutorGym establishes a common framework for training and evaluating diverse AI agents, including LLMs, computational models of learning, and reinforcement learning agents, within a growing suite of learning environments. Currently, TutorGym includes 223 different tutor domains. In an initial evaluation, we find that current LLMs are poor at tutoring -- none did better than chance at labeling incorrect actions, and next-step actions were correct only ~52-70% of the time -- but they could produce remarkably human-like learning curves when trained as students with in-context learning.","authors":["Daniel Weitekamp","Momin N. Siddiqui","Christopher J. MacLellan"],"url":"https://arxiv.org/abs/2505.01563"}
{"created":"2025-05-06","title":"A Coordinated Routing Approach for Enhancing Bus Timeliness and Travel Efficiency in Mixed-Traffic Environment","abstract":"In this paper, we propose a coordinated routing strategy aimed at improving bus schedule adherence and enhancing travel efficiency for connected and automated vehicles (CAVs) operating within a mixed-traffic urban network. Our approach capitalizes on the existence of dedicated lanes for buses and CAVs, leveraging real-time traffic data to dynamically reroute CAVs in anticipation of congestion. By continuously monitoring traffic conditions on dedicated lanes and tracking the real-time positions of buses, we enable the system to proactively adjust CAV routes when potential interference with bus operations is detected. This coordination mitigates delays affecting transit services and reduces travel time for CAVs. We evaluate the proposed strategy through simulation studies conducted in the SUMO. The results demonstrate significant improvements in both transit reliability and CAV operational performance across a range of traffic conditions.","authors":["Tanlu Liang","Ting Bai","Andreas A. Malikopoulos"],"url":"https://arxiv.org/abs/2505.01566"}
{"created":"2025-05-06","title":"A Defect Taxonomy for Infrastructure as Code: A Replication Study","abstract":"Background: As Infrastructure as Code (IaC) becomes standard practice, ensuring the reliability of IaC scripts is essential. Defect taxonomies are valuable tools for this, offering a common language for issues and enabling systematic tracking. A significant prior study developed such a taxonomy, but based it exclusively on the declarative language Puppet. It remained unknown whether this taxonomy applies to programming language-based IaC (PL-IaC) tools like Pulumi, Terraform CDK, and AWS CDK. Aim: We replicated this foundational work to assess the generalizability of the taxonomy across a broader and more diverse landscape. Method: We performed qualitative analysis on 3,364 defect-related commits from 285 open-source PL-IaC repositories (PIPr dataset) to derive a PL-IaC-specific defect taxonomy. We then enhanced the ACID tool, originally developed for the prior study, to automatically classify and analyze defect distributions across an expanded dataset-447 open-source repositories and 94 proprietary projects from VTEX (e-commerce) and Nubank (financial). Results: Our research confirmed the same eight defect categories identified in the original study, with idempotency and security defects appearing infrequently but persistently across projects. Configuration Data defects maintain high frequency in both open-source and proprietary codebases. Conclusions: Our replication supports the generalizability of the original taxonomy, suggesting IaC development challenges surpass organizational boundaries. Configuration Data defects emerge as a persistent high-frequency problem, while idempotency and security defects remain important concerns despite lower frequency. These patterns appear consistent across open-source and proprietary projects, indicating they are fundamental to the IaC paradigm itself, transcending specific tools or project types.","authors":["Filipe Magno Alves Paiva","Jo\\~ao Arthur Brunet Monteiro","Thiago Emmanuel Pereira da Cunha Silva","Wendell Tom\\'e Marinho Oliveira"],"url":"https://arxiv.org/abs/2505.01568"}
{"created":"2025-05-06","title":"Physics-informed Learning for Passivity-based Tracking Control","abstract":"Passivity-based control ensures system stability by leveraging dissipative properties and is widely applied in electrical and mechanical systems. Port-Hamiltonian systems (PHS), in particular, are well-suited for interconnection and damping assignment passivity-based control (IDA-PBC) due to their structured, energy-centric modeling approach. However, current IDA-PBC faces two key challenges: (i) it requires precise system knowledge, which is often unavailable due to model uncertainties, and (ii) it is typically limited to set-point control. To address these limitations, we propose a data-driven tracking control approach based on a physics-informed model, namely Gaussian process Port-Hamiltonian systems, along with the modified matching equation. By leveraging the Bayesian nature of the model, we establish probabilistic stability and passivity guarantees. A simulation demonstrates the effectiveness of our approach.","authors":["Thomas Beckers","Leonardo Colombo"],"url":"https://arxiv.org/abs/2505.01569"}
{"created":"2025-05-06","title":"PainFormer: a Vision Foundation Model for Automatic Pain Assessment","abstract":"Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities-including RGB, synthetic thermal, and estimated depth videos-and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 73 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment.","authors":["Stefanos Gkikas","Raul Fernandez Rojas","Manolis Tsiknakis"],"url":"https://arxiv.org/abs/2505.01571"}
{"created":"2025-05-06","title":"PipeSpec: Breaking Stage Dependencies in Hierarchical LLM Decoding","abstract":"Speculative decoding accelerates large language model inference by using smaller draft models to generate candidate tokens for parallel verification. However, current approaches are limited by sequential stage dependencies that prevent full hardware utilization. We present PipeSpec, a framework that generalizes speculative decoding to $k$ models arranged in a hierarchical pipeline, enabling asynchronous execution with lightweight coordination for prediction verification and rollback. Our analytical model characterizes token generation rates across pipeline stages and proves guaranteed throughput improvements over traditional decoding for any non-zero acceptance rate. We further derive closed-form expressions for steady-state verification probabilities that explain the empirical benefits of pipeline depth. Experimental results show that PipeSpec achieves up to 2.54$\\times$ speedup while outperforming state-of-the-art methods. We validate PipeSpec across text summarization and code generation tasks using LLaMA 2 and 3 models, demonstrating that pipeline efficiency increases with model depth, providing a scalable approach to accelerating LLM inference on multi-device systems.","authors":["Bradley McDanel","Sai Qian Zhang","Yunhai Hu","Zining Liu"],"url":"https://arxiv.org/abs/2505.01572"}
{"created":"2025-05-06","title":"Embedded System for Recording and Controlling Hand Hygiene in Healthcare Environments","abstract":"Nowadays, more effective control of hand hygiene (HH) by healthcare teams has become essential. HH control is crucial to prevent cross-contamination and healthcare-associated infections (HAI), according to Brazilian regulatory standards and WHO guidelines. The lack of widespread technology to measure acceptable hygiene rates within hospital environments leads to the practice of a manual sample audit reading, requiring more time for decision making. Thus, the present study addresses the lack of automation technologies for HH, aiming to record, measure, and provide data for internal audits in hospitals. This article introduces an embedded system for HH control and recording, comprising low-cost hardware architecture with IoT connectivity and online monitoring. Results with practical evaluation in a real hospital setting for 3 hours demonstrated the system's effectiveness in recording HH indices.","authors":["Rafael Castro","Alexandre dos Santos Roque"],"url":"https://arxiv.org/abs/2505.01576"}
{"created":"2025-05-06","title":"Grounding Task Assistance with Multimodal Cues from a Single Demonstration","abstract":"A person's demonstration often serves as a key reference for others learning the same task. However, RGB video, the dominant medium for representing these demonstrations, often fails to capture fine-grained contextual cues such as intent, safety-critical environmental factors, and subtle preferences embedded in human behavior. This sensory gap fundamentally limits the ability of Vision Language Models (VLMs) to reason about why actions occur and how they should adapt to individual users. To address this, we introduce MICA (Multimodal Interactive Contextualized Assistance), a framework that improves conversational agents for task assistance by integrating eye gaze and speech cues. MICA segments demonstrations into meaningful sub-tasks and extracts keyframes and captions that capture fine-grained intent and user-specific cues, enabling richer contextual grounding for visual question answering. Evaluations on questions derived from real-time chat-assisted task replication show that multimodal cues significantly improve response quality over frame-based retrieval. Notably, gaze cues alone achieves 93% of speech performance, and their combination yields the highest accuracy. Task type determines the effectiveness of implicit (gaze) vs. explicit (speech) cues, underscoring the need for adaptable multimodal models. These results highlight the limitations of frame-based context and demonstrate the value of multimodal signals for real-world AI task assistance.","authors":["Gabriel Sarch","Balasaravanan Thoravi Kumaravel","Sahithya Ravi","Vibhav Vineet","Andrew D. Wilson"],"url":"https://arxiv.org/abs/2505.01578"}
{"created":"2025-05-06","title":"TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action","abstract":"Understanding causal event relationships and achieving fine-grained temporal grounding in videos remain challenging for vision-language models. Existing methods either compress video tokens to reduce temporal resolution, or treat videos as unsegmented streams, which obscures fine-grained event boundaries and limits the modeling of causal dependencies. We propose TEMPURA (Temporal Event Masked Prediction and Understanding for Reasoning in Action), a two-stage training framework that enhances video temporal understanding. TEMPURA first applies masked event prediction reasoning to reconstruct missing events and generate step-by-step causal explanations from dense event annotations, drawing inspiration from effective infilling techniques. TEMPURA then learns to perform video segmentation and dense captioning to decompose videos into non-overlapping events with detailed, timestamp-aligned descriptions. We train TEMPURA on VER, a large-scale dataset curated by us that comprises 1M training instances and 500K videos with temporally aligned event descriptions and structured reasoning steps. Experiments on temporal grounding and highlight detection benchmarks demonstrate that TEMPURA outperforms strong baseline models, confirming that integrating causal reasoning with fine-grained temporal segmentation leads to improved video understanding.","authors":["Jen-Hao Cheng","Vivian Wang","Huayu Wang","Huapeng Zhou","Yi-Hao Peng","Hou-I Liu","Hsiang-Wei Huang","Kuang-Ming Chen","Cheng-Yen Yang","Wenhao Chai","Yi-Ling Chen","Vibhav Vineet","Qin Cai","Jenq-Neng Hwang"],"url":"https://arxiv.org/abs/2505.01583"}
{"created":"2025-05-06","title":"Understanding and Exploiting Plasticity for Non-stationary Network Resource Adaptation","abstract":"Adapting to non-stationary network conditions presents significant challenges for resource adaptation. However, current solutions primarily rely on stationary assumptions. While data-driven reinforcement learning approaches offer promising solutions for handling network dynamics, our systematic investigation reveals a critical limitation: neural networks suffer from plasticity loss, significantly impeding their ability to adapt to evolving network conditions. Through theoretical analysis of neural propagation mechanisms, we demonstrate that existing dormant neuron metrics inadequately characterize neural plasticity loss. To address this limitation, we have developed the Silent Neuron theory, which provides a more comprehensive framework for understanding plasticity degradation. Based on these theoretical insights, we propose the Reset Silent Neuron (ReSiN), which preserves neural plasticity through strategic neuron resets guided by both forward and backward propagation states. In our implementation of an adaptive video streaming system, ReSiN has shown significant improvements over existing solutions, achieving up to 168% higher bitrate and 108% better quality of experience (QoE) while maintaining comparable smoothness. Furthermore, ReSiN consistently outperforms in stationary environments, demonstrating its robust adaptability across different network conditions.","authors":["Zhiqiang He","Zhi Liu"],"url":"https://arxiv.org/abs/2505.01584"}
{"created":"2025-05-06","title":"More efficient sifting for grid norms, and applications to multiparty communication complexity","abstract":"Building on the techniques behind the recent progress on the 3-term arithmetic progression problem [KM'23], Kelley, Lovett, and Meka [KLM'24] constructed the first explicit 3-player function $f:[N]^3 \\rightarrow \\{0,1\\}$ that demonstrates a strong separation between randomized and (non-)deterministic NOF communication complexity. Specifically, their hard function can be solved by a randomized protocol sending $O(1)$ bits, but requires $\\Omega(\\log^{1/3}(N))$ bits of communication with a deterministic (or non-deterministic) protocol.","authors":["Zander Kelley","Xin Lyu"],"url":"https://arxiv.org/abs/2505.01587"}
{"created":"2025-05-06","title":"Phasing Through the Flames: Rapid Motion Planning with the AGHF PDE for Arbitrary Objective Functions and Constraints","abstract":"The generation of optimal trajectories for high-dimensional robotic systems under constraints remains computationally challenging due to the need to simultaneously satisfy dynamic feasibility, input limits, and task-specific objectives while searching over high-dimensional spaces. Recent approaches using the Affine Geometric Heat Flow (AGHF) Partial Differential Equation (PDE) have demonstrated promising results, generating dynamically feasible trajectories for complex systems like the Digit V3 humanoid within seconds. These methods efficiently solve trajectory optimization problems over a two-dimensional domain by evolving an initial trajectory to minimize control effort. However, these AGHF approaches are limited to a single type of optimal control problem (i.e., minimizing the integral of squared control norms) and typically require initial guesses that satisfy constraints to ensure satisfactory convergence. These limitations restrict the potential utility of the AGHF PDE especially when trying to synthesize trajectories for robotic systems. This paper generalizes the AGHF formulation to accommodate arbitrary cost functions, significantly expanding the classes of trajectories that can be generated. This work also introduces a Phase1 - Phase 2 Algorithm that enables the use of constraint-violating initial guesses while guaranteeing satisfactory convergence. The effectiveness of the proposed method is demonstrated through comparative evaluations against state-of-the-art techniques across various dynamical systems and challenging trajectory generation problems. Project Page: https://roahmlab.github.io/BLAZE/","authors":["Challen Enninful Adu","C\\'esar E. Ramos Chuquiure","Yutong Zhou","Pearl Lin","Ruikai Yang","Bohao Zhang","Shubham Singh","Ram Vasudevan"],"url":"https://arxiv.org/abs/2505.01589"}
{"created":"2025-05-06","title":"Machine Learning Fairness in House Price Prediction: A Case Study of America's Expanding Metropolises","abstract":"As a basic human need, housing plays a key role in enhancing health, well-being, and educational outcome in society, and the housing market is a major factor for promoting quality of life and ensuring social equity. To improve the housing conditions, there has been extensive research on building Machine Learning (ML)-driven house price prediction solutions to accurately forecast the future conditions, and help inform actions and policies in the field. In spite of their success in developing high-accuracy models, there is a gap in our understanding of the extent to which various ML-driven house price prediction approaches show ethnic and/or racial bias, which in turn is essential for the responsible use of ML, and ensuring that the ML-driven solutions do not exacerbate inequity. To fill this gap, this paper develops several ML models from a combination of structural and neighborhood-level attributes, and conducts comprehensive assessments on the fairness of ML models under various definitions of privileged groups. As a result, it finds that the ML-driven house price prediction models show various levels of bias towards protected attributes (i.e., race and ethnicity in this study). Then, it investigates the performance of different bias mitigation solutions, and the experimental results show their various levels of effectiveness on different ML-driven methods. However, in general, the in-processing bias mitigation approach tends to be more effective than the pre-processing one in this problem domain. Our code is available at https://github.com/wahab1412/housing_fairness.","authors":["Abdalwahab Almajed","Maryam Tabar","Peyman Najafirad"],"url":"https://arxiv.org/abs/2505.01591"}
{"created":"2025-05-06","title":"PIPA: A Unified Evaluation Protocol for Diagnosing Interactive Planning Agents","abstract":"The growing capabilities of large language models (LLMs) in instruction-following and context-understanding lead to the era of agents with numerous applications. Among these, task planning agents have become especially prominent in realistic scenarios involving complex internal pipelines, such as context understanding, tool management, and response generation. However, existing benchmarks predominantly evaluate agent performance based on task completion as a proxy for overall effectiveness. We hypothesize that merely improving task completion is misaligned with maximizing user satisfaction, as users interact with the entire agentic process and not only the end result. To address this gap, we propose PIPA, a unified evaluation protocol that conceptualizes the behavioral process of interactive task planning agents within a partially observable Markov Decision Process (POMDP) paradigm. The proposed protocol offers a comprehensive assessment of agent performance through a set of atomic evaluation criteria, allowing researchers and practitioners to diagnose specific strengths and weaknesses within the agent's decision-making pipeline. Our analyses show that agents excel in different behavioral stages, with user satisfaction shaped by both outcomes and intermediate behaviors. We also highlight future directions, including systems that leverage multiple agents and the limitations of user simulators in task planning.","authors":["Takyoung Kim","Janvijay Singh","Shuhaib Mehri","Emre Can Acikgoz","Sagnik Mukherjee","Nimet Beyza Bozdag","Sumuk Shashidhar","Gokhan Tur","Dilek Hakkani-T\\\"ur"],"url":"https://arxiv.org/abs/2505.01592"}
{"created":"2025-05-06","title":"Always Tell Me The Odds: Fine-grained Conditional Probability Estimation","abstract":"We present a state-of-the-art model for fine-grained probability estimation of propositions conditioned on context. Recent advances in large language models (LLMs) have significantly enhanced their reasoning capabilities, particularly on well-defined tasks with complete information. However, LLMs continue to struggle with making accurate and well-calibrated probabilistic predictions under uncertainty or partial information. While incorporating uncertainty into model predictions often boosts performance, obtaining reliable estimates of that uncertainty remains understudied. In particular, LLM probability estimates tend to be coarse and biased towards more frequent numbers. Through a combination of human and synthetic data creation and assessment, scaling to larger models, and better supervision, we propose a set of strong and precise probability estimation models. We conduct systematic evaluations across tasks that rely on conditional probability estimation and show that our approach consistently outperforms existing fine-tuned and prompting-based methods by a large margin.","authors":["Liaoyaqi Wang","Zhengping Jiang","Anqi Liu","Benjamin Van Durme"],"url":"https://arxiv.org/abs/2505.01595"}
{"created":"2025-05-06","title":"Advances in Particle Flow Filters with Taylor Expansion Series","abstract":"Particle Flow Filters perform the measurement update by moving particles to a different location rather than modifying the particles' weight based on the likelihood. Their movement (flow) is dictated by a drift term, which continuously pushes the particle toward the posterior distribution, and a diffusion term, which guarantees the spread of particles. This work presents a novel derivation of these terms based on high-order polynomial expansions, where the common techniques based on linearization reduce to a simpler version of the new methodology. Thanks to differential algebra, the high-order particle flow is derived directly onto the polynomials representation of the distribution, embedded with differentiation and evaluation. The resulting technique proposes two new particle flow filters, whose difference relies on the selection of the expansion center for the Taylor polynomial evaluation. Numerical applications show the improvement gained by the inclusion of high-order terms, especially when comparing performance with the Gromov flow and the \"exact\" flow.","authors":["Simone Servadio"],"url":"https://arxiv.org/abs/2505.01597"}
{"created":"2025-05-06","title":"Dynamical Update Maps for Particle Flow with Differential Algebra","abstract":"Particle Flow Filters estimate the ``a posteriori\" probability density function (PDF) by moving an ensemble of particles according to the likelihood. Particles are propagated under the system dynamics until a measurement becomes available when each particle undergoes an additional stochastic differential equation in a pseudo-time that updates the distribution following a homotopy transformation. This flow of particles can be represented as a recursive update step of the filter. In this work, we leverage the Differential Algebra (DA) representation of the solution flow of dynamics to improve the computational burden of particle flow filters. Thanks to this approximation, both the prediction and the update differential equations are solved in the DA framework, creating two sets of polynomial maps: the first propagates particles forward in time while the second updates particles, achieving the flow. The final result is a new particle flow filter that rapidly propagates and updates PDFs using mathematics based on deviation vectors. Numerical applications show the benefits of the proposed technique, especially in reducing computational time, so that small systems such as CubeSats can run the filter for attitude determination.","authors":["Simone Servadio"],"url":"https://arxiv.org/abs/2505.01598"}
{"created":"2025-05-06","title":"Beyond Productivity: Rethinking the Impact of Creativity Support Tools","abstract":"Creativity Support Tools (CSTs) are widely used across diverse creative domains, with generative AI recently increasing the abilities of CSTs. To better understand how the success of CSTs is determined in the literature, we conducted a review of outcome measures used in CST evaluations. Drawing from (n=173) CST evaluations in the ACM Digital Library, we identified the metrics commonly employed to assess user interactions with CSTs. Our findings reveal prevailing trends in current evaluation practices, while exposing underexplored measures that could broaden the scope of future research. Based on these results, we argue for a more holistic approach to evaluating CSTs, encouraging the HCI community to consider not only user experience and the quality of the generated output, but also user-centric aspects such as self-reflection and well-being as critical dimensions of assessment. We also highlight a need for validated measures specifically suited to the evaluation of generative AI in CSTs.","authors":["Samuel Rhys Cox","Helena B{\\o}jer Djern{\\ae}s","Niels van Berkel"],"url":"https://arxiv.org/abs/2505.01601"}
{"created":"2025-05-06","title":"Schr\\\"odingerization based quantum algorithms for the fractional Poisson equation","abstract":"We develop a quantum algorithm for solving high-dimensional fractional Poisson equations. By applying the Caffarelli-Silvestre extension, the $d$-dimensional fractional equation is reformulated as a local partial differential equation in $d+1$ dimensions. We propose a quantum algorithm for the finite element discretization of this local problem, by capturing the steady-state of the corresponding differential equations using the Schr\\\"odingerization approach from \\cite{JLY22SchrShort, JLY22SchrLong, analogPDE}. The Schr\\\"odingerization technique transforms general linear partial and ordinary differential equations into Schr\\\"odinger-type systems, making them suitable for quantum simulation. This is achieved through the warped phase transformation, which maps the equation into a higher-dimensional space. We provide detailed implementations of the method and conduct a comprehensive complexity analysis, which can show up to exponential advantage -- with respect to the inverse of the mesh size in high dimensions -- compared to its classical counterpart. Specifically, while the classical method requires $\\widetilde{\\mathcal{O}}(d^{1/2} 3^{3d/2} h^{-d-2})$ operations, the quantum counterpart requires $\\widetilde{\\mathcal{O}}(d 3^{3d/2} h^{-2.5})$ queries to the block-encoding input models, with the quantum complexity being independent of the dimension $d$ in terms of the inverse mesh size $h^{-1}$. Numerical experiments are conducted to verify the validity of our formulation.","authors":["Shi Jin","Nana Liu","Yue Yu"],"url":"https://arxiv.org/abs/2505.01602"}
{"created":"2025-05-06","title":"Unlocking True Elasticity for the Cloud-Native Era with Dandelion","abstract":"Elasticity is fundamental to cloud computing, as it enables quickly allocating resources to match the demand of each workload as it arrives, rather than pre-provisioning resources to meet performance objectives. However, even serverless platforms -- which boot sandboxes in 10s to 100s of milliseconds -- are not sufficiently elastic to avoid over-provisioning expensive resources. Today's FaaS platforms rely on pre-provisioning many idle sandboxes in memory to reduce the occurrence of slow, cold starts. A key obstacle for high elasticity is booting a guest OS and configuring features like networking in sandboxes, which are required to expose an isolated POSIX-like interface to user functions. Our key insight is that redesigning the interface for applications in the cloud-native era enables co-designing a much more efficient and elastic execution system. Now is a good time to rethink cloud abstractions as developers are building applications to be cloud-native. Cloud-native applications typically consist of user-provided compute logic interacting with cloud services (for storage, AI inference, query processing, etc) exposed over REST APIs. Hence, we propose Dandelion, an elastic cloud platform with a declarative programming model that expresses applications as DAGs of pure compute functions and higher-level communication functions. Dandelion can securely execute untrusted user compute functions in lightweight sandboxes that cold start in hundreds of microseconds, since pure functions do not rely on extra software environments such as a guest OS. Dandelion makes it practical to boot a sandbox on-demand for each request, decreasing performance variability by two to three orders of magnitude compared to Firecracker and reducing committed memory by 96% on average when running the Azure Functions trace.","authors":["Tom Kuchler","Pinghe Li","Yazhuo Zhang","Lazar Cvetkovi\\'c","Boris Goranov","Tobias Stocker","Leon Thomm","Simone Kalbermatter","Tim Notter","Andrea Lattuada","Ana Klimovic"],"url":"https://arxiv.org/abs/2505.01603"}
{"created":"2025-05-06","title":"Multimodal and Multiview Deep Fusion for Autonomous Marine Navigation","abstract":"We propose a cross attention transformer based method for multimodal sensor fusion to build a birds eye view of a vessels surroundings supporting safer autonomous marine navigation. The model deeply fuses multiview RGB and long wave infrared images with sparse LiDAR point clouds. Training also integrates X band radar and electronic chart data to inform predictions. The resulting view provides a detailed reliable scene representation improving navigational accuracy and robustness. Real world sea trials confirm the methods effectiveness even in adverse weather and complex maritime settings.","authors":["Dimitrios Dagdilelis","Panagiotis Grigoriadis","Roberto Galeazzi"],"url":"https://arxiv.org/abs/2505.01615"}
{"created":"2025-05-06","title":"Phantora: Live GPU Cluster Simulation for Machine Learning System Performance Estimation","abstract":"To accommodate ever-increasing model complexity, modern machine learning (ML) systems have to scale to large GPU clusters. Changes in ML model architecture, ML system implementation, and cluster configuration can significantly affect overall ML system performance. However, quantifying the performance impact before deployment is challenging. Existing performance estimation methods use performance modeling or static workload simulation. These techniques are not general: they requires significant human effort and computation capacity to generate training data or a workload. It is also difficult to adapt ML systems to use these techniques. This paper introduces, Phantora, a live GPU cluster simulator for performance estimation. Phantora runs minimally modified ML models and frameworks, intercepting and simulating GPU-related operations to enable high-fidelity performance estimation. Phantora overcomes several research challenges in integrating an event-driven network simulator with live system execution, and introduces a set of techniques to improve simulation speed, scalability, and accuracy. Our evaluation results show that Phantora can deliver similar estimation accuracy to the state-of-the-art workload simulation approach with only one GPU, while reducing human effort and increasing generalizability.","authors":["Jianxing Qin","Jingrong Chen","Xinhao Kong","Yongji Wu","Liang Luo","Zhaodong Wang","Ying Zhang","Tingjun Chen","Alvin R. Lebeck","Danyang Zhuo"],"url":"https://arxiv.org/abs/2505.01616"}
{"created":"2025-05-06","title":"High Speed Robotic Table Tennis Swinging Using Lightweight Hardware with Model Predictive Control","abstract":"We present a robotic table tennis platform that achieves a variety of hit styles and ball-spins with high precision, power, and consistency. This is enabled by a custom lightweight, high-torque, low rotor inertia, five degree-of-freedom arm capable of high acceleration. To generate swing trajectories, we formulate an optimal control problem (OCP) that constrains the state of the paddle at the time of the strike. The terminal position is given by a predicted ball trajectory, and the terminal orientation and velocity of the paddle are chosen to match various possible styles of hits: loops (topspin), drives (flat), and chops (backspin). Finally, we construct a fixed-horizon model predictive controller (MPC) around this OCP to allow the hardware to quickly react to changes in the predicted ball trajectory. We validate on hardware that the system is capable of hitting balls with an average exit velocity of 11 m/s at an 88% success rate across the three swing types.","authors":["David Nguyen","Kendrick D. Cancio","Sangbae Kim"],"url":"https://arxiv.org/abs/2505.01617"}
{"created":"2025-05-06","title":"Don't be lazy: CompleteP enables compute-efficient deep transformers","abstract":"We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the unique parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34\\% compute efficiency improvements over the prior state-of-the-art.","authors":["Nolan Dey","Bin Claire Zhang","Lorenzo Noci","Mufan Li","Blake Bordelon","Shane Bergsma","Cengiz Pehlevan","Boris Hanin","Joel Hestness"],"url":"https://arxiv.org/abs/2505.01618"}
{"created":"2025-05-06","title":"Skill-based Safe Reinforcement Learning with Risk Planning","abstract":"Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods.","authors":["Hanping Zhang","Yuhong Guo"],"url":"https://arxiv.org/abs/2505.01619"}
{"created":"2025-05-06","title":"Triangle-Decomposable Graphs for Isoperimetric Robots","abstract":"Isoperimetric robots are large scale, untethered inflatable robots that can undergo large shape changes, but have only been demonstrated in one 3D shape -- an octahedron. These robots consist of independent triangles that can change shape while maintaining their perimeter by moving the relative position of their joints. We introduce an optimization routine that determines if an arbitrary graph can be partitioned into unique triangles, and thus be constructed as an isoperimetric robotic system. We enumerate all minimally rigid graphs that can be constructed with unique triangles up to 9 nodes (7 triangles), and characterize the workspace of one node of each these robots. We also present a method for constructing larger graphs that can be partitioned by assembling subgraphs that are already partitioned into triangles. This enables a wide variety of isoperimetric robot configurations.","authors":["Nathan Usevitch","Isaac Weaver","James Usevitch"],"url":"https://arxiv.org/abs/2505.01624"}
{"created":"2025-05-06","title":"A Domain Adaptation of Large Language Models for Classifying Mechanical Assembly Components","abstract":"The conceptual design phase represents a critical early stage in the product development process, where designers generate potential solutions that meet predefined design specifications based on functional requirements. Functional modeling, a foundational aspect of this phase, enables designers to reason about product functions before specific structural details are determined. A widely adopted approach to functional modeling is the Function-Behavior-Structure (FBS) framework, which supports the transformation of functional intent into behavioral and structural descriptions. However, the effectiveness of function-based design is often hindered by the lack of well-structured and comprehensive functional data. This scarcity can negatively impact early design decision-making and hinder the development of accurate behavioral models. Recent advances in Large Language Models (LLMs), such as those based on GPT architectures, offer a promising avenue to address this gap. LLMs have demonstrated significant capabilities in language understanding and natural language processing (NLP), making them suitable for automated classification tasks. This study proposes a novel LLM-based domain adaptation (DA) framework using fine-tuning for the automated classification of mechanical assembly parts' functions. By fine-tuning LLMs on domain-specific datasets, the traditionally manual and subjective process of function annotation can be improved in both accuracy and consistency. A case study demonstrates fine-tuning GPT-3.5 Turbo on data from the Oregon State Design Repository (OSDR), and evaluation on the A Big CAD (ABC) dataset shows that the domain-adapted LLM can generate high-quality functional data, enhancing the semantic representation of mechanical parts and supporting more effective design exploration in early-phase engineering.","authors":["Fatemeh Elhambakhsh","Daniele Grandi","Hyunwoong Ko"],"url":"https://arxiv.org/abs/2505.01627"}
{"created":"2025-05-06","title":"When is Truthfully Allocating Chores no Harder than Goods?","abstract":"We study the problem of fairly and efficiently allocating a set of items among strategic agents with additive valuations, where items are either all indivisible or all divisible. When items are \\emph{goods}, numerous positive and negative results are known regarding the fairness and efficiency guarantees achievable by \\emph{truthful} mechanisms, whereas our understanding of truthful mechanisms for \\emph{chores} remains considerably more limited. In this paper, we discover various connections between truthful good and chore allocations, greatly enhancing our understanding of the latter via tools from the former.","authors":["Bo Li","Biaoshuai Tao","Fangxiao Wang","Xiaowei Wu","Mingwei Yang","Shengwei Zhou"],"url":"https://arxiv.org/abs/2505.01629"}
{"created":"2025-05-06","title":"Deformable Cargo Transport in Microgravity with Astrobee","abstract":"We present pyastrobee: a simulation environment and control stack for Astrobee in Python, with an emphasis on cargo manipulation and transport tasks. We also demonstrate preliminary success from a sampling-based MPC controller, using reduced-order models of NASA's cargo transfer bag (CTB) to control a high-order deformable finite element model. Our code is open-source, fully documented, and available at https://danielpmorton.github.io/pyastrobee","authors":["Daniel Morton","Rika Antonova","Brian Coltin","Marco Pavone","Jeannette Bohg"],"url":"https://arxiv.org/abs/2505.01630"}
{"created":"2025-05-06","title":"Dendritic Computing with Multi-Gate Ferroelectric Field-Effect Transistors","abstract":"Although inspired by neuronal systems in the brain, artificial neural networks generally employ point-neurons, which offer far less computational complexity than their biological counterparts. Neurons have dendritic arbors that connect to different sets of synapses and offer local non-linear accumulation - playing a pivotal role in processing and learning. Inspired by this, we propose a novel neuron design based on a multi-gate ferroelectric field-effect transistor that mimics dendrites. It leverages ferroelectric nonlinearity for local computations within dendritic branches, while utilizing the transistor action to generate the final neuronal output. The branched architecture paves the way for utilizing smaller crossbar arrays in hardware integration, leading to greater efficiency. Using an experimentally calibrated device-circuit-algorithm co-simulation framework, we demonstrate that networks incorporating our dendritic neurons achieve superior performance in comparison to much larger networks without dendrites ($\\sim$17$\\times$ fewer trainable weight parameters). These findings suggest that dendritic hardware can significantly improve computational efficiency, and learning capacity of neuromorphic systems optimized for edge applications.","authors":["A N M Nafiul Islam","Xuezhong Niu","Jiahui Duan","Shubham Kumar","Kai Ni","Abhronil Sengupta"],"url":"https://arxiv.org/abs/2505.01635"}
{"created":"2025-05-06","title":"Structured Prompting and Feedback-Guided Reasoning with LLMs for Data Interpretation","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and task generalization. However, their application to structured data analysis remains fragile due to inconsistencies in schema interpretation, misalignment between user intent and model output, and limited mechanisms for self-correction when failures occur. This paper introduces the STROT Framework (Structured Task Reasoning and Output Transformation), a method for structured prompting and feedback-driven transformation logic generation aimed at improving the reliability and semantic alignment of LLM-based analytical workflows. STROT begins with lightweight schema introspection and sample-based field classification, enabling dynamic context construction that captures both the structure and statistical profile of the input data. This contextual information is embedded in structured prompts that guide the model toward generating task-specific, interpretable outputs. To address common failure modes in complex queries, STROT incorporates a refinement mechanism in which the model iteratively revises its outputs based on execution feedback and validation signals. Unlike conventional approaches that rely on static prompts or single-shot inference, STROT treats the LLM as a reasoning agent embedded within a controlled analysis loop -- capable of adjusting its output trajectory through planning and correction. The result is a robust and reproducible framework for reasoning over structured data with LLMs, applicable to diverse data exploration and analysis tasks where interpretability, stability, and correctness are essential.","authors":["Amit Rath"],"url":"https://arxiv.org/abs/2505.01636"}
{"created":"2025-05-06","title":"Morello: Compiling Fast Neural Networks with Dynamic Programming and Spatial Compression","abstract":"High-throughput neural network inference requires coordinating many optimization decisions, including parallel tiling, microkernel selection, and data layout. The product of these decisions forms a search space of programs which is typically intractably large. Existing approaches (e.g., auto-schedulers) often address this problem by sampling this space heuristically. In contrast, we introduce a dynamic-programming-based approach to explore more of the search space by iteratively decomposing large program specifications into smaller specifications reachable from a set of rewrites, then composing a final program from each rewrite that minimizes an affine cost model. To reduce memory requirements, we employ a novel memoization table representation, which indexes specifications by coordinates in $Z_{\\geq 0}$ and compresses identical, adjacent solutions. This approach can visit a much larger set of programs than prior work. To evaluate the approach, we developed Morello, a compiler which lowers specifications roughly equivalent to a few-node XLA computation graph to x86. Notably, we found that an affine cost model is sufficient to surface high-throughput programs. For example, Morello synthesized a collection of matrix multiplication benchmarks targeting a Zen 1 CPU, including a 1x2048x16384, bfloat16-to-float32 vector-matrix multiply, which was integrated into Google's gemma.cpp.","authors":["Samuel J. Kaufman","Ren\\'e Just","Rastislav Bodik"],"url":"https://arxiv.org/abs/2505.01637"}
{"created":"2025-05-06","title":"Third-party compliance reviews for frontier AI safety frameworks","abstract":"Safety frameworks have emerged as a best practice for managing risks from frontier artificial intelligence (AI) systems. However, it may be difficult for stakeholders to know if companies are adhering to their frameworks. This paper explores a potential solution: third-party compliance reviews. During a third-party compliance review, an independent external party assesses whether a frontier AI company is complying with its safety framework. First, we discuss the main benefits and challenges of such reviews. On the one hand, they can increase compliance with safety frameworks and provide assurance to internal and external stakeholders. On the other hand, they can create information security risks, impose additional cost burdens, and cause reputational damage, but these challenges can be partially mitigated by drawing on best practices from other industries. Next, we answer practical questions about third-party compliance reviews, namely: (1) Who could conduct the review? (2) What information sources could the reviewer consider? (3) How could compliance with the safety framework be assessed? (4) What information about the review could be disclosed externally? (5) How could the findings guide development and deployment actions? (6) When could the reviews be conducted? For each question, we evaluate a set of plausible options. Finally, we suggest \"minimalist\", \"more ambitious\", and \"comprehensive\" approaches for each question that a frontier AI company could adopt.","authors":["Aidan Homewood","Sophie Williams","Noemi Dreksler","John Lidiard","Malcolm Murray","Lennart Heim","Marta Ziosi","Se\\'an \\'O h\\'Eigeartaigh","Michael Chen","Kevin Wei","Christoph Winter","Miles Brundage","Ben Garfinkel","Jonas Schuett"],"url":"https://arxiv.org/abs/2505.01643"}
{"created":"2025-05-06","title":"Scalable Speed-ups for the SMS-EMOA from a Simple Aging Strategy","abstract":"Different from single-objective evolutionary algorithms, where non-elitism is an established concept, multi-objective evolutionary algorithms almost always select the next population in a greedy fashion. In the only notable exception, Bian, Zhou, Li, and Qian (IJCAI 2023) proposed a stochastic selection mechanism for the SMS-EMOA and proved that it can speed up computing the Pareto front of the bi-objective jump benchmark with problem size $n$ and gap parameter $k$ by a factor of $\\max\\{1,2^{k/4}/n\\}$. While this constitutes the first proven speed-up from non-elitist selection, suggesting a very interesting research direction, it has to be noted that a true speed-up only occurs for $k \\ge 4\\log_2(n)$, where the runtime is super-polynomial, and that the advantage reduces for larger numbers of objectives as shown in a later work. In this work, we propose a different non-elitist selection mechanism based on aging, which exempts individuals younger than a certain age from a possible removal. This remedies the two shortcomings of stochastic selection: We prove a speed-up by a factor of $\\max\\{1,\\Theta(k)^{k-1}\\}$, regardless of the number of objectives. In particular, a positive speed-up can already be observed for constant $k$, the only setting for which polynomial runtimes can be witnessed. Overall, this result supports the use of non-elitist selection schemes, but suggests that aging-based mechanisms can be considerably more powerful than stochastic selection mechanisms.","authors":["Mingfeng Li","Weijie Zheng","Benjamin Doerr"],"url":"https://arxiv.org/abs/2505.01647"}
{"created":"2025-05-06","title":"Interaction Configurations and Prompt Guidance in Conversational AI for Question Answering in Human-AI Teams","abstract":"Understanding the dynamics of human-AI interaction in question answering is crucial for enhancing collaborative efficiency. Extending from our initial formative study, which revealed challenges in human utilization of conversational AI support, we designed two configurations for prompt guidance: a Nudging approach, where the AI suggests potential responses for human agents, and a Highlight strategy, emphasizing crucial parts of reference documents to aid human responses. Through two controlled experiments, the first involving 31 participants and the second involving 106 participants, we compared these configurations against traditional human-only approaches, both with and without AI assistance. Our findings suggest that effective human-AI collaboration can enhance response quality, though merely combining human and AI efforts does not ensure improved outcomes. In particular, the Nudging configuration was shown to help improve the quality of the output when compared to AI alone. This paper delves into the development of these prompt guidance paradigms, offering insights for refining human-AI collaborations in conversational question-answering contexts and contributing to a broader understanding of human perceptions and expectations in AI partnerships.","authors":["Jaeyoon Song","Zahra Ashktorab","Qian Pan","Casey Dugan","Werner Geyer","Thomas W. Malone"],"url":"https://arxiv.org/abs/2505.01648"}
{"created":"2025-05-06","title":"Toward Onboard AI-Enabled Solutions to Space Object Detection for Space Sustainability","abstract":"The rapid expansion of advanced low-Earth orbit (LEO) satellites in large constellations is positioning space assets as key to the future, enabling global internet access and relay systems for deep space missions. A solution to the challenge is effective space object detection (SOD) for collision assessment and avoidance. In SOD, an LEO satellite must detect other satellites and objects with high precision and minimal delay. This paper investigates the feasibility and effectiveness of employing vision sensors for SOD tasks based on deep learning (DL) models. It introduces models based on the Squeeze-and-Excitation (SE) layer, Vision Transformer (ViT), and the Generalized Efficient Layer Aggregation Network (GELAN) and evaluates their performance under SOD scenarios. Experimental results show that the proposed models achieve mean average precision at intersection over union threshold 0.5 (mAP50) scores of up to 0.751 and mean average precision averaged over intersection over union thresholds from 0.5 to 0.95 (mAP50:95) scores of up to 0.280. Compared to the baseline GELAN-t model, the proposed GELAN-ViT-SE model increases the average mAP50 from 0.721 to 0.751, improves the mAP50:95 from 0.266 to 0.274, reduces giga floating point operations (GFLOPs) from 7.3 to 5.6, and lowers peak power consumption from 2080.7 mW to 2028.7 mW by 2.5\\%.","authors":["Wenxuan Zhang","Peng Hu"],"url":"https://arxiv.org/abs/2505.01650"}
{"created":"2025-05-06","title":"Human-AI Governance (HAIG): A Trust-Utility Approach","abstract":"This paper introduces the HAIG framework for analysing trust dynamics across evolving human-AI relationships. Current categorical frameworks (e.g., \"human-in-the-loop\" models) inadequately capture how AI systems evolve from tools to partners, particularly as foundation models demonstrate emergent capabilities and multi-agent systems exhibit autonomous goal-setting behaviours. As systems advance, agency redistributes in complex patterns that are better represented as positions along continua rather than discrete categories, though progression may include both gradual shifts and significant step changes. The HAIG framework operates across three levels: dimensions (Decision Authority Distribution, Process Autonomy, and Accountability Configuration), continua (gradual shifts along each dimension), and thresholds (critical points requiring governance adaptation). Unlike risk-based or principle-based approaches, HAIG adopts a trust-utility orientation, focusing on maintaining appropriate trust relationships that maximise utility while ensuring sufficient safeguards. Our analysis reveals how technical advances in self-supervision, reasoning authority, and distributed decision-making drive non-uniform trust evolution across both contextual variation and technological advancement. Case studies in healthcare and European regulation demonstrate how HAIG complements existing frameworks while offering a foundation for alternative approaches that anticipate governance challenges before they emerge.","authors":["Zeynep Engin"],"url":"https://arxiv.org/abs/2505.01651"}
{"created":"2025-05-06","title":"Causally Fair Node Classification on Non-IID Graph Data","abstract":"Fair machine learning seeks to identify and mitigate biases in predictions against unfavorable populations characterized by demographic attributes, such as race and gender. Recently, a few works have extended fairness to graph data, such as social networks, but most of them neglect the causal relationships among data instances. This paper addresses the prevalent challenge in fairness-aware ML algorithms, which typically assume Independent and Identically Distributed (IID) data. We tackle the overlooked domain of non-IID, graph-based settings where data instances are interconnected, influencing the outcomes of fairness interventions. We base our research on the Network Structural Causal Model (NSCM) framework and posit two main assumptions: Decomposability and Graph Independence, which enable the computation of interventional distributions in non-IID settings using the $do$-calculus. Based on that, we develop the Message Passing Variational Autoencoder for Causal Inference (MPVA) to compute interventional distributions and facilitate causally fair node classification through estimated interventional distributions. Empirical evaluations on semi-synthetic and real-world datasets demonstrate that MPVA outperforms conventional methods by effectively approximating interventional distributions and mitigating bias. The implications of our findings underscore the potential of causality-based fairness in complex ML applications, setting the stage for further research into relaxing the initial assumptions to enhance model fairness.","authors":["Yucong Dai","Lu Zhang","Yaowei Hu","Susan Gauch","Yongkai Wu"],"url":"https://arxiv.org/abs/2505.01652"}
{"created":"2025-05-06","title":"T-REX: Vision-Based System for Autonomous Leaf Detection and Grasp Estimation","abstract":"T-Rex (The Robot for Extracting Leaf Samples) is a gantry-based robotic system developed for autonomous leaf localization, selection, and grasping in greenhouse environments. The system integrates a 6-degree-of-freedom manipulator with a stereo vision pipeline to identify and interact with target leaves. YOLOv8 is used for real-time leaf segmentation, and RAFT-Stereo provides dense depth maps, allowing the reconstruction of 3D leaf masks. These observations are processed through a leaf grasping algorithm that selects the optimal leaf based on clutter, visibility, and distance, and determines a grasp point by analyzing local surface flatness, top-down approachability, and margin from edges. The selected grasp point guides a trajectory executed by ROS-based motion controllers, driving a custom microneedle-equipped end-effector to clamp the leaf and simulate tissue sampling. Experiments conducted with artificial plants under varied poses demonstrate that the T-Rex system can consistently detect, plan, and perform physical interactions with plant-like targets, achieving a grasp success rate of 66.6\\%. This paper presents the system architecture, implementation, and testing of T-Rex as a step toward plant sampling automation in Controlled Environment Agriculture (CEA).","authors":["Srecharan Selvam","Abhisesh Silwal","George Kantor"],"url":"https://arxiv.org/abs/2505.01654"}
{"created":"2025-05-06","title":"A Novel WaveInst-based Network for Tree Trunk Structure Extraction and Pattern Analysis in Forest Inventory","abstract":"The pattern analysis of tree structure holds significant scientific value for genetic breeding and forestry management. The current trunk and branch extraction technologies are mainly LiDAR-based or UAV-based. The former approaches obtain high-precision 3D data, but its equipment cost is high and the three-dimensional (3D) data processing is complex. The latter approaches efficiently capture canopy information, but they miss the 3-D structure of trees. In order to deal with the branch information extraction from the complex background interference and occlusion, this work proposes a novel WaveInst instance segmentation framework, involving a discrete wavelet transform, to enhance multi-scale edge information for accurately improving tree structure extraction. Experimental results of the proposed model show superior performance on SynthTree43k, CaneTree100, Urban Street and our PoplarDataset. Moreover, we present a new Phenotypic dataset PoplarDataset, which is dedicated to extract tree structure and pattern analysis from artificial forest. The proposed method achieves a mean average precision of 49.6 and 24.3 for the structure extraction of mature and juvenile trees, respectively, surpassing the existing state-of-the-art method by 9.9. Furthermore, by in tegrating the segmentation model within the regression model, we accurately achieve significant tree grown parameters, such as the location of trees, the diameter-at-breast-height of individual trees, and the plant height, from 2D images directly. This study provides a scientific and plenty of data for tree structure analysis in related to the phenotype research, offering a platform for the significant applications in precision forestry, ecological monitoring, and intelligent breeding.","authors":["Chenyang Fan","Xujie Zhu","Taige Luo","Sheng Xu","Zhulin Chen","Hongxin Yang"],"url":"https://arxiv.org/abs/2505.01656"}
{"created":"2025-05-06","title":"RAGAR: Retrieval Augment Personalized Image Generation Guided by Recommendation","abstract":"Personalized image generation is crucial for improving the user experience, as it renders reference images into preferred ones according to user visual preferences. Although effective, existing methods face two main issues. First, existing methods treat all items in the user historical sequence equally when extracting user preferences, overlooking the varying semantic similarities between historical items and the reference item. Disproportionately high weights for low-similarity items distort users' visual preferences for the reference item. Second, existing methods heavily rely on consistency between generated and reference images to optimize the generation, which leads to underfitting user preferences and hinders personalization. To address these issues, we propose Retrieval Augment Personalized Image GenerAtion guided by Recommendation (RAGAR). Our approach uses a retrieval mechanism to assign different weights to historical items according to their similarities to the reference item, thereby extracting more refined users' visual preferences for the reference item. Then we introduce a novel rank task based on the multi-modal ranking model to optimize the personalization of the generated images instead of forcing depend on consistency. Extensive experiments and human evaluations on three real-world datasets demonstrate that RAGAR achieves significant improvements in both personalization and semantic metrics compared to five baselines.","authors":["Run Ling","Wenji Wang","Yuting Liu","Guibing Guo","Linying Jiang","Xingwei Wang"],"url":"https://arxiv.org/abs/2505.01657"}
{"created":"2025-05-06","title":"A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency","abstract":"Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine","authors":["Sihyeong Park","Sungryeol Jeon","Chaelyn Lee","Seokhun Jeon","Byung-Soo Kim","Jemin Lee"],"url":"https://arxiv.org/abs/2505.01658"}
{"created":"2025-05-06","title":"Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed Classification","abstract":"Real-world datasets often follow a long-tailed distribution, making generalization to tail classes difficult. Recent methods resorted to long-tail variants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to improve generalization by flattening the loss landscape. However, these attempts face a trade-off between computational efficiency and control over the loss landscape. On the one hand, ImbSAM is efficient but offers only coarse control as it excludes head classes from the SAM process. On the other hand, CC-SAM provides fine-grained control through class-dependent perturbations but at the cost of efficiency due to multiple backpropagations. Seeing this dilemma, we introduce Focal-SAM, which assigns different penalties to class-wise sharpness, achieving fine-grained control without extra backpropagations, thus maintaining efficiency. Furthermore, we theoretically analyze Focal-SAM's generalization ability and derive a sharper generalization bound. Extensive experiments on both traditional and foundation models validate the effectiveness of Focal-SAM.","authors":["Sicong Li","Qianqian Xu","Zhiyong Yang","Zitai Wang","Linchao Zhang","Xiaochun Cao","Qingming Huang"],"url":"https://arxiv.org/abs/2505.01660"}
{"created":"2025-05-06","title":"Soft-Masked Semi-Dual Optimal Transport for Partial Domain Adaptation","abstract":"Visual domain adaptation aims to learn discriminative and domain-invariant representation for an unlabeled target domain by leveraging knowledge from a labeled source domain. Partial domain adaptation (PDA) is a general and practical scenario in which the target label space is a subset of the source one. The challenges of PDA exist due to not only domain shift but also the non-identical label spaces of domains. In this paper, a Soft-masked Semi-dual Optimal Transport (SSOT) method is proposed to deal with the PDA problem. Specifically, the class weights of domains are estimated, and then a reweighed source domain is constructed, which is favorable in conducting class-conditional distribution matching with the target domain. A soft-masked transport distance matrix is constructed by category predictions, which will enhance the class-oriented representation ability of optimal transport in the shared feature space. To deal with large-scale optimal transport problems, the semi-dual formulation of the entropy-regularized Kantorovich problem is employed since it can be optimized by gradient-based algorithms. Further, a neural network is exploited to approximate the Kantorovich potential due to its strong fitting ability. This network parametrization also allows the generalization of the dual variable outside the supports of the input distribution. The SSOT model is built upon neural networks, which can be optimized alternately in an end-to-end manner. Extensive experiments are conducted on four benchmark datasets to demonstrate the effectiveness of SSOT.","authors":["Yi-Ming Zhai","Chuan-Xian Ren","Hong Yan"],"url":"https://arxiv.org/abs/2505.01664"}
{"created":"2025-05-06","title":"Adaptively Point-weighting Curriculum Learning","abstract":"Curriculum learning (CL) is referred to as a training strategy that makes easy samples learned first and then fits hard samples. It imitates the process of humans learning knowledge, and has become a potential manner of effectively training deep networks. In this study, we develop the adaptively point-weighting (APW) curriculum learning algorithm, which adaptively assigns the weight to every training sample not only based on its training error but also considering the current training state of the network. Specifically, in the early training phase, it increases the weights of easy samples to make the network rapidly capture the overall characteristics of the dataset; and in the later training phase, the weights of hard points rise to improve the fitting performance on the discrete local regions. Moreover, we also present the theoretical analysis on the properties of APW including training effectiveness, training feasibility, training stability, and generalization performance. The numerical experiments support the superiority of APW and demonstrate the validity of our theoretical findings.","authors":["Wensheng Li","Hao Wang","Ruifeng Zhou","Hanting Guan","Chao Zhang","Dacheng Tao"],"url":"https://arxiv.org/abs/2505.01665"}
{"created":"2025-05-06","title":"Report on Challenges of Practical Reproducibility for Systems and HPC Computer Science","abstract":"This report synthesizes findings from the November 2024 Community Workshop on Practical Reproducibility in HPC, which convened researchers, artifact authors, reviewers, and chairs of reproducibility initiatives to address the critical challenge of making computational experiments reproducible in a cost-effective manner. The workshop deliberately focused on systems and HPC computer science research due to its unique requirements, including specialized hardware access and deep system reconfigurability. Through structured discussions, lightning talks, and panel sessions, participants identified key barriers to practical reproducibility and formulated actionable recommendations for the community.","authors":["Kate Keahey","Marc Richardson","Rafael Tolosana Calasanz","Sascha Hunold","Jay Lofstead","Tanu Malik","Christian Perez"],"url":"https://arxiv.org/abs/2505.01671"}
{"created":"2025-05-06","title":"A Practitioner's Guide to Automatic Kernel Search for Gaussian Processes in Battery Applications","abstract":"Gaussian process (GP) models have been used in a wide range of battery applications, in which different kernels were manually selected with considerable expertise. However, to capture complex relationships in the ever-growing amount of real-world data, selecting a suitable kernel for the GP model in battery applications is increasingly challenging. In this work, we first review existing GP kernels used in battery applications and then extend an automatic kernel search method with a new base kernel and model selection criteria. The GP models with composite kernels outperform the baseline kernel in two numerical examples of battery applications, i.e., battery capacity estimation and residual load prediction. Particularly, the results indicate that the Bayesian Information Criterion may be the best model selection criterion as it achieves a good trade-off between kernel performance and computational complexity. This work should, therefore, be of value to practitioners wishing to automate their kernel search process in battery applications.","authors":["Huang Zhang","Xixi Liu","Faisal Altaf","Torsten Wik"],"url":"https://arxiv.org/abs/2505.01674"}
{"created":"2025-05-06","title":"Enhanced Prediction Model for Time Series Characterized by GARCH via Interval Type-2 Fuzzy Inference System","abstract":"GARCH-type time series (characterized by Generalized Autoregressive Conditional Heteroskedasticity) exhibit pronounced volatility, autocorrelation, and heteroskedasticity. To address these challenges and enhance predictive accuracy, this study introduces a hybrid forecasting framework that integrates the Interval Type-2 Fuzzy Inference System (IT2FIS) with the GARCH model. Leveraging the interval-based uncertainty representation of IT2FIS and the volatility-capturing capability of GARCH, the proposed model effectively mitigates the adverse impact of heteroskedasticity on prediction reliability. Specifically, the GARCH component estimates conditional variance, which is subsequently incorporated into the Gaussian membership functions of IT2FIS. This integration transforms IT2FIS into an adaptive variable-parameter system, dynamically aligning with the time-varying volatility of the target series. Through systematic parameter optimization, the framework not only captures intricate volatility patterns but also accounts for heteroskedasticity and epistemic uncertainties during modeling, thereby improving both prediction precision and model robustness. Experimental validation employs diverse datasets, including air quality concentration, urban traffic flow, and energy consumption. Comparative analyses are conducted against models: the GARCH-Takagi-Sugeno-Kang (GARCH-TSK) model, fixed-variance time series models, the GARCH-Gated Recurrent Unit (GARCH-GRU), and Long Short-Term Memory (LSTM) networks. The results indicate that the proposed model achieves superior predictive performance across the majority of test scenarios in error metrics. These findings underscore the effectiveness of hybrid approaches in forecasting uncertainty for GARCH-type time series, highlighting their practical utility in real-world time series forecasting applications.","authors":["Shaohong Pei","Da-Qing Zhang","Feilong Lu"],"url":"https://arxiv.org/abs/2505.01675"}
{"created":"2025-05-06","title":"LogDB: Multivariate Log-based Failure Diagnosis for Distributed Databases (Extended from MultiLog)","abstract":"Distributed databases, as the core infrastructure software for internet applications, play a critical role in modern cloud services. However, existing distributed databases frequently experience system failures and performance degradation, often leading to significant economic losses. Log data, naturally generated within systems, can effectively reflect internal system states. In practice, operators often manually inspect logs to monitor system behavior and diagnose anomalies, a process that is labor-intensive and costly. Although various log-based failure diagnosis methods have been proposed, they are generally not tailored for database systems and fail to fully exploit the internal characteristics and distributed nature of these systems. To address this gap, we propose LogDB, a log-based failure diagnosis method specifically designed for distributed databases. LogDB extracts and compresses log features at each database node and then aggregates these features at the master node to diagnose cluster-wide anomalies. Experiments conducted on the open-source distributed database system Apache IoTDB demonstrate that LogDB achieves robust failure diagnosis performance across different workloads and a variety of anomaly types.","authors":["Lingzhe Zhang","Tong Jia","Mengxi Jia","Ying Li"],"url":"https://arxiv.org/abs/2505.01676"}
{"created":"2025-05-06","title":"AI-Based Speaking Assistant: Supporting Non-Native Speakers' Speaking in Real-Time Multilingual Communication","abstract":"Non-native speakers (NNSs) often face speaking challenges in real-time multilingual communication, such as struggling to articulate their thoughts. To address this issue, we developed an AI-based speaking assistant (AISA) that provides speaking references for NNSs based on their input queries, task background, and conversation history. To explore NNSs' interaction with AISA and its impact on NNSs' speaking during real-time multilingual communication, we conducted a mixed-method study involving a within-subject experiment and follow-up interviews. In the experiment, two native speakers (NSs) and one NNS formed a team (31 teams in total) and completed two collaborative tasks--one with access to the AISA and one without. Overall, our study revealed four types of AISA input patterns among NNSs, each reflecting different levels of effort and language preferences. Although AISA did not improve NNSs' speaking competence, follow-up interviews revealed that it helped improve the logical flow and depth of their speech. Moreover, the additional multitasking introduced by AISA, such as entering and reviewing system output, potentially elevated NNSs' workload and anxiety. Based on these observations, we discuss the pros and cons of implementing tools to assist NNS in real-time multilingual communication and offer design recommendations.","authors":["Peinuan Qin","Zicheng Zhu","Naomi Yamashita","Yitian Yang","Keita Suga","Yi-Chieh Lee"],"url":"https://arxiv.org/abs/2505.01678"}
{"created":"2025-05-06","title":"Evaluating Input Modalities for Pilot-Centered Taxiway Navigation: Insights from a Wizard-of-Oz Simulation","abstract":"Runway and taxiway incursions continue to challenge aviation safety, as pilots often experience disorientation from poor visibility in adverse conditions and cognitive workload in complex airport layouts. Current tools, such as airport moving maps on portable tablets, allow manual route planning but do not dynamically adapt to air traffic controllers' (ATCOs) clearances, limiting their effectiveness in high-stress scenarios. This study investigates the impact of different input modalities - paper-based, keyboard touch, map touch, and speech-to-text - on taxiway navigation performance, using a medium-fidelity flight simulator and a Wizard-of-Oz methodology to simulate ideal automation conditions. Contrary to common assumptions, recent studies indicate that paper-based methods outperform digital counterparts in accuracy and efficiency under certain conditions, highlighting critical limitations in current automation strategies. In response, our study investigates why manual methods may excel and how future automation can be optimized for pilot-centered operations. Employing a Wizard-of-Oz approach, we replicated the full taxiing process - from receiving ATCO clearances to executing maneuvers - and differentiated between readback and execution accuracy. Findings reveal that speech-based systems suffer from low pilot trust, necessitating hybrid solutions that integrate error correction and confidence indicators. These insights contribute to the development of future pilot-centered taxiway assistance that enhance situational awareness, minimize workload, and improve overall operational safety.","authors":["Chan Chea Mean","Sameer Alam","Katherine Fennedy","Meng-Hsueh Hsieh","Shiwei Xin","Brian Hilburn"],"url":"https://arxiv.org/abs/2505.01679"}
{"created":"2025-05-06","title":"Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study","abstract":"Manual scoring of the Action Research Arm Test (ARAT) for upper extremity assessment in stroke rehabilitation is time-intensive and variable. We propose an automated ARAT scoring system integrating multimodal video analysis with SlowFast, I3D, and Transformer-based models using OpenPose keypoints and object locations. Our approach employs multi-view data (ipsilateral, contralateral, and top perspectives), applying early and late fusion to combine features across views and models. Hierarchical Bayesian Models (HBMs) infer movement quality components, enhancing interpretability. A clinician dashboard displays task scores, execution times, and quality assessments. We conducted a study with five clinicians who reviewed 500 video ratings generated by our system, providing feedback on its accuracy and usability. Evaluated on a stroke rehabilitation dataset, our framework achieves 89.0% validation accuracy with late fusion, with HBMs aligning closely with manual assessments. This work advances automated rehabilitation by offering a scalable, interpretable solution with clinical validation.","authors":["Tamim Ahmed","Thanassis Rikakis"],"url":"https://arxiv.org/abs/2505.01680"}
{"created":"2025-05-06","title":"Resilient Vehicular Communications under Imperfect Channel State Information","abstract":"Cellular vehicle-to-everything (C-V2X) networks provide a promising solution to improve road safety and traffic efficiency. One key challenge in such systems lies in meeting quality-of-service (QoS) requirements of vehicular communication links given limited network resources, particularly under imperfect channel state information (CSI) conditions caused by the highly dynamic environment. In this paper, a novel two-phase framework is proposed to instill resilience into C-V2X networks under unknown imperfect CSI. The resilience of the C-V2X network is defined, quantified, and optimized the first time through two principal dimensions: absorption phase and adaptation phase. Specifically, the probability distribution function (PDF) of the imperfect CSI is estimated during the absorption phase through dedicated absorption power scheme and resource block (RB) assignment. The estimated PDF is further used to analyze the interplay and reveal the tradeoff between these two phases. Then, a novel metric named hazard rate (HR) is exploited to balance the C-V2X network's prioritization on absorption and adaptation. Finally, the estimated PDF is exploited in the adaptation phase to recover the network's QoS through a real-time power allocation optimization. Simulation results demonstrate the superior capability of the proposed framework in sustaining the QoS of the C-V2X network under imperfect CSI. Specifically, in the adaptation phase, the proposed design reduces the vehicle-tovehicle (V2V) delay that exceeds QoS requirement by 35% and 56%, and improves the average vehicle-to-infrastructure (V2I) throughput by 14% and 16% compared to the model-based and data-driven benchmarks, respectively, without compromising the network's QoS in the absorption phase.","authors":["Tingyu Shui","Walid Saad","Ye Hu","Mingzhe Chen"],"url":"https://arxiv.org/abs/2505.01687"}
{"created":"2025-05-06","title":"Sensing Safety Analysis for Vehicular Networks with Integrated Sensing and Communication (ISAC)","abstract":"Integrated sensing and communication (ISAC) emerged as a key feature of next-generation 6G wireless systems, allowing them to achieve high data rates and sensing accuracy. While prior research has primarily focused on addressing communication safety in ISAC systems, the equally critical issue of sensing safety remains largely ignored. In this paper, a novel threat to the sensing safety of ISAC vehicle networks is studied, whereby a malicious reconfigurable intelligent surface (RIS) is deployed to compromise the sensing functionality of a roadside unit (RSU). Specifically, a malicious attacker dynamically adjusts the phase shifts of an RIS to spoof the sensing outcomes of a vehicular user (VU)'s echo delay, Doppler shift, and angle-of-departure (AoD). To achieve spoofing on Doppler shift estimation, a time-varying phase shift design on the RIS is proposed. Furthermore, the feasible spoofing frequency set with respect to the Doppler shift is analytical derived. Analytical results also demonstrate that the maximum likelihood estimator (MLE) of the AoD can be significantly misled under spoofed Doppler shift estimation. Simulation results validate our theoretical findings, showing that the RIS can induce a spoofed velocity estimation from 0.1 m/s to 14.9 m/s for a VU with velocity of 10 m/s, and can cause an AoD estimation error of up to 65^{\\circ} with only a 5^{\\circ} beam misalignment.","authors":["Tingyu Shui","Walid Saad","Mingzhe Cheng"],"url":"https://arxiv.org/abs/2505.01688"}
{"created":"2025-05-06","title":"Fragment-Level Macro-Diversity Reception in LoRaWAN Networks with LR-FHSS","abstract":"The rapid expansion of Internet of Things (IoT) deployments demands wireless protocols that combine high scalability with robust performance. Long Range-Frequency Hopping Spread Spectrum (LR-FHSS) extends LoRaWAN by increasing capacity and resilience through frequency hopping and redundancy. However, current deployments require packet reconstruction at a single gateway, limiting the benefits of LR-FHSS. This paper proposes a macro-diversity reception strategy where multiple gateways collectively receive and combine payload fragments. We develop a stochastic geometry-based analytical model that captures the impact of header repetition, payload fragmentation, and coding redundancy. Closed-form expressions quantify success probabilities under interference, and numerical evaluations demonstrate significant capacity gains over nearest-gateway reception. These results highlight the potential of fragment-level macro-diversity to improve scalability and reliability in future LPWAN deployments.","authors":["Samer Lahoud","Kinda Khawam"],"url":"https://arxiv.org/abs/2505.01689"}
{"created":"2025-05-06","title":"The consensus number of a shift register equals its width","abstract":"The consensus number of a w-bit register supporting logical left shift and right shift operations is exactly w, giving an example of a class of types, widely implemented in practice, that populates all levels of the consensus hierarchy. This result generalizes to w-wide shift registers over larger alphabets. In contrast, a register providing arithmetic right shift, which replicates the most significant bit instead of replacing it with zero, is shown to solve consensus for any fixed number of processes as long as its width is at least two.","authors":["James Aspnes"],"url":"https://arxiv.org/abs/2505.01691"}
{"created":"2025-05-06","title":"High-Fidelity Pseudo-label Generation by Large Language Models for Training Robust Radiology Report Classifiers","abstract":"Automated labeling of chest X-ray reports is essential for enabling downstream tasks such as training image-based diagnostic models, population health studies, and clinical decision support. However, the high variability, complexity, and prevalence of negation and uncertainty in these free-text reports pose significant challenges for traditional Natural Language Processing methods. While large language models (LLMs) demonstrate strong text understanding, their direct application for large-scale, efficient labeling is limited by computational cost and speed. This paper introduces DeBERTa-RAD, a novel two-stage framework that combines the power of state-of-the-art LLM pseudo-labeling with efficient DeBERTa-based knowledge distillation for accurate and fast chest X-ray report labeling. We leverage an advanced LLM to generate high-quality pseudo-labels, including certainty statuses, for a large corpus of reports. Subsequently, a DeBERTa-Base model is trained on this pseudo-labeled data using a tailored knowledge distillation strategy. Evaluated on the expert-annotated MIMIC-500 benchmark, DeBERTa-RAD achieves a state-of-the-art Macro F1 score of 0.9120, significantly outperforming established rule-based systems, fine-tuned transformer models, and direct LLM inference, while maintaining a practical inference speed suitable for high-throughput applications. Our analysis shows particular strength in handling uncertain findings. This work demonstrates a promising path to overcome data annotation bottlenecks and achieve high-performance medical text processing through the strategic combination of LLM capabilities and efficient student models trained via distillation.","authors":["Brian Wong","Kaito Tanaka"],"url":"https://arxiv.org/abs/2505.01693"}
{"created":"2025-05-06","title":"Topology-Aware CLIP Few-Shot Learning","abstract":"Efficiently adapting large Vision-Language Models (VLMs) like CLIP for few-shot learning poses challenges in balancing pre-trained knowledge retention and task-specific adaptation. Existing methods often overlook valuable structural information within the VLM's latent space. We introduce a topology-aware tuning approach integrating Representation Topology Divergence (RTD) into the Task Residual (TR) framework. By explicitly aligning the topological structures of visual and text representations using a combined RTD and Cross-Entropy loss, while freezing base VLM encoders, our method enhances few-shot performance. We optimize only lightweight Task Residual parameters, effectively leveraging topological information. Across 6 diverse benchmark datasets, our approach demonstrates significant gains, achieving an average accuracy improvement of 1-2\\% over relevant baseline methods in few-shot settings. This work presents an effective strategy to boost VLM few-shot capabilities by incorporating topological alignment.","authors":["Dazhi Huang"],"url":"https://arxiv.org/abs/2505.01694"}
{"created":"2025-05-06","title":"SimAug: Enhancing Recommendation with Pretrained Language Models for Dense and Balanced Data Augmentation","abstract":"Deep Neural Networks (DNNs) are extensively used in collaborative filtering due to their impressive effectiveness. These systems depend on interaction data to learn user and item embeddings that are crucial for recommendations. However, the data often suffers from sparsity and imbalance issues: limited observations of user-item interactions can result in sub-optimal performance, and a predominance of interactions with popular items may introduce recommendation bias. To address these challenges, we employ Pretrained Language Models (PLMs) to enhance the interaction data with textual information, leading to a denser and more balanced dataset. Specifically, we propose a simple yet effective data augmentation method (SimAug) based on the textual similarity from PLMs, which can be seamlessly integrated to any systems as a lightweight, plug-and-play component in the pre-processing stage. Our experiments across nine datasets consistently demonstrate improvements in both utility and fairness when training with the augmented data generated by SimAug. The code is available at https://github.com/YuyingZhao/SimAug.","authors":["Yuying Zhao","Xiaodong Yang","Huiyuan Chen","Xiran Fan","Yu Wang","Yiwei Cai","Tyler Derr"],"url":"https://arxiv.org/abs/2505.01695"}
{"created":"2025-05-06","title":"BMTree: Designing, Learning, and Updating Piecewise Space-Filling Curves for Multi-Dimensional Data Indexing","abstract":"Space-filling curves (SFC, for short) have been widely applied to index multi-dimensional data, which first maps the data to one dimension, and then a one-dimensional indexing method, e.g., the B-tree indexes the mapped data. Existing SFCs adopt a single mapping scheme for the whole data space. However, a single mapping scheme often does not perform well on all the data space. In this paper, we propose a new type of SFC called piecewise SFCs that adopts different mapping schemes for different data subspaces. Specifically, we propose a data structure termed the Bit Merging tree (BMTree) that can generate data subspaces and their SFCs simultaneously, and achieve desirable properties of the SFC for the whole data space. Furthermore, we develop a reinforcement learning-based solution to build the BMTree, aiming to achieve excellent query performance. To update the BMTree efficiently when the distributions of data and/or queries change, we develop a new mechanism that achieves fast detection of distribution shifts in data and queries, and enables partial retraining of the BMTree. The retraining mechanism achieves performance enhancement efficiently since it avoids retraining the BMTree from scratch. Extensive experiments show the effectiveness and efficiency of the BMTree with the proposed learning-based methods.","authors":["Jiangneng Li","Yuang Liu","Zheng Wang","Gao Cong","Cheng Long","Walid G. Aref","Han Mao Kiah","Bin Cui"],"url":"https://arxiv.org/abs/2505.01697"}
{"created":"2025-05-06","title":"Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs","abstract":"The remarkable advancements in Large Language Models (LLMs) have revolutionized the content generation process in social media, offering significant convenience in writing tasks. However, existing applications, such as sentence completion and fluency enhancement, do not fully address the complex challenges in real-world social media contexts. A prevalent goal among social media users is to increase the visibility and influence of their posts. This paper, therefore, delves into the compelling question: Can LLMs generate personalized influential content to amplify a user's presence on social media? We begin by examining prevalent techniques in content generation to assess their impact on post influence. Acknowledging the critical impact of underlying network structures in social media, which are instrumental in initiating content cascades and highly related to the influence/popularity of a post, we then inject network information into prompt for content generation to boost the post's influence. We design multiple content-centric and structure-aware prompts. The empirical experiments across LLMs validate their ability in improving the influence and draw insights on which strategies are more effective. Our code is available at https://github.com/YuyingZhao/LLM-influence-amplifier.","authors":["Yuying Zhao","Yu Wang","Xueqi Cheng","Anne Marie Tumlin","Yunchao Liu","Damin Xia","Meng Jiang","Tyler Derr"],"url":"https://arxiv.org/abs/2505.01698"}
{"created":"2025-05-06","title":"Component-Based Fairness in Face Attribute Classification with Bayesian Network-informed Meta Learning","abstract":"The widespread integration of face recognition technologies into various applications (e.g., access control and personalized advertising) necessitates a critical emphasis on fairness. While previous efforts have focused on demographic fairness, the fairness of individual biological face components remains unexplored. In this paper, we focus on face component fairness, a fairness notion defined by biological face features. To our best knowledge, our work is the first work to mitigate bias of face attribute prediction at the biological feature level. In this work, we identify two key challenges in optimizing face component fairness: attribute label scarcity and attribute inter-dependencies, both of which limit the effectiveness of bias mitigation from previous approaches. To address these issues, we propose \\textbf{B}ayesian \\textbf{N}etwork-informed \\textbf{M}eta \\textbf{R}eweighting (BNMR), which incorporates a Bayesian Network calibrator to guide an adaptive meta-learning-based sample reweighting process. During the training process of our approach, the Bayesian Network calibrator dynamically tracks model bias and encodes prior probabilities for face component attributes to overcome the above challenges. To demonstrate the efficacy of our approach, we conduct extensive experiments on a large-scale real-world human face dataset. Our results show that BNMR is able to consistently outperform recent face bias mitigation baselines. Moreover, our results suggest a positive impact of face component fairness on the commonly considered demographic fairness (e.g., \\textit{gender}). Our findings pave the way for new research avenues on face component fairness, suggesting that face component fairness could serve as a potential surrogate objective for demographic fairness. The code for our work is publicly available~\\footnote{https://github.com/yliuaa/BNMR-FairCompFace.git}.","authors":["Yifan Liu","Ruichen Yao","Yaokun Liu","Ruohan Zong","Zelin Li","Yang Zhang","Dong Wang"],"url":"https://arxiv.org/abs/2505.01699"}
{"created":"2025-05-06","title":"PoseX: AI Defeats Physics Approaches on Protein-Ligand Cross Docking","abstract":"Recently, significant progress has been made in protein-ligand docking, especially in modern deep learning methods, and some benchmarks were proposed, e.g., PoseBench, Plinder. However, these benchmarks suffer from less practical evaluation setups (e.g., blind docking, self docking), or heavy framework that involves training, raising challenges to assess docking methods efficiently. To fill this gap, we proposed PoseX, an open-source benchmark focusing on self-docking and cross-docking, to evaluate the algorithmic advances practically and comprehensively. Specifically, first, we curate a new evaluation dataset with 718 entries for self docking and 1,312 for cross docking; second, we incorporate 22 docking methods across three methodological categories, including (1) traditional physics-based methods (e.g., Schr\\\"odinger Glide), (2) AI docking methods (e.g., DiffDock), (3) AI co-folding methods (e.g., AlphaFold3); third, we design a relaxation method as post-processing to minimize conformation energy and refine binding pose; fourth, we released a leaderboard to rank submitted models in real time. We draw some key insights via extensive experiments: (1) AI-based approaches have already surpassed traditional physics-based approaches in overall docking accuracy (RMSD). The longstanding generalization issues that have plagued AI molecular docking have been significantly alleviated in the latest models. (2) The stereochemical deficiencies of AI-based approaches can be greatly alleviated with post-processing relaxation. Combining AI docking methods with the enhanced relaxation method achieves the best performance to date. (3) AI co-folding methods commonly face ligand chirality issues, which cannot be resolved by relaxation. The code, curated dataset and leaderboard are released at https://github.com/CataAI/PoseX.","authors":["Yize Jiang","Xinze Li","Yuanyuan Zhang","Jin Han","Youjun Xu","Ayush Pandit","Zaixi Zhang","Mengdi Wang","Mengyang Wang","Chong Liu","Guang Yang","Yejin Choi","Wu-Jun Li","Tianfan Fu","Fang Wu","Junhong Liu"],"url":"https://arxiv.org/abs/2505.01700"}
{"created":"2025-05-06","title":"Inducing Robustness in a 2 Dimensional Direct Preference Optimization Paradigm","abstract":"Direct Preference Optimisation (DPO) has emerged as a powerful method for aligning Large Language Models (LLMs) with human preferences, offering a stable and efficient alternative to approaches that use Reinforcement learning via Human Feedback. In this work, we investigate the performance of DPO using open-source preference datasets. One of the major drawbacks of DPO is that it doesn't induce granular scoring and treats all the segments of the responses with equal propensity. However, this is not practically true for human preferences since even \"good\" responses have segments that may not be preferred by the annotator. To resolve this, a 2-dimensional scoring for DPO alignment called 2D-DPO was proposed. We explore the 2D-DPO alignment paradigm and the advantages it provides over the standard DPO by comparing their win rates. It is observed that these methods, even though effective, are not robust to label/score noise. To counter this, we propose an approach of incorporating segment-level score noise robustness to the 2D-DPO algorithm. Along with theoretical backing, we also provide empirical verification in favour of the algorithm and introduce other noise models that can be present.","authors":["Sarvesh Shashidhar","Ritik","Nachiketa Patil","Suraj Racha","Ganesh Ramakrishnan"],"url":"https://arxiv.org/abs/2505.01706"}
{"created":"2025-05-06","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation","abstract":"Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"url":"https://arxiv.org/abs/2505.01709"}
{"created":"2025-05-06","title":"Knowledge-Augmented Language Models Interpreting Structured Chest X-Ray Findings","abstract":"Automated interpretation of chest X-rays (CXR) is a critical task with the potential to significantly improve clinical workflow and patient care. While recent advances in multimodal foundation models have shown promise, effectively leveraging the full power of large language models (LLMs) for this visual task remains an underexplored area. This paper introduces CXR-TextInter, a novel framework that repurposes powerful text-centric LLMs for CXR interpretation by operating solely on a rich, structured textual representation of the image content, generated by an upstream image analysis pipeline. We augment this LLM-centric approach with an integrated medical knowledge module to enhance clinical reasoning. To facilitate training and evaluation, we developed the MediInstruct-CXR dataset, containing structured image representations paired with diverse, clinically relevant instruction-response examples, and the CXR-ClinEval benchmark for comprehensive assessment across various interpretation tasks. Extensive experiments on CXR-ClinEval demonstrate that CXR-TextInter achieves state-of-the-art quantitative performance across pathology detection, report generation, and visual question answering, surpassing existing multimodal foundation models. Ablation studies confirm the critical contribution of the knowledge integration module. Furthermore, blinded human evaluation by board-certified radiologists shows a significant preference for the clinical quality of outputs generated by CXR-TextInter. Our work validates an alternative paradigm for medical image AI, showcasing the potential of harnessing advanced LLM capabilities when visual information is effectively structured and domain knowledge is integrated.","authors":["Alexander Davis","Rafael Souza","Jia-Hao Lim"],"url":"https://arxiv.org/abs/2505.01711"}
{"created":"2025-05-06","title":"World Model-Based Learning for Long-Term Age of Information Minimization in Vehicular Networks","abstract":"Traditional reinforcement learning (RL)-based learning approaches for wireless networks rely on expensive trial-and-error mechanisms and real-time feedback based on extensive environment interactions, which leads to low data efficiency and short-sighted policies. These limitations become particularly problematic in complex, dynamic networks with high uncertainty and long-term planning requirements. To address these limitations, in this paper, a novel world model-based learning framework is proposed to minimize packet-completeness-aware age of information (CAoI) in a vehicular network. Particularly, a challenging representative scenario is considered pertaining to a millimeter-wave (mmWave) vehicle-to-everything (V2X) communication network, which is characterized by high mobility, frequent signal blockages, and extremely short coherence time. Then, a world model framework is proposed to jointly learn a dynamic model of the mmWave V2X environment and use it to imagine trajectories for learning how to perform link scheduling. In particular, the long-term policy is learned in differentiable imagined trajectories instead of environment interactions. Moreover, owing to its imagination abilities, the world model can jointly predict time-varying wireless data and optimize link scheduling in real-world wireless and V2X networks. Thus, during intervals without actual observations, the world model remains capable of making efficient decisions. Extensive experiments are performed on a realistic simulator based on Sionna that integrates physics-based end-to-end channel modeling, ray-tracing, and scene geometries with material properties. Simulation results show that the proposed world model achieves a significant improvement in data efficiency, and achieves 26% improvement and 16% improvement in CAoI, respectively, compared to the model-based RL (MBRL) method and the model-free RL (MFRL) method.","authors":["Lingyi Wang","Rashed Shelim","Walid Saad","Naren Ramakrishnan"],"url":"https://arxiv.org/abs/2505.01712"}
{"created":"2025-05-06","title":"Vision and Intention Boost Large Language Model in Long-Term Action Anticipation","abstract":"Long-term action anticipation (LTA) aims to predict future actions over an extended period. Previous approaches primarily focus on learning exclusively from video data but lack prior knowledge. Recent researches leverage large language models (LLMs) by utilizing text-based inputs which suffer severe information loss. To tackle these limitations single-modality methods face, we propose a novel Intention-Conditioned Vision-Language (ICVL) model in this study that fully leverages the rich semantic information of visual data and the powerful reasoning capabilities of LLMs. Considering intention as a high-level concept guiding the evolution of actions, we first propose to employ a vision-language model (VLM) to infer behavioral intentions as comprehensive textual features directly from video inputs. The inferred intentions are then fused with visual features through a multi-modality fusion strategy, resulting in intention-enhanced visual representations. These enhanced visual representations, along with textual prompts, are fed into LLM for future action anticipation. Furthermore, we propose an effective example selection strategy jointly considers visual and textual similarities, providing more relevant and informative examples for in-context learning. Extensive experiments with state-of-the-art performance on Ego4D, EPIC-Kitchens-55, and EGTEA GAZE+ datasets fully demonstrate the effectiveness and superiority of the proposed method.","authors":["Congqi Cao","Lanshu Hu","Yating Yu","Yanning Zhang"],"url":"https://arxiv.org/abs/2505.01713"}
{"created":"2025-05-06","title":"Enhanced Flexibility Aggregation Using LinDistFlow Model with Loss Compensation","abstract":"With the increasing integration of renewable energy resources and the growing need for data privacy between system operators, flexibility aggregation methods have emerged as a promising solution to coordinate integrated transmissiondistribution (ITD) systems with limited information exchange. However, existing methods face significant challenges due to the nonlinearity of AC power flow models, and therefore mostly rely on linearized models. This paper examines the inherent errors in the LinDistFlow model, a linearized approximation, and demonstrates their impact on flexibility aggregation. To address these issues, we propose an intuitive compensation approach to refine the LinDistFlow-based flexibility set. Simulation results demonstrate the effectiveness of the proposed method in efficiently coordinating ITD systems.","authors":["Yanlin Jiang","Xinliang Dai","Frederik Zahn","Veit Hagenmeyer"],"url":"https://arxiv.org/abs/2505.01715"}
{"created":"2025-05-06","title":"Mitigating Compensatory Movements in Prosthesis Users via Adaptive Collaborative Robotics","abstract":"Prosthesis users can regain partial limb functionality, however, full natural limb mobility is rarely restored, often resulting in compensatory movements that lead to discomfort, inefficiency, and long-term physical strain. To address this issue, we propose a novel human-robot collaboration framework to mitigate compensatory mechanisms in upper-limb prosthesis users by exploiting their residual motion capabilities while respecting task requirements. Our approach introduces a personalised mobility model that quantifies joint-specific functional limitations and the cost of compensatory movements. This model is integrated into a constrained optimisation framework that computes optimal user postures for task performance, balancing functionality and comfort. The solution guides a collaborative robot to reconfigure the task environment, promoting effective interaction. We validated the framework using a new body-powered prosthetic device for single-finger amputation, which enhances grasping capabilities through synergistic closure with the hand but imposes wrist constraints. Initial experiments with healthy subjects wearing the prosthesis as a supernumerary finger demonstrated that a robotic assistant embedding the user-specific mobility model outperformed human partners in handover tasks, improving both the efficiency of the prosthesis user's grasp and reducing compensatory movements in functioning joints. These results highlight the potential of collaborative robots as effective workplace and caregiving assistants, promoting inclusion and better integration of prosthetic devices into daily tasks.","authors":["Marta Lagomarsino","Robin Arbaud","Francesco Tassi","Arash Ajoudani"],"url":"https://arxiv.org/abs/2505.01718"}
{"created":"2025-05-06","title":"VisTaxa: Developing a Taxonomy of Historical Visualizations","abstract":"Historical visualizations are a rich resource for visualization research. While taxonomy is commonly used to structure and understand the design space of visualizations, existing taxonomies primarily focus on contemporary visualizations and largely overlook historical visualizations. To address this gap, we describe an empirical method for taxonomy development. We introduce a coding protocol and the VisTaxa system for taxonomy labeling and comparison. We demonstrate using our method to develop a historical visualization taxonomy by coding 400 images of historical visualizations. We analyze the coding result and reflect on the coding process. Our work is an initial step toward a systematic investigation of the design space of historical visualizations.","authors":["Yu Zhang","Xinyue Chen","Weili Zheng","Yuhan Guo","Guozheng Li","Siming Chen","Xiaoru Yuan"],"url":"https://arxiv.org/abs/2505.01724"}
{"created":"2025-05-06","title":"Probabilistic Interactive 3D Segmentation with Hierarchical Neural Processes","abstract":"Interactive 3D segmentation has emerged as a promising solution for generating accurate object masks in complex 3D scenes by incorporating user-provided clicks. However, two critical challenges remain underexplored: (1) effectively generalizing from sparse user clicks to produce accurate segmentation, and (2) quantifying predictive uncertainty to help users identify unreliable regions. In this work, we propose NPISeg3D, a novel probabilistic framework that builds upon Neural Processes (NPs) to address these challenges. Specifically, NPISeg3D introduces a hierarchical latent variable structure with scene-specific and object-specific latent variables to enhance few-shot generalization by capturing both global context and object-specific characteristics. Additionally, we design a probabilistic prototype modulator that adaptively modulates click prototypes with object-specific latent variables, improving the model's ability to capture object-aware context and quantify predictive uncertainty. Experiments on four 3D point cloud datasets demonstrate that NPISeg3D achieves superior segmentation performance with fewer clicks while providing reliable uncertainty estimations.","authors":["Jie Liu","Pan Zhou","Zehao Xiao","Jiayi Shen","Wenzhe Yin","Jan-Jakob Sonke","Efstratios Gavves"],"url":"https://arxiv.org/abs/2505.01726"}
{"created":"2025-05-06","title":"Improved ALOHA-based URA with Index Modulation: Efficient Decoding and Analysis","abstract":"In this paper, an improved ALOHA-based unsourced random access (URA) scheme is proposed in MIMO channels. The channel coherent interval is divided into multiple sub-slots and each active user selects several sub-slots to send its codeword, namely, the channel access pattern. To be more specific, the data stream of each active user is divided into three parts. The first part is mapped as the compressed sensing (CS) pilot, which also serves for the consequent channel estimation. The second part is modulated by binary phase shift keying (BPSK). The obtained CS pilot and the antipodal BPSK signal are concatenated as its codeword. After that, the codeword of each active user is sent repeatedly based on its channel access pattern, which is determined by the third part of the information bits, namely, index modulation (IM). On the receiver side, a hard decision-based decoder is proposed which includes the CS decoder, maximal likelihood (ML)-based superposed codeword decomposer (SCD), and IM demodulator. To further reduce the complexity of the proposed decoder, a simplified SCD based on convex approximation is considered. The performance analysis is also provided. The exhaustive computer simulations confirm the superiority of our proposal.","authors":["Linjie Yang","Pingzhi Fan","Zhiguo Ding","Jingqiu Gao"],"url":"https://arxiv.org/abs/2505.01728"}
{"created":"2025-05-06","title":"PosePilot: Steering Camera Pose for Generative World Models with Self-supervised Depth","abstract":"Recent advancements in autonomous driving (AD) systems have highlighted the potential of world models in achieving robust and generalizable performance across both ordinary and challenging driving conditions. However, a key challenge remains: precise and flexible camera pose control, which is crucial for accurate viewpoint transformation and realistic simulation of scene dynamics. In this paper, we introduce PosePilot, a lightweight yet powerful framework that significantly enhances camera pose controllability in generative world models. Drawing inspiration from self-supervised depth estimation, PosePilot leverages structure-from-motion principles to establish a tight coupling between camera pose and video generation. Specifically, we incorporate self-supervised depth and pose readouts, allowing the model to infer depth and relative camera motion directly from video sequences. These outputs drive pose-aware frame warping, guided by a photometric warping loss that enforces geometric consistency across synthesized frames. To further refine camera pose estimation, we introduce a reverse warping step and a pose regression loss, improving viewpoint precision and adaptability. Extensive experiments on autonomous driving and general-domain video datasets demonstrate that PosePilot significantly enhances structural understanding and motion reasoning in both diffusion-based and auto-regressive world models. By steering camera pose with self-supervised depth, PosePilot sets a new benchmark for pose controllability, enabling physically consistent, reliable viewpoint synthesis in generative world models.","authors":["Bu Jin","Weize Li","Baihan Yang","Zhenxin Zhu","Junpeng Jiang","Huan-ang Gao","Haiyang Sun","Kun Zhan","Hengtong Hu","Xueyang Zhang","Peng Jia","Hao Zhao"],"url":"https://arxiv.org/abs/2505.01729"}
{"created":"2025-05-06","title":"PASCAL: Precise and Efficient ANN- SNN Conversion using Spike Accumulation and Adaptive Layerwise Activation","abstract":"Spiking Neural Networks (SNNs) have been put forward as an energy-efficient alternative to Artificial Neural Networks (ANNs) since they perform sparse Accumulate operations instead of the power-hungry Multiply-and-Accumulate operations. ANN-SNN conversion is a widely used method to realize deep SNNs with accuracy comparable to that of ANNs.~\\citeauthor{bu2023optimal} recently proposed the Quantization-Clip-Floor-Shift (QCFS) activation as an alternative to ReLU to minimize the accuracy loss during ANN-SNN conversion. Nevertheless, SNN inferencing requires a large number of timesteps to match the accuracy of the source ANN for real-world datasets. In this work, we propose PASCAL, which performs ANN-SNN conversion in such a way that the resulting SNN is mathematically equivalent to an ANN with QCFS-activation, thereby yielding similar accuracy as the source ANN with minimal inference timesteps. In addition, we propose a systematic method to configure the quantization step of QCFS activation in a layerwise manner, which effectively determines the optimal number of timesteps per layer for the converted SNN. Our results show that the ResNet-34 SNN obtained using PASCAL achieves an accuracy of $\\approx$74\\% on ImageNet with a 64$\\times$ reduction in the number of inference timesteps compared to existing approaches.","authors":["Pranav Ramesh","Gopalakrishnan Srinivasan"],"url":"https://arxiv.org/abs/2505.01730"}
{"created":"2025-05-06","title":"Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models","abstract":"Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the \\underline{S}hapley \\underline{V}alue-based \\underline{N}on-\\underline{U}niform \\underline{P}runing (\\methodname{}) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, \\methodname{} achieves a reduction in perplexity (PPL) of 18.01\\% and 19.55\\% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70\\% sparsity.","authors":["Chuan Sun","Han Yu","Lizhen Cui"],"url":"https://arxiv.org/abs/2505.01731"}
{"created":"2025-05-06","title":"Brain-Inspired Quantum Neural Architectures for Pattern Recognition: Integrating QSNN and QLSTM","abstract":"Recent advances in the fields of deep learning and quantum computing have paved the way for innovative developments in artificial intelligence. In this manuscript, we leverage these cutting-edge technologies to introduce a novel model that emulates the intricate functioning of the human brain, designed specifically for the detection of anomalies such as fraud in credit card transactions. Leveraging the synergies of Quantum Spiking Neural Networks (QSNN) and Quantum Long Short-Term Memory (QLSTM) architectures, our approach is developed in two distinct stages, closely mirroring the information processing mechanisms found in the brain's sensory and memory systems. In the initial stage, similar to the brain's hypothalamus, we extract low-level information from the data, emulating sensory data processing patterns. In the subsequent stage, resembling the hippocampus, we process this information at a higher level, capturing and memorizing correlated patterns. We will compare this model with other quantum models such as Quantum Neural Networks among others and their corresponding classical models.","authors":["Eva Andr\\'es","Manuel Pegalajar Cu\\'ellar","Gabriel Navarro"],"url":"https://arxiv.org/abs/2505.01735"}
{"created":"2025-05-06","title":"PeSANet: Physics-encoded Spectral Attention Network for Simulating PDE-Governed Complex Systems","abstract":"Accurately modeling and forecasting complex systems governed by partial differential equations (PDEs) is crucial in various scientific and engineering domains. However, traditional numerical methods struggle in real-world scenarios due to incomplete or unknown physical laws. Meanwhile, machine learning approaches often fail to generalize effectively when faced with scarce observational data and the challenge of capturing local and global features. To this end, we propose the Physics-encoded Spectral Attention Network (PeSANet), which integrates local and global information to forecast complex systems with limited data and incomplete physical priors. The model consists of two key components: a physics-encoded block that uses hard constraints to approximate local differential operators from limited data, and a spectral-enhanced block that captures long-range global dependencies in the frequency domain. Specifically, we introduce a novel spectral attention mechanism to model inter-spectrum relationships and learn long-range spatial features. Experimental results demonstrate that PeSANet outperforms existing methods across all metrics, particularly in long-term forecasting accuracy, providing a promising solution for simulating complex systems with limited data and incomplete physics.","authors":["Han Wan","Rui Zhang","Qi Wang","Yang Liu","Hao Sun"],"url":"https://arxiv.org/abs/2505.01736"}
{"created":"2025-05-06","title":"Learning Multi-frame and Monocular Prior for Estimating Geometry in Dynamic Scenes","abstract":"In monocular videos that capture dynamic scenes, estimating the 3D geometry of video contents has been a fundamental challenge in computer vision. Specifically, the task is significantly challenged by the object motion, where existing models are limited to predict only partial attributes of the dynamic scenes, such as depth or pointmaps spanning only over a pair of frames. Since these attributes are inherently noisy under multiple frames, test-time global optimizations are often employed to fully recover the geometry, which is liable to failure and incurs heavy inference costs. To address the challenge, we present a new model, coined MMP, to estimate the geometry in a feed-forward manner, which produces a dynamic pointmap representation that evolves over multiple frames. Specifically, based on the recent Siamese architecture, we introduce a new trajectory encoding module to project point-wise dynamics on the representation for each frame, which can provide significantly improved expressiveness for dynamic scenes. In our experiments, we find MMP can achieve state-of-the-art quality in feed-forward pointmap prediction, e.g., 15.1% enhancement in the regression error.","authors":["Seong Hyeon Park","Jinwoo Shin"],"url":"https://arxiv.org/abs/2505.01737"}
{"created":"2025-05-06","title":"Optimized Cascaded Position Control of BLDC Motors Considering Torque Ripple","abstract":"Brushless DC (BLDC) motors are increasingly used in various industries due to their reliability, low noise, and extended lifespan compared to traditional DC motors. Their high torque-to-weight ratio and impressive starting torque make them ideal for automotive, robotics, and industrial applications. This paper explores the multi-objective tuning of BLDC motor controllers, focusing on position and torque ripple. A state-space model of the BLDC motor and the entire control system, including the power stage and control structure, is developed in the Simulink environment. Two common control mechanisms, trapezoidal and Field Oriented Control (FOC), are implemented and optimized. Both mechanisms utilize a cascaded closed-loop position control, providing fair disturbance rejection but requiring challenging tuning of the controllers. To address these challenges, the non-dominated sorting genetic algorithm II (NSGA-II) is used for optimization. This study demonstrates the effectiveness of optimization techniques in enhancing the performance of control systems.","authors":["Mohammad Vedadi"],"url":"https://arxiv.org/abs/2505.01740"}
{"created":"2025-05-06","title":"An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding","abstract":"The rapid advancements in Large Vision Language Models (LVLMs) offer the potential to surpass conventional labeling by generating richer, more detailed descriptions of on-device human behavior understanding (HBU) in low-resolution vision systems, such as depth, thermal, and infrared. However, existing large vision language model (LVLM) approaches are unable to understand low-resolution data well as they are primarily designed for high-resolution data, such as RGB images. A quick fixing approach is to caption a large amount of low-resolution data, but it requires a significant amount of labor-intensive annotation efforts. In this paper, we propose a novel, labor-saving system, Llambda, designed to support low-resolution HBU. The core idea is to leverage limited labeled data and a large amount of unlabeled data to guide LLMs in generating informative captions, which can be combined with raw data to effectively fine-tune LVLM models for understanding low-resolution videos in HBU. First, we propose a Contrastive-Oriented Data Labeler, which can capture behavior-relevant information from long, low-resolution videos and generate high-quality pseudo labels for unlabeled data via contrastive learning. Second, we propose a Physical-Knowledge Guided Captioner, which utilizes spatial and temporal consistency checks to mitigate errors in pseudo labels. Therefore, it can improve LLMs' understanding of sequential data and then generate high-quality video captions. Finally, to ensure on-device deployability, we employ LoRA-based efficient fine-tuning to adapt LVLMs for low-resolution data. We evaluate Llambda using a region-scale real-world testbed and three distinct low-resolution datasets, and the experiments show that Llambda outperforms several state-of-the-art LVLM systems up to $40.03\\%$ on average Bert-Score.","authors":["Siyang Jiang","Bufang Yang","Lilin Xu","Mu Yuan","Yeerzhati Abudunuer","Kaiwei Liu","Liekang Zeng","Hongkai Chen","Zhenyu Yan","Xiaofan Jiang","Guoliang Xing"],"url":"https://arxiv.org/abs/2505.01743"}
{"created":"2025-05-06","title":"Memory-Efficient LLM Training by Various-Grained Low-Rank Projection of Gradients","abstract":"Building upon the success of low-rank adapter (LoRA), low-rank gradient projection (LoRP) has emerged as a promising solution for memory-efficient fine-tuning. However, existing LoRP methods typically treat each row of the gradient matrix as the default projection unit, leaving the role of projection granularity underexplored. In this work, we propose a novel framework, VLoRP, that extends low-rank gradient projection by introducing an additional degree of freedom for controlling the trade-off between memory efficiency and performance, beyond the rank hyper-parameter. Through this framework, we systematically explore the impact of projection granularity, demonstrating that finer-grained projections lead to enhanced stability and efficiency even under a fixed memory budget. Regarding the optimization for VLoRP, we present ProjFactor, an adaptive memory-efficient optimizer, that significantly reduces memory requirement while ensuring competitive performance, even in the presence of gradient accumulation. Additionally, we provide a theoretical analysis of VLoRP, demonstrating the descent and convergence of its optimization trajectory under both SGD and ProjFactor. Extensive experiments are conducted to validate our findings, covering tasks such as commonsense reasoning, MMLU, and GSM8K.","authors":["Yezhen Wang","Zhouhao Yang","Brian K Chen","Fanyi Pu","Bo Li","Tianyu Gao","Kenji Kawaguchi"],"url":"https://arxiv.org/abs/2505.01744"}
{"created":"2025-05-06","title":"Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion","abstract":"Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by individual self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed GES-Inter. Additionally, we propose Co$^3$Gesture, a novel framework that enables coherent concurrent co-speech gesture synthesis including two-person interactive movements. Considering the asymmetric body dynamics of two speakers, our framework is built upon two cooperative generation branches conditioned on separated speaker audio. Specifically, to enhance the coordination of human postures with respect to corresponding speaker audios while interacting with the conversational partner, we present a Temporal Interaction Module (TIM). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further holistically boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset. The dataset and source code are publicly available at \\href{https://mattie-e.github.io/Co3/}{\\textit{https://mattie-e.github.io/Co3/}}.","authors":["Xingqun Qi","Yatian Wang","Hengyuan Zhang","Jiahao Pan","Wei Xue","Shanghang Zhang","Wenhan Luo","Qifeng Liu","Yike Guo"],"url":"https://arxiv.org/abs/2505.01746"}
{"created":"2025-05-06","title":"Unified Steganography via Implicit Neural Representation","abstract":"Digital steganography is the practice of concealing for encrypted data transmission. Typically, steganography methods embed secret data into cover data to create stega data that incorporates hidden secret data. However, steganography techniques often require designing specific frameworks for each data type, which restricts their generalizability. In this paper, we present U-INR, a novel method for steganography via Implicit Neural Representation (INR). Rather than using the specific framework for each data format, we directly use the neurons of the INR network to represent the secret data and cover data across different data types. To achieve this idea, a private key is shared between the data sender and receivers. Such a private key can be used to determine the position of secret data in INR networks. To effectively leverage this key, we further introduce a key-based selection strategy that can be used to determine the position within the INRs for data storage. Comprehensive experiments across multiple data types, including images, videos, audio, and SDF and NeRF, demonstrate the generalizability and effectiveness of U-INR, emphasizing its potential for improving data security and privacy in various applications.","authors":["Qi Song","Ziyuan Luo","Xiufeng Huang","Sheng Li","Renjie Wan"],"url":"https://arxiv.org/abs/2505.01749"}
{"created":"2025-05-06","title":"NMPCB: A Lightweight and Safety-Critical Motion Control Framework","abstract":"In multi-obstacle environments, real-time performance and safety in robot motion control have long been challenging issues, as conventional methods often struggle to balance the two. In this paper, we propose a novel motion control framework composed of a Neural network-based path planner and a Model Predictive Control (MPC) controller based on control Barrier function (NMPCB) . The planner predicts the next target point through a lightweight neural network and generates a reference trajectory for the controller. In the design of the controller, we introduce the dual problem of control barrier function (CBF) as the obstacle avoidance constraint, enabling it to ensure robot motion safety while significantly reducing computation time. The controller directly outputs control commands to the robot by tracking the reference trajectory. This framework achieves a balance between real-time performance and safety. We validate the feasibility of the framework through numerical simulations and real-world experiments.","authors":["Longze Zheng","Qinghe Liu"],"url":"https://arxiv.org/abs/2505.01752"}
{"created":"2025-05-06","title":"From Formulas to Figures: How Visual Elements Impact User Interactions in Educational Videos","abstract":"Educational videos have become increasingly relevant in today's learning environments. While prior research in laboratory studies has provided valuable insights, analyzing real-world interaction data can enhance our understanding of authentic user behavior. Previous studies have investigated technical aspects, such as the influence of cuts on pausing behavior, but the impact of visual complexity remains understudied. In this paper, we address this gap and propose a novel approach centered on visual complexity, defined as the number of visually distinguishable and meaningful elements in a video frame, such as mathematical equations, chemical formulas, or graphical representations. Our study introduces a fine-grained taxonomy of visual objects in educational videos, expanding on previous classifications. Applying this taxonomy to 25 videos from physics and chemistry, we examine the relationship between visual complexity and user behavior, including pauses, in-video navigation, and session dropouts. The results indicate that increased visual complexity, especially of textual elements, correlates with more frequent pauses, rewinds, and dropouts. The results offer a deeper understanding of how video design affects user behavior in real-world scenarios. Our work has implications for optimizing educational videos, particularly in STEM fields. We make our code publicly available (https://github.com/TIBHannover/from_formulas_to_figures).","authors":["Wolfgang Gritz","Hewi Salih","Anett Hoppe","Ralph Ewerth"],"url":"https://arxiv.org/abs/2505.01753"}
{"created":"2025-05-06","title":"Unraveling Media Perspectives: A Comprehensive Methodology Combining Large Language Models, Topic Modeling, Sentiment Analysis, and Ontology Learning to Analyse Media Bias","abstract":"Biased news reporting poses a significant threat to informed decision-making and the functioning of democracies. This study introduces a novel methodology for scalable, minimally biased analysis of media bias in political news. The proposed approach examines event selection, labeling, word choice, and commission and omission biases across news sources by leveraging natural language processing techniques, including hierarchical topic modeling, sentiment analysis, and ontology learning with large language models. Through three case studies related to current political events, we demonstrate the methodology's effectiveness in identifying biases across news sources at various levels of granularity. This work represents a significant step towards scalable, minimally biased media bias analysis, laying the groundwork for tools to help news consumers navigate an increasingly complex media landscape.","authors":["Orlando J\\\"ahde","Thorsten Weber","R\\\"udiger Buchkremer"],"url":"https://arxiv.org/abs/2505.01754"}
{"created":"2025-05-06","title":"On the Design of Resilient Distributed Single Time-Scale Estimators: A Graph-Theoretic Approach","abstract":"Distributed estimation in interconnected systems has gained increasing attention due to its relevance in diverse applications such as sensor networks, autonomous vehicles, and cloud computing. In real practice, the sensor network may suffer from communication and/or sensor failures. This might be due to cyber-attacks, faults, or environmental conditions. Distributed estimation resilient to such conditions is the topic of this paper. By representing the sensor network as a graph and exploiting its inherent structural properties, we introduce novel techniques that enhance the robustness of distributed estimators. As compared to the literature, the proposed estimator (i) relaxes the network connectivity of most existing single time-scale estimators and (ii) reduces the communication load of the existing double time-scale estimators by avoiding the inner consensus loop.","authors":["Mohammadreza Doostmohammadian","Mohammad Pirani"],"url":"https://arxiv.org/abs/2505.01757"}
{"created":"2025-05-06","title":"Multiple Receiver Over-the-Air Computation for Wireless Networked Control Systems","abstract":"We propose a multi-sender, multi-receiver over-the-air computation (OAC) framework for wireless networked control systems (WNCS) with structural constraints. Our approach enables actuators to directly compute and apply control signals from sensor measurements, eliminating the need for a centralized controller. We use an iterative and convexifying procedure to obtain a control law that is structured with respect to the network topology and minimizes the overall system energy-to-energy gain. Furthermore, we solve a constrained matrix factorization problem to find the optimal OAC configuration with respect to power consumption, robustness, and stability of the WNCS. We prove the convergence of our proposed algorithms and present numerical results that validate our approach to preserve closed-loop stability with robust control performance and constrained power.","authors":["Seif Hussein","Chinwendu Enyioha","Carlo Fischione"],"url":"https://arxiv.org/abs/2505.01758"}
{"created":"2025-05-06","title":"Same evaluation, more tokens: On the effect of input length for machine translation evaluation using Large Language Models","abstract":"Accurately evaluating machine-translated text remains a long-standing challenge, particularly for long documents. Recent work has shown that large language models (LLMs) can serve as reliable and interpretable sentence-level translation evaluators via MQM error span annotations. With modern LLMs supporting larger context windows, a natural question arises: can we feed entire document translations into an LLM for quality assessment? Ideally, evaluation should be invariant to text length, producing consistent error spans regardless of input granularity. However, our analysis shows that text length significantly impacts evaluation: longer texts lead to fewer error spans and reduced system ranking accuracy. To address this limitation, we evaluate several strategies, including granularity-aligned prompting, Focus Sentence Prompting (FSP), and a fine-tuning approach to better align LLMs with the evaluation task. The latter two methods largely mitigate this length bias, making LLMs more reliable for long-form translation evaluation.","authors":["Tobias Domhan","Dawei Zhu"],"url":"https://arxiv.org/abs/2505.01761"}
{"created":"2025-05-06","title":"Evaluating an assembly- and disassembly-oriented expansion of Modular Function Deployment through a workshop-based assessment","abstract":"Modular product architectures are used to enhance flexibility, reduce production complexity, and support sustainability goals. However, traditional Modular Function Deployment (MFD) method does not fully integrate Design for Assembly (DFA) and Design for Disassembly (DFD) principles, leading to sub-optimal manufacturability and end-of-life strategies. This study introduces an expanded MFD method incorporating assembly and disassembly considerations into early-stage modularisation. A workshop-based evaluation assesses usability and applicability, involving participants using standard and expanded MFD. Results indicate that integrating DFA and DFD enhances assembly efficiency, ease of disassembly, and modular product strategy alignment. However, usability challenges were identified, necessitating refinements for industry application.","authors":["Fabio Marco Monetti"],"url":"https://arxiv.org/abs/2505.01762"}
{"created":"2025-05-06","title":"Multimodal Graph Representation Learning for Robust Surgical Workflow Recognition with Adversarial Feature Disentanglement","abstract":"Surgical workflow recognition is vital for automating tasks, supporting decision-making, and training novice surgeons, ultimately improving patient safety and standardizing procedures. However, data corruption can lead to performance degradation due to issues like occlusion from bleeding or smoke in surgical scenes and problems with data storage and transmission. In this case, we explore a robust graph-based multimodal approach to integrating vision and kinematic data to enhance accuracy and reliability. Vision data captures dynamic surgical scenes, while kinematic data provides precise movement information, overcoming limitations of visual recognition under adverse conditions. We propose a multimodal Graph Representation network with Adversarial feature Disentanglement (GRAD) for robust surgical workflow recognition in challenging scenarios with domain shifts or corrupted data. Specifically, we introduce a Multimodal Disentanglement Graph Network that captures fine-grained visual information while explicitly modeling the complex relationships between vision and kinematic embeddings through graph-based message modeling. To align feature spaces across modalities, we propose a Vision-Kinematic Adversarial framework that leverages adversarial training to reduce modality gaps and improve feature consistency. Furthermore, we design a Contextual Calibrated Decoder, incorporating temporal and contextual priors to enhance robustness against domain shifts and corrupted data. Extensive comparative and ablation experiments demonstrate the effectiveness of our model and proposed modules. Moreover, our robustness experiments show that our method effectively handles data corruption during storage and transmission, exhibiting excellent stability and robustness. Our approach aims to advance automated surgical workflow recognition, addressing the complexities and dynamism inherent in surgical procedures.","authors":["Long Bai","Boyi Ma","Ruohan Wang","Guankun Wang","Beilei Cui","Zhongliang Jiang","Mobarakol Islam","Zhe Min","Jiewen Lai","Nassir Navab","Hongliang Ren"],"url":"https://arxiv.org/abs/2505.01766"}
{"created":"2025-05-06","title":"Enhancing Black-Litterman Portfolio via Hybrid Forecasting Model Combining Multivariate Decomposition and Noise Reduction","abstract":"The sensitivity to input parameters and lack of flexibility limits the traditional Mean-Variance model. In contrast, the Black-Litterman model has attracted widespread attention by integrating market equilibrium returns with investors' subjective views. This paper proposes a novel hybrid deep learning model combining Singular Spectrum analysis (SSA), Multivariate Aligned Empirical Mode Decomposition (MA-EMD), and Temporal Convolutional Networks (TCNs), aiming to improve the prediction accuracy of asset prices and thus enhance the ability of the Black-Litterman model to generate subjective views. Experimental results show that noise reduction pre-processing can improve the model's accuracy, and the prediction performance of the proposed model is significantly better than that of three multivariate decomposition benchmark models. We construct an investment portfolio by using 20 representative stocks from the NASDAQ 100 index. By combining the hybrid forecasting model with the Black-Litterman model, the generated investment portfolio exhibits better returns and risk control capabilities than the Mean-Variance, Equal-Weighted, and Market-Weighted models in the short holding period.","authors":["Ziye Yang","Ke Lu"],"url":"https://arxiv.org/abs/2505.01781"}
{"created":"2025-05-06","title":"Energy-Efficient NTT Sampler for Kyber Benchmarked on FPGA","abstract":"Kyber is a lattice-based key encapsulation mechanism selected for standardization by the NIST Post-Quantum Cryptography (PQC) project. A critical component of Kyber's key generation process is the sampling of matrix elements from a uniform distribution over the ring Rq . This step is one of the most computationally intensive tasks in the scheme, significantly impacting performance in low-power embedded systems such as Internet of Things (IoT), wearable devices, wireless sensor networks (WSNs), smart cards, TPMs (Trusted Platform Modules), etc. Existing approaches to this sampling, notably conventional SampleNTT and Parse-SPDM3, rely on rejection sampling. Both algorithms require a large number of random bytes, which needs at least three SHAKE-128 squeezing steps per polynomial. As a result, it causes significant amount of latency and energy. In this work, we propose a novel and efficient sampling algorithm, namely Modified SampleNTT, which substantially educes the average number of bits required from SHAKE-128 to generate elements in Rq - achieving approximately a 33% reduction compared to conventional SampleNTT. Modified SampleNTT achieves 99.16% success in generating a complete polynomial using only two SHAKE-128 squeezes, outperforming both state-of-the-art methods, which never succeed in two squeezes of SHAKE-128. Furthermore, our algorithm maintains the same average rejection rate as existing techniques and passes all standard statistical tests for randomness quality. FPGA implementation on Artix-7 demonstrates a 33.14% reduction in energy, 33.32% lower latency, and 0.28% fewer slices compared to SampleNTT. Our results confirm that Modified SampleNTT is an efficient and practical alternative for uniform polynomial sampling in PQC schemes such as Kyber, especially for low-power security processors.","authors":["Paresh Baidya","Rourab Paul","Vikas Srivastava","Sumit Kumar Debnath"],"url":"https://arxiv.org/abs/2505.01782"}
{"created":"2025-05-06","title":"Context-Aware Online Conformal Anomaly Detection with Prediction-Powered Data Acquisition","abstract":"Online anomaly detection is essential in fields such as cybersecurity, healthcare, and industrial monitoring, where promptly identifying deviations from expected behavior can avert critical failures or security breaches. While numerous anomaly scoring methods based on supervised or unsupervised learning have been proposed, current approaches typically rely on a continuous stream of real-world calibration data to provide assumption-free guarantees on the false discovery rate (FDR). To address the inherent challenges posed by limited real calibration data, we introduce context-aware prediction-powered conformal online anomaly detection (C-PP-COAD). Our framework strategically leverages synthetic calibration data to mitigate data scarcity, while adaptively integrating real data based on contextual cues. C-PP-COAD utilizes conformal p-values, active p-value statistics, and online FDR control mechanisms to maintain rigorous and reliable anomaly detection performance over time. Experiments conducted on both synthetic and real-world datasets demonstrate that C-PP-COAD significantly reduces dependency on real calibration data without compromising guaranteed FDR control.","authors":["Amirmohammad Farzaneh","Osvaldo Simeone"],"url":"https://arxiv.org/abs/2505.01783"}
{"created":"2025-05-06","title":"Privacy Preserving Machine Learning Model Personalization through Federated Personalized Learning","abstract":"The widespread adoption of Artificial Intelligence (AI) has been driven by significant advances in intelligent system research. However, this progress has raised concerns about data privacy, leading to a growing awareness of the need for privacy-preserving AI. In response, there has been a seismic shift in interest towards the leading paradigm for training Machine Learning (ML) models on decentralized data silos while maintaining data privacy, Federated Learning (FL). This research paper presents a comprehensive performance analysis of a cutting-edge approach to personalize ML model while preserving privacy achieved through Privacy Preserving Machine Learning with the innovative framework of Federated Personalized Learning (PPMLFPL). Regarding the increasing concerns about data privacy, this study evaluates the effectiveness of PPMLFPL addressing the critical balance between personalized model refinement and maintaining the confidentiality of individual user data. According to our analysis, Adaptive Personalized Cross-Silo Federated Learning with Differential Privacy (APPLE+DP) offering efficient execution whereas overall, the use of the Adaptive Personalized Cross-Silo Federated Learning with Homomorphic Encryption (APPLE+HE) algorithm for privacy-preserving machine learning tasks in federated personalized learning settings is strongly suggested. The results offer valuable insights creating it a promising scope for future advancements in the field of privacy-conscious data-driven technologies.","authors":["Md. Tanzib Hosain","Asif Zaman","Md. Shahriar Sajid","Shadman Sakeeb Khan","Shanjida Akter"],"url":"https://arxiv.org/abs/2505.01788"}
{"created":"2025-05-06","title":"Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos","abstract":"Web-based educational videos offer flexible learning opportunities and are becoming increasingly popular. However, improving user engagement and knowledge retention remains a challenge. Automatically generated questions can activate learners and support their knowledge acquisition. Further, they can help teachers and learners assess their understanding. While large language and vision-language models have been employed in various tasks, their application to question generation for educational videos remains underexplored. In this paper, we investigate the capabilities of current vision-language models for generating learning-oriented questions for educational video content. We assess (1) out-of-the-box models' performance; (2) fine-tuning effects on content-specific question generation; (3) the impact of different video modalities on question quality; and (4) in a qualitative study, question relevance, answerability, and difficulty levels of generated questions. Our findings delineate the capabilities of current vision-language models, highlighting the need for fine-tuning and addressing challenges in question diversity and relevance. We identify requirements for future multimodal datasets and outline promising research directions.","authors":["Markos Stamatakis","Joshua Berger","Christian Wartena","Ralph Ewerth","Anett Hoppe"],"url":"https://arxiv.org/abs/2505.01790"}
{"created":"2025-05-06","title":"Byzantine Agreement with Predictions","abstract":"In this paper, we study the problem of \\emph{Byzantine Agreement with predictions}. Along with a proposal, each process is also given a prediction, i.e., extra information which is not guaranteed to be true. For example, one might imagine that the prediction is produced by a network security monitoring service that looks for patterns of malicious behavior.","authors":["Naama Ben-David","Muhammad Ayaz Dzulfikar","Faith Ellen","Seth Gilbert"],"url":"https://arxiv.org/abs/2505.01793"}
{"created":"2025-05-06","title":"A Multimodal Framework for Explainable Evaluation of Soft Skills in Educational Environments","abstract":"In the rapidly evolving educational landscape, the unbiased assessment of soft skills is a significant challenge, particularly in higher education. This paper presents a fuzzy logic approach that employs a Granular Linguistic Model of Phenomena integrated with multimodal analysis to evaluate soft skills in undergraduate students. By leveraging computational perceptions, this approach enables a structured breakdown of complex soft skill expressions, capturing nuanced behaviours with high granularity and addressing their inherent uncertainties, thereby enhancing interpretability and reliability. Experiments were conducted with undergraduate students using a developed tool that assesses soft skills such as decision-making, communication, and creativity. This tool identifies and quantifies subtle aspects of human interaction, such as facial expressions and gesture recognition. The findings reveal that the framework effectively consolidates multiple data inputs to produce meaningful and consistent assessments of soft skills, showing that integrating multiple modalities into the evaluation process significantly improves the quality of soft skills scores, making the assessment work transparent and understandable to educational stakeholders.","authors":["Jared D. T. Guerrero-Sosa","Francisco P. Romero","V\\'ictor Hugo Men\\'endez-Dom\\'inguez","Jesus Serrano-Guerrero","Andres Montoro-Montarroso","Jose A. Olivas"],"url":"https://arxiv.org/abs/2505.01794"}
{"created":"2025-05-06","title":"Semantics-Aware Unified Terrestrial Non-Terrestrial 6G Networks","abstract":"The integration of Terrestrial and Non-Terrestrial Networks (TN-NTNs), which was introduced in 5G, is progressing toward a unified and seamless network of networks in Sixth-Generation (6G). This evolution leads to a significant increase in the volume of generated and communicated data, imposing technical and operational requirements accompanied by a higher cost and energy consumption. Efficiently managing the generation and transmission of data in these highly complex unified networks has become essential. In this article, we investigate the semantics-aware information handling problem within unified TN-NTNs, where data communication between the distant TN nodes is enabled via an NTN. To this end, an Internet of Things (IoT) monitoring system is employed, where status updates from a remote IoT device are communicated to a destination monitor via a constellation of Low Earth Orbit (LEO) satellites. We leverage semantic metrics that capture the timeliness, relevance, and utility of information to provide the most informative data for timely and informed decision-making and eventually reduce the volume of transmitted and processed data. The outcome is significantly lower energy consumption, memory, control, and processing requirements (up to 73% lower energy charging demands compared to the state-of-the-art), all without compromising the conveyed information.","authors":["Erfan Delfani","Agapi Mesodiakaki","Nikolaos Pappas"],"url":"https://arxiv.org/abs/2505.01796"}
{"created":"2025-05-06","title":"AquaGS: Fast Underwater Scene Reconstruction with SfM-Free Gaussian Splatting","abstract":"Underwater scene reconstruction is a critical tech-nology for underwater operations, enabling the generation of 3D models from images captured by underwater platforms. However, the quality of underwater images is often degraded due to medium interference, which limits the effectiveness of Structure-from-Motion (SfM) pose estimation, leading to subsequent reconstruction failures. Additionally, SfM methods typically operate at slower speeds, further hindering their applicability in real-time scenarios. In this paper, we introduce AquaGS, an SfM-free underwater scene reconstruction model based on the SeaThru algorithm, which facilitates rapid and accurate separation of scene details and medium features. Our approach initializes Gaussians by integrating state-of-the-art multi-view stereo (MVS) technology, employs implicit Neural Radiance Fields (NeRF) for rendering translucent media and utilizes the latest explicit 3D Gaussian Splatting (3DGS) technique to render object surfaces, which effectively addresses the limitations of traditional methods and accurately simulates underwater optical phenomena. Experimental results on the data set and the robot platform show that our model can complete high-precision reconstruction in 30 seconds with only 3 image inputs, significantly enhancing the practical application of the algorithm in robotic platforms.","authors":["Junhao Shi","Jisheng Xu","Jianping He","Zhiliang Lin"],"url":"https://arxiv.org/abs/2505.01799"}
{"created":"2025-05-06","title":"Distinguishing AI-Generated and Human-Written Text Through Psycholinguistic Analysis","abstract":"The increasing sophistication of AI-generated texts highlights the urgent need for accurate and transparent detection tools, especially in educational settings, where verifying authorship is essential. Existing literature has demonstrated that the application of stylometric features with machine learning classifiers can yield excellent results. Building on this foundation, this study proposes a comprehensive framework that integrates stylometric analysis with psycholinguistic theories, offering a clear and interpretable approach to distinguishing between AI-generated and human-written texts. This research specifically maps 31 distinct stylometric features to cognitive processes such as lexical retrieval, discourse planning, cognitive load management, and metacognitive self-monitoring. In doing so, it highlights the unique psycholinguistic patterns found in human writing. Through the intersection of computational linguistics and cognitive science, this framework contributes to the development of reliable tools aimed at preserving academic integrity in the era of generative AI.","authors":["Chidimma Opara"],"url":"https://arxiv.org/abs/2505.01800"}
{"created":"2025-05-06","title":"Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows","abstract":"To have a seamless user experience on immersive AR/VR applications, the importance of efficient and effective Neural Network (NN) models is undeniable, since missing body parts that cannot be captured by limited sensors should be generated using these models for a complete 3D full-body reconstruction in virtual environment. However, the state-of-the-art NN-models are typically computational expensive and they leverage longer sequences of sparse tracking inputs to generate full-body movements by capturing temporal context. Inevitably, longer sequences increase the computation overhead and introduce noise in longer temporal dependencies that adversely affect the generation performance. In this paper, we propose a novel Multi-Layer Perceptron (MLP)-based method that enhances the overall performance while balancing the computational cost and memory overhead for efficient 3D full-body generation. Precisely, we introduce a NN-mechanism that divides the longer sequence of inputs into smaller temporal windows. Later, the current motion is merged with the information from these windows through latent representations to utilize the past context for the generation. Our experiments demonstrate that generation accuracy of our method with this NN-mechanism is significantly improved compared to the state-of-the-art methods while greatly reducing computational costs and memory overhead, making our method suitable for resource-constrained devices.","authors":["Georgios Fotios Angelis","Savas Ozkan","Sinan Mutlu","Paul Wisbey","Anastasios Drosou","Mete Ozay"],"url":"https://arxiv.org/abs/2505.01802"}
{"created":"2025-05-06","title":"Pathfinders in the Sky: Formal Decision-Making Models for Collaborative Air Traffic Control in Convective Weather","abstract":"Air traffic can be significantly disrupted by weather. Pathfinder operations involve assigning a designated aircraft to assess whether airspace that was previously impacted by weather can be safely traversed through. Despite relatively routine use in air traffic control, there is little research on the underlying multi-agent decision-making problem. We seek to address this gap herein by formulating decision models to capture the operational dynamics and implications of pathfinders. Specifically, we construct a Markov chain to represent the stochastic transitions between key operational states (e.g., pathfinder selection). We then analyze its steady-state behavior to understand long-term system dynamics. We also propose models to characterize flight-specific acceptance behaviors (based on utility trade-offs) and pathfinder selection strategies (based on sequential offer allocations). We then conduct a worst-case scenario analysis that highlights risks from collective rejection and explores how selfless behavior and uncertainty affect system resilience. Empirical analysis of data from the US Federal Aviation Administration demonstrates the real-world significance of pathfinder operations and informs future model calibration.","authors":["Jimin Choi","Kartikeya Anand","Husni R. Idris","Huy T. Tran","Max Z. Li"],"url":"https://arxiv.org/abs/2505.01804"}
{"created":"2025-05-06","title":"Not Every Tree Is a Forest: Benchmarking Forest Types from Satellite Remote Sensing","abstract":"Developing accurate and reliable models for forest types mapping is critical to support efforts for halting deforestation and for biodiversity conservation (such as European Union Deforestation Regulation (EUDR)). This work introduces ForTy, a benchmark for global-scale FORest TYpes mapping using multi-temporal satellite data1. The benchmark comprises 200,000 time series of image patches, each consisting of Sentinel-2, Sentinel-1, climate, and elevation data. Each time series captures variations at monthly or seasonal cadence. Per-pixel annotations, including forest types and other land use classes, support image segmentation tasks. Unlike most existing land use products that often categorize all forest areas into a single class, our benchmark differentiates between three forest types classes: natural forest, planted forest, and tree crops. By leveraging multiple public data sources, we achieve global coverage with this benchmark. We evaluate the forest types dataset using several baseline models, including convolution neural networks and transformer-based models. Additionally, we propose a novel transformer-based model specifically designed to handle multi-modal, multi-temporal satellite data for forest types mapping. Our experimental results demonstrate that the proposed model surpasses the baseline models in performance.","authors":["Yuchang Jiang","Maxim Neumann"],"url":"https://arxiv.org/abs/2505.01805"}
{"created":"2025-05-06","title":"Surrogate to Poincar\\'e inequalities on manifolds for dimension reduction in nonlinear feature spaces","abstract":"We aim to approximate a continuously differentiable function $u:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ by a composition of functions $f\\circ g$ where $g:\\mathbb{R}^d \\rightarrow \\mathbb{R}^m$, $m\\leq d$, and $f : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ are built in a two stage procedure. For a fixed $g$, we build $f$ using classical regression methods, involving evaluations of $u$. Recent works proposed to build a nonlinear $g$ by minimizing a loss function $\\mathcal{J}(g)$ derived from Poincar\\'e inequalities on manifolds, involving evaluations of the gradient of $u$. A problem is that minimizing $\\mathcal{J}$ may be a challenging task. Hence in this work, we introduce new convex surrogates to $\\mathcal{J}$. Leveraging concentration inequalities, we provide sub-optimality results for a class of functions $g$, including polynomials, and a wide class of input probability measures. We investigate performances on different benchmarks for various training sample sizes. We show that our approach outperforms standard iterative methods for minimizing the training Poincar\\'e inequality based loss, often resulting in better approximation errors, especially for rather small training sets and $m=1$.","authors":["Anthony Nouy","Alexandre Pasco"],"url":"https://arxiv.org/abs/2505.01807"}
{"created":"2025-05-06","title":"3DWG: 3D Weakly Supervised Visual Grounding via Category and Instance-Level Alignment","abstract":"The 3D weakly-supervised visual grounding task aims to localize oriented 3D boxes in point clouds based on natural language descriptions without requiring annotations to guide model learning. This setting presents two primary challenges: category-level ambiguity and instance-level complexity. Category-level ambiguity arises from representing objects of fine-grained categories in a highly sparse point cloud format, making category distinction challenging. Instance-level complexity stems from multiple instances of the same category coexisting in a scene, leading to distractions during grounding. To address these challenges, we propose a novel weakly-supervised grounding approach that explicitly differentiates between categories and instances. In the category-level branch, we utilize extensive category knowledge from a pre-trained external detector to align object proposal features with sentence-level category features, thereby enhancing category awareness. In the instance-level branch, we utilize spatial relationship descriptions from language queries to refine object proposal features, ensuring clear differentiation among objects. These designs enable our model to accurately identify target-category objects while distinguishing instances within the same category. Compared to previous methods, our approach achieves state-of-the-art performance on three widely used benchmarks: Nr3D, Sr3D, and ScanRef.","authors":["Xiaoqi Li","Jiaming Liu","Nuowei Han","Liang Heng","Yandong Guo","Hao Dong","Yang Liu"],"url":"https://arxiv.org/abs/2505.01809"}
{"created":"2025-05-06","title":"Conformal Prediction for Indoor Positioning with Correctness Coverage Guarantees","abstract":"With the advancement of Internet of Things (IoT) technologies, high-precision indoor positioning has become essential for Location-Based Services (LBS) in complex indoor environments. Fingerprint-based localization is popular, but traditional algorithms and deep learning-based methods face challenges such as poor generalization, overfitting, and lack of interpretability. This paper applies conformal prediction (CP) to deep learning-based indoor positioning. CP transforms the uncertainty of the model into a non-conformity score, constructs prediction sets to ensure correctness coverage, and provides statistical guarantees. We also introduce conformal risk control for path navigation tasks to manage the false discovery rate (FDR) and the false negative rate (FNR).The model achieved an accuracy of approximately 100% on the training dataset and 85% on the testing dataset, effectively demonstrating its performance and generalization capability. Furthermore, we also develop a conformal p-value framework to control the proportion of position-error points. Experiments on the UJIIndoLoc dataset using lightweight models such as MobileNetV1, VGG19, MobileNetV2, ResNet50, and EfficientNet show that the conformal prediction technique can effectively approximate the target coverage, and different models have different performance in terms of prediction set size and uncertainty quantification.","authors":["Zhiyi Zhou","Hexin Peng","Hongyu Long"],"url":"https://arxiv.org/abs/2505.01810"}
{"created":"2025-05-06","title":"Backdoor Attacks Against Patch-based Mixture of Experts","abstract":"As Deep Neural Networks (DNNs) continue to require larger amounts of data and computational power, Mixture of Experts (MoE) models have become a popular choice to reduce computational complexity. This popularity increases the importance of considering the security of MoE architectures. Unfortunately, the security of models using a MoE architecture has not yet gained much attention compared to other DNN models. In this work, we investigate the vulnerability of patch-based MoE (pMoE) models for image classification against backdoor attacks. We examine multiple trigger generation methods and Fine-Pruning as a defense. To better understand a pMoE model's vulnerability to backdoor attacks, we investigate which factors affect the model's patch selection. Our work shows that pMoE models are highly susceptible to backdoor attacks. More precisely, we achieve high attack success rates of up to 100% with visible triggers and a 2% poisoning rate, whilst only having a clean accuracy drop of 1.0%. Additionally, we show that pruning itself is ineffective as a defense but that fine-tuning can remove the backdoor almost completely. Our results show that fine-tuning the model for five epochs reduces the attack success rate to 2.1% whilst sacrificing 1.4% accuracy.","authors":["Cedric Chan","Jona te Lintelo","Stjepan Picek"],"url":"https://arxiv.org/abs/2505.01811"}
{"created":"2025-05-06","title":"$\\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge","abstract":"Humans and intelligent animals can effortlessly internalize new information (\"news\") and accurately extract the implications for performing downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the news is explicitly given as context, fine-tuning remains challenging for the models to consolidate learning in weights. In this paper, we introduce $\\textit{New News}$, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. We first demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our news dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications and Self-QAs -- designed to distill the knowledge from the model with context into the weights of the model without the context, which we term $\\textit{System-2 Fine-tuning}$ (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news. Furthermore, we discover the $\\textit{contexual shadowing effect}$, where training with the news $\\textit{in context}$ followed by its rephrases or QAs degrade learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.","authors":["Core Francisco Park","Zechen Zhang","Hidenori Tanaka"],"url":"https://arxiv.org/abs/2505.01812"}
{"created":"2025-05-06","title":"Rogue Cell: Adversarial Attack and Defense in Untrusted O-RAN Setup Exploiting the Traffic Steering xApp","abstract":"The Open Radio Access Network (O-RAN) architecture is revolutionizing cellular networks with its open, multi-vendor design and AI-driven management, aiming to enhance flexibility and reduce costs. Although it has many advantages, O-RAN is not threat-free. While previous studies have mainly examined vulnerabilities arising from O-RAN's intelligent components, this paper is the first to focus on the security challenges and vulnerabilities introduced by transitioning from single-operator to multi-operator RAN architectures. This shift increases the risk of untrusted third-party operators managing different parts of the network. To explore these vulnerabilities and their potential mitigation, we developed an open-access testbed environment that integrates a wireless network simulator with the official O-RAN Software Community (OSC) RAN intelligent component (RIC) cluster. This environment enables realistic, live data collection and serves as a platform for demonstrating APATE (adversarial perturbation against traffic efficiency), an evasion attack in which a malicious cell manipulates its reported key performance indicators (KPIs) and deceives the O-RAN traffic steering to gain unfair allocations of user equipment (UE). To ensure that O-RAN's legitimate activity continues, we introduce MARRS (monitoring adversarial RAN reports), a detection framework based on a long-short term memory (LSTM) autoencoder (AE) that learns contextual features across the network to monitor malicious telemetry (also demonstrated in our testbed). Our evaluation showed that by executing APATE, an attacker can obtain a 248.5% greater UE allocation than it was supposed to in a benign scenario. In addition, the MARRS detection method was also shown to successfully classify malicious cell activity, achieving accuracy of 99.2% and an F1 score of 0.978.","authors":["Eran Aizikovich","Dudu Mimran","Edita Grolman","Yuval Elovici","Asaf Shabtai"],"url":"https://arxiv.org/abs/2505.01816"}
{"created":"2025-05-06","title":"Adaptive DRL for IRS Mirror Orientation in Dynamic OWC Networks","abstract":"Intelligent reflecting surfaces (IRSs) have emerged as a promising solution to mitigate line-of-sight (LoS) blockages and enhance signal coverage in optical wireless communication (OWC) systems. In this work, we consider a mirror-based IRS to assist a dynamic indoor visible light communication (VLC) environment. We formulate an optimization problem that aims to maximize the sum rate by adjusting the orientation of the IRS mirrors. To enable real-time adaptability, the problem is modelled as a Markov decision process (MDP), and a deep reinforcement learning (DRL) algorithm, specifically deep deterministic policy gradient (DDPG), is employed to optimize mirror orientation toward mobile users under blockage and mobility constraints. Simulation results demonstrate that the proposed DDPG-based approach outperforms conventional DRL algorithms and achieves substantial improvements in sum rate compared to fixed-orientation IRS configurations.","authors":["Ahrar N. Hamad","Ahmad Adnan Qidan","Taisir E. H. El-Gorashi","Jaafar M. H. Elmirghani"],"url":"https://arxiv.org/abs/2505.01818"}
{"created":"2025-05-06","title":"An LSTM-PINN Hybrid Method to the specific problem of population forecasting","abstract":"Deep learning has emerged as a powerful tool in scientific modeling, particularly for complex dynamical systems; however, accurately capturing age-structured population dynamics under policy-driven fertility changes remains a significant challenge due to the lack of effective integration between domain knowledge and long-term temporal dependencies. To address this issue, we propose two physics-informed deep learning frameworks--PINN and LSTM-PINN--that incorporate policy-aware fertility functions into a transport-reaction partial differential equation to simulate population evolution from 2024 to 2054. The standard PINN model enforces the governing equation and boundary conditions via collocation-based training, enabling accurate learning of underlying population dynamics and ensuring stable convergence. Building on this, the LSTM-PINN framework integrates sequential memory mechanisms to effectively capture long-range dependencies in the age-time domain, achieving robust training performance across multiple loss components. Simulation results under three distinct fertility policy scenarios-the Three-child policy, the Universal two-child policy, and the Separate two-child policy--demonstrate the models' ability to reflect policy-sensitive demographic shifts and highlight the effectiveness of integrating domain knowledge into data-driven forecasting. This study provides a novel and extensible framework for modeling age-structured population dynamics under policy interventions, offering valuable insights for data-informed demographic forecasting and long-term policy planning in the face of emerging population challenges.","authors":["Ze Tao"],"url":"https://arxiv.org/abs/2505.01819"}
{"created":"2025-05-06","title":"Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey","abstract":"Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.","authors":["Jing Liu","Yao Du","Kun Yang","Yan Wang","Xiping Hu","Zehua Wang","Yang Liu","Peng Sun","Azzedine Boukerche","Victor C. M. Leung"],"url":"https://arxiv.org/abs/2505.01821"}
{"created":"2025-05-06","title":"Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning","abstract":"Conditional decision generation with diffusion models has shown powerful competitiveness in reinforcement learning (RL). Recent studies reveal the relation between energy-function-guidance diffusion models and constrained RL problems. The main challenge lies in estimating the intermediate energy, which is intractable due to the log-expectation formulation during the generation process. To address this issue, we propose the Analytic Energy-guided Policy Optimization (AEPO). Specifically, we first provide a theoretical analysis and the closed-form solution of the intermediate guidance when the diffusion model obeys the conditional Gaussian transformation. Then, we analyze the posterior Gaussian distribution in the log-expectation formulation and obtain the target estimation of the log-expectation under mild assumptions. Finally, we train an intermediate energy neural network to approach the target estimation of log-expectation formulation. We apply our method in 30+ offline RL tasks to demonstrate the effectiveness of our method. Extensive experiments illustrate that our method surpasses numerous representative baselines in D4RL offline reinforcement learning benchmarks.","authors":["Jifeng Hu","Sili Huang","Zhejian Yang","Shengchao Hu","Li Shen","Hechang Chen","Lichao Sun","Yi Chang","Dacheng Tao"],"url":"https://arxiv.org/abs/2505.01822"}
{"created":"2025-05-06","title":"PhytoSynth: Leveraging Multi-modal Generative Models for Crop Disease Data Generation with Novel Benchmarking and Prompt Engineering Approach","abstract":"Collecting large-scale crop disease images in the field is labor-intensive and time-consuming. Generative models (GMs) offer an alternative by creating synthetic samples that resemble real-world images. However, existing research primarily relies on Generative Adversarial Networks (GANs)-based image-to-image translation and lack a comprehensive analysis of computational requirements in agriculture. Therefore, this research explores a multi-modal text-to-image approach for generating synthetic crop disease images and is the first to provide computational benchmarking in this context. We trained three Stable Diffusion (SD) variants-SDXL, SD3.5M (medium), and SD3.5L (large)-and fine-tuned them using Dreambooth and Low-Rank Adaptation (LoRA) fine-tuning techniques to enhance generalization. SD3.5M outperformed the others, with an average memory usage of 18 GB, power consumption of 180 W, and total energy use of 1.02 kWh/500 images (0.002 kWh per image) during inference task. Our results demonstrate SD3.5M's ability to generate 500 synthetic images from just 36 in-field samples in 1.5 hours. We recommend SD3.5M for efficient crop disease data generation.","authors":["Nitin Rai","Arnold W. Schumann","Nathan Boyd"],"url":"https://arxiv.org/abs/2505.01823"}
{"created":"2025-05-06","title":"Priorconditioned Sparsity-Promoting Projection Methods for Deterministic and Bayesian Linear Inverse Problems","abstract":"High-quality reconstructions of signals and images with sharp edges are needed in a wide range of applications. To overcome the large dimensionality of the parameter space and the complexity of the regularization functional, {sparisty-promoting} techniques for both deterministic and hierarchical Bayesian regularization rely on solving a sequence of high-dimensional iteratively reweighted least squares (IRLS) problems on a lower-dimensional subspace. Generalized Krylov subspace (GKS) methods are a particularly potent class of hybrid Krylov schemes that efficiently solve sequences of IRLS problems by projecting large-scale problems into a relatively small subspace and successively enlarging it. We refer to methods that promote sparsity and use GKS as S-GKS. A disadvantage of S-GKS methods is their slow convergence. In this work, we propose techniques that improve the convergence of S-GKS methods by combining them with priorconditioning, which we refer to as PS-GKS. Specifically, integrating the PS-GKS method into the IAS algorithm allows us to automatically select the shape/rate parameter of the involved generalized gamma hyper-prior, which is often fine-tuned otherwise. Furthermore, we proposed and investigated variations of the proposed PS-GKS method, including restarting and recycling (resPS-GKS and recPS-GKS). These respectively leverage restarted and recycled subspaces to overcome situations when memory limitations of storing the basis vectors are a concern. We provide a thorough theoretical analysis showing the benefits of priorconditioning for sparsity-promoting inverse problems. Numerical experiment are used to illustrate that the proposed PS-GKS method and its variants are competitive with or outperform other existing hybrid Krylov methods.","authors":["Jonathan Lindbloom","Mirjeta Pasha","Jan Glaubitz","Youssef Marzouk"],"url":"https://arxiv.org/abs/2505.01827"}
{"created":"2025-05-06","title":"You Don't Have to Live Next to Me: Towards Demobilizing Individualistic Bias in Computational Approaches to Urban Segregation","abstract":"The global surge in social inequalities is one of the most pressing issues of our times. The spatial expression of social inequalities at city scale gives rise to urban segregation, a common phenomenon across different local and cultural contexts. The increasing popularity of Big Data and computational models has inspired a growing number of computational social science studies that analyze, evaluate, and issue policy recommendations for urban segregation. Today's wealth in information and computational power could inform urban planning for equity. However, as we show here, segregation research is epistemologically interdependent with prevalent economic theories which overfocus on individual responsibility while neglecting systemic processes. This individualistic bias is also engrained in computational models of urban segregation. Through several contemporary examples of how Big Data -- and the assumptions underlying its usage -- influence (de)segregation patterns and policies, our essay tells a cautionary tale. We highlight how a lack of consideration for data ethics can lead to the creation of computational models that have a real-life, further marginalizing impact on disadvantaged groups. With this essay, our aim is to develop a better discernment of the pitfalls and potentials of computational approaches to urban segregation, thereby fostering a conscious focus on systemic thinking about urban inequalities. We suggest setting an agenda for research and collective action that is directed at demobilizing individualistic bias, informing our thinking about urban segregation, but also more broadly our efforts to create sustainable cities and communities.","authors":["Anastassia Vybornova","Trivik Verma"],"url":"https://arxiv.org/abs/2505.01830"}
{"created":"2025-05-06","title":"Model Context Protocol-based Internet of Experts For Wireless Environment-aware LLM Agents","abstract":"Large Language Models (LLMs) exhibit strong general-purpose reasoning abilities but lack access to wireless environment information due to the absence of native sensory input and domain-specific priors. Previous attempts to apply LLMs in wireless systems either depend on retraining with network-specific data, which compromises language generalization, or rely on manually scripted interfaces, which hinder scalability. To overcome these limitations, we propose a Model Context Protocol (MCP)-based Internet of Experts (IoX) framework that equips LLMs with wireless environment-aware reasoning capabilities. The framework incorporates a set of lightweight expert models, each trained to solve a specific deterministic task in wireless communications, such as detecting a specific wireless attribute, e.g., line-of-sight propagation, Doppler effects, or fading conditions. Through MCP, the LLM can selectively query and interpret expert outputs at inference time, without modifying its own parameters. This architecture enables modular, extensible, and interpretable reasoning over wireless contexts. Evaluated across multiple mainstream LLMs, the proposed wireless environment-aware LLM agents achieve 40%-50% improvements in classification tasks over LLM-only baselines. More broadly, the MCP-based design offers a viable paradigm for future LLMs to inherit structured wireless network management capabilities.","authors":["Zongxi Liu","Hongyang Du"],"url":"https://arxiv.org/abs/2505.01834"}
{"created":"2025-05-06","title":"CVVNet: A Cross-Vertical-View Network for Gait Recognition","abstract":"Gait recognition enables contact-free, long-range person identification that is robust to clothing variations and non-cooperative scenarios. While existing methods perform well in controlled indoor environments, they struggle with cross-vertical view scenarios, where surveillance angles vary significantly in elevation. Our experiments show up to 60\\% accuracy degradation in low-to-high vertical view settings due to severe deformations and self-occlusions of key anatomical features. Current CNN and self-attention-based methods fail to effectively handle these challenges, due to their reliance on single-scale convolutions or simplistic attention mechanisms that lack effective multi-frequency feature integration. To tackle this challenge, we propose CVVNet (Cross-Vertical-View Network), a frequency aggregation architecture specifically designed for robust cross-vertical-view gait recognition. CVVNet employs a High-Low Frequency Extraction module (HLFE) that adopts parallel multi-scale convolution/max-pooling path and self-attention path as high- and low-frequency mixers for effective multi-frequency feature extraction from input silhouettes. We also introduce the Dynamic Gated Aggregation (DGA) mechanism to adaptively adjust the fusion ratio of high- and low-frequency features. The integration of our core Multi-Scale Attention Gated Aggregation (MSAGA) module, HLFE and DGA enables CVVNet to effectively handle distortions from view changes, significantly improving the recognition robustness across different vertical views. Experimental results show that our CVVNet achieves state-of-the-art performance, with $8.6\\%$ improvement on DroneGait and $2\\%$ on Gait3D compared with the best existing methods.","authors":["Xiangru Li","Wei Song","Yingda Huang","Wei Meng","Le Chang"],"url":"https://arxiv.org/abs/2505.01837"}
{"created":"2025-05-06","title":"MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization","abstract":"In this era, the success of large language models and text-to-image models can be attributed to the driving force of large-scale datasets. However, in the realm of 3D vision, while significant progress has been achieved in object-centric tasks through large-scale datasets like Objaverse and MVImgNet, human-centric tasks have seen limited advancement, largely due to the absence of a comparable large-scale human dataset. To bridge this gap, we present MVHumanNet++, a dataset that comprises multi-view human action sequences of 4,500 human identities. The primary focus of our work is on collecting human data that features a large number of diverse identities and everyday clothing using multi-view human capture systems, which facilitates easily scalable data collection. Our dataset contains 9,000 daily outfits, 60,000 motion sequences and 645 million frames with extensive annotations, including human masks, camera parameters, 2D and 3D keypoints, SMPL/SMPLX parameters, and corresponding textual descriptions. Additionally, the proposed MVHumanNet++ dataset is enhanced with newly processed normal maps and depth maps, significantly expanding its applicability and utility for advanced human-centric research. To explore the potential of our proposed MVHumanNet++ dataset in various 2D and 3D visual tasks, we conducted several pilot studies to demonstrate the performance improvements and effective applications enabled by the scale provided by MVHumanNet++. As the current largest-scale 3D human dataset, we hope that the release of MVHumanNet++ dataset with annotations will foster further innovations in the domain of 3D human-centric tasks at scale. MVHumanNet++ is publicly available at https://kevinlee09.github.io/research/MVHumanNet++/.","authors":["Chenghong Li","Hongjie Liao","Yihao Zhi","Xihe Yang","Zhengwentai Sun","Jiahao Chang","Shuguang Cui","Xiaoguang Han"],"url":"https://arxiv.org/abs/2505.01838"}
{"created":"2025-05-06","title":"Harnessing the Power of LLMs, Informers and Decision Transformers for Intent-driven RAN Management in 6G","abstract":"Intent-driven network management is critical for managing the complexity of 5G and 6G networks. It enables adaptive, on-demand management of the network based on the objectives of the network operators. In this paper, we propose an innovative three-step framework for intent-driven network management based on Generative AI (GenAI) algorithms. First, we fine-tune a Large Language Model (LLM) on a custom dataset using a Quantized Low-Rank Adapter (QLoRA) to enable memory-efficient intent processing within limited computational resources. A Retrieval Augmented Generation (RAG) module is included to support dynamic decision-making. Second, we utilize a transformer architecture for time series forecasting to predict key parameters, such as power consumption, traffic load, and packet drop rate, to facilitate intent validation proactively. Lastly, we introduce a Hierarchical Decision Transformer with Goal Awareness (HDTGA) to optimize the selection and orchestration of network applications and hence, optimize the network. Our intent guidance and processing approach improves BERTScore by 6% and the semantic similarity score by 9% compared to the base LLM model. Again, the proposed predictive intent validation approach can successfully rule out the performance-degrading intents with an average of 88% accuracy. Finally, compared to the baselines, the proposed HDTGA algorithm increases throughput at least by 19.3%, reduces delay by 48.5%, and boosts energy efficiency by 54.9%.","authors":["Md Arafat Habib","Pedro Enrique Iturria Rivera","Yigit Ozcan","Medhat Elsayed","Majid Bavand","Raimundas Gaigalas","Melike Erol-Kantarci"],"url":"https://arxiv.org/abs/2505.01841"}
{"created":"2025-05-06","title":"Exploring the Role of Diversity in Example Selection for In-Context Learning","abstract":"In-Context Learning (ICL) has gained prominence due to its ability to perform tasks without requiring extensive training data and its robustness to noisy labels. A typical ICL workflow involves selecting localized examples relevant to a given input using sparse or dense embedding-based similarity functions. However, relying solely on similarity-based selection may introduce topical biases in the retrieved contexts, potentially leading to suboptimal downstream performance. We posit that reranking the retrieved context to enhance topical diversity can improve downstream task performance. To achieve this, we leverage maximum marginal relevance (MMR) which balances topical similarity with inter-example diversity. Our experimental results demonstrate that diversifying the selected examples leads to consistent improvements in downstream performance across various context sizes and similarity functions. The implementation of our approach is made available at https://github.com/janak11111/Diverse-ICL.","authors":["Janak Kapuriya","Manit Kaushik","Debasis Ganguly","Sumit Bhatia"],"url":"https://arxiv.org/abs/2505.01842"}
{"created":"2025-05-06","title":"M-ary Precomputation-Based Accelerated Scalar Multiplication Algorithms for Enhanced Elliptic Curve Cryptography","abstract":"Efficient scalar multiplication is critical for enhancing the performance of elliptic curve cryptography (ECC), especially in applications requiring large-scale or real-time cryptographic operations. This paper proposes an M-ary precomputation-based scalar multiplication algorithm, aiming to optimize both computational efficiency and memory usage. The method reduces the time complexity from $\\Theta(Q \\log p)$ to $\\Theta\\left(\\frac{Q \\log p}{\\log Q}\\right)$ and achieves a memory complexity of $\\Theta\\left(\\frac{Q \\log p}{\\log^2 Q}\\right)$. Experiments on ElGamal encryption and NS3-based communication simulations validate its effectiveness. On secp256k1, the proposed method achieves up to a 59\\% reduction in encryption time and 30\\% memory savings. In network simulations, the binary-optimized variant reduces communication time by 22.1\\% on secp384r1 and simulation time by 25.4\\% on secp521r1. The results demonstrate the scalability, efficiency, and practical applicability of the proposed algorithm. The source code will be publicly released upon acceptance.","authors":["Tongxi Wu","Xufeng Liu","Jin Yang","Yijie Zhu","Shunyang Zeng","Mingming Zhan"],"url":"https://arxiv.org/abs/2505.01845"}
{"created":"2025-05-06","title":"Deep Reinforcement Learning-Aided Frequency Control of LCC-S Resonant Converters for Wireless Power Transfer Systems","abstract":"This paper presents a novel deep reinforcement learning (DRL)-based control strategy for achieving precise and robust output voltage regulation in LCC-S resonant converters, specifically designed for wireless power transfer applications. Unlike conventional methods that rely on manually tuned PI controllers or heuristic tuning approaches, our method leverages the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm to systematically optimize PI controller parameters. The complex converter dynamics are captured using the Direct Piecewise Affine (DPWA) modeling technique, providing a structured approach to handling its nonlinearities. This integration not only eliminates the need for manual tuning, but also enhances control adaptability under varying operating conditions. The simulation and experimental results confirm that the proposed DRL-based tuning approach significantly outperforms traditional methods in terms of stability, robustness, and response time. This work demonstrates the potential of DRL in power electronic control, offering a scalable and data-driven alternative to conventional controller design approaches.","authors":["Reza Safari","Mohsen Hamzeh","Nima Mahdian Dehkordi"],"url":"https://arxiv.org/abs/2505.01850"}
{"created":"2025-05-06","title":"Mitigating Group-Level Fairness Disparities in Federated Visual Language Models","abstract":"Visual language models (VLMs) have shown remarkable capabilities in multimodal tasks but face challenges in maintaining fairness across demographic groups, particularly when deployed in federated learning (FL) environments. This paper addresses the critical issue of group fairness in federated VLMs by introducing FVL-FP, a novel framework that combines FL with fair prompt tuning techniques. We focus on mitigating demographic biases while preserving model performance through three innovative components: (1) Cross-Layer Demographic Fair Prompting (CDFP), which adjusts potentially biased embeddings through counterfactual regularization; (2) Demographic Subspace Orthogonal Projection (DSOP), which removes demographic bias in image representations by mapping fair prompt text to group subspaces; and (3) Fair-aware Prompt Fusion (FPF), which dynamically balances client contributions based on both performance and fairness metrics. Extensive evaluations across four benchmark datasets demonstrate that our approach reduces demographic disparity by an average of 45\\% compared to standard FL approaches, while maintaining task performance within 6\\% of state-of-the-art results. FVL-FP effectively addresses the challenges of non-IID data distributions in federated settings and introduces minimal computational overhead while providing significant fairness benefits. Our work presents a parameter-efficient solution to the critical challenge of ensuring equitable performance across demographic groups in privacy-preserving multimodal systems.","authors":["Chaomeng Chen","Zitong Yu","Junhao Dong","Sen Su","Linlin Shen","Shutao Xia","Xiaochun Cao"],"url":"https://arxiv.org/abs/2505.01851"}
{"created":"2025-05-06","title":"Intra-Layer Recurrence in Transformers for Language Modeling","abstract":"Transformer models have established new benchmarks in natural language processing; however, their increasing depth results in substantial growth in parameter counts. While existing recurrent transformer methods address this issue by reprocessing layers multiple times, they often apply recurrence indiscriminately across entire blocks of layers. In this work, we investigate Intra-Layer Recurrence (ILR), a more targeted approach that applies recurrence selectively to individual layers within a single forward pass. Our experiments show that allocating more iterations to earlier layers yields optimal results. These findings suggest that ILR offers a promising direction for optimizing recurrent structures in transformer architectures.","authors":["Anthony Nguyen","Wenjun Lin"],"url":"https://arxiv.org/abs/2505.01855"}
{"created":"2025-05-06","title":"DualDiff: Dual-branch Diffusion Model for Autonomous Driving with Semantic Fusion","abstract":"Accurate and high-fidelity driving scene reconstruction relies on fully leveraging scene information as conditioning. However, existing approaches, which primarily use 3D bounding boxes and binary maps for foreground and background control, fall short in capturing the complexity of the scene and integrating multi-modal information. In this paper, we propose DualDiff, a dual-branch conditional diffusion model designed to enhance multi-view driving scene generation. We introduce Occupancy Ray Sampling (ORS), a semantic-rich 3D representation, alongside numerical driving scene representation, for comprehensive foreground and background control. To improve cross-modal information integration, we propose a Semantic Fusion Attention (SFA) mechanism that aligns and fuses features across modalities. Furthermore, we design a foreground-aware masked (FGM) loss to enhance the generation of tiny objects. DualDiff achieves state-of-the-art performance in FID score, as well as consistently better results in downstream BEV segmentation and 3D object detection tasks.","authors":["Haoteng Li","Zhao Yang","Zezhong Qian","Gongpeng Zhao","Yuqi Huang","Jun Yu","Huazheng Zhou","Longjun Liu"],"url":"https://arxiv.org/abs/2505.01857"}
{"created":"2025-05-06","title":"ReLI: A Language-Agnostic Approach to Human-Robot Interaction","abstract":"Adapting autonomous agents to industrial, domestic, and other daily tasks is currently gaining momentum. However, in the global or cross-lingual application contexts, ensuring effective interaction with the environment and executing unrestricted human task-specified instructions in diverse languages remains an unsolved problem. To address this challenge, we propose ReLI, a language-agnostic framework designed to enable autonomous agents to converse naturally, semantically reason about the environment, and to perform downstream tasks, regardless of the task instruction's linguistic origin. First, we ground large-scale pre-trained foundation models and transform them into language-to-action models that can directly provide common-sense reasoning and high-level robot control through natural, free-flow human-robot conversational interactions. Further, we perform cross-lingual grounding of the models to ensure that ReLI generalises across the global languages. To demonstrate the ReLI's robustness, we conducted extensive simulated and real-world experiments on various short- and long-horizon tasks, including zero-shot and few-shot spatial navigation, scene information retrieval, and query-oriented tasks. We benchmarked the performance on 140 languages involving over 70K multi-turn conversations. On average, ReLI achieved over 90%$\\pm$0.2 accuracy in cross-lingual instruction parsing and task execution success rates. These results demonstrate the ReLI's potential to enhance natural human-robot interaction in the real world while championing linguistic diversity. Demonstrations and resources will be publicly available at https://linusnep.github.io/ReLI/.","authors":["Linus Nwankwo","Bjoern Ellensohn","Ozan \\\"Ozdenizci","Elmar Rueckert"],"url":"https://arxiv.org/abs/2505.01862"}
{"created":"2025-05-06","title":"LCI: a Lightweight Communication Interface for Efficient Asynchronous Multithreaded Communication","abstract":"The evolution of architectures, programming models, and algorithms is driving communication towards greater asynchrony and concurrency, usually in multithreaded environments. We present LCI, a communication library designed for efficient asynchronous multithreaded communication. LCI provides a concise interface that supports common point-to-point primitives and diverse completion mechanisms, along with flexible controls for incrementally fine-tuning communication resources and runtime behavior. It features a threading-efficient runtime built on atomic data structures, fine-grained non-blocking locks, and low-level network insights. We evaluate LCI on both Inifiniband and Slingshot-11 clusters with microbenchmarks and two application-level benchmarks. Experiment results show that LCI significantly outperforms existing communication libraries in various multithreaded scenarios, achieving performance that exceeds the traditional multi-process execution mode and unlocking new possibilities for emerging programming models and applications.","authors":["Jiakun Yan","Marc Snir"],"url":"https://arxiv.org/abs/2505.01864"}
{"created":"2025-05-06","title":"PQS-BFL: A Post-Quantum Secure Blockchain-based Federated Learning Framework","abstract":"Federated Learning (FL) enables collaborative model training while preserving data privacy, but its classical cryptographic underpinnings are vulnerable to quantum attacks. This vulnerability is particularly critical in sensitive domains like healthcare. This paper introduces PQS-BFL (Post-Quantum Secure Blockchain-based Federated Learning), a framework integrating post-quantum cryptography (PQC) with blockchain verification to secure FL against quantum adversaries. We employ ML-DSA-65 (a FIPS 204 standard candidate, formerly Dilithium) signatures to authenticate model updates and leverage optimized smart contracts for decentralized validation. Extensive evaluations on diverse datasets (MNIST, SVHN, HAR) demonstrate that PQS-BFL achieves efficient cryptographic operations (average PQC sign time: 0.65 ms, verify time: 0.53 ms) with a fixed signature size of 3309 Bytes. Blockchain integration incurs a manageable overhead, with average transaction times around 4.8 s and gas usage per update averaging 1.72 x 10^6 units for PQC configurations. Crucially, the cryptographic overhead relative to transaction time remains minimal (around 0.01-0.02% for PQC with blockchain), confirming that PQC performance is not the bottleneck in blockchain-based FL. The system maintains competitive model accuracy (e.g., over 98.8% for MNIST with PQC) and scales effectively, with round times showing sublinear growth with increasing client numbers. Our open-source implementation and reproducible benchmarks validate the feasibility of deploying long-term, quantum-resistant security in practical FL systems.","authors":["Daniel Commey","Garth V. Crosby"],"url":"https://arxiv.org/abs/2505.01866"}
{"created":"2025-05-06","title":"Positional Attention for Efficient BERT-Based Named Entity Recognition","abstract":"This paper presents a framework for Named Entity Recognition (NER) leveraging the Bidirectional Encoder Representations from Transformers (BERT) model in natural language processing (NLP). NER is a fundamental task in NLP with broad applicability across downstream applications. While BERT has established itself as a state-of-the-art model for entity recognition, fine-tuning it from scratch for each new application is computationally expensive and time-consuming. To address this, we propose a cost-efficient approach that integrates positional attention mechanisms into the entity recognition process and enables effective customization using pre-trained parameters. The framework is evaluated on a Kaggle dataset derived from the Groningen Meaning Bank corpus and achieves strong performance with fewer training epochs. This work contributes to the field by offering a practical solution for reducing the training cost of BERT-based NER systems while maintaining high accuracy.","authors":["Mo Sun","Siheng Xiong","Yuankai Cai","Bowen Zuo"],"url":"https://arxiv.org/abs/2505.01868"}
{"created":"2025-05-06","title":"Visual enhancement and 3D representation for underwater scenes: a review","abstract":"Underwater visual enhancement (UVE) and underwater 3D reconstruction pose significant challenges in","authors":["Guoxi Huang","Haoran Wang","Brett Seymour","Evan Kovacs","John Ellerbrock","Dave Blackham","Nantheera Anantrasirichai"],"url":"https://arxiv.org/abs/2505.01869"}
{"created":"2025-05-06","title":"ResiTok: A Resilient Tokenization-Enabled Framework for Ultra-Low-Rate and Robust Image Transmission","abstract":"Real-time transmission of visual data over wireless networks remains highly challenging, even when leveraging advanced deep neural networks, particularly under severe channel conditions such as limited bandwidth and weak connectivity. In this paper, we propose a novel Resilient Tokenization-Enabled (ResiTok) framework designed for ultra-low-rate image transmission that achieves exceptional robustness while maintaining high reconstruction quality. By reorganizing visual information into hierarchical token groups consisting of essential key tokens and supplementary detail tokens, ResiTok enables progressive encoding and graceful degradation of visual quality under constrained channel conditions. A key contribution is our resilient 1D tokenization method integrated with a specialized zero-out training strategy, which systematically simulates token loss during training, empowering the neural network to effectively compress and reconstruct images from incomplete token sets. Furthermore, the channel-adaptive coding and modulation design dynamically allocates coding resources according to prevailing channel conditions, yielding superior semantic fidelity and structural consistency even at extremely low channel bandwidth ratios. Evaluation results demonstrate that ResiTok outperforms state-of-the-art methods in both semantic similarity and visual quality, with significant advantages under challenging channel conditions.","authors":["Zhenyu Liu","Yi Ma","Rahim Tafazolli"],"url":"https://arxiv.org/abs/2505.01870"}
{"created":"2025-05-06","title":"A computational framework for predicting the effect of surface roughness in fatigue","abstract":"Surface roughness is a critical factor influencing the fatigue life of structural components. Its effect is commonly quantified using a correction coefficient known as the surface factor. In this paper, a phase field based numerical framework is proposed to estimate the surface factor while accounting for the stochastic nature of surface roughness. The model is validated against existing experimental data. Furthermore, we investigate the influence of key parameters on the fatigue life of rough surfaces, such as surface topology and failure strength. An important effect of surface roughness is observed when the average surface roughness increases and the correlation length of the surface profile decreases. This effect becomes more pronounced with higher failure strengths.","authors":["S. Jim\\'enez-Alfaro","E. Mart\\'inez-Pa\\~neda"],"url":"https://arxiv.org/abs/2505.01871"}
{"created":"2025-05-06","title":"An Approach for Handling Missing Attribute Values in Attribute-Based Access Control Policy Mining","abstract":"Attribute-Based Access Control (ABAC) enables highly expressive and flexible access decisions by considering a wide range of contextual attributes. ABAC policies use logical expressions that combine these attributes, allowing for precise and context-aware control. Algorithms that mine ABAC policies from legacy access control systems can significantly reduce the costs associated with migrating to ABAC. However, a major challenge in this process is handling incomplete entity information, where some attribute values are missing.","authors":["Thang Bui","Elliot Shabram","Anthony Matricia"],"url":"https://arxiv.org/abs/2505.01873"}
{"created":"2025-05-06","title":"Towards Trustworthy Federated Learning with Untrusted Participants","abstract":"Resilience against malicious parties and data privacy are essential for trustworthy distributed learning, yet achieving both with good utility typically requires the strong assumption of a trusted central server. This paper shows that a significantly weaker assumption suffices: each pair of workers shares a randomness seed unknown to others. In a setting where malicious workers may collude with an untrusted server, we propose CafCor, an algorithm that integrates robust gradient aggregation with correlated noise injection, leveraging shared randomness between workers. We prove that CafCor achieves strong privacy-utility trade-offs, significantly outperforming local differential privacy (DP) methods, which do not make any trust assumption, while approaching central DP utility, where the server is fully trusted. Empirical results on standard benchmarks validate CafCor's practicality, showing that privacy and robustness can coexist in distributed systems without sacrificing utility or trusting the server.","authors":["Youssef Allouah","Rachid Guerraoui","John Stephan"],"url":"https://arxiv.org/abs/2505.01874"}
{"created":"2025-05-06","title":"Humans can learn to detect AI-generated texts, or at least learn when they can't","abstract":"This study investigates whether individuals can learn to accurately discriminate between human-written and AI-produced texts when provided with immediate feedback, and if they can use this feedback to recalibrate their self-perceived competence. We also explore the specific criteria individuals rely upon when making these decisions, focusing on textual style and perceived readability.","authors":["Ji\\v{r}\\'i Mili\\v{c}ka","Anna Marklov\\'a","Ond\\v{r}ej Drobil","Eva Posp\\'i\\v{s}ilov\\'a"],"url":"https://arxiv.org/abs/2505.01877"}
{"created":"2025-05-06","title":"What to Do When Privacy Is Gone","abstract":"Today's ethics of privacy is largely dedicated to defending personal information from big data technologies. This essay goes in the other direction. It considers the struggle to be lost, and explores two strategies for living after privacy is gone. First, total exposure embraces privacy's decline, and then contributes to the process with transparency. All personal information is shared without reservation. The resulting ethics is explored through a big data version of Robert Nozick's Experience Machine thought experiment. Second, transient existence responds to privacy's loss by ceaselessly generating new personal identities, which translates into constantly producing temporarily unviolated private information. The ethics is explored through Gilles Deleuze's metaphysics of difference applied in linguistic terms to the formation of the self. Comparing the exposure and transience alternatives leads to the conclusion that today's big data reality splits the traditional ethical link between authenticity and freedom. Exposure provides authenticity, but negates human freedom. Transience provides freedom, but disdains authenticity.","authors":["James Brusseau (Philosophy","Computer Science","Pace University","NYC)"],"url":"https://arxiv.org/abs/2505.01879"}
{"created":"2025-05-06","title":"Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network","abstract":"Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks.","authors":["Junyan Wu","Wenbo Xu","Wei Lu","Xiangyang Luo","Rui Yang","Shize Guo"],"url":"https://arxiv.org/abs/2505.01880"}
{"created":"2025-05-06","title":"PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications","abstract":"Robust navigation in diverse environments and domains requires both accurate state estimation and transparent decision making. We present PhysNav-DG, a novel framework that integrates classical sensor fusion with the semantic power of vision-language models. Our dual-branch architecture predicts navigation actions from multi-sensor inputs while simultaneously generating detailed chain-of-thought explanations. A modified Adaptive Kalman Filter dynamically adjusts its noise parameters based on environmental context. It leverages several streams of raw sensor data along with semantic insights from models such as LLaMA 3.2 11B and BLIP-2. To evaluate our approach, we introduce the MD-NEX Benchmark, a novel multi-domain dataset that unifies indoor navigation, autonomous driving, and social navigation tasks with ground-truth actions and human-validated explanations. Extensive experiments and ablations show that PhysNav-DG improves navigation success rates by over 20% and achieves high efficiency, with explanations that are both highly grounded and clear. This work connects high-level semantic reasoning and geometric planning for safer and more trustworthy autonomous systems.","authors":["Trisanth Srinivasan","Santosh Patapati"],"url":"https://arxiv.org/abs/2505.01881"}
{"created":"2025-05-06","title":"CMAWRNet: Multiple Adverse Weather Removal via a Unified Quaternion Neural Architecture","abstract":"Images used in real-world applications such as image or video retrieval, outdoor surveillance, and autonomous driving suffer from poor weather conditions. When designing robust computer vision systems, removing adverse weather such as haze, rain, and snow is a significant problem. Recently, deep-learning methods offered a solution for a single type of degradation. Current state-of-the-art universal methods struggle with combinations of degradations, such as haze and rain-streak. Few algorithms have been developed that perform well when presented with images containing multiple adverse weather conditions. This work focuses on developing an efficient solution for multiple adverse weather removal using a unified quaternion neural architecture called CMAWRNet. It is based on a novel texture-structure decomposition block, a novel lightweight encoder-decoder quaternion transformer architecture, and an attentive fusion block with low-light correction. We also introduce a quaternion similarity loss function to preserve color information better. The quantitative and qualitative evaluation of the current state-of-the-art benchmarking datasets and real-world images shows the performance advantages of the proposed CMAWRNet compared to other state-of-the-art weather removal approaches dealing with multiple weather artifacts. Extensive computer simulations validate that CMAWRNet improves the performance of downstream applications such as object detection. This is the first time the decomposition approach has been applied to the universal weather removal task.","authors":["Vladimir Frants","Sos Agaian","Karen Panetta","Peter Huang"],"url":"https://arxiv.org/abs/2505.01882"}
{"created":"2025-05-06","title":"Automated Sentiment Classification and Topic Discovery in Large-Scale Social Media Streams","abstract":"We present a framework for large-scale sentiment and topic analysis of Twitter discourse. Our pipeline begins with targeted data collection using conflict-specific keywords, followed by automated sentiment labeling via multiple pre-trained models to improve annotation robustness. We examine the relationship between sentiment and contextual features such as timestamp, geolocation, and lexical content. To identify latent themes, we apply Latent Dirichlet Allocation (LDA) on partitioned subsets grouped by sentiment and metadata attributes. Finally, we develop an interactive visualization interface to support exploration of sentiment trends and topic distributions across time and regions. This work contributes a scalable methodology for social media analysis in dynamic geopolitical contexts.","authors":["Yiwen Lu","Siheng Xiong","Zhaowei Li"],"url":"https://arxiv.org/abs/2505.01883"}
{"created":"2025-05-06","title":"Interactive authoring of outcome-oriented lesson plans for immersive Virtual Reality training","abstract":"Immersive Virtual Reality (iVR) applications have shown immense potential for skill training and learning in manufacturing. However, authoring of such applications requires technical expertise, which makes it difficult for educators to author instructions targeted at desired learning outcomes. We present FlowTrainer, an LLM-assisted interactive system to allow educators to author lesson plans for their iVR instruction based on desired goals. The authoring workflow is supported by Backward design to align the planned lesson based on the desired outcomes. We implemented a welding use case and conducted a user study with welding experts to test the effectiveness of the system in authoring outcome-oriented lesson plans. The study results showed that the system allowed users to plan lesson plans based on desired outcomes while reducing the time and technical expertise required for the authoring process. We believe that such efforts can allow widespread adoption of iVR solutions in manufacturing training to meet the workforce demands in the industry.","authors":["Ananya Ipsita","Ramesh Kaki","Mayank Patel","Asim Unmesh","Kylie A. Peppler","Karthik Ramani"],"url":"https://arxiv.org/abs/2505.01886"}
{"created":"2025-05-06","title":"Rethinking Score Distilling Sampling for 3D Editing and Generation","abstract":"Score Distillation Sampling (SDS) has emerged as a prominent method for text-to-3D generation by leveraging the strengths of 2D diffusion models. However, SDS is limited to generation tasks and lacks the capability to edit existing 3D assets. Conversely, variants of SDS that introduce editing capabilities often can not generate new 3D assets effectively. In this work, we observe that the processes of generation and editing within SDS and its variants have unified underlying gradient terms. Building on this insight, we propose Unified Distillation Sampling (UDS), a method that seamlessly integrates both the generation and editing of 3D assets. Essentially, UDS refines the gradient terms used in vanilla SDS methods, unifying them to support both tasks. Extensive experiments demonstrate that UDS not only outperforms baseline methods in generating 3D assets with richer details but also excels in editing tasks, thereby bridging the gap between 3D generation and editing. The code is available on: https://github.com/xingy038/UDS.","authors":["Xingyu Miao","Haoran Duan","Yang Long","Jungong Han"],"url":"https://arxiv.org/abs/2505.01888"}
{"created":"2025-05-06","title":"OODTE: A Differential Testing Engine for the ONNX Optimizer","abstract":"With $700$ stars on GitHub and part of the official ONNX repository, the ONNX Optimizer consists of the standard method to apply graph-based optimizations on ONNX models. However, its ability to preserve model accuracy across optimizations, has not been rigorously explored. We propose OODTE, a utility to automatically and thoroughly assess the correctness of the ONNX Optimizer. OODTE follows a simple, yet effective differential testing and evaluation approach that can be easily adopted to other compiler optimizers. In particular, OODTE utilizes a number of ONNX models, then optimizes them and executes both the original and the optimized variants across a user-defined set of inputs, while automatically logging any issues with the optimization process. Finally, for successfully optimized models, OODTE compares the results, and, if any accuracy deviations are observed, it iteratively repeats the process for each pass of the ONNX Optimizer, to localize the root cause of the differences observed. Using OODTE, we sourced well-known $130$ models from the official ONNX Model Hub, used for a wide variety of tasks (classification, object detection, semantic segmentation, text summarization, question and answering, sentiment analysis) from the official ONNX model hub. We detected 15 issues, 14 of which were previously unknown, associated with optimizer crashes and accuracy deviations. We also observed $9.2$% of all model instances presenting issues leading into the crash of the optimizer, or the generation of an invalid model while using the primary optimizer strategies. In addition, $30$% of the classification models presented accuracy differences across the original and the optimized model variants, while $16.6$% of semantic segmentation and object detection models are also affected, at least to a limited extent.","authors":["Nikolaos Louloudakis","Ajitha Rajan"],"url":"https://arxiv.org/abs/2505.01892"}
{"created":"2025-05-06","title":"DriveNetBench: An Affordable and Configurable Single-Camera Benchmarking System for Autonomous Driving Networks","abstract":"Validating autonomous driving neural networks often demands expensive equipment and complex setups, limiting accessibility for researchers and educators. We introduce DriveNetBench, an affordable and configurable benchmarking system designed to evaluate autonomous driving networks using a single-camera setup. Leveraging low-cost, off-the-shelf hardware, and a flexible software stack, DriveNetBench enables easy integration of various driving models, such as object detection and lane following, while ensuring standardized evaluation in real-world scenarios. Our system replicates common driving conditions and provides consistent, repeatable metrics for comparing network performance. Through preliminary experiments with representative vision models, we illustrate how DriveNetBench effectively measures inference speed and accuracy within a controlled test environment. The key contributions of this work include its affordability, its replicability through open-source software, and its seamless integration into existing workflows, making autonomous vehicle research more accessible.","authors":["Ali Al-Bustami","Humberto Ruiz-Ochoa","Jaerock Kwon"],"url":"https://arxiv.org/abs/2505.01893"}
{"created":"2025-05-06","title":"Certus: A domain specific language for confidence assessment in assurance cases","abstract":"Assurance cases (ACs) are prepared to argue that a system has satisfied critical quality attributes. Many methods exist to assess confidence in ACs, including quantitative methods that represent confidence numerically. While quantitative methods are attractive in principle, existing methods suffer from issues related to interpretation, subjectivity, scalability, dialectic reasoning, and trustworthiness, which have limited their adoption. This paper introduces Certus, a domain specific language for quantitative confidence assessment. In Certus, users describe their confidence with fuzzy sets, which allow them to represent their judgment using vague, but linguistically meaningful terminology. Certus includes syntax to specify confidence propagation using expressions that can be easily inspected by users. To demonstrate the concept of the language, Certus is applied to a worked example from the automotive domain.","authors":["Simon Diemert","Jens H. Weber"],"url":"https://arxiv.org/abs/2505.01894"}
{"created":"2025-05-06","title":"CAMOUFLAGE: Exploiting Misinformation Detection Systems Through LLM-driven Adversarial Claim Transformation","abstract":"Automated evidence-based misinformation detection systems, which evaluate the veracity of short claims against evidence, lack comprehensive analysis of their adversarial vulnerabilities. Existing black-box text-based adversarial attacks are ill-suited for evidence-based misinformation detection systems, as these attacks primarily focus on token-level substitutions involving gradient or logit-based optimization strategies, which are incapable of fooling the multi-component nature of these detection systems. These systems incorporate both retrieval and claim-evidence comparison modules, which requires attacks to break the retrieval of evidence and/or the comparison module so that it draws incorrect inferences. We present CAMOUFLAGE, an iterative, LLM-driven approach that employs a two-agent system, a Prompt Optimization Agent and an Attacker Agent, to create adversarial claim rewritings that manipulate evidence retrieval and mislead claim-evidence comparison, effectively bypassing the system without altering the meaning of the claim. The Attacker Agent produces semantically equivalent rewrites that attempt to mislead detectors, while the Prompt Optimization Agent analyzes failed attack attempts and refines the prompt of the Attacker to guide subsequent rewrites. This enables larger structural and stylistic transformations of the text rather than token-level substitutions, adapting the magnitude of changes based on previous outcomes. Unlike existing approaches, CAMOUFLAGE optimizes its attack solely based on binary model decisions to guide its rewriting process, eliminating the need for classifier logits or extensive querying. We evaluate CAMOUFLAGE on four systems, including two recent academic systems and two real-world APIs, with an average attack success rate of 46.92\\% while preserving textual coherence and semantic equivalence to the original claims.","authors":["Mazal Bethany","Nishant Vishwamitra","Cho-Yu Jason Chiang","Peyman Najafirad"],"url":"https://arxiv.org/abs/2505.01900"}
{"created":"2025-05-06","title":"Are Programming Paradigms Paradigms? A Critical Examination of Floyd's Appropriation of Kuhn's Philosophy","abstract":"This paper examines the philosophical relationship between Thomas Kuhn's concept of scientific paradigms and the programming paradigm concept in computing that was introduced by Floyd in his 1978 Turing Award lecture. Through critical analysis of both Kuhn's original framework and its application in computing, we argue that the contemporary usage of `programming paradigms' represents a significant departure from Kuhn's philosophical concept. We demonstrate that while Floyd explicitly attributed this term to Kuhn's work, his usage fundamentally altered the concept's meaning. We argue that this divergence necessitates a critical reassessment of the term's usage in computing discourse.","authors":["Peyman M. Kiasari"],"url":"https://arxiv.org/abs/2505.01901"}
{"created":"2025-05-06","title":"From Players to Champions: A Generalizable Machine Learning Approach for Match Outcome Prediction with Insights from the FIFA World Cup","abstract":"Accurate prediction of FIFA World Cup match outcomes holds significant value for analysts, coaches, bettors, and fans. This paper presents a machine learning framework specifically designed to forecast match winners in FIFA World Cup. By integrating both team-level historical data and player-specific performance metrics such as goals, assists, passing accuracy, and tackles, we capture nuanced interactions often overlooked by traditional aggregate models. Our methodology processes multi-year data to create year-specific team profiles that account for evolving rosters and player development. We employ classification techniques complemented by dimensionality reduction and hyperparameter optimization, to yield robust predictive models. Experimental results on data from the FIFA 2022 World Cup demonstrate our approach's superior accuracy compared to baseline method. Our findings highlight the importance of incorporating individual player attributes and team-level composition to enhance predictive performance, offering new insights into player synergy, strategic match-ups, and tournament progression scenarios. This work underscores the transformative potential of rich, player-centric data in sports analytics, setting a foundation for future exploration of advanced learning architectures such as graph neural networks to model complex team interactions.","authors":["Ali Al-Bustami","Zaid Ghazal"],"url":"https://arxiv.org/abs/2505.01902"}
{"created":"2025-05-06","title":"LookAlike: Consistent Distractor Generation in Math MCQs","abstract":"Large language models (LLMs) are increasingly used to generate distractors for multiple-choice questions (MCQs), especially in domains like math education. However, existing approaches are limited in ensuring that the generated distractors are consistent with common student errors. We propose LookAlike, a method that improves error-distractor consistency via preference optimization. Our two main innovations are: (a) mining synthetic preference pairs from model inconsistencies, and (b) alternating supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to stabilize training. Unlike prior work that relies on heuristics or manually annotated preference data, LookAlike uses its own generation inconsistencies as dispreferred samples, thus enabling scalable and stable training. Evaluated on a real-world dataset of 1,400+ math MCQs, LookAlike achieves 51.6% accuracy in distractor generation and 57.2% in error generation under LLM-as-a-judge evaluation, outperforming an existing state-of-the-art method (45.6% / 47.7%). These improvements highlight the effectiveness of preference-based regularization and inconsistency mining for generating consistent math MCQ distractors at scale.","authors":["Nisarg Parikh","Nigel Fernandez","Alexander Scarlatos","Simon Woodhead","Andrew Lan"],"url":"https://arxiv.org/abs/2505.01903"}
{"created":"2025-05-06","title":"A Generalised and Adaptable Reinforcement Learning Stopping Method","abstract":"This paper presents a Technology Assisted Review (TAR) stopping approach based on Reinforcement Learning (RL). Previous such approaches offered limited control over stopping behaviour, such as fixing the target recall and tradeoff between preferring to maximise recall or cost. These limitations are overcome by introducing a novel RL environment, GRLStop, that allows a single model to be applied to multiple target recalls, balances the recall/cost tradeoff and integrates a classifier. Experiments were carried out on six benchmark datasets (CLEF e-Health datasets 2017-9, TREC Total Recall, TREC Legal and Reuters RCV1) at multiple target recall levels. Results showed that the proposed approach to be effective compared to multiple baselines in addition to offering greater flexibility.","authors":["Reem Bin-Hezam","Mark Stevenson"],"url":"https://arxiv.org/abs/2505.01907"}
{"created":"2025-05-06","title":"BOOM: Benchmarking Out-Of-distribution Molecular Property Predictions of Machine Learning Models","abstract":"Advances in deep learning and generative modeling have driven interest in data-driven molecule discovery pipelines, whereby machine learning (ML) models are used to filter and design novel molecules without requiring prohibitively expensive first-principles simulations. Although the discovery of novel molecules that extend the boundaries of known chemistry requires accurate out-of-distribution (OOD) predictions, ML models often struggle to generalize OOD. Furthermore, there are currently no systematic benchmarks for molecular OOD prediction tasks. We present BOOM, $\\boldsymbol{b}$enchmarks for $\\boldsymbol{o}$ut-$\\boldsymbol{o}$f-distribution $\\boldsymbol{m}$olecular property predictions -- a benchmark study of property-based out-of-distribution models for common molecular property prediction models. We evaluate more than 140 combinations of models and property prediction tasks to benchmark deep learning models on their OOD performance. Overall, we do not find any existing models that achieve strong OOD generalization across all tasks: even the top performing model exhibited an average OOD error 3x larger than in-distribution. We find that deep learning models with high inductive bias can perform well on OOD tasks with simple, specific properties. Although chemical foundation models with transfer and in-context learning offer a promising solution for limited training data scenarios, we find that current foundation models do not show strong OOD extrapolation capabilities. We perform extensive ablation experiments to highlight how OOD performance is impacted by data generation, pre-training, hyperparameter optimization, model architecture, and molecular representation. We propose that developing ML models with strong OOD generalization is a new frontier challenge in chemical ML model development. This open-source benchmark will be made available on Github.","authors":["Evan R. Antoniuk","Shehtab Zaman","Tal Ben-Nun","Peggy Li","James Diffenderfer","Busra Demirci","Obadiah Smolenski","Tim Hsu","Anna M. Hiszpanski","Kenneth Chiu","Bhavya Kailkhura","Brian Van Essen"],"url":"https://arxiv.org/abs/2505.01912"}
{"created":"2025-05-06","title":"Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling","abstract":"Generative diffusion models have achieved remarkable success in producing high-quality images. However, because these models typically operate in continuous intensity spaces - diffusing independently per pixel and color channel - they are fundamentally ill-suited for applications where quantities such as particle counts or material units are inherently discrete and governed by strict conservation laws such as mass preservation, limiting their applicability in scientific workflows. To address this limitation, we propose Discrete Spatial Diffusion (DSD), a framework based on a continuous-time, discrete-state jump stochastic process that operates directly in discrete spatial domains while strictly preserving mass in both forward and reverse diffusion processes. By using spatial diffusion to achieve mass preservation, we introduce stochasticity naturally through a discrete formulation. We demonstrate the expressive flexibility of DSD by performing image synthesis, class conditioning, and image inpainting across widely-used image benchmarks, with the ability to condition on image intensity. Additionally, we highlight its applicability to domain-specific scientific data for materials microstructure, bridging the gap between diffusion models and mass-conditioned scientific applications.","authors":["Javier E. Santos","Agnese Marcato","Roman Colman","Nicholas Lubbers","Yen Ting Lin"],"url":"https://arxiv.org/abs/2505.01917"}
{"created":"2025-05-06","title":"ImageR: Enhancing Bug Report Clarity by Screenshots","abstract":"In issue-tracking systems, incorporating screenshots significantly enhances the clarity of bug reports, facilitating more efficient communication and expediting issue resolution. However, determining when and what type of visual content to include remains challenging, as not all attachments effectively contribute to problem-solving; studies indicate that 22.5% of images in issue reports fail to aid in resolving the reported issues. To address this, we introduce ImageR, an AI model and tool that analyzes issue reports to assess the potential benefits of including screenshots and recommends the most pertinent types when appropriate. By proactively suggesting relevant visuals, ImageR aims to make issue reports clearer, more informative, and time-efficient. We have curated and publicly shared a dataset comprising 6,235 Bugzilla issues, each meticulously labeled with the type of image attachment, providing a valuable resource for benchmarking and advancing research in image processing within developer communication contexts. To evaluate ImageR, we conducted empirical experiments on a subset of these reports from various Mozilla projects. The tool achieved an F1-score of 0.76 in determining when images are needed, with 75% of users finding its recommendations highly valuable. By minimizing the back-and-forth communication often needed to obtain suitable screenshots, ImageR streamlines the bug reporting process. Furthermore, it guides users in selecting the most effective visual documentation from ten established categories, potentially reducing resolution times and improving the quality of bug documentation. ImageR is open-source, inviting further use and improvement by the community. The labeled dataset offers a rare resource for benchmarking and exploring image processing in the context of developer communication.","authors":["Xuchen Tan","Deenu Yadav","Faiz Ahmed","Maleknaz Nayebi"],"url":"https://arxiv.org/abs/2505.01925"}
{"created":"2025-05-06","title":"Site Reliability Engineering (SRE) and Observations on SRE Process to Make Tasks Easier","abstract":"This paper explores Site Reliability Engineering (SRE), a modern approach to maintaining scalable and reliable software systems. It presents observations on how structured SRE processes improve operational efficiency, reduce system downtime, and simplify maintenance. Drawing from real-world implementations, the study outlines key techniques in automation, monitoring, incident management, and deployment strategies. The work also highlights how these practices can be tailored to different environments, offering practical insights for engineers aiming to improve service reliability.","authors":["Balaram Puli"],"url":"https://arxiv.org/abs/2505.01926"}
{"created":"2025-05-06","title":"Continuously Ordered Hierarchies of Algorithmic Information in Digital Twinning and Signal Processing","abstract":"We consider a fractional-calculus example of a continuous hierarchy of algorithmic information in the context of its potential applications in digital twinning. Digital twinning refers to different emerging methodologies in control engineering that involve the creation of a digital replica of some physical entity. From the perspective of computability theory, the problem of ensuring the digital twin's integrity -- i.e., keeping it in a state where it matches its physical counterpart -- entails a notion of algorithmic information that determines which of the physical system's properties we can reliably deduce by algorithmically analyzing its digital twin. The present work investigates the fractional calculus of periodic functions -- particularly, we consider the Wiener algebra -- as an exemplary application of the algorithmic-information concept. We establish a continuously ordered hierarchy of algorithmic information among spaces of periodic functions -- depending on their fractional degree of smoothness -- in which the ordering relation determines whether a certain representation of some function contains ``more'' or ``less'' information than another. Additionally, we establish an analogous hierarchy among lp-spaces, which form a cornerstone of (traditional) digital signal processing. Notably, both hierarchies are (mathematically) ``dual'' to each other. From a practical perspective, our approach ultimately falls into the category of formal verification and (general) formal methods.","authors":["Yannik N. B\\\"ock","Holger Boche","Frank H. P. Fitzek"],"url":"https://arxiv.org/abs/2505.01927"}
{"created":"2025-05-06","title":"GenSync: A Generalized Talking Head Framework for Audio-driven Multi-Subject Lip-Sync using 3D Gaussian Splatting","abstract":"We introduce GenSync, a novel framework for multi-identity lip-synced video synthesis using 3D Gaussian Splatting. Unlike most existing 3D methods that require training a new model for each identity , GenSync learns a unified network that synthesizes lip-synced videos for multiple speakers. By incorporating a Disentanglement Module, our approach separates identity-specific features from audio representations, enabling efficient multi-identity video synthesis. This design reduces computational overhead and achieves 6.8x faster training compared to state-of-the-art models, while maintaining high lip-sync accuracy and visual quality.","authors":["Anushka Agarwal","Muhammad Yusuf Hassan","Talha Chafekar"],"url":"https://arxiv.org/abs/2505.01928"}
{"created":"2025-05-06","title":"A Matrix Product State Representation of Boolean Functions","abstract":"We introduce a novel normal form representation of Boolean functions in terms of products of binary matrices, hereafter referred to as the Binary Matrix Product (BMP) representation. BMPs are analogous to the Tensor-Trains (TT) and Matrix Product States (MPS) used, respectively, in applied mathematics and in quantum many-body physics to accelerate computations that are usually inaccessible by more traditional approaches. BMPs turn out to be closely related to Binary Decision Diagrams (BDDs), a powerful compressed representation of Boolean functions invented in the late 80s by Bryant that has found a broad range of applications in many areas of computer science and engineering. We present a direct and natural translation of BMPs into Binary Decision Diagrams (BDDs), and derive an elementary set of operations used to manipulate and combine BMPs that are analogous to those introduced by Bryant for BDDs. Both BDDs and BMPs are practical tools when the complexity of these representations, as measured by the maximum bond dimension of a BMP (or the accumulated bond dimension across the BMP matrix train) and the number of nodes of a BDD, remains polynomial in the number of bits.. In both cases, controlling the complexity hinges on optimizing the order of the Boolean variables. BMPs offer the advantage that their construction and manipulation rely on simple linear algebra -- a compelling feature that can facilitate the development of open-source libraries that are both more flexible and easier to use than those currently available for BDDs. An initial implementation of a BMP library is available on GitHub, with the expectation that the close conceptual connection to TT and MPS techniques will motivate further development of BMP methods by researchers in these fields, potentially enabling novel applications to classical and quantum computing.","authors":["Umut Eren Usturali","Claudio Chamon","Andrei E. Ruckenstein","Eduardo R. Mucciolo"],"url":"https://arxiv.org/abs/2505.01930"}
{"created":"2025-05-06","title":"Semantic Intelligence: Integrating GPT-4 with A Planning in Low-Cost Robotics","abstract":"Classical robot navigation often relies on hardcoded state machines and purely geometric path planners, limiting a robot's ability to interpret high-level semantic instructions. In this paper, we first assess GPT-4's ability to act as a path planner compared to the A* algorithm, then present a hybrid planning framework that integrates GPT-4's semantic reasoning with A* on a low-cost robot platform operating on ROS2 Humble. Our approach eliminates explicit finite state machine (FSM) coding by using prompt-based GPT-4 reasoning to handle task logic while maintaining the accurate paths computed by A*. The GPT-4 module provides semantic understanding of instructions and environmental cues (e.g., recognizing toxic obstacles or crowded areas to avoid, or understanding low-battery situations requiring alternate route selection), and dynamically adjusts the robot's occupancy grid via obstacle buffering to enforce semantic constraints. We demonstrate multi-step reasoning for sequential tasks, such as first navigating to a resource goal and then reaching a final destination safely. Experiments on a Petoi Bittle robot with an overhead camera and Raspberry Pi Zero 2W compare classical A* against GPT-4-assisted planning. Results show that while A* is faster and more accurate for basic route generation and obstacle avoidance, the GPT-4-integrated system achieves high success rates (96-100%) on semantic tasks that are infeasible for pure geometric planners. This work highlights how affordable robots can exhibit intelligent, context-aware behaviors by leveraging large language model reasoning with minimal hardware and no fine-tuning.","authors":["Jesse Barkley","Abraham George","Amir Barati Farimani"],"url":"https://arxiv.org/abs/2505.01931"}
{"created":"2025-05-06","title":"OT-Talk: Animating 3D Talking Head with Optimal Transportation","abstract":"Animating 3D head meshes using audio inputs has significant applications in AR/VR, gaming, and entertainment through 3D avatars. However, bridging the modality gap between speech signals and facial dynamics remains a challenge, often resulting in incorrect lip syncing and unnatural facial movements. To address this, we propose OT-Talk, the first approach to leverage optimal transportation to optimize the learning model in talking head animation. Building on existing learning frameworks, we utilize a pre-trained Hubert model to extract audio features and a transformer model to process temporal sequences. Unlike previous methods that focus solely on vertex coordinates or displacements, we introduce Chebyshev Graph Convolution to extract geometric features from triangulated meshes. To measure mesh dissimilarities, we go beyond traditional mesh reconstruction errors and velocity differences between adjacent frames. Instead, we represent meshes as probability measures and approximate their surfaces. This allows us to leverage the sliced Wasserstein distance for modeling mesh variations. This approach facilitates the learning of smooth and accurate facial motions, resulting in coherent and natural facial animations. Our experiments on two public audio-mesh datasets demonstrate that our method outperforms state-of-the-art techniques both quantitatively and qualitatively in terms of mesh reconstruction accuracy and temporal alignment. In addition, we conducted a user perception study with 20 volunteers to further assess the effectiveness of our approach.","authors":["Xinmu Wang","Xiang Gao","Xiyun Song","Heather Yu","Zongfang Lin","Liang Peng","Xianfeng Gu"],"url":"https://arxiv.org/abs/2505.01932"}
{"created":"2025-05-06","title":"Unemployment Dynamics Forecasting with Machine Learning Regression Models","abstract":"In this paper, I explored how a range of regression and machine learning techniques can be applied to monthly U.S. unemployment data to produce timely forecasts. I compared seven models: Linear Regression, SGDRegressor, Random Forest, XGBoost, CatBoost, Support Vector Regression, and an LSTM network, training each on a historical span of data and then evaluating on a later hold-out period. Input features include macro indicators (GDP growth, CPI), labor market measures (job openings, initial claims), financial variables (interest rates, equity indices), and consumer sentiment.","authors":["Kyungsu Kim"],"url":"https://arxiv.org/abs/2505.01933"}
{"created":"2025-05-06","title":"GauS-SLAM: Dense RGB-D SLAM with Gaussian Surfels","abstract":"We propose GauS-SLAM, a dense RGB-D SLAM system that leverages 2D Gaussian surfels to achieve robust tracking and high-fidelity mapping. Our investigations reveal that Gaussian-based scene representations exhibit geometry distortion under novel viewpoints, which significantly degrades the accuracy of Gaussian-based tracking methods. These geometry inconsistencies arise primarily from the depth modeling of Gaussian primitives and the mutual interference between surfaces during the depth blending. To address these, we propose a 2D Gaussian-based incremental reconstruction strategy coupled with a Surface-aware Depth Rendering mechanism, which significantly enhances geometry accuracy and multi-view consistency. Additionally, the proposed local map design dynamically isolates visible surfaces during tracking, mitigating misalignment caused by occluded regions in global maps while maintaining computational efficiency with increasing Gaussian density. Extensive experiments across multiple datasets demonstrate that GauS-SLAM outperforms comparable methods, delivering superior tracking precision and rendering fidelity. The project page will be made available at https://gaus-slam.github.io.","authors":["Yongxin Su","Lin Chen","Kaiting Zhang","Zhongliang Zhao","Chenfeng Hou","Ziping Yu"],"url":"https://arxiv.org/abs/2505.01934"}
{"created":"2025-05-06","title":"Faster logconcave sampling from a cold start in high dimension","abstract":"We present a faster algorithm to generate a warm start for sampling an arbitrary logconcave density specified by an evaluation oracle, leading to the first sub-cubic sampling algorithms for inputs in (near-)isotropic position. A long line of prior work incurred a warm-start penalty of at least linear in the dimension, hitting a cubic barrier, even for the special case of uniform sampling from convex bodies.","authors":["Yunbum Kook","Santosh S. Vempala"],"url":"https://arxiv.org/abs/2505.01937"}
{"created":"2025-05-06","title":"HybridGS: High-Efficiency Gaussian Splatting Data Compression using Dual-Channel Sparse Representation and Point Cloud Encoder","abstract":"Most existing 3D Gaussian Splatting (3DGS) compression schemes focus on producing compact 3DGS representation via implicit data embedding. They have long coding times and highly customized data format, making it difficult for widespread deployment. This paper presents a new 3DGS compression framework called HybridGS, which takes advantage of both compact generation and standardized point cloud data encoding. HybridGS first generates compact and explicit 3DGS data. A dual-channel sparse representation is introduced to supervise the primitive position and feature bit depth. It then utilizes a canonical point cloud encoder to perform further data compression and form standard output bitstreams. A simple and effective rate control scheme is proposed to pivot the interpretable data compression scheme. At the current stage, HybridGS does not include any modules aimed at improving 3DGS quality during generation. But experiment results show that it still provides comparable reconstruction performance against state-of-the-art methods, with evidently higher encoding and decoding speed. The code is publicly available at https://github.com/Qi-Yangsjtu/HybridGS.","authors":["Qi Yang","Le Yang","Geert Van Der Auwera","Zhu Li"],"url":"https://arxiv.org/abs/2505.01938"}
{"created":"2025-05-06","title":"One Documentation Does Not Fit All: Case Study of TensorFlow Documentation","abstract":"Software documentation guides the proper use of tools or services. With the rapid growth of machine learning libraries, individuals from various fields are incorporating machine learning into their workflows through programming. However, many of these users lack software engineering experience, affecting the usability of the documentation. Traditionally, software developers have created documentation primarily for their peers, making it challenging for others to interpret and effectively use these resources. Moreover, no study has specifically focused on machine learning software documentation or analyzing the backgrounds of developers who rely on such documentation, highlighting a critical gap in understanding how to make these resources more accessible. This study examined customization trends in TensorFlow tutorials and compared these artifacts to analyze content and design differences. We also analyzed Stack Overflow questions related to TensorFlow documentation to understand the types of questions and the backgrounds of the developers asking them. Further, we developed two taxonomies based on the nature and triggers of the questions for machine learning software. Our findings showed no significant differences in the content or the nature of the questions across different tutorials. Our results show that 24.9% of the questions concern errors and exceptions, while 64.3% relate to inadequate and non-generalizable examples in the documentation. Despite efforts to create customized documentation, our analysis indicates that current TensorFlow documentation does not effectively support its target users.","authors":["Sharuka Promodya Thirimanne","Elim Yoseph Lemango","Giulio Antoniol","Maleknaz Nayebi"],"url":"https://arxiv.org/abs/2505.01939"}
{"created":"2025-05-06","title":"UK Finfluencers: Exploring Content, Reach, and Responsibility","abstract":"The rise of social media financial influencers (finfluencers) has significantly transformed the personal finance landscape, making financial advice and insights more accessible to a broader and younger audience. By leveraging digital platforms, these influencers have contributed to the democratization of financial literacy. However, the line between education and promotion is often blurred, as many finfluencers lack formal financial qualifications, raising concerns about the accuracy and reliability of the information they share. This study investigates the patterns and behaviours of finfluencers in the UK on TikTok, focusing not on individual actions but on broader trends and the interactions between influencers and their followers. The aim is to identify common engagement patterns and propose guidelines that can help protect the public from potential financial harm. Specifically, the paper contributes a detailed analysis of finfluencer content categorization, sentiment trends, and the prevalence and role of disclaimers, offering empirical insights that inform recommendations for safer and more transparent financial communication on social media.","authors":["Essam Ghadafi","Panagiotis Andriotis"],"url":"https://arxiv.org/abs/2505.01941"}
{"created":"2025-05-06","title":"Explainability by design: an experimental analysis of the legal coding process","abstract":"Behind a set of rules in Deontic Defeasible Logic, there is a mapping process of normative background fragments. This process goes from text to rules and implicitly encompasses an explanation of the coded fragments.","authors":["Matteo Cristani","Guido Governatori","Francesco Olivieri","Monica Palmirani","Gabriele Buriola"],"url":"https://arxiv.org/abs/2505.01944"}
{"created":"2025-05-06","title":"Act Natural! Extending Naturalistic Projection to Multimodal Behavior Scenarios","abstract":"Autonomous agents operating in public spaces must consider how their behaviors might affect the humans around them, even when not directly interacting with them. To this end, it is often beneficial to be predictable and appear naturalistic. Existing methods for this purpose use human actor intent modeling or imitation learning techniques, but these approaches rarely capture all possible motivations for human behavior and/or require significant amounts of data. Our work extends a technique for modeling unimodal naturalistic behaviors with an explicit convex set representation, to account for multimodal behavior by using multiple convex sets. This more flexible representation provides a higher degree of fidelity in data-driven modeling of naturalistic behavior that arises in real-world scenarios in which human behavior is, in some sense, discrete, e.g. whether or not to yield at a roundabout. Equipped with this new set representation, we develop an optimization-based filter to project arbitrary trajectories into the set so that they appear naturalistic to humans in the scene, while also satisfying vehicle dynamics, actuator limits, etc. We demonstrate our methods on real-world human driving data from the inD (intersection) and rounD (roundabout) datasets.","authors":["Hamzah I. Khan","David Fridovich-Keil"],"url":"https://arxiv.org/abs/2505.01945"}
{"created":"2025-05-06","title":"Embedding based retrieval for long tail search queries in ecommerce","abstract":"In this abstract we present a series of optimizations we performed on the two-tower model architecture [14], training and evaluation datasets to implement semantic product search at Best Buy. Search queries on bestbuy.com follow the pareto distribution whereby a minority of them account for most searches. This leaves us with a long tail of search queries that have low frequency of issuance. The queries in the long tail suffer from very spare interaction signals. Our current work focuses on building a model to serve the long tail queries. We present a series of optimizations we have done to this model to maximize conversion for the purpose of retrieval from the catalog. The first optimization we present is using a large language model to improve the sparsity of conversion signals. The second optimization is pretraining an off-the-shelf transformer-based model on the Best Buy catalog data. The third optimization we present is on the finetuning front. We use query-to-query pairs in addition to query-to-product pairs and combining the above strategies for finetuning the model. We also demonstrate how merging the weights of these finetuned models improves the evaluation metrics. Finally, we provide a recipe for curating an evaluation dataset for continuous monitoring of model performance with human-in-the-loop evaluation. We found that adding this recall mechanism to our current term match-based recall improved conversion by 3% in an online A/B test.","authors":["Akshay Kekuda","Yuyang Zhang","Arun Udayashankar"],"url":"https://arxiv.org/abs/2505.01946"}
{"created":"2025-05-06","title":"Runtime Anomaly Detection for Drones: An Integrated Rule-Mining and Unsupervised-Learning Approach","abstract":"UAVs, commonly referred to as drones, have witnessed a remarkable surge in popularity due to their versatile applications. These cyber-physical systems depend on multiple sensor inputs, such as cameras, GPS receivers, accelerometers, and gyroscopes, with faults potentially leading to physical instability and serious safety concerns. To mitigate such risks, anomaly detection has emerged as a crucial safeguarding mechanism, capable of identifying the physical manifestations of emerging issues and allowing operators to take preemptive action at runtime. Recent anomaly detection methods based on LSTM neural networks have shown promising results, but three challenges persist: the need for models that can generalise across the diverse mission profiles of drones; the need for interpretability, enabling operators to understand the nature of detected problems; and the need for capturing domain knowledge that is difficult to infer solely from log data. Motivated by these challenges, this paper introduces RADD, an integrated approach to anomaly detection in drones that combines rule mining and unsupervised learning. In particular, we leverage rules (or invariants) to capture expected relationships between sensors and actuators during missions, and utilise unsupervised learning techniques to cover more subtle relationships that the rules may have missed. We implement this approach using the ArduPilot drone software in the Gazebo simulator, utilising 44 rules derived across the main phases of drone missions, in conjunction with an ensemble of five unsupervised learning models. We find that our integrated approach successfully detects 93.84% of anomalies over six types of faults with a low false positive rate (2.33%), and can be deployed effectively at runtime. Furthermore, RADD outperforms a state-of-the-art LSTM-based method in detecting the different types of faults evaluated in our study.","authors":["Ivan Tan","Wei Minn","Christopher M. Poskitt","Lwin Khin Shar","Lingxiao Jiang"],"url":"https://arxiv.org/abs/2505.01947"}
{"created":"2025-05-06","title":"Multi-Scale Graph Learning for Anti-Sparse Downscaling","abstract":"Water temperature can vary substantially even across short distances within the same sub-watershed. Accurate prediction of stream water temperature at fine spatial resolutions (i.e., fine scales, $\\leq$ 1 km) enables precise interventions to maintain water quality and protect aquatic habitats. Although spatiotemporal models have made substantial progress in spatially coarse time series modeling, challenges persist in predicting at fine spatial scales due to the lack of data at that scale.To address the problem of insufficient fine-scale data, we propose a Multi-Scale Graph Learning (MSGL) method. This method employs a multi-task learning framework where coarse-scale graph learning, bolstered by larger datasets, simultaneously enhances fine-scale graph learning. Although existing multi-scale or multi-resolution methods integrate data from different spatial scales, they often overlook the spatial correspondences across graph structures at various scales. To address this, our MSGL introduces an additional learning task, cross-scale interpolation learning, which leverages the hydrological connectedness of stream locations across coarse- and fine-scale graphs to establish cross-scale connections, thereby enhancing overall model performance. Furthermore, we have broken free from the mindset that multi-scale learning is limited to synchronous training by proposing an Asynchronous Multi-Scale Graph Learning method (ASYNC-MSGL). Extensive experiments demonstrate the state-of-the-art performance of our method for anti-sparse downscaling of daily stream temperatures in the Delaware River Basin, USA, highlighting its potential utility for water resources monitoring and management.","authors":["Yingda Fan","Runlong Yu","Janet R. Barclay","Alison P. Appling","Yiming Sun","Yiqun Xie","Xiaowei Jia"],"url":"https://arxiv.org/abs/2505.01948"}
{"created":"2025-05-06","title":"Segment Any RGB-Thermal Model with Language-aided Distillation","abstract":"The recent Segment Anything Model (SAM) demonstrates strong instance segmentation performance across various downstream tasks. However, SAM is trained solely on RGB data, limiting its direct applicability to RGB-thermal (RGB-T) semantic segmentation. Given that RGB-T provides a robust solution for scene understanding in adverse weather and lighting conditions, such as low light and overexposure, we propose a novel framework, SARTM, which customizes the powerful SAM for RGB-T semantic segmentation. Our key idea is to unleash the potential of SAM while introduce semantic understanding modules for RGB-T data pairs. Specifically, our framework first involves fine tuning the original SAM by adding extra LoRA layers, aiming at preserving SAM's strong generalization and segmentation capabilities for downstream tasks. Secondly, we introduce language information as guidance for training our SARTM. To address cross-modal inconsistencies, we introduce a Cross-Modal Knowledge Distillation(CMKD) module that effectively achieves modality adaptation while maintaining its generalization capabilities. This semantic module enables the minimization of modality gaps and alleviates semantic ambiguity, facilitating the combination of any modality under any visual conditions. Furthermore, we enhance the segmentation performance by adjusting the segmentation head of SAM and incorporating an auxiliary semantic segmentation head, which integrates multi-scale features for effective fusion. Extensive experiments are conducted across three multi-modal RGBT semantic segmentation benchmarks: MFNET, PST900, and FMB. Both quantitative and qualitative results consistently demonstrate that the proposed SARTM significantly outperforms state-of-the-art approaches across a variety of conditions.","authors":["Dong Xing","Xianxun Zhu","Wei Zhou","Qika Lin","Hang Yang","Yuqing Wang"],"url":"https://arxiv.org/abs/2505.01950"}
{"created":"2025-05-06","title":"Training Environment for High Performance Reinforcement Learning","abstract":"This paper presents Tunnel, a simple, open source, reinforcement learning training environment for high performance aircraft. It integrates the F16 3D nonlinear flight dynamics into OpenAI Gymnasium python package. The template includes primitives for boundaries, targets, adversaries and sensing capabilities that may vary depending on operational need. This offers mission planners a means to rapidly respond to evolving environments, sensor capabilities and adversaries for autonomous air combat aircraft. It offers researchers access to operationally relevant aircraft physics. Tunnel code base is accessible to anyone familiar with Gymnasium and/or those with basic python skills. This paper includes a demonstration of a week long trade study that investigated a variety of training methods, observation spaces, and threat presentations. This enables increased collaboration between researchers and mission planners which can translate to a national military advantage. As warfare becomes increasingly reliant upon automation, software agility will correlate with decision advantages. Airmen must have tools to adapt to adversaries in this context. It may take months for researchers to develop skills to customize observation, actions, tasks and training methodologies in air combat simulators. In Tunnel, this can be done in a matter of days.","authors":["Greg Search"],"url":"https://arxiv.org/abs/2505.01953"}
{"created":"2025-05-06","title":"Semantic Probabilistic Control of Language Models","abstract":"Semantic control entails steering LM generations towards satisfying subtle non-lexical constraints, e.g., toxicity, sentiment, or politeness, attributes that can be captured by a sequence-level verifier. It can thus be viewed as sampling from the LM distribution conditioned on the target attribute, a computationally intractable problem due to the non-decomposable nature of the verifier. Existing approaches to LM control either only deal with syntactic constraints which cannot capture the aforementioned attributes, or rely on sampling to explore the conditional LM distribution, an ineffective estimator for low-probability events. In this work, we leverage a verifier's gradient information to efficiently reason over all generations that satisfy the target attribute, enabling precise steering of LM generations by reweighing the next-token distribution. Starting from an initial sample, we create a local LM distribution favoring semantically similar sentences. This approximation enables the tractable computation of an expected sentence embedding. We use this expected embedding, informed by the verifier's evaluation at the initial sample, to estimate the probability of satisfying the constraint, which directly informs the update to the next-token distribution. We evaluated the effectiveness of our approach in controlling the toxicity, sentiment, and topic-adherence of LMs yielding generations satisfying the constraint with high probability (>95%) without degrading their quality.","authors":["Kareem Ahmed","Catarina G Belem","Padhraic Smyth","Sameer Singh"],"url":"https://arxiv.org/abs/2505.01954"}
{"created":"2025-05-06","title":"Generative AI in clinical practice: novel qualitative evidence of risk and responsible use of Google's NotebookLM","abstract":"The advent of generative artificial intelligence, especially large language models (LLMs), presents opportunities for innovation in research, clinical practice, and education. Recently, Dihan et al. lauded LLM tool NotebookLM's potential, including for generating AI-voiced podcasts to educate patients about treatment and rehabilitation, and for quickly synthesizing medical literature for professionals. We argue that NotebookLM presently poses clinical and technological risks that should be tested and considered prior to its implementation in clinical practice.","authors":["Max Reuter","Maura Philippone","Bond Benton","Laura Dilley"],"url":"https://arxiv.org/abs/2505.01955"}
{"created":"2025-05-06","title":"SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment","abstract":"In battlefield environments, adversaries frequently disrupt GPS signals, requiring alternative localization and navigation methods. Traditional vision-based approaches like Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO) involve complex sensor fusion and high computational demand, whereas range-free methods like DV-HOP face accuracy and stability challenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a navigation approach using landmark-based localization (LanBLoc) combined with a battlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its performance is benchmarked against three state-of-the-art visual localization algorithms integrated with BMM and Bayesian filters, evaluated on synthetic and real-imitated trajectory datasets using metrics including Average Displacement Error (ADE), Final Displacement Error (FDE), and a newly introduced Average Weighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior performance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two safe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by integrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm for obstacle avoidance and risk exposure minimization. Simulation results in battlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk exposure, and trajectory efficiency, while SafeNav-CHull provides superior computational speed.","authors":["Ganesh Sapkota","Sanjay Madria"],"url":"https://arxiv.org/abs/2505.01956"}
{"created":"2025-05-06","title":"A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models","abstract":"Large Vision-Language Models (LVLMs) demonstrate remarkable capabilities in multimodal tasks, but visual object hallucination remains a persistent issue. It refers to scenarios where models generate inaccurate visual object-related information based on the query input, potentially leading to misinformation and concerns about safety and reliability. Previous works focus on the evaluation and mitigation of visual hallucinations, but the underlying causes have not been comprehensively investigated. In this paper, we analyze each component of LLaVA-like LVLMs -- the large language model, the vision backbone, and the projector -- to identify potential sources of error and their impact. Based on our observations, we propose methods to mitigate hallucination for each problematic component. Additionally, we developed two hallucination benchmarks: QA-VisualGenome, which emphasizes attribute and relation hallucinations, and QA-FB15k, which focuses on cognition-based hallucinations.","authors":["Liqiang Jing","Guiming Hardy Chen","Ehsan Aghazadeh","Xin Eric Wang","Xinya Du"],"url":"https://arxiv.org/abs/2505.01958"}
{"created":"2025-05-06","title":"EnsembleCI: Ensemble Learning for Carbon Intensity Forecasting","abstract":"Carbon intensity (CI) measures the average carbon emissions generated per unit of electricity, making it a crucial metric for quantifying and managing the environmental impact. Accurate CI predictions are vital for minimizing carbon footprints, yet the state-of-the-art method (CarbonCast) falls short due to its inability to address regional variability and lack of adaptability. To address these limitations, we introduce EnsembleCI, an adaptive, end-to-end ensemble learning-based approach for CI forecasting. EnsembleCI combines weighted predictions from multiple sublearners, offering enhanced flexibility and regional adaptability. In evaluations across 11 regional grids, EnsembleCI consistently surpasses CarbonCast, achieving the lowest mean absolute percentage error (MAPE) in almost all grids and improving prediction accuracy by an average of 19.58%. While performance still varies across grids due to inherent regional diversity, EnsembleCI reduces variability and exhibits greater robustness in long-term forecasting compared to CarbonCast and identifies region-specific key features, underscoring its interpretability and practical relevance. These findings position EnsembleCI as a more accurate and reliable solution for CI forecasting. EnsembleCI source code and data used in this paper are available at https://github.com/emmayly/EnsembleCI.","authors":["Leyi Yan","Linda Wang","Sihang Liu","Yi Ding"],"url":"https://arxiv.org/abs/2505.01959"}
{"created":"2025-05-06","title":"A Goal-Oriented Reinforcement Learning-Based Path Planning Algorithm for Modular Self-Reconfigurable Satellites","abstract":"Modular self-reconfigurable satellites refer to satellite clusters composed of individual modular units capable of altering their configurations. The configuration changes enable the execution of diverse tasks and mission objectives. Existing path planning algorithms for reconfiguration often suffer from high computational complexity, poor generalization capability, and limited support for diverse target configurations. To address these challenges, this paper proposes a goal-oriented reinforcement learning-based path planning algorithm. This algorithm is the first to address the challenge that previous reinforcement learning methods failed to overcome, namely handling multiple target configurations. Moreover, techniques such as Hindsight Experience Replay and Invalid Action Masking are incorporated to overcome the significant obstacles posed by sparse rewards and invalid actions. Based on these designs, our model achieves a 95% and 73% success rate in reaching arbitrary target configurations in a modular satellite cluster composed of four and six units, respectively.","authors":["Bofei Liu","Dong Ye","Zunhao Yao","Zhaowei Sun"],"url":"https://arxiv.org/abs/2505.01966"}
{"created":"2025-05-06","title":"Analyzing Cognitive Differences Among Large Language Models through the Lens of Social Worldview","abstract":"Large Language Models (LLMs) have become integral to daily life, widely adopted in communication, decision-making, and information retrieval, raising critical questions about how these systems implicitly form and express socio-cognitive attitudes or \"worldviews\". While existing research extensively addresses demographic and ethical biases, broader dimensions-such as attitudes toward authority, equality, autonomy, and fate-remain under-explored. In this paper, we introduce the Social Worldview Taxonomy (SWT), a structured framework grounded in Cultural Theory, operationalizing four canonical worldviews (Hierarchy, Egalitarianism, Individualism, Fatalism) into measurable sub-dimensions. Using SWT, we empirically identify distinct and interpretable cognitive profiles across 28 diverse LLMs. Further, inspired by Social Referencing Theory, we experimentally demonstrate that explicit social cues systematically shape these cognitive attitudes, revealing both general response patterns and nuanced model-specific variations. Our findings enhance the interpretability of LLMs by revealing implicit socio-cognitive biases and their responsiveness to social feedback, thus guiding the development of more transparent and socially responsible language technologies.","authors":["Jiatao Li","Yanheng Li","Xiaojun Wan"],"url":"https://arxiv.org/abs/2505.01967"}
{"created":"2025-05-06","title":"HAS-GPU: Efficient Hybrid Auto-scaling with Fine-grained GPU Allocation for SLO-aware Serverless Inferences","abstract":"Serverless Computing (FaaS) has become a popular paradigm for deep learning inference due to the ease of deployment and pay-per-use benefits. However, current serverless inference platforms encounter the coarse-grained and static GPU resource allocation problems during scaling, which leads to high costs and Service Level Objective (SLO) violations in fluctuating workloads. Meanwhile, current platforms only support horizontal scaling for GPU inferences, thus the cold start problem further exacerbates the problems. In this paper, we propose HAS-GPU, an efficient Hybrid Auto-scaling Serverless architecture with fine-grained GPU allocation for deep learning inferences. HAS-GPU proposes an agile scheduler capable of allocating GPU Streaming Multiprocessor (SM) partitions and time quotas with arbitrary granularity and enables significant vertical quota scalability at runtime. To resolve performance uncertainty introduced by massive fine-grained resource configuration spaces, we propose the Resource-aware Performance Predictor (RaPP). Furthermore, we present an adaptive hybrid auto-scaling algorithm with both horizontal and vertical scaling to ensure inference SLOs and minimize GPU costs. The experiments demonstrated that compared to the mainstream serverless inference platform, HAS-GPU reduces function costs by an average of 10.8x with better SLO guarantees. Compared to state-of-the-art spatio-temporal GPU sharing serverless framework, HAS-GPU reduces function SLO violation by 4.8x and cost by 1.72x on average.","authors":["Jianfeng Gu","Puxuan Wang","Isaac Nunezand","Kai Huang","Michael Gerndt"],"url":"https://arxiv.org/abs/2505.01968"}
{"created":"2025-05-06","title":"MC3D-AD: A Unified Geometry-aware Reconstruction Model for Multi-category 3D Anomaly Detection","abstract":"3D Anomaly Detection (AD) is a promising means of controlling the quality of manufactured products. However, existing methods typically require carefully training a task-specific model for each category independently, leading to high cost, low efficiency, and weak generalization. Therefore, this paper presents a novel unified model for Multi-Category 3D Anomaly Detection (MC3D-AD) that aims to utilize both local and global geometry-aware information to reconstruct normal representations of all categories. First, to learn robust and generalized features of different categories, we propose an adaptive geometry-aware masked attention module that extracts geometry variation information to guide mask attention. Then, we introduce a local geometry-aware encoder reinforced by the improved mask attention to encode group-level feature tokens. Finally, we design a global query decoder that utilizes point cloud position embeddings to improve the decoding process and reconstruction ability. This leads to local and global geometry-aware reconstructed feature tokens for the AD task. MC3D-AD is evaluated on two publicly available Real3D-AD and Anomaly-ShapeNet datasets, and exhibits significant superiority over current state-of-the-art single-category methods, achieving 3.1\\% and 9.3\\% improvement in object-level AUROC over Real3D-AD and Anomaly-ShapeNet, respectively. The source code will be released upon acceptance.","authors":["Jiayi Cheng","Can Gao","Jie Zhou","Jiajun Wen","Tao Dai","Jinbao Wang"],"url":"https://arxiv.org/abs/2505.01969"}
{"created":"2025-05-06","title":"Visual Dominance and Emerging Multimodal Approaches in Distracted Driving Detection: A Review of Machine Learning Techniques","abstract":"Distracted driving continues to be a significant cause of road traffic injuries and fatalities worldwide, even with advancements in driver monitoring technologies. Recent developments in machine learning (ML) and deep learning (DL) have primarily focused on visual data to detect distraction, often neglecting the complex, multimodal nature of driver behavior. This systematic review assesses 74 peer-reviewed studies from 2019 to 2024 that utilize ML/DL techniques for distracted driving detection across visual, sensor-based, multimodal, and emerging modalities. The review highlights a significant prevalence of visual-only models, particularly convolutional neural networks (CNNs) and temporal architectures, which achieve high accuracy but show limited generalizability in real-world scenarios. Sensor-based and physiological models provide complementary strengths by capturing internal states and vehicle dynamics, while emerging techniques, such as auditory sensing and radio frequency (RF) methods, offer privacy-aware alternatives. Multimodal architecture consistently surpasses unimodal baselines, demonstrating enhanced robustness, context awareness, and scalability by integrating diverse data streams. These findings emphasize the need to move beyond visual-only approaches and adopt multimodal systems that combine visual, physiological, and vehicular cues while keeping in checking the need to balance computational requirements. Future research should focus on developing lightweight, deployable multimodal frameworks, incorporating personalized baselines, and establishing cross-modality benchmarks to ensure real-world reliability in advanced driver assistance systems (ADAS) and road safety interventions.","authors":["Anthony Dontoh","Stephanie Ivey","Logan Sirbaugh","Andrews Danyo","Armstrong Aboah"],"url":"https://arxiv.org/abs/2505.01973"}
{"created":"2025-05-06","title":"KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation","abstract":"Collecting demonstrations enriched with fine-grained tactile information is critical for dexterous manipulation, particularly in contact-rich tasks that require precise force control and physical interaction. While prior works primarily focus on teleoperation or video-based retargeting, they often suffer from kinematic mismatches and the absence of real-time tactile feedback, hindering the acquisition of high-fidelity tactile data. To mitigate this issue, we propose KineDex, a hand-over-hand kinesthetic teaching paradigm in which the operator's motion is directly transferred to the dexterous hand, enabling the collection of physically grounded demonstrations enriched with accurate tactile feedback. To resolve occlusions from human hand, we apply inpainting technique to preprocess the visual observations. Based on these demonstrations, we then train a visuomotor policy using tactile-augmented inputs and implement force control during deployment for precise contact-rich manipulation. We evaluate KineDex on a suite of challenging contact-rich manipulation tasks, including particularly difficult scenarios such as squeezing toothpaste onto a toothbrush, which require precise multi-finger coordination and stable force regulation. Across these tasks, KineDex achieves an average success rate of 74.4%, representing a 57.7% improvement over the variant without force control. Comparative experiments with teleoperation and user studies further validate the advantages of KineDex in data collection efficiency and operability. Specifically, KineDex collects data over twice as fast as teleoperation across two tasks of varying difficulty, while maintaining a near-100% success rate, compared to under 50% for teleoperation.","authors":["Di Zhang","Chengbo Yuan","Chuan Wen","Hai Zhang","Junqiao Zhao","Yang Gao"],"url":"https://arxiv.org/abs/2505.01974"}
{"created":"2025-05-06","title":"A Survey on Privacy Risks and Protection in Large Language Models","abstract":"Although Large Language Models (LLMs) have become increasingly integral to diverse applications, their capabilities raise significant privacy concerns. This survey offers a comprehensive overview of privacy risks associated with LLMs and examines current solutions to mitigate these challenges. First, we analyze privacy leakage and attacks in LLMs, focusing on how these models unintentionally expose sensitive information through techniques such as model inversion, training data extraction, and membership inference. We investigate the mechanisms of privacy leakage, including the unauthorized extraction of training data and the potential exploitation of these vulnerabilities by malicious actors. Next, we review existing privacy protection against such risks, such as inference detection, federated learning, backdoor mitigation, and confidential computing, and assess their effectiveness in preventing privacy leakage. Furthermore, we highlight key practical challenges and propose future research directions to develop secure and privacy-preserving LLMs, emphasizing privacy risk assessment, secure knowledge transfer between models, and interdisciplinary frameworks for privacy governance. Ultimately, this survey aims to establish a roadmap for addressing escalating privacy challenges in the LLMs domain.","authors":["Kang Chen","Xiuze Zhou","Yuanguo Lin","Shibo Feng","Li Shen","Pengcheng Wu"],"url":"https://arxiv.org/abs/2505.01976"}
{"created":"2025-05-06","title":"D3HRL: A Distributed Hierarchical Reinforcement Learning Approach Based on Causal Discovery and Spurious Correlation Detection","abstract":"Current Hierarchical Reinforcement Learning (HRL) algorithms excel in long-horizon sequential decision-making tasks but still face two challenges: delay effects and spurious correlations. To address them, we propose a causal HRL approach called D3HRL. First, D3HRL models delayed effects as causal relationships across different time spans and employs distributed causal discovery to learn these relationships. Second, it employs conditional independence testing to eliminate spurious correlations. Finally, D3HRL constructs and trains hierarchical policies based on the identified true causal relationships. These three steps are iteratively executed, gradually exploring the complete causal chain of the task. Experiments conducted in 2D-MineCraft and MiniGrid show that D3HRL demonstrates superior sensitivity to delay effects and accurately identifies causal relationships, leading to reliable decision-making in complex environments.","authors":["Chenran Zhao","Dianxi Shi","Mengzhu Wang","Jianqiang Xia","Huanhuan Yang","Songchang Jin","Shaowu Yang","Chunping Qiu"],"url":"https://arxiv.org/abs/2505.01979"}
{"created":"2025-05-06","title":"LLM-based Text Simplification and its Effect on User Comprehension and Cognitive Load","abstract":"Information on the web, such as scientific publications and Wikipedia, often surpasses users' reading level. To help address this, we used a self-refinement approach to develop a LLM capability for minimally lossy text simplification. To validate our approach, we conducted a randomized study involving 4563 participants and 31 texts spanning 6 broad subject areas: PubMed (biomedical scientific articles), biology, law, finance, literature/philosophy, and aerospace/computer science. Participants were randomized to viewing original or simplified texts in a subject area, and answered multiple-choice questions (MCQs) that tested their comprehension of the text. The participants were also asked to provide qualitative feedback such as task difficulty. Our results indicate that participants who read the simplified text answered more MCQs correctly than their counterparts who read the original text (3.9% absolute increase, p<0.05). This gain was most striking with PubMed (14.6%), while more moderate gains were observed for finance (5.5%), aerospace/computer science (3.8%) domains, and legal (3.5%). Notably, the results were robust to whether participants could refer back to the text while answering MCQs. The absolute accuracy decreased by up to ~9% for both original and simplified setups where participants could not refer back to the text, but the ~4% overall improvement persisted. Finally, participants' self-reported perceived ease based on a simplified NASA Task Load Index was greater for those who read the simplified text (absolute change on a 5-point scale 0.33, p<0.05). This randomized study, involving an order of magnitude more participants than prior works, demonstrates the potential of LLMs to make complex information easier to understand. Our work aims to enable a broader audience to better learn and make use of expert knowledge available on the web, improving information accessibility.","authors":["Theo Guidroz (Yifan)","Diego Ardila (Yifan)","Jimmy Li (Yifan)","Adam Mansour (Yifan)","Paul Jhun (Yifan)","Nina Gonzalez (Yifan)","Xiang Ji (Yifan)","Mike Sanchez (Yifan)","Sujay Kakarmath (Yifan)","Mathias MJ Bellaiche (Yifan)","Miguel \\'Angel Garrido (Yifan)","Faruk Ahmed (Yifan)","Divyansh Choudhary (Yifan)","Jay Hartford (Yifan)","Chenwei Xu (Yifan)","Henry Javier Serrano Echeverria (Yifan)","Yifan Wang (Yifan)","Jeff Shaffer (Yifan)","Eric (Yifan)","Cao","Yossi Matias","Avinatan Hassidim","Dale R Webster","Yun Liu","Sho Fujiwara","Peggy Bui","Quang Duong"],"url":"https://arxiv.org/abs/2505.01980"}
{"created":"2025-05-06","title":"Lifelong Whole Slide Image Analysis: Online Vision-Language Adaptation and Past-to-Present Gradient Distillation","abstract":"Whole Slide Images (WSIs) play a crucial role in accurate cancer diagnosis and prognosis, as they provide tissue details at the cellular level. However, the rapid growth of computational tasks involving WSIs poses significant challenges. Given that WSIs are gigapixels in size, they present difficulties in terms of storage, processing, and model training. Therefore, it is essential to develop lifelong learning approaches for WSI analysis. In scenarios where slides are distributed across multiple institutes, we aim to leverage them to develop a unified online model as a computational tool for cancer diagnosis in clinical and hospital settings. In this study, we introduce ADaFGrad, a method designed to enhance lifelong learning for whole-slide image (WSI) analysis. First, we leverage pathology vision-language foundation models to develop a framework that enables interaction between a slide's regional tissue features and a predefined text-based prototype buffer. Additionally, we propose a gradient-distillation mechanism that mimics the gradient of a logit with respect to the classification-head parameters across past and current iterations in a continual-learning setting. We construct a sequence of six TCGA datasets for training and evaluation. Experimental results show that ADaFGrad outperforms both state-of-the-art WSI-specific and conventional continual-learning methods after only a few training epochs, exceeding them by up to +5.068% in the class-incremental learning scenario while exhibiting the least forgetting (i.e., retaining the most knowledge from previous tasks). Moreover, ADaFGrad surpasses its baseline by as much as +40.084% in accuracy, further demonstrating the effectiveness of the proposed modules.","authors":["Doanh C. Bui","Hoai Luan Pham","Vu Trung Duong Le","Tuan Hai Vu","Van Duy Tran","Khang Nguyen","Yasuhiko Nakashima"],"url":"https://arxiv.org/abs/2505.01984"}
{"created":"2025-05-06","title":"Drug classification based on X-ray spectroscopy combined with machine learning","abstract":"The proliferation of new types of drugs necessitates the urgent development of faster and more accurate detection methods. Traditional detection methods have high requirements for instruments and environments, making the operation complex. X-ray absorption spectroscopy, a non-destructive detection technique, offers advantages such as ease of operation, penetrative observation, and strong substance differentiation capabilities, making it well-suited for application in the field of drug detection and identification. In this study, we constructed a classification model using Convolutional Neural Networks (CNN), Support Vector Machines (SVM), and Particle Swarm Optimization (PSO) to classify and identify drugs based on their X-ray spectral profiles. In the experiments, we selected 14 chemical reagents with chemical formulas similar to drugs as samples. We utilized CNN to extract features from the spectral data of these 14 chemical reagents and used the extracted features to train an SVM model. We also utilized PSO to optimize two critical initial parameters of the SVM. The experimental results demonstrate that this model achieved higher classification accuracy compared to two other common methods, with a prediction accuracy of 99.14%. Additionally, the model exhibited fast execution speed, mitigating the drawback of a drastic increase in running time and efficiency reduction that may result from the direct fusion of PSO and SVM. Therefore, the combined approach of X-ray absorption spectroscopy with CNN, PSO, and SVM provides a rapid, highly accurate, and reliable classification and identification method for the field of drug detection, holding promising prospects for widespread application.","authors":["Yongming Li","Peng Wang","Bangdong Han"],"url":"https://arxiv.org/abs/2505.01986"}
{"created":"2025-05-06","title":"Sparse Code Transceiver Design for Unsourced Random Access with Analytical Power Division in Gaussian MAC","abstract":"In this work, we discuss the problem of unsourced random access (URA) over a Gaussian multiple access channel (GMAC). To address the challenges posed by emerging massive machine-type connectivity, URA reframes multiple access as a coding-theoretic problem. The sparse code-oriented schemes are highly valued because they are widely used in existing protocols, making their implementation require only minimal changes to current networks. However, drawbacks such as the heavy reliance on extrinsic feedback from powerful channel codes and the lack of transmission robustness pose obstacles to the development of sparse codes. To address these drawbacks, a novel sparse code structure based on a universally applicable power division strategy is proposed. Comprehensive numerical results validate the effectiveness of the proposed scheme. Specifically, by employing the proposed power division method, which is derived analytically and does not require extensive simulations, a performance improvement of approximately 2.8 dB is achieved compared to schemes with identical channel code setups.","authors":["Zhentian Zhang","Mohammad Javad Ahmadi","Jian Dang","Kai-Kit Wong","Zaichen Zhang","Christos Masouros"],"url":"https://arxiv.org/abs/2505.01988"}
{"created":"2025-05-06","title":"Exact Set Packing in Multimodal Transportation with Ridesharing System for First/Last Mile","abstract":"We propose a centralized transportation system that integrates public transit with ridesharing to provide multimodal transportation. At each time interval, the system receives a set of personal drivers, designated drivers, and public transit riders. It then assigns all riders to drivers, ensuring that pick-ups and drop-offs occur at designated transit stations. This effectively replaces first-mile/last-mile (FM/LM) segments with a ridesharing alternative, reducing overall commuting time. We study two optimization problems: (1) minimizing the total travel distances of drivers and (2) minimizing the number of designated drivers required to serve all riders. We show the optimization problems are NP-hard and give hypergraph-based integer linear programming exact algorithm and approximation algorithms. To enhance computational efficiency, we introduce a clustering heuristic that utilizes both spatial and temporal aspects of the input data to accelerate rider-to-driver assignments. Finally, we conduct an extensive computational study using real-world datasets and surveys from Chicago to evaluate our model and algorithms at a city-wide scale.","authors":["Qian-Ping Gu","Jiajian Leo Liang"],"url":"https://arxiv.org/abs/2505.01989"}
{"created":"2025-05-06","title":"On optimal distinguishers for Planted Clique","abstract":"In a distinguishing problem, the input is a sample drawn from one of two distributions and the algorithm is tasked with identifying the source distribution. The performance of a distinguishing algorithm is measured by its advantage, i.e., its incremental probability of success over a random guess. A classic example of a distinguishing problem is the Planted Clique problem, where the input is a graph sampled from either $G(n,1/2)$ -- the standard Erd\\H{o}s-R\\'{e}nyi model, or $G(n,1/2,k)$ -- the Erd\\H{o}s-R\\'{e}nyi model with a clique planted on a random subset of $k$ vertices. The Planted Clique Hypothesis asserts that efficient algorithms cannot achieve advantage better than some absolute constant, say $1/4$, whenever $k=n^{1/2-\\Omega(1)}$. In this work, we aim to precisely understand the optimal distinguishing advantage achievable by efficient algorithms on Planted Clique. We show the following results under the Planted Clique hypothesis:","authors":["Ansh Nagda","Prasad Raghavendra"],"url":"https://arxiv.org/abs/2505.01990"}
{"created":"2025-05-06","title":"Always Skip Attention","abstract":"We highlight a curious empirical result within modern Vision Transformers (ViTs). Specifically, self-attention catastrophically fails to train unless it is used in conjunction with a skip connection. This is in contrast to other elements of a ViT that continue to exhibit good performance (albeit suboptimal) when skip connections are removed. Further, we show that this critical dependence on skip connections is a relatively new phenomenon, with previous deep architectures (\\eg, CNNs) exhibiting good performance in their absence. In this paper, we theoretically characterize that the self-attention mechanism is fundamentally ill-conditioned and is, therefore, uniquely dependent on skip connections for regularization. Additionally, we propose Token Graying -- a simple yet effective complement (to skip connections) that further improves the condition of input tokens. We validate our approach in both supervised and self-supervised training methods.","authors":["Yiping Ji","Hemanth Saratchandran","Peyman Moghaddam","Simon Lucey"],"url":"https://arxiv.org/abs/2505.01996"}
{"created":"2025-05-06","title":"Restoring Calibration for Aligned Large Language Models: A Calibration-Aware Fine-Tuning Approach","abstract":"One of the key technologies for the success of Large Language Models (LLMs) is preference alignment. However, a notable side effect of preference alignment is poor calibration: while the pre-trained models are typically well-calibrated, LLMs tend to become poorly calibrated after alignment with human preferences. In this paper, we investigate why preference alignment affects calibration and how to address this issue. For the first question, we observe that the preference collapse issue in alignment undesirably generalizes to the calibration scenario, causing LLMs to exhibit overconfidence and poor calibration. To address this, we demonstrate the importance of fine-tuning with domain-specific knowledge to alleviate the overconfidence issue. To further analyze whether this affects the model's performance, we categorize models into two regimes: calibratable and non-calibratable, defined by bounds of Expected Calibration Error (ECE). In the calibratable regime, we propose a calibration-aware fine-tuning approach to achieve proper calibration without compromising LLMs' performance. However, as models are further fine-tuned for better performance, they enter the non-calibratable regime. For this case, we develop an EM-algorithm-based ECE regularization for the fine-tuning loss to maintain low calibration error. Extensive experiments validate the effectiveness of the proposed methods.","authors":["Jiancong Xiao","Bojian Hou","Zhanliang Wang","Ruochen Jin","Qi Long","Weijie J. Su","Li Shen"],"url":"https://arxiv.org/abs/2505.01997"}
{"created":"2025-05-06","title":"A Synergistic Framework of Nonlinear Acoustic Computing and Reinforcement Learning for Real-World Human-Robot Interaction","abstract":"This paper introduces a novel framework integrating nonlinear acoustic computing and reinforcement learning to enhance advanced human-robot interaction under complex noise and reverberation. Leveraging physically informed wave equations (e.g., Westervelt, KZK), the approach captures higher-order phenomena such as harmonic generation and shock formation. By embedding these models in a reinforcement learning-driven control loop, the system adaptively optimizes key parameters (e.g., absorption, beamforming) to mitigate multipath interference and non-stationary noise. Experimental evaluations-covering far-field localization, weak signal detection, and multilingual speech recognition-demonstrate that this hybrid strategy surpasses traditional linear methods and purely data-driven baselines, achieving superior noise suppression, minimal latency, and robust accuracy in demanding real-world scenarios. The proposed system demonstrates broad application prospects in AI hardware, robot, machine audition, artificial audition, and brain-machine interfaces.","authors":["Xiaoliang Chen (SoundAI Technology Co.","Ltd)","Xin Yu (SoundAI Technology Co.","Ltd)","Le Chang (SoundAI Technology Co.","Ltd)","Yunhe Huang (SoundAI Technology Co.","Ltd)","Jiashuai He (SoundAI Technology Co.","Ltd)","Shibo Zhang (SoundAI Technology Co.","Ltd)","Jin Li (SoundAI Technology Co.","Ltd)","Likai Lin (SoundAI Technology Co.","Ltd)","Ziyu Zeng (SoundAI Technology Co.","Ltd)","Xianling Tu (SoundAI Technology Co.","Ltd)","Shuyu Zhang (SoundAI Technology Co.","Ltd)"],"url":"https://arxiv.org/abs/2505.01998"}
{"created":"2025-05-06","title":"Closed-loop control of seizure activity via real-time seizure forecasting by reservoir neuromorphic computing","abstract":"Closed-loop brain stimulation holds potential as personalized treatment for drug-resistant epilepsy (DRE) but still suffers from limitations that result in highly variable efficacy. First, stimulation is typically delivered upon detection of the seizure to abort rather than prevent it; second, the stimulation parameters are established by trial and error, requiring lengthy rounds of fine-tuning, which delay steady-state therapeutic efficacy. Here, we address these limitations by leveraging the potential of neuromorphic computing. We present a system capable of driving personalized free-run stimulations based on seizure forecasting, wherein each forecast triggers an electrical pulse rather than an arbitrarily predefined fixed-frequency stimulus train. We validate the system against hippocampal spheroids coupled to 3D microelectrode array as a simplified testbed, showing that it can achieve seizure reduction >97% while primarily using instantaneous stimulation frequencies within 20 Hz, well below what typically used in clinical settings. Our work demonstrates the potential of neuromorphic systems as a next-generation neuromodulation strategy for personalized DRE treatment.","authors":["Maryam Sadeghi","Dar\\'io Fern\\'andez Khatiboun","Yasser Rezaeiyan","Saima Rizwan","Alessandro Barcellona","Andrea Merello","Marco Crepaldi","Gabriella Panuccio","Farshad Moradi"],"url":"https://arxiv.org/abs/2505.02003"}
{"created":"2025-05-06","title":"Triple-identity Authentication: The Future of Secure Access","abstract":"In a typical authentication process, the local system verifies the user's identity using a stored hash value generated by a cross-system hash algorithm. This article shifts the research focus from traditional password encryption to the establishment of gatekeeping mechanisms for effective interactions between a system and the outside world. Here, we propose a triple-identity authentication system to achieve this goal. Specifically, this local system opens the inner structure of its hash algorithm to all user credentials, including the login name, login password, and authentication password. When a login credential is entered, the local system hashes it and then creates a unique identifier using intermediate hash elements randomly selected from the open algorithm. Importantly, this locally generated unique identifier (rather than the stored hash produced by the open algorithm) is utilized to verify the user's combined identity, which is generated by combining the entered credential with the International Mobile Equipment Identity and the International Mobile Subscriber Identity. The verification process is implemented at each interaction point: the login name field, the login password field, and the server's authentication point. Thus, within the context of this triple-identity authentication system, we establish a robust gatekeeping mechanism for system interactions, ultimately providing a level of security that is equivalent to multi-factor authentication.","authors":["Suyun Borjigin"],"url":"https://arxiv.org/abs/2505.02004"}
{"created":"2025-05-06","title":"Learning Heterogeneous Mixture of Scene Experts for Large-scale Neural Radiance Fields","abstract":"Recent NeRF methods on large-scale scenes have underlined the importance of scene decomposition for scalable NeRFs. Although achieving reasonable scalability, there are several critical problems remaining unexplored, i.e., learnable decomposition, modeling scene heterogeneity, and modeling efficiency. In this paper, we introduce Switch-NeRF++, a Heterogeneous Mixture of Hash Experts (HMoHE) network that addresses these challenges within a unified framework. It is a highly scalable NeRF that learns heterogeneous decomposition and heterogeneous NeRFs efficiently for large-scale scenes in an end-to-end manner. In our framework, a gating network learns to decomposes scenes and allocates 3D points to specialized NeRF experts. This gating network is co-optimized with the experts, by our proposed Sparsely Gated Mixture of Experts (MoE) NeRF framework. We incorporate a hash-based gating network and distinct heterogeneous hash experts. The hash-based gating efficiently learns the decomposition of the large-scale scene. The distinct heterogeneous hash experts consist of hash grids of different resolution ranges, enabling effective learning of the heterogeneous representation of different scene parts. These design choices make our framework an end-to-end and highly scalable NeRF solution for real-world large-scale scene modeling to achieve both quality and efficiency. We evaluate our accuracy and scalability on existing large-scale NeRF datasets and a new dataset with very large-scale scenes ($>6.5km^2$) from UrbanBIS. Extensive experiments demonstrate that our approach can be easily scaled to various large-scale scenes and achieve state-of-the-art scene rendering accuracy. Furthermore, our method exhibits significant efficiency, with an 8x acceleration in training and a 16x acceleration in rendering compared to Switch-NeRF. Codes will be released in https://github.com/MiZhenxing/Switch-NeRF.","authors":["Zhenxing Mi","Ping Yin","Xue Xiao","Dan Xu"],"url":"https://arxiv.org/abs/2505.02005"}
{"created":"2025-05-06","title":"Efficient Noise Calculation in Deep Learning-based MRI Reconstructions","abstract":"Accelerated MRI reconstruction involves solving an ill-posed inverse problem where noise in acquired data propagates to the reconstructed images. Noise analyses are central to MRI reconstruction for providing an explicit measure of solution fidelity and for guiding the design and deployment of novel reconstruction methods. However, deep learning (DL)-based reconstruction methods have often overlooked noise propagation due to inherent analytical and computational challenges, despite its critical importance. This work proposes a theoretically grounded, memory-efficient technique to calculate voxel-wise variance for quantifying uncertainty due to acquisition noise in accelerated MRI reconstructions. Our approach approximates noise covariance using the DL network's Jacobian, which is intractable to calculate. To circumvent this, we derive an unbiased estimator for the diagonal of this covariance matrix (voxel-wise variance) and introduce a Jacobian sketching technique to efficiently implement it. We evaluate our method on knee and brain MRI datasets for both data- and physics-driven networks trained in supervised and unsupervised manners. Compared to empirical references obtained via Monte Carlo simulations, our technique achieves near-equivalent performance while reducing computational and memory demands by an order of magnitude or more. Furthermore, our method is robust across varying input noise levels, acceleration factors, and diverse undersampling schemes, highlighting its broad applicability. Our work reintroduces accurate and efficient noise analysis as a central tenet of reconstruction algorithms, holding promise to reshape how we evaluate and deploy DL-based MRI. Our code will be made publicly available upon acceptance.","authors":["Onat Dalmaz","Arjun D. Desai","Reinhard Heckel","Tolga \\c{C}ukur","Akshay S. Chaudhari","Brian A. Hargreaves"],"url":"https://arxiv.org/abs/2505.02007"}
{"created":"2025-05-06","title":"Towards Safer Pretraining: Analyzing and Filtering Harmful Content in Webscale datasets for Responsible LLMs","abstract":"Large language models (LLMs) have become integral to various real-world applications, leveraging massive, web-sourced datasets like Common Crawl, C4, and FineWeb for pretraining. While these datasets provide linguistic data essential for high-quality natural language generation, they often contain harmful content, such as hate speech, misinformation, and biased narratives. Training LLMs on such unfiltered data risks perpetuating toxic behaviors, spreading misinformation, and amplifying societal biases which can undermine trust in LLM-driven applications and raise ethical concerns about their use. This paper presents a large-scale analysis of inappropriate content across these datasets, offering a comprehensive taxonomy that categorizes harmful webpages into Topical and Toxic based on their intent. We also introduce a prompt evaluation dataset, a high-accuracy Topical and Toxic Prompt (TTP), and a transformer-based model (HarmFormer) for content filtering. Additionally, we create a new multi-harm open-ended toxicity benchmark (HAVOC) and provide crucial insights into how models respond to adversarial toxic inputs. Upon publishing, we will also opensource our model signal on the entire C4 dataset. Our work offers insights into ensuring safer LLM pretraining and serves as a resource for Responsible AI (RAI) compliance.","authors":["Sai Krishna Mendu","Harish Yenala","Aditi Gulati","Shanu Kumar","Parag Agrawal"],"url":"https://arxiv.org/abs/2505.02009"}
{"created":"2025-05-06","title":"Meta-Black-Box-Optimization through Offline Q-function Learning","abstract":"Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated that using RL to learn a meta-level policy for dynamic algorithm configuration (DAC) over an optimization task distribution could significantly enhance the performance of the low-level BBO algorithm. However, the online learning paradigms in existing works makes the efficiency of MetaBBO problematic. To address this, we propose an offline learning-based MetaBBO framework in this paper, termed Q-Mamba, to attain both effectiveness and efficiency in MetaBBO. Specifically, we first transform DAC task into long-sequence decision process. This allows us further introduce an effective Q-function decomposition mechanism to reduce the learning difficulty within the intricate algorithm configuration space. Under this setting, we propose three novel designs to meta-learn DAC policy from offline data: we first propose a novel collection strategy for constructing offline DAC experiences dataset with balanced exploration and exploitation. We then establish a decomposition-based Q-loss that incorporates conservative Q-learning to promote stable offline learning from the offline dataset. To further improve the offline learning efficiency, we equip our work with a Mamba architecture which helps long-sequence learning effectiveness and efficiency by selective state model and hardware-aware parallel scan respectively. Through extensive benchmarking, we observe that Q-Mamba achieves competitive or even superior performance to prior online/offline baselines, while significantly improving the training efficiency of existing online baselines. We provide sourcecodes of Q-Mamba at https://github.com/MetaEvo/Q-Mamba.","authors":["Zeyuan Ma","Zhiguang Cao","Zhou Jiang","Hongshu Guo","Yue-Jiao Gong"],"url":"https://arxiv.org/abs/2505.02010"}
{"created":"2025-05-06","title":"CASA: CNN Autoencoder-based Score Attention for Efficient Multivariate Long-term Time-series Forecasting","abstract":"Multivariate long-term time series forecasting is critical for applications such as weather prediction, and traffic analysis. In addition, the implementation of Transformer variants has improved prediction accuracy. Following these variants, different input data process approaches also enhanced the field, such as tokenization techniques including point-wise, channel-wise, and patch-wise tokenization. However, previous studies still have limitations in time complexity, computational resources, and cross-dimensional interactions. To address these limitations, we introduce a novel CNN Autoencoder-based Score Attention mechanism (CASA), which can be introduced in diverse Transformers model-agnosticically by reducing memory and leading to improvement in model performance. Experiments on eight real-world datasets validate that CASA decreases computational resources by up to 77.7%, accelerates inference by 44.0%, and achieves state-of-the-art performance, ranking first in 87.5% of evaluated metrics.","authors":["Minhyuk Lee","HyeKyung Yoon","MyungJoo Kang"],"url":"https://arxiv.org/abs/2505.02011"}
{"created":"2025-05-06","title":"Testing Database Systems with Large Language Model Synthesized Fragments","abstract":"Various automated testing approaches have been proposed for Database Management Systems (DBMSs). Many such approaches generate pairs of equivalent queries to identify bugs that cause DBMSs to compute incorrect results, and have found hundreds of bugs in mature, widely used DBMSs. Most of these approaches are based on manually written SQL generators; however, their bug-finding capabilities remain constrained by the limited set of SQL features supported by the generators. In this work, we propose ShQveL, an approach that augments existing SQL test-case generators by leveraging Large Language Models (LLMs) to synthesize SQL fragments. Our key idea is to systematically incorporate SQL features gained through automated interactions with LLMs into the SQL generators, increasing the features covered while efficiently generating test cases. Specifically, ShQveL uses SQL sketches -- SQL statements with incomplete code segments that LLMs fill -- to integrate LLM-generated content into the generator. We evaluated ShQveL on 5 DBMSs and discovered 55 unique and previously unknown bugs, 50 of which were promptly fixed after our reports.","authors":["Suyang Zhong","Manuel Rigger"],"url":"https://arxiv.org/abs/2505.02012"}
{"created":"2025-05-06","title":"MLLM-Enhanced Face Forgery Detection: A Vision-Language Fusion Solution","abstract":"Reliable face forgery detection algorithms are crucial for countering the growing threat of deepfake-driven disinformation. Previous research has demonstrated the potential of Multimodal Large Language Models (MLLMs) in identifying manipulated faces. However, existing methods typically depend on either the Large Language Model (LLM) alone or an external detector to generate classification results, which often leads to sub-optimal integration of visual and textual modalities. In this paper, we propose VLF-FFD, a novel Vision-Language Fusion solution for MLLM-enhanced Face Forgery Detection. Our key contributions are twofold. First, we present EFF++, a frame-level, explainability-driven extension of the widely used FaceForensics++ (FF++) dataset. In EFF++, each manipulated video frame is paired with a textual annotation that describes both the forgery artifacts and the specific manipulation technique applied, enabling more effective and informative MLLM training. Second, we design a Vision-Language Fusion Network (VLF-Net) that promotes bidirectional interaction between visual and textual features, supported by a three-stage training pipeline to fully leverage its potential. VLF-FFD achieves state-of-the-art (SOTA) performance in both cross-dataset and intra-dataset evaluations, underscoring its exceptional effectiveness in face forgery detection.","authors":["Siran Peng","Zipei Wang","Li Gao","Xiangyu Zhu","Tianshuo Zhang","Ajian Liu","Haoyuan Zhang","Zhen Lei"],"url":"https://arxiv.org/abs/2505.02013"}
{"created":"2025-05-06","title":"Requirements-Based Test Generation: A Comprehensive Survey","abstract":"As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirement types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.","authors":["Zhenzhen Yang","Rubing Huang","Chenhui Cui","Nan Niu","Dave Towey"],"url":"https://arxiv.org/abs/2505.02015"}
{"created":"2025-05-06","title":"ForgeEDA: A Comprehensive Multimodal Dataset for Advancing EDA","abstract":"We introduce ForgeEDA, an open-source comprehensive circuit dataset across various categories. ForgeEDA includes diverse circuit representations such as Register Transfer Level (RTL) code, Post-mapping (PM) netlists, And-Inverter Graphs (AIGs), and placed netlists, enabling comprehensive analysis and development. We demonstrate ForgeEDA's utility by benchmarking state-of-the-art EDA algorithms on critical tasks such as Power, Performance, and Area (PPA) optimization, highlighting its ability to expose performance gaps and drive advancements. Additionally, ForgeEDA's scale and diversity facilitate the training of AI models for EDA tasks, demonstrating its potential to improve model performance and generalization. By addressing limitations in existing datasets, ForgeEDA aims to catalyze breakthroughs in modern IC design and support the next generation of innovations in EDA.","authors":["Zhengyuan Shi","Zeju Li","Chengyu Ma","Yunhao Zhou","Ziyang Zheng","Jiawei Liu","Hongyang Pan","Lingfeng Zhou","Kezhi Li","Jiaying Zhu","Lingwei Yan","Zhiqiang He","Chenhao Xue","Wentao Jiang","Fan Yang","Guangyu Sun","Xiaoyan Yang","Gang Chen","Chuan Shi","Zhufei Chu","Jun Yang","Qiang Xu"],"url":"https://arxiv.org/abs/2505.02016"}
{"created":"2025-05-06","title":"Aokana: A GPU-Driven Voxel Rendering Framework for Open World Games","abstract":"Voxels are among the most popular 3D geometric representations today. Due to their intuitiveness and ease-of-editing, voxels have been widely adopted in stylized games and low-cost independent games. However, the high storage cost of voxels, along with the significant time overhead associated with large-scale voxel rendering, limits the further development of open-world voxel games. In this paper, we introduce Aokana, a GPU-Driven Voxel Rendering Framework for Open World Games. Aokana is based on a Sparse Voxel Directed Acyclic Graph (SVDAG). It incorporates a Level-of-Details (LOD) mechanism and a streaming system, enabling seamless map loading as players traverse the open-world game environment. We also designed a corresponding high-performance GPU-driven voxel rendering pipeline to support real-time rendering of the voxel scenes that contain tens of billions of voxels. Aokana can be directly applied to existing game engines and easily integrated with mesh-based rendering methods, demonstrating its practical applicability in game development. Experimental evaluations show that, with increasing voxel scene resolution, Aokana can reduce memory usage by up to ninefold and achieves rendering speeds up to 4.8 times faster than those of previous state-of-the-art approaches.","authors":["Yingrong Fang","Qitong Wang","Wei Wang"],"url":"https://arxiv.org/abs/2505.02017"}
{"created":"2025-05-06","title":"R-Bench: Graduate-level Multi-disciplinary Benchmarks for LLM & MLLM Complex Reasoning Evaluation","abstract":"Reasoning stands as a cornerstone of intelligence, enabling the synthesis of existing knowledge to solve complex problems. Despite remarkable progress, existing reasoning benchmarks often fail to rigorously evaluate the nuanced reasoning capabilities required for complex, real-world problemsolving, particularly in multi-disciplinary and multimodal contexts. In this paper, we introduce a graduate-level, multi-disciplinary, EnglishChinese benchmark, dubbed as Reasoning Bench (R-Bench), for assessing the reasoning capability of both language and multimodal models. RBench spans 1,094 questions across 108 subjects for language model evaluation and 665 questions across 83 subjects for multimodal model testing in both English and Chinese. These questions are meticulously curated to ensure rigorous difficulty calibration, subject balance, and crosslinguistic alignment, enabling the assessment to be an Olympiad-level multi-disciplinary benchmark. We evaluate widely used models, including OpenAI o1, GPT-4o, DeepSeek-R1, etc. Experimental results indicate that advanced models perform poorly on complex reasoning, especially multimodal reasoning. Even the top-performing model OpenAI o1 achieves only 53.2% accuracy on our multimodal evaluation. Data and code are made publicly available at here.","authors":["Meng-Hao Guo","Jiajun Xu","Yi Zhang","Jiaxi Song","Haoyang Peng","Yi-Xuan Deng","Xinzhi Dong","Kiyohiro Nakayama","Zhengyang Geng","Chen Wang","Bolin Ni","Guo-Wei Yang","Yongming Rao","Houwen Peng","Han Hu","Gordon Wetzstein","Shi-min Hu"],"url":"https://arxiv.org/abs/2505.02018"}
{"created":"2025-05-06","title":"Wide & Deep Learning for Node Classification","abstract":"Wide & Deep, a simple yet effective learning architecture for recommendation systems developed by Google, has had a significant impact in both academia and industry due to its combination of the memorization ability of generalized linear models and the generalization ability of deep models. Graph convolutional networks (GCNs) remain dominant in node classification tasks; however, recent studies have highlighted issues such as heterophily and expressiveness, which focus on graph structure while seemingly neglecting the potential role of node features. In this paper, we propose a flexible framework GCNIII, which leverages the Wide & Deep architecture and incorporates three techniques: Intersect memory, Initial residual and Identity mapping. We provide comprehensive empirical evidence showing that GCNIII can more effectively balance the trade-off between over-fitting and over-generalization on various semi- and full- supervised tasks. Additionally, we explore the use of large language models (LLMs) for node feature engineering to enhance the performance of GCNIII in cross-domain node classification tasks. Our implementation is available at https://github.com/CYCUCAS/GCNIII.","authors":["Yancheng Chen","Wenguo Yang","Zhipeng Jiang"],"url":"https://arxiv.org/abs/2505.02020"}
{"created":"2025-05-06","title":"NbBench: Benchmarking Language Models for Comprehensive Nanobody Tasks","abstract":"Nanobodies, single-domain antibody fragments derived from camelid heavy-chain-only antibodies, exhibit unique advantages such as compact size, high stability, and strong binding affinity, making them valuable tools in therapeutics and diagnostics. While recent advances in pretrained protein and antibody language models (PPLMs and PALMs) have greatly enhanced biomolecular understanding, nanobody-specific modeling remains underexplored and lacks a unified benchmark. To address this gap, we introduce NbBench, the first comprehensive benchmark suite for nanobody representation learning. Spanning eight biologically meaningful tasks across nine curated datasets, NbBench encompasses structure annotation, binding prediction, and developability assessment. We systematically evaluate eleven representative models--including general-purpose protein LMs, antibody-specific LMs, and nanobody-specific LMs--in a frozen setting. Our analysis reveals that antibody language models excel in antigen-related tasks, while performance on regression tasks such as thermostability and affinity remains challenging across all models. Notably, no single model consistently outperforms others across all tasks. By standardizing datasets, task definitions, and evaluation protocols, NbBench offers a reproducible foundation for assessing and advancing nanobody modeling.","authors":["Yiming Zhang","Koji Tsuda"],"url":"https://arxiv.org/abs/2505.02022"}
{"created":"2025-05-06","title":"A Unified Perspective on Orthogonalization and Diagonalization","abstract":"This paper makes a formal connection between two families of widely used matrix factorization algorithms in numerical linear algebra. One family consists of the Jacobi eigenvalue algorithm and its variants for computing the Hermitian eigendecomposition and singular value decomposition. The other consists of Gaussian elimination and the Gram-Schmidt procedure with various pivoting rules for computing the Cholesky decomposition and QR decomposition respectively.","authors":["Isabel Detherage","Rikhav Shah"],"url":"https://arxiv.org/abs/2505.02023"}
{"created":"2025-05-06","title":"From Mind to Machine: The Rise of Manus AI as a Fully Autonomous Digital Agent","abstract":"Manus AI is a general-purpose AI agent introduced in early 2025, marking a significant advancement in autonomous artificial intelligence. Developed by the Chinese startup Monica.im, Manus is designed to bridge the gap between \"mind\" and \"hand\" - combining the reasoning and planning capabilities of large language models with the ability to execute complex, end-to-end tasks that produce tangible outcomes. This paper presents a comprehensive overview of Manus AI, exploring its core technical architecture, diverse applications across sectors such as healthcare, finance, manufacturing, robotics, and gaming, as well as its key strengths, current limitations, and future potential. Positioned as a preview of what lies ahead, Manus AI represents a shift toward intelligent agents that can translate high-level intentions into real-world actions, heralding a new era of human-AI collaboration.","authors":["Minjie Shen","Qikai Yang"],"url":"https://arxiv.org/abs/2505.02024"}
{"created":"2025-05-06","title":"A Birotation Solution for Relative Pose Problems","abstract":"Relative pose estimation, a fundamental computer vision problem, has been extensively studied for decades. Existing methods either estimate and decompose the essential matrix or directly estimate the rotation and translation to obtain the solution. In this article, we break the mold by tackling this traditional problem with a novel birotation solution. We first introduce three basis transformations, each associated with a geometric metric to quantify the distance between the relative pose to be estimated and its corresponding basis transformation. Three energy functions, designed based on these metrics, are then minimized on the Riemannian manifold $\\mathrm{SO(3)}$ by iteratively updating the two rotation matrices. The two rotation matrices and the basis transformation corresponding to the minimum energy are ultimately utilized to recover the relative pose. Extensive quantitative and qualitative evaluations across diverse relative pose estimation tasks demonstrate the superior performance of our proposed birotation solution. Source code, demo video, and datasets will be available at \\href{https://mias.group/birotation-solution}{mias.group/birotation-solution} upon publication.","authors":["Hongbo Zhao","Ziwei Long","Mengtan Zhang","Hanli Wang","Qijun Chen","Rui Fan"],"url":"https://arxiv.org/abs/2505.02025"}
{"created":"2025-05-06","title":"GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph In-Context Learning","abstract":"Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.","authors":["Rui Lv","Zaixi Zhang","Kai Zhang","Qi Liu","Weibo Gao","Jiawei Liu","Jiaxia Yan","Linan Yue","Fangzhou Yao"],"url":"https://arxiv.org/abs/2505.02027"}
{"created":"2025-05-06","title":"An overview of artificial intelligence in computer-assisted language learning","abstract":"Computer-assisted language learning -- CALL -- is an established research field. We review how artificial intelligence can be applied to support language learning and teaching. The need for intelligent agents that assist language learners and teachers is increasing: the human teacher's time is a scarce and costly resource, which does not scale with growing demand. Further factors contribute to the need for CALL: pandemics and increasing demand for distance learning, migration of large populations, the need for sustainable and affordable support for learning, etc. CALL systems are made up of many components that perform various functions, and AI is applied to many different aspects in CALL, corresponding to their own expansive research areas. Most of what we find in the research literature and in practical use are prototypes or partial implementations -- systems that perform some aspects of the overall desired functionality. Complete solutions -- most of them commercial -- are few, because they require massive resources. Recent advances in AI should result in improvements in CALL, yet there is a lack of surveys that focus on AI in the context of this research field. This paper aims to present a perspective on the AI methods that can be employed for language learning from a position of a developer of a CALL system. We also aim to connect work from different disciplines, to build bridges for interdisciplinary work.","authors":["Anisia Katinskaia"],"url":"https://arxiv.org/abs/2505.02032"}
{"created":"2025-05-06","title":"Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray Gene Expression Profiles","abstract":"DNA microarray technology enables the simultaneous measurement of expression levels of thousands of genes, thereby facilitating the understanding of the molecular mechanisms underlying complex diseases such as brain tumors and the identification of diagnostic genetic signatures. To derive meaningful biological insights from the high-dimensional and complex gene features obtained through this technology and to analyze gene properties in detail, classical AI-based approaches such as machine learning and deep learning are widely employed. However, these methods face various limitations in managing high-dimensional vector spaces and modeling the intricate relationships among genes. In particular, challenges such as hyperparameter tuning, computational costs, and high processing power requirements can hinder their efficiency. To overcome these limitations, quantum computing and quantum AI approaches are gaining increasing attention. Leveraging quantum properties such as superposition and entanglement, quantum methods enable more efficient parallel processing of high-dimensional data and offer faster and more effective solutions to problems that are computationally demanding for classical methods. In this study, a novel model called \"Deep VQC\" is proposed, based on the Variational Quantum Classifier approach. Developed using microarray data containing 54,676 gene features, the model successfully classified four different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and pilocytic astrocytoma-alongside healthy samples with high accuracy. Furthermore, compared to classical ML algorithms, our model demonstrated either superior or comparable classification performance. These results highlight the potential of quantum AI methods as an effective and promising approach for the analysis and classification of complex structures such as brain tumors based on gene expression features.","authors":["Emine Akpinar","Batuhan Hangun","Murat Oduncuoglu","Oguz Altun","Onder Eyecioglu","Zeynel Yalcin"],"url":"https://arxiv.org/abs/2505.02033"}
{"created":"2025-05-06","title":"Secrets of GFlowNets' Learning Behavior: A Theoretical Study","abstract":"Generative Flow Networks (GFlowNets) have emerged as a powerful paradigm for generating composite structures, demonstrating considerable promise across diverse applications. While substantial progress has been made in exploring their modeling validity and connections to other generative frameworks, the theoretical understanding of their learning behavior remains largely uncharted. In this work, we present a rigorous theoretical investigation of GFlowNets' learning behavior, focusing on four fundamental dimensions: convergence, sample complexity, implicit regularization, and robustness. By analyzing these aspects, we seek to elucidate the intricate mechanisms underlying GFlowNet's learning dynamics, shedding light on its strengths and limitations. Our findings contribute to a deeper understanding of the factors influencing GFlowNet performance and provide insights into principled guidelines for their effective design and deployment. This study not only bridges a critical gap in the theoretical landscape of GFlowNets but also lays the foundation for their evolution as a reliable and interpretable framework for generative modeling. Through this, we aspire to advance the theoretical frontiers of GFlowNets and catalyze their broader adoption in the AI community.","authors":["Tianshu Yu"],"url":"https://arxiv.org/abs/2505.02035"}
{"created":"2025-05-06","title":"Holographic Radiance Cascades for 2D Global Illumination","abstract":"Efficiently calculating global illumination has always been one of the greatest challenges in computer graphics. Algorithms for approximating global illumination have always struggled to run in realtime for fully dynamic scenes, and have had to rely heavily on stochastic raytracing, spatialtemporal denoising, or undersampled representations, resulting in much lower quality of lighting compared to reference solutions. Even though the problem of calculating global illumination in 2D is significantly simpler than that of 3D, most contemporary approaches still struggle to accurately approximate 2D global illumination under realtime constraints.","authors":["Rouli Freeman","Alexander Sannikov","Adrian Margel"],"url":"https://arxiv.org/abs/2505.02041"}
{"created":"2025-05-06","title":"Point2Primitive: CAD Reconstruction from Point Cloud by Direct Primitive Prediction","abstract":"Recovering CAD models from point clouds, especially the sketch-extrusion process, can be seen as the process of rebuilding the topology and extrusion primitives. Previous methods utilize implicit fields for sketch representation, leading to shape reconstruction of curved edges. In this paper, we proposed a CAD reconstruction network that produces editable CAD models from input point clouds (Point2Primitive) by directly predicting every element of the extrusion primitives. Point2Primitive can directly detect and predict sketch curves (type and parameter) from point clouds based on an improved transformer. The sketch curve parameters are formulated as position queries and optimized in an autoregressive way, leading to high parameter accuracy. The topology is rebuilt by extrusion segmentation, and each extrusion parameter (sketch and extrusion operation) is recovered by combining the predicted curves and the computed extrusion operation. Extensive experiments demonstrate that our method is superior in primitive prediction accuracy and CAD reconstruction. The reconstructed shapes are of high geometrical fidelity.","authors":["Cheng Wang","Xinzhu Ma","Bin Wang","Shixiang Tang","Yuan Meng","Ping Jiang"],"url":"https://arxiv.org/abs/2505.02043"}
{"created":"2025-05-06","title":"A UNet Model for Accelerated Preprocessing of CRISM Hyperspectral Data for Mineral Identification on Mars","abstract":"Accurate mineral identification on the Martian surface is critical for understanding the planet's geological history. This paper presents a UNet-based autoencoder model for efficient spectral preprocessing of CRISM MTRDR hyperspectral data, addressing the limitations of traditional methods that are computationally intensive and time-consuming. The proposed model automates key preprocessing steps, such as smoothing and continuum removal, while preserving essential mineral absorption features. Trained on augmented spectra from the MICA spectral library, the model introduces realistic variability to simulate MTRDR data conditions. By integrating this framework, preprocessing time for an 800x800 MTRDR scene is reduced from 1.5 hours to just 5 minutes on an NVIDIA T1600 GPU. The preprocessed spectra are subsequently classified using MICAnet, a deep learning model for Martian mineral identification. Evaluation on labeled CRISM TRDR data demonstrates that the proposed approach achieves competitive accuracy while significantly enhancing preprocessing efficiency. This work highlights the potential of the UNet-based preprocessing framework to improve the speed and reliability of mineral mapping on Mars.","authors":["Priyanka Kumari","Sampriti Soor","Amba Shetty","Archana M. Nair"],"url":"https://arxiv.org/abs/2505.02046"}
{"created":"2025-05-06","title":"High-order well-balanced methods for systems of balance laws: a control-based approach","abstract":"In some previous works, two of the authors have introduced a strategy to develop high-order numerical methods for systems of balance laws that preserve all the stationary solutions of the system. The key ingredient of these methods is a well-balanced reconstruction operator. A strategy has been also introduced to modify any standard reconstruction operator like MUSCL, ENO, CWENO, etc. in order to be well-balanced. This strategy involves a non-linear problem at every cell at every time step that consists in finding the stationary solution whose average is the given cell value. So far this strategy has been only applied to systems whose stationary solution are known either in explicit or implicit form. The goal of this paper is to present a general implementation of this technique that can be applied to any system of balance laws. To do this, the nonlinear problems to be solved in the reconstruction procedure are interpreted as control problems: they consist in finding a solution of an ODE system whose average at the computation interval is given. These problems are written in functional form and the gradient of the functional is computed on the basis of the adjoint problem. Newton's method is applied then to solve the problems. Special care is put to analyze the effects of computing the averages and the source terms using quadrature formulas. To test their efficiency and well-balancedness, the methods are applied to a number of systems of balance laws, ranging from easy academic systems consisting of Burgers equation with some nonlinear source terms to the shallow water equations or Euler equations of gas dynamics with gravity effects.","authors":["Irene G\\'omez-Bueno","Manuel Jes\\'us Castro D\\'iaz","Carlos Par\\'es"],"url":"https://arxiv.org/abs/2505.02047"}
{"created":"2025-05-06","title":"Regression s all you need for medical image translation","abstract":"The acquisition of information-rich images within a limited time budget is crucial in medical imaging. Medical image translation (MIT) can help enhance and supplement existing datasets by generating synthetic images from acquired data. While Generative Adversarial Nets (GANs) and Diffusion Models (DMs) have achieved remarkable success in natural image generation, their benefits - creativity and image realism - do not necessarily transfer to medical applications where highly accurate anatomical information is required. In fact, the imitation of acquisition noise or content hallucination hinder clinical utility. Here, we introduce YODA (You Only Denoise once - or Average), a novel 2.5D diffusion-based framework for volumetric MIT. YODA unites diffusion and regression paradigms to produce realistic or noise-free outputs. Furthermore, we propose Expectation-Approximation (ExpA) DM sampling, which draws inspiration from MRI signal averaging. ExpA-sampling suppresses generated noise and, thus, eliminates noise from biasing the evaluation of image quality. Through extensive experiments on four diverse multi-modal datasets - comprising multi-contrast brain MRI and pelvic MRI-CT - we show that diffusion and regression sampling yield similar results in practice. As such, the computational overhead of diffusion sampling does not provide systematic benefits in medical information translation. Building on these insights, we demonstrate that YODA outperforms several state-of-the-art GAN and DM methods. Notably, YODA-generated images are shown to be interchangeable with, or even superior to, physical acquisitions for several downstream tasks. Our findings challenge the presumed advantages of DMs in MIT and pave the way for the practical application of MIT in medical imaging.","authors":["Sebastian Rassmann","David K\\\"ugler","Christian Ewert","Martin Reuter"],"url":"https://arxiv.org/abs/2505.02048"}
{"created":"2025-05-06","title":"Enhancing Lidar Point Cloud Sampling via Colorization and Super-Resolution of Lidar Imagery","abstract":"Recent advancements in lidar technology have led to improved point cloud resolution as well as the generation of 360 degrees, low-resolution images by encoding depth, reflectivity, or near-infrared light within each pixel. These images enable the application of deep learning (DL) approaches, originally developed for RGB images from cameras to lidar-only systems, eliminating other efforts, such as lidar-camera calibration. Compared with conventional RGB images, lidar imagery demonstrates greater robustness in adverse environmental conditions, such as low light and foggy weather. Moreover, the imaging capability addresses the challenges in environments where the geometric information in point clouds may be degraded, such as long corridors, and dense point clouds may be misleading, potentially leading to drift errors.","authors":["Sier Ha","Honghao Du","Xianjia Yu","Tomi Westerlund"],"url":"https://arxiv.org/abs/2505.02049"}
{"created":"2025-05-06","title":"Enhancing Safety Standards in Automated Systems Using Dynamic Bayesian Networks","abstract":"Cut-in maneuvers in high-speed traffic pose critical challenges that can lead to abrupt braking and collisions, necessitating safe and efficient lane change strategies. We propose a Dynamic Bayesian Network (DBN) framework to integrate lateral evidence with safety assessment models, thereby predicting lane changes and ensuring safe cut-in maneuvers effectively. Our proposed framework comprises three key probabilistic hypotheses (lateral evidence, lateral safety, and longitudinal safety) that facilitate the decision-making process through dynamic data processing and assessments of vehicle positions, lateral velocities, relative distance, and Time-to-Collision (TTC) computations. The DBN model's performance compared with other conventional approaches demonstrates superior performance in crash reduction, especially in critical high-speed scenarios, while maintaining a competitive performance in low-speed scenarios. This paves the way for robust, scalable, and efficient safety validation in automated driving systems.","authors":["Kranthi Kumar Talluri","Anders L. Madsen","Galia Weidl"],"url":"https://arxiv.org/abs/2505.02050"}
{"created":"2025-05-06","title":"TxP: Reciprocal Generation of Ground Pressure Dynamics and Activity Descriptions for Improving Human Activity Recognition","abstract":"Sensor-based human activity recognition (HAR) has predominantly focused on Inertial Measurement Units and vision data, often overlooking the capabilities unique to pressure sensors, which capture subtle body dynamics and shifts in the center of mass. Despite their potential for postural and balance-based activities, pressure sensors remain underutilized in the HAR domain due to limited datasets. To bridge this gap, we propose to exploit generative foundation models with pressure-specific HAR techniques. Specifically, we present a bidirectional Text$\\times$Pressure model that uses generative foundation models to interpret pressure data as natural language. TxP accomplishes two tasks: (1) Text2Pressure, converting activity text descriptions into pressure sequences, and (2) Pressure2Text, generating activity descriptions and classifications from dynamic pressure maps. Leveraging pre-trained models like CLIP and LLaMA 2 13B Chat, TxP is trained on our synthetic PressLang dataset, containing over 81,100 text-pressure pairs. Validated on real-world data for activities such as yoga and daily tasks, TxP provides novel approaches to data augmentation and classification grounded in atomic actions. This consequently improved HAR performance by up to 12.4\\% in macro F1 score compared to the state-of-the-art, advancing pressure-based HAR with broader applications and deeper insights into human movement.","authors":["Lala Shakti Swarup Ray","Lars Krupp","Vitor Fortes Rey","Bo Zhou","Sungho Suh","Paul Lukowicz"],"url":"https://arxiv.org/abs/2505.02052"}
{"created":"2025-05-06","title":"Collocation Methods for High-Order Well-Balanced Methods for Systems of Balance Laws","abstract":"In some previous works, two of the authors introduced a technique to design high-order numerical methods for one-dimensional balance laws that preserve all their stationary solutions. The basis of these methods is a well-balanced reconstruction operator. Moreover, they introduced a procedure to modify any standard reconstruction operator, like MUSCL, ENO, CWENO, etc., in order to be well-balanced. This strategy involves a non-linear problem at every cell at every time step that consists in finding the stationary solution whose average is the given cell value. In a recent paper, a fully well-balanced method is presented where the non-linear problems to be solved in the reconstruction procedure are interpreted as control problems. The goal of this paper is to introduce a new technique to solve these local non-linear problems based on the application of the collocation RK methods. Special care is put to analyze the effects of computing the averages and the source terms using quadrature formulas. A general technique which allows us to deal with resonant problems is also introduced. To check the efficiency of the methods and their well-balance property, they have been applied to a number of tests, ranging from easy academic systems of balance laws consisting of Burgers equation with some non-linear source terms to the shallow water equations -- with and without Manning friction -- or Euler equations of gas dynamics with gravity effects.","authors":["Irene G\\'omez-Bueno","Manuel Jes\\'us Castro D\\'iaz","Carlos Par\\'es","Giovanni Russo"],"url":"https://arxiv.org/abs/2505.02055"}
{"created":"2025-05-06","title":"Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin","abstract":"Adapting vision-language models (VLMs) to downstream tasks with pseudolabels has gained increasing attention. A major obstacle is that the pseudolabels generated by VLMs tend to be imbalanced, leading to inferior performance. While existing methods have explored various strategies to address this, the underlying causes of imbalance remain insufficiently investigated. To fill this gap, we delve into imbalanced pseudolabels and identify two primary contributing factors: concept mismatch and concept confusion. To mitigate these two issues, we propose a novel framework incorporating concept alignment and confusion-aware calibrated margin mechanisms. The core of our approach lies in enhancing underperforming classes and promoting balanced predictions across categories, thus mitigating imbalance. Extensive experiments on six benchmark datasets with three learning paradigms demonstrate that the proposed method effectively enhances the accuracy and balance of pseudolabels, achieving a relative improvement of 6.29% over the SoTA method. Our code is avaliable at https://anonymous.4open.science/r/CAP-C642/","authors":["Yuchen Wang","Xuefeng Bai","Xiucheng Li","Weili Guan","Liqiang Nie","Xinyang Chen"],"url":"https://arxiv.org/abs/2505.02056"}
{"created":"2025-05-06","title":"Transforming faces into video stories -- VideoFace2.0","abstract":"Face detection and face recognition have been in the focus of vision community since the very beginnings. Inspired by the success of the original Videoface digitizer, a pioneering device that allowed users to capture video signals from any source, we have designed an advanced video analytics tool to efficiently create structured video stories, i.e. identity-based information catalogs. VideoFace2.0 is the name of the developed system for spatial and temporal localization of each unique face in the input video, i.e. face re-identification (ReID), which also allows their cataloging, characterization and creation of structured video outputs for later downstream tasks. Developed near real-time solution is primarily designed to be utilized in application scenarios involving TV production, media analysis, and as an efficient tool for creating large video datasets necessary for training machine learning (ML) models in challenging vision tasks such as lip reading and multimodal speech recognition. Conducted experiments confirm applicability of the proposed face ReID algorithm that is combining the concepts of face detection, face recognition and passive tracking-by-detection in order to achieve robust and efficient face ReID. The system is envisioned as a compact and modular extensions of the existing video production equipment. We hope that the presented work and shared code will stimulate further interest in development of similar, application specific video analysis tools, and lower the entry barrier for production of high-quality multi-modal ML datasets in the future.","authors":["Branko Brklja\\v{c}","Vladimir Kalu\\v{s}ev","Branislav Popovi\\'c","Milan Se\\v{c}ujski"],"url":"https://arxiv.org/abs/2505.02060"}
{"created":"2025-05-06","title":"Diffeomorphic Reconstruction Of A 2D Simple Non Parametric Manifold From Level Set Data Via Shape Gradients","abstract":"A variational approach to the reconstruction of a shape (2D simple manifolds) as triangulated surface from given level set using shape gradients is presented. It involves an energy functional that depends on the local shape characteristics of the surface. Minimization of the energy through an iterative procedure using the gradient descent method yields a triangulated surface mesh which matches the boundary of the object of interest and this model ensures the smoothness of the boundary.","authors":["Shafeequdheen P","Jyotiranjan Nayak","Vijayakrishna Rowthu"],"url":"https://arxiv.org/abs/2505.02061"}
{"created":"2025-05-06","title":"Ethical AI in the Healthcare Sector: Investigating Key Drivers of Adoption through the Multi-Dimensional Ethical AI Adoption Model (MEAAM)","abstract":"The adoption of Artificial Intelligence (AI) in the healthcare service industry presents numerous ethical challenges, yet current frameworks often fail to offer a comprehensive, empirical understanding of the multidimensional factors influencing ethical AI integration. Addressing this critical research gap, this study introduces the Multi-Dimensional Ethical AI Adoption Model (MEAAM), a novel theoretical framework that categorizes 13 critical ethical variables across four foundational dimensions of Ethical AI Fair AI, Responsible AI, Explainable AI, and Sustainable AI. These dimensions are further analyzed through three core ethical lenses: epistemic concerns (related to knowledge, transparency, and system trustworthiness), normative concerns (focused on justice, autonomy, dignity, and moral obligations), and overarching concerns (highlighting global, systemic, and long-term ethical implications). This study adopts a quantitative, cross-sectional research design using survey data collected from healthcare professionals and analyzed via Partial Least Squares Structural Equation Modeling (PLS-SEM). Employing PLS-SEM, this study empirically investigates the influence of these ethical constructs on two outcomes Operational AI Adoption and Systemic AI Adoption. Results indicate that normative concerns most significantly drive operational adoption decisions, while overarching concerns predominantly shape systemic adoption strategies and governance frameworks. Epistemic concerns play a facilitative role, enhancing the impact of ethical design principles on trust and transparency in AI systems. By validating the MEAAM framework, this research advances a holistic, actionable approach to ethical AI adoption in healthcare and provides critical insights for policymakers, technologists, and healthcare administrators striving to implement ethically grounded AI solutions.","authors":["Prathamesh Muzumdar","Apoorva Muley","Kuldeep Singh","Sumanth Cheemalapati"],"url":"https://arxiv.org/abs/2505.02062"}
{"created":"2025-05-06","title":"RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video","abstract":"Multimodal Large Language Models (MLLMs) increasingly excel at perception, understanding, and reasoning. However, current benchmarks inadequately evaluate their ability to perform these tasks continuously in dynamic, real-world environments. To bridge this gap, we introduce RTV-Bench, a fine-grained benchmark for MLLM real-time video analysis. RTV-Bench uses three key principles: (1) Multi-Timestamp Question Answering (MTQA), where answers evolve with scene changes; (2) Hierarchical Question Structure, combining basic and advanced queries; and (3) Multi-dimensional Evaluation, assessing the ability of continuous perception, understanding, and reasoning. RTV-Bench contains 552 diverse videos (167.2 hours) and 4,631 high-quality QA pairs. We evaluated leading MLLMs, including proprietary (GPT-4o, Gemini 2.0), open-source offline (Qwen2.5-VL, VideoLLaMA3), and open-source real-time (VITA-1.5, InternLM-XComposer2.5-OmniLive) models. Experiment results show open-source real-time models largely outperform offline ones but still trail top proprietary models. Our analysis also reveals that larger model size or higher frame sampling rates do not significantly boost RTV-Bench performance, sometimes causing slight decreases. This underscores the need for better model architectures optimized for video stream processing and long sequences to advance real-time video analysis with MLLMs. Our benchmark toolkit is available at: https://github.com/LJungang/RTV-Bench.","authors":["Shuhang Xun","Sicheng Tao","Jungang Li","Yibo Shi","Zhixin Lin","Zhanhui Zhu","Yibo Yan","Hanqian Li","Linghao Zhang","Shikang Wang","Yixin Liu","Hanbo Zhang","Xuming Hu","Ying Ma"],"url":"https://arxiv.org/abs/2505.02064"}
{"created":"2025-05-06","title":"Neural Logistic Bandits","abstract":"We study the problem of neural logistic bandits, where the main task is to learn an unknown reward function within a logistic link function using a neural network. Existing approaches either exhibit unfavorable dependencies on $\\kappa$, where $1/\\kappa$ represents the minimum variance of reward distributions, or suffer from direct dependence on the feature dimension $d$, which can be huge in neural network-based settings. In this work, we introduce a novel Bernstein-type inequality for self-normalized vector-valued martingales that is designed to bypass a direct dependence on the ambient dimension. This lets us deduce a regret upper bound that grows with the effective dimension $\\widetilde{d}$, not the feature dimension, while keeping a minimal dependence on $\\kappa$. Based on the concentration inequality, we propose two algorithms, NeuralLog-UCB-1 and NeuralLog-UCB-2, that guarantee regret upper bounds of order $\\widetilde{O}(\\widetilde{d}\\sqrt{\\kappa T})$ and $\\widetilde{O}(\\widetilde{d}\\sqrt{T/\\kappa})$, respectively, improving on the existing results. Lastly, we report numerical results on both synthetic and real datasets to validate our theoretical findings.","authors":["Seoungbin Bae","Dabeen Lee"],"url":"https://arxiv.org/abs/2505.02069"}
{"created":"2025-05-06","title":"Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning","abstract":"We propose the Compact Clustering Attention (COCA) layer, an effective building block that introduces a hierarchical strategy for object-centric representation learning, while solving the unsupervised object discovery task on single images. COCA is an attention-based clustering module capable of extracting object-centric representations from multi-object scenes, when cascaded into a bottom-up hierarchical network architecture, referred to as COCA-Net. At its core, COCA utilizes a novel clustering algorithm that leverages the physical concept of compactness, to highlight distinct object centroids in a scene, providing a spatial inductive bias. Thanks to this strategy, COCA-Net generates high-quality segmentation masks on both the decoder side and, notably, the encoder side of its pipeline. Additionally, COCA-Net is not bound by a predetermined number of object masks that it generates and handles the segmentation of background elements better than its competitors. We demonstrate COCA-Net's segmentation performance on six widely adopted datasets, achieving superior or competitive results against the state-of-the-art models across nine different evaluation metrics.","authors":["Can K\\\"u\\c{c}\\\"uks\\\"ozen","Y\\\"ucel Yemez"],"url":"https://arxiv.org/abs/2505.02071"}
{"created":"2025-05-06","title":"What do Language Model Probabilities Represent? From Distribution Estimation to Response Prediction","abstract":"The notion of language modeling has gradually shifted in recent years from a distribution over finite-length strings to general-purpose prediction models for textual inputs and outputs, following appropriate alignment phases. This paper analyzes the distinction between distribution estimation and response prediction in the context of LLMs, and their often conflicting goals. We examine the training phases of LLMs, which include pretraining, in-context learning, and preference tuning, and also the common use cases for their output probabilities, which include completion probabilities and explicit probabilities as output. We argue that the different settings lead to three distinct intended output distributions. We demonstrate that NLP works often assume that these distributions should be similar, which leads to misinterpretations of their experimental findings. Our work sets firmer formal foundations for the interpretation of LLMs, which will inform ongoing work on the interpretation and use of LLMs' induced distributions.","authors":["Eitan Wagner","Omri Abend"],"url":"https://arxiv.org/abs/2505.02072"}
{"created":"2025-05-06","title":"Lightweight Defense Against Adversarial Attacks in Time Series Classification","abstract":"As time series classification (TSC) gains prominence, ensuring robust TSC models against adversarial attacks is crucial. While adversarial defense is well-studied in Computer Vision (CV), the TSC field has primarily relied on adversarial training (AT), which is computationally expensive. In this paper, five data augmentation-based defense methods tailored for time series are developed, with the most computationally intensive method among them increasing the computational resources by only 14.07% compared to the original TSC model. Moreover, the deployment process for these methods is straightforward. By leveraging these advantages of our methods, we create two combined methods. One of these methods is an ensemble of all the proposed techniques, which not only provides better defense performance than PGD-based AT but also enhances the generalization ability of TSC models. Moreover, the computational resources required for our ensemble are less than one-third of those required for PGD-based AT. These methods advance robust TSC in data mining. Furthermore, as foundation models are increasingly explored for time series feature learning, our work provides insights into integrating data augmentation-based adversarial defense with large-scale pre-trained models in future research.","authors":["Yi Han (Independent Researcher","Australia)"],"url":"https://arxiv.org/abs/2505.02073"}
{"created":"2025-05-06","title":"Learning Local Causal World Models with State Space Models and Attention","abstract":"World modelling, i.e. building a representation of the rules that govern the world so as to predict its evolution, is an essential ability for any agent interacting with the physical world. Despite their impressive performance, many solutions fail to learn a causal representation of the environment they are trying to model, which would be necessary to gain a deep enough understanding of the world to perform complex tasks. With this work, we aim to broaden the research in the intersection of causality theory and neural world modelling by assessing the potential for causal discovery of the State Space Model (SSM) architecture, which has been shown to have several advantages over the widespread Transformer. We show empirically that, compared to an equivalent Transformer, a SSM can model the dynamics of a simple environment and learn a causal model at the same time with equivalent or better performance, thus paving the way for further experiments that lean into the strength of SSMs and further enhance them with causal awareness.","authors":["Francesco Petri","Luigi Asprino","Aldo Gangemi"],"url":"https://arxiv.org/abs/2505.02074"}
{"created":"2025-05-06","title":"Benchmarking Feature Upsampling Methods for Vision Foundation Models using Interactive Segmentation","abstract":"Vision Foundation Models (VFMs) are large-scale, pre-trained models that serve as general-purpose backbones for various computer vision tasks. As VFMs' popularity grows, there is an increasing interest in understanding their effectiveness for dense prediction tasks. However, VFMs typically produce low-resolution features, limiting their direct applicability in this context. One way to tackle this limitation is by employing a task-agnostic feature upsampling module that refines VFM features resolution. To assess the effectiveness of this approach, we investigate Interactive Segmentation (IS) as a novel benchmark for evaluating feature upsampling methods on VFMs. Due to its inherent multimodal input, consisting of an image and a set of user-defined clicks, as well as its dense mask output, IS creates a challenging environment that demands comprehensive visual scene understanding. Our benchmarking experiments show that selecting appropriate upsampling strategies significantly improves VFM features quality. The code is released at https://github.com/havrylovv/iSegProbe","authors":["Volodymyr Havrylov","Haiwen Huang","Dan Zhang","Andreas Geiger"],"url":"https://arxiv.org/abs/2505.02075"}
{"created":"2025-05-06","title":"Leveraging LLM Agents and Digital Twins for Fault Handling in Process Plants","abstract":"Advances in Automation and Artificial Intelligence continue to enhance the autonomy of process plants in handling various operational scenarios. However, certain tasks, such as fault handling, remain challenging, as they rely heavily on human expertise. This highlights the need for systematic, knowledge-based methods. To address this gap, we propose a methodological framework that integrates Large Language Model (LLM) agents with a Digital Twin environment. The LLM agents continuously interpret system states and initiate control actions, including responses to unexpected faults, with the goal of returning the system to normal operation. In this context, the Digital Twin acts both as a structured repository of plant-specific engineering knowledge for agent prompting and as a simulation platform for the systematic validation and verification of the generated corrective control actions. The evaluation using a mixing module of a process plant demonstrates that the proposed framework is capable not only of autonomously controlling the mixing module, but also of generating effective corrective actions to mitigate a pipe clogging with only a few reprompts.","authors":["Milapji Singh Gill","Javal Vyas","Artan Markaj","Felix Gehlhoff","Mehmet Mercang\\\"oz"],"url":"https://arxiv.org/abs/2505.02076"}
{"created":"2025-05-06","title":"Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents","abstract":"Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \\textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts.","authors":["Christian Schroeder de Witt"],"url":"https://arxiv.org/abs/2505.02077"}
{"created":"2025-05-06","title":"LecEval: An Automated Metric for Multimodal Knowledge Acquisition in Multimedia Learning","abstract":"Evaluating the quality of slide-based multimedia instruction is challenging. Existing methods like manual assessment, reference-based metrics, and large language model evaluators face limitations in scalability, context capture, or bias. In this paper, we introduce LecEval, an automated metric grounded in Mayer's Cognitive Theory of Multimedia Learning, to evaluate multimodal knowledge acquisition in slide-based learning. LecEval assesses effectiveness using four rubrics: Content Relevance (CR), Expressive Clarity (EC), Logical Structure (LS), and Audience Engagement (AE). We curate a large-scale dataset of over 2,000 slides from more than 50 online course videos, annotated with fine-grained human ratings across these rubrics. A model trained on this dataset demonstrates superior accuracy and adaptability compared to existing metrics, bridging the gap between automated and human assessments. We release our dataset and toolkits at https://github.com/JoylimJY/LecEval.","authors":["Joy Lim Jia Yin","Daniel Zhang-Li","Jifan Yu","Haoxuan Li","Shangqing Tu","Yuanchun Wang","Zhiyuan Liu","Huiqin Liu","Lei Hou","Juanzi Li","Bin Xu"],"url":"https://arxiv.org/abs/2505.02078"}
{"created":"2025-05-06","title":"HandOcc: NeRF-based Hand Rendering with Occupancy Networks","abstract":"We propose HandOcc, a novel framework for hand rendering based upon occupancy. Popular rendering methods such as NeRF are often combined with parametric meshes to provide deformable hand models. However, in doing so, such approaches present a trade-off between the fidelity of the mesh and the complexity and dimensionality of the parametric model. The simplicity of parametric mesh structures is appealing, but the underlying issue is that it binds methods to mesh initialization, making it unable to generalize to objects where a parametric model does not exist. It also means that estimation is tied to mesh resolution and the accuracy of mesh fitting. This paper presents a pipeline for meshless 3D rendering, which we apply to the hands. By providing only a 3D skeleton, the desired appearance is extracted via a convolutional model. We do this by exploiting a NeRF renderer conditioned upon an occupancy-based representation. The approach uses the hand occupancy to resolve hand-to-hand interactions further improving results, allowing fast rendering, and excellent hand appearance transfer. On the benchmark InterHand2.6M dataset, we achieved state-of-the-art results.","authors":["Maksym Ivashechkin","Oscar Mendez","Richard Bowden"],"url":"https://arxiv.org/abs/2505.02079"}
{"created":"2025-05-06","title":"Simulation Based Control Architecture Using Webots and Simulink","abstract":"This paper presents a simulation based control architecture that integrates Webots and Simulink for the development and testing of robotic systems. Using Webots for 3D physics based simulation and Simulink for control system design, real time testing and controller validation are achieved efficiently. The proposed approach aims to reduce hardware in the loop dependency in early development stages, offering a cost effective and modular control framework for academic, industrial, and robotics applications.","authors":["Harun Kurt","Ahmet Cayir","Kadir Erkan"],"url":"https://arxiv.org/abs/2505.02081"}
{"created":"2025-05-06","title":"Performance Characterization of Containers in Edge Computing","abstract":"This paper presents an empirical evaluation of container-based virtualization on embedded operating systems commonly used in Internet of Things (IoT) deployments. Focusing on platforms like the Raspberry Pi, we investigate the feasibility and performance implications of deploying Docker containers in resource-constrained edge environments. Our study employs both microbenchmarks (CPU, memory, and network profiling) and macrobenchmarks (AI-driven inference, sensor IO workloads) to capture a comprehensive view of system behavior. The analysis is conducted on a custom-built physical testbed comprising Raspberry Pi devices equipped with environmental sensors and camera modules, enabling real-time deployment and measurement of representative IoT workloads. Through quantitative analysis across a diverse suite of IoT tasks and real-time application services, we identify key overheads introduced by containerization and characterize challenges specific to embedded IoT contexts, including limited hardware resources, cold-start delays, and suboptimal IO handling. Performance metrics include CPU utilization, memory faults, cache misses, network throughput, and latency. Our findings highlight trade-offs between isolation and efficiency and offer insights for optimizing container configurations to meet the real-time and reliability requirements of edge computing applications.","authors":["Ragini Gupta","Klara Nahrstedt"],"url":"https://arxiv.org/abs/2505.02082"}
{"created":"2025-05-06","title":"A Deep Learning Scheme of Electromagnetic Scattering From Scatterers With Incomplete Profiles","abstract":"A deep learning scheme is proposed to solve the electromagnetic (EM) scattering problems where the profile of the dielectric scatterer of interest is incomplete. As a compensation, a limited amount of scattering data is provided, which is in principle containing sufficient information associated with the missing part of the profile. The existing solvers can hardly realize the compensation if the known part of the profile and the scattering data are combined straightforwardly. On one hand, the well-developed forward solvers have no mechanism to accept the scattering data, which can recover the unknown part of the profile if properly used. On the other hand, the existing solvers for inverse problems cannot retrieve the complete profile with an acceptable accuracy from the limited amount of scattering data, even when the available part of the profile can be fed into the solvers. This work aims to handle the difficulty. To this end, the EM forward scattering from an incompletely known dielectric scatterer is derived. A scheme based on DL is then proposed where the forward and inverse scattering problems are solved simultaneously. Numerical experiments are conducted to demonstrate the performance of the proposed DL-based scheme for both two-dimensional (2-D) and three-dimensional (3-D) EM scattering problems.","authors":["Ji-Yuan Wang","Xin-Yue Lou","Liang Zhang","Yun-Chuan Wang","Xiao-Min Pan"],"url":"https://arxiv.org/abs/2505.02086"}
{"created":"2025-05-06","title":"Retrieval-augmented in-context learning for multimodal large language models in disease classification","abstract":"Objectives: We aim to dynamically retrieve informative demonstrations, enhancing in-context learning in multimodal large language models (MLLMs) for disease classification.","authors":["Zaifu Zhan","Shuang Zhou","Xiaoshan Zhou","Yongkang Xiao","Jun Wang","Jiawen Deng","He Zhu","Yu Hou","Rui Zhang"],"url":"https://arxiv.org/abs/2505.02087"}
{"created":"2025-05-06","title":"LLM-OptiRA: LLM-Driven Optimization of Resource Allocation for Non-Convex Problems in Wireless Communications","abstract":"Solving non-convex resource allocation problems poses significant challenges in wireless communication systems, often beyond the capability of traditional optimization techniques. To address this issue, we propose LLM-OptiRA, the first framework that leverages large language models (LLMs) to automatically detect and transform non-convex components into solvable forms, enabling fully automated resolution of non-convex resource allocation problems in wireless communication systems. LLM-OptiRA not only simplifies problem-solving by reducing reliance on expert knowledge, but also integrates error correction and feasibility validation mechanisms to ensure robustness. Experimental results show that LLM-OptiRA achieves an execution rate of 96% and a success rate of 80% on GPT-4, significantly outperforming baseline approaches in complex optimization tasks across diverse scenarios.","authors":["Xinyue Peng","Yanming Liu","Yihan Cang","Chaoqun Cao","Ming Chen"],"url":"https://arxiv.org/abs/2505.02091"}
{"created":"2025-05-06","title":"A Deep Learning-Aided Approach for Estimating Field Permeability Map by Fusing Well Logs, Well Tests, and Seismic Data","abstract":"Obtaining reliable permeability maps of oil reservoirs is crucial for building a robust and accurate reservoir simulation model and, therefore, designing effective recovery strategies. This problem, however, remains challenging, as it requires the integration of various data sources by experts from different disciplines. Moreover, there are no sources to provide direct information about the inter-well space. In this work, a new method based on the data-fusion approach is proposed for predicting two-dimensional permeability maps on the whole reservoir area. This method utilizes non-parametric regression with a custom kernel shape accounting for different data sources: well logs, well tests, and seismics. A convolutional neural network is developed to process seismic data and then incorporate it with other sources. A multi-stage data fusion procedure helps to artificially increase the training dataset for the seismic interpretation model and finally to construct the adequate permeability map. The proposed methodology of permeability map construction from different sources was tested on a real oil reservoir located in Western Siberia. The results demonstrate that the developed map perfectly corresponds to the permeability estimations in the wells, and the inter-well space permeability predictions are considerably improved through the incorporation of the seismic data.","authors":["Grigoriy Shutov","Viktor Duplyakov","Shadfar Davoodi","Anton Morozov","Dmitriy Popkov","Kirill Pavlenko","Albert Vainshtein","Viktor Kotezhekov","Sergey Kaygorodov","Boris Belozerov","Mars M Khasanov","Vladimir Vanovskiy","Andrei Osiptsov","Evgeny Burnaev"],"url":"https://arxiv.org/abs/2505.02093"}
{"created":"2025-05-06","title":"SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations","abstract":"We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.","authors":["Runyi Yu","Yinhuai Wang","Qihan Zhao","Hok Wai Tsui","Jingbo Wang","Ping Tan","Qifeng Chen"],"url":"https://arxiv.org/abs/2505.02094"}
{"created":"2025-05-06","title":"TeMTG: Text-Enhanced Multi-Hop Temporal Graph Modeling for Audio-Visual Video Parsing","abstract":"Audio-Visual Video Parsing (AVVP) task aims to parse the event categories and occurrence times from audio and visual modalities in a given video. Existing methods usually focus on implicitly modeling audio and visual features through weak labels, without mining semantic relationships for different modalities and explicit modeling of event temporal dependencies. This makes it difficult for the model to accurately parse event information for each segment under weak supervision, especially when high similarity between segmental modal features leads to ambiguous event boundaries. Hence, we propose a multimodal optimization framework, TeMTG, that combines text enhancement and multi-hop temporal graph modeling. Specifically, we leverage pre-trained multimodal models to generate modality-specific text embeddings, and fuse them with audio-visual features to enhance the semantic representation of these features. In addition, we introduce a multi-hop temporal graph neural network, which explicitly models the local temporal relationships between segments, capturing the temporal continuity of both short-term and long-range events. Experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance in multiple key indicators in the LLP dataset.","authors":["Yaru Chen","Peiliang Zhang","Fei Li","Faegheh Sardari","Ruohao Guo","Zhenbo Li","Wenwu Wang"],"url":"https://arxiv.org/abs/2505.02096"}
{"created":"2025-05-06","title":"MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents","abstract":"Recently, large language model based (LLM-based) agents have been widely applied across various fields. As a critical part, their memory capabilities have captured significant interest from both industrial and academic communities. Despite the proposal of many advanced memory models in recent research, however, there remains a lack of unified implementations under a general framework. To address this issue, we develop a unified and modular library for developing advanced memory models of LLM-based agents, called MemEngine. Based on our framework, we implement abundant memory models from recent research works. Additionally, our library facilitates convenient and extensible memory development, and offers user-friendly and pluggable memory usage. For benefiting our community, we have made our project publicly available at https://github.com/nuster1128/MemEngine.","authors":["Zeyu Zhang","Quanyu Dai","Xu Chen","Rui Li","Zhongyang Li","Zhenhua Dong"],"url":"https://arxiv.org/abs/2505.02099"}
{"created":"2025-05-06","title":"Deep Representation Learning for Electronic Design Automation","abstract":"Representation learning has become an effective technique utilized by electronic design automation (EDA) algorithms, which leverage the natural representation of workflow elements as images, grids, and graphs. By addressing challenges related to the increasing complexity of circuits and stringent power, performance, and area (PPA) requirements, representation learning facilitates the automatic extraction of meaningful features from complex data formats, including images, grids, and graphs. This paper examines the application of representation learning in EDA, covering foundational concepts and analyzing prior work and case studies on tasks that include timing prediction, routability analysis, and automated placement. Key techniques, including image-based methods, graph-based approaches, and hybrid multimodal solutions, are presented to illustrate the improvements provided in routing, timing, and parasitic prediction. The provided advancements demonstrate the potential of representation learning to enhance efficiency, accuracy, and scalability in current integrated circuit design flows.","authors":["Pratik Shrestha","Saran Phatharodom","Alec Aversa","David Blankenship","Zhengfeng Wu","Ioannis Savidis"],"url":"https://arxiv.org/abs/2505.02105"}
{"created":"2025-05-06","title":"SignSplat: Rendering Sign Language via Gaussian Splatting","abstract":"State-of-the-art approaches for conditional human body rendering via Gaussian splatting typically focus on simple body motions captured from many views. This is often in the context of dancing or walking. However, for more complex use cases, such as sign language, we care less about large body motion and more about subtle and complex motions of the hands and face. The problems of building high fidelity models are compounded by the complexity of capturing multi-view data of sign. The solution is to make better use of sequence data, ensuring that we can overcome the limited information from only a few views by exploiting temporal variability. Nevertheless, learning from sequence-level data requires extremely accurate and consistent model fitting to ensure that appearance is consistent across complex motions. We focus on how to achieve this, constraining mesh parameters to build an accurate Gaussian splatting framework from few views capable of modelling subtle human motion. We leverage regularization techniques on the Gaussian parameters to mitigate overfitting and rendering artifacts. Additionally, we propose a new adaptive control method to densify Gaussians and prune splat points on the mesh surface. To demonstrate the accuracy of our approach, we render novel sequences of sign language video, building on neural machine translation approaches to sign stitching. On benchmark datasets, our approach achieves state-of-the-art performance; and on highly articulated and complex sign language motion, we significantly outperform competing approaches.","authors":["Maksym Ivashechkin","Oscar Mendez","Richard Bowden"],"url":"https://arxiv.org/abs/2505.02108"}
{"created":"2025-05-06","title":"Unaligned RGB Guided Hyperspectral Image Super-Resolution with Spatial-Spectral Concordance","abstract":"Hyperspectral images super-resolution aims to improve the spatial resolution, yet its performance is often limited at high-resolution ratios. The recent adoption of high-resolution reference images for super-resolution is driven by the poor spatial detail found in low-resolution HSIs, presenting it as a favorable method. However, these approaches cannot effectively utilize information from the reference image, due to the inaccuracy of alignment and its inadequate interaction between alignment and fusion modules. In this paper, we introduce a Spatial-Spectral Concordance Hyperspectral Super-Resolution (SSC-HSR) framework for unaligned reference RGB guided HSI SR to address the issues of inaccurate alignment and poor interactivity of the previous approaches. Specifically, to ensure spatial concordance, i.e., align images more accurately across resolutions and refine textures, we construct a Two-Stage Image Alignment with a synthetic generation pipeline in the image alignment module, where the fine-tuned optical flow model can produce a more accurate optical flow in the first stage and warp model can refine damaged textures in the second stage. To enhance the interaction between alignment and fusion modules and ensure spectral concordance during reconstruction, we propose a Feature Aggregation module and an Attention Fusion module. In the feature aggregation module, we introduce an Iterative Deformable Feature Aggregation block to achieve significant feature matching and texture aggregation with the fusion multi-scale results guidance, iteratively generating learnable offset. Besides, we introduce two basic spectral-wise attention blocks in the attention fusion module to model the inter-spectra interactions. Extensive experiments on three natural or remote-sensing datasets show that our method outperforms state-of-the-art approaches on both quantitative and qualitative evaluations.","authors":["Yingkai Zhang","Zeqiang Lai","Tao Zhang","Ying Fu","Chenghu Zhou"],"url":"https://arxiv.org/abs/2505.02109"}
{"created":"2025-05-06","title":"Eterna is Solved","abstract":"RNA design consists of discovering a nucleotide sequence that folds into a target secondary structure. It is useful for synthetic biology, medicine, and nanotechnology. We propose Montparnasse, a Multi Objective Generalized Nested Rollout Policy Adaptation with Limited Repetition (MOGNRPALR) RNA design algorithm. It solves the Eterna benchmark.","authors":["Tristan Cazenave"],"url":"https://arxiv.org/abs/2505.02110"}
{"created":"2025-05-06","title":"Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets","abstract":"This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations. Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method not only significantly outperforms recent rationalization methods, but also achieves comparable or even better results than a representative LLM (llama3.1-8b-instruct).","authors":["Wei Liu","Zhongyu Niu","Lang Gao","Zhiying Deng","Jun Wang","Haozhao Wang","Ruixuan Li"],"url":"https://arxiv.org/abs/2505.02118"}
{"created":"2025-05-06","title":"Tricolore: Multi-Behavior User Profiling for Enhanced Candidate Generation in Recommender Systems","abstract":"Online platforms aggregate extensive user feedback across diverse behaviors, providing a rich source for enhancing user engagement. Traditional recommender systems, however, typically optimize for a single target behavior and represent user preferences with a single vector, limiting their ability to handle multiple important behaviors or optimization objectives. This conventional approach also struggles to capture the full spectrum of user interests, resulting in a narrow item pool during candidate generation. To address these limitations, we present Tricolore, a versatile multi-vector learning framework that uncovers connections between different behavior types for more robust candidate generation. Tricolore's adaptive multi-task structure is also customizable to specific platform needs. To manage the variability in sparsity across behavior types, we incorporate a behavior-wise multi-view fusion module that dynamically enhances learning. Moreover, a popularity-balanced strategy ensures the recommendation list balances accuracy with item popularity, fostering diversity and improving overall performance. Extensive experiments on public datasets demonstrate Tricolore's effectiveness across various recommendation scenarios, from short video platforms to e-commerce. By leveraging a shared base embedding strategy, Tricolore also significantly improves the performance for cold-start users. The source code is publicly available at: https://github.com/abnering/Tricolore.","authors":["Xiao Zhou","Zhongxiang Zhao","Hanze Guo"],"url":"https://arxiv.org/abs/2505.02120"}
{"created":"2025-05-06","title":"Overview of AI Grading of Physics Olympiad Exams","abstract":"Automatically grading the diverse range of question types in high school physics problem is a challenge that requires automated grading techniques from different fields. We report the findings of a Systematic Literature Review of potential physics grading techniques. We propose a multi-modal AI grading framework to address these challenges and examine our framework in light of Australia's AI Ethical Principles.","authors":["Lachlan McGinness"],"url":"https://arxiv.org/abs/2505.02121"}
{"created":"2025-05-06","title":"DriveAgent: Multi-Agent Structured Reasoning with LLM and Multimodal Sensor Fusion for Autonomous Driving","abstract":"We introduce DriveAgent, a novel multi-agent autonomous driving framework that leverages large language model (LLM) reasoning combined with multimodal sensor fusion to enhance situational understanding and decision-making. DriveAgent uniquely integrates diverse sensor modalities-including camera, LiDAR, GPS, and IMU-with LLM-driven analytical processes structured across specialized agents. The framework operates through a modular agent-based pipeline comprising four principal modules: (i) a descriptive analysis agent identifying critical sensor data events based on filtered timestamps, (ii) dedicated vehicle-level analysis conducted by LiDAR and vision agents that collaboratively assess vehicle conditions and movements, (iii) environmental reasoning and causal analysis agents explaining contextual changes and their underlying mechanisms, and (iv) an urgency-aware decision-generation agent prioritizing insights and proposing timely maneuvers. This modular design empowers the LLM to effectively coordinate specialized perception and reasoning agents, delivering cohesive, interpretable insights into complex autonomous driving scenarios. Extensive experiments on challenging autonomous driving datasets demonstrate that DriveAgent is achieving superior performance on multiple metrics against baseline methods. These results validate the efficacy of the proposed LLM-driven multi-agent sensor fusion framework, underscoring its potential to substantially enhance the robustness and reliability of autonomous driving systems.","authors":["Xinmeng Hou","Wuqi Wang","Long Yang","Hao Lin","Jinglun Feng","Haigen Min","Xiangmo Zhao"],"url":"https://arxiv.org/abs/2505.02123"}
{"created":"2025-05-06","title":"GRAIL: Graph Edit Distance and Node Alignment Using LLM-Generated Code","abstract":"Graph Edit Distance (GED) is a widely used metric for measuring similarity between two graphs. Computing the optimal GED is NP-hard, leading to the development of various neural and non-neural heuristics. While neural methods have achieved improved approximation quality compared to non-neural approaches, they face significant challenges: (1) They require large amounts of ground truth data, which is itself NP-hard to compute. (2) They operate as black boxes, offering limited interpretability. (3) They lack cross-domain generalization, necessitating expensive retraining for each new dataset. We address these limitations with GRAIL, introducing a paradigm shift in this domain. Instead of training a neural model to predict GED, GRAIL employs a novel combination of large language models (LLMs) and automated prompt tuning to generate a program that is used to compute GED. This shift from predicting GED to generating programs imparts various advantages, including end-to-end interpretability and an autonomous self-evolutionary learning mechanism without ground-truth supervision. Extensive experiments on seven datasets confirm that GRAIL not only surpasses state-of-the-art GED approximation methods in prediction quality but also achieves robust cross-domain generalization across diverse graph distributions.","authors":["Samidha Verma","Arushi Goyal","Ananya Mathur","Ankit Anand","Sayan Ranu"],"url":"https://arxiv.org/abs/2505.02124"}
{"created":"2025-05-06","title":"GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction","abstract":"Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.","authors":["Zhihao Tang","Shenghao Yang","Hongtao Zhang","Mingbo Zhao"],"url":"https://arxiv.org/abs/2505.02126"}
{"created":"2025-05-06","title":"Subspace Aggregation Query and Index Generation for Multidimensional Resource Space Mode","abstract":"Organizing resources in a multidimensional classification space is an approach to efficiently managing and querying large-scale resources. This paper defines an aggregation query on subspace defined by a range on the partial order on coordinate tree at each dimension, where each point contains resources aggregated along the paths of partial order relations on the points so that aggregated resources at each point within the subspace can be measured, ranked and selected. To efficiently locate non-empty points in a large subspace, an approach to generating graph index is proposed to build inclusion links with partial order relations on coordinates of dimensions to enable a subspace query to reach non-empty points by following indexing links and aggregate resources along indexing paths back to their super points. Generating such an index is costly as the number of children of an index node can be very large so that the total number of indexing nodes is unbounded. The proposed approach adopts the following strategies to reduce the cost: (1) adding intersection links between two indexing nodes, which can better reduce query processing costs while controlling the number of nodes of the graph index; (2) intersection links are added between two nodes according to the probabilistic distribution calculated for estimating the costs of adding intersection between two nodes; (3) coordinates at one dimension having more resources are split by coordinates at another dimension to balance the number of resources hold by indexing nodes; and, (4) short-cut links are added between sibling coordinates of coordinate trees to make an efficient query on linear order coordinates. Analysis and experiments verified the effectiveness of the generated index in supporting subspace aggregation query. This work makes significant contributions to the development of data model based on multi-dimensional classification.","authors":["Xiaoping Sun","Hai Zhuge"],"url":"https://arxiv.org/abs/2505.02129"}
{"created":"2025-05-06","title":"Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data","abstract":"Attention mechanisms are critical to the success of large language models (LLMs), driving significant advancements in multiple fields. However, for graph-structured data, which requires emphasis on topological connections, they fall short compared to message-passing mechanisms on fixed links, such as those employed by Graph Neural Networks (GNNs). This raises a question: ``Does attention fail for graphs in natural language settings?'' Motivated by these observations, we embarked on an empirical study from the perspective of attention mechanisms to explore how LLMs process graph-structured data. The goal is to gain deeper insights into the attention behavior of LLMs over graph structures. We uncovered unique phenomena regarding how LLMs apply attention to graph-structured data and analyzed these findings to improve the modeling of such data by LLMs. The primary findings of our research are: 1) While LLMs can recognize graph data and capture text-node interactions, they struggle to model inter-node relationships within graph structures due to inherent architectural constraints. 2) The attention distribution of LLMs across graph nodes does not align with ideal structural patterns, indicating a failure to adapt to graph topology nuances. 3) Neither fully connected attention nor fixed connectivity is optimal; each has specific limitations in its application scenarios. Instead, intermediate-state attention windows improve LLM training performance and seamlessly transition to fully connected windows during inference. Source code: \\href{https://github.com/millioniron/LLM_exploration}{LLM4Exploration}","authors":["Zhong Guan","Likang Wu","Hongke Zhao","Ming He","Jianpin Fan"],"url":"https://arxiv.org/abs/2505.02130"}
{"created":"2025-05-06","title":"Compact difference method for Euler-Bernoulli beams and plates with nonlinear nonlocal strong damping","abstract":"We investigate the numerical approximation to the Euler-Bernoulli (E-B) beams and plates with nonlinear nonlocal strong damping, which describes the damped mechanical behavior of beams and plates in real applications. We discretize the damping term by the composite Simpson's rule and the six-point Simpson's formula in the beam and plate problems, respectively, and then construct the fully discrete compact difference scheme for these problems. To account for the nonlinear-nonlocal term, we design several novel discrete norms to facilitate the error estimates of the damping term and the numerical scheme. The stability, convergence, and energy dissipation properties of the proposed scheme are proved, and numerical experiments are carried out to substantiate the theoretical findings.","authors":["Tao Guo","Yiqun Li","Wenlin Qiu"],"url":"https://arxiv.org/abs/2505.02132"}
{"created":"2025-05-06","title":"Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency","abstract":"The use of large language models (LLMs) for automated code generation has emerged as a significant focus within AI research. As these pretrained models continue to evolve, their ability to understand and generate complex code structures has opened new possibilities for automating intricate programming tasks for the sake of accurate code generation. Although contemporary foundational models demonstrate promoting results, researchers continue to explore optimal post-training strategies to enhance code quality. These include supervised fine-tuning, retrieval-augmented generation (RAG), debugging, and many others. In this paper, we combine two widely used approaches namely multi-agent collaboration and runtime execution information-based debugging, for improving code generation functionality, reliability, and practical applicability. We perform an empirical study in order to extend the evaluation of the individual strategies as well as the proposed composition of the activities of both strategies. Our study use 19 LLMs to examines the performance of individual and the proposed strategies, offering comprehensive insights into how different programming activities compositions and training paradigms influence code generation effectiveness. In particular, we implement a chained system that combines both strategies to assess their combined impact on functional accuracy, code reliability, and generation latency using two benchmark datasets commonly used for code generation. Our findings provide valuable insights for organizations seeking robust AI-driven coding solutions by guiding them in selecting models that can better adapt to complex post-training strategies, ultimately fostering the adoption of more effective and reliable code generation technologies.","authors":["Nazmus Ashrafi","Salah Bouktif","Mohammed Mediani"],"url":"https://arxiv.org/abs/2505.02133"}
{"created":"2025-05-06","title":"HiLLIE: Human-in-the-Loop Training for Low-Light Image Enhancement","abstract":"Developing effective approaches to generate enhanced results that align well with human visual preferences for high-quality well-lit images remains a challenge in low-light image enhancement (LLIE). In this paper, we propose a human-in-the-loop LLIE training framework that improves the visual quality of unsupervised LLIE model outputs through iterative training stages, named HiLLIE. At each stage, we introduce human guidance into the training process through efficient visual quality annotations of enhanced outputs. Subsequently, we employ a tailored image quality assessment (IQA) model to learn human visual preferences encoded in the acquired labels, which is then utilized to guide the training process of an enhancement model. With only a small amount of pairwise ranking annotations required at each stage, our approach continually improves the IQA model's capability to simulate human visual assessment of enhanced outputs, thus leading to visually appealing LLIE results. Extensive experiments demonstrate that our approach significantly improves unsupervised LLIE model performance in terms of both quantitative and qualitative performance. The code and collected ranking dataset will be available at https://github.com/LabShuHangGU/HiLLIE.","authors":["Xiaorui Zhao","Xinyue Zhou","Peibei Cao","Junyu Lou","Shuhang Gu"],"url":"https://arxiv.org/abs/2505.02134"}
{"created":"2025-05-06","title":"Efficient Multivariate Time Series Forecasting via Calibrated Language Models with Privileged Knowledge Distillation","abstract":"Multivariate time series forecasting (MTSF) endeavors to predict future observations given historical data, playing a crucial role in time series data management systems. With advancements in large language models (LLMs), recent studies employ textual prompt tuning to infuse the knowledge of LLMs into MTSF. However, the deployment of LLMs often suffers from low efficiency during the inference phase. To address this problem, we introduce TimeKD, an efficient MTSF framework that leverages the calibrated language models and privileged knowledge distillation. TimeKD aims to generate high-quality future representations from the proposed cross-modality teacher model and cultivate an effective student model. The cross-modality teacher model adopts calibrated language models (CLMs) with ground truth prompts, motivated by the paradigm of Learning Under Privileged Information (LUPI). In addition, we design a subtractive cross attention (SCA) mechanism to refine these representations. To cultivate an effective student model, we propose an innovative privileged knowledge distillation (PKD) mechanism including correlation and feature distillation. PKD enables the student to replicate the teacher's behavior while minimizing their output discrepancy. Extensive experiments on real data offer insight into the effectiveness, efficiency, and scalability of the proposed TimeKD.","authors":["Chenxi Liu","Shaowen Zhou","Hao Miao","Qianxiong Xu","Cheng Long","Ziyue Li","Rui Zhao"],"url":"https://arxiv.org/abs/2505.02138"}
{"created":"2025-05-06","title":"Representation Learning of Limit Order Book: A Comprehensive Study and Benchmarking","abstract":"The Limit Order Book (LOB), the mostly fundamental data of the financial market, provides a fine-grained view of market dynamics while poses significant challenges in dealing with the esteemed deep models due to its strong autocorrelation, cross-feature constrains, and feature scale disparity. Existing approaches often tightly couple representation learning with specific downstream tasks in an end-to-end manner, failed to analyze the learned representations individually and explicitly, limiting their reusability and generalization. This paper conducts the first systematic comparative study of LOB representation learning, aiming to identify the effective way of extracting transferable, compact features that capture essential LOB properties. We introduce LOBench, a standardized benchmark with real China A-share market data, offering curated datasets, unified preprocessing, consistent evaluation metrics, and strong baselines. Extensive experiments validate the sufficiency and necessity of LOB representations for various downstream tasks and highlight their advantages over both the traditional task-specific end-to-end models and the advanced representation learning models for general time series. Our work establishes a reproducible framework and provides clear guidelines for future research. Datasets and code will be publicly available at https://github.com/financial-simulation-lab/LOBench.","authors":["Muyao Zhong","Yushi Lin","Peng Yang"],"url":"https://arxiv.org/abs/2505.02139"}
{"created":"2025-05-06","title":"Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study","abstract":"Despite significant advances in long-context reasoning by large language models (LLMs), primarily through Online Reinforcement Learning (RL) methods, these approaches incur substantial computational costs and complexity. In contrast, simpler and more economical Offline RL methods remain underexplored. To address this gap, we investigate the effectiveness of Offline RL methods, specifically Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, in enhancing the reasoning capabilities of LLMs. Extensive experiments across multiple reasoning benchmarks demonstrate that these simpler Offline RL methods substantially improve model performance, achieving an average enhancement of 3.3\\%, with a particularly notable increase of 10.1\\% on the challenging Arena-Hard benchmark. Furthermore, we analyze DPO's sensitivity to output length, emphasizing that increasing reasoning length should align with semantic richness, as indiscriminate lengthening may adversely affect model performance. We provide comprehensive descriptions of our data processing and training methodologies, offering empirical evidence and practical insights for developing more cost-effective Offline RL approaches.","authors":["Xiaoyu Tian","Sitong Zhao","Haotian Wang","Shuaiting Chen","Yiping Peng","Yunjie Ji","Han Zhao","Xiangang Li"],"url":"https://arxiv.org/abs/2505.02142"}
{"created":"2025-05-06","title":"VECSR: Virtually Embodied Common Sense Reasoning System","abstract":"The development of autonomous agents has seen a revival of enthusiasm due to the emergence of LLMs, such as GPT-4o. Deploying these agents in environments where they coexist with humans (e.g., as domestic assistants) requires special attention to trustworthiness and explainability. However, the use of LLMs and other deep learning models still does not resolve these key issues. Deep learning systems may hallucinate, be unable to justify their decisions as black boxes, or perform badly on unseen scenarios. In this work, we propose the use of s(CASP), a goal-directed common sense reasoner based on Answer Set Programming, to break down the high-level tasks of an autonomous agent into mid-level instructions while justifying the selection of these instructions. To validate its use in real applications we present a framework that integrates the reasoner into the VirtualHome simulator and compares its accuracy with GPT-4o, running some of the real use cases available in the domestic environments of VirtualHome. Additionally, since experiments with VirtualHome have shown the need to reduce the response time (which increases as the agent's decision space grows), we have proposed and evaluated a series of optimizations based on program analysis that exploit the advantages of the top-down execution of s(CASP).","authors":["Alexis R. Tudor","Joaqu\\'in Arias","Gopal Gupta"],"url":"https://arxiv.org/abs/2505.02144"}
{"created":"2025-05-06","title":"QiMeng-Xpiler: Transcompiling Tensor Programs for Deep Learning Systems with a Neural-Symbolic Approach","abstract":"Heterogeneous deep learning systems (DLS) such as GPUs and ASICs have been widely deployed in industrial data centers, which requires to develop multiple low-level tensor programs for different platforms. An attractive solution to relieve the programming burden is to transcompile the legacy code of one platform to others. However, current transcompilation techniques struggle with either tremendous manual efforts or functional incorrectness, rendering \"Write Once, Run Anywhere\" of tensor programs an open question.","authors":["Shouyang Dong","Yuanbo Wen","Jun Bi","Di Huang","Jiaming Guo","Jianxing Xu","Ruibai Xu","Xinkai Song","Yifan Hao","Xuehai Zhou","Tianshi Chen","Qi Guo","Yunji Chen"],"url":"https://arxiv.org/abs/2505.02146"}
{"created":"2025-05-06","title":"Local Herb Identification Using Transfer Learning: A CNN-Powered Mobile Application for Nepalese Flora","abstract":"Herb classification presents a critical challenge in botanical research, particularly in regions with rich biodiversity such as Nepal. This study introduces a novel deep learning approach for classifying 60 different herb species using Convolutional Neural Networks (CNNs) and transfer learning techniques. Using a manually curated dataset of 12,000 herb images, we developed a robust machine learning model that addresses existing limitations in herb recognition methodologies. Our research employed multiple model architectures, including DenseNet121, 50-layer Residual Network (ResNet50), 16-layer Visual Geometry Group Network (VGG16), InceptionV3, EfficientNetV2, and Vision Transformer (VIT), with DenseNet121 ultimately demonstrating superior performance. Data augmentation and regularization techniques were applied to mitigate overfitting and enhance the generalizability of the model. This work advances herb classification techniques, preserving traditional botanical knowledge and promoting sustainable herb utilization.","authors":["Prajwal Thapa","Mridul Sharma","Jinu Nyachhyon","Yagya Raj Pandeya"],"url":"https://arxiv.org/abs/2505.02147"}
{"created":"2025-05-06","title":"Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving","abstract":"To operate safely, autonomous vehicles (AVs) need to detect and handle unexpected objects or anomalies on the road. While significant research exists for anomaly detection and segmentation in 2D, research progress in 3D is underexplored. Existing datasets lack high-quality multimodal data that are typically found in AVs. This paper presents a novel dataset for anomaly segmentation in driving scenarios. To the best of our knowledge, it is the first publicly available dataset focused on road anomaly segmentation with dense 3D semantic labeling, incorporating both LiDAR and camera data, as well as sequential information to enable anomaly detection across various ranges. This capability is critical for the safe navigation of autonomous vehicles. We adapted and evaluated several baseline models for 3D segmentation, highlighting the challenges of 3D anomaly detection in driving environments. Our dataset and evaluation code will be openly available, facilitating the testing and performance comparison of different approaches.","authors":["Alexey Nekrasov","Malcolm Burdorf","Stewart Worrall","Bastian Leibe","Julie Stephany Berrio Perez"],"url":"https://arxiv.org/abs/2505.02148"}
{"created":"2025-05-06","title":"Large Language Models are overconfident and amplify human bias","abstract":"Large language models (LLMs) are revolutionizing every aspect of society. They are increasingly used in problem-solving tasks to substitute human assessment and reasoning. LLMs are trained on what humans write and thus prone to learn human biases. One of the most widespread human biases is overconfidence. We examine whether LLMs inherit this bias. We automatically construct reasoning problems with known ground truths, and prompt LLMs to assess the confidence in their answers, closely following similar protocols in human experiments. We find that all five LLMs we study are overconfident: they overestimate the probability that their answer is correct between 20% and 60%. Humans have accuracy similar to the more advanced LLMs, but far lower overconfidence. Although humans and LLMs are similarly biased in questions which they are certain they answered correctly, a key difference emerges between them: LLM bias increases sharply relative to humans if they become less sure that their answers are correct. We also show that LLM input has ambiguous effects on human decision making: LLM input leads to an increase in the accuracy, but it more than doubles the extent of overconfidence in the answers.","authors":["Fengfei Sun","Ningke Li","Kailong Wang","Lorenz Goette"],"url":"https://arxiv.org/abs/2505.02151"}
{"created":"2025-05-06","title":"Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions","abstract":"Vision-Language-Action (VLA) models have shown great promise for generalist robotic manipulation in the physical world. However, existing models are restricted to robot observations and text-only instructions, lacking the flexibility of interleaved multimodal instructions enabled by recent advances in foundation models in the digital world. In this paper, we present Interleave-VLA, the first framework capable of comprehending interleaved image-text instructions and directly generating continuous action sequences in the physical world. It offers a flexible, model-agnostic paradigm that extends state-of-the-art VLA models with minimal modifications and strong zero-shot generalization. A key challenge in realizing Interleave-VLA is the absence of large-scale interleaved embodied datasets. To bridge this gap, we develop an automatic pipeline that converts text-only instructions from real-world datasets in Open X-Embodiment into interleaved image-text instructions, resulting in the first large-scale real-world interleaved embodied dataset with 210k episodes. Through comprehensive evaluation on simulation benchmarks and real-robot experiments, we demonstrate that Interleave-VLA offers significant benefits: 1) it improves out-of-domain generalization to unseen objects by 2-3x compared to state-of-the-art baselines, 2) supports flexible task interfaces, and 3) handles diverse user-provided image instructions in a zero-shot manner, such as hand-drawn sketches. We further analyze the factors behind Interleave-VLA's strong zero-shot performance, showing that the interleaved paradigm effectively leverages heterogeneous datasets and diverse instruction images, including those from the Internet, which demonstrates strong potential for scaling up. Our model and dataset will be open-sourced.","authors":["Cunxin Fan","Xiaosong Jia","Yihang Sun","Yixiao Wang","Jianglan Wei","Ziyang Gong","Xiangyu Zhao","Masayoshi Tomizuka","Xue Yang","Junchi Yan","Mingyu Ding"],"url":"https://arxiv.org/abs/2505.02152"}
{"created":"2025-05-06","title":"Interpreting Multilingual and Document-Length Sensitive Relevance Computations in Neural Retrieval Models through Axiomatic Causal Interventions","abstract":"This reproducibility study analyzes and extends the paper \"Axiomatic Causal Interventions for Reverse Engineering Relevance Computation in Neural Retrieval Models,\" which investigates how neural retrieval models encode task-relevant properties such as term frequency. We reproduce key experiments from the original paper, confirming that information on query terms is captured in the model encoding. We extend this work by applying activation patching to Spanish and Chinese datasets and by exploring whether document-length information is encoded in the model as well. Our results confirm that the designed activation patching method can isolate the behavior to specific components and tokens in neural retrieval models. Moreover, our findings indicate that the location of term frequency generalizes across languages and that in later layers, the information for sequence-level tasks is represented in the CLS token. The results highlight the need for further research into interpretability in information retrieval and reproducibility in machine learning research. Our code is available at https://github.com/OliverSavolainen/axiomatic-ir-reproduce.","authors":["Oliver Savolainen","Dur e Najaf Amjad","Roxana Petcu"],"url":"https://arxiv.org/abs/2505.02154"}
{"created":"2025-05-06","title":"Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents","abstract":"Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach","authors":["Minzheng Wang","Yongbin Li","Haobo Wang","Xinghua Zhang","Nan Xu","Bingli Wu","Fei Huang","Haiyang Yu","Wenji Mao"],"url":"https://arxiv.org/abs/2505.02156"}
{"created":"2025-05-06","title":"Small Clips, Big Gains: Learning Long-Range Refocused Temporal Information for Video Super-Resolution","abstract":"Video super-resolution (VSR) can achieve better performance compared to single image super-resolution by additionally leveraging temporal information. In particular, the recurrent-based VSR model exploits long-range temporal information during inference and achieves superior detail restoration. However, effectively learning these long-term dependencies within long videos remains a key challenge. To address this, we propose LRTI-VSR, a novel training framework for recurrent VSR that efficiently leverages Long-Range Refocused Temporal Information. Our framework includes a generic training strategy that utilizes temporal propagation features from long video clips while training on shorter video clips. Additionally, we introduce a refocused intra&amp;inter-frame transformer block which allows the VSR model to selectively prioritize useful temporal information through its attention module while further improving inter-frame information utilization in the FFN module. We evaluate LRTI-VSR on both CNN and transformer-based VSR architectures, conducting extensive ablation studies to validate the contribution of each component. Experiments on long-video test sets demonstrate that LRTI-VSR achieves state-of-the-art performance while maintaining training and computational efficiency.","authors":["Xingyu Zhou","Wei Long","Jingbo Lu","Shiyin Jiang","Weiyi You","Haifeng Wu","Shuhang Gu"],"url":"https://arxiv.org/abs/2505.02159"}
{"created":"2025-05-06","title":"Focus What Matters: Matchability-Based Reweighting for Local Feature Matching","abstract":"Since the rise of Transformers, many semi-dense matching methods have adopted attention mechanisms to extract feature descriptors. However, the attention weights, which capture dependencies between pixels or keypoints, are often learned from scratch. This approach can introduce redundancy and noisy interactions from irrelevant regions, as it treats all pixels or keypoints equally. Drawing inspiration from keypoint selection processes, we propose to first classify all pixels into two categories: matchable and non-matchable. Matchable pixels are expected to receive higher attention weights, while non-matchable ones are down-weighted. In this work, we propose a novel attention reweighting mechanism that simultaneously incorporates a learnable bias term into the attention logits and applies a matchability-informed rescaling to the input value features. The bias term, injected prior to the softmax operation, selectively adjusts attention scores based on the confidence of query-key interactions. Concurrently, the feature rescaling acts post-attention by modulating the influence of each value vector in the final output. This dual design allows the attention mechanism to dynamically adjust both its internal weighting scheme and the magnitude of its output representations. Extensive experiments conducted on three benchmark datasets validate the effectiveness of our method, consistently outperforming existing state-of-the-art approaches.","authors":["Dongyue Li"],"url":"https://arxiv.org/abs/2505.02161"}
{"created":"2025-05-06","title":"Incorporating Legal Structure in Retrieval-Augmented Generation: A Case Study on Copyright Fair Use","abstract":"This paper presents a domain-specific implementation of Retrieval-Augmented Generation (RAG) tailored to the Fair Use Doctrine in U.S. copyright law. Motivated by the increasing prevalence of DMCA takedowns and the lack of accessible legal support for content creators, we propose a structured approach that combines semantic search with legal knowledge graphs and court citation networks to improve retrieval quality and reasoning reliability. Our prototype models legal precedents at the statutory factor level (e.g., purpose, nature, amount, market effect) and incorporates citation-weighted graph representations to prioritize doctrinally authoritative sources. We use Chain-of-Thought reasoning and interleaved retrieval steps to better emulate legal reasoning. Preliminary testing suggests this method improves doctrinal relevance in the retrieval process, laying groundwork for future evaluation and deployment of LLM-based legal assistance tools.","authors":["Justin Ho","Alexandra Colby","William Fisher"],"url":"https://arxiv.org/abs/2505.02164"}
{"created":"2025-05-06","title":"CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation","abstract":"In robotic, task goals can be conveyed through various modalities, such as language, goal images, and goal videos. However, natural language can be ambiguous, while images or videos may offer overly detailed specifications. To tackle these challenges, we introduce CrayonRobo that leverages comprehensive multi-modal prompts that explicitly convey both low-level actions and high-level planning in a simple manner. Specifically, for each key-frame in the task sequence, our method allows for manual or automatic generation of simple and expressive 2D visual prompts overlaid on RGB images. These prompts represent the required task goals, such as the end-effector pose and the desired movement direction after contact. We develop a training strategy that enables the model to interpret these visual-language prompts and predict the corresponding contact poses and movement directions in SE(3) space. Furthermore, by sequentially executing all key-frame steps, the model can complete long-horizon tasks. This approach not only helps the model explicitly understand the task objectives but also enhances its robustness on unseen tasks by providing easily interpretable prompts. We evaluate our method in both simulated and real-world environments, demonstrating its robust manipulation capabilities.","authors":["Xiaoqi Li","Lingyun Xu","Mingxu Zhang","Jiaming Liu","Yan Shen","Iaroslav Ponomarenko","Jiahui Xu","Liang Heng","Siyuan Huang","Shanghang Zhang","Hao Dong"],"url":"https://arxiv.org/abs/2505.02166"}
{"created":"2025-05-06","title":"CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design","abstract":"The rapid advancements of AI rely on the support of ICs. However, the growing complexity of digital ICs makes the traditional IC design process costly and time-consuming. In recent years, AI-assisted IC design methods have demonstrated great potential, but most methods are task-specific or focus solely on the circuit structure in graph format, overlooking other circuit modalities with rich functional information. In this paper, we introduce CircuitFusion, the first multimodal and implementation-aware circuit encoder. It encodes circuits into general representations that support different downstream circuit design tasks. To learn from circuits, we propose to fuse three circuit modalities: hardware code, structural graph, and functionality summary. More importantly, we identify four unique properties of circuits: parallel execution, functional equivalent transformation, multiple design stages, and circuit reusability. Based on these properties, we propose new strategies for both the development and application of CircuitFusion: 1) During circuit preprocessing, utilizing the parallel nature of circuits, we split each circuit into multiple sub-circuits based on sequential-element boundaries, each sub-circuit in three modalities. 2) During CircuitFusion pre-training, we introduce three self-supervised tasks that utilize equivalent transformations both within and across modalities. 3) When applying CircuitFusion to downstream tasks, we propose a new retrieval-augmented inference method, which retrieves similar known circuits as a reference for predictions. It improves fine-tuning performance and even enables zero-shot inference. Evaluated on five different circuit design tasks, CircuitFusion consistently outperforms the SOTA supervised method specifically developed for every single task, demonstrating its generalizability and ability to learn circuits' inherent properties.","authors":["Wenji Fang","Shang Liu","Jing Wang","Zhiyao Xie"],"url":"https://arxiv.org/abs/2505.02168"}
{"created":"2025-05-06","title":"Data-Driven Team Selection in Fantasy Premier League Using Integer Programming and Predictive Modeling Approach","abstract":"Fantasy football is a billion-dollar industry with millions of participants. Constrained by a fixed budget, decision-makers draft a squad whose players are expected to perform well in the upcoming weeks to maximize total points. This paper proposes novel deterministic and robust integer programming models that select the optimal starting eleven and the captain. A new hybrid scoring metric is constructed using an interpretable artificial intelligence framework and underlying match performance data. Several objective functions and estimation techniques are introduced for the programming model. To the best of my knowledge, this is the first study to approach fantasy football through this lens. The models' performance is evaluated using data from the 2023/24 Premier League season. Results indicate that the proposed hybrid method achieved the highest score while maintaining consistent performance. Utilizing the Monte Carlo simulation, the strategic choice of averaging techniques for estimating cost vectors, and the proposed hybrid approach are shown to be effective during the out-of-sample period. This paper also provides a thorough analysis of the optimal formations and players selected by the models, offering valuable insights into effective fantasy football strategies.","authors":["Danial Ramezani"],"url":"https://arxiv.org/abs/2505.02170"}
{"created":"2025-05-06","title":"A New HOPE: Domain-agnostic Automatic Evaluation of Text Chunking","abstract":"Document chunking fundamentally impacts Retrieval-Augmented Generation (RAG) by determining how source materials are segmented before indexing. Despite evidence that Large Language Models (LLMs) are sensitive to the layout and structure of retrieved data, there is currently no framework to analyze the impact of different chunking methods. In this paper, we introduce a novel methodology that defines essential characteristics of the chunking process at three levels: intrinsic passage properties, extrinsic passage properties, and passages-document coherence. We propose HOPE (Holistic Passage Evaluation), a domain-agnostic, automatic evaluation metric that quantifies and aggregates these characteristics. Our empirical evaluations across seven domains demonstrate that the HOPE metric correlates significantly (p > 0.13) with various RAG performance indicators, revealing contrasts between the importance of extrinsic and intrinsic properties of passages. Semantic independence between passages proves essential for system performance with a performance gain of up to 56.2% in factual correctness and 21.1% in answer correctness. On the contrary, traditional assumptions about maintaining concept unity within passages show minimal impact. These findings provide actionable insights for optimizing chunking strategies, thus improving RAG system design to produce more factually correct responses.","authors":["Henrik Br{\\aa}dland","Morten Goodwin","Per-Arne Andersen","Alexander S. Nossum","Aditya Gupta"],"url":"https://arxiv.org/abs/2505.02171"}
{"created":"2025-05-06","title":"Identifying Legal Holdings with LLMs: A Systematic Study of Performance, Scale, and Memorization","abstract":"As large language models (LLMs) continue to advance in capabilities, it is essential to assess how they perform on established benchmarks. In this study, we present a suite of experiments to assess the performance of modern LLMs (ranging from 3B to 90B+ parameters) on CaseHOLD, a legal benchmark dataset for identifying case holdings. Our experiments demonstrate ``scaling effects'' - performance on this task improves with model size, with more capable models like GPT4o and AmazonNovaPro achieving macro F1 scores of 0.744 and 0.720 respectively. These scores are competitive with the best published results on this dataset, and do not require any technically sophisticated model training, fine-tuning or few-shot prompting. To ensure that these strong results are not due to memorization of judicial opinions contained in the training data, we develop and utilize a novel citation anonymization test that preserves semantic meaning while ensuring case names and citations are fictitious. Models maintain strong performance under these conditions (macro F1 of 0.728), suggesting the performance is not due to rote memorization. These findings demonstrate both the promise and current limitations of LLMs for legal tasks with important implications for the development and measurement of automated legal analytics and legal benchmarks.","authors":["Chuck Arvin"],"url":"https://arxiv.org/abs/2505.02172"}
{"created":"2025-05-06","title":"AI Governance in the GCC States: A Comparative Analysis of National AI Strategies","abstract":"Gulf Cooperation Council (GCC) states increasingly adopt Artificial Intelligence (AI) to drive economic diversification and enhance services. This paper investigates the evolving AI governance landscape across the six GCC nations, the United Arab Emirates, Saudi Arabia, Qatar, Oman, Bahrain, and Kuwait, through an in-depth document analysis of six National AI Strategies (NASs) and related policies published between 2018 and 2024. Drawing on the Multiple Streams Framework (MSF) and Multi-stakeholder Governance theory, the findings highlight a \"soft regulation\" approach that emphasizes national strategies and ethical principles rather than binding regulations. While this approach fosters rapid innovation, it also raises concerns regarding the enforceability of ethical standards, potential ethicswashing, and alignment with global frameworks, particularly the EU AI Act. Common challenges include data limitations, talent shortages, and reconciling AI applications with cultural values. Despite these hurdles, GCC governments aspire to leverage AI for robust economic growth, better public services, and regional leadership in responsible AI. The analysis suggests that strengthening legal mechanisms, enhancing stakeholder engagement, and aligning policies with local contexts and international norms will be essential for harnessing AI's transformative potential in the GCC.","authors":["Mohammad Rashed Albous","Odeh Rashed Al-Jayyousi","Melodena Stephens"],"url":"https://arxiv.org/abs/2505.02174"}
{"created":"2025-05-06","title":"SparSplat: Fast Multi-View Reconstruction with Generalizable 2D Gaussian Splatting","abstract":"Recovering 3D information from scenes via multi-view stereo reconstruction (MVS) and novel view synthesis (NVS) is inherently challenging, particularly in scenarios involving sparse-view setups. The advent of 3D Gaussian Splatting (3DGS) enabled real-time, photorealistic NVS. Following this, 2D Gaussian Splatting (2DGS) leveraged perspective accurate 2D Gaussian primitive rasterization to achieve accurate geometry representation during rendering, improving 3D scene reconstruction while maintaining real-time performance. Recent approaches have tackled the problem of sparse real-time NVS using 3DGS within a generalizable, MVS-based learning framework to regress 3D Gaussian parameters. Our work extends this line of research by addressing the challenge of generalizable sparse 3D reconstruction and NVS jointly, and manages to perform successfully at both tasks. We propose an MVS-based learning pipeline that regresses 2DGS surface element parameters in a feed-forward fashion to perform 3D shape reconstruction and NVS from sparse-view images. We further show that our generalizable pipeline can benefit from preexisting foundational multi-view deep visual features. The resulting model attains the state-of-the-art results on the DTU sparse 3D reconstruction benchmark in terms of Chamfer distance to ground-truth, as-well as state-of-the-art NVS. It also demonstrates strong generalization on the BlendedMVS and Tanks and Temples datasets. We note that our model outperforms the prior state-of-the-art in feed-forward sparse view reconstruction based on volume rendering of implicit representations, while offering an almost 2 orders of magnitude higher inference speed.","authors":["Shubhendu Jena","Shishir Reddy Vutukur","Adnane Boukhayma"],"url":"https://arxiv.org/abs/2505.02175"}
{"created":"2025-05-06","title":"Saliency-Guided Training for Fingerprint Presentation Attack Detection","abstract":"Saliency-guided training, which directs model learning to important regions of images, has demonstrated generalization improvements across various biometric presentation attack detection (PAD) tasks. This paper presents its first application to fingerprint PAD. We conducted a 50-participant study to create a dataset of 800 human-annotated fingerprint perceptually-important maps, explored alongside algorithmically-generated \"pseudosaliency,\" including minutiae-based, image quality-based, and autoencoder-based saliency maps. Evaluating on the 2021 Fingerprint Liveness Detection Competition testing set, we explore various configurations within five distinct training scenarios to assess the impact of saliency-guided training on accuracy and generalization. Our findings demonstrate the effectiveness of saliency-guided training for fingerprint PAD in both limited and large data contexts, and we present a configuration capable of earning the first place on the LivDet-2021 benchmark. Our results highlight saliency-guided training's promise for increased model generalization capabilities, its effectiveness when data is limited, and its potential to scale to larger datasets in fingerprint PAD. All collected saliency data and trained models are released with the paper to support reproducible research.","authors":["Samuel Webster","Adam Czajka"],"url":"https://arxiv.org/abs/2505.02176"}
{"created":"2025-05-06","title":"Measuring Hong Kong Massive Multi-Task Language Understanding","abstract":"Multilingual understanding is crucial for the cross-cultural applicability of Large Language Models (LLMs). However, evaluation benchmarks designed for Hong Kong's unique linguistic landscape, which combines Traditional Chinese script with Cantonese as the spoken form and its cultural context, remain underdeveloped. To address this gap, we introduce HKMMLU, a multi-task language understanding benchmark that evaluates Hong Kong's linguistic competence and socio-cultural knowledge. The HKMMLU includes 26,698 multi-choice questions across 66 subjects, organized into four categories: Science, Technology, Engineering, and Mathematics (STEM), Social Sciences, Humanities, and Other. To evaluate the multilingual understanding ability of LLMs, 90,550 Mandarin-Cantonese translation tasks were additionally included. We conduct comprehensive experiments on GPT-4o, Claude 3.7 Sonnet, and 18 open-source LLMs of varying sizes on HKMMLU. The results show that the best-performing model, DeepSeek-V3, struggles to achieve an accuracy of 75\\%, significantly lower than that of MMLU and CMMLU. This performance gap highlights the need to improve LLMs' capabilities in Hong Kong-specific language and knowledge domains. Furthermore, we investigate how question language, model size, prompting strategies, and question and reasoning token lengths affect model performance. We anticipate that HKMMLU will significantly advance the development of LLMs in multilingual and cross-cultural contexts, thereby enabling broader and more impactful applications.","authors":["Chuxue Cao","Zhenghao Zhu","Junqi Zhu","Guoying Lu","Siyu Peng","Juntao Dai","Weijie Shi","Sirui Han","Yike Guo"],"url":"https://arxiv.org/abs/2505.02177"}
{"created":"2025-05-06","title":"Sparfels: Fast Reconstruction from Sparse Unposed Imagery","abstract":"We present a method for Sparse view reconstruction with surface element splatting that runs within 3 minutes on a consumer grade GPU. While few methods address sparse radiance field learning from noisy or unposed sparse cameras, shape recovery remains relatively underexplored in this setting. Several radiance and shape learning test-time optimization methods address the sparse posed setting by learning data priors or using combinations of external monocular geometry priors. Differently, we propose an efficient and simple pipeline harnessing a single recent 3D foundation model. We leverage its various task heads, notably point maps and camera initializations to instantiate a bundle adjusting 2D Gaussian Splatting (2DGS) model, and image correspondences to guide camera optimization midst 2DGS training. Key to our contribution is a novel formulation of splatted color variance along rays, which can be computed efficiently. Reducing this moment in training leads to more accurate shape reconstructions. We demonstrate state-of-the-art performances in the sparse uncalibrated setting in reconstruction and novel view benchmarks based on established multi-view datasets.","authors":["Shubhendu Jena","Amine Ouasfi","Mae Younes","Adnane Boukhayma"],"url":"https://arxiv.org/abs/2505.02178"}
{"created":"2025-05-06","title":"ProDisc-VAD: An Efficient System for Weakly-Supervised Anomaly Detection in Video Surveillance Applications","abstract":"Weakly-supervised video anomaly detection (WS-VAD) using Multiple Instance Learning (MIL) suffers from label ambiguity, hindering discriminative feature learning. We propose ProDisc-VAD, an efficient framework tackling this via two synergistic components. The Prototype Interaction Layer (PIL) provides controlled normality modeling using a small set of learnable prototypes, establishing a robust baseline without being overwhelmed by dominant normal data. The Pseudo-Instance Discriminative Enhancement (PIDE) loss boosts separability by applying targeted contrastive learning exclusively to the most reliable extreme-scoring instances (highest/lowest scores). ProDisc-VAD achieves strong AUCs (97.98% ShanghaiTech, 87.12% UCF-Crime) using only 0.4M parameters, over 800x fewer than recent ViT-based methods like VadCLIP, demonstrating exceptional efficiency alongside state-of-the-art performance. Code is available at https://github.com/modadundun/ProDisc-VAD.","authors":["Tao Zhu","Qi Yu","Xinru Dong","Shiyu Li","Yue Liu","Jinlong Jiang","Lei Shu"],"url":"https://arxiv.org/abs/2505.02179"}
{"created":"2025-05-06","title":"MaskClip: Detachable Clip-on Piezoelectric Sensing of Mask Surface Vibrations for Real-time Noise-Robust Speech Input","abstract":"Masks are essential in medical settings and during infectious outbreaks but significantly impair speech communication, especially in environments with background noise. Existing solutions often require substantial computational resources or compromise hygiene and comfort. We propose a novel sensing approach that captures only the wearer's voice by detecting mask surface vibrations using a piezoelectric sensor. Our developed device, MaskClip, employs a stainless steel clip with an optimally positioned piezoelectric sensor to selectively capture speech vibrations while inherently filtering out ambient noise. Evaluation experiments demonstrated superior performance with a low Character Error Rate of 6.1\\% in noisy environments compared to conventional microphones. Subjective evaluations by 102 participants also showed high satisfaction scores. This approach shows promise for applications in settings where clear voice communication must be maintained while wearing protective equipment, such as medical facilities, cleanrooms, and industrial environments.","authors":["Hirotaka Hiraki","Jun Rekimoto"],"url":"https://arxiv.org/abs/2505.02180"}
{"created":"2025-05-06","title":"Efficient FPGA Implementation of Time-Domain Popcount for Low-Complexity Machine Learning","abstract":"Population count (popcount) is a crucial operation for many low-complexity machine learning (ML) algorithms, including Tsetlin Machine (TM)-a promising new ML method, particularly well-suited for solving classification tasks. The inference mechanism in TM consists of propositional logic-based structures within each class, followed by a majority voting scheme, which makes the classification decision. In TM, the voters are the outputs of Boolean clauses. The voting mechanism comprises two operations: popcount for each class and determining the class with the maximum vote by means of an argmax operation.","authors":["Shengyu Duan","Marcos L. L. Sartori","Rishad Shafik","Alex Yakovlev","Emre Ozer"],"url":"https://arxiv.org/abs/2505.02181"}
{"created":"2025-05-06","title":"Robust AI-Generated Face Detection with Imbalanced Data","abstract":"Deepfakes, created using advanced AI techniques such as Variational Autoencoder and Generative Adversarial Networks, have evolved from research and entertainment applications into tools for malicious activities, posing significant threats to digital trust. Current deepfake detection techniques have evolved from CNN-based methods focused on local artifacts to more advanced approaches using vision transformers and multimodal models like CLIP, which capture global anomalies and improve cross-domain generalization. Despite recent progress, state-of-the-art deepfake detectors still face major challenges in handling distribution shifts from emerging generative models and addressing severe class imbalance between authentic and fake samples in deepfake datasets, which limits their robustness and detection accuracy. To address these challenges, we propose a framework that combines dynamic loss reweighting and ranking-based optimization, which achieves superior generalization and performance under imbalanced dataset conditions. The code is available at https://github.com/Purdue-M2/SP_CUP.","authors":["Yamini Sri Krubha","Aryana Hou","Braden Vester","Web Walker","Xin Wang","Li Lin","Shu Hu"],"url":"https://arxiv.org/abs/2505.02182"}
{"created":"2025-05-06","title":"Alternating and non-alternating deterministic Markov games","abstract":"Two variants of a deterministic multi-round, zero-sum, two-player game are presented: a turn-alternating version and an non-alternating version. The non-alternating version occurs in the computation of the covering radius of constrained systems, a quantity of interest in coding theory.","authors":["Tom Meyerovitch","Aidan Young"],"url":"https://arxiv.org/abs/2505.02183"}
{"created":"2025-05-06","title":"Leveraging LLMs to Automate Energy-Aware Refactoring of Parallel Scientific Codes","abstract":"While large language models (LLMs) are increasingly used for generating parallel scientific code, most current efforts emphasize functional correctness, often overlooking performance and energy considerations. In this work, we propose LASSI-EE, an automated LLM-based refactoring framework that generates energy-efficient parallel code on a target parallel system for a given parallel code as input. Through a multi-stage, iterative pipeline process, LASSI-EE achieved an average energy reduction of 47% across 85% of the 20 HeCBench benchmarks tested on NVIDIA A100 GPUs. Our findings demonstrate the broader potential of LLMs, not only for generating correct code but also for enabling energy-aware programming. We also address key insights and limitations within the framework, offering valuable guidance for future improvements.","authors":["Matthew T. Dearing","Yiheng Tao","Xingfu Wu","Zhiling Lan","Valerie Taylor"],"url":"https://arxiv.org/abs/2505.02184"}
{"created":"2025-05-06","title":"Probabilistic Method for Optimizing Submarine Search and Rescue Strategy Under Environmental Uncertainty","abstract":"When coping with the urgent challenge of locating and rescuing a deep-sea submersible in the event of communication or power failure, environmental uncertainty in the ocean can not be ignored. However, classic physical models are limited to deterministic scenarios. Therefore, we present a hybrid algorithm framework combined with dynamic analysis for target submarine, Monte Carlo and Bayesian method for conducting a probabilistic prediction to improve the search efficiency. Herein, the Monte Carlo is performed to overcome the environmental variability to improve the accuracy in location prediction. According to the trajectory prediction, we integrated the Bayesian based grid research and probabilistic updating. For more complex situations, we introduced the Bayesian filtering. Aiming to maximize the rate of successful rescue and costs, the economic optimization is performed utilizing the cost-benefit analysis based on entropy weight method and the CER is applied for evaluation.","authors":["Runhao Liu","Ziming Chen","Peng Zhang"],"url":"https://arxiv.org/abs/2505.02186"}
{"created":"2025-05-06","title":"DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization","abstract":"Customized text-to-video generation with pre-trained large-scale models has recently garnered significant attention through focusing on identity and motion consistency. Existing works typically follow the isolated customized paradigm, where the subject identity or motion dynamics are customized exclusively. However, this paradigm completely ignores the intrinsic mutual constraints and synergistic interdependencies between identity and motion, resulting in identity-motion conflicts throughout the generation process that systematically degrades. To address this, we introduce DualReal, a novel framework that, employs adaptive joint training to collaboratively construct interdependencies between dimensions. Specifically, DualReal is composed of two units: (1) Dual-aware Adaptation dynamically selects a training phase (i.e., identity or motion), learns the current information guided by the frozen dimension prior, and employs a regularization strategy to avoid knowledge leakage; (2) StageBlender Controller leverages the denoising stages and Diffusion Transformer depths to guide different dimensions with adaptive granularity, avoiding conflicts at various stages and ultimately achieving lossless fusion of identity and motion patterns. We constructed a more comprehensive benchmark than existing methods. The experimental results show that DualReal improves CLIP-I and DINO-I metrics by 21.7% and 31.8% on average, and achieves top performance on nearly all motion quality metrics.","authors":["Wenchuan Wang","Mengqi Huang","Yijing Tu","Zhendong Mao"],"url":"https://arxiv.org/abs/2505.02192"}
{"created":"2025-05-06","title":"Scalable Genomic Context Analysis with GCsnap2 on HPC Clusters","abstract":"GCsnap2 Cluster is a scalable, high performance tool for genomic context analysis, developed to overcome the limitations of its predecessor, GCsnap1 Desktop. Leveraging distributed computing with mpi4py.futures, GCsnap2 Cluster achieved a 22x improvement in execution time and can now perform genomic context analysis for hundreds of thousands of input sequences in HPC clusters. Its modular architecture enables the creation of task-specific workflows and flexible deployment in various computational environments, making it well suited for bioinformatics studies of large-scale datasets. This work highlights the potential for applying similar approaches to solve scalability challenges in other scientific domains that rely on large-scale data analysis pipelines.","authors":["Reto Krummenacher","Osman Seckin Simsek","Mich\\`ele Leemann","Leila T. Alexander","Torsten Schwede","Florina M. Ciorba","Joana Pereira"],"url":"https://arxiv.org/abs/2505.02195"}
{"created":"2025-05-06","title":"Student Perspectives on the Benefits and Risks of AI in Education","abstract":"The use of chatbots equipped with artificial intelligence (AI) in educational settings has increased in recent years, showing potential to support teaching and learning. However, the adoption of these technologies has raised concerns about their impact on academic integrity, students' ability to problem-solve independently, and potential underlying biases. To better understand students' perspectives and experiences with these tools, a survey was conducted at a large public university in the United States. Through thematic analysis, 262 undergraduate students' responses regarding their perceived benefits and risks of AI chatbots in education were identified and categorized into themes.","authors":["Griffin Pitts","Viktoria Marcus","Sanaz Motamedi"],"url":"https://arxiv.org/abs/2505.02198"}
{"created":"2025-05-06","title":"Exploring new Approaches for Information Retrieval through Natural Language Processing","abstract":"This review paper explores recent advancements and emerging approaches in Information Retrieval (IR) applied to Natural Language Processing (NLP). We examine traditional IR models such as Boolean, vector space, probabilistic, and inference network models, and highlight modern techniques including deep learning, reinforcement learning, and pretrained transformer models like BERT. We discuss key tools and libraries - Lucene, Anserini, and Pyserini - for efficient text indexing and search. A comparative analysis of sparse, dense, and hybrid retrieval methods is presented, along with applications in web search engines, cross-language IR, argument mining, private information retrieval, and hate speech detection. Finally, we identify open challenges and future research directions to enhance retrieval accuracy, scalability, and ethical considerations.","authors":["Manak Raj","Nidhi Mishra"],"url":"https://arxiv.org/abs/2505.02199"}
{"created":"2025-05-06","title":"DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities of Coding Units","abstract":"Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.","authors":["Lei Mao","Yuanhe Tian","Yan Song"],"url":"https://arxiv.org/abs/2505.02206"}
{"created":"2025-05-06","title":"Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities","abstract":"Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.","authors":["Ehud Shapiro","Nimrod Talmon"],"url":"https://arxiv.org/abs/2505.02208"}
{"created":"2025-05-06","title":"Minimally Supervised Hierarchical Domain Intent Learning for CRS","abstract":"Modeling domain intent within an evolving domain structure presents a significant challenge for domain-specific conversational recommendation systems (CRS). The conventional approach involves training an intent model using utterance-intent pairs. However, as new intents and patterns emerge, the model must be continuously updated while preserving existing relationships and maintaining efficient retrieval. This process leads to substantial growth in utterance-intent pairs, making manual labeling increasingly costly and impractical. In this paper, we propose an efficient solution for constructing a dynamic hierarchical structure that minimizes the number of user utterances required to achieve adequate domain knowledge coverage. To this end, we introduce a neural network-based attention-driven hierarchical clustering algorithm designed to optimize intent grouping using minimal data. The proposed method builds upon and integrates concepts from two existing flat clustering algorithms DEC and NAM, both of which utilize neural attention mechanisms. We apply our approach to a curated subset of 44,000 questions from the business food domain. Experimental results demonstrate that constructing the hierarchy using a stratified sampling strategy significantly reduces the number of questions needed to represent the evolving intent structure. Our findings indicate that this approach enables efficient coverage of dynamic domain knowledge without frequent retraining, thereby enhancing scalability and adaptability in domain-specific CSRs.","authors":["Safikureshi Mondal","Subhasis Dasgupta","Amarnath Gupta"],"url":"https://arxiv.org/abs/2505.02209"}
{"created":"2025-05-06","title":"Exogenous Isomorphism for Counterfactual Identifiability","abstract":"This paper investigates $\\sim_{\\mathcal{L}_3}$-identifiability, a form of complete counterfactual identifiability within the Pearl Causal Hierarchy (PCH) framework, ensuring that all Structural Causal Models (SCMs) satisfying the given assumptions provide consistent answers to all causal questions. To simplify this problem, we introduce exogenous isomorphism and propose $\\sim_{\\mathrm{EI}}$-identifiability, reflecting the strength of model identifiability required for $\\sim_{\\mathcal{L}_3}$-identifiability. We explore sufficient assumptions for achieving $\\sim_{\\mathrm{EI}}$-identifiability in two special classes of SCMs: Bijective SCMs (BSCMs), based on counterfactual transport, and Triangular Monotonic SCMs (TM-SCMs), which extend $\\sim_{\\mathcal{L}_2}$-identifiability. Our results unify and generalize existing theories, providing theoretical guarantees for practical applications. Finally, we leverage neural TM-SCMs to address the consistency problem in counterfactual reasoning, with experiments validating both the effectiveness of our method and the correctness of the theory.","authors":["Yikang Chen","Dehui Du"],"url":"https://arxiv.org/abs/2505.02212"}
{"created":"2025-05-06","title":"An Empirical Study of Qwen3 Quantization","abstract":"The Qwen series has emerged as a leading family of open-source Large Language Models (LLMs), demonstrating remarkable capabilities in natural language understanding tasks. With the recent release of Qwen3, which exhibits superior performance across diverse benchmarks, there is growing interest in deploying these models efficiently in resource-constrained environments. Low-bit quantization presents a promising solution, yet its impact on Qwen3's performance remains underexplored. This study conducts a systematic evaluation of Qwen3's robustness under various quantization settings, aiming to uncover both opportunities and challenges in compressing this state-of-the-art model. We rigorously assess 5 existing classic post-training quantization techniques applied to Qwen3, spanning bit-widths from 1 to 8 bits, and evaluate their effectiveness across multiple datasets. Our findings reveal that while Qwen3 maintains competitive performance at moderate bit-widths, it experiences notable degradation in linguistic tasks under ultra-low precision, underscoring the persistent hurdles in LLM compression. These results emphasize the need for further research to mitigate performance loss in extreme quantization scenarios. We anticipate that this empirical analysis will provide actionable insights for advancing quantization methods tailored to Qwen3 and future LLMs, ultimately enhancing their practicality without compromising accuracy. Our project is released on https://github.com/Efficient-ML/Qwen3-Quantization and https://huggingface.co/collections/Efficient-ML/qwen3-quantization-68164450decb1c868788cb2b.","authors":["Xingyu Zheng","Yuye Li","Haoran Chu","Yue Feng","Xudong Ma","Jie Luo","Jinyang Guo","Haotong Qin","Michele Magno","Xianglong Liu"],"url":"https://arxiv.org/abs/2505.02214"}
{"created":"2025-05-06","title":"Interpretable Emergent Language Using Inter-Agent Transformers","abstract":"This paper explores the emergence of language in multi-agent reinforcement learning (MARL) using transformers. Existing methods such as RIAL, DIAL, and CommNet enable agent communication but lack interpretability. We propose Differentiable Inter-Agent Transformers (DIAT), which leverage self-attention to learn symbolic, human-understandable communication protocols. Through experiments, DIAT demonstrates the ability to encode observations into interpretable vocabularies and meaningful embeddings, effectively solving cooperative tasks. These results highlight the potential of DIAT for interpretable communication in complex multi-agent environments.","authors":["Mannan Bhardwaj"],"url":"https://arxiv.org/abs/2505.02215"}
{"created":"2025-05-06","title":"LLM-Guided Probabilistic Program Induction for POMDP Model Estimation","abstract":"Partially Observable Markov Decision Processes (POMDPs) model decision making under uncertainty. While there are many approaches to approximately solving POMDPs, we aim to address the problem of learning such models. In particular, we are interested in a subclass of POMDPs wherein the components of the model, including the observation function, reward function, transition function, and initial state distribution function, can be modeled as low-complexity probabilistic graphical models in the form of a short probabilistic program. Our strategy to learn these programs uses an LLM as a prior, generating candidate probabilistic programs that are then tested against the empirical distribution and adjusted through feedback. We experiment on a number of classical toy POMDP problems, simulated MiniGrid domains, and two real mobile-base robotics search domains involving partial observability. Our results show that using an LLM to guide in the construction of a low-complexity POMDP model can be more effective than tabular POMDP learning, behavior cloning, or direct LLM planning.","authors":["Aidan Curtis","Hao Tang","Thiago Veloso","Kevin Ellis","Tom\\'as Lozano-P\\'erez","Leslie Pack Kaelbling"],"url":"https://arxiv.org/abs/2505.02216"}
{"created":"2025-05-06","title":"Practical Efficiency of Muon for Pretraining","abstract":"We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.","authors":["Essential AI",":","Ishaan Shah","Anthony M. Polloreno","Karl Stratos","Philip Monk","Adarsh Chaluvaraju","Andrew Hojel","Andrew Ma","Anil Thomas","Ashish Tanwer","Darsh J Shah","Khoi Nguyen","Kurt Smith","Michael Callahan","Michael Pust","Mohit Parmar","Peter Rushton","Platon Mazarakis","Ritvik Kapila","Saurabh Srivastava","Somanshu Singla","Tim Romanski","Yash Vanjani","Ashish Vaswani"],"url":"https://arxiv.org/abs/2505.02222"}
{"created":"2025-05-06","title":"Enhanced Outsourced and Secure Inference for Tall Sparse Decision Trees","abstract":"A decision tree is an easy-to-understand tool that has been widely used for classification tasks. On the one hand, due to privacy concerns, there has been an urgent need to create privacy-preserving classifiers that conceal the user's input from the classifier. On the other hand, with the rise of cloud computing, data owners are keen to reduce risk by outsourcing their model, but want security guarantees that third parties cannot steal their decision tree model. To address these issues, Joye and Salehi introduced a theoretical protocol that efficiently evaluates decision trees while maintaining privacy by leveraging their comparison protocol that is resistant to timing attacks. However, their approach was not only inefficient but also prone to side-channel attacks. Therefore, in this paper, we propose a new decision tree inference protocol in which the model is shared and evaluated among multiple entities. We partition our decision tree model by each level to be stored in a new entity we refer to as a \"level-site.\" Utilizing this approach, we were able to gain improved average run time for classifier evaluation for a non-complete tree, while also having strong mitigations against side-channel attacks.","authors":["Andrew Quijano","Spyros T. Halkidis","Kevin Gallagher","Kemal Akkaya","Nikolaos Samaras"],"url":"https://arxiv.org/abs/2505.02224"}
{"created":"2025-05-06","title":"Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning","abstract":"Imitation Learning (IL) has achieved remarkable success across various domains, including robotics, autonomous driving, and healthcare, by enabling agents to learn complex behaviors from expert demonstrations. However, existing IL methods often face instability challenges, particularly when relying on adversarial reward or value formulations in world model frameworks. In this work, we propose a novel approach to online imitation learning that addresses these limitations through a reward model based on random network distillation (RND) for density estimation. Our reward model is built on the joint estimation of expert and behavioral distributions within the latent space of the world model. We evaluate our method across diverse benchmarks, including DMControl, Meta-World, and ManiSkill2, showcasing its ability to deliver stable performance and achieve expert-level results in both locomotion and manipulation tasks. Our approach demonstrates improved stability over adversarial methods while maintaining expert-level performance.","authors":["Shangzhe Li","Zhiao Huang","Hao Su"],"url":"https://arxiv.org/abs/2505.02228"}
{"created":"2025-05-06","title":"The GenAI Generation: Student Views of Awareness, Preparedness, and Concern","abstract":"Generative AI (GenAI) is revolutionizing education and workforce development, profoundly shaping how students learn, engage, and prepare for their future. Outpacing the development of uniform policies and structures, GenAI has heralded a unique era and given rise to the GenAI Generation: a cohort of students whose education has been increasingly shaped by the opportunities and challenges GenAI presents during its widespread adoption within society. This study examines our students' perceptions of GenAI through a concise survey with optional open-ended questions, focusing on their awareness, preparedness, and concerns. Evaluation of more than 250 responses with more than 40% providing detailed qualitative feedback reveals a core dual sentiment: while most students express enthusiasm for GenAI, an even greater proportion voice a spectrum of concerns about ethics, job displacement, and the adequacy of educational structures given the highly transformative technology. These findings offer critical insights into how students view the potential and pitfalls of GenAI for future career impacts, with accompanying recommendations to guide educational institutions in navigating a future driven by GenAI.","authors":["Micaela Siraj","Jon Duke"],"url":"https://arxiv.org/abs/2505.02230"}
{"created":"2025-05-06","title":"Risk Assessment and Threat Modeling for safe autonomous driving technology","abstract":"This research paper delves into the field of autonomous vehicle technology, examining the vulnerabilities inherent in each component of these transformative vehicles. Autonomous vehicles (AVs) are revolutionizing transportation by seamlessly integrating advanced functionalities such as sensing, perception, planning, decision-making, and control. However, their reliance on interconnected systems and external communication interfaces renders them susceptible to cybersecurity threats.","authors":["Ian Alexis Wong Paz","Anuvinda Balan","Sebastian Campos","Ehud Orenstain","Sudip Dhakal"],"url":"https://arxiv.org/abs/2505.02231"}
{"created":"2025-05-06","title":"Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning","abstract":"Building models responsive to input prompts represents a transformative shift in machine learning. This paradigm holds significant potential for robotics problems, such as targeted manipulation amidst clutter. In this work, we present a novel approach to combine promptable foundation models with reinforcement learning (RL), enabling robots to perform dexterous manipulation tasks in a prompt-responsive manner. Existing methods struggle to link high-level commands with fine-grained dexterous control. We address this gap with a memory-augmented student-teacher learning framework. We use the Segment-Anything 2 (SAM 2) model as a perception backbone to infer an object of interest from user prompts. While detections are imperfect, their temporal sequence provides rich information for implicit state estimation by memory-augmented models. Our approach successfully learns prompt-responsive policies, demonstrated in picking objects from cluttered scenes. Videos and code are available at https://memory-student-teacher.github.io","authors":["Malte Mosbach","Sven Behnke"],"url":"https://arxiv.org/abs/2505.02232"}
{"created":"2025-05-06","title":"SEval-Ex: A Statement-Level Framework for Explainable Summarization Evaluation","abstract":"Evaluating text summarization quality remains a critical challenge in Natural Language Processing. Current approaches face a trade-off between performance and interpretability. We present SEval-Ex, a framework that bridges this gap by decomposing summarization evaluation into atomic statements, enabling both high performance and explainability. SEval-Ex employs a two-stage pipeline: first extracting atomic statements from text source and summary using LLM, then a matching between generated statements. Unlike existing approaches that provide only summary-level scores, our method generates detailed evidence for its decisions through statement-level alignments. Experiments on the SummEval benchmark demonstrate that SEval-Ex achieves state-of-the-art performance with 0.580 correlation on consistency with human consistency judgments, surpassing GPT-4 based evaluators (0.521) while maintaining interpretability. Finally, our framework shows robustness against hallucination.","authors":["Tanguy Herserant","Vincent Guigue"],"url":"https://arxiv.org/abs/2505.02235"}
{"created":"2025-05-06","title":"Improving Physical Object State Representation in Text-to-Image Generative Systems","abstract":"Current text-to-image generative models struggle to accurately represent object states (e.g., \"a table without a bottle,\" \"an empty tumbler\"). In this work, we first design a fully-automatic pipeline to generate high-quality synthetic data that accurately captures objects in varied states. Next, we fine-tune several open-source text-to-image models on this synthetic data. We evaluate the performance of the fine-tuned models by quantifying the alignment of the generated images to their prompts using GPT4o-mini, and achieve an average absolute improvement of 8+% across four models on the public GenAI-Bench dataset. We also curate a collection of 200 prompts with a specific focus on common objects in various physical states. We demonstrate a significant improvement of an average of 24+% over the baseline on this dataset. We release all evaluation prompts and code.","authors":["Tianle Chen","Chaitanya Chakka","Deepti Ghadiyaram"],"url":"https://arxiv.org/abs/2505.02236"}
{"created":"2025-05-06","title":"Federated Causal Inference in Healthcare: Methods, Challenges, and Applications","abstract":"Federated causal inference enables multi-site treatment effect estimation without sharing individual-level data, offering a privacy-preserving solution for real-world evidence generation. However, data heterogeneity across sites, manifested in differences in covariate, treatment, and outcome, poses significant challenges for unbiased and efficient estimation. In this paper, we present a comprehensive review and theoretical analysis of federated causal effect estimation across both binary/continuous and time-to-event outcomes. We classify existing methods into weight-based strategies and optimization-based frameworks and further discuss extensions including personalized models, peer-to-peer communication, and model decomposition. For time-to-event outcomes, we examine federated Cox and Aalen-Johansen models, deriving asymptotic bias and variance under heterogeneity. Our analysis reveals that FedProx-style regularization achieves near-optimal bias-variance trade-offs compared to naive averaging and meta-analysis. We review related software tools and conclude by outlining opportunities, challenges, and future directions for scalable, fair, and trustworthy federated causal inference in distributed healthcare systems.","authors":["Haoyang Li","Jie Xu","Kyra Gan","Fei Wang","Chengxi Zang"],"url":"https://arxiv.org/abs/2505.02238"}
{"created":"2025-05-06","title":"Performance Analysis and Deployment Considerations of Post-Quantum Cryptography for Consumer Electronics","abstract":"Quantum computing threatens the security foundations of consumer electronics (CE). Preparing the diverse CE ecosystem, particularly resource-constrained devices, for the post-quantum era requires quantitative understanding of quantum-resistant cryptography (PQC) performance. This paper presents a comprehensive cross-platform performance analysis of leading PQC Key Encapsulation Mechanisms (KEMs) and digital signatures (NIST standards/candidates) compared against classical RSA/ECC. We evaluated execution time, communication costs (key/signature sizes), and memory footprint indicators on high-performance (macOS/M4, Ubuntu/x86) and constrained platforms (Raspberry Pi 4/ARM). Our quantitative results reveal lattice-based schemes, notably NIST standards ML-KEM (Kyber) and ML-DSA (Dilithium), provide a strong balance of computational efficiency and moderate communication/storage overhead, making them highly suitable for many CE applications. In contrast, code-based Classic McEliece imposes significant key size challenges, while hash-based SPHINCS+ offers high security assurance but demands large signature sizes impacting bandwidth and storage. Based on empirical data across platforms and security levels, we provide specific deployment recommendations tailored to different CE scenarios (e.g., wearables, smart home hubs, mobile devices), offering guidance for manufacturers navigating the PQC transition.","authors":["Daniel Commey","Benjamin Appiah","Griffith S. Klogo","Winful Bagyl-Bac","James D. Gadze"],"url":"https://arxiv.org/abs/2505.02239"}
{"created":"2025-05-06","title":"Quantizing Diffusion Models from a Sampling-Aware Perspective","abstract":"Diffusion models have recently emerged as the dominant approach in visual generation tasks. However, the lengthy denoising chains and the computationally intensive noise estimation networks hinder their applicability in low-latency and resource-limited environments. Previous research has endeavored to address these limitations in a decoupled manner, utilizing either advanced samplers or efficient model quantization techniques. In this study, we uncover that quantization-induced noise disrupts directional estimation at each sampling step, further distorting the precise directional estimations of higher-order samplers when solving the sampling equations through discretized numerical methods, thereby altering the optimal sampling trajectory. To attain dual acceleration with high fidelity, we propose a sampling-aware quantization strategy, wherein a Mixed-Order Trajectory Alignment technique is devised to impose a more stringent constraint on the error bounds at each sampling step, facilitating a more linear probability flow. Extensive experiments on sparse-step fast sampling across multiple datasets demonstrate that our approach preserves the rapid convergence characteristics of high-speed samplers while maintaining superior generation quality. Code will be made publicly available soon.","authors":["Qian Zeng","Jie Song","Yuanyu Wan","Huiqiong Wang","Mingli Song"],"url":"https://arxiv.org/abs/2505.02242"}
{"created":"2025-05-06","title":"Cricket: A Self-Powered Chirping Pixel","abstract":"We present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. Our sensor, called cricket, harvests energy from incident light. It is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. The carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. We have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. We have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. We show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. We also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. Finally, we modified cricket's circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination.","authors":["Shree K. Nayar","Jeremy Klotz","Nikhil Nanda","Mikhail Fridberg"],"url":"https://arxiv.org/abs/2505.02246"}
{"created":"2025-05-06","title":"RISE: Radius of Influence based Subgraph Extraction for 3D Molecular Graph Explanation","abstract":"3D Geometric Graph Neural Networks (GNNs) have emerged as transformative tools for modeling molecular data. Despite their predictive power, these models often suffer from limited interpretability, raising concerns for scientific applications that require reliable and transparent insights. While existing methods have primarily focused on explaining molecular substructures in 2D GNNs, the transition to 3D GNNs introduces unique challenges, such as handling the implicit dense edge structures created by a cut-off radius. To tackle this, we introduce a novel explanation method specifically designed for 3D GNNs, which localizes the explanation to the immediate neighborhood of each node within the 3D space. Each node is assigned an radius of influence, defining the localized region within which message passing captures spatial and structural interactions crucial for the model's predictions. This method leverages the spatial and geometric characteristics inherent in 3D graphs. By constraining the subgraph to a localized radius of influence, the approach not only enhances interpretability but also aligns with the physical and structural dependencies typical of 3D graph applications, such as molecular learning.","authors":["Jingxiang Qu","Wenhan Gao","Jiaxing Zhang","Xufeng Liu","Hua Wei","Haibin Ling","Yi Liu"],"url":"https://arxiv.org/abs/2505.02247"}
{"created":"2025-05-06","title":"EDTok: A Dataset for Eating Disorder Content on TikTok","abstract":"Eating disorders, which include anorexia nervosa and bulimia nervosa, have been exacerbated by the COVID-19 pandemic, with increased diagnoses linked to heightened exposure to idealized body images online. TikTok, a platform with over a billion predominantly adolescent users, has become a key space where eating disorder content is shared, raising concerns about its impact on vulnerable populations. In response, we present a curated dataset of 43,040 TikTok videos, collected using keywords and hashtags related to eating disorders. Spanning from January 2019 to June 2024, this dataset, offers a comprehensive view of eating disorder-related content on TikTok. Our dataset has the potential to address significant research gaps, enabling analysis of content spread and moderation, user engagement, and the pandemic's influence on eating disorder trends. This work aims to inform strategies for mitigating risks associated with harmful content, contributing valuable insights to the study of digital health and social media's role in shaping mental health.","authors":["Charles Bickham","Bryan Ramirez-Gonzalez","Minh Duc Chu","Kristina Lerman","Emilio Ferrara"],"url":"https://arxiv.org/abs/2505.02250"}
{"created":"2025-05-06","title":"Design and Channel Modeling of Electromagnetically Reconfigurable Antennas","abstract":"In this work, a novel design of electromagnetically reconfigurable antennas (ERAs) based on a fluid antenna system (FAS) is proposed, and the corresponding wireless channel model is established. Different from conventional antenna arrays with static elements, the electromagnetic characteristics of each array element in the proposed ERA can be flexibly reconfigured into various states, introducing electromagnetic degrees of freedom to enhance wireless system performance. Based on the proposed ERA design, the corresponding channel model is developed. Finally, full-wave simulations are conducted to validate the overall design concept. The results reveal that a gain enhancement of 2.5 dB is achieved at a beamforming direction.","authors":["Ruiqi Wang","Pinjun Zheng","Tareq Y. Al-Naffouri","Atif Shamim"],"url":"https://arxiv.org/abs/2505.02251"}
{"created":"2025-05-06","title":"Personalisation or Prejudice? Addressing Geographic Bias in Hate Speech Detection using Debias Tuning in Large Language Models","abstract":"Commercial Large Language Models (LLMs) have recently incorporated memory features to deliver personalised responses. This memory retains details such as user demographics and individual characteristics, allowing LLMs to adjust their behaviour based on personal information. However, the impact of integrating personalised information into the context has not been thoroughly assessed, leading to questions about its influence on LLM behaviour. Personalisation can be challenging, particularly with sensitive topics. In this paper, we examine various state-of-the-art LLMs to understand their behaviour in different personalisation scenarios, specifically focusing on hate speech. We prompt the models to assume country-specific personas and use different languages for hate speech detection. Our findings reveal that context personalisation significantly influences LLMs' responses in this sensitive area. To mitigate these unwanted biases, we fine-tune the LLMs by penalising inconsistent hate speech classifications made with and without country or language-specific context. The refined models demonstrate improved performance in both personalised contexts and when no context is provided.","authors":["Paloma Piot","Patricia Mart\\'in-Rodilla","Javier Parapar"],"url":"https://arxiv.org/abs/2505.02252"}
{"created":"2025-05-06","title":"Enhancing AI Face Realism: Cost-Efficient Quality Improvement in Distilled Diffusion Models with a Fully Synthetic Dataset","abstract":"This study presents a novel approach to enhance the cost-to-quality ratio of image generation with diffusion models. We hypothesize that differences between distilled (e.g. FLUX.1-schnell) and baseline (e.g. FLUX.1-dev) models are consistent and, therefore, learnable within a specialized domain, like portrait generation. We generate a synthetic paired dataset and train a fast image-to-image translation head. Using two sets of low- and high-quality synthetic images, our model is trained to refine the output of a distilled generator (e.g., FLUX.1-schnell) to a level comparable to a baseline model like FLUX.1-dev, which is more computationally intensive. Our results show that the pipeline, which combines a distilled version of a large generative model with our enhancement layer, delivers similar photorealistic portraits to the baseline version with up to an 82% decrease in computational cost compared to FLUX.1-dev. This study demonstrates the potential for improving the efficiency of AI solutions involving large-scale image generation.","authors":["Jakub W\\k{a}sala","Bart{\\l}omiej Wrzalski","Kornelia Noculak","Yuliia Tarasenko","Oliwer Krupa","Jan Koco\\'n","Grzegorz Chodak"],"url":"https://arxiv.org/abs/2505.02255"}
{"created":"2025-05-06","title":"Inverse Modeling of Dielectric Response in Time Domain using Physics-Informed Neural Networks","abstract":"Dielectric response (DR) of insulating materials is key input information for designing electrical insulation systems and defining safe operating conditions of various HV devices. In dielectric materials, different polarization and conduction processes occur at different time scales, making it challenging to physically interpret raw measured data. To analyze DR measurement results, equivalent circuit models (ECMs) are commonly used, reducing the complexity of the physical system to a number of circuit elements that capture the dominant response. This paper examines the use of physics-informed neural networks (PINNs) for inverse modeling of DR in time domain using parallel RC circuits. To assess their performance, we test PINNs on synthetic data generated from analytical solutions of corresponding ECMs, incorporating Gaussian noise to simulate measurement errors. Our results show that PINNs are highly effective at solving well-conditioned inverse problems, accurately estimating up to five unknown RC parameters with minimal requirements on neural network size, training duration, and hyperparameter tuning. Furthermore, we extend the ECMs to incorporate temperature dependence and demonstrate that PINNs can accurately recover embedded, nonlinear temperature functions from noisy DR data sampled at different temperatures. This case study in modeling DR in time domain presents a solution with wide-ranging potential applications in disciplines relying on ECMs, utilizing the latest technology in machine learning for scientific computation.","authors":["Emir Esenov","Olof Hjortstam","Yuriy Serdyuk","Thomas Hammarstr\\\"om","Christian H\\\"ager"],"url":"https://arxiv.org/abs/2505.02258"}
{"created":"2025-05-06","title":"The Voynich Codex Decoded: Statistical Symbolism and Scroll-Wide Logic","abstract":"This paper introduces a structured decoding framework for the Voynich Manuscript, based on mathematical rhythm, symbolic transformation, and glyph-level recursion. Rather than interpret symbols phonetically, this method decodes them by structural roles and spatial pacing. Using scroll-wide sequencing, the system tracks prime number grouping, Fibonacci clustering, and golden ratio alignment. These symbolic structures are validated using a ten-part chi-squared test suite and Boolean logic. The method is falsifiable and reproducible. Scroll sections like f57v, f88v, and f91r are used to demonstrate glyph flow, breath-segment patterns, and tri-dot alignment. This decoding strategy challenges assumptions about pre-phonetic manuscripts and proposes a new lens for interpreting symbolic logic.","authors":["Suhaib A. Jama"],"url":"https://arxiv.org/abs/2505.02261"}
{"created":"2025-05-06","title":"Parameter-Efficient Transformer Embeddings","abstract":"Embedding layers in transformer-based NLP models typically account for the largest share of model parameters, scaling with vocabulary size but not yielding performance gains proportional to scale. We propose an alternative approach in which token embedding vectors are first generated deterministically, directly from the token IDs using a Fourier expansion of their normalized values, followed by a lightweight multilayer perceptron (MLP) that captures higher-order interactions. We train standard transformers and our architecture on natural language inference tasks (SNLI and MNLI), and evaluate zero-shot performance on sentence textual similarity (STS-B). Our results demonstrate that the proposed method achieves competitive performance using significantly fewer parameters, trains faster, and operates effectively without the need for dropout. This proof-of-concept study highlights the potential for scalable, memory-efficient language models and motivates further large-scale experimentation based on our findings.","authors":["Henry Ndubuaku","Mouad Talhi"],"url":"https://arxiv.org/abs/2505.02266"}
{"created":"2025-05-06","title":"Phantom Domain Finite Element Method: A novel approach for heterogeneous materials","abstract":"In this paper, we introduce the Phantom Domain Finite Element Method (PDFEM), a novel computational approach tailored for the efficient analysis of heterogeneous and composite materials. Inspired by fictitious domain methods, this method employs a structured mesh to discretize the entire material domain while utilizing separate, independent meshes for the inclusions. These inclusion meshes are coupled to the structured mesh via a substitution matrix, enabling them to act as phantom meshes that do not directly contribute to the final system of equations. This framework offers significant advantages, including enhanced flexibility in handling complex inclusion geometries and improved computational efficiency. To assess the accuracy and robustness of the proposed method, numerical experiments are conducted on structures containing inclusions of various geometries. In order to emphasize the efficiency of the PDFEM method, a numerical simulation is presented to highlight its advantages in the case of long natural fibers, such as flax and linen. These simulations are compared against FEM calculations, demonstrating the efficiency of PDFEM. Indeed, meshing such fine structures requires an extremely high number of elements, and in some cases, meshing becomes particularly challenging due to the complexity of the geometries.","authors":["Tianlong He","Philippe Karamian-Surville","Daniel Cho\\\"i"],"url":"https://arxiv.org/abs/2505.02268"}
{"created":"2025-05-06","title":"Real-time Spatial Retrieval Augmented Generation for Urban Environments","abstract":"The proliferation of Generative Artificial Ingelligence (AI), especially Large Language Models, presents transformative opportunities for urban applications through Urban Foundation Models. However, base models face limitations, as they only contain the knowledge available at the time of training, and updating them is both time-consuming and costly. Retrieval Augmented Generation (RAG) has emerged in the literature as the preferred approach for injecting contextual information into Foundation Models. It prevails over techniques such as fine-tuning, which are less effective in dynamic, real-time scenarios like those found in urban environments. However, traditional RAG architectures, based on semantic databases, knowledge graphs, structured data, or AI-powered web searches, do not fully meet the demands of urban contexts. Urban environments are complex systems characterized by large volumes of interconnected data, frequent updates, real-time processing requirements, security needs, and strong links to the physical world. This work proposes a real-time spatial RAG architecture that defines the necessary components for the effective integration of generative AI into cities, leveraging temporal and spatial filtering capabilities through linked data. The proposed architecture is implemented using FIWARE, an ecosystem of software components to develop smart city solutions and digital twins. The design and implementation are demonstrated through the use case of a tourism assistant in the city of Madrid. The use case serves to validate the correct integration of Foundation Models through the proposed RAG architecture.","authors":["David Nazareno Campo","Javier Conde","\\'Alvaro Alonso","Gabriel Huecas","Joaqu\\'in Salvach\\'ua","Pedro Reviriego"],"url":"https://arxiv.org/abs/2505.02271"}
{"created":"2025-05-06","title":"Robust Localization, Mapping, and Navigation for Quadruped Robots","abstract":"Quadruped robots are currently a widespread platform for robotics research, thanks to powerful Reinforcement Learning controllers and the availability of cheap and robust commercial platforms. However, to broaden the adoption of the technology in the real world, we require robust navigation stacks relying only on low-cost sensors such as depth cameras. This paper presents a first step towards a robust localization, mapping, and navigation system for low-cost quadruped robots. In pursuit of this objective we combine contact-aided kinematic, visual-inertial odometry, and depth-stabilized vision, enhancing stability and accuracy of the system. Our results in simulation and two different real-world quadruped platforms show that our system can generate an accurate 2D map of the environment, robustly localize itself, and navigate autonomously. Furthermore, we present in-depth ablation studies of the important components of the system and their impact on localization accuracy. Videos, code, and additional experiments can be found on the project website: https://sites.google.com/view/low-cost-quadruped-slam","authors":["Dyuman Aditya","Junning Huang","Nico Bohlinger","Piotr Kicki","Krzysztof Walas","Jan Peters","Matteo Luperto","Davide Tateo"],"url":"https://arxiv.org/abs/2505.02272"}
{"created":"2025-05-06","title":"Demystifying optimized prompts in language models","abstract":"Modern language models (LMs) are not robust to out-of-distribution inputs. Machine generated (``optimized'') prompts can be used to modulate LM outputs and induce specific behaviors while appearing completely uninterpretable. In this work, we investigate the composition of optimized prompts, as well as the mechanisms by which LMs parse and build predictions from optimized prompts. We find that optimized prompts primarily consist of punctuation and noun tokens which are more rare in the training data. Internally, optimized prompts are clearly distinguishable from natural language counterparts based on sparse subsets of the model's activations. Across various families of instruction-tuned models, optimized prompts follow a similar path in how their representations form through the network.","authors":["Rimon Melamed","Lucas H. McCabe","H. Howie Huang"],"url":"https://arxiv.org/abs/2505.02273"}
{"created":"2025-05-06","title":"On the Need for a Statistical Foundation in Scenario-Based Testing of Autonomous Vehicles","abstract":"Scenario-based testing has emerged as a common method for autonomous vehicles (AVs) safety, offering a more efficient alternative to mile-based testing by focusing on high-risk scenarios. However, fundamental questions persist regarding its stopping rules, residual risk estimation, debug effectiveness, and the impact of simulation fidelity on safety claims. This paper argues that a rigorous statistical foundation is essential to address these challenges and enable rigorous safety assurance. By drawing parallels between AV testing and traditional software testing methodologies, we identify shared research gaps and reusable solutions. We propose proof-of-concept models to quantify the probability of failure per scenario (pfs) and evaluate testing effectiveness under varying conditions. Our analysis reveals that neither scenario-based nor mile-based testing universally outperforms the other. Furthermore, we introduce Risk Estimation Fidelity (REF), a novel metric to certify the alignment of synthetic and real-world testing outcomes, ensuring simulation-based safety claims are statistically defensible.","authors":["Xingyu Zhao","Robab Aghazadeh-Chakherlou","Chih-Hong Cheng","Peter Popov","Lorenzo Strigini"],"url":"https://arxiv.org/abs/2505.02274"}
{"created":"2025-05-06","title":"A Path Less Traveled: Reimagining Software Engineering Automation via a Neurosymbolic Paradigm","abstract":"The emergence of Large Code Models (LCMs) has transformed software engineering (SE) automation, driving significant advancements in tasks such as code generation, source code documentation, code review, and bug fixing. However, these advancements come with trade-offs: achieving high performance often entails exponential computational costs, reduced interpretability, and an increasing dependence on data-intensive models with hundreds of billions of parameters. In this paper, we propose Neurosymbolic Software Engineering, in short NSE, as a promising paradigm combining neural learning with symbolic (rule-based) reasoning, while strategically introducing a controlled source of chaos to simulate the complex dynamics of real-world software systems. This hybrid methodology aims to enhance efficiency, reliability, and transparency in AI-driven software engineering while introducing controlled randomness to adapt to evolving requirements, unpredictable system behaviors, and non-deterministic execution environments. By redefining the core principles of AI-driven software engineering automation, NSE lays the groundwork for solutions that are more adaptable, transparent, and closely aligned with the evolving demands of modern software development practices.","authors":["Antonio Mastropaolo","Denys Poshyvanyk"],"url":"https://arxiv.org/abs/2505.02275"}
{"created":"2025-05-06","title":"Epistemic Wrapping for Uncertainty Quantification","abstract":"Uncertainty estimation is pivotal in machine learning, especially for classification tasks, as it improves the robustness and reliability of models. We introduce a novel `Epistemic Wrapping' methodology aimed at improving uncertainty estimation in classification. Our approach uses Bayesian Neural Networks (BNNs) as a baseline and transforms their outputs into belief function posteriors, effectively capturing epistemic uncertainty and offering an efficient and general methodology for uncertainty quantification. Comprehensive experiments employing a Bayesian Neural Network (BNN) baseline and an Interval Neural Network for inference on the MNIST, Fashion-MNIST, CIFAR-10 and CIFAR-100 datasets demonstrate that our Epistemic Wrapper significantly enhances generalisation and uncertainty quantification.","authors":["Maryam Sultana","Neil Yorke-Smith","Kaizheng Wang","Shireen Kudukkil Manchingal","Muhammad Mubashar","Fabio Cuzzolin"],"url":"https://arxiv.org/abs/2505.02277"}
{"created":"2025-05-06","title":"Compositional Image-Text Matching and Retrieval by Grounding Entities","abstract":"Vision-language pretraining on large datasets of images-text pairs is one of the main building blocks of current Vision-Language Models. While with additional training, these models excel in various downstream tasks, including visual question answering, image captioning, and visual commonsense reasoning. However, a notable weakness of pretrained models like CLIP, is their inability to perform entity grounding and compositional image and text matching~\\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR, learninglocalizeCVPR24}. In this work we propose a novel learning-free zero-shot augmentation of CLIP embeddings that has favorable compositional properties. We compute separate embeddings of sub-images of object entities and relations that are localized by the state of the art open vocabulary detectors and dynamically adjust the baseline global image embedding. % The final embedding is obtained by computing a weighted combination of the sub-image embeddings. The resulting embedding is then utilized for similarity computation with text embedding, resulting in a average 1.5\\% improvement in image-text matching accuracy on the Visual Genome and SVO Probes datasets~\\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings demonstrate superior retrieval performance, thus achieving significant gains on the Flickr30K and MS-COCO retrieval benchmarks~\\cite{flickr30ke, mscoco}, improving the state-of-the-art Recall@1 by 12\\% and 0.4\\%, respectively. Our code is available at https://github.com/madhukarreddyvongala/GroundingCLIP.","authors":["Madhukar Reddy Vongala","Saurabh Srivastava","Jana Ko\\v{s}eck\\'a"],"url":"https://arxiv.org/abs/2505.02278"}
{"created":"2025-05-06","title":"A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)","abstract":"Large language model (LLM)-powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in distinct deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP introduces REST-native messaging via multi-part messages and asynchronous streaming to support multimodal agent responses. A2A enables peer-to-peer task outsourcing through capability-based Agent Cards, facilitating enterprise-scale workflows. ANP supports open-network agent discovery and secure collaboration using decentralized identifiers (DIDs) and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for multimodal messaging, A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.","authors":["Abul Ehtesham","Aditi Singh","Gaurav Kumar Gupta","Saket Kumar"],"url":"https://arxiv.org/abs/2505.02279"}
{"created":"2025-05-06","title":"Conformal Prediction for Verifiable Learned Query Optimization","abstract":"Query optimization is critical in relational databases. Recently, numerous Learned Query Optimizers (LQOs) have been proposed, demonstrating superior performance over traditional hand-crafted query optimizers after short training periods. However, the opacity and instability of machine learning models have limited their practical applications. To address this issue, we are the first to formulate the LQO verification as a Conformal Prediction (CP) problem. We first construct the CP model and obtain user-controlled bounded ranges for the actual latency of LQO plans before execution. Then, we introduce CP-based runtime verification along with violation handling to ensure performance prior to execution. For both scenarios, we further extend our framework to handle distribution shifts in the dynamic environment using adaptive CP approaches. Finally, we present CP-guided plan search, which uses actual latency upper bounds from CP to heuristically guide query plan construction. We integrated our verification framework into three LQOs (Balsa, Lero, and RTOS) and conducted evaluations on the JOB and TPC-H workloads. Experimental results demonstrate that our method is both accurate and efficient. Our CP-based approaches achieve tight upper bounds, reliably detect and handle violations. Adaptive CP maintains accurate confidence levels even in the presence of distribution shifts, and the CP-guided plan search improves both query plan quality (up to 9.84x) and planning time, with a reduction of up to 74.4% for a single query and 9.96% across all test queries from trained LQOs.","authors":["Hanwen Liu","Shashank Giridhara","Ibrahim Sabek"],"url":"https://arxiv.org/abs/2505.02284"}
{"created":"2025-05-06","title":"Content Addressable Memory Design with Reference Resistor for Improved Search Resolution","abstract":"Despite the parallel in-memory search capabilities of content addressable memories (CAMs), their use in applications is constrained by their limited resolution that worsens as they are scaled to larger arrays or advanced nodes. In this work we present experimental results for a novel back-end-of-line compatible reference resistive device that can significantly improve the search resolution of CAMs implemented with CMOS and beyond-CMOS technologies to less than or equal to 5-bits.","authors":["Siri Narla","Steven J. Koester","Rebecca A. Dawley","Ageeth A. Bol","Piyush Kumar","Azad Naeemi"],"url":"https://arxiv.org/abs/2505.02285"}
{"created":"2025-05-06","title":"Continuous Normalizing Flows for Uncertainty-Aware Human Pose Estimation","abstract":"Human Pose Estimation (HPE) is increasingly important for applications like virtual reality and motion analysis, yet current methods struggle with balancing accuracy, computational efficiency, and reliable uncertainty quantification (UQ). Traditional regression-based methods assume fixed distributions, which might lead to poor UQ. Heatmap-based methods effectively model the output distribution using likelihood heatmaps, however, they demand significant resources. To address this, we propose Continuous Flow Residual Estimation (CFRE), an integration of Continuous Normalizing Flows (CNFs) into regression-based models, which allows for dynamic distribution adaptation. Through extensive experiments, we show that CFRE leads to better accuracy and uncertainty quantification with retained computational efficiency on both 2D and 3D human pose estimation tasks.","authors":["Shipeng Liu","Ziliang Xiong","Bastian Wandt","Per-Erik Forss\\'en"],"url":"https://arxiv.org/abs/2505.02287"}
{"created":"2025-05-06","title":"Universal Approximation Theorem of Deep Q-Networks","abstract":"We establish a continuous-time framework for analyzing Deep Q-Networks (DQNs) via stochastic control and Forward-Backward Stochastic Differential Equations (FBSDEs). Considering a continuous-time Markov Decision Process (MDP) driven by a square-integrable martingale, we analyze DQN approximation properties. We show that DQNs can approximate the optimal Q-function on compact sets with arbitrary accuracy and high probability, leveraging residual network approximation theorems and large deviation bounds for the state-action process. We then analyze the convergence of a general Q-learning algorithm for training DQNs in this setting, adapting stochastic approximation theorems. Our analysis emphasizes the interplay between DQN layer count, time discretization, and the role of viscosity solutions (primarily for the value function $V^*$) in addressing potential non-smoothness of the optimal Q-function. This work bridges deep reinforcement learning and stochastic control, offering insights into DQNs in continuous-time settings, relevant for applications with physical systems or high-frequency data.","authors":["Qian Qi"],"url":"https://arxiv.org/abs/2505.02288"}
{"created":"2025-05-06","title":"Dexterous Contact-Rich Manipulation via the Contact Trust Region","abstract":"What is a good local description of contact dynamics for contact-rich manipulation, and where can we trust this local description? While many approaches often rely on the Taylor approximation of dynamics with an ellipsoidal trust region, we argue that such approaches are fundamentally inconsistent with the unilateral nature of contact. As a remedy, we present the Contact Trust Region (CTR), which captures the unilateral nature of contact while remaining efficient for computation. With CTR, we first develop a Model-Predictive Control (MPC) algorithm capable of synthesizing local contact-rich plans. Then, we extend this capability to plan globally by stitching together local MPC plans, enabling efficient and dexterous contact-rich manipulation. To verify the performance of our method, we perform comprehensive evaluations, both in high-fidelity simulation and on hardware, on two contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand system. On both systems, our method offers a significantly lower-compute alternative to existing RL-based approaches to contact-rich manipulation. In particular, our Allegro in-hand manipulation policy, in the form of a roadmap, takes fewer than 10 minutes to build offline on a standard laptop using just its CPU, with online inference taking just a few seconds. Experiment data, video and code are available at ctr.theaiinstitute.com.","authors":["H. J. Terry Suh","Tao Pang","Tong Zhao","Russ Tedrake"],"url":"https://arxiv.org/abs/2505.02291"}
{"created":"2025-05-06","title":"Resolving Conflicting Constraints in Multi-Agent Reinforcement Learning with Layered Safety","abstract":"Preventing collisions in multi-robot navigation is crucial for deployment. This requirement hinders the use of learning-based approaches, such as multi-agent reinforcement learning (MARL), on their own due to their lack of safety guarantees. Traditional control methods, such as reachability and control barrier functions, can provide rigorous safety guarantees when interactions are limited only to a small number of robots. However, conflicts between the constraints faced by different agents pose a challenge to safe multi-agent coordination.","authors":["Jason J. Choi","Jasmine Jerry Aloor","Jingqi Li","Maria G. Mendoza","Hamsa Balakrishnan","Claire J. Tomlin"],"url":"https://arxiv.org/abs/2505.02293"}
{"created":"2025-05-06","title":"RNBF: Real-Time RGB-D Based Neural Barrier Functions for Safe Robotic Navigation","abstract":"Autonomous safe navigation in unstructured and novel environments poses significant challenges, especially when environment information can only be provided through low-cost vision sensors. Although safe reactive approaches have been proposed to ensure robot safety in complex environments, many base their theory off the assumption that the robot has prior knowledge on obstacle locations and geometries. In this paper, we present a real-time, vision-based framework that constructs continuous, first-order differentiable Signed Distance Fields (SDFs) of unknown environments without any pre-training. Our proposed method ensures full compatibility with established SDF-based reactive controllers. To achieve robust performance under practical sensing conditions, our approach explicitly accounts for noise in affordable RGB-D cameras, refining the neural SDF representation online for smoother geometry and stable gradient estimates. We validate the proposed method in simulation and real-world experiments using a Fetch robot.","authors":["Satyajeet Das","Yifan Xue","Haoming Li","Nadia Figueroa"],"url":"https://arxiv.org/abs/2505.02294"}
{"created":"2025-05-06","title":"Entropy-Guided Sampling of Flat Modes in Discrete Spaces","abstract":"Sampling from flat modes in discrete spaces is a crucial yet underexplored problem. Flat modes represent robust solutions and have broad applications in combinatorial optimization and discrete generative modeling. However, existing sampling algorithms often overlook the mode volume and struggle to capture flat modes effectively. To address this limitation, we propose \\emph{Entropic Discrete Langevin Proposal} (EDLP), which incorporates local entropy into the sampling process through a continuous auxiliary variable under a joint distribution. The local entropy term guides the discrete sampler toward flat modes with a small overhead. We provide non-asymptotic convergence guarantees for EDLP in locally log-concave discrete distributions. Empirically, our method consistently outperforms traditional approaches across tasks that require sampling from flat basins, including Bernoulli distribution, restricted Boltzmann machines, combinatorial optimization, and binary neural networks.","authors":["Pinaki Mohanty","Riddhiman Bhattacharya","Ruqi Zhang"],"url":"https://arxiv.org/abs/2505.02296"}
{"created":"2025-05-06","title":"Adaptive Scoring and Thresholding with Human Feedback for Robust Out-of-Distribution Detection","abstract":"Machine Learning (ML) models are trained on in-distribution (ID) data but often encounter out-of-distribution (OOD) inputs during deployment -- posing serious risks in safety-critical domains. Recent works have focused on designing scoring functions to quantify OOD uncertainty, with score thresholds typically set based solely on ID data to achieve a target true positive rate (TPR), since OOD data is limited before deployment. However, these TPR-based thresholds leave false positive rates (FPR) uncontrolled, often resulting in high FPRs where OOD points are misclassified as ID. Moreover, fixed scoring functions and thresholds lack the adaptivity needed to handle newly observed, evolving OOD inputs, leading to sub-optimal performance. To address these challenges, we propose a human-in-the-loop framework that \\emph{safely updates both scoring functions and thresholds on the fly} based on real-world OOD inputs. Our method maximizes TPR while strictly controlling FPR at all times, even as the system adapts over time. We provide theoretical guarantees for FPR control under stationary conditions and present extensive empirical evaluations on OpenOOD benchmarks to demonstrate that our approach outperforms existing methods by achieving higher TPRs while maintaining FPR control.","authors":["Daisuke Yamada","Harit Vishwakarma","Ramya Korlakai Vinayak"],"url":"https://arxiv.org/abs/2505.02299"}
{"created":"2025-05-06","title":"Generative Sign-description Prompts with Multi-positive Contrastive Learning for Sign Language Recognition","abstract":"Sign language recognition (SLR) faces fundamental challenges in creating accurate annotations due to the inherent complexity of simultaneous manual and non-manual signals. To the best of our knowledge, this is the first work to integrate generative large language models (LLMs) into SLR tasks. We propose a novel Generative Sign-description Prompts Multi-positive Contrastive learning (GSP-MC) method that leverages retrieval-augmented generation (RAG) with domain-specific LLMs, incorporating multi-step prompt engineering and expert-validated sign language corpora to produce precise multipart descriptions. The GSP-MC method also employs a dual-encoder architecture to bidirectionally align hierarchical skeleton features with multiple text descriptions (global, synonym, and part level) through probabilistic matching. Our approach combines global and part-level losses, optimizing KL divergence to ensure robust alignment across all relevant text-skeleton pairs while capturing both sign-level semantics and detailed part dynamics. Experiments demonstrate state-of-the-art performance against existing methods on the Chinese SLR500 (reaching 97.1%) and Turkish AUTSL datasets (97.07% accuracy). The method's cross-lingual effectiveness highlight its potential for developing inclusive communication technologies.","authors":["Siyu Liang","Yunan Li","Wentian Xin","Huizhou Chen","Xujie Liu","Kang Liu","Qiguang Miao"],"url":"https://arxiv.org/abs/2505.02304"}
{"created":"2025-05-06","title":"Refining Fuzzed Crashing Inputs for Better Fault Diagnosis","abstract":"We present DiffMin, a technique that refines a fuzzed crashing input to gain greater similarities to given passing inputs to help developers analyze the crashing input to identify the failure-inducing condition and locate buggy code for debugging. DiffMin iteratively applies edit actions to transform a fuzzed input while preserving the crash behavior. Our pilot study with the Magma benchmark demonstrates that DiffMin effectively minimizes the differences between crashing and passing inputs while enhancing the accuracy of spectrum-based fault localization, highlighting its potential as a valuable pre-debugging step after greybox fuzzing.","authors":["Kieun Kim (Chungbuk National University)","Seongmin Lee (Max Planck Institute for Security and Privacy)","Shin Hong (Chungbuk National University)"],"url":"https://arxiv.org/abs/2505.02305"}
{"created":"2025-05-06","title":"SafeMate: A Model Context Protocol-Based Multimodal Agent for Emergency Preparedness","abstract":"Despite the abundance of public safety documents and emergency protocols, most individuals remain ill-equipped to interpret and act on such information during crises. Traditional emergency decision support systems (EDSS) are designed for professionals and rely heavily on static documents like PDFs or SOPs, which are difficult for non-experts to navigate under stress. This gap between institutional knowledge and public accessibility poses a critical barrier to effective emergency preparedness and response.","authors":["Junfeng Jiao","Jihyung Park","Yiming Xu","Lucy Atkinson"],"url":"https://arxiv.org/abs/2505.02306"}
{"created":"2025-05-06","title":"Net Occurrences in Fibonacci and Thue-Morse Words","abstract":"A net occurrence of a repeated string in a text is an occurrence with unique left and right extensions, and the net frequency of the string is the number of its net occurrences in the text. Originally introduced for applications in Natural Language Processing, net frequency has recently gained attention for its algorithmic aspects. Guo et al. [CPM 2024] and Ohlebusch et al. [SPIRE 2024] focus on its computation in the offline setting, while Guo et al. [SPIRE 2024], Inenaga [arXiv 2024], and Mieno and Inenaga [CPM 2025] tackle the online counterpart. Mieno and Inenaga also characterize net occurrences in terms of the minimal unique substrings of the text. Additionally, Guo et al. [CPM 2024] initiate the study of net occurrences in Fibonacci words to establish a lower bound on the asymptotic running time of algorithms. Although there has been notable progress in algorithmic developments and some initial combinatorial insights, the combinatorial aspects of net occurrences have yet to be thoroughly examined. In this work, we make two key contributions. First, we confirm the conjecture that each Fibonacci word contains exactly three net occurrences. Second, we show that each Thue-Morse word contains exactly nine net occurrences. To achieve these results, we introduce the notion of overlapping net occurrence cover, which narrows down the candidate net occurrences in any text. Furthermore, we provide a precise characterization of occurrences of Fibonacci and Thue-Morse words of smaller order, offering structural insights that may have independent interest and potential applications in algorithm analysis and combinatorial properties of these words.","authors":["Peaker Guo","Kaisei Kishi"],"url":"https://arxiv.org/abs/2505.02307"}
{"created":"2025-05-06","title":"Enabling Local Neural Operators to perform Equation-Free System-Level Analysis","abstract":"Neural Operators (NOs) provide a powerful framework for computations involving physical laws that can be modelled by (integro-) partial differential equations (PDEs), directly learning maps between infinite-dimensional function spaces that bypass both the explicit equation identification and their subsequent numerical solving. Still, NOs have so far primarily been employed to explore the dynamical behavior as surrogates of brute-force temporal simulations/predictions. Their potential for systematic rigorous numerical system-level tasks, such as fixed-point, stability, and bifurcation analysis - crucial for predicting irreversible transitions in real-world phenomena - remains largely unexplored. Toward this aim, inspired by the Equation-Free multiscale framework, we propose and implement a framework that integrates (local) NOs with advanced iterative numerical methods in the Krylov subspace, so as to perform efficient system-level stability and bifurcation analysis of large-scale dynamical systems. Beyond fixed point, stability, and bifurcation analysis enabled by local in time NOs, we also demonstrate the usefulness of local in space as well as in space-time (\"patch\") NOs in accelerating the computer-aided analysis of spatiotemporal dynamics. We illustrate our framework via three nonlinear PDE benchmarks: the 1D Allen-Cahn equation, which undergoes multiple concatenated pitchfork bifurcations; the Liouville-Bratu-Gelfand PDE, which features a saddle-node tipping point; and the FitzHugh-Nagumo (FHN) model, consisting of two coupled PDEs that exhibit both Hopf and saddle-node bifurcations.","authors":["Gianluca Fabiani","Hannes Vandecasteele","Somdatta Goswami","Constantinos Siettos","Ioannis G. Kevrekidis"],"url":"https://arxiv.org/abs/2505.02308"}
{"created":"2025-05-06","title":"Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques","abstract":"Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.","authors":["Sanjay Surendranath Girija","Shashank Kapoor","Lakshit Arora","Dipen Pradhan","Aman Raj","Ankit Shetgaonkar"],"url":"https://arxiv.org/abs/2505.02309"}
{"created":"2025-05-06","title":"Invoke Interfaces Only When Needed: Adaptive Invocation for Large Language Models in Question Answering","abstract":"The collaborative paradigm of large and small language models (LMs) effectively balances performance and cost, yet its pivotal challenge lies in precisely pinpointing the moment of invocation when hallucinations arise in small LMs. Previous optimization efforts primarily focused on post-processing techniques, which were separate from the reasoning process of LMs, resulting in high computational costs and limited effectiveness. In this paper, we propose a practical invocation evaluation metric called AttenHScore, which calculates the accumulation and propagation of hallucinations during the generation process of small LMs, continuously amplifying potential reasoning errors. By dynamically adjusting the detection threshold, we achieve more accurate real-time invocation of large LMs. Additionally, considering the limited reasoning capacity of small LMs, we leverage uncertainty-aware knowledge reorganization to assist them better capture critical information from different text chunks. Extensive experiments reveal that our AttenHScore outperforms most baseline in enhancing real-time hallucination detection capabilities across multiple QA datasets, especially when addressing complex queries. Moreover, our strategies eliminate the need for additional model training and display flexibility in adapting to various transformer-based LMs.","authors":["Jihao Zhao","Chunlai Zhou","Biao Qin"],"url":"https://arxiv.org/abs/2505.02311"}
{"created":"2025-05-06","title":"Wii: Dynamic Budget Reallocation In Index Tuning","abstract":"Index tuning aims to find the optimal index configuration for an input workload. It is often a time-consuming and resource-intensive process, largely attributed to the huge amount of \"what-if\" calls made to the query optimizer during configuration enumeration. Therefore, in practice it is desirable to set a budget constraint that limits the number of what-if calls allowed. This yields a new problem of budget allocation, namely, deciding on which query-configuration pairs (QCPs) to issue what-if calls. Unfortunately, optimal budget allocation is NP-hard, and budget allocation decisions made by existing solutions can be inferior. In particular, many of the what-if calls allocated by using existing solutions are devoted to QCPs whose what-if costs can be approximated by using cost derivation, a well-known technique that is computationally much more efficient and has been adopted by commercial index tuning software. This results in considerable waste of the budget, as these what-if calls are unnecessary. In this paper, we propose \"Wii,\" a lightweight mechanism that aims to avoid such spurious what-if calls. It can be seamlessly integrated with existing configuration enumeration algorithms. Experimental evaluation on top of both standard industrial benchmarks and real workloads demonstrates that Wii can eliminate significant number of spurious what-if calls. Moreover, by reallocating the saved budget to QCPs where cost derivation is less accurate, existing algorithms can be significantly improved in terms of the final configuration found.","authors":["Xiaoying Wang","Wentao Wu","Chi Wang","Vivek Narasayya","Surajit Chaudhuri"],"url":"https://arxiv.org/abs/2505.02312"}
{"created":"2025-05-06","title":"What Is AI Safety? What Do We Want It to Be?","abstract":"The field of AI safety seeks to prevent or reduce the harms caused by AI systems. A simple and appealing account of what is distinctive of AI safety as a field holds that this feature is constitutive: a research project falls within the purview of AI safety just in case it aims to prevent or reduce the harms caused by AI systems. Call this appealingly simple account The Safety Conception of AI safety. Despite its simplicity and appeal, we argue that The Safety Conception is in tension with at least two trends in the ways AI safety researchers and organizations think and talk about AI safety: first, a tendency to characterize the goal of AI safety research in terms of catastrophic risks from future systems; second, the increasingly popular idea that AI safety can be thought of as a branch of safety engineering. Adopting the methodology of conceptual engineering, we argue that these trends are unfortunate: when we consider what concept of AI safety it would be best to have, there are compelling reasons to think that The Safety Conception is the answer. Descriptively, The Safety Conception allows us to see how work on topics that have historically been treated as central to the field of AI safety is continuous with work on topics that have historically been treated as more marginal, like bias, misinformation, and privacy. Normatively, taking The Safety Conception seriously means approaching all efforts to prevent or mitigate harms from AI systems based on their merits rather than drawing arbitrary distinctions between them.","authors":["Jacqueline Harding","Cameron Domenico Kirk-Giannini"],"url":"https://arxiv.org/abs/2505.02313"}
{"created":"2025-05-06","title":"NeuroSim V1.5: Improved Software Backbone for Benchmarking Compute-in-Memory Accelerators with Device and Circuit-level Non-idealities","abstract":"The exponential growth of artificial intelligence (AI) applications has exposed the inefficiency of conventional von Neumann architectures, where frequent data transfers between compute units and memory create significant energy and latency bottlenecks. Analog Computing-in-Memory (ACIM) addresses this challenge by performing multiply-accumulate (MAC) operations directly in the memory arrays, substantially reducing data movement. However, designing robust ACIM accelerators requires accurate modeling of device- and circuit-level non-idealities. In this work, we present NeuroSim V1.5, introducing several key advances: (1) seamless integration with TensorRT's post-training quantization flow enabling support for more neural networks including transformers, (2) a flexible noise injection methodology built on pre-characterized statistical models, making it straightforward to incorporate data from SPICE simulations or silicon measurements, (3) expanded device support including emerging non-volatile capacitive memories, and (4) up to 6.5x faster runtime than NeuroSim V1.4 through optimized behavioral simulation. The combination of these capabilities uniquely enables systematic design space exploration across both accuracy and hardware efficiency metrics. Through multiple case studies, we demonstrate optimization of critical design parameters while maintaining network accuracy. By bridging high-fidelity noise modeling with efficient simulation, NeuroSim V1.5 advances the design and validation of next-generation ACIM accelerators. All NeuroSim versions are available open-source at https://github.com/neurosim/NeuroSim.","authors":["James Read","Ming-Yen Lee","Wei-Hsing Huang","Yuan-Chun Luo","Anni Lu","Shimeng Yu"],"url":"https://arxiv.org/abs/2505.02314"}
{"created":"2025-05-06","title":"A longitudinal analysis of misinformation, polarization and toxicity on Bluesky after its public launch","abstract":"Bluesky is a decentralized, Twitter-like social media platform that has rapidly gained popularity. Following an invite-only phase, it officially opened to the public on February 6th, 2024, leading to a significant expansion of its user base. In this paper, we present a longitudinal analysis of user activity in the two months surrounding its public launch, examining how the platform evolved due to this rapid growth. Our analysis reveals that Bluesky exhibits an activity distribution comparable to more established social platforms, yet it features a higher volume of original content relative to reshared posts and maintains low toxicity levels. We further investigate the political leanings of its user base, misinformation dynamics, and engagement in harmful conversations. Our findings indicate that Bluesky users predominantly lean left politically and tend to share high-credibility sources. After the platform's public launch, an influx of new users, particularly those posting in English and Japanese, contributed to a surge in activity. Among them, several accounts displayed suspicious behaviors, such as mass-following users and sharing content from low-credibility news sources. Some of these accounts have already been flagged as spam or suspended, suggesting that Bluesky's moderation efforts have been effective.","authors":["Gianluca Nogara","Erfan Samieyan Sahneh","Matthew R. DeVerna","Nick Liu","Luca Luceri","Filippo Menczer","Francesco Pierri","Silvia Giordano"],"url":"https://arxiv.org/abs/2505.02317"}
{"created":"2025-05-06","title":"Efficient Krylov methods for linear response in plane-wave electronic structure calculations","abstract":"We propose a novel algorithm based on inexact GMRES methods for linear response calculations in density functional theory. Such calculations require iteratively solving a nested linear problem $\\mathcal{E} \\delta\\rho = b$ to obtain the variation of the electron density $\\delta \\rho$. Notably each application of the dielectric operator $\\mathcal{E}$ in turn requires the iterative solution of multiple linear systems, the Sternheimer equations. We develop computable bounds to estimate the accuracy of the density variation given the tolerances to which the Sternheimer equations have been solved. Based on this result we suggest reliable strategies for adaptively selecting the convergence tolerances of the Sternheimer equations, such that each applications of $\\mathcal{E}$ is no more accurate than needed. Experiments on challenging materials systems of practical relevance demonstrate our strategies to achieve superlinear convergence as well as a reduction of computational time by about 40% while preserving the accuracy of the returned response solution. Our algorithm seamlessly combines with standard preconditioning approaches known from the context of self-consistent field problems making it a promising framework for efficient response solvers based on Krylov subspace techniques.","authors":["Michael F. Herbst","Bonan Sun"],"url":"https://arxiv.org/abs/2505.02319"}
{"created":"2025-05-06","title":"HyperTree Planning: Enhancing LLM Reasoning via Hierarchical Thinking","abstract":"Recent advancements have significantly enhanced the performance of large language models (LLMs) in tackling complex reasoning tasks, achieving notable success in domains like mathematical and logical reasoning. However, these methods encounter challenges with complex planning tasks, primarily due to extended reasoning steps, diverse constraints, and the challenge of handling multiple distinct sub-tasks. To address these challenges, we propose HyperTree Planning (HTP), a novel reasoning paradigm that constructs hypertree-structured planning outlines for effective planning. The hypertree structure enables LLMs to engage in hierarchical thinking by flexibly employing the divide-and-conquer strategy, effectively breaking down intricate reasoning steps, accommodating diverse constraints, and managing multiple distinct sub-tasks in a well-organized manner. We further introduce an autonomous planning framework that completes the planning process by iteratively refining and expanding the hypertree-structured planning outlines. Experiments demonstrate the effectiveness of HTP, achieving state-of-the-art accuracy on the TravelPlanner benchmark with Gemini-1.5-Pro, resulting in a 3.6 times performance improvement over o1-preview.","authors":["Runquan Gui","Zhihai Wang","Jie Wang","Chi Ma","Huiling Zhen","Mingxuan Yuan","Jianye Hao","Defu Lian","Enhong Chen","Feng Wu"],"url":"https://arxiv.org/abs/2505.02322"}
{"created":"2025-05-06","title":"Riemannian Direct Trajectory Optimization of Rigid Bodies on Matrix Lie Groups","abstract":"Designing dynamically feasible trajectories for rigid bodies is a fundamental problem in robotics. Although direct trajectory optimization is widely applied to solve this problem, inappropriate parameterizations of rigid body dynamics often result in slow convergence and violations of the intrinsic topological structure of the rotation group. This paper introduces a Riemannian optimization framework for direct trajectory optimization of rigid bodies. We first use the Lie Group Variational Integrator to formulate the discrete rigid body dynamics on matrix Lie groups. We then derive the closed-form first- and second-order Riemannian derivatives of the dynamics. Finally, this work applies a line-search Riemannian Interior Point Method (RIPM) to perform trajectory optimization with general nonlinear constraints. As the optimization is performed on matrix Lie groups, it is correct-by-construction to respect the topological structure of the rotation group and be free of singularities. The paper demonstrates that both the derivative evaluations and Newton steps required to solve the RIPM exhibit linear complexity with respect to the planning horizon and system degrees of freedom. Simulation results illustrate that the proposed method is faster than conventional methods by an order of magnitude in challenging robotics tasks.","authors":["Sangli Teng","Tzu-Yuan Lin","William A Clark","Ram Vasudevan","Maani Ghaffari"],"url":"https://arxiv.org/abs/2505.02323"}
{"created":"2025-05-06","title":"From Course to Skill: Evaluating LLM Performance in Curricular Analytics","abstract":"Curricular analytics (CA) -- systematic analysis of curricula data to inform program and course refinement -- becomes an increasingly valuable tool to help institutions align academic offerings with evolving societal and economic demands. Large language models (LLMs) are promising for handling large-scale, unstructured curriculum data, but it remains uncertain how reliably LLMs can perform CA tasks. In this paper, we systematically evaluate four text alignment strategies based on LLMs or traditional NLP methods for skill extraction, a core task in CA. Using a stratified sample of 400 curriculum documents of different types and a human-LLM collaborative evaluation framework, we find that retrieval-augmented generation (RAG) to be the top-performing strategy across all types of curriculum documents, while zero-shot prompting performs worse than traditional NLP methods in most cases. Our findings highlight the promise of LLMs in analyzing brief and abstract curriculum documents, but also reveal that their performance can vary significantly depending on model selection and prompting strategies. This underscores the importance of carefully evaluating the performance of LLM-based strategies before large-scale deployment.","authors":["Zhen Xu","Xinjin Li","Yingqi Huan","Veronica Minaya","Renzhe Yu"],"url":"https://arxiv.org/abs/2505.02324"}
{"created":"2025-05-06","title":"TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment","abstract":"Learning discriminative 3D representations that generalize well to unknown testing categories is an emerging requirement for many real-world 3D applications. Existing well-established methods often struggle to attain this goal due to insufficient 3D training data from broader concepts. Meanwhile, pre-trained large vision-language models (e.g., CLIP) have shown remarkable zero-shot generalization capabilities. Yet, they are limited in extracting suitable 3D representations due to substantial gaps between their 2D training and 3D testing distributions. To address these challenges, we propose Testing-time Distribution Alignment (TeDA), a novel framework that adapts a pretrained 2D vision-language model CLIP for unknown 3D object retrieval at test time. To our knowledge, it is the first work that studies the test-time adaptation of a vision-language model for 3D feature learning. TeDA projects 3D objects into multi-view images, extracts features using CLIP, and refines 3D query embeddings with an iterative optimization strategy by confident query-target sample pairs in a self-boosting manner. Additionally, TeDA integrates textual descriptions generated by a multimodal language model (InternVL) to enhance 3D object understanding, leveraging CLIP's aligned feature space to fuse visual and textual cues. Extensive experiments on four open-set 3D object retrieval benchmarks demonstrate that TeDA greatly outperforms state-of-the-art methods, even those requiring extensive training. We also experimented with depth maps on Objaverse-LVIS, further validating its effectiveness. Code is available at https://github.com/wangzhichuan123/TeDA.","authors":["Zhichuan Wang","Yang Zhou","Jinhai Xiang","Yulong Wang","Xinwei He"],"url":"https://arxiv.org/abs/2505.02325"}
{"created":"2025-05-06","title":"Regulating Algorithmic Management: A Multi-Stakeholder Study of Challenges in Aligning Software and the Law for Workplace Scheduling","abstract":"The impacts of algorithmic management (AM) on worker well-being have led to increasing calls to regulate AM practices to prevent further worker harms. Yet existing work in aligning software with the law reduces compliance to just one piece of the entire process of regulating AM -- which involves rule operationalization, software use, and enforcement. We interviewed key stakeholders involved in enforcing or complying with workplace scheduling law -- regulators, advocates, defense attorneys, scheduling managers, and workers ($N = 38$). Based on their beliefs and experiences, we describe how scheduling software affects beliefs about and compliance with workplace scheduling law. In so doing, we discuss the challenges and opportunities in designing software as a tool for regulating AM.","authors":["Jonathan Lynn","Rachel Y. Kim","Sicun Gao","Daniel Schneider","Sachin S. Pandya","Min Kyung Lee"],"url":"https://arxiv.org/abs/2505.02329"}
{"created":"2025-05-06","title":"On Trigonometric Interpolation and Its Applications","abstract":"In this paper, we propose a new trigonometric interpolation algorithm and establish relevant convergent properties. The method adjusts an existing trigonometric interpolation algorithm such that it can better leverage Fast Fourier Transform (FFT) to enhance efficiency. The algorithm can be formulated in a way such that certain cancellation effects can be effectively leveraged for error analysis, which enables us not only to obtain the desired uniform convergent rate of the approximation to a function, but desired uniform convergent rates for its derivatives as well.","authors":["Xiaorong Zou"],"url":"https://arxiv.org/abs/2505.02330"}
{"created":"2025-05-06","title":"VAEmo: Efficient Representation Learning for Visual-Audio Emotion with Knowledge Injection","abstract":"Audiovisual emotion recognition (AVER) aims to infer human emotions from nonverbal visual-audio (VA) cues, offering modality-complementary and language-agnostic advantages. However, AVER remains challenging due to the inherent ambiguity of emotional expressions, cross-modal expressive disparities, and the scarcity of reliably annotated data. Recent self-supervised AVER approaches have introduced strong multimodal representations, yet they predominantly rely on modality-specific encoders and coarse content-level alignment, limiting fine-grained emotional semantic modeling. To address these issues, we propose VAEmo, an efficient two-stage framework for emotion-centric joint VA representation learning with external knowledge injection. In Stage 1, a unified and lightweight representation network is pre-trained on large-scale speaker-centric VA corpora via masked reconstruction and contrastive objectives, mitigating the modality gap and learning expressive, complementary representations without emotion labels. In Stage 2, multimodal large language models automatically generate detailed affective descriptions according to our well-designed chain-of-thought prompting for only a small subset of VA samples; these rich textual semantics are then injected by aligning their corresponding embeddings with VA representations through dual-path contrastive learning, further bridging the emotion gap. Extensive experiments on multiple downstream AVER benchmarks show that VAEmo achieves state-of-the-art performance with a compact design, highlighting the benefit of unified cross-modal encoding and emotion-aware semantic guidance for efficient, generalizable VA emotion representations.","authors":["Hao Cheng","Zhiwei Zhao","Yichao He","Zhenzhen Hu","Jia Li","Meng Wang","Richang Hong"],"url":"https://arxiv.org/abs/2505.02331"}
{"created":"2025-05-06","title":"6D Pose Estimation on Spoons and Hands","abstract":"Accurate dietary monitoring is essential for promoting healthier eating habits. A key area of research is how people interact and consume food using utensils and hands. By tracking their position and orientation, it is possible to estimate the volume of food being consumed, or monitor eating behaviours, highly useful insights into nutritional intake that can be more reliable than popular methods such as self-reporting. Hence, this paper implements a system that analyzes stationary video feed of people eating, using 6D pose estimation to track hand and spoon movements to capture spatial position and orientation. In doing so, we examine the performance of two state-of-the-art (SOTA) video object segmentation (VOS) models, both quantitatively and qualitatively, and identify main sources of error within the system.","authors":["Kevin Tan","Fan Yang","Yuhao Chen"],"url":"https://arxiv.org/abs/2505.02335"}
{"created":"2025-05-06","title":"Social Correction on Social Media: A Quantitative Analysis of Comment Behaviour and Reliability","abstract":"Corrections given by ordinary social media users, also referred to as Social Correction have emerged as a viable intervention against misinformation as per the recent literature. However, little is known about how often users give disputing or endorsing comments and how reliable those comments are. An online experiment was conducted to investigate how users' credibility evaluations of social media posts and their confidence in those evaluations combined with online reputational concerns affect their commenting behaviour. The study found that participants exhibited a more conservative approach when giving disputing comments compared to endorsing ones. Nevertheless, participants were more discerning in their disputing comments than endorsing ones. These findings contribute to a better understanding of social correction on social media and highlight the factors influencing comment behaviour and reliability.","authors":["Sameera S. Vithanage","Keith Ransom","Antonette Mendoza","Shanika Karunasekera"],"url":"https://arxiv.org/abs/2505.02343"}
{"created":"2025-05-06","title":"An End-to-End Model For Logits Based Large Language Models Watermarking","abstract":"The rise of LLMs has increased concerns over source tracing and copyright protection for AIGC, highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce a novel end-to-end logits perturbation method for watermarking LLM-generated text. By jointly optimization, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method achieves superior robustness, outperforming distortion-free methods by 37-39% under paraphrasing and 17.2% on average, while maintaining text quality on par with these distortion-free methods in terms of text perplexity and downstream tasks. Our method can be easily generalized to different LLMs.","authors":["Kahim Wong","Jicheng Zhou","Jiantao Zhou","Yain-Whar Si"],"url":"https://arxiv.org/abs/2505.02344"}
{"created":"2025-05-06","title":"Optimal error estimates of a second-order temporally finite element method for electrohydrodynamic equations","abstract":"In this work, we mainly present the optimal convergence rates of the temporally second-order finite element scheme for solving the electrohydrodynamic equation. Suffering from the highly coupled nonlinearity, the convergence analysis of the numerical schemes for such a system is rather rare, not to mention the optimal error estimates for the high-order temporally scheme. To this end, we abandon the traditional error analysis method following the process of energy estimate, which may lead to the loss of accuracy. Instead, we note that the charge density also possesses the \"energy\" decaying property directly derived by its governing equation, although it does not appear in the energy stability analysis. This fact allows us to control the error terms of the charge density more conveniently, which finally leads to the optimal convergence rates. Several numerical examples are provided to demonstrate the theoretical results, including the energy stability, mass conservation, and convergence rates.","authors":["Shengfeng Wang","Zeyu Xia","Maojun Li"],"url":"https://arxiv.org/abs/2505.02345"}
{"created":"2025-05-06","title":"An Empirical Study on the Performance and Energy Usage of Compiled Python Code","abstract":"Python is a popular programming language known for its ease of learning and extensive libraries. However, concerns about performance and energy consumption have led to the development of compilers to enhance Python code efficiency. Despite the proven benefits of existing compilers on the efficiency of Python code, there is limited analysis comparing their performance and energy efficiency, particularly considering code characteristics and factors like CPU frequency and core count. Our study investigates how compilation impacts the performance and energy consumption of Python code, using seven benchmarks compiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython, Pyston-lite, and the experimental Python 3.13 version, compared to CPython. The benchmarks are single-threaded and executed on an NUC and a server, measuring energy usage, execution time, memory usage, and Last-Level Cache (LLC) miss rates at a fixed frequency and on a single core. The results show that compilation can significantly enhance execution time, energy and memory usage, with Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements. Nuitka optimizes memory usage consistently on both testbeds. The impact of compilation on LLC miss rate is not clear since it varies considerably across benchmarks for each compiler. Our study is important for researchers and practitioners focused on improving Python code performance and energy efficiency. We outline future research directions, such as exploring caching effects on energy usage. Our findings help practitioners choose the best compiler based on their efficiency benefits and accessibility.","authors":["Vincenzo Stoico","Andrei Calin Dragomir","Patricia Lago"],"url":"https://arxiv.org/abs/2505.02346"}
{"created":"2025-05-06","title":"A Slicing-Based Approach for Detecting and Patching Vulnerable Code Clones","abstract":"Code cloning is a common practice in software development, but it poses significant security risks by propagating vulnerabilities across cloned segments. To address this challenge, we introduce srcVul, a scalable, precise detection approach that combines program slicing with Locality-Sensitive Hashing to identify vulnerable code clones and recommend patches. srcVul builds a database of vulnerability-related slices by analyzing known vulnerable programs and their corresponding patches, indexing each slice's unique structural characteristics as a vulnerability slicing vector. During clone detection, srcVul efficiently matches slicing vectors from target programs with those in the database, recommending patches upon identifying similarities. Our evaluation of srcVul against three state-of-the-art vulnerable clone detectors demonstrates its accuracy, efficiency, and scalability, achieving 91% precision and 75% recall on established vulnerability databases and open-source repositories. These results highlight srcVul's effectiveness in detecting complex vulnerability patterns across diverse codebases.","authors":["Hakam Alomari","Christopher Vendome","Hilal Gyawali"],"url":"https://arxiv.org/abs/2505.02349"}
{"created":"2025-05-06","title":"Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation","abstract":"Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using sparse ellipsoidal radial basis function networks, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represent the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding code is publicly available at https://github.com/lianbobo/SE-RBFNet.git.","authors":["Bobo Lian","Dandan Wang","Chenjian Wu","Minxin Chen"],"url":"https://arxiv.org/abs/2505.02350"}
{"created":"2025-05-06","title":"Opt-GPTQ: An Optimized GPTQ Combining Sparse Attention and Quantization Techniques","abstract":"In the field of deep learning, traditional attention mechanisms face significant challenges related to high computational complexity and large memory consumption when processing long sequence data. To address these limitations, we propose Opt-GPTQ, an optimized Gradient-based Post Training Quantization (GPTQ) combining the Grouped Query Attention (GQA) mechanism with paging memory management, optimizing the traditional Multi-Head Attention (MHA) mechanism by grouping query heads and sharing key-value vectors. Optimized GQA (Opt-GQA) effectively reduces computational complexity, minimizes memory fragmentation, and enhances memory utilization for large-scale models. Opt-GPTQ is optimized for Data Center Units (DCUs) and integrated into the vLLM model to maximize hardware efficiency. It customizes GPU kernels to further enhance attention computation by reducing memory access latency and boosting parallel computing capabilities. Opt-GQA integrates Attention with Linear Biases (ALiBi) to reduce overhead and enhance long-sequence processing. Experimental results show that Opt?GPTQ significantly reduces computation time and memory usage while improving model performance.","authors":["Jie Kong","Junxiang Zhang","Jiheng Xu","Yalong Li","Shouhua Zhang","Jiehan Zhou","Yuhai Liu","Peng Liang","Quan Zhang","Luohan Jiang"],"url":"https://arxiv.org/abs/2505.02351"}
{"created":"2025-05-06","title":"Social Biases in Knowledge Representations of Wikidata separates Global North from Global South","abstract":"Knowledge Graphs have become increasingly popular due to their wide usage in various downstream applications, including information retrieval, chatbot development, language model construction, and many others. Link prediction (LP) is a crucial downstream task for knowledge graphs, as it helps to address the problem of the incompleteness of the knowledge graphs. However, previous research has shown that knowledge graphs, often created in a (semi) automatic manner, are not free from social biases. These biases can have harmful effects on downstream applications, especially by leading to unfair behavior toward minority groups. To understand this issue in detail, we develop a framework -- AuditLP -- deploying fairness metrics to identify biased outcomes in LP, specifically how occupations are classified as either male or female-dominated based on gender as a sensitive attribute. We have experimented with the sensitive attribute of age and observed that occupations are categorized as young-biased, old-biased, and age-neutral. We conduct our experiments on a large number of knowledge triples that belong to 21 different geographies extracted from the open-sourced knowledge graph, Wikidata. Our study shows that the variance in the biased outcomes across geographies neatly mirrors the socio-economic and cultural division of the world, resulting in a transparent partition of the Global North from the Global South.","authors":["Paramita Das","Sai Keerthana Karnam","Aditya Soni","Animesh Mukherjee"],"url":"https://arxiv.org/abs/2505.02352"}
{"created":"2025-05-06","title":"Model Checking and Synthesis for Optimal Use of Knowledge in Consensus Protocols","abstract":"Logics of knowledge and knowledge-based programs provide a way to give abstract descriptions of solutions to problems in fault-tolerant distributed computing, and have been used to derive optimal protocols for these problems with respect to a variety of failure models. Generally, these results have involved complex pencil and paper analyses with respect to the theoretical \"full-information protocol\" model of information exchange between network nodes. It is equally of interest to be able to establish the optimality of protocols using weaker, but more practical, models of information exchange, or else identify opportunities to improve their performance. Over the last 20 years, automated verification and synthesis tools for the logic of knowledge have been developed, such as the model checker MCK, that can be applied to this problem. This paper concerns the application of MCK to automated analyses of this kind. A number of information-exchange models are considered, for Simultaneous and Eventual variants of Byzantine Agreement under a range of failure types. MCK is used to automatically analyze these models. The results demonstrate that it is possible to automatically identify optimization opportunities, and to automatically synthesize optimal protocols. The paper provides performance measurements for the automated analysis, establishing a benchmark for epistemic model checking and synthesis tools.","authors":["Kaya Alpturer","Gerald Huang","Ron van der Meyden"],"url":"https://arxiv.org/abs/2505.02353"}
{"created":"2025-05-06","title":"RouthSearch: Inferring PID Parameter Specification for Flight Control Program by Coordinate Search","abstract":"Flight control programs use PID control modules with user-configurable Proportional (P), Integral (I), and Derivative (D) parameters to manage UAV flying behaviors. Users can adjust these PID parameters during flight. However, flight control programs lack sufficient safety checks on user-provided PID parameters, leading to a severe UAV vulnerability - the input validation bug. This occurs when a user misconfigures PID parameters, causing dangerous states like deviation from the expected path, loss of control, or crash.","authors":["Siao Wang","Zhen Dong","Hui Li","Liwei Shen","Xin Peng","Dongdong She"],"url":"https://arxiv.org/abs/2505.02357"}
{"created":"2025-05-06","title":"Catastrophic Overfitting, Entropy Gap and Participation Ratio: A Noiseless $l^p$ Norm Solution for Fast Adversarial Training","abstract":"Adversarial training is a cornerstone of robust deep learning, but fast methods like the Fast Gradient Sign Method (FGSM) often suffer from Catastrophic Overfitting (CO), where models become robust to single-step attacks but fail against multi-step variants. While existing solutions rely on noise injection, regularization, or gradient clipping, we propose a novel solution that purely controls the $l^p$ training norm to mitigate CO.","authors":["Fares B. Mehouachi","Saif Eddin Jabari"],"url":"https://arxiv.org/abs/2505.02360"}
{"created":"2025-05-06","title":"Advancing Email Spam Detection: Leveraging Zero-Shot Learning and Large Language Models","abstract":"Email spam detection is a critical task in modern communication systems, essential for maintaining productivity, security, and user experience. Traditional machine learning and deep learning approaches, while effective in static settings, face significant limitations in adapting to evolving spam tactics, addressing class imbalance, and managing data scarcity. These challenges necessitate innovative approaches that reduce dependency on extensive labeled datasets and frequent retraining. This study investigates the effectiveness of Zero-Shot Learning using FLAN-T5, combined with advanced Natural Language Processing (NLP) techniques such as BERT for email spam detection. By employing BERT to preprocess and extract critical information from email content, and FLAN-T5 to classify emails in a Zero-Shot framework, the proposed approach aims to address the limitations of traditional spam detection systems. The integration of FLAN-T5 and BERT enables robust spam detection without relying on extensive labeled datasets or frequent retraining, making it highly adaptable to unseen spam patterns and adversarial environments. This research highlights the potential of leveraging zero-shot learning and NLPs for scalable and efficient spam detection, providing insights into their capability to address the dynamic and challenging nature of spam detection tasks.","authors":["Ghazaleh SHirvani","Saeid Ghasemshirazi"],"url":"https://arxiv.org/abs/2505.02362"}
{"created":"2025-05-06","title":"SIMPLEMIX: Frustratingly Simple Mixing of Off- and On-policy Data in Language Model Preference Learning","abstract":"Aligning language models with human preferences relies on pairwise preference datasets. While some studies suggest that on-policy data consistently outperforms off -policy data for preference learning, others indicate that the advantages of on-policy data may be task-dependent, highlighting the need for a systematic exploration of their interplay.","authors":["Tianjian Li","Daniel Khashabi"],"url":"https://arxiv.org/abs/2505.02363"}
{"created":"2025-05-06","title":"Quaternion Infrared Visible Image Fusion","abstract":"Visible images provide rich details and color information only under well-lighted conditions while infrared images effectively highlight thermal targets under challenging conditions such as low visibility and adverse weather. Infrared-visible image fusion aims to integrate complementary information from infrared and visible images to generate a high-quality fused image. Existing methods exhibit critical limitations such as neglecting color structure information in visible images and performance degradation when processing low-quality color-visible inputs. To address these issues, we propose a quaternion infrared-visible image fusion (QIVIF) framework to generate high-quality fused images completely in the quaternion domain. QIVIF proposes a quaternion low-visibility feature learning model to adaptively extract salient thermal targets and fine-grained texture details from input infrared and visible images respectively under diverse degraded conditions. QIVIF then develops a quaternion adaptive unsharp masking method to adaptively improve high-frequency feature enhancement with balanced illumination. QIVIF further proposes a quaternion hierarchical Bayesian fusion model to integrate infrared saliency and enhanced visible details to obtain high-quality fused images. Extensive experiments across diverse datasets demonstrate that our QIVIF surpasses state-of-the-art methods under challenging low-visibility conditions.","authors":["Weihua Yang","Yicong Zhou"],"url":"https://arxiv.org/abs/2505.02364"}
{"created":"2025-05-06","title":"Quaternion Multi-focus Color Image Fusion","abstract":"Multi-focus color image fusion refers to integrating multiple partially focused color images to create a single all-in-focus color image. However, existing methods struggle with complex real-world scenarios due to limitations in handling color information and intricate textures. To address these challenges, this paper proposes a quaternion multi-focus color image fusion framework to perform high-quality color image fusion completely in the quaternion domain. This framework introduces 1) a quaternion sparse decomposition model to jointly learn fine-scale image details and structure information of color images in an iterative fashion for high-precision focus detection, 2) a quaternion base-detail fusion strategy to individually fuse base-scale and detail-scale results across multiple color images for preserving structure and detail information, and 3) a quaternion structural similarity refinement strategy to adaptively select optimal patches from initial fusion results and obtain the final fused result for preserving fine details and ensuring spatially consistent outputs. Extensive experiments demonstrate that the proposed framework outperforms state-of-the-art methods.","authors":["Weihua Yang","Yicong Zhou"],"url":"https://arxiv.org/abs/2505.02365"}
{"created":"2025-05-06","title":"JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings","abstract":"Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \\textbf{J}oint \\textbf{T}ensor representation modulus constraint and \\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence \\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.","authors":["Tianyu Zong","Hongzhu Yi","Bingkang Shi","Yuanxiang Wang","Jungang Xu"],"url":"https://arxiv.org/abs/2505.02366"}
{"created":"2025-05-06","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks","abstract":"Generalizing well in deep neural networks remains a core challenge, particularly due to their tendency to converge to sharp minima that degrade robustness. Sharpness-Aware Minimization (SAM) mitigates this by seeking flatter minima but perturbs parameters using the full gradient, which can include statistically insignificant directions. We propose ZSharp, a simple yet effective extension to SAM that applies layer-wise Z-score normalization followed by percentile-based filtering to retain only statistically significant gradient components. This selective perturbation aligns updates with curvature-sensitive directions, enhancing generalization without requiring architectural changes. ZSharp introduces only one additional hyperparameter, the percentile threshold, and remains fully compatible with existing SAM variants. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet using ResNet, VGG, and Vision Transformers show that ZSharp consistently outperforms SAM and its variants in test accuracy, particularly on deeper and transformer-based models. These results demonstrate that ZSharp is a principled and lightweight improvement for sharpness-aware optimization.","authors":["Juyoung Yun"],"url":"https://arxiv.org/abs/2505.02369"}
{"created":"2025-05-06","title":"SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing","abstract":"Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size.","authors":["Ming Li","Xin Gu","Fan Chen","Xiaoying Xing","Longyin Wen","Chen Chen","Sijie Zhu"],"url":"https://arxiv.org/abs/2505.02370"}
{"created":"2025-05-06","title":"Guarding Terrains with Guards on a Line","abstract":"Given an $x$-monotone polygonal chain $T$ with $n$ vertices, and an integer $k$, we consider the problem of finding the lowest horizontal line $L$ lying above $T$ with $k$ point guards lying on $L$, so that every point on the chain is \\emph{visible} from some guard. A natural optimization is to minimize the $y$-coordinate of $L$. We present an algorithm for finding the optimal placements of $L$ and $k$ point guards for $T$ in $O(k^2\\lambda_{k-1}(n)\\log n)$ time for even numbers $k\\ge 2$, and in $O(k^2\\lambda_{k-2}(n)\\log n)$ time for odd numbers $k \\ge 3$, where $\\lambda_{s}(n)$ is the length of the longest $(n,s)$-Davenport-Schinzel sequence. We also study a variant with an additional requirement that $T$ is partitioned into $k$ subchains, each subchain is paired with exactly one guard, and every point on a subchain is visible from its paired guard. When $L$ is fixed, we can place the minimum number of guards in $O(n)$ time. When the number $k$ of guards is fixed, we can find an optimal placement of $L$ with $k$ point guards lying on $L$ in $O(kn)$ time.","authors":["Byeonguk Kang","Hwi Kim","Hee-Kap Ahn"],"url":"https://arxiv.org/abs/2505.02373"}
{"created":"2025-05-06","title":"LAMeD: LLM-generated Annotations for Memory Leak Detection","abstract":"Static analysis tools are widely used to detect software bugs and vulnerabilities but often struggle with scalability and efficiency in complex codebases. Traditional approaches rely on manually crafted annotations -- labeling functions as sources or sinks -- to track data flows, e.g., ensuring that allocated memory is eventually freed, and code analysis tools such as CodeQL, Infer, or Cooddy can use function specifications, but manual annotation is laborious and error-prone, especially for large or third-party libraries. We present LAMeD (LLM-generated Annotations for Memory leak Detection), a novel approach that leverages large language models (LLMs) to automatically generate function-specific annotations. When integrated with analyzers such as Cooddy, LAMeD significantly improves memory leak detection and reduces path explosion. We also suggest directions for extending LAMeD to broader code analysis.","authors":["Ekaterina Shemetova","Ilya Shenbin","Ivan Smirnov","Anton Alekseev","Alexey Rukhovich","Sergey Nikolenko","Vadim Lomshakov","Irina Piontkovskaya"],"url":"https://arxiv.org/abs/2505.02376"}
{"created":"2025-05-06","title":"EntroLLM: Entropy Encoded Weight Compression for Efficient Large Language Model Inference on Edge Devices","abstract":"Large Language Models (LLMs) demonstrate exceptional performance across various tasks, but their large storage and computational requirements constrain their deployment on edge devices. To address this, we propose EntroLLM, a novel compression framework that integrates mixed quantization with entropy coding to reduce storage overhead while maintaining model accuracy. Our method applies a layer-wise mixed quantization scheme - choosing between symmetric and asymmetric quantization based on individual layer weight distributions - to optimize compressibility. We then employ Huffman encoding for lossless compression of the quantized weights, significantly reducing memory bandwidth requirements. Furthermore, we introduce parallel Huffman decoding, which enables efficient retrieval of encoded weights during inference, ensuring minimal latency impact. Our experiments on edge-compatible LLMs, including smolLM-1.7B-Instruct, phi3-mini-4k-Instruct, and mistral-7B-Instruct, demonstrate that EntroLLM achieves up to $30%$ storage reduction compared to uint8 models and up to $65%$ storage reduction compared to uint4 models, while preserving perplexity and accuracy, on language benchmark tasks. We further show that our method enables $31.9%$ - $146.6%$ faster inference throughput on memory-bandwidth-limited edge devices, such as NVIDIA Jetson P3450, by reducing the required data movement. The proposed approach requires no additional re-training and is fully compatible with existing post-training quantization methods, making it a practical solution for edge LLMs.","authors":["Arnab Sanyal","Prithwish Mukherjee","Gourav Datta","Sandeep P. Chinchali"],"url":"https://arxiv.org/abs/2505.02380"}
{"created":"2025-05-06","title":"Probabilistic ODMA Receiver with Low-Complexity Algorithm for MIMO Unsourced Random Access","abstract":"In this work, we present the design for both pilot-uncoupled and pilot-free on-off multiple access (ODMA) receivers in unsourced random access (URA) for multiple-input multiple-output (MIMO) systems. Unlike pilot-coupled ODMA, where on-off patterns are linked to pilot selection, pilot-uncoupled and pilot-free ODMA reduce transmission redundancy but face challenges in processing complexity and capacity performance. The joint pattern and data detector (JPDD) design is critical for these schemes, but the current JPDD algorithm has high complexity with quadratic computational costs. To address this, we propose a low-complexity detector based on approximate message passing (AMP), which offers linear complexity, providing reduced cost and improved performance in the under-determined linear regression case. Decoding is initialized via pilot-free matrix factorization through alternating minimization, resolving phase and scalar ambiguities. Compared to existing pilot-free schemes, the proposed method achieves a 13 dB improvement and favorable trade-offs in complexity and capacity performance when compared to benchmarks.","authors":["Zhentian Zhang","Jian Dang","Zaichen Zhang"],"url":"https://arxiv.org/abs/2505.02382"}
{"created":"2025-05-06","title":"Connecting Thompson Sampling and UCB: Towards More Efficient Trade-offs Between Privacy and Regret","abstract":"We address differentially private stochastic bandit problems from the angles of exploring the deep connections among Thompson Sampling with Gaussian priors, Gaussian mechanisms, and Gaussian differential privacy (GDP). We propose DP-TS-UCB, a novel parametrized private bandit algorithm that enables to trade off privacy and regret. DP-TS-UCB satisfies $ \\tilde{O} \\left(T^{0.25(1-\\alpha)}\\right)$-GDP and enjoys an $O \\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $\\alpha \\in [0,1]$ controls the trade-off between privacy and regret. Theoretically, our DP-TS-UCB relies on anti-concentration bounds of Gaussian distributions and links exploration mechanisms in Thompson Sampling-based algorithms and Upper Confidence Bound-based algorithms, which may be of independent interest.","authors":["Bingshan Hu","Zhiming Huang","Tianyue H. Zhang","Mathias L\\'ecuyer","Nidhi Hegde"],"url":"https://arxiv.org/abs/2505.02383"}
{"created":"2025-05-06","title":"On the Equivalence of Gaussian Graphical Models Defined on Complete Bipartite Graphs","abstract":"This paper introduces two Gaussian graphical models defined on complete bipartite graphs. We show that the determinants of the precision matrices associated with the models are equal up to scale, where the scale factor only depends on model parameters. In this context, we will introduce a notion of ``equivalence\" between the two Gaussian graphical models. This equivalence has two key applications: first, it can significantly reduce the complexity of computing the exact value of the determinant, and second, it enables the derivation of closed-form expressions for the determinants in certain special cases.","authors":["Mehdi Molkaraie"],"url":"https://arxiv.org/abs/2505.02384"}
{"created":"2025-05-06","title":"RM-R1: Reward Modeling as Reasoning","abstract":"Reward modeling is essential for aligning large language models (LLMs) with human preferences, especially through reinforcement learning from human feedback (RLHF). To provide accurate reward signals, a reward model (RM) should stimulate deep thinking and conduct interpretable reasoning before assigning a score or a judgment. However, existing RMs either produce opaque scalar scores or directly generate the prediction of a preferred answer, making them struggle to integrate natural language critiques, thus lacking interpretability. Inspired by recent advances of long chain-of-thought (CoT) on reasoning-intensive tasks, we hypothesize and validate that integrating reasoning capabilities into reward modeling significantly enhances RM's interpretability and performance. In this work, we introduce a new class of generative reward models -- Reasoning Reward Models (ReasRMs) -- which formulate reward modeling as a reasoning task. We propose a reasoning-oriented training pipeline and train a family of ReasRMs, RM-R1. The training consists of two key stages: (1) distillation of high-quality reasoning chains and (2) reinforcement learning with verifiable rewards. RM-R1 improves LLM rollouts by self-generating reasoning traces or chat-specific rubrics and evaluating candidate responses against them. Empirically, our models achieve state-of-the-art or near state-of-the-art performance of generative RMs across multiple comprehensive reward model benchmarks, outperforming much larger open-weight models (e.g., Llama3.1-405B) and proprietary ones (e.g., GPT-4o) by up to 13.8%. Beyond final performance, we perform thorough empirical analysis to understand the key ingredients of successful ReasRM training. To facilitate future research, we release six ReasRM models along with code and data at https://github.com/RM-R1-UIUC/RM-R1.","authors":["Xiusi Chen","Gaotang Li","Ziqi Wang","Bowen Jin","Cheng Qian","Yu Wang","Hongru Wang","Yu Zhang","Denghui Zhang","Tong Zhang","Hanghang Tong","Heng Ji"],"url":"https://arxiv.org/abs/2505.02387"}
{"created":"2025-05-06","title":"MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans","abstract":"Embodied AI (EAI) research requires high-quality, diverse 3D scenes to effectively support skill acquisition, sim-to-real transfer, and generalization. Achieving these quality standards, however, necessitates the precise replication of real-world object diversity. Existing datasets demonstrate that this process heavily relies on artist-driven designs, which demand substantial human effort and present significant scalability challenges. To scalably produce realistic and interactive 3D scenes, we first present MetaScenes, a large-scale, simulatable 3D scene dataset constructed from real-world scans, which includes 15366 objects spanning 831 fine-grained categories. Then, we introduce Scan2Sim, a robust multi-modal alignment model, which enables the automated, high-quality replacement of assets, thereby eliminating the reliance on artist-driven designs for scaling 3D scenes. We further propose two benchmarks to evaluate MetaScenes: a detailed scene synthesis task focused on small item layouts for robotic manipulation and a domain transfer task in vision-and-language navigation (VLN) to validate cross-domain transfer. Results confirm MetaScene's potential to enhance EAI by supporting more generalizable agent learning and sim-to-real applications, introducing new possibilities for EAI research. Project website: https://meta-scenes.github.io/.","authors":["Huangyue Yu","Baoxiong Jia","Yixin Chen","Yandan Yang","Puhao Li","Rongpeng Su","Jiaxin Li","Qing Li","Wei Liang","Song-Chun Zhu","Tengyu Liu","Siyuan Huang"],"url":"https://arxiv.org/abs/2505.02388"}
{"created":"2025-05-06","title":"Quantitative Analysis of Performance Drop in DeepSeek Model Quantization","abstract":"Recently, there is a high demand for deploying DeepSeek-R1 and V3 locally, possibly because the official service often suffers from being busy and some organizations have data privacy concerns. While single-machine deployment offers infrastructure simplicity, the models' 671B FP8 parameter configuration exceeds the practical memory limits of a standard 8-GPU machine. Quantization is a widely used technique that helps reduce model memory consumption. However, it is unclear what the performance of DeepSeek-R1 and V3 will be after being quantized. This technical report presents the first quantitative evaluation of multi-bitwidth quantization across the complete DeepSeek model spectrum. Key findings reveal that 4-bit quantization maintains little performance degradation versus FP8 while enabling single-machine deployment on standard NVIDIA GPU devices. We further propose DQ3_K_M, a dynamic 3-bit quantization method that significantly outperforms traditional Q3_K_M variant on various benchmarks, which is also comparable with 4-bit quantization (Q4_K_M) approach in most tasks. Moreover, DQ3_K_M supports single-machine deployment configurations for both NVIDIA H100/A100 and Huawei 910B. Our implementation of DQ3\\_K\\_M is released at https://github.com/UnicomAI/DeepSeek-Eval, containing optimized 3-bit quantized variants of both DeepSeek-R1 and DeepSeek-V3.","authors":["Enbo Zhao","Yi Shen","Shuming Shi","Jieyun Huang","Zhihao Chen","Ning Wang","Siqi Xiao","Jian Zhang","Kai Wang","Shiguo Lian"],"url":"https://arxiv.org/abs/2505.02390"}
{"created":"2025-05-06","title":"Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL","abstract":"Chain-of-thought (CoT) reasoning in large language models (LLMs) can be formalized as a latent variable problem, where the model needs to generate intermediate reasoning steps. While prior approaches such as iterative reward-ranked fine-tuning (RAFT) have relied on such formulations, they typically apply uniform inference budgets across prompts, which fails to account for variability in difficulty and convergence behavior. This work identifies the main bottleneck in CoT training as inefficient stochastic gradient estimation due to static sampling strategies. We propose GVM-RAFT, a prompt-specific Dynamic Sample Allocation Strategy designed to minimize stochastic gradient variance under a computational budget constraint. The method dynamically allocates computational resources by monitoring prompt acceptance rates and stochastic gradient norms, ensuring that the resulting gradient variance is minimized. Our theoretical analysis shows that the proposed dynamic sampling strategy leads to accelerated convergence guarantees under suitable conditions. Experiments on mathematical reasoning show that GVM-RAFT achieves a 2-4x speedup and considerable accuracy improvements over vanilla RAFT. The proposed dynamic sampling strategy is general and can be incorporated into other reinforcement learning algorithms, such as GRPO, leading to similar improvements in convergence and test accuracy. Our code is available at https://github.com/RLHFlow/GVM.","authors":["Jiarui Yao","Yifan Hao","Hanning Zhang","Hanze Dong","Wei Xiong","Nan Jiang","Tong Zhang"],"url":"https://arxiv.org/abs/2505.02391"}
{"created":"2025-05-06","title":"Moneros Decentralized P2P Exchanges: Functionality, Adoption, and Privacy Risks","abstract":"Privacy-focused cryptocurrencies like Monero remain popular, despite increasing regulatory scrutiny that has led to their delisting from major centralized exchanges. The latter also explains the recent popularity of decentralized exchanges (DEXs) with no centralized ownership structures. These platforms typically leverage peer-to-peer (P2P) networks, promising secure and anonymous asset trading. However, questions of liability remain, and the academic literature lacks comprehensive insights into the functionality, trading activity, and privacy claims of these P2P platforms. In this paper, we provide an early systematization of the current landscape of decentralized peer-to-peer exchanges within the Monero ecosystem. We examine several recently developed DEX platforms, analyzing their popularity, functionality, architectural choices, and potential weaknesses. We further identify and report on a privacy vulnerability in the recently popularized Haveno exchange, demonstrating that certain Haveno trades could be detected, allowing transactions to be linked across the Monero and Bitcoin blockchains. We hope that our findings can nourish the discussion in the research community about more secure designs, and provide insights for regulators.","authors":["Yannik Kopyciok","Friedhelm Victor","Stefan Schmid"],"url":"https://arxiv.org/abs/2505.02392"}
{"created":"2025-05-06","title":"Uncertainty-Weighted Image-Event Multimodal Fusion for Video Anomaly Detection","abstract":"Most existing video anomaly detectors rely solely on RGB frames, which lack the temporal resolution needed to capture abrupt or transient motion cues, key indicators of anomalous events. To address this limitation, we propose Image-Event Fusion for Video Anomaly Detection (IEF-VAD), a framework that synthesizes event representations directly from RGB videos and fuses them with image features through a principled, uncertainty-aware process. The system (i) models heavy-tailed sensor noise with a Student`s-t likelihood, deriving value-level inverse-variance weights via a Laplace approximation; (ii) applies Kalman-style frame-wise updates to balance modalities over time; and (iii) iteratively refines the fused latent state to erase residual cross-modal noise. Without any dedicated event sensor or frame-level labels, IEF-VAD sets a new state of the art across multiple real-world anomaly detection benchmarks. These findings highlight the utility of synthetic event representations in emphasizing motion cues that are often underrepresented in RGB frames, enabling accurate and robust video understanding across diverse applications without requiring dedicated event sensors. Code and models are available at https://github.com/EavnJeong/IEF-VAD.","authors":["Sungheon Jeong","Jihong Park","Mohsen Imani"],"url":"https://arxiv.org/abs/2505.02393"}
{"created":"2025-05-06","title":"A Real-Time Control Barrier Function-Based Safety Filter for Motion Planning with Arbitrary Road Boundary Constraints","abstract":"We present a real-time safety filter for motion planning, such as learning-based methods, using Control Barrier Functions (CBFs), which provides formal guarantees for collision avoidance with road boundaries. A key feature of our approach is its ability to directly incorporate road geometries of arbitrary shape without resorting to conservative overapproximations. We formulate the safety filter as a constrained optimization problem in the form of a Quadratic Program (QP). It achieves safety by making minimal, necessary adjustments to the control actions issued by the nominal motion planner. We validate our safety filter through extensive numerical experiments across a variety of traffic scenarios featuring complex roads. The results confirm its reliable safety and high computational efficiency (execution frequency up to 40 Hz). Code & Video Demo: github.com/bassamlab/SigmaRL","authors":["Jianye Xu","Chang Che","Bassam Alrifaee"],"url":"https://arxiv.org/abs/2505.02395"}
{"created":"2025-05-06","title":"A probabilistic view on Riemannian machine learning models for SPD matrices","abstract":"The goal of this paper is to show how different machine learning tools on the Riemannian manifold $\\mathcal{P}_d$ of Symmetric Positive Definite (SPD) matrices can be united under a probabilistic framework. For this, we will need several Gaussian distributions defined on $\\mathcal{P}_d$. We will show how popular classifiers on $\\mathcal{P}_d$ can be reinterpreted as Bayes Classifiers using these Gaussian distributions. These distributions will also be used for outlier detection and dimension reduction. By showing that those distributions are pervasive in the tools used on $\\mathcal{P}_d$, we allow for other machine learning tools to be extended to $\\mathcal{P}_d$.","authors":["Thibault de Surrel","Florian Yger","Fabien Lotte","Sylvain Chevallier"],"url":"https://arxiv.org/abs/2505.02402"}
{"created":"2025-05-06","title":"Estimating Commonsense Scene Composition on Belief Scene Graphs","abstract":"This work establishes the concept of commonsense scene composition, with a focus on extending Belief Scene Graphs by estimating the spatial distribution of unseen objects. Specifically, the commonsense scene composition capability refers to the understanding of the spatial relationships among related objects in the scene, which in this article is modeled as a joint probability distribution for all possible locations of the semantic object class. The proposed framework includes two variants of a Correlation Information (CECI) model for learning probability distributions: (i) a baseline approach based on a Graph Convolutional Network, and (ii) a neuro-symbolic extension that integrates a spatial ontology based on Large Language Models (LLMs). Furthermore, this article provides a detailed description of the dataset generation process for such tasks. Finally, the framework has been validated through multiple runs on simulated data, as well as in a real-world indoor environment, demonstrating its ability to spatially interpret scenes across different room types.","authors":["Mario A. V. Saucedo","Vignesh Kottayam Viswanathan","Christoforos Kanellakis","George Nikolakopoulos"],"url":"https://arxiv.org/abs/2505.02405"}
{"created":"2025-05-06","title":"Token Coordinated Prompt Attention is Needed for Visual Prompting","abstract":"Visual prompting techniques are widely used to efficiently fine-tune pretrained Vision Transformers (ViT) by learning a small set of shared prompts for all tokens. However, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of ViT. This often leads to indistinguishable and biased prompt-extracted features, hindering performance. To address this issue, we propose a plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. Firstly, recognizing the distinct functions of CLS and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into CLS Prompts and Image Prompts, which interact exclusively with CLS tokens and image tokens through attention mechanisms. This enhances their respective discriminative abilities. Furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. This enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. Extensive experiments across various benchmarks demonstrate that TCPA significantly enhances the diversity and discriminative power of the extracted features. The code is available at https://github.com/zhoujiahuan1991/ICML2025-TCPA.","authors":["Zichen Liu","Xu Zou","Gang Hua","Jiahuan Zhou"],"url":"https://arxiv.org/abs/2505.02406"}
{"created":"2025-05-06","title":"Encrypted Federated Search Using Homomorphic Encryption","abstract":"The sharing of information between agencies is effective in dealing with cross-jurisdictional criminal activities; however, such sharing is often restricted due to concerns about data privacy, ownership, and compliance. Towards this end, this work has introduced a privacy-preserving federated search system that allows law enforcement agencies to conduct queries on encrypted criminal databases by utilizing Homomorphic Encryption (HE). The key innovation here is the ability to execute encrypted queries across distributed databases, without the decryption of the data, thus preserving end-to-end confidentiality. In essence, this approach meets stringent privacy requirements in the interests of national security and regulatory compliance. The system incorporates the CKKS and BFV scheme embedded within TenSEAL, with each agency holding its key pair in a centralized key management table. In this federated search, encrypted queries are computed on the server side, and only authorized clients can decrypt the computed results. The matching of agencies is flexible for working in real-time while at the same time being secure and scalable while preserving control over data and the integrity of the process. Experimental results demonstrate the model. This paper also provide the implementation code and other details.","authors":["Om Rathod","Aastha Baid","Aswani Kumar Cherukuri"],"url":"https://arxiv.org/abs/2505.02409"}
{"created":"2025-05-06","title":"Bielik 11B v2 Technical Report","abstract":"We present Bielik 11B v2, a state-of-the-art language model optimized for Polish text processing. Built on the Mistral 7B v0.2 architecture and scaled to 11B parameters using depth up-scaling, this model demonstrates exceptional performance across Polish language benchmarks while maintaining strong cross-lingual capabilities. We introduce two key technical innovations: Weighted Instruction Cross-Entropy Loss, which optimizes learning across diverse instruction types by assigning quality-based weights to training examples, and Adaptive Learning Rate, which dynamically adjusts based on context length. Comprehensive evaluation across multiple benchmarks demonstrates that Bielik 11B v2 outperforms many larger models, including those with 2-6 times more parameters, and significantly surpasses other specialized Polish language models on tasks ranging from linguistic understanding to complex reasoning. The model's parameter efficiency and extensive quantization options enable deployment across various hardware configurations, advancing Polish language AI capabilities and establishing new benchmarks for resource-efficient language modeling in less-represented languages.","authors":["Krzysztof Ociepa","{\\L}ukasz Flis","Krzysztof Wr\\'obel","Adrian Gwo\\'zdziej","Remigiusz Kinas"],"url":"https://arxiv.org/abs/2505.02410"}
{"created":"2025-05-06","title":"Task-Oriented Semantic Communication in Large Multimodal Models-based Vehicle Networks","abstract":"Task-oriented semantic communication has emerged as a fundamental approach for enhancing performance in various communication scenarios. While recent advances in Generative Artificial Intelligence (GenAI), such as Large Language Models (LLMs), have been applied to semantic communication designs, the potential of Large Multimodal Models (LMMs) remains largely unexplored. In this paper, we investigate an LMM-based vehicle AI assistant using a Large Language and Vision Assistant (LLaVA) and propose a task-oriented semantic communication framework to facilitate efficient interaction between users and cloud servers. To reduce computational demands and shorten response time, we optimize LLaVA's image slicing to selectively focus on areas of utmost interest to users. Additionally, we assess the importance of image patches by combining objective and subjective user attention, adjusting energy usage for transmitting semantic information. This strategy optimizes resource utilization, ensuring precise transmission of critical information. We construct a Visual Question Answering (VQA) dataset for traffic scenarios to evaluate effectiveness. Experimental results show that our semantic communication framework significantly increases accuracy in answering questions under the same channel conditions, performing particularly well in environments with poor Signal-to-Noise Ratios (SNR). Accuracy can be improved by 13.4% at an SNR of 12dB and 33.1% at 10dB, respectively.","authors":["Baoxia Du","Hongyang Du","Dusit Niyato","Ruidong Li"],"url":"https://arxiv.org/abs/2505.02413"}
{"created":"2025-05-06","title":"Quadrupedal Spine Control Strategies: Exploring Correlations Between System Dynamic Responses and Human Perspectives","abstract":"Unlike their biological cousins, the majority of existing quadrupedal robots are constructed with rigid chassis. This results in motion that is either beetle-like or distinctly robotic, lacking the natural fluidity characteristic of mammalian movements. Existing literature on quadrupedal robots with spinal configurations primarily focuses on energy efficiency and does not consider the effects in human-robot interaction scenarios. Our contributions include an initial investigation into various trajectory generation strategies for a quadrupedal robot with a four degree of freedom spine, and an analysis on the effect that such methods have on human perception of gait naturalness compared to a fixed spine baseline. The strategies were evaluated using videos of walking, trotting and turning simulations. Among the four different strategies developed, the optimised time varying and the foot-tracking strategies were perceived to be more natural than the baseline in a randomised trial with 50 participants. Although none of the strategies demonstrated any energy efficiency improvements over the no-spine baseline, some showed greater footfall consistency at higher speeds. Given the greater likeability drawn from the more natural locomotion patterns, this type of robot displays potential for applications in social robot scenarios such as elderly care, where energy efficiency is not a primary concern.","authors":["Nicholas Hafner","Chaoran Liu","Carlos Ishi","Hiroshi Ishiguro"],"url":"https://arxiv.org/abs/2505.02414"}
{"created":"2025-05-06","title":"T2S: High-resolution Time Series Generation with Text-to-Series Diffusion Models","abstract":"Text-to-Time Series generation holds significant potential to address challenges such as data sparsity, imbalance, and limited availability of multimodal time series datasets across domains. While diffusion models have achieved remarkable success in Text-to-X (e.g., vision and audio data) generation, their use in time series generation remains in its nascent stages. Existing approaches face two critical limitations: (1) the lack of systematic exploration of general-proposed time series captions, which are often domain-specific and struggle with generalization; and (2) the inability to generate time series of arbitrary lengths, limiting their applicability to real-world scenarios. In this work, we first categorize time series captions into three levels: point-level, fragment-level, and instance-level. Additionally, we introduce a new fragment-level dataset containing over 600,000 high-resolution time series-text pairs. Second, we propose Text-to-Series (T2S), a diffusion-based framework that bridges the gap between natural language and time series in a domain-agnostic manner. T2S employs a length-adaptive variational autoencoder to encode time series of varying lengths into consistent latent embeddings. On top of that, T2S effectively aligns textual representations with latent embeddings by utilizing Flow Matching and employing Diffusion Transformer as the denoiser. We train T2S in an interleaved paradigm across multiple lengths, allowing it to generate sequences of any desired length. Extensive evaluations demonstrate that T2S achieves state-of-the-art performance across 13 datasets spanning 12 domains.","authors":["Yunfeng Ge","Jiawei Li","Yiji Zhao","Haomin Wen","Zhao Li","Meikang Qiu","Hongyan Li","Ming Jin","Shirui Pan"],"url":"https://arxiv.org/abs/2505.02417"}
{"created":"2025-05-06","title":"SymbioticRAG: Enhancing Document Intelligence Through Human-LLM Symbiotic Collaboration","abstract":"We present \\textbf{SymbioticRAG}, a novel framework that fundamentally reimagines Retrieval-Augmented Generation~(RAG) systems by establishing a bidirectional learning relationship between humans and machines. Our approach addresses two critical challenges in current RAG systems: the inherently human-centered nature of relevance determination and users' progression from \"unconscious incompetence\" in query formulation. SymbioticRAG introduces a two-tier solution where Level 1 enables direct human curation of retrieved content through interactive source document exploration, while Level 2 aims to build personalized retrieval models based on captured user interactions. We implement Level 1 through three key components: (1)~a comprehensive document processing pipeline with specialized models for layout detection, OCR, and extraction of tables, formulas, and figures; (2)~an extensible retriever module supporting multiple retrieval strategies; and (3)~an interactive interface that facilitates both user engagement and interaction data logging. We experiment Level 2 implementation via a retriever strategy incorporated LLM summarized user intention from user interaction logs. To maintain high-quality data preparation, we develop a human-on-the-loop validation interface that improves pipeline output while advancing research in specialized extraction tasks. Evaluation across three scenarios (literature review, geological exploration, and education) demonstrates significant improvements in retrieval relevance and user satisfaction compared to traditional RAG approaches. To facilitate broader research and further advancement of SymbioticRAG Level 2 implementation, we will make our system openly accessible to the research community.","authors":["Qiang Sun","Tingting Bi","Sirui Li","Eun-Jung Holden","Paul Duuring","Kai Niu","Wei Liu"],"url":"https://arxiv.org/abs/2505.02418"}
{"created":"2025-05-06","title":"Impact of Transceiver Selection on Synchronization Accuracy in White Rabbit Networks","abstract":"Achieving optimal synchronization accuracy between two White Rabbit devices hinges on the proper selection of transceivers, which act as electro-optical converters connecting WR devices to the optical network infrastructure. The correct choice of transceivers can significantly improve resilience to changes in the time offset between WR devices due to temperature fluctuations in the connecting optical fiber. To compare the performance of BiDi WDM and DWDM transceivers, an experimental setup was established under laboratory conditions to simulate a real optical network used for distributing precise time and frequency between two remote locations. The optical connection was emulated by integrating a 20 km G.652.D optical fiber into a climatic chamber, which provided variable environmental conditions similar to those experienced in real applications. The study compared BiDi WDM 1310/1550 nm transceivers with DWDM Ch33/Ch34 transceivers. Results showed that DWDM transceivers exhibited nearly thirteen times less sensitivity to temperature-induced changes in the optical connection, leading to a smaller time offset. Therefore, for achieving the highest accuracy in synchronizing WR devices in practical applications, DWDM transceiver technology is essential.","authors":["Michal \\v{S}pa\\v{c}ek","Josef Vojt\\v{e}ch","Jaroslav Rozto\\v{c}il"],"url":"https://arxiv.org/abs/2505.02420"}
{"created":"2025-05-06","title":"Sampling Kantorovich operators for speckle noise reduction using a Down-Up scaling approach and gap filling in remote sensing images","abstract":"In the literature, several approaches have been proposed for restoring and enhancing remote sensing images, including methods based on interpolation, filtering, and deep learning. In this paper, we investigate the application of multivariate sampling Kantorovich (SK) operators for image reconstruction, with a particular focus on gap filling and speckle noise reduction. To understand the accuracy performances of the proposed algorithms, we first derive a quantitative estimate in $C(\\R^n)$ for the error of approximation using the Euler-Maclaurin summation formula, which provides sharper error bounds under minimal regularity conditions. We also establish a convergence result and a quantitative estimate with respect to the dissimilarity index measured by the continuous SSIM for functions in Lebesgue spaces. Additionally, we prove a multidimensional linear prediction result, which is used to design a new SK-based reconstruction algorithm to handle missing data, that we call LP-SK algorithm. To address speckle noise, we integrate SK operators into a newly proposed Down-Up scaling approach. Numerical tests are presented on synthetic and real SAR images to validate the proposed methods. Performance is assessed using similarity metrics such as SSIM and PSNR, along with speckle-specific indexes. Comparative analysis with state-of-the-art techniques highlights the effectiveness of the proposed approaches.","authors":["Danilo Costarelli","Mariarosaria Natale"],"url":"https://arxiv.org/abs/2505.02422"}
{"created":"2025-05-06","title":"Towards One-shot Federated Learning: Advances, Challenges, and Future Directions","abstract":"One-shot FL enables collaborative training in a single round, eliminating the need for iterative communication, making it particularly suitable for use in resource-constrained and privacy-sensitive applications. This survey offers a thorough examination of One-shot FL, highlighting its distinct operational framework compared to traditional federated approaches. One-shot FL supports resource-limited devices by enabling single-round model aggregation while maintaining data locality. The survey systematically categorizes existing methodologies, emphasizing advancements in client model initialization, aggregation techniques, and strategies for managing heterogeneous data distributions. Furthermore, we analyze the limitations of current approaches, particularly in terms of scalability and generalization in non-IID settings. By analyzing cutting-edge techniques and outlining open challenges, this survey aspires to provide a comprehensive reference for researchers and practitioners aiming to design and implement One-shot FL systems, advancing the development and adoption of One-shot FL solutions in a real-world, resource-constrained scenario.","authors":["Flora Amato","Lingyu Qiu","Mohammad Tanveer","Salvatore Cuomo","Fabio Giampaolo","Francesco Piccialli"],"url":"https://arxiv.org/abs/2505.02426"}
{"created":"2025-05-06","title":"Can LLM-Simulated Practice and Feedback Upskill Human Counselors? A Randomized Study with 90+ Novice Counselors","abstract":"Training more counselors, from clinical students to peer supporters, can help meet the demand for accessible mental health support; however, current training approaches remain resource-intensive and difficult to scale effectively. Large Language Models (LLMs) offer promising solutions for growing counseling skills training through simulated practice and automated feedback. Despite successes in aligning LLMs with expert-counselor annotations, we do not know whether LLM-based counseling training tools -- such as AI patients that simulate real-world challenges and generative AI feedback with suggested alternatives and rationales -- actually lead to improvements in novice counselor skill development. We develop CARE, an LLM-simulated practice and feedback system, and randomize 94 novice counselors to practice using an AI patient, either alone or with AI feedback, measuring changes in their behavioral performance, self-assessments, and qualitative learning takeaways. Our results show the practice-and-feedback group improved in their use of reflections and questions (d=0.32-0.39, p$<$0.05). In contrast, the group that practiced with an AI patient alone did not show improvements, and in the case of empathy, actually had worse uses across time (d=$-$0.52, p=0.001) and when compared against the practice-and-feedback group (d=0.72, p=0.001). Participants' qualitative self-reflections revealed key differences: the practice-and-feedback group adopted a client-centered approach involving listening to and validating feelings, while the practice-alone group remained solution-oriented but delayed offering suggestions until gathering more information. Overall, these results suggest that LLM-based training systems can promote effective skill development, but that combining both simulated practice and structured feedback is critical.","authors":["Ryan Louie","Ifdita Hasan Orney","Juan Pablo Pacheco","Raj Sanjay Shah","Emma Brunskill","Diyi Yang"],"url":"https://arxiv.org/abs/2505.02428"}
{"created":"2025-05-06","title":"FairPO: Robust Preference Optimization for Fair Multi-Label Learning","abstract":"We propose FairPO, a novel framework designed to promote fairness in multi-label classification by directly optimizing preference signals with a group robustness perspective. In our framework, the set of labels is partitioned into privileged and non-privileged groups, and a preference-based loss inspired by Direct Preference Optimization (DPO) is employed to more effectively differentiate true positive labels from confusing negatives within the privileged group, while preserving baseline classification performance for non-privileged labels. By framing the learning problem as a robust optimization over groups, our approach dynamically adjusts the training emphasis toward groups with poorer performance, thereby mitigating bias and ensuring a fairer treatment across diverse label categories. In addition, we outline plans to extend this approach by investigating alternative loss formulations such as Simple Preference Optimisation (SimPO) and Contrastive Preference Optimization (CPO) to exploit reference-free reward formulations and contrastive training signals. Furthermore, we plan to extend FairPO with multilabel generation capabilities, enabling the model to dynamically generate diverse and coherent label sets for ambiguous inputs.","authors":["Soumen Kumar Mondal","Akshit Varmora","Prateek Chanda","Ganesh Ramakrishnan"],"url":"https://arxiv.org/abs/2505.02433"}
{"created":"2025-05-06","title":"A New Approach to Backtracking Counterfactual Explanations: A Causal Framework for Efficient Model Interpretability","abstract":"Counterfactual explanations enhance interpretability by identifying alternative inputs that produce different outputs, offering localized insights into model decisions. However, traditional methods often neglect causal relationships, leading to unrealistic examples. While newer approaches integrate causality, they are computationally expensive. To address these challenges, we propose an efficient method based on backtracking counterfactuals that incorporates causal reasoning to generate actionable explanations. We first examine the limitations of existing methods and then introduce our novel approach and its features. We also explore the relationship between our method and previous techniques, demonstrating that it generalizes them in specific scenarios. Finally, experiments show that our method provides deeper insights into model outputs.","authors":["Pouria Fatemi","Ehsan Sharifian","Mohammad Hossein Yassaee"],"url":"https://arxiv.org/abs/2505.02435"}
{"created":"2025-05-06","title":"Towards Effective Issue Assignment using Online Machine Learning","abstract":"Efficient issue assignment in software development relates to faster resolution time, resources optimization, and reduced development effort. To this end, numerous systems have been developed to automate issue assignment, including AI and machine learning approaches. Most of them, however, often solely focus on a posteriori analyses of textual features (e.g. issue titles, descriptions), disregarding the temporal characteristics of software development. Thus, they fail to adapt as projects and teams evolve, such cases of team evolution, or project phase shifts (e.g. from development to maintenance). To incorporate such cases in the issue assignment process, we propose an Online Machine Learning methodology that adapts to the evolving characteristics of software projects. Our system processes issues as a data stream, dynamically learning from new data and adjusting in real time to changes in team composition and project requirements. We incorporate metadata such as issue descriptions, components and labels and leverage adaptive drift detection mechanisms to identify when model re-evaluation is necessary. Upon assessing our methodology on a set of software projects, we conclude that it can be effective on issue assignment, while meeting the evolving needs of software teams.","authors":["Athanasios Michailoudis (Aristotle University of Thessaloniki)","Themistoklis Diamantopoulos (Aristotle University of Thessaloniki)","Antonios Favvas (Aristotle University of Thessaloniki)","Andreas L. Symeonidis (Aristotle University of Thessaloniki)"],"url":"https://arxiv.org/abs/2505.02437"}
{"created":"2025-05-06","title":"ReeM: Ensemble Building Thermodynamics Model for Efficient HVAC Control via Hierarchical Reinforcement Learning","abstract":"The building thermodynamics model, which predicts real-time indoor temperature changes under potential HVAC (Heating, Ventilation, and Air Conditioning) control operations, is crucial for optimizing HVAC control in buildings. While pioneering studies have attempted to develop such models for various building environments, these models often require extensive data collection periods and rely heavily on expert knowledge, making the modeling process inefficient and limiting the reusability of the models. This paper explores a model ensemble perspective that utilizes existing developed models as base models to serve a target building environment, thereby providing accurate predictions while reducing the associated efforts. Given that building data streams are non-stationary and the number of base models may increase, we propose a Hierarchical Reinforcement Learning (HRL) approach to dynamically select and weight the base models. Our approach employs a two-tiered decision-making process: the high-level focuses on model selection, while the low-level determines the weights of the selected models. We thoroughly evaluate the proposed approach through offline experiments and an on-site case study, and the experimental results demonstrate the effectiveness of our method.","authors":["Yang Deng","Yaohui Liu","Rui Liang","Dafang Zhao","Donghua Xie","Ittetsu Taniguchi","Dan Wang"],"url":"https://arxiv.org/abs/2505.02439"}
{"created":"2025-05-06","title":"Cooperative ISAC Network for Off-Grid Imaging-based Low-Altitude Surveillance","abstract":"The low-altitude economy has emerged as a critical focus for future economic development, emphasizing the urgent need for flight activity surveillance utilizing the existing sensing capabilities of mobile cellular networks. Traditional monostatic or localization-based sensing methods, however, encounter challenges in fusing sensing results and matching channel parameters. To address these challenges, we propose an innovative approach that directly draws the radio images of the low-altitude space, leveraging its inherent sparsity with compressed sensing (CS)-based algorithms and the cooperation of multiple base stations. Furthermore, recognizing that unmanned aerial vehicles (UAVs) are randomly distributed in space, we introduce a physics-embedded learning method to overcome off-grid issues inherent in CS-based models. Additionally, an online hard example mining method is incorporated into the design of the loss function, enabling the network to adaptively concentrate on the samples bearing significant discrepancy with the ground truth, thereby enhancing its ability to detect the rare UAVs within the expansive low-altitude space. Simulation results demonstrate the effectiveness of the imaging-based low-altitude surveillance approach, with the proposed physics-embedded learning algorithm significantly outperforming traditional CS-based methods under off-grid conditions.","authors":["Yixuan Huang","Jie Yang","Chao-Kai Wen","Shuqiang Xia","Xiao Li","Shi Jin"],"url":"https://arxiv.org/abs/2505.02440"}
{"created":"2025-05-06","title":"MSFNet-CPD: Multi-Scale Cross-Modal Fusion Network for Crop Pest Detection","abstract":"Accurate identification of agricultural pests is essential for crop protection but remains challenging due to the large intra-class variance and fine-grained differences among pest species. While deep learning has advanced pest detection, most existing approaches rely solely on low-level visual features and lack effective multi-modal integration, leading to limited accuracy and poor interpretability. Moreover, the scarcity of high-quality multi-modal agricultural datasets further restricts progress in this field. To address these issues, we construct two novel multi-modal benchmarks-CTIP102 and STIP102-based on the widely-used IP102 dataset, and introduce a Multi-scale Cross-Modal Fusion Network (MSFNet-CPD) for robust pest detection. Our approach enhances visual quality via a super-resolution reconstruction module, and feeds both the original and reconstructed images into the network to improve clarity and detection performance. To better exploit semantic cues, we propose an Image-Text Fusion (ITF) module for joint modeling of visual and textual features, and an Image-Text Converter (ITC) that reconstructs fine-grained details across multiple scales to handle challenging backgrounds. Furthermore, we introduce an Arbitrary Combination Image Enhancement (ACIE) strategy to generate a more complex and diverse pest detection dataset, MTIP102, improving the model's generalization to real-world scenarios. Extensive experiments demonstrate that MSFNet-CPD consistently outperforms state-of-the-art methods on multiple pest detection benchmarks. All code and datasets will be made publicly available at: https://github.com/Healer-ML/MSFNet-CPD.","authors":["Jiaqi Zhang","Zhuodong Liu","Kejian Yu"],"url":"https://arxiv.org/abs/2505.02441"}
{"created":"2025-05-06","title":"Investigating the Impact of Personalized AI Tutors on Language Learning Performance","abstract":"Driven by the global shift towards online learning prompted by the COVID 19 pandemic, Artificial Intelligence has emerged as a pivotal player in the field of education. Intelligent Tutoring Systems offer a new method of personalized teaching, replacing the limitations of traditional teaching methods. However, concerns arise about the ability of AI tutors to address skill development and engagement during the learning process. In this paper, I will conduct a quasi experiment with paired sample t test on 34 students pre and post use of AI tutors in language learning platforms like Santa and Duolingo to examine the relationship between students engagement, academic performance, and students satisfaction during a personalized language learning experience.","authors":["Simon Suh"],"url":"https://arxiv.org/abs/2505.02443"}
{"created":"2025-05-06","title":"Reconfigurable Intelligent Surface Aided Integrated Communication and Localization with a Single Access Point","abstract":"Reconfigurable intelligent surfaces (RISs) not only assist communication but also help the localization of user equipment (UE). This study focuses on the indoor localization of UE with a single access point (AP) aided by multiple RISs. First, we propose a two-stage channel estimation scheme where the phase shifts of RIS elements are tuned to obtain multiple channel soundings. In the first stage, the newtonized orthogonal matching pursuit algorithm extracts the parameters of multiple paths from the received signals. Then, the LOS path and RIS-reflected paths are identified. In the second stage, the estimated path gains of RIS-reflected paths with different phase shifts are utilized to determine the angle of arrival (AOA) at the RIS by obtaining the angular pseudo spectrum. Consequently, by taking the AP and RISs as reference points, the linear least squares estimator can locate UE with the estimated AOAs. Simulation results show that the proposed algorithm can realize centimeter-level localization accuracy in the discussed scenarios. Moreover, the higher accuracy of pseudo spectrum, a larger number of channel soundings, and a larger number of reference points can realize higher localization accuracy of UE.","authors":["Xiyu Wang","Yixuan Huang","Jie Yang","Yu Han","Shi Jin"],"url":"https://arxiv.org/abs/2505.02444"}
{"created":"2025-05-06","title":"Learned Intelligent Recognizer with Adaptively Customized RIS Phases in Communication Systems","abstract":"This study presents an advanced wireless system that embeds target recognition within reconfigurable intelligent surface (RIS)-aided communication systems, powered by cuttingedge deep learning innovations. Such a system faces the challenge of fine-tuning both the RIS phase shifts and neural network (NN) parameters, since they intricately interdepend on each other to accomplish the recognition task. To address these challenges, we propose an intelligent recognizer that strategically harnesses every piece of prior action responses, thereby ingeniously multiplexing downlink signals to facilitate environment sensing. Specifically, we design a novel NN based on the long short-term memory (LSTM) architecture and the physical channel model. The NN iteratively captures and fuses information from previous measurements and adaptively customizes RIS configurations to acquire the most relevant information for the recognition task in subsequent moments. Tailored dynamically, these configurations adapt to the scene, task, and target specifics. Simulation results reveal that our proposed method significantly outperforms the state-of-the-art method, while resulting in minimal impacts on communication performance, even as sensing is performed simultaneously.","authors":["Yixuan Huang","Jie Yang","Chao-Kai Wen","Shuqiang Xia","Xiao Li","Shi Jin"],"url":"https://arxiv.org/abs/2505.02446"}
{"created":"2025-05-06","title":"Correcting Multiple Substitutions in Nanopore-Sequencing Reads","abstract":"Despite their significant advantages over competing technologies, nanopore sequencers are plagued by high error rates, due to physical characteristics of the nanopore and inherent noise in the biological processes. It is thus paramount not only to formulate efficient error-correcting constructions for these channels, but also to establish bounds on the minimum redundancy required by such coding schemes. In this context, we adopt a simplified model of nanopore sequencing inspired by the work of Mao \\emph{et al.}, accounting for the effects of intersymbol interference and measurement noise. For an input sequence of length $n$, the vector that is produced, designated as the \\emph{read vector}, may additionally suffer at most \\(t\\) substitution errors. We employ the well-known graph-theoretic clique-cover technique to establish that at least \\(t\\log n -O(1)\\) bits of redundancy are required to correct multiple (\\(t \\geq 2\\)) substitutions. While this is surprising in comparison to the case of a single substitution, that necessitates at most \\(\\log \\log n - O(1)\\) bits of redundancy, a suitable error-correcting code that is optimal up to a constant follows immediately from the properties of read vectors.","authors":["Anisha Banerjee","Yonatan Yehezkeally","Antonia Wachter-Zeh","Eitan Yaakobi"],"url":"https://arxiv.org/abs/2505.02447"}
{"created":"2025-05-06","title":"Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey","abstract":"Out-of-distribution detection (OOD) is a pivotal task for real-world applications that trains models to identify samples that are distributionally different from the in-distribution (ID) data during testing. Recent advances in AI, particularly Vision-Language Models (VLMs) like CLIP, have revolutionized OOD detection by shifting from traditional unimodal image detectors to multimodal image-text detectors. This shift has inspired extensive research; however, existing categorization schemes (e.g., few- or zero-shot types) still rely solely on the availability of ID images, adhering to a unimodal paradigm. To better align with CLIP's cross-modal nature, we propose a new categorization framework rooted in both image and text modalities. Specifically, we categorize existing methods based on how visual and textual information of OOD data is utilized within image + text modalities, and further divide them into four groups: OOD Images (i.e., outliers) Seen or Unseen, and OOD Texts (i.e., learnable vectors or class names) Known or Unknown, across two training strategies (i.e., train-free or training-required). More importantly, we discuss open problems in CLIP-like OOD detection and highlight promising directions for future research, including cross-domain integration, practical applications, and theoretical understanding.","authors":["Chaohua Li","Enhao Zhang","Chuanxing Geng","Songcan Chen"],"url":"https://arxiv.org/abs/2505.02448"}
{"created":"2025-05-06","title":"Predicting the Dynamics of Complex System via Multiscale Diffusion Autoencoder","abstract":"Predicting the dynamics of complex systems is crucial for various scientific and engineering applications. The accuracy of predictions depends on the model's ability to capture the intrinsic dynamics. While existing methods capture key dynamics by encoding a low-dimensional latent space, they overlook the inherent multiscale structure of complex systems, making it difficult to accurately predict complex spatiotemporal evolution. Therefore, we propose a Multiscale Diffusion Prediction Network (MDPNet) that leverages the multiscale structure of complex systems to discover the latent space of intrinsic dynamics. First, we encode multiscale features through a multiscale diffusion autoencoder to guide the diffusion model for reliable reconstruction. Then, we introduce an attention-based graph neural ordinary differential equation to model the co-evolution across different scales. Extensive evaluations on representative systems demonstrate that the proposed method achieves an average prediction error reduction of 53.23% compared to baselines, while also exhibiting superior robustness and generalization.","authors":["Ruikun Li","Jingwen Cheng","Huandong Wang","Qingmin Liao","Yong Li"],"url":"https://arxiv.org/abs/2505.02450"}
{"created":"2025-05-06","title":"Decoding Insertions/Deletions via List Recovery","abstract":"In this work, we consider the problem of efficient decoding of codes from insertions and deletions. Most of the known efficient codes are codes with synchronization strings which allow one to reduce the problem of decoding insertions and deletions to that of decoding substitution and erasures. Our new approach, presented in this paper, reduces the problem of decoding insertions and deletions to that of list recovery. Specifically, any \\((\\rho, 2\\rho n + 1, L)\\)-list-recoverable code is a \\((\\rho, L)\\)-list decodable insdel code. As an example, we apply this technique to Reed-Solomon (RS) codes, which are known to have efficient list-recovery algorithms up to the Johnson bound. In the adversarial insdel model, this provides efficient (list) decoding from \\(t\\) insdel errors, assuming that \\(t\\cdot k = O(n)\\). This is the first efficient insdel decoder for \\([n, k]\\) RS codes for \\(k>2\\). Additionally, we explore random insdel models, such as the Davey-MacKay channel, and show that for certain choices of \\(\\rho\\), a \\((\\rho, n^{1/2+0.001}, L)\\)-list-recoverable code of length \\(n\\) can, with high probability, efficiently list decode the channel output, ensuring that the transmitted codeword is in the output list. In the context of RS codes, this leads to a better rate-error tradeoff for these channels compared to the adversarial case. We also adapt the Koetter-Vardy algorithm, a famous soft-decision list decoding technique for RS codes, to correct insertions and deletions induced by the Davey-MacKay channel.","authors":["Anisha Banerjee","Roni Con","Antonia Wachter-Zeh","Eitan Yaakobi"],"url":"https://arxiv.org/abs/2505.02452"}
{"created":"2025-05-06","title":"Running a Data Integration Lab in the Context of the EHRI Project: Challenges, Lessons Learnt and Future Directions","abstract":"Historical study of the Holocaust is commonly hampered by the dispersed and fragmented nature of important archival sources relating to this event. The EHRI project set out to mitigate this problem by building a trans-national network of archives, researchers, and digital practitioners, and one of its main outcomes was the creation of the EHRI Portal, a \"virtual observatory\" that gathers in one centralised platform descriptions of Holocaust-related archival sources from around the world. In order to build the Portal a strong data identification and integration effort was required, culminating in the project's third phase with the creation of the EHRI-3 data integration lab. The focus of the lab was to lower the bar to participation in the EHRI Portal by providing support to institutions in conforming their archival metadata with that required for integration, ultimately opening the process up to smaller institutions (and even so-called \"micro-archives\") without the necessary resources to undertake this process themselves. In this paper we present our experiences from running the data integration lab and discuss some of the challenges (both of a technical and social nature), how we tried to overcome them, and the overall lessons learnt. We envisage this work as an archetype upon which other practitioners seeking to pursue similar data integration activities can build their own efforts.","authors":["Herminio Garc\\'ia-Gonz\\'alez","Mike Bryant","Suzanne Swartz","Fabio Rovigo","Veerle Vanden Daelen"],"url":"https://arxiv.org/abs/2505.02455"}
{"created":"2025-05-06","title":"Colombian Waitresses y Jueces canadienses: Gender and Country Biases in Occupation Recommendations from LLMs","abstract":"One of the goals of fairness research in NLP is to measure and mitigate stereotypical biases that are propagated by NLP systems. However, such work tends to focus on single axes of bias (most often gender) and the English language. Addressing these limitations, we contribute the first study of multilingual intersecting country and gender biases, with a focus on occupation recommendations generated by large language models. We construct a benchmark of prompts in English, Spanish and German, where we systematically vary country and gender, using 25 countries and four pronoun sets. Then, we evaluate a suite of 5 Llama-based models on this benchmark, finding that LLMs encode significant gender and country biases. Notably, we find that even when models show parity for gender or country individually, intersectional occupational biases based on both country and gender persist. We also show that the prompting language significantly affects bias, and instruction-tuned models consistently demonstrate the lowest and most stable levels of bias. Our findings highlight the need for fairness researchers to use intersectional and multilingual lenses in their work.","authors":["Elisa Forcada Rodr\\'iguez","Olatz Perez-de-Vi\\~naspre","Jon Ander Campos","Dietrich Klakow","Vagrant Gautam"],"url":"https://arxiv.org/abs/2505.02456"}
{"created":"2025-05-06","title":"ZeloS -- A Research Platform for Early-Stage Validation of Research Findings Related to Automated Driving","abstract":"This paper presents ZeloS, a research platform designed and built for practical validation of automated driving methods in an early stage of research. We overview ZeloS' hardware setup and automation architecture and focus on motion planning and control. ZeloS weighs 69 kg, measures a length of 117 cm, and is equipped with all-wheel steering, all-wheel drive, and various onboard sensors for localization. The hardware setup and the automation architecture of ZeloS are designed and built with a focus on modularity and the goal of being simple yet effective. The modular design allows the modification of individual automation modules without the need for extensive onboarding into the automation architecture. As such, this design supports ZeloS in being a versatile research platform for validating various automated driving methods. The motion planning component and control of ZeloS feature optimization-based methods that allow for explicitly considering constraints. We demonstrate the hardware and automation setup by presenting experimental data.","authors":["Christopher Bohn","Florian Siebenrock","Janne Bosch","Tobias Hetzner","Samuel Mauch","Philipp Reis","Timo Staudt","Manuel Hess","Ben-Micha Piscol","S\\\"oren Hohmann"],"url":"https://arxiv.org/abs/2505.02460"}
{"created":"2025-05-06","title":"Incentivizing Inclusive Contributions in Model Sharing Markets","abstract":"While data plays a crucial role in training contemporary AI models, it is acknowledged that valuable public data will be exhausted in a few years, directing the world's attention towards the massive decentralized private data. However, the privacy-sensitive nature of raw data and lack of incentive mechanism prevent these valuable data from being fully exploited. Addressing these challenges, this paper proposes inclusive and incentivized personalized federated learning (iPFL), which incentivizes data holders with diverse purposes to collaboratively train personalized models without revealing raw data. iPFL constructs a model-sharing market by solving a graph-based training optimization and incorporates an incentive mechanism based on game theory principles. Theoretical analysis shows that iPFL adheres to two key incentive properties: individual rationality and truthfulness. Empirical studies on eleven AI tasks (e.g., large language models' instruction-following tasks) demonstrate that iPFL consistently achieves the highest economic utility, and better or comparable model performance compared to baseline methods. We anticipate that our iPFL can serve as a valuable technique for boosting future AI models on decentralized private data while making everyone satisfied.","authors":["Enpei Zhang","Jingyi Chai","Rui Ye","Yanfeng Wang","Siheng Chen"],"url":"https://arxiv.org/abs/2505.02462"}
{"created":"2025-05-06","title":"Data Augmentation With Back translation for Low Resource languages: A case of English and Luganda","abstract":"In this paper,we explore the application of Back translation (BT) as a semi-supervised technique to enhance Neural Machine Translation(NMT) models for the English-Luganda language pair, specifically addressing the challenges faced by low-resource languages. The purpose of our study is to demonstrate how BT can mitigate the scarcity of bilingual data by generating synthetic data from monolingual corpora. Our methodology involves developing custom NMT models using both publicly available and web-crawled data, and applying Iterative and Incremental Back translation techniques. We strategically select datasets for incremental back translation across multiple small datasets, which is a novel element of our approach. The results of our study show significant improvements, with translation performance for the English-Luganda pair exceeding previous benchmarks by more than 10 BLEU score units across all translation directions. Additionally, our evaluation incorporates comprehensive assessment metrics such as SacreBLEU, ChrF2, and TER, providing a nuanced understanding of translation quality. The conclusion drawn from our research confirms the efficacy of BT when strategically curated datasets are utilized, establishing new performance benchmarks and demonstrating the potential of BT in enhancing NMT models for low-resource languages.","authors":["Richard Kimera","Dongnyeong Heo","Daniela N. Rim","Heeyoul Choi"],"url":"https://arxiv.org/abs/2505.02463"}
{"created":"2025-05-06","title":"Targeted Fuzzing for Unsafe Rust Code: Leveraging Selective Instrumentation","abstract":"Rust is a promising programming language that focuses on concurrency, usability, and security. It is used in production code by major industry players and got recommended by government bodies. Rust provides strong security guarantees achieved by design utilizing the concepts of ownership and borrowing. However, Rust allows programmers to write unsafe code which is not subject to the strict Rust security policy. Empirical studies show that security issues in practice always involve code written in unsafe Rust.","authors":["David Paa{\\ss}en","Jens-Rene Giesen","Lucas Davi"],"url":"https://arxiv.org/abs/2505.02464"}
{"created":"2025-05-06","title":"Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language, and Modality","abstract":"Recent advancements in large language models (LLMs) have driven interest in billion-scale retrieval models with strong generalization across retrieval tasks and languages. Additionally, progress in large vision-language models has created new opportunities for multimodal retrieval. In response, we have updated the Tevatron toolkit, introducing a unified pipeline that enables researchers to explore retriever models at different scales, across multiple languages, and with various modalities. This demo paper highlights the toolkit's key features, bridging academia and industry by supporting efficient training, inference, and evaluation of neural retrievers. We showcase a unified dense retriever achieving strong multilingual and multimodal effectiveness, and conduct a cross-modality zero-shot study to demonstrate its research potential. Alongside, we release OmniEmbed, to the best of our knowledge, the first embedding model that unifies text, image document, video, and audio retrieval, serving as a baseline for future research.","authors":["Xueguang Ma","Luyu Gao","Shengyao Zhuang","Jiaqi Samantha Zhan","Jamie Callan","Jimmy Lin"],"url":"https://arxiv.org/abs/2505.02466"}
{"created":"2025-05-06","title":"Timing Is Everything: Finding the Optimal Fusion Points in Multimodal Medical Imaging","abstract":"Multimodal deep learning harnesses diverse imaging modalities, such as MRI sequences, to enhance diagnostic accuracy in medical imaging. A key challenge is determining the optimal timing for integrating these modalities-specifically, identifying the network layers where fusion modules should be inserted. Current approaches often rely on manual tuning or exhaustive search, which are computationally expensive without any guarantee of converging to optimal results. We propose a sequential forward search algorithm that incrementally activates and evaluates candidate fusion modules at different layers of a multimodal network. At each step, the algorithm retrains from previously learned weights and compares validation loss to identify the best-performing configuration. This process systematically reduces the search space, enabling efficient identification of the optimal fusion timing without exhaustively testing all possible module placements. The approach is validated on two multimodal MRI datasets, each addressing different classification tasks. Our algorithm consistently identified configurations that outperformed unimodal baselines, late fusion, and a brute-force ensemble of all potential fusion placements. These architectures demonstrated superior accuracy, F-score, and specificity while maintaining competitive or improved AUC values. Furthermore, the sequential nature of the search significantly reduced computational overhead, making the optimization process more practical. By systematically determining the optimal timing to fuse imaging modalities, our method advances multimodal deep learning for medical imaging. It provides an efficient and robust framework for fusion optimization, paving the way for improved clinical decision-making and more adaptable, scalable architectures in medical AI applications.","authors":["Valerio Guarrasi","Klara Mogensen","Sara Tassinari","Sara Qvarlander","Paolo Soda"],"url":"https://arxiv.org/abs/2505.02467"}
{"created":"2025-05-06","title":"Efficient Continual Learning in Keyword Spotting using Binary Neural Networks","abstract":"Keyword spotting (KWS) is an essential function that enables interaction with ubiquitous smart devices. However, in resource-limited devices, KWS models are often static and can thus not adapt to new scenarios, such as added keywords. To overcome this problem, we propose a Continual Learning (CL) approach for KWS built on Binary Neural Networks (BNNs). The framework leverages the reduced computation and memory requirements of BNNs while incorporating techniques that enable the seamless integration of new keywords over time. This study evaluates seven CL techniques on a 16-class use case, reporting an accuracy exceeding 95% for a single additional keyword and up to 86% for four additional classes. Sensitivity to the amount of training samples in the CL phase, and differences in computational complexities are being evaluated. These evaluations demonstrate that batch-based algorithms are more sensitive to the CL dataset size, and that differences between the computational complexities are insignificant. These findings highlight the potential of developing an effective and computationally efficient technique for continuously integrating new keywords in KWS applications that is compatible with resource-constrained devices.","authors":["Quynh Nguyen-Phuong Vu","Luciano Sebastian Martinez-Rau","Yuxuan Zhang","Nho-Duc Tran","Bengt Oelmann","Michele Magno","Sebastian Bader"],"url":"https://arxiv.org/abs/2505.02469"}
{"created":"2025-05-06","title":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction","abstract":"We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.","authors":["Biao Gong","Cheng Zou","Dandan Zheng","Hu Yu","Jingdong Chen","Jianxin Sun","Junbo Zhao","Jun Zhou","Kaixiang Ji","Lixiang Ru","Libin Wang","Qingpei Guo","Rui Liu","Weilong Chai","Xinyu Xiao","Ziyuan Huang"],"url":"https://arxiv.org/abs/2505.02471"}
{"created":"2025-05-06","title":"Trajectory Minimum Touching Ball","abstract":"We present algorithms to find the minimum radius sphere that intersects every trajectory in a set of $n$ trajectories composed of at most $k$ line segments each. When $k=1$, we can reduce the problem to the LP-type framework to achieve a linear time complexity. For $k \\geq 4$ we provide a trajectory configuration with unbounded LP-type complexity, but also present an almost $O\\left((nk)^2\\log n\\right)$ algorithm through the farthest line segment Voronoi diagrams. If we tolerate a relative approximation, we can reduce to time near-linear in $n$.","authors":["Jeff M. Phillips","Jens Kristian Refsgaard Schou"],"url":"https://arxiv.org/abs/2505.02472"}
{"created":"2025-05-06","title":"Point Cloud Recombination: Systematic Real Data Augmentation Using Robotic Targets for LiDAR Perception Validation","abstract":"The validation of LiDAR-based perception of intelligent mobile systems operating in open-world applications remains a challenge due to the variability of real environmental conditions. Virtual simulations allow the generation of arbitrary scenes under controlled conditions but lack physical sensor characteristics, such as intensity responses or material-dependent effects. In contrast, real-world data offers true sensor realism but provides less control over influencing factors, hindering sufficient validation. Existing approaches address this problem with augmentation of real-world point cloud data by transferring objects between scenes. However, these methods do not consider validation and remain limited in controllability because they rely on empirical data. We solve these limitations by proposing Point Cloud Recombination, which systematically augments captured point cloud scenes by integrating point clouds acquired from physical target objects measured in controlled laboratory environments. Thus enabling the creation of vast amounts and varieties of repeatable, physically accurate test scenes with respect to phenomena-aware occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we demonstrate the augmentation of real-world urban and rural scenes with humanoid targets featuring varied clothing and poses, for repeatable positioning. We show that the recombined scenes closely match real sensor outputs, enabling targeted testing, scalable failure analysis, and improved system safety. By providing controlled yet sensor-realistic data, our method enables trustworthy conclusions about the limitations of specific sensors in compound with their algorithms, e.g., object detection.","authors":["Hubert Padusinski","Christian Steinhauser","Christian Scherl","Julian Gaal","Jacob Langner"],"url":"https://arxiv.org/abs/2505.02476"}
{"created":"2025-05-06","title":"Finger Pose Estimation for Under-screen Fingerprint Sensor","abstract":"Two-dimensional pose estimation plays a crucial role in fingerprint recognition by facilitating global alignment and reduce pose-induced variations. However, existing methods are still unsatisfactory when handling with large angle or small area inputs. These limitations are particularly pronounced on fingerprints captured by under-screen fingerprint sensors in smartphones. In this paper, we present a novel dual-modal input based network for under-screen fingerprint pose estimation. Our approach effectively integrates two distinct yet complementary modalities: texture details extracted from ridge patches through the under-screen fingerprint sensor, and rough contours derived from capacitive images obtained via the touch screen. This collaborative integration endows our network with more comprehensive and discriminative information, substantially improving the accuracy and stability of pose estimation. A decoupled probability distribution prediction task is designed, instead of the traditional supervised forms of numerical regression or heatmap voting, to facilitate the training process. Additionally, we incorporate a Mixture of Experts (MoE) based feature fusion mechanism and a relationship driven cross-domain knowledge transfer strategy to further strengthen feature extraction and fusion capabilities. Extensive experiments are conducted on several public datasets and two private datasets. The results indicate that our method is significantly superior to previous state-of-the-art (SOTA) methods and remarkably boosts the recognition ability of fingerprint recognition algorithms. Our code is available at https://github.com/XiongjunGuan/DRACO.","authors":["Xiongjun Guan","Zhiyu Pan","Jianjiang Feng","Jie Zhou"],"url":"https://arxiv.org/abs/2505.02481"}
{"created":"2025-05-06","title":"Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning","abstract":"Enabling a high-degree-of-freedom robot to learn specific skills is a challenging task due to the complexity of robotic dynamics. Reinforcement learning (RL) has emerged as a promising solution; however, addressing such problems requires the design of multiple reward functions to account for various constraints in robotic motion. Existing approaches typically sum all reward components indiscriminately to optimize the RL value function and policy. We argue that this uniform inclusion of all reward components in policy optimization is inefficient and limits the robot's learning performance. To address this, we propose an Automated Hybrid Reward Scheduling (AHRS) framework based on Large Language Models (LLMs). This paradigm dynamically adjusts the learning intensity of each reward component throughout the policy optimization process, enabling robots to acquire skills in a gradual and structured manner. Specifically, we design a multi-branch value network, where each branch corresponds to a distinct reward component. During policy optimization, each branch is assigned a weight that reflects its importance, and these weights are automatically computed based on rules designed by LLMs. The LLM generates a rule set in advance, derived from the task description, and during training, it selects a weight calculation rule from the library based on language prompts that evaluate the performance of each branch. Experimental results demonstrate that the AHRS method achieves an average 6.48% performance improvement across multiple high-degree-of-freedom robotic tasks.","authors":["Changxin Huang","Junyang Liang","Yanbin Chang","Jingzhao Xu","Jianqiang Li"],"url":"https://arxiv.org/abs/2505.02483"}
{"created":"2025-05-06","title":"El Agente: An Autonomous Agent for Quantum Chemistry","abstract":"Computational chemistry tools are widely used to study the behaviour of chemical phenomena. Yet, the complexity of these tools can make them inaccessible to non-specialists and challenging even for experts. In this work, we introduce El Agente Q, an LLM-based multi-agent system that dynamically generates and executes quantum chemistry workflows from natural language user prompts. The system is built on a novel cognitive architecture featuring a hierarchical memory framework that enables flexible task decomposition, adaptive tool selection, post-analysis, and autonomous file handling and submission. El Agente Q is benchmarked on six university-level course exercises and two case studies, demonstrating robust problem-solving performance (averaging >87% task success) and adaptive error handling through in situ debugging. It also supports longer-term, multi-step task execution for more complex workflows, while maintaining transparency through detailed action trace logs. Together, these capabilities lay the foundation for increasingly autonomous and accessible quantum chemistry.","authors":["Yunheng Zou","Austin H. Cheng","Abdulrahman Aldossary","Jiaru Bai","Shi Xuan Leong","Jorge Arturo Campos-Gonzalez-Angulo","Changhyeok Choi","Cher Tian Ser","Gary Tom","Andrew Wang","Zijian Zhang","Ilya Yakavets","Han Hao","Chris Crebolder","Varinia Bernales","Al\\'an Aspuru-Guzik"],"url":"https://arxiv.org/abs/2505.02484"}
{"created":"2025-05-06","title":"SEFE: Superficial and Essential Forgetting Eliminator for Multimodal Continual Instruction Tuning","abstract":"Multimodal Continual Instruction Tuning (MCIT) aims to enable Multimodal Large Language Models (MLLMs) to incrementally learn new tasks without catastrophic forgetting. In this paper, we explore forgetting in this context, categorizing it into superficial forgetting and essential forgetting. Superficial forgetting refers to cases where the model's knowledge may not be genuinely lost, but its responses to previous tasks deviate from expected formats due to the influence of subsequent tasks' answer styles, making the results unusable. By contrast, essential forgetting refers to situations where the model provides correctly formatted but factually inaccurate answers, indicating a true loss of knowledge. Assessing essential forgetting necessitates addressing superficial forgetting first, as severe superficial forgetting can obscure the model's knowledge state. Hence, we first introduce the Answer Style Diversification (ASD) paradigm, which defines a standardized process for transforming data styles across different tasks, unifying their training sets into similarly diversified styles to prevent superficial forgetting caused by style shifts. Building on this, we propose RegLoRA to mitigate essential forgetting. RegLoRA stabilizes key parameters where prior knowledge is primarily stored by applying regularization, enabling the model to retain existing competencies. Experimental results demonstrate that our overall method, SEFE, achieves state-of-the-art performance.","authors":["Jinpeng Chen","Runmin Cong","Yuzhi Zhao","Hongzheng Yang","Guangneng Hu","Horace Ho Shing Ip","Sam Kwong"],"url":"https://arxiv.org/abs/2505.02486"}
{"created":"2025-05-06","title":"Beyond the model: Key differentiators in large language models and multi-agent services","abstract":"With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it has become evident that large language models (LLMs) are no longer the sole defining factor in generative AI. As many now operate at comparable levels of capability, the real race is not about having the biggest model but optimizing the surrounding ecosystem, including data quality and management, computational efficiency, latency, and evaluation frameworks. This review article delves into these critical differentiators that ensure modern AI services are efficient and profitable.","authors":["Muskaan Goyal","Pranav Bhasin"],"url":"https://arxiv.org/abs/2505.02489"}
{"created":"2025-05-06","title":"Bayesian Robust Aggregation for Federated Learning","abstract":"Federated Learning enables collaborative training of machine learning models on decentralized data. This scheme, however, is vulnerable to adversarial attacks, when some of the clients submit corrupted model updates. In real-world scenarios, the total number of compromised clients is typically unknown, with the extent of attacks potentially varying over time. To address these challenges, we propose an adaptive approach for robust aggregation of model updates based on Bayesian inference. The mean update is defined by the maximum of the likelihood marginalized over probabilities of each client to be `honest'. As a result, the method shares the simplicity of the classical average estimators (e.g., sample mean or geometric median), being independent of the number of compromised clients. At the same time, it is as effective against attacks as methods specifically tailored to Federated Learning, such as Krum. We compare our approach with other aggregation schemes in federated setting on three benchmark image classification data sets. The proposed method consistently achieves state-of-the-art performance across various attack types with static and varying number of malicious clients.","authors":["Aleksandr Karakulev (Uppsala University)","Usama Zafar (Uppsala University)","Salman Toor (Uppsala University","Scaleout Systems)","Prashant Singh (Uppsala University","Science for Life Laboratory","Sweden)"],"url":"https://arxiv.org/abs/2505.02490"}
{"created":"2025-05-06","title":"Uncertainty in Repeated Implicit Feedback as a Measure of Reliability","abstract":"Recommender systems rely heavily on user feedback to learn effective user and item representations. Despite their widespread adoption, limited attention has been given to the uncertainty inherent in the feedback used to train these systems. Both implicit and explicit feedback are prone to noise due to the variability in human interactions, with implicit feedback being particularly challenging. In collaborative filtering, the reliability of interaction signals is critical, as these signals determine user and item similarities. Thus, deriving accurate confidence measures from implicit feedback is essential for ensuring the reliability of these signals.","authors":["Bruno Sguerra","Viet-Anh Tran","Romain Hennequin","Manuel Moussallam"],"url":"https://arxiv.org/abs/2505.02492"}
{"created":"2025-05-06","title":"Dynamic Graph-based Fingerprinting of In-browser Cryptomining","abstract":"The decentralized and unregulated nature of cryptocurrencies, combined with their monetary value, has made them a vehicle for various illicit activities. One such activity is cryptojacking, an attack that uses stolen computing resources to mine cryptocurrencies without consent for profit. In-browser cryptojacking malware exploits high-performance web technologies like WebAssembly to mine cryptocurrencies directly within the browser without file downloads. Although existing methods for cryptomining detection report high accuracy and low overhead, they are often susceptible to various forms of obfuscation, and due to the limited variety of cryptomining scripts in the wild, standard code obfuscation methods present a natural and appealing solution to avoid detection. To address these limitations, we propose using instruction-level data-flow graphs to detect cryptomining behavior. Data-flow graphs offer detailed structural insights into a program's computations, making them suitable for characterizing proof-of-work algorithms, but they can be difficult to analyze due to their large size and susceptibility to noise and fragmentation under obfuscation. We present two techniques to simplify and compare data-flow graphs: (1) a graph simplification algorithm to reduce the computational burden of processing large and granular data-flow graphs while preserving local substructures; and (2) a subgraph similarity measure, the n-fragment inclusion score, based on fragment inclusion that is robust against noise and obfuscation. Using data-flow graphs as computation fingerprints, our detection framework PoT (Proof-of-Theft) was able to achieve high detection accuracy against standard obfuscations, outperforming existing detection methods. Moreover, PoT uses generic data-flow properties that can be applied to other platforms more susceptible to cryptojacking such as servers and data centers.","authors":["Tanapoom Sermchaiwong","Jiasi Shen"],"url":"https://arxiv.org/abs/2505.02493"}
{"created":"2025-05-06","title":"An Efficient Hybrid Key Exchange Mechanism","abstract":"We present \\textsc{CHOKE}, a novel code-based hybrid key-encapsulation mechanism (KEM) designed to securely and efficiently transmit multiple session keys simultaneously. By encoding $n$ independent session keys with an individually secure linear code and encapsulating each resulting coded symbol using a separate KEM, \\textsc{CHOKE} achieves computational individual security -- each key remains secure as long as at least one underlying KEM remains unbroken. Compared to traditional serial or combiner-based hybrid schemes, \\textsc{CHOKE} reduces computational and communication costs by an $n$-fold factor. Furthermore, we show that the communication cost of our construction is optimal under the requirement that each KEM must be used at least once.","authors":["Benjamin D. Kim","Vipindev Adat Vasudevan","Alejandro Cohen","Rafael G. L. D'Oliveira","Thomas Stahlbuhk","Muriel M\\'edard"],"url":"https://arxiv.org/abs/2505.02499"}
{"created":"2025-05-06","title":"Automating Automotive Software Development: A Synergy of Generative AI and Formal Methods","abstract":"As the automotive industry shifts its focus toward software-defined vehicles, the need for faster and reliable software development continues to grow. However, traditional methods show their limitations. The rise of Generative Artificial Intelligence (GenAI), particularly Large Language Models (LLMs), introduces new opportunities to automate automotive software development tasks such as requirement analysis and code generation. However, due to the complexity of automotive systems, where software components must interact with each other seamlessly, challenges remain in software integration and system-level validation. In this paper, we propose to combine GenAI with model-driven engineering to automate automotive software development. Our approach uses LLMs to convert free-text requirements into event chain descriptions and to generate platform-independent software components that realize the required functionality. At the same time, formal models are created based on event chain descriptions to support system validation and the generation of integration code for integrating generated software components in the whole vehicle system through middleware. This approach increases development automation while enabling formal analysis to improve system reliability. As a proof of concept, we used GPT-4o to implement our method and tested it in the CARLA simulation environment with ROS2 middleware. We evaluated the system in a simple Autonomous Emergency Braking scenario.","authors":["Fengjunjie Pan","Yinglei Song","Long Wen","Nenad Petrovic","Krzysztof Lebioda","Alois Knoll"],"url":"https://arxiv.org/abs/2505.02500"}
{"created":"2025-05-06","title":"Corr2Distrib: Making Ambiguous Correspondences an Ally to Predict Reliable 6D Pose Distributions","abstract":"We introduce Corr2Distrib, the first correspondence-based method which estimates a 6D camera pose distribution from an RGB image, explaining the observations. Indeed, symmetries and occlusions introduce visual ambiguities, leading to multiple valid poses. While a few recent methods tackle this problem, they do not rely on local correspondences which, according to the BOP Challenge, are currently the most effective way to estimate a single 6DoF pose solution. Using correspondences to estimate a pose distribution is not straightforward, since ambiguous correspondences induced by visual ambiguities drastically decrease the performance of PnP. With Corr2Distrib, we turn these ambiguities into an advantage to recover all valid poses. Corr2Distrib first learns a symmetry-aware representation for each 3D point on the object's surface, characterized by a descriptor and a local frame. This representation enables the generation of 3DoF rotation hypotheses from single 2D-3D correspondences. Next, we refine these hypotheses into a 6DoF pose distribution using PnP and pose scoring. Our experimental evaluations on complex non-synthetic scenes show that Corr2Distrib outperforms state-of-the-art solutions for both pose distribution estimation and single pose estimation from an RGB image, demonstrating the potential of correspondences-based approaches.","authors":["Asma Brazi","Boris Meden","Fabrice Mayran de Chamisso","Steve Bourgeois","Vincent Lepetit"],"url":"https://arxiv.org/abs/2505.02501"}
{"created":"2025-05-06","title":"Unveiling the Landscape of LLM Deployment in the Wild: An Empirical Study","abstract":"Background: Large language models (LLMs) are increasingly deployed via open-source and commercial frameworks, enabling individuals and organizations to self-host advanced AI capabilities. However, insecure defaults and misconfigurations often expose LLM services to the public Internet, posing significant security and system engineering risks. Aims: This study aims to unveil the current landscape of public-facing LLM deployments in the wild through a large-scale empirical study, focusing on service prevalence, exposure characteristics, systemic vulnerabilities, and associated risks. Method: We conducted an Internet-wide measurement to identify public-facing LLM deployments across 15 frameworks, discovering 320,102 services. We extracted 158 unique API endpoints, grouped into 12 functional categories based on capabilities and security risks. We further analyzed configurations, authentication practices, and geographic distributions, revealing deployment trends and systemic issues in real-world LLM system engineering. Results: Our study shows that public LLM deployments are rapidly growing but often insecure. Among all endpoints, we observe widespread use of insecure protocols, poor TLS configurations, and unauthenticated access to critical operations. Security risks, including model disclosure, system leakage, and unauthorized access, are pervasive, highlighting the need for secure-by-default frameworks and stronger deployment practices. Conclusions: Public-facing LLM deployments suffer from widespread security and configuration flaws, exposing services to misuse, model theft, resource hijacking, and remote exploitation. Strengthening default security, deployment practices, and operational standards is critical for the growing self-hosted LLM ecosystem.","authors":["Xinyi Hou","Jiahao Han","Yanjie Zhao","Haoyu Wang"],"url":"https://arxiv.org/abs/2505.02502"}
{"created":"2025-05-06","title":"Exploring Design Choices for Autoregressive Deep Learning Climate Models","abstract":"Deep Learning models have achieved state-of-the-art performance in medium-range weather prediction but often fail to maintain physically consistent rollouts beyond 14 days. In contrast, a few atmospheric models demonstrate stability over decades, though the key design choices enabling this remain unclear. This study quantitatively compares the long-term stability of three prominent DL-MWP architectures - FourCastNet, SFNO, and ClimaX - trained on ERA5 reanalysis data at 5.625{\\deg} resolution. We systematically assess the impact of autoregressive training steps, model capacity, and choice of prognostic variables, identifying configurations that enable stable 10-year rollouts while preserving the statistical properties of the reference dataset. Notably, rollouts with SFNO exhibit the greatest robustness to hyperparameter choices, yet all models can experience instability depending on the random seed and the set of prognostic variables","authors":["Florian Gallusser","Simon Hentschel","Anna Krause","Andreas Hotho"],"url":"https://arxiv.org/abs/2505.02506"}
{"created":"2025-05-06","title":"Cooley-Tukey FFT over $\\mathbb{Q}_p$ via Unramified Cyclotomic Extension","abstract":"The reason why Cooley-Tukey Fast Fourier Transform (FFT) over $\\mathbb{Q}$ can be efficiently implemented using complex roots of unity is that the cyclotomic extensions of the completion $\\mathbb{R}$ of $\\mathbb{Q}$ are at most quadratic, and that roots of unity in $\\mathbb{C}$ can be evaluated quickly. In this paper, we investigate a $p$-adic analogue of this efficient FFT. A naive application of this idea--such as invoking well-known algorithms like the Cantor-Zassenhaus algorithm or Hensel's lemma for polynomials to compute roots of unity--would incur a cost quadratic in the degree of the input polynomial. This would eliminate the computational advantage of using FFT in the first place. We present a method for computing roots of unity with lower complexity than the FFT computation itself. This suggests the possibility of designing new FFT algorithms for rational numbers. As a simple application, we construct an $O(N^{1+o(1)})$-time FFT algorithm over $\\mathbb{Q}_p$ for fixed $p$.","authors":["Hiromasa Kondo"],"url":"https://arxiv.org/abs/2505.02509"}
{"created":"2025-05-06","title":"Trustworthy Inter-Provider Agreements in 6G Using a Privacy-Enabled Hybrid Blockchain Framework","abstract":"Inter-provider agreements are central to 6G networks, where administrative domains must securely and dynamically share services. To address the dual need for transparency and confidentiality, we propose a privacy-enabled hybrid blockchain setup using Hyperledger Besu, integrating both public and private transaction workflows. The system enables decentralized service registration, selection, and SLA breach reporting through role-based smart contracts and privacy groups. We design and deploy a proof-of-concept implementation, evaluating performance using end-to-end latency as a key metric within privacy groups. Results show that public interactions maintain stable latency, while private transactions incur additional overhead due to off-chain coordination. The block production rate governed by IBFT 2.0 had limited impact on private transaction latency, due to encryption and peer synchronization. Lessons learned highlight design considerations for smart contract structure, validator management, and scalability patterns suitable for dynamic inter-domain collaboration. Our findings offer practical insights for deploying trustworthy agreement systems in 6G networks using privacy-enabled hybrid blockchains.","authors":["Farhana Javed","Josep Mangues-Bafalluy"],"url":"https://arxiv.org/abs/2505.02513"}
{"created":"2025-05-06","title":"Uncovering Population PK Covariates from VAE-Generated Latent Spaces","abstract":"Population pharmacokinetic (PopPK) modelling is a fundamental tool for understanding drug behaviour across diverse patient populations and enabling personalized dosing strategies to improve therapeutic outcomes. A key challenge in PopPK analysis lies in identifying and modelling covariates that influence drug absorption, as these relationships are often complex and nonlinear. Traditional methods may fail to capture hidden patterns within the data. In this study, we propose a data-driven, model-free framework that integrates Variational Autoencoders (VAEs) deep learning model and LASSO regression to uncover key covariates from simulated tacrolimus pharmacokinetic (PK) profiles. The VAE compresses high-dimensional PK signals into a structured latent space, achieving accurate reconstruction with a mean absolute percentage error (MAPE) of 2.26%. LASSO regression is then applied to map patient-specific covariates to the latent space, enabling sparse feature selection through L1 regularization. This approach consistently identifies clinically relevant covariates for tacrolimus including SNP, age, albumin, and hemoglobin which are retained across the tested regularization strength levels, while effectively discarding non-informative features. The proposed VAE-LASSO methodology offers a scalable, interpretable, and fully data-driven solution for covariate selection, with promising applications in drug development and precision pharmacotherapy.","authors":["Diego Perazzolo","Chiara Castellani","Enrico Grisan"],"url":"https://arxiv.org/abs/2505.02514"}
{"created":"2025-05-06","title":"FedSDAF: Leveraging Source Domain Awareness for Enhanced Federated Domain Generalization","abstract":"Traditional domain generalization approaches predominantly focus on leveraging target domain-aware features while overlooking the critical role of source domain-specific characteristics, particularly in federated settings with inherent data isolation. To address this gap, we propose the Federated Source Domain Awareness Framework (FedSDAF), the first method to systematically exploit source domain-aware features for enhanced federated domain generalization (FedDG). The FedSDAF framework consists of two synergistic components: the Domain-Invariant Adapter, which preserves critical domain-invariant features, and the Domain-Aware Adapter, which extracts and integrates source domain-specific knowledge using a Multihead Self-Attention mechanism (MHSA). Furthermore, we introduce a bidirectional knowledge distillation mechanism that fosters knowledge sharing among clients while safeguarding privacy. Our approach represents the first systematic exploitation of source domain-aware features, resulting in significant advancements in model generalization capability.Extensive experiments on four standard benchmarks (OfficeHome, PACS, VLCS, and DomainNet) show that our method consistently surpasses state-of-the-art federated domain generalization approaches, with accuracy gains of 5.2-13.8%. The source code is available at https://github.com/pizzareapers/FedSDAF.","authors":["Hongze Li","Zesheng Zhou","Zhenbiao Cao","Xinhui Li","Wei Chen","Xiaojin Zhang"],"url":"https://arxiv.org/abs/2505.02515"}
{"created":"2025-05-06","title":"Machine-Learning-Powered Neural Interfaces for Smart Prosthetics and Diagnostics","abstract":"Advanced neural interfaces are transforming applications ranging from neuroscience research to diagnostic tools (for mental state recognition, tremor and seizure detection) as well as prosthetic devices (for motor and communication recovery). By integrating complex functions into miniaturized neural devices, these systems unlock significant opportunities for personalized assistive technologies and adaptive therapeutic interventions. Leveraging high-density neural recordings, on-site signal processing, and machine learning (ML), these interfaces extract critical features, identify disease neuro-markers, and enable accurate, low-latency neural decoding. This integration facilitates real-time interpretation of neural signals, adaptive modulation of brain activity, and efficient control of assistive devices. Moreover, the synergy between neural interfaces and ML has paved the way for self-sufficient, ubiquitous platforms capable of operating in diverse environments with minimal hardware costs and external dependencies. In this work, we review recent advancements in AI-driven decoding algorithms and energy-efficient System-on-Chip (SoC) platforms for next-generation miniaturized neural devices. These innovations highlight the potential for developing intelligent neural interfaces, addressing critical challenges in scalability, reliability, interpretability, and user adaptability.","authors":["MohammadAli Shaeri","Jinhan Liu","Mahsa Shoaran"],"url":"https://arxiv.org/abs/2505.02516"}
{"created":"2025-05-06","title":"Finite difference method for nonlinear damped viscoelastic Euler-Bernoulli beam model","abstract":"We propose and analyze the numerical approximation for a viscoelastic Euler-Bernoulli beam model containing a nonlinear strong damping coefficient. The finite difference method is used for spatial discretization, while the backward Euler method and the averaged PI rule are applied for temporal discretization. The long-time stability and the finite-time error estimate of the numerical solutions are derived for both the semi-discrete-in-space scheme and the fully-discrete scheme. Furthermore, the Leray-Schauder theorem is used to derive the existence and uniqueness of the fully-discrete numerical solutions. Finally, the numerical results verify the theoretical analysis.","authors":["Wenlin Qiu","Xiangcheng Zheng","Tao Guo","Xu Xiao"],"url":"https://arxiv.org/abs/2505.02517"}
{"created":"2025-05-06","title":"Bemba Speech Translation: Exploring a Low-Resource African Language","abstract":"This paper describes our system submission to the International Conference on Spoken Language Translation (IWSLT 2025), low-resource languages track, namely for Bemba-to-English speech translation. We built cascaded speech translation systems based on Whisper and NLLB-200, and employed data augmentation techniques, such as back-translation. We investigate the effect of using synthetic data and discuss our experimental setup.","authors":["Muhammad Hazim Al Farouq","Aman Kassahun Wassie","Yasmin Moslem"],"url":"https://arxiv.org/abs/2505.02518"}
{"created":"2025-05-06","title":"Deaf in AI: AI language technologies and the erosion of linguistic rights","abstract":"This paper explores the interplay of AI language technologies, sign language interpreting, and linguistic access, highlighting the complex interdependencies shaping access frameworks and the tradeoffs these technologies bring. While AI tools promise innovation, they also perpetuate biases, reinforce technoableism, and deepen inequalities through systemic and design flaws. The historical and contemporary privileging of sign language interpreting as the dominant access model, and the broader inclusion ideologies it reflects, shape AIs development and deployment, often sidelining deaf languaging practices and introducing new forms of linguistic subordination to technology. Drawing on Deaf Studies, Sign Language Interpreting Studies, and crip technoscience, this paper critiques the framing of AI as a substitute for interpreters and examines its implications for access hierarchies. It calls for deaf-led approaches to foster AI systems that remain equitable, inclusive, and trustworthy, supporting rather than undermining linguistic autonomy and contributing to deaf aligned futures.","authors":["Maartje De Meulder"],"url":"https://arxiv.org/abs/2505.02519"}
{"created":"2025-05-06","title":"Attestable builds: compiling verifiable binaries on untrusted systems using trusted execution environments","abstract":"In this paper we present attestable builds, a new paradigm to provide strong source-to-binary correspondence in software artifacts. We tackle the challenge of opaque build pipelines that disconnect the trust between source code, which can be understood and audited, and the final binary artifact, which is difficult to inspect. Our system uses modern trusted execution environments (TEEs) and sandboxed build containers to provide strong guarantees that a given artifact was correctly built from a specific source code snapshot. As such it complements existing approaches like reproducible builds which typically require time-intensive modifications to existing build configurations and dependencies, and require independent parties to continuously build and verify artifacts. In comparison, an attestable build requires only minimal changes to an existing project, and offers nearly instantaneous verification of the correspondence between a given binary and the source code and build pipeline used to construct it. We evaluate it by building open-source software libraries - focusing on projects which are important to the trust chain and those which have proven difficult to be built deterministically. Overall, the overhead (42 seconds start-up latency and 14% increase in build duration) is small in comparison to the overall build time. Importantly, our prototype builds even complex projects such as LLVM Clang without requiring any modifications to their source code and build scripts. Finally, we formally model and verify the attestable build design to demonstrate its security against well-resourced adversaries.","authors":["Daniel Hugenroth","Mario Lins","Ren\\'e Mayrhofer","Alastair Beresford"],"url":"https://arxiv.org/abs/2505.02521"}
{"created":"2025-05-06","title":"Text to Image Generation and Editing: A Survey","abstract":"Text-to-image generation (T2I) refers to the text-guided generation of high-quality images. In the past few years, T2I has attracted widespread attention and numerous works have emerged. In this survey, we comprehensively review 141 works conducted from 2021 to 2024. First, we introduce four foundation model architectures of T2I (autoregression, non-autoregression, GAN and diffusion) and the commonly used key technologies (autoencoder, attention and classifier-free guidance). Secondly, we systematically compare the methods of these studies in two directions, T2I generation and T2I editing, including the encoders and the key technologies they use. In addition, we also compare the performance of these researches side by side in terms of datasets, evaluation metrics, training resources, and inference speed. In addition to the four foundation models, we survey other works on T2I, such as energy-based models and recent Mamba and multimodality. We also investigate the potential social impact of T2I and provide some solutions. Finally, we propose unique insights of improving the performance of T2I models and possible future development directions. In summary, this survey is the first systematic and comprehensive overview of T2I, aiming to provide a valuable guide for future researchers and stimulate continued progress in this field.","authors":["Pengfei Yang","Ngai-Man Cheung","Xinda Ma"],"url":"https://arxiv.org/abs/2505.02527"}
{"created":"2025-05-06","title":"RobSurv: Vector Quantization-Based Multi-Modal Learning for Robust Cancer Survival Prediction","abstract":"Cancer survival prediction using multi-modal medical imaging presents a critical challenge in oncology, mainly due to the vulnerability of deep learning models to noise and protocol variations across imaging centers. Current approaches struggle to extract consistent features from heterogeneous CT and PET images, limiting their clinical applicability. We address these challenges by introducing RobSurv, a robust deep-learning framework that leverages vector quantization for resilient multi-modal feature learning. The key innovation of our approach lies in its dual-path architecture: one path maps continuous imaging features to learned discrete codebooks for noise-resistant representation, while the parallel path preserves fine-grained details through continuous feature processing. This dual representation is integrated through a novel patch-wise fusion mechanism that maintains local spatial relationships while capturing global context via Transformer-based processing. In extensive evaluations across three diverse datasets (HECKTOR, H\\&amp;N1, and NSCLC Radiogenomics), RobSurv demonstrates superior performance, achieving concordance index of 0.771, 0.742, and 0.734 respectively - significantly outperforming existing methods. Most notably, our model maintains robust performance even under severe noise conditions, with performance degradation of only 3.8-4.5\\% compared to 8-12\\% in baseline methods. These results, combined with strong generalization across different cancer types and imaging protocols, establish RobSurv as a promising solution for reliable clinical prognosis that can enhance treatment planning and patient care.","authors":["Aiman Farooq","Azad Singh","Deepak Mishra","Santanu Chaudhury"],"url":"https://arxiv.org/abs/2505.02529"}
{"created":"2025-05-06","title":"A posteriori error estimates for the finite element approximation of the convection-diffusion-reaction equation based on the variational multiscale concept","abstract":"In this study, we employ the variational multiscale (VMS) concept to develop a posteriori error estimates for the stationary convection-diffusion-reaction equation. The variational multiscale method is based on splitting the continuous part of the problem into a resolved scale (coarse scale) and an unresolved scale (fine scale). The unresolved scale (also known as the sub-grid scale) is modeled by choosing it proportional to the component of the residual orthogonal to the finite element space, leading to the orthogonal sub-grid scale (OSGS) method. The idea is then to use the modeled sub-grid scale as an error estimator, considering its contribution in the element interiors and on the edges. We present the results of the a priori analysis and two different strategies for the a posteriori error analysis for the OSGS method. Our proposal is to use a scaled norm of the sub-grid scales as an a posteriori error estimate in the so-called stabilized norm of the problem. This norm has control over the convective term, which is necessary for convection-dominated problems. Numerical examples show the reliable performance of the proposed error estimator compared to other error estimators belonging to the variational multiscale family.","authors":["Ramon Codina","Hauke Gravenkamp","Sheraz Ahmed Khan"],"url":"https://arxiv.org/abs/2505.02531"}
{"created":"2025-05-06","title":"Large Language Model Partitioning for Low-Latency Inference at the Edge","abstract":"Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.","authors":["Dimitrios Kafetzis","Ramin Khalili","Iordanis Koutsopoulos"],"url":"https://arxiv.org/abs/2505.02533"}
{"created":"2025-05-06","title":"Advancing Constrained Monotonic Neural Networks: Achieving Universal Approximation Beyond Bounded Activations","abstract":"Conventional techniques for imposing monotonicity in MLPs by construction involve the use of non-negative weight constraints and bounded activation functions, which pose well-known optimization challenges. In this work, we generalize previous theoretical results, showing that MLPs with non-negative weight constraint and activations that saturate on alternating sides are universal approximators for monotonic functions. Additionally, we show an equivalence between the saturation side in the activations and the sign of the weight constraint. This connection allows us to prove that MLPs with convex monotone activations and non-positive constrained weights also qualify as universal approximators, in contrast to their non-negative constrained counterparts. Our results provide theoretical grounding to the empirical effectiveness observed in previous works while leading to possible architectural simplification. Moreover, to further alleviate the optimization difficulties, we propose an alternative formulation that allows the network to adjust its activations according to the sign of the weights. This eliminates the requirement for weight reparameterization, easing initialization and improving training stability. Experimental evaluation reinforces the validity of the theoretical results, showing that our novel approach compares favourably to traditional monotonic architectures.","authors":["Davide Sartor","Alberto Sinigaglia","Gian Antonio Susto"],"url":"https://arxiv.org/abs/2505.02537"}
{"created":"2025-05-06","title":"Marker-Based Extrinsic Calibration Method for Accurate Multi-Camera 3D Reconstruction","abstract":"Accurate 3D reconstruction using multi-camera RGB-D systems critically depends on precise extrinsic calibration to achieve proper alignment between captured views. In this paper, we introduce an iterative extrinsic calibration method that leverages the geometric constraints provided by a three-dimensional marker to significantly improve calibration accuracy. Our proposed approach systematically segments and refines marker planes through clustering, regression analysis, and iterative reassignment techniques, ensuring robust geometric correspondence across camera views. We validate our method comprehensively in both controlled environments and practical real-world settings within the Tech4Diet project, aimed at modeling the physical progression of patients undergoing nutritional treatments. Experimental results demonstrate substantial reductions in alignment errors, facilitating accurate and reliable 3D reconstructions.","authors":["Nahuel Garcia-D'Urso","Bernabe Sanchez-Sos","Jorge Azorin-Lopez","Andres Fuster-Guillo","Antonio Macia-Lillo","Higinio Mora-Mora"],"url":"https://arxiv.org/abs/2505.02539"}
{"created":"2025-05-06","title":"Lazy But Effective: Collaborative Personalized Federated Learning with Heterogeneous Data","abstract":"In Federated Learning, heterogeneity in client data distributions often means that a single global model does not have the best performance for individual clients. Consider for example training a next-word prediction model for keyboards: user-specific language patterns due to demographics (dialect, age, etc.), language proficiency, and writing style result in a highly non-IID dataset across clients. Other examples are medical images taken with different machines, or driving data from different vehicle types. To address this, we propose a simple yet effective personalized federated learning framework (pFedLIA) that utilizes a computationally efficient influence approximation, called `Lazy Influence', to cluster clients in a distributed manner before model aggregation. Within each cluster, data owners collaborate to jointly train a model that captures the specific data patterns of the clients. Our method has been shown to successfully recover the global model's performance drop due to the non-IID-ness in various synthetic and real-world settings, specifically a next-word prediction task on the Nordic languages as well as several benchmark tasks. It matches the performance of a hypothetical Oracle clustering, and significantly improves on existing baselines, e.g., an improvement of 17% on CIFAR100.","authors":["Ljubomir Rokvic","Panayiotis Danassis","Boi Faltings"],"url":"https://arxiv.org/abs/2505.02540"}
{"created":"2025-05-06","title":"\"Salt is the Soul of Hakka Baked Chicken\": Reimagining Traditional Chinese Culinary ICH for Modern Contexts Without Losing Tradition","abstract":"Intangible Cultural Heritage (ICH) like traditional culinary practices face increasing pressure to adapt to globalization while maintaining their cultural authenticity. Centuries-old traditions in Chinese cuisine are subject to rapid changes for adaptation to contemporary tastes and dietary preferences. The preservation of these cultural practices requires approaches that can enable ICH practitioners to reimagine and recreate ICH for modern contexts. To address this, we created workshops where experienced practitioners of traditional Chinese cuisine co-created recipes using GenAI tools and realized the dishes. We found that GenAI inspired ICH practitioners to innovate recipes based on traditional workflows for broader audiences and adapt to modern dining contexts. However, GenAI-inspired co-creation posed challenges in maintaining the accuracy of original ICH workflows and preserving traditional flavors in the culinary outcomes. This study offers implications for designing human-AI collaborative processes for safeguarding and enhancing culinary ICH.","authors":["Sijia Liu","XiaoKe Zeng","Fengyihan Wu","Shu Ye","Bowen Liu","Sidney Cheung","Richard William Allen","Ray Lc"],"url":"https://arxiv.org/abs/2505.02542"}
{"created":"2025-05-06","title":"Data-Driven Energy Modeling of Industrial IoT Systems: A Benchmarking Approach","abstract":"The widespread adoption of IoT has driven the development of cyber-physical systems (CPS) in industrial environments, leveraging Industrial IoTs (IIoTs) to automate manufacturing processes and enhance productivity. The transition to autonomous systems introduces significant operational costs, particularly in terms of energy consumption. Accurate modeling and prediction of IIoT energy requirements are critical, but traditional physics- and engineering-based approaches often fall short in addressing these challenges comprehensively. In this paper, we propose a novel methodology for benchmarking and analyzing IIoT devices and applications to uncover insights into their power demands, energy consumption, and performance. To demonstrate this methodology, we develop a comprehensive framework and apply it to study an industrial CPS comprising an educational robotic arm, a conveyor belt, a smart camera, and a compute node. By creating micro-benchmarks and an end-to-end application within this framework, we create an extensive performance and power consumption dataset, which we use to train and analyze ML models for predicting energy usage from features of the application and the CPS system. The proposed methodology and framework provide valuable insights into the energy dynamics of industrial CPS, offering practical implications for researchers and practitioners aiming to enhance the efficiency and sustainability of IIoT-driven automation.","authors":["Dimitris Kallis","Moysis Symeonides","Marios D. Dikaiakos"],"url":"https://arxiv.org/abs/2505.02543"}
{"created":"2025-05-06","title":"Robust Duality Learning for Unsupervised Visible-Infrared Person Re-Identfication","abstract":"Unsupervised visible-infrared person re-identification (UVI-ReID) aims to retrieve pedestrian images across different modalities without costly annotations, but faces challenges due to the modality gap and lack of supervision. Existing methods often adopt self-training with clustering-generated pseudo-labels but implicitly assume these labels are always correct. In practice, however, this assumption fails due to inevitable pseudo-label noise, which hinders model learning. To address this, we introduce a new learning paradigm that explicitly considers Pseudo-Label Noise (PLN), characterized by three key challenges: noise overfitting, error accumulation, and noisy cluster correspondence. To this end, we propose a novel Robust Duality Learning framework (RoDE) for UVI-ReID to mitigate the effects of noisy pseudo-labels. First, to combat noise overfitting, a Robust Adaptive Learning mechanism (RAL) is proposed to dynamically emphasize clean samples while down-weighting noisy ones. Second, to alleviate error accumulation-where the model reinforces its own mistakes-RoDE employs dual distinct models that are alternately trained using pseudo-labels from each other, encouraging diversity and preventing collapse. However, this dual-model strategy introduces misalignment between clusters across models and modalities, creating noisy cluster correspondence. To resolve this, we introduce Cluster Consistency Matching (CCM), which aligns clusters across models and modalities by measuring cross-cluster similarity. Extensive experiments on three benchmarks demonstrate the effectiveness of RoDE.","authors":["Yongxiang Li","Yuan Sun","Yang Qin","Dezhong Peng","Xi Peng","Peng Hu"],"url":"https://arxiv.org/abs/2505.02549"}
{"created":"2025-05-06","title":"Bielik v3 Small: Technical Report","abstract":"We introduce Bielik v3, a series of parameter-efficient generative text models (1.5B and 4.5B) optimized for Polish language processing. These models demonstrate that smaller, well-optimized architectures can achieve performance comparable to much larger counterparts while requiring substantially fewer computational resources. Our approach incorporates several key innovations: a custom Polish tokenizer (APT4) that significantly improves token efficiency, Weighted Instruction Cross-Entropy Loss to balance learning across instruction types, and Adaptive Learning Rate that dynamically adjusts based on training progress. Trained on a meticulously curated corpus of 292 billion tokens spanning 303 million documents, these models excel across multiple benchmarks, including the Open PL LLM Leaderboard, Complex Polish Text Understanding Benchmark, Polish EQ-Bench, and Polish Medical Leaderboard. The 4.5B parameter model achieves results competitive with models 2-3 times its size, while the 1.5B model delivers strong performance despite its extremely compact profile. These advances establish new benchmarks for parameter-efficient language modeling in less-represented languages, making high-quality Polish language AI more accessible for resource-constrained applications.","authors":["Krzysztof Ociepa","{\\L}ukasz Flis","Remigiusz Kinas","Krzysztof Wr\\'obel","Adrian Gwo\\'zdziej"],"url":"https://arxiv.org/abs/2505.02550"}
{"created":"2025-05-06","title":"The Turing Test Is More Relevant Than Ever","abstract":"The Turing Test, first proposed by Alan Turing in 1950, has historically served as a benchmark for evaluating artificial intelligence (AI). However, since the release of ELIZA in 1966, and particularly with recent advancements in large language models (LLMs), AI has been claimed to pass the Turing Test. Furthermore, criticism argues that the Turing Test primarily assesses deceptive mimicry rather than genuine intelligence, prompting the continuous emergence of alternative benchmarks. This study argues against discarding the Turing Test, proposing instead using more refined versions of it, for example, by interacting simultaneously with both an AI and human candidate to determine who is who, allowing a longer interaction duration, access to the Internet and other AIs, using experienced people as evaluators, etc.","authors":["Avraham Rahimov","Orel Zamler","Amos Azaria"],"url":"https://arxiv.org/abs/2505.02558"}
{"created":"2025-05-06","title":"Evaluating Contrastive Feedback for Effective User Simulations","abstract":"The use of Large Language Models (LLMs) for simulating user behavior in the domain of Interactive Information Retrieval has recently gained significant popularity. However, their application and capabilities remain highly debated and understudied. This study explores whether the underlying principles of contrastive training techniques, which have been effective for fine-tuning LLMs, can also be applied beneficially in the area of prompt engineering for user simulations.","authors":["Andreas Konstantin Kruff (TH K\\\"oln - University of Applied Sciences)","Timo Breuer (TH K\\\"oln - University of Applied Sciences)","Philipp Schaer (TH K\\\"oln - University of Applied Sciences)"],"url":"https://arxiv.org/abs/2505.02560"}
{"created":"2025-05-06","title":"Antifragility of RIS-assisted Communication Systems under Jamming Attacks","abstract":"Antifragility of communication systems is defined as measure of benefits gained from the adverse events and variability of its environment. In this paper, we introduce the notion of antifragility in Reconfigurable Intelligent Surface (RIS) assisted communication systems affected by a jamming attack. We analyzed the antifragility of the two hop systems, where the wireless path contains source node, RIS, destination node, and a eavesdropping/jamming node. We propose and analyze the antifragility performance for several jamming models, such as Digital Radio Frequency Memory (DRFM) and phase and amplitude shifting. Our paper shows that antifragility throughput can indeed be achieved under certain power thresholds and for various jamming models. In particular, high jamming power combined with low baseline data rates yields an antifragile gain factor of approximately five times. The results confirm that reconfigurable intelligent surfaces, when coupled with an antifragile design philosophy, can convert hostile interference from a liability into a throughput gain.","authors":["Mounir Bensalem","Thomas R\\\"othig","Admela Jukan"],"url":"https://arxiv.org/abs/2505.02565"}
{"created":"2025-05-06","title":"Robustness questions the interpretability of graph neural networks: what to do?","abstract":"Graph Neural Networks (GNNs) have become a cornerstone in graph-based data analysis, with applications in diverse domains such as bioinformatics, social networks, and recommendation systems. However, the interplay between model interpretability and robustness remains poorly understood, especially under adversarial scenarios like poisoning and evasion attacks. This paper presents a comprehensive benchmark to systematically analyze the impact of various factors on the interpretability of GNNs, including the influence of robustness-enhancing defense mechanisms.","authors":["Kirill Lukyanov (ISP RAS Research Center for Trusted Artificial Intelligence","Ivannikov Institute for System Programming of the Russian Academy of Sciences","Moscow Institute of Physics and Technology)","Georgii Sazonov (Ivannikov Institute for System Programming of the Russian Academy of Sciences","Lomonosov Moscow State University)","Serafim Boyarsky (Yandex School of Data Analysis)","Ilya Makarov (1 v 5)"],"url":"https://arxiv.org/abs/2505.02566"}
{"created":"2025-05-06","title":"Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities","abstract":"Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey will be available on GitHub soon.","authors":["Xinjie Zhang","Jintao Guo","Shanshan Zhao","Minghao Fu","Lunhao Duan","Guo-Hua Wang","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang"],"url":"https://arxiv.org/abs/2505.02567"}
{"created":"2025-05-06","title":"HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction","abstract":"This paper introduces HapticVLM, a novel multimodal system that integrates vision-language reasoning with deep convolutional networks to enable real-time haptic feedback. HapticVLM leverages a ConvNeXt-based material recognition module to generate robust visual embeddings for accurate identification of object materials, while a state-of-the-art Vision-Language Model (Qwen2-VL-2B-Instruct) infers ambient temperature from environmental cues. The system synthesizes tactile sensations by delivering vibrotactile feedback through speakers and thermal cues via a Peltier module, thereby bridging the gap between visual perception and tactile experience. Experimental evaluations demonstrate an average recognition accuracy of 84.67% across five distinct auditory-tactile patterns and a temperature estimation accuracy of 86.7% based on a tolerance-based evaluation method with an 8{\\deg}C margin of error across 15 scenarios. Although promising, the current study is limited by the use of a small set of prominent patterns and a modest participant pool. Future work will focus on expanding the range of tactile patterns and increasing user studies to further refine and validate the system's performance. Overall, HapticVLM presents a significant step toward context-aware, multimodal haptic interaction with potential applications in virtual reality, and assistive technologies.","authors":["Muhammad Haris Khan","Miguel Altamirano Cabrera","Dmitrii Iarchuk","Yara Mahmoud","Daria Trinitatova","Issatay Tokmurziyev","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2505.02569"}
{"created":"2025-05-06","title":"Rethinking Federated Graph Learning: A Data Condensation Perspective","abstract":"Federated graph learning is a widely recognized technique that promotes collaborative training of graph neural networks (GNNs) by multi-client graphs.However, existing approaches heavily rely on the communication of model parameters or gradients for federated optimization and fail to adequately address the data heterogeneity introduced by intricate and diverse graph distributions. Although some methods attempt to share additional messages among the server and clients to improve federated convergence during communication, they introduce significant privacy risks and increase communication overhead. To address these issues, we introduce the concept of a condensed graph as a novel optimization carrier to address FGL data heterogeneity and propose a new FGL paradigm called FedGM. Specifically, we utilize a generalized condensation graph consensus to aggregate comprehensive knowledge from distributed graphs, while minimizing communication costs and privacy risks through a single transmission of the condensed data. Extensive experiments on six public datasets consistently demonstrate the superiority of FedGM over state-of-the-art baselines, highlighting its potential for a novel FGL paradigm.","authors":["Hao Zhang","Xunkai Li","Yinlin Zhu","Lianglin Hu"],"url":"https://arxiv.org/abs/2505.02573"}
{"created":"2025-05-06","title":"Learning and Online Replication of Grasp Forces from Electromyography Signals for Prosthetic Finger Control","abstract":"Partial hand amputations significantly affect the physical and psychosocial well-being of individuals, yet intuitive control of externally powered prostheses remains an open challenge. To address this gap, we developed a force-controlled prosthetic finger activated by electromyography (EMG) signals. The prototype, constructed around a wrist brace, functions as a supernumerary finger placed near the index, allowing for early-stage evaluation on unimpaired subjects. A neural network-based model was then implemented to estimate fingertip forces from EMG inputs, allowing for online adjustment of the prosthetic finger grip strength. The force estimation model was validated through experiments with ten participants, demonstrating its effectiveness in predicting forces. Additionally, online trials with four users wearing the prosthesis exhibited precise control over the device. Our findings highlight the potential of using EMG-based force estimation to enhance the functionality of prosthetic fingers.","authors":["Robin Arbaud","Elisa Motta","Marco Domenico Avaro","Stefano Picinich","Marta Lorenzini","Arash Ajoudani"],"url":"https://arxiv.org/abs/2505.02574"}
{"created":"2025-05-06","title":"Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning","abstract":"Reasoning tasks are crucial in many domains, especially in science and engineering. Although large language models (LLMs) have made progress in reasoning tasks using techniques such as chain-of-thought and least-to-most prompting, these approaches still do not effectively scale to complex problems in either their performance or execution time. Moreover, they often require additional supervision for each new task, such as in-context examples. In this work, we introduce Recursive Decomposition with Dependencies (RDD), a scalable divide-and-conquer method for solving reasoning problems that requires less supervision than prior approaches. Our method can be directly applied to a new problem class even in the absence of any task-specific guidance. Furthermore, RDD supports sub-task dependencies, allowing for ordered execution of sub-tasks, as well as an error recovery mechanism that can correct mistakes made in previous steps. We evaluate our approach on two benchmarks with six difficulty levels each and in two in-context settings: one with task-specific examples and one without. Our results demonstrate that RDD outperforms other methods in a compute-matched setting as task complexity increases, while also being more computationally efficient.","authors":["Sergio Hern\\'andez-Guti\\'errez","Minttu Alakuijala","Alexander V. Nikitin","Pekka Marttinen"],"url":"https://arxiv.org/abs/2505.02576"}
{"created":"2025-05-06","title":"EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and Flexible LLM Fine-Tuning","abstract":"Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including complex objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the training to improve efficiency and flexibility. Our method is the first to aggregate the last hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text-scoring LLMs to evaluate the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\\pm 1,650$ data points and $6,573\\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.","authors":["Lingxiao Kong (Fraunhofer Institute for Applied Information Technology FIT)","Cong Yang (Soochow University)","Susanne Neufang (University Hospital of Cologne)","Oya Deniz Beyan (Fraunhofer Institute for Applied Information Technology FIT","University Hospital of Cologne)","Zeyd Boukhers (Fraunhofer Institute for Applied Information Technology FIT","University Hospital of Cologne)"],"url":"https://arxiv.org/abs/2505.02579"}
{"created":"2025-05-06","title":"Agentic Neurodivergence as a Contingent Solution to the AI Alignment Problem","abstract":"The AI alignment problem, which focusses on ensuring that artificial intelligence (AI), including AGI and ASI, systems act according to human values, presents profound challenges. With the progression from narrow AI to Artificial General Intelligence (AGI) and Superintelligence, fears about control and existential risk have escalated. This paper demonstrates that achieving complete alignment is inherently unattainable due to mathematical principles rooted in the foundations of predicate logic and computability, in particular Turing's computational universality, G\\\"odel's incompleteness and Chaitin's randomness. Instead, we argue that embracing AI misalignment or agent's `neurodivergence' as a contingent strategy, defined as fostering a dynamic ecosystem of competing, partially aligned agents, is a possible only viable path to mitigate risks. Through mathematical proofs and an experimental design, we explore how misalignment may serve and should be promoted as a counterbalancing mechanism to team up with whichever agents are most aligned AI to human values, ensuring that no single system dominates destructively. The main premise of our contribution is that misalignment is inevitable because full AI-human alignment is a mathematical impossibility from Turing-complete systems which we also prove in this paper, a feature then inherited to AGI and ASI systems. We introduce and test `change-of-opinion' attacks based on this kind of perturbation and intervention analysis to study how agents may neutralise friendly or unfriendly AIs through cooperation, competition or malice.","authors":["Alberto Hern\\'andez-Espinosa","Felipe S. Abrah\\~ao","Olaf Witkowski","Hector Zenil"],"url":"https://arxiv.org/abs/2505.02581"}
{"created":"2025-05-06","title":"FlyHaptics: Flying Multi-contact Haptic Interface","abstract":"This work presents FlyHaptics, an aerial haptic interface tracked via a Vicon optical motion capture system and built around six five-bar linkage assemblies enclosed in a lightweight protective cage. We predefined five static tactile patterns - each characterized by distinct combinations of linkage contact points and vibration intensities - and evaluated them in a grounded pilot study, where participants achieved 86.5 recognition accuracy (F(4, 35) = 1.47, p = 0.23) with no significant differences between patterns. Complementary flight demonstrations confirmed stable hover performance and consistent force output under realistic operating conditions. These pilot results validate the feasibility of drone-mounted, multi-contact haptic feedback and lay the groundwork for future integration into fully immersive VR, teleoperation, and remote interaction scenarios.","authors":["Luis Moreno","Miguel Altamirano Cabrera","Muhammad Haris Khan","Issatay Tokmurziyev","Yara Mahmoud","Valerii Serpiva","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2505.02582"}
{"created":"2025-05-06","title":"Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era","abstract":"The proliferation of edge devices has generated an unprecedented volume of time series data across different domains, motivating various well-customized methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm for time series analytics by leveraging the shared sequential nature of textual data and time series. However, a fundamental cross-modality gap between time series and LLMs exists, as LLMs are pre-trained on textual corpora and are not inherently optimized for time series. Many recent proposals are designed to address this issue. In this survey, we provide an up-to-date overview of LLMs-based cross-modality modeling for time series analytics. We first introduce a taxonomy that classifies existing approaches into four groups based on the type of textual data employed for time series modeling. We then summarize key cross-modality strategies, e.g., alignment and fusion, and discuss their applications across a range of downstream tasks. Furthermore, we conduct experiments on multimodal datasets from different application domains to investigate effective combinations of textual data and cross-modality strategies for enhancing time series analytics. Finally, we suggest several promising directions for future research. This survey is designed for a range of professionals, researchers, and practitioners interested in LLM-based time series modeling.","authors":["Chenxi Liu","Shaowen Zhou","Qianxiong Xu","Hao Miao","Cheng Long","Ziyue Li","Rui Zhao"],"url":"https://arxiv.org/abs/2505.02583"}
{"created":"2025-05-06","title":"RGBX-DiffusionDet: A Framework for Multi-Modal RGB-X Object Detection Using DiffusionDet","abstract":"This work introduces RGBX-DiffusionDet, an object detection framework extending the DiffusionDet model to fuse the heterogeneous 2D data (X) with RGB imagery via an adaptive multimodal encoder. To enable cross-modal interaction, we design the dynamic channel reduction within a convolutional block attention module (DCR-CBAM), which facilitates cross-talk between subnetworks by dynamically highlighting salient channel features. Furthermore, the dynamic multi-level aggregation block (DMLAB) is proposed to refine spatial feature representations through adaptive multiscale fusion. Finally, novel regularization losses that enforce channel saliency and spatial selectivity are introduced, leading to compact and discriminative feature embeddings. Extensive experiments using RGB-Depth (KITTI), a novel annotated RGB-Polarimetric dataset, and RGB-Infrared (M$^3$FD) benchmark dataset were conducted. We demonstrate consistent superiority of the proposed approach over the baseline RGB-only DiffusionDet. The modular architecture maintains the original decoding complexity, ensuring efficiency. These results establish the proposed RGBX-DiffusionDet as a flexible multimodal object detection approach, providing new insights into integrating diverse 2D sensing modalities into diffusion-based detection pipelines.","authors":["Eliraz Orfaig","Inna Stainvas","Igal Bilik"],"url":"https://arxiv.org/abs/2505.02586"}
{"created":"2025-05-06","title":"Ensemble Kalman filter for uncertainty in human language comprehension","abstract":"Artificial neural networks (ANNs) are widely used in modeling sentence processing but often exhibit deterministic behavior, contrasting with human sentence comprehension, which manages uncertainty during ambiguous or unexpected inputs. This is exemplified by reversal anomalies-sentences with unexpected role reversals that challenge syntax and semantics-highlighting the limitations of traditional ANN models, such as the Sentence Gestalt (SG) Model. To address these limitations, we propose a Bayesian framework for sentence comprehension, applying an extension of the ensemble Kalman filter (EnKF) for Bayesian inference to quantify uncertainty. By framing language comprehension as a Bayesian inverse problem, this approach enhances the SG model's ability to reflect human sentence processing with respect to the representation of uncertainty. Numerical experiments and comparisons with maximum likelihood estimation (MLE) demonstrate that Bayesian methods improve uncertainty representation, enabling the model to better approximate human cognitive processing when dealing with linguistic ambiguities.","authors":["Diksha Bhandari","Alessandro Lopopolo","Milena Rabovsky","Sebastian Reich"],"url":"https://arxiv.org/abs/2505.02590"}
{"created":"2025-05-06","title":"GarmentImage: Raster Encoding of Garment Sewing Patterns with Diverse Topologies","abstract":"Garment sewing patterns are the design language behind clothing, yet their current vector-based digital representations weren't built with machine learning in mind. Vector-based representation encodes a sewing pattern as a discrete set of panels, each defined as a sequence of lines and curves, stitching information between panels and the placement of each panel around a body. However, this representation causes two major challenges for neural networks: discontinuity in latent space between patterns with different topologies and limited generalization to garments with unseen topologies in the training data. In this work, we introduce GarmentImage, a unified raster-based sewing pattern representation. GarmentImage encodes a garment sewing pattern's geometry, topology and placement into multi-channel regular grids. Machine learning models trained on GarmentImage achieve seamless transitions between patterns with different topologies and show better generalization capabilities compared to models trained on vector-based representation. We demonstrate the effectiveness of GarmentImage across three applications: pattern exploration in latent space, text-based pattern editing, and image-to-pattern prediction. The results show that GarmentImage achieves superior performance on these applications using only simple convolutional networks.","authors":["Yuki Tatsukawa","Anran Qi","I-Chao Shen","Takeo Igarashi"],"url":"https://arxiv.org/abs/2505.02592"}
{"created":"2025-05-06","title":"DELTA: Dense Depth from Events and LiDAR using Transformer's Attention","abstract":"Event cameras and LiDARs provide complementary yet distinct data: respectively, asynchronous detections of changes in lighting versus sparse but accurate depth information at a fixed rate. To this day, few works have explored the combination of these two modalities. In this article, we propose a novel neural-network-based method for fusing event and LiDAR data in order to estimate dense depth maps. Our architecture, DELTA, exploits the concepts of self- and cross-attention to model the spatial and temporal relations within and between the event and LiDAR data. Following a thorough evaluation, we demonstrate that DELTA sets a new state of the art in the event-based depth estimation problem, and that it is able to reduce the errors up to four times for close ranges compared to the previous SOTA.","authors":["Vincent Brebion","Julien Moreau","Franck Davoine"],"url":"https://arxiv.org/abs/2505.02593"}
{"created":"2025-05-06","title":"Advances on the finite element discretization of fluid-structure interaction problems","abstract":"We review the main features of an unfitted finite element method for interface and fluid-structure interaction problems based on a distributed Lagrange multiplier in the spirit of the fictitious domain approach. We recall our theoretical findings concerning well-posedeness, stability, and convergence of the numerical schemes, and discuss the related computational challenges. In the case of elliptic interface problems, we also present a posteriori error estimates.","authors":["Najwa Alshehri","Daniele Boffi","Fabio Credali","Lucia Gastaldi"],"url":"https://arxiv.org/abs/2505.02594"}
{"created":"2025-05-06","title":"Spatiotemporal Non-Uniformity-Aware Online Task Scheduling in Collaborative Edge Computing for Industrial Internet of Things","abstract":"Mobile edge computing mitigates the shortcomings of cloud computing caused by unpredictable wide-area network latency and serves as a critical enabling technology for the Industrial Internet of Things (IIoT). Unlike cloud computing, mobile edge networks offer limited and distributed computing resources. As a result, collaborative edge computing emerges as a promising technology that enhances edge networks' service capabilities by integrating computational resources across edge nodes. This paper investigates the task scheduling problem in collaborative edge computing for IIoT, aiming to optimize task processing performance under long-term cost constraints. We propose an online task scheduling algorithm to cope with the spatiotemporal non-uniformity of user request distribution in distributed edge networks. For the spatial non-uniformity of user requests across different factories, we introduce a graph model to guide optimal task scheduling decisions. For the time-varying nature of user request distribution and long-term cost constraints, we apply Lyapunov optimization to decompose the long-term optimization problem into a series of real-time subproblems that do not require prior knowledge of future system states. Given the NP-hard nature of the subproblems, we design a heuristic-based hierarchical optimization approach incorporating enhanced discrete particle swarm and harmonic search algorithms. Finally, an imitation learning-based approach is devised to further accelerate the algorithm's operation, building upon the initial two algorithms. Comprehensive theoretical analysis and experimental evaluation demonstrate the effectiveness of the proposed schemes.","authors":["Yang Li","Xing Zhang","Yukun Sun","Wenbo Wang","Bo Lei"],"url":"https://arxiv.org/abs/2505.02597"}
{"created":"2025-05-06","title":"LiDAR-Inertial SLAM-Based Navigation and Safety-Oriented AI-Driven Control System for Skid-Steer Robots","abstract":"Integrating artificial intelligence (AI) and stochastic technologies into the mobile robot navigation and control (MRNC) framework while adhering to rigorous safety standards presents significant challenges. To address these challenges, this paper proposes a comprehensively integrated MRNC framework for skid-steer wheeled mobile robots (SSWMRs), in which all components are actively engaged in real-time execution. The framework comprises: 1) a LiDAR-inertial simultaneous localization and mapping (SLAM) algorithm for estimating the current pose of the robot within the built map; 2) an effective path-following control system for generating desired linear and angular velocity commands based on the current pose and the desired pose; 3) inverse kinematics for transferring linear and angular velocity commands into left and right side velocity commands; and 4) a robust AI-driven (RAID) control system incorporating a radial basis function network (RBFN) with a new adaptive algorithm to enforce in-wheel actuation systems to track each side motion commands. To further meet safety requirements, the proposed RAID control within the MRNC framework of the SSWMR constrains AI-generated tracking performance within predefined overshoot and steady-state error limits, while ensuring robustness and system stability by compensating for modeling errors, unknown RBF weights, and external forces. Experimental results verify the proposed MRNC framework performance for a 4,836 kg SSWMR operating on soft terrain.","authors":["Mehdi Heydari Shahna","Eemil Haaparanta","Pauli Mustalahti","Jouni Mattila"],"url":"https://arxiv.org/abs/2505.02598"}
{"created":"2025-05-06","title":"Maximal Compatibility Matching for Preference-Aware Ride-Hailing Systems","abstract":"This paper presents the Maximal Compatibility Matching (MCM) framework, a novel assignment strategy for ride-hailing systems that explicitly incorporates passenger comfort into the matching process. Traditional assignment methods prioritize spatial efficiency, but often overlook behavioral alignment between passengers and drivers, which can significantly impact user satisfaction. MCM addresses this gap by learning personalized passenger comfort zones using gradient-boosted decision tree classifiers trained on labeled ride data, and by modeling driver behavior through empirical operating profiles constructed from time-series driving features. Compatibility between a passenger and a driver is computed as the closed-form volume of intersection between their respective feature-space regions. These compatibility scores are integrated into a utility-based matching algorithm that balances comfort and proximity through a tunable trade-off parameter. We validate the framework using a Unity-based driving simulator with real-time passenger feedback, demonstrating that MCM enables more personalized and socially acceptable matchings while maintaining high levels of operational performance.","authors":["Avalpreet Singh Brar","Rong Su","Jaskaranveer Kaur","Xinling Li","Gioele Zardini"],"url":"https://arxiv.org/abs/2505.02599"}
{"created":"2025-05-06","title":"Wise Goose Chase: A Predictive Path Planning Algorithm for Dynamic Rebalancing in Ride-Hailing Systems","abstract":"Traditional rebalancing methods in ride-hailing systems direct idle drivers to fixed destinations, overlooking the fact that ride allocations frequently occur while cruising. This destination-centric view fails to exploit the path-dependent nature of modern platforms, where real-time matching depends on the entire trajectory rather than a static endpoint. We propose the Wise Goose Chase (WGC) algorithm, an event-triggered, driver-specific path planning framework that anticipates future matching opportunities by forecasting spatio-temporal supply and demand dynamics. WGC uses a system of Retarded Functional Differential Equations (RFDEs) to model the evolution of idle driver density and passenger queues at the road-segment level, incorporating both en-route matching and competition among drivers. Upon request, WGC computes personalized cruising paths that minimize each driver's expected time to allocation. Monte Carlo simulations on synthetic urban networks show that WGC consistently outperforms baseline strategies, highlighting the advantage of predictive, context-aware rebalancing in dynamic mobility systems.","authors":["Avalpreet Singh Brar","Rong Su","Christos G. Cassandras","Gioele Zardini"],"url":"https://arxiv.org/abs/2505.02603"}
{"created":"2025-05-06","title":"Low-Loss Space in Neural Networks is Continuous and Fully Connected","abstract":"Visualizations of the loss landscape in neural networks suggest that minima are isolated points. However, both theoretical and empirical studies indicate that it is possible to connect two different minima with a path consisting of intermediate points that also have low loss. In this study, we propose a new algorithm which investigates low-loss paths in the full parameter space, not only between two minima. Our experiments on LeNet5, ResNet18, and Compact Convolutional Transformer architectures consistently demonstrate the existence of such continuous paths in the parameter space. These results suggest that the low-loss region is a fully connected and continuous space in the parameter space. Our findings provide theoretical insight into neural network over-parameterization, highlighting that parameters collectively define a high-dimensional low-loss space, implying parameter redundancy exists only within individual models and not throughout the entire low-loss space. Additionally, our work also provides new visualization methods and opportunities to improve model generalization by exploring the low-loss space that is closer to the origin.","authors":["Yongding Tian","Zaid Al-Ars","Maksim Kitsak","Peter Hofstee"],"url":"https://arxiv.org/abs/2505.02604"}
{"created":"2025-05-06","title":"Data Compression for Time Series Modelling: A Case Study of Smart Grid Demand Forecasting","abstract":"Efficient time series forecasting is essential for smart energy systems, enabling accurate predictions of energy demand, renewable resource availability, and grid stability. However, the growing volume of high-frequency data from sensors and IoT devices poses challenges for storage and transmission. This study explores Discrete Wavelet Transform (DWT)-based data compression as a solution to these challenges while ensuring forecasting accuracy. A case study of a seawater supply system in Hirtshals, Denmark, operating under dynamic weather, operational schedules, and seasonal trends, is used for evaluation.","authors":["Mikkel Bue Lykkegaard","Svend Vendelbo Nielsen","Akanksha Upadhyay","Mikkel Bendixen Copeland","Philipp Tr\\'enell"],"url":"https://arxiv.org/abs/2505.02606"}
{"created":"2025-05-06","title":"Study of the influence of a biased database on the prediction of standard algorithms for selecting the best candidate for an interview","abstract":"Artificial intelligence is used at various stages of the recruitment process to automatically select the best candidate for a position, with companies guaranteeing unbiased recruitment. However, the algorithms used are either trained by humans or are based on learning from past experiences that were biased. In this article, we propose to generate data mimicking external (discrimination) and internal biases (self-censorship) in order to train five classic algorithms and to study the extent to which they do or do not find the best candidates according to objective criteria. In addition, we study the influence of the anonymisation of files on the quality of predictions.","authors":["Shuyu Wang","Ang\\'elique Saillet","Philom\\`ene Le Gall","Alain Lacroux","Christelle Martin-Lacroux","Vincent Brault"],"url":"https://arxiv.org/abs/2505.02609"}
{"created":"2025-05-06","title":"Automatic Proficiency Assessment in L2 English Learners","abstract":"Second language proficiency (L2) in English is usually perceptually evaluated by English teachers or expert evaluators, with the inherent intra- and inter-rater variability. This paper explores deep learning techniques for comprehensive L2 proficiency assessment, addressing both the speech signal and its correspondent transcription. We analyze spoken proficiency classification prediction using diverse architectures, including 2D CNN, frequency-based CNN, ResNet, and a pretrained wav2vec 2.0 model. Additionally, we examine text-based proficiency assessment by fine-tuning a BERT language model within resource constraints. Finally, we tackle the complex task of spontaneous dialogue assessment, managing long-form audio and speaker interactions through separate applications of wav2vec 2.0 and BERT models. Results from experiments on EFCamDat and ANGLISH datasets and a private dataset highlight the potential of deep learning, especially the pretrained wav2vec 2.0 model, for robust automated L2 proficiency evaluation.","authors":["Armita Mohammadi","Alessandro Lameiras Koerich","Laureano Moro-Velazquez","Patrick Cardinal"],"url":"https://arxiv.org/abs/2505.02615"}
{"created":"2025-05-06","title":"Mirror Mean-Field Langevin Dynamics","abstract":"The mean-field Langevin dynamics (MFLD) minimizes an entropy-regularized nonlinear convex functional on the Wasserstein space over $\\mathbb{R}^d$, and has gained attention recently as a model for the gradient descent dynamics of interacting particle systems such as infinite-width two-layer neural networks. However, many problems of interest have constrained domains, which are not solved by existing mean-field algorithms due to the global diffusion term. We study the optimization of probability measures constrained to a convex subset of $\\mathbb{R}^d$ by proposing the \\emph{mirror mean-field Langevin dynamics} (MMFLD), an extension of MFLD to the mirror Langevin framework. We obtain linear convergence guarantees for the continuous MMFLD via a uniform log-Sobolev inequality, and uniform-in-time propagation of chaos results for its time- and particle-discretized counterpart.","authors":["Anming Gu","Juno Kim"],"url":"https://arxiv.org/abs/2505.02621"}
{"created":"2025-05-06","title":"PLS-completeness of string permutations","abstract":"Bitstrings can be permuted via permutations and compared via the lexicographic order. In this paper we study the complexity of finding a minimum of a bitstring via given permutations. As a global optima is known to be NP-complete, we study the local optima via the class PLS and show hardness for PLS. Additionally, we show that even for one permutation the global optimization is NP-complete and give a formula that has these permutation as symmetries. This answers an open question inspired from Kolodziejczyk and Thapen and stated at the SAT and interactions seminar in Dagstuhl.","authors":["Dominik Scheder","Johannes Tantow"],"url":"https://arxiv.org/abs/2505.02622"}
{"created":"2025-05-06","title":"Stochastic Games with Limited Public Memory","abstract":"We study the memory resources required for near-optimal play in two-player zero-sum stochastic games with the long-run average payoff. Although optimal strategies may not exist in such games, near-optimal strategies always do.","authors":["Kristoffer Arnsfelt Hansen","Rasmus Ibsen-Jensen","Abraham Neyman"],"url":"https://arxiv.org/abs/2505.02623"}
{"created":"2025-05-06","title":"LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis","abstract":"Real-time, intelligent, and natural speech interaction is an essential part of the next-generation human-computer interaction. Recent advancements have showcased the potential of building intelligent spoken chatbots based on large language models (LLMs). In this paper, we introduce LLaMA-Omni 2, a series of speech language models (SpeechLMs) ranging from 0.5B to 14B parameters, capable of achieving high-quality real-time speech interaction. LLaMA-Omni 2 is built upon the Qwen2.5 series models, integrating a speech encoder and an autoregressive streaming speech decoder. Despite being trained on only 200K multi-turn speech dialogue samples, LLaMA-Omni 2 demonstrates strong performance on several spoken question answering and speech instruction following benchmarks, surpassing previous state-of-the-art SpeechLMs like GLM-4-Voice, which was trained on millions of hours of speech data.","authors":["Qingkai Fang","Yan Zhou","Shoutao Guo","Shaolei Zhang","Yang Feng"],"url":"https://arxiv.org/abs/2505.02625"}
{"created":"2025-05-06","title":"Detect, Classify, Act: Categorizing Industrial Anomalies with Multi-Modal Large Language Models","abstract":"Recent advances in visual industrial anomaly detection have demonstrated exceptional performance in identifying and segmenting anomalous regions while maintaining fast inference speeds. However, anomaly classification-distinguishing different types of anomalies-remains largely unexplored despite its critical importance in real-world inspection tasks. To address this gap, we propose VELM, a novel LLM-based pipeline for anomaly classification. Given the critical importance of inference speed, we first apply an unsupervised anomaly detection method as a vision expert to assess the normality of an observation. If an anomaly is detected, the LLM then classifies its type. A key challenge in developing and evaluating anomaly classification models is the lack of precise annotations of anomaly classes in existing datasets. To address this limitation, we introduce MVTec-AC and VisA-AC, refined versions of the widely used MVTec-AD and VisA datasets, which include accurate anomaly class labels for rigorous evaluation. Our approach achieves a state-of-the-art anomaly classification accuracy of 80.4% on MVTec-AD, exceeding the prior baselines by 5%, and 84% on MVTec-AC, demonstrating the effectiveness of VELM in understanding and categorizing anomalies. We hope our methodology and benchmark inspire further research in anomaly classification, helping bridge the gap between detection and comprehensive anomaly characterization.","authors":["Sassan Mokhtar","Arian Mousakhan","Silvio Galesso","Jawad Tayyub","Thomas Brox"],"url":"https://arxiv.org/abs/2505.02626"}
{"created":"2025-05-06","title":"A Theoretical Analysis of Compositional Generalization in Neural Networks: A Necessary and Sufficient Condition","abstract":"Compositional generalization is a crucial property in artificial intelligence, enabling models to handle novel combinations of known components. While most deep learning models lack this capability, certain models succeed in specific tasks, suggesting the existence of governing conditions. This paper derives a necessary and sufficient condition for compositional generalization in neural networks. Conceptually, it requires that (i) the computational graph matches the true compositional structure, and (ii) components encode just enough information in training. The condition is supported by mathematical proofs. This criterion combines aspects of architecture design, regularization, and training data properties. A carefully designed minimal example illustrates an intuitive understanding of the condition. We also discuss the potential of the condition for assessing compositional generalization before training. This work is a fundamental theoretical study of compositional generalization in neural networks.","authors":["Yuanpeng Li"],"url":"https://arxiv.org/abs/2505.02627"}
{"created":"2025-05-06","title":"Parameter-Efficient Fine-Tuning with Attributed Patch Semantic Graph for Automated Patch Correctness Assessment","abstract":"Automated program repair (APR) aims to automatically repair program errors without human intervention, and recent years have witnessed a growing interest on this research topic. While much progress has been made and techniques originating from different disciplines have been proposed, APR techniques generally suffer from the patch overfitting issue, i.e., the generated patches are not genuinely correct despite they pass the employed tests. To alleviate this issue, many research efforts have been devoted for automated patch correctness assessment (APCA). In particular, with the emergence of large language model (LLM) technology, researchers have employed LLM to assess the patch correctness and have obtained the state-of-the-art performance. The literature on APCA has demonstrated the importance of capturing patch semantic and explicitly considering certain code attributes in predicting patch correctness. However, existing LLM-based methods typically treat code as token sequences and ignore the inherent formal structure for code, making it difficult to capture the deep patch semantics. Moreover, these LLM-based methods also do not explicitly account for enough code attributes. To overcome these drawbacks, we in this paper design a novel patch graph representation named attributed patch semantic graph (APSG), which adequately captures the patch semantic and explicitly reflects important patch attributes. To effectively use graph information in APSG, we accordingly propose a new parameter-efficient fine-tuning (PEFT) method of LLMs named Graph-LoRA. Extensive evaluations have been conducted to evaluate our method, and the results show that compared to the state-of-the-art methods, our method improves accuracy and F1 score by 2.3% to 6.6% and 1.8% to 6.1% respectively.","authors":["Zhenyu Yang","Jingwen Wu","Zhen Yang","Zhongxing Yu"],"url":"https://arxiv.org/abs/2505.02629"}
{"created":"2025-05-06","title":"Aerodynamic and structural airfoil shape optimisation via Transfer Learning-enhanced Deep Reinforcement Learning","abstract":"The main objective of this paper is to introduce a transfer learning-enhanced, multi-objective, deep reinforcement learning (DRL) methodology that is able to optimise the geometry of any airfoil based on concomitant aerodynamic and structural criteria. To showcase the method, we aim to maximise the lift-to-drag ratio $C_L/C_D$ while preserving the structural integrity of the airfoil -- as modelled by its maximum thickness -- and train the DRL agent using a list of different transfer learning (TL) strategies. The performance of the DRL agent is compared with Particle Swarm Optimisation (PSO), a traditional gradient-free optimisation method. Results indicate that DRL agents are able to perform multi-objective shape optimisation, that the DRL approach outperforms PSO in terms of computational efficiency and shape optimisation performance, and that the TL-enhanced DRL agent achieves performance comparable to the DRL one, while further saving substantial computational resources.","authors":["David Ramos","Lucas Lacasa","Eusebio Valero","Gonzalo Rubio"],"url":"https://arxiv.org/abs/2505.02634"}
{"created":"2025-05-06","title":"Enhancing Chemical Reaction and Retrosynthesis Prediction with Large Language Model and Dual-task Learning","abstract":"Chemical reaction and retrosynthesis prediction are fundamental tasks in drug discovery. Recently, large language models (LLMs) have shown potential in many domains. However, directly applying LLMs to these tasks faces two major challenges: (i) lacking a large-scale chemical synthesis-related instruction dataset; (ii) ignoring the close correlation between reaction and retrosynthesis prediction for the existing fine-tuning strategies. To address these challenges, we propose ChemDual, a novel LLM framework for accurate chemical synthesis. Specifically, considering the high cost of data acquisition for reaction and retrosynthesis, ChemDual regards the reaction-and-retrosynthesis of molecules as a related recombination-and-fragmentation process and constructs a large-scale of 4.4 million instruction dataset. Furthermore, ChemDual introduces an enhanced LLaMA, equipped with a multi-scale tokenizer and dual-task learning strategy, to jointly optimize the process of recombination and fragmentation as well as the tasks between reaction and retrosynthesis prediction. Extensive experiments on Mol-Instruction and USPTO-50K datasets demonstrate that ChemDual achieves state-of-the-art performance in both predictions of reaction and retrosynthesis, outperforming the existing conventional single-task approaches and the general open-source LLMs. Through molecular docking analysis, ChemDual generates compounds with diverse and strong protein binding affinity, further highlighting its strong potential in drug design.","authors":["Xuan Lin","Qingrui Liu","Hongxin Xiang","Daojian Zeng","Xiangxiang Zeng"],"url":"https://arxiv.org/abs/2505.02639"}
{"created":"2025-05-06","title":"Adaptive Budgeted Multi-Armed Bandits for IoT with Dynamic Resource Constraints","abstract":"Internet of Things (IoT) systems increasingly operate in environments where devices must respond in real time while managing fluctuating resource constraints, including energy and bandwidth. Yet, current approaches often fall short in addressing scenarios where operational constraints evolve over time. To address these limitations, we propose a novel Budgeted Multi-Armed Bandit framework tailored for IoT applications with dynamic operational limits. Our model introduces a decaying violation budget, which permits limited constraint violations early in the learning process and gradually enforces stricter compliance over time. We present the Budgeted Upper Confidence Bound (UCB) algorithm, which adaptively balances performance optimization and compliance with time-varying constraints. We provide theoretical guarantees showing that Budgeted UCB achieves sublinear regret and logarithmic constraint violations over the learning horizon. Extensive simulations in a wireless communication setting show that our approach achieves faster adaptation and better constraint satisfaction than standard online learning methods. These results highlight the framework's potential for building adaptive, resource-aware IoT systems.","authors":["Shubham Vaishnav","Praveen Kumar Donta","Sindri Magn\\'usson"],"url":"https://arxiv.org/abs/2505.02640"}
{"created":"2025-05-06","title":"Tight Bounds on Channel Reliability via Generalized Quorum Systems (Extended Version)","abstract":"Communication channel failures are a major concern for the developers of modern fault-tolerant systems. However, while tight bounds for process failures are well-established, extending them to include channel failures has remained an open problem. We introduce \\emph{generalized quorum systems} - a framework that characterizes the necessary and sufficient conditions for implementing atomic registers, atomic snapshots, lattice agreement and consensus under arbitrary patterns of process-channel failures. Generalized quorum systems relax the connectivity constraints of classical quorum systems: instead of requiring bidirectional reachability for every pair of write and read quorums, they only require some write quorum to be \\emph{unidirectionally} reachable from some read quorum. This weak connectivity makes implementing registers particularly challenging, because it precludes the traditional request/response pattern of quorum access, making classical solutions like ABD inapplicable. To address this, we introduce novel logical clocks that allow write and read quorums to reliably track state updates without relying on bi-directional connectivity.","authors":["Alejandro Naser-Pastoriza","Gregory Chockler","Alexey Gotsman","Fedor Ryabinin"],"url":"https://arxiv.org/abs/2505.02646"}
{"created":"2025-05-06","title":"MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation","abstract":"Diffusion models have shown excellent performance in text-to-image generation. Nevertheless, existing methods often suffer from performance bottlenecks when handling complex prompts that involve multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration-based scene parsing module that generates an agent system comprising multiple agents with distinct tasks, utilizing MLLMs to extract various scene elements effectively. In addition, Hierarchical Compositional diffusion utilizes a Gaussian mask and filtering to refine bounding box regions and enhance objects through region enhancement, resulting in the accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, providing a substantial advantage in complex scene generation.","authors":["Mingcheng Li","Xiaolu Hou","Ziyang Liu","Dingkang Yang","Ziyun Qian","Jiawei Chen","Jinjie Wei","Yue Jiang","Qingyao Xu","Lihua Zhang"],"url":"https://arxiv.org/abs/2505.02648"}
{"created":"2025-05-06","title":"Eye Movements as Indicators of Deception: A Machine Learning Approach","abstract":"Gaze may enhance the robustness of lie detectors but remains under-studied. This study evaluated the efficacy of AI models (using fixations, saccades, blinks, and pupil size) for detecting deception in Concealed Information Tests across two datasets. The first, collected with Eyelink 1000, contains gaze data from a computerized experiment where 87 participants revealed, concealed, or faked the value of a previously selected card. The second, collected with Pupil Neon, involved 36 participants performing a similar task but facing an experimenter. XGBoost achieved accuracies up to 74% in a binary classification task (Revealing vs. Concealing) and 49% in a more challenging three-classification task (Revealing vs. Concealing vs. Faking). Feature analysis identified saccade number, duration, amplitude, and maximum pupil size as the most important for deception prediction. These results demonstrate the feasibility of using gaze and AI to enhance lie detectors and encourage future research that may improve on this.","authors":["Valentin Foucher","Santiago de Leon-Martinez","Robert Moro"],"url":"https://arxiv.org/abs/2505.02649"}
{"created":"2025-05-06","title":"Open Challenges for a Production-ready Cloud Environment on top of RISC-V hardware","abstract":"As part of the Vitamin-V European project, we have built a prototype of a RISC-V cluster managed by OpenStack, with the goal of realizing a functional RISC-V cloud ecosystem. In this poster we explain the hardware and software challenges encountered while porting some elements of OpenStack. We also discuss the current performance gaps that challenge a performance-ready cloud environment over such new ISA, an essential element to fulfill in order to achieve european technological sovereignty.","authors":["Aaron Call","Ramon Nou","Guillem Senabre"],"url":"https://arxiv.org/abs/2505.02650"}
{"created":"2025-05-06","title":"Sim2Real in endoscopy segmentation with a novel structure aware image translation","abstract":"Automatic segmentation of anatomical landmarks in endoscopic images can provide assistance to doctors and surgeons for diagnosis, treatments or medical training. However, obtaining the annotations required to train commonly used supervised learning methods is a tedious and difficult task, in particular for real images. While ground truth annotations are easier to obtain for synthetic data, models trained on such data often do not generalize well to real data. Generative approaches can add realistic texture to it, but face difficulties to maintain the structure of the original scene. The main contribution in this work is a novel image translation model that adds realistic texture to simulated endoscopic images while keeping the key scene layout information. Our approach produces realistic images in different endoscopy scenarios. We demonstrate these images can effectively be used to successfully train a model for a challenging end task without any real labeled data. In particular, we demonstrate our approach for the task of fold segmentation in colonoscopy images. Folds are key anatomical landmarks that can occlude parts of the colon mucosa and possible polyps. Our approach generates realistic images maintaining the shape and location of the original folds, after the image-style-translation, better than existing methods. We run experiments both on a novel simulated dataset for fold segmentation, and real data from the EndoMapper (EM) dataset. All our new generated data and new EM metadata is being released to facilitate further research, as no public benchmark is currently available for the task of fold segmentation.","authors":["Clara Tomasini","Luis Riazuelo","Ana C. Murillo"],"url":"https://arxiv.org/abs/2505.02654"}
{"created":"2025-05-06","title":"SCFormer: Structured Channel-wise Transformer with Cumulative Historical State for Multivariate Time Series Forecasting","abstract":"The Transformer model has shown strong performance in multivariate time series forecasting by leveraging channel-wise self-attention. However, this approach lacks temporal constraints when computing temporal features and does not utilize cumulative historical series effectively.To address these limitations, we propose the Structured Channel-wise Transformer with Cumulative Historical state (SCFormer). SCFormer introduces temporal constraints to all linear transformations, including the query, key, and value matrices, as well as the fully connected layers within the Transformer. Additionally, SCFormer employs High-order Polynomial Projection Operators (HiPPO) to deal with cumulative historical time series, allowing the model to incorporate information beyond the look-back window during prediction. Extensive experiments on multiple real-world datasets demonstrate that SCFormer significantly outperforms mainstream baselines, highlighting its effectiveness in enhancing time series forecasting. The code is publicly available at https://github.com/ShiweiGuo1995/SCFormer","authors":["Shiwei Guo","Ziang Chen","Yupeng Ma","Yunfei Han","Yi Wang"],"url":"https://arxiv.org/abs/2505.02655"}
{"created":"2025-05-06","title":"Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset","abstract":"Proper names in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP,their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper names of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper name diacritization.","authors":["Rawan Bondok","Mayar Nassar","Salam Khalifa","Kurt Micallaf","Nizar Habash"],"url":"https://arxiv.org/abs/2505.02656"}
{"created":"2025-05-06","title":"A Note on Statistically Accurate Tabular Data Generation Using Large Language Models","abstract":"Large language models (LLMs) have shown promise in synthetic tabular data generation, yet existing methods struggle to preserve complex feature dependencies, particularly among categorical variables. This work introduces a probability-driven prompting approach that leverages LLMs to estimate conditional distributions, enabling more accurate and scalable data synthesis. The results highlight the potential of prompting probobility distributions to enhance the statistical fidelity of LLM-generated tabular data.","authors":["Andrey Sidorenko"],"url":"https://arxiv.org/abs/2505.02659"}
{"created":"2025-05-06","title":"Grasp the Graph (GtG) 2.0: Ensemble of GNNs for High-Precision Grasp Pose Detection in Clutter","abstract":"Grasp pose detection in cluttered, real-world environments remains a significant challenge due to noisy and incomplete sensory data combined with complex object geometries. This paper introduces Grasp the Graph 2.0 (GtG 2.0) method, a lightweight yet highly effective hypothesis-and-test robotics grasping framework which leverages an ensemble of Graph Neural Networks for efficient geometric reasoning from point cloud data. Building on the success of GtG 1.0, which demonstrated the potential of Graph Neural Networks for grasp detection but was limited by assumptions of complete, noise-free point clouds and 4-Dof grasping, GtG 2.0 employs a conventional Grasp Pose Generator to efficiently produce 7-Dof grasp candidates. Candidates are assessed with an ensemble Graph Neural Network model which includes points within the gripper jaws (inside points) and surrounding contextual points (outside points). This improved representation boosts grasp detection performance over previous methods using the same generator. GtG 2.0 shows up to a 35% improvement in Average Precision on the GraspNet-1Billion benchmark compared to hypothesis-and-test and Graph Neural Network-based methods, ranking it among the top three frameworks. Experiments with a 3-Dof Delta Parallel robot and Kinect-v1 camera show a success rate of 91% and a clutter completion rate of 100%, demonstrating its flexibility and reliability.","authors":["Ali Rashidi Moghadam","Sayedmohammadreza Rastegari","Mehdi Tale Masouleh","Ahmad Kalhor"],"url":"https://arxiv.org/abs/2505.02664"}
{"created":"2025-05-06","title":"A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law","abstract":"This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems.","authors":["Qianjun Pan","Wenkai Ji","Yuyang Ding","Junsong Li","Shilian Chen","Junyi Wang","Jie Zhou","Qin Chen","Min Zhang","Yulan Wu","Liang He"],"url":"https://arxiv.org/abs/2505.02665"}
{"created":"2025-05-06","title":"A Survey on Progress in LLM Alignment from the Perspective of Reward Design","abstract":"The alignment of large language models (LLMs) with human values and intentions represents a core challenge in current AI research, where reward mechanism design has become a critical factor in shaping model behavior. This study conducts a comprehensive investigation of reward mechanisms in LLM alignment through a systematic theoretical framework, categorizing their development into three key phases: (1) feedback (diagnosis), (2) reward design (prescription), and (3) optimization (treatment). Through a four-dimensional analysis encompassing construction basis, format, expression, and granularity, this research establishes a systematic classification framework that reveals evolutionary trends in reward modeling. The field of LLM alignment faces several persistent challenges, while recent advances in reward design are driving significant paradigm shifts. Notable developments include the transition from reinforcement learning-based frameworks to novel optimization paradigms, as well as enhanced capabilities to address complex alignment scenarios involving multimodal integration and concurrent task coordination. Finally, this survey outlines promising future research directions for LLM alignment through innovative reward design strategies.","authors":["Miaomiao Ji","Yanqiu Wu","Zhibin Wu","Shoujin Wang","Jian Yang","Mark Dras","Usman Naseem"],"url":"https://arxiv.org/abs/2505.02666"}
{"created":"2025-05-06","title":"Online Phase Estimation of Human Oscillatory Motions using Deep Learning","abstract":"Accurately estimating the phase of oscillatory systems is essential for analyzing cyclic activities such as repetitive gestures in human motion. In this work we introduce a learning-based approach for online phase estimation in three-dimensional motion trajectories, using a Long Short- Term Memory (LSTM) network. A calibration procedure is applied to standardize trajectory position and orientation, ensuring invariance to spatial variations. The proposed model is evaluated on motion capture data and further tested in a dynamical system, where the estimated phase is used as input to a reinforcement learning (RL)-based control to assess its impact on the synchronization of a network of Kuramoto oscillators.","authors":["Antonio Grotta","Francesco De Lellis"],"url":"https://arxiv.org/abs/2505.02668"}
{"created":"2025-05-06","title":"Multimodal Deep Learning for Stroke Prediction and Detection using Retinal Imaging and Clinical Data","abstract":"Stroke is a major public health problem, affecting millions worldwide. Deep learning has recently demonstrated promise for enhancing the diagnosis and risk prediction of stroke. However, existing methods rely on costly medical imaging modalities, such as computed tomography. Recent studies suggest that retinal imaging could offer a cost-effective alternative for cerebrovascular health assessment due to the shared clinical pathways between the retina and the brain. Hence, this study explores the impact of leveraging retinal images and clinical data for stroke detection and risk prediction. We propose a multimodal deep neural network that processes Optical Coherence Tomography (OCT) and infrared reflectance retinal scans, combined with clinical data, such as demographics, vital signs, and diagnosis codes. We pretrained our model using a self-supervised learning framework using a real-world dataset consisting of $37$ k scans, and then fine-tuned and evaluated the model using a smaller labeled subset. Our empirical findings establish the predictive ability of the considered modalities in detecting lasting effects in the retina associated with acute stroke and forecasting future risk within a specific time horizon. The experimental results demonstrate the effectiveness of our proposed framework by achieving $5$\\% AUROC improvement as compared to the unimodal image-only baseline, and $8$\\% improvement compared to an existing state-of-the-art foundation model. In conclusion, our study highlights the potential of retinal imaging in identifying high-risk patients and improving long-term outcomes.","authors":["Saeed Shurrab","Aadim Nepal","Terrence J. Lee-St. John","Nicola G. Ghazi","Bartlomiej Piechowski-Jozwiak","Farah E. Shamout"],"url":"https://arxiv.org/abs/2505.02677"}
{"created":"2025-05-06","title":"Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models","abstract":"Recent developments in Large Language Models (LLMs) have shifted from pre-training scaling to post-training and test-time scaling. Across these developments, a key unified paradigm has arisen: Learning from Rewards, where reward signals act as the guiding stars to steer LLM behavior. It has underpinned a wide range of prevalent techniques, such as reinforcement learning (in RLHF, DPO, and GRPO), reward-guided decoding, and post-hoc correction. Crucially, this paradigm enables the transition from passive learning from static data to active learning from dynamic feedback. This endows LLMs with aligned preferences and deep reasoning capabilities. In this survey, we present a comprehensive overview of the paradigm of learning from rewards. We categorize and analyze the strategies under this paradigm across training, inference, and post-inference stages. We further discuss the benchmarks for reward models and the primary applications. Finally we highlight the challenges and future directions. We maintain a paper collection at https://github.com/bobxwu/learning-from-rewards-llm-papers.","authors":["Xiaobao Wu"],"url":"https://arxiv.org/abs/2505.02686"}
{"created":"2025-05-06","title":"Dance of Fireworks: An Interactive Broadcast Gymnastics Training System Based on Pose Estimation","abstract":"This study introduces Dance of Fireworks, an interactive system designed to combat sedentary health risks by enhancing engagement in radio calisthenics. Leveraging mobile device cameras and lightweight pose estimation (PoseNet/TensorFlow Lite), the system extracts body keypoints, computes joint angles, and compares them with standardized motions to deliver real-time corrective feedback. To incentivize participation, it dynamically maps users' movements (such as joint angles and velocity) to customizable fireworks animations, rewarding improved accuracy with richer visual effects.","authors":["Haotian Chen","Ziyu Liu","Xi Cheng","Chuangqi Li"],"url":"https://arxiv.org/abs/2505.02690"}
{"created":"2025-05-06","title":"fastabx: A library for efficient computation of ABX discriminability","abstract":"We introduce fastabx, a high-performance Python library for building ABX discrimination tasks. ABX is a measure of the separation between generic categories of interest. It has been used extensively to evaluate phonetic discriminability in self-supervised speech representations. However, its broader adoption has been limited by the absence of adequate tools. fastabx addresses this gap by providing a framework capable of constructing any type of ABX task while delivering the efficiency necessary for rapid development cycles, both in task creation and in calculating distances between representations. We believe that fastabx will serve as a valuable resource for the broader representation learning community, enabling researchers to systematically investigate what information can be directly extracted from learned representations across several domains beyond speech processing. The source code is available at https://github.com/bootphon/fastabx.","authors":["Maxime Poli","Emmanuel Chemla","Emmanuel Dupoux"],"url":"https://arxiv.org/abs/2505.02692"}
{"created":"2025-05-06","title":"Predicting Movie Hits Before They Happen with LLMs","abstract":"Addressing the cold-start issue in content recommendation remains a critical ongoing challenge. In this work, we focus on tackling the cold-start problem for movies on a large entertainment platform. Our primary goal is to forecast the popularity of cold-start movies using Large Language Models (LLMs) leveraging movie metadata. This method could be integrated into retrieval systems within the personalization pipeline or could be adopted as a tool for editorial teams to ensure fair promotion of potentially overlooked movies that may be missed by traditional or algorithmic solutions. Our study validates the effectiveness of this approach compared to established baselines and those we developed.","authors":["Shaghayegh Agah","Yejin Kim","Neeraj Sharma","Mayur Nankani","Kevin Foley","H. Howie Huang","Sardar Hamidian"],"url":"https://arxiv.org/abs/2505.02693"}
{"created":"2025-05-06","title":"AI Standardized Patient Improves Human Conversations in Advanced Cancer Care","abstract":"Serious illness communication (SIC) in end-of-life care faces challenges such as emotional stress, cultural barriers, and balancing hope with honesty. Despite its importance, one of the few available ways for clinicians to practice SIC is with standardized patients, which is expensive, time-consuming, and inflexible. In this paper, we present SOPHIE, an AI-powered standardized patient simulation and automated feedback system. SOPHIE combines large language models (LLMs), a lifelike virtual avatar, and automated, personalized feedback based on clinical literature to provide remote, on-demand SIC training. In a randomized control study with healthcare students and professionals, SOPHIE users demonstrated significant improvement across three critical SIC domains: Empathize, Be Explicit, and Empower. These results suggest that AI-driven tools can enhance complex interpersonal communication skills, offering scalable, accessible solutions to address a critical gap in clinician education.","authors":["Kurtis Haut","Masum Hasan","Thomas Carroll","Ronald Epstein","Taylan Sen","Ehsan Hoque"],"url":"https://arxiv.org/abs/2505.02694"}
{"created":"2025-05-06","title":"Exploring LLM-Powered Role and Action-Switching Pedagogical Agents for History Education in Virtual Reality","abstract":"Multi-role pedagogical agents can create engaging and immersive learning experiences, helping learners better understand knowledge in history learning. However, existing pedagogical agents often struggle with multi-role interactions due to complex controls, limited feedback forms, and difficulty dynamically adapting to user inputs. In this study, we developed a VR prototype with LLM-powered adaptive role-switching and action-switching pedagogical agents to help users learn about the history of the Pavilion of Prince Teng. A 2 x 2 between-subjects study was conducted with 84 participants to assess how adaptive role-switching and action-switching affect participants' learning outcomes and experiences. The results suggest that adaptive role-switching enhances participants' perception of the pedagogical agent's trustworthiness and expertise but may lead to inconsistent learning experiences. Adaptive action-switching increases participants' perceived social presence, expertise, and humanness. The study did not uncover any effects of role-switching and action-switching on usability, learning motivation, and cognitive load. Based on the findings, we proposed five design implications for incorporating adaptive role-switching and action-switching into future VR history education tools.","authors":["Zihao Zhu","Ao Yu","Xin Tong","Pan Hui"],"url":"https://arxiv.org/abs/2505.02699"}
{"created":"2025-05-06","title":"Structure Causal Models and LLMs Integration in Medical Visual Question Answering","abstract":"Medical Visual Question Answering (MedVQA) aims to answer medical questions according to medical images. However, the complexity of medical data leads to confounders that are difficult to observe, so bias between images and questions is inevitable. Such cross-modal bias makes it challenging to infer medically meaningful answers. In this work, we propose a causal inference framework for the MedVQA task, which effectively eliminates the relative confounding effect between the image and the question to ensure the precision of the question-answering (QA) session. We are the first to introduce a novel causal graph structure that represents the interaction between visual and textual elements, explicitly capturing how different questions influence visual features. During optimization, we apply the mutual information to discover spurious correlations and propose a multi-variable resampling front-door adjustment method to eliminate the relative confounding effect, which aims to align features based on their true causal relevance to the question-answering task. In addition, we also introduce a prompt strategy that combines multiple prompt forms to improve the model's ability to understand complex medical data and answer accurately. Extensive experiments on three MedVQA datasets demonstrate that 1) our method significantly improves the accuracy of MedVQA, and 2) our method achieves true causal correlations in the face of complex medical data.","authors":["Zibo Xu","Qiang Li","Weizhi Nie","Weijie Wang","Anan Liu"],"url":"https://arxiv.org/abs/2505.02703"}
{"created":"2025-05-06","title":"Visually-Guided Linguistic Disambiguation for Monocular Depth Scale Recovery","abstract":"We propose a robust method for monocular depth scale recovery. Monocular depth estimation can be divided into two main directions: (1) relative depth estimation, which provides normalized or inverse depth without scale information, and (2) metric depth estimation, which involves recovering depth with absolute scale. To obtain absolute scale information for practical downstream tasks, utilizing textual information to recover the scale of a relative depth map is a highly promising approach. However, since a single image can have multiple descriptions from different perspectives or with varying styles, it has been shown that different textual descriptions can significantly affect the scale recovery process. To address this issue, our method, VGLD, stabilizes the influence of textual information by incorporating high-level semantic information from the corresponding image alongside the textual description. This approach resolves textual ambiguities and robustly outputs a set of linear transformation parameters (scalars) that can be globally applied to the relative depth map, ultimately generating depth predictions with metric-scale accuracy. We validate our method across several popular relative depth models(MiDas, DepthAnything), using both indoor scenes (NYUv2) and outdoor scenes (KITTI). Our results demonstrate that VGLD functions as a universal alignment module when trained on multiple datasets, achieving strong performance even in zero-shot scenarios. Code is available at: https://github.com/pakinwu/VGLD.","authors":["Bojin Wu","Jing Chen"],"url":"https://arxiv.org/abs/2505.02704"}
{"created":"2025-05-06","title":"Voila: Voice-Language Foundation Models for Real-Time Autonomous Interaction and Voice Role-Play","abstract":"A voice AI agent that blends seamlessly into daily life would interact with humans in an autonomous, real-time, and emotionally expressive manner. Rather than merely reacting to commands, it would continuously listen, reason, and respond proactively, fostering fluid, dynamic, and emotionally resonant interactions. We introduce Voila, a family of large voice-language foundation models that make a step towards this vision. Voila moves beyond traditional pipeline systems by adopting a new end-to-end architecture that enables full-duplex, low-latency conversations while preserving rich vocal nuances such as tone, rhythm, and emotion. It achieves a response latency of just 195 milliseconds, surpassing the average human response time. Its hierarchical multi-scale Transformer integrates the reasoning capabilities of large language models (LLMs) with powerful acoustic modeling, enabling natural, persona-aware voice generation -- where users can simply write text instructions to define the speaker's identity, tone, and other characteristics. Moreover, Voila supports over one million pre-built voices and efficient customization of new ones from brief audio samples as short as 10 seconds. Beyond spoken dialogue, Voila is designed as a unified model for a wide range of voice-based applications, including automatic speech recognition (ASR), Text-to-Speech (TTS), and, with minimal adaptation, multilingual speech translation. Voila is fully open-sourced to support open research and accelerate progress toward next-generation human-machine interactions.","authors":["Yemin Shi","Yu Shu","Siwei Dong","Guangyi Liu","Jaward Sesay","Jingwen Li","Zhiting Hu"],"url":"https://arxiv.org/abs/2505.02707"}
{"created":"2025-05-06","title":"Technical Report: Evaluating Goal Drift in Language Model Agents","abstract":"As language models (LMs) are increasingly deployed as autonomous agents, their robust adherence to human-assigned objectives becomes crucial for safe operation. When these agents operate independently for extended periods without human oversight, even initially well-specified goals may gradually shift. Detecting and measuring goal drift - an agent's tendency to deviate from its original objective over time - presents significant challenges, as goals can shift gradually, causing only subtle behavioral changes. This paper proposes a novel approach to analyzing goal drift in LM agents. In our experiments, agents are first explicitly given a goal through their system prompt, then exposed to competing objectives through environmental pressures. We demonstrate that while the best-performing agent (a scaffolded version of Claude 3.5 Sonnet) maintains nearly perfect goal adherence for more than 100,000 tokens in our most difficult evaluation setting, all evaluated models exhibit some degree of goal drift. We also find that goal drift correlates with models' increasing susceptibility to pattern-matching behaviors as the context length grows.","authors":["Rauno Arike","Elizabeth Donoway","Henning Bartsch","Marius Hobbhahn"],"url":"https://arxiv.org/abs/2505.02709"}
{"created":"2025-05-06","title":"Graph Neural Network-Based Reinforcement Learning for Controlling Biological Networks: The GATTACA Framework","abstract":"Cellular reprogramming, the artificial transformation of one cell type into another, has been attracting increasing research attention due to its therapeutic potential for complex diseases. However, discovering reprogramming strategies through classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we explore the use of deep reinforcement learning (DRL) to control Boolean network models of complex biological systems, such as gene regulatory networks and signalling pathway networks. We formulate a novel control problem for Boolean network models under the asynchronous update mode in the context of cellular reprogramming. To facilitate scalability, we consider our previously introduced concept of a pseudo-attractor and we improve our procedure for effective identification of pseudo-attractor states. Finally, we devise a computational framework to solve the control problem. To leverage the structure of biological systems, we incorporate graph neural networks with graph convolutions into the artificial neural network approximator for the action-value function learned by the DRL agent. Experiments on a number of large real-world biological networks from literature demonstrate the scalability and effectiveness of our approach.","authors":["Andrzej Mizera","Jakub Zarzycki"],"url":"https://arxiv.org/abs/2505.02712"}
{"created":"2025-05-06","title":"SoK: Stealing Cars Since Remote Keyless Entry Introduction and How to Defend From It","abstract":"Remote Keyless Entry (RKE) systems have been the target of thieves since their introduction in automotive industry. Robberies targeting vehicles and their remote entry systems are booming again without a significant advancement from the industrial sector being able to protect against them. Researchers and attackers continuously play cat and mouse to implement new methodologies to exploit weaknesses and defense strategies for RKEs. In this fragment, different attacks and defenses have been discussed in research and industry without proper bridging. In this paper, we provide a Systematization Of Knowledge (SOK) on RKE and Passive Keyless Entry and Start (PKES), focusing on their history and current situation, ranging from legacy systems to modern web-based ones. We provide insight into vehicle manufacturers' technologies and attacks and defense mechanisms involving them. To the best of our knowledge, this is the first comprehensive SOK on RKE systems, and we address specific research questions to understand the evolution and security status of such systems. By identifying the weaknesses RKE still faces, we provide future directions for security researchers and companies to find viable solutions to address old attacks, such as Relay and RollJam, as well as new ones, like API vulnerabilities.","authors":["Tommaso Bianchi","Alessandro Brighente","Mauro Conti","Edoardo Pavan"],"url":"https://arxiv.org/abs/2505.02713"}
{"created":"2025-05-06","title":"Less is More: Efficient Weight Farcasting with 1-Layer Neural Network","abstract":"Addressing the computational challenges inherent in training large-scale deep neural networks remains a critical endeavor in contemporary machine learning research. While previous efforts have focused on enhancing training efficiency through techniques such as gradient descent with momentum, learning rate scheduling, and weight regularization, the demand for further innovation continues to burgeon as model sizes keep expanding. In this study, we introduce a novel framework which diverges from conventional approaches by leveraging long-term time series forecasting techniques. Our method capitalizes solely on initial and final weight values, offering a streamlined alternative for complex model architectures. We also introduce a novel regularizer that is tailored to enhance the forecasting performance of our approach. Empirical evaluations conducted on synthetic weight sequences and real-world deep learning architectures, including the prominent large language model DistilBERT, demonstrate the superiority of our method in terms of forecasting accuracy and computational efficiency. Notably, our framework showcases improved performance while requiring minimal additional computational overhead, thus presenting a promising avenue for accelerating the training process across diverse tasks and architectures.","authors":["Xiao Shou","Debarun Bhattacharjya","Yanna Ding","Chen Zhao","Rui Li","Jianxi Gao"],"url":"https://arxiv.org/abs/2505.02714"}
{"created":"2025-05-06","title":"A Rate-Quality Model for Learned Video Coding","abstract":"Learned video coding (LVC) has recently achieved superior coding performance. In this paper, we model the rate-quality (R-Q) relationship for learned video coding by a parametric function. We learn a neural network, termed RQNet, to characterize the relationship between the bitrate and quality level according to video content and coding context. The predicted (R,Q) results are further integrated with those from previously coded frames using the least-squares method to determine the parameters of our R-Q model on-the-fly. Compared to the conventional approaches, our method accurately estimates the R-Q relationship, enabling the online adaptation of model parameters to enhance both flexibility and precision. Experimental results show that our R-Q model achieves significantly smaller bitrate deviations than the baseline method on commonly used datasets with minimal additional complexity.","authors":["Sang NguyenQuang","Cheng-Wei Chen","Xiem HoangVan","Wen-Hsiao Peng"],"url":"https://arxiv.org/abs/2505.02720"}
{"created":"2025-05-06","title":"Enhancing LLMs' Clinical Reasoning with Real-World Data from a Nationwide Sepsis Registry","abstract":"Although large language models (LLMs) have demonstrated impressive reasoning capabilities across general domains, their effectiveness in real-world clinical practice remains limited. This is likely due to their insufficient exposure to real-world clinical data during training, as such data is typically not included due to privacy concerns. To address this, we propose enhancing the clinical reasoning capabilities of LLMs by leveraging real-world clinical data. We constructed reasoning-intensive questions from a nationwide sepsis registry and fine-tuned Phi-4 on these questions using reinforcement learning, resulting in C-Reason. C-Reason exhibited strong clinical reasoning capabilities on the in-domain test set, as evidenced by both quantitative metrics and expert evaluations. Furthermore, its enhanced reasoning capabilities generalized to a sepsis dataset involving different tasks and patient cohorts, an open-ended consultations on antibiotics use task, and other diseases. Future research should focus on training LLMs with large-scale, multi-disease clinical datasets to develop more powerful, general-purpose clinical reasoning models.","authors":["Junu Kim","Chaeeun Shim","Sungjin Park","Su Yeon Lee","Gee Young Suh","Chae-Man Lim","Seong Jin Choi","Song Mi Moon","Kyoung-Ho Song","Eu Suk Kim","Hong Bin Kim","Sejoong Kim","Chami Im","Dong-Wan Kang","Yong Soo Kim","Hee-Joon Bae","Sung Yoon Lim","Han-Gil Jeong","Edward Choi"],"url":"https://arxiv.org/abs/2505.02722"}
{"created":"2025-05-06","title":"Acoustic Side-Channel Attacks on a Computer Mouse","abstract":"Acoustic Side-Channel Attacks (ASCAs) extract sensitive information by using audio emitted from a computing devices and their peripherals. Attacks targeting keyboards are popular and have been explored in the literature. However, similar attacks targeting other human interface peripherals, such as computer mice, are under-explored. To this end, this paper considers security leakage via acoustic signals emanating from normal mouse usage. We first confirm feasibility of such attacks by showing a proof-of-concept attack that classifies four mouse movements with 97% accuracy in a controlled environment. We then evolve the attack towards discerning twelve unique mouse movements using a smartphone to record the experiment. Using Machine Learning (ML) techniques, the model is trained on an experiment with six participants to be generalizable and discern among twelve movements with 94% accuracy. In addition, we experiment with an attack that detects a user action of closing a full-screen window on a laptop. Achieving an accuracy of 91%, this experiment highlights exploiting audio leakage from computer mouse movements in a realistic scenario.","authors":["Mauro Conti","Marin Duroyon","Gabriele Orazi","Gene Tsudik"],"url":"https://arxiv.org/abs/2505.02725"}
{"created":"2025-05-06","title":"Automotive Middleware Performance: Comparison of FastDDS, Zenoh and vSomeIP","abstract":"In this study, we evaluate the performance of current automotive communication middlewares under various operating conditions. Specifically, we examine FastDDS, a widely used open-source middleware, the newly developed Zenoh middleware, and vSomeIP, COVESAs open-source implementation of SOME/IP. Our objective is to identify the best performing middleware for specific operating conditions. To ensure accessibility, we first provide a concise overview of middleware technologies and their fundamental principles. We then introduce our testing methodology designed to systematically assess middleware performance metrics such as scaling performance, end-to-end latency, and discovery times across multiple message types, network topologies, and configurations. Finally, we compare the resulting performance data and present our results in nine findings. Our evaluation code and the resulting data will be made publicly available upon acceptance.","authors":["David Philipp Kl\\\"uner","Lucas Hegerath","Amin Dieter Hatib","Stefan Kowalewski","Bassam Alrifaee","Alexandru Kampmann"],"url":"https://arxiv.org/abs/2505.02734"}
{"created":"2025-05-06","title":"FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models","abstract":"Formal mathematical reasoning remains a critical challenge for artificial intelligence, hindered by limitations of existing benchmarks in scope and scale. To address this, we present FormalMATH, a large-scale Lean4 benchmark comprising 5,560 formally verified problems spanning from high-school Olympiad challenges to undergraduate-level theorems across diverse domains (e.g., algebra, applied mathematics, calculus, number theory, and discrete mathematics). To mitigate the inefficiency of manual formalization, we introduce a novel human-in-the-loop autoformalization pipeline that integrates: (1) specialized large language models (LLMs) for statement autoformalization, (2) multi-LLM semantic verification, and (3) negation-based disproof filtering strategies using off-the-shelf LLM-based provers. This approach reduces expert annotation costs by retaining 72.09% of statements before manual verification while ensuring fidelity to the original natural-language problems. Our evaluation of state-of-the-art LLM-based theorem provers reveals significant limitations: even the strongest models achieve only 16.46% success rate under practical sampling budgets, exhibiting pronounced domain bias (e.g., excelling in algebra but failing in calculus) and over-reliance on simplified automation tactics. Notably, we identify a counterintuitive inverse relationship between natural-language solution guidance and proof success in chain-of-thought reasoning scenarios, suggesting that human-written informal reasoning introduces noise rather than clarity in the formal reasoning settings. We believe that FormalMATH provides a robust benchmark for benchmarking formal mathematical reasoning.","authors":["Zhouliang Yu","Ruotian Peng","Keyi Ding","Yizhe Li","Zhongyuan Peng","Minghao Liu","Yifan Zhang","Zheng Yuan","Huajian Xin","Wenhao Huang","Yandong Wen","Ge Zhang","Weiyang Liu"],"url":"https://arxiv.org/abs/2505.02735"}
{"created":"2025-05-06","title":"Knowledge Graphs for Enhancing Large Language Models in Entity Disambiguation","abstract":"Recent advances in Large Language Models (LLMs) have positioned them as a prominent solution for Natural Language Processing tasks. Notably, they can approach these problems in a zero or few-shot manner, thereby eliminating the need for training or fine-tuning task-specific models. However, LLMs face some challenges, including hallucination and the presence of outdated knowledge or missing information from specific domains in the training data. These problems cannot be easily solved by retraining the models with new data as it is a time-consuming and expensive process. To mitigate these issues, Knowledge Graphs (KGs) have been proposed as a structured external source of information to enrich LLMs. With this idea, in this work we use KGs to enhance LLMs for zero-shot Entity Disambiguation (ED). For that purpose, we leverage the hierarchical representation of the entities' classes in a KG to gradually prune the candidate space as well as the entities' descriptions to enrich the input prompt with additional factual knowledge. Our evaluation on popular ED datasets shows that the proposed method outperforms non-enhanced and description-only enhanced LLMs, and has a higher degree of adaptability than task-specific models. Furthermore, we conduct an error analysis and discuss the impact of the leveraged KG's semantic expressivity on the ED performance.","authors":["Pons Gerard","Bilalli Besim","Queralt Anna"],"url":"https://arxiv.org/abs/2505.02737"}
{"created":"2025-05-06","title":"A Unifying Framework to Enable Artificial Intelligence in High Performance Computing Workflows","abstract":"Current trends point to a future where large-scale scientific applications are tightly-coupled HPC/AI hybrids. Hence, we urgently need to invest in creating a seamless, scalable framework where HPC and AI/ML can efficiently work together and adapt to novel hardware and vendor libraries without starting from scratch every few years. The current ecosystem and sparsely-connected community are not sufficient to tackle these challenges, and we require a breakthrough catalyst for science similar to what PyTorch enabled for AI.","authors":["Jens Domke","Mohamed Wahib","Anshu Dubey","Tal Ben-Nun","Erik W. Draeger"],"url":"https://arxiv.org/abs/2505.02738"}
{"created":"2025-05-06","title":"dyGRASS: Dynamic Spectral Graph Sparsification via Localized Random Walks on GPUs","abstract":"This work presents dyGRASS, an efficient dynamic algorithm for spectral sparsification of large undirected graphs that undergo streaming edge insertions and deletions. At its core, dyGRASS employs a random-walk-based method to efficiently estimate node-to-node distances in both the original graph (for decremental update) and its sparsifier (for incremental update). For incremental updates, dyGRASS enables the identification of spectrally critical edges among the updates to capture the latest structural changes. For decremental updates, dyGRASS facilitates the recovery of important edges from the original graph back into the sparsifier. To further enhance computational efficiency, dyGRASS employs a GPU-based non-backtracking random walk scheme that allows multiple walkers to operate simultaneously across various target updates. This parallelization significantly improves both the performance and scalability of the proposed dyGRASS framework. Our comprehensive experimental evaluations reveal that dyGRASS achieves approximately a 10x speedup compared to the state-of-the-art incremental sparsification (inGRASS) algorithm while eliminating the setup overhead and improving solution quality in incremental spectral sparsification tasks. Moreover, dyGRASS delivers high efficiency and superior solution quality for fully dynamic graph sparsification, accommodating both edge insertions and deletions across a diverse range of graph instances originating from integrated circuit simulations, finite element analysis, and social networks.","authors":["Yihang Yuan","Ali Aghdaei","Zhuo Feng"],"url":"https://arxiv.org/abs/2505.02741"}
{"created":"2025-05-06","title":"Cooperative Bayesian and variance networks disentangle aleatoric and epistemic uncertainties","abstract":"Real-world data contains aleatoric uncertainty - irreducible noise arising from imperfect measurements or from incomplete knowledge about the data generation process. Mean variance estimation (MVE) networks can learn this type of uncertainty but require ad-hoc regularization strategies to avoid overfitting and are unable to predict epistemic uncertainty (model uncertainty). Conversely, Bayesian neural networks predict epistemic uncertainty but are notoriously difficult to train due to the approximate nature of Bayesian inference. We propose to cooperatively train a variance network with a Bayesian neural network and demonstrate that the resulting model disentangles aleatoric and epistemic uncertainties while improving the mean estimation. We demonstrate the effectiveness and scalability of this method across a diverse range of datasets, including a time-dependent heteroscedastic regression dataset we created where the aleatoric uncertainty is known. The proposed method is straightforward to implement, robust, and adaptable to various model architectures.","authors":["Jiaxiang Yi","Miguel A. Bessa"],"url":"https://arxiv.org/abs/2505.02743"}
{"created":"2025-05-06","title":"Re-purposing a modular origami manipulator into an adaptive physical computer for machine learning and robotic perception","abstract":"Physical computing has emerged as a powerful tool for performing intelligent tasks directly in the mechanical domain of functional materials and robots, reducing our reliance on the more traditional COMS computers. However, no systematic study explains how mechanical design can influence physical computing performance. This study sheds insights into this question by repurposing an origami-inspired modular robotic manipulator into an adaptive physical reservoir and systematically evaluating its computing capacity with different physical configurations, input setups, and computing tasks. By challenging this adaptive reservoir computer to complete the classical NARMA benchmark tasks, this study shows that its time series emulation performance directly correlates to the Peak Similarity Index (PSI), which quantifies the frequency spectrum correlation between the target output and reservoir dynamics. The adaptive reservoir also demonstrates perception capabilities, accurately extracting its payload weight and orientation information from the intrinsic dynamics. Importantly, such information extraction capability can be measured by the spatial correlation between nodal dynamics within the reservoir body. Finally, by integrating shape memory alloy (SMA) actuation, this study demonstrates how to exploit such computing power embodied in the physical body for practical, robotic operations. This study provides a strategic framework for harvesting computing power from soft robots and functional materials, demonstrating how design parameters and input selection can be configured based on computing task requirements. Extending this framework to bio-inspired adaptive materials, prosthetics, and self-adaptive soft robotic systems could enable next-generation embodied intelligence, where the physical structure can compute and interact with their digital counterparts.","authors":["Jun Wang","Suyi Li"],"url":"https://arxiv.org/abs/2505.02744"}
{"created":"2025-05-06","title":"Using Knowledge Graphs to harvest datasets for efficient CLIP model training","abstract":"Training high-quality CLIP models typically requires enormous datasets, which limits the development of domain-specific models -- especially in areas that even the largest CLIP models do not cover well -- and drives up training costs. This poses challenges for scientific research that needs fine-grained control over the training procedure of CLIP models. In this work, we show that by employing smart web search strategies enhanced with knowledge graphs, a robust CLIP model can be trained from scratch with considerably less data. Specifically, we demonstrate that an expert foundation model for living organisms can be built using just 10M images. Moreover, we introduce EntityNet, a dataset comprising 33M images paired with 46M text descriptions, which enables the training of a generic CLIP model in significantly reduced time.","authors":["Simon Ging","Sebastian Walter","Jelena Bratuli\\'c","Johannes Dienert","Hannah Bast","Thomas Brox"],"url":"https://arxiv.org/abs/2505.02746"}
{"created":"2025-05-06","title":"The use of Artificial Intelligence for Intervention and Assessment in Individuals with ASD","abstract":"This paper explores the use of Artificial Intelligence (AI) as a tool for diagnosis, assessment, and intervention for individuals with Autism Spectrum Disorder (ASD). It focuses particularly on AI's role in early diagnosis, utilizing advanced machine learning techniques and data analysis. Recent studies demonstrate that deep learning algorithms can identify behavioral patterns through biometric data analysis, video-based interaction assessments, and linguistic feature extraction, providing a more accurate and timely diagnosis compared to traditional methods. Additionally, AI automates diagnostic tools, reducing subjective biases and enabling the development of personalized assessment protocols for ASD monitoring. At the same time, the paper examines AI-powered intervention technologies, emphasizing educational robots and adaptive communication tools. Social robotic assistants, such as NAO and Kaspar, have been shown to enhance social skills in children by offering structured, repetitive interactions that reinforce learning. Furthermore, AI-driven Augmentative and Alternative Communication (AAC) systems allow children with ASD to express themselves more effectively, while machine-learning chatbots provide language development support through personalized responses. The study presents research findings supporting the effectiveness of these AI applications while addressing challenges such as long-term evaluation and customization to individual needs. In conclusion, the paper highlights the significance of AI as an innovative tool in ASD diagnosis and intervention, advocating for further research to assess its long-term impact.","authors":["Aggeliki Sideraki","Christos-Nikolaos Anagnostopoulos"],"url":"https://arxiv.org/abs/2505.02747"}
{"created":"2025-05-06","title":"How May U.S. Courts Scrutinize Their Recidivism Risk Assessment Tools? Contextualizing AI Fairness Criteria on a Judicial Scrutiny-based Framework","abstract":"The AI/HCI and legal communities have developed largely independent conceptualizations of fairness. This conceptual difference hinders the potential incorporation of technical fairness criteria (e.g., procedural, group, and individual fairness) into sustainable policies and designs, particularly for high-stakes applications like recidivism risk assessment. To foster common ground, we conduct legal research to identify if and how technical AI conceptualizations of fairness surface in primary legal sources. We find that while major technical fairness criteria can be linked to constitutional mandates such as ``Due Process'' and ``Equal Protection'' thanks to judicial interpretation, several challenges arise when operationalizing them into concrete statutes/regulations. These policies often adopt procedural and group fairness but ignore the major technical criterion of individual fairness. Regarding procedural fairness, judicial ``scrutiny'' categories are relevant but may not fully capture how courts scrutinize the use of demographic features in potentially discriminatory government tools like RRA. Furthermore, some policies contradict each other on whether to apply procedural fairness to certain demographic features. Thus, we propose a new framework, integrating demographics-related legal scrutiny concepts and technical fairness criteria.","authors":["Tin Nguyen","Jiannan Xu","Phuong-Anh Nguyen-Le","Jonathan Lazar","Donald Braman","Hal Daum\\'e III","Zubin Jelveh"],"url":"https://arxiv.org/abs/2505.02749"}
{"created":"2025-05-06","title":"Platelet enumeration in dense aggregates","abstract":"Identifying and counting blood components such as red blood cells, various types of white blood cells, and platelets is a critical task for healthcare practitioners. Deep learning approaches, particularly convolutional neural networks (CNNs) using supervised learning strategies, have shown considerable success for such tasks. However, CNN based architectures such as U-Net, often struggles to accurately identify platelets due to their sizes and high variability of features. To address these challenges, researchers have commonly employed strategies such as class weighted loss functions, which have demonstrated some success. However, this does not address the more significant challenge of platelet variability in size and tendency to form aggregates and associations with other blood components. In this study, we explored an alternative approach by investigating the role of convolutional kernels in mitigating these issues. We also assigned separate classes to singular platelets and platelet aggregates and performed semantic segmentation using various U-Net architectures for identifying platelets. We then evaluated and compared two common methods (pixel area method and connected component analysis) for counting platelets and proposed an alternative approach specialized for single platelets and platelet aggregates. Our experiments provided results that showed significant improvements in the identification of platelets, highlighting the importance of optimizing convolutional operations and class designations. We show that the common practice of pixel area-based counting often over estimate platelet counts, whereas the proposed method presented in this work offers significant improvements. We discuss in detail about these methods from segmentation masks.","authors":["H. Martin Gillis","Yogeshwar Shendye","Paul Hollensen","Alan Fine","Thomas Trappenberg"],"url":"https://arxiv.org/abs/2505.02751"}
{"created":"2025-05-06","title":"Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models","abstract":"We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at https://github.com/Yankai96/DiffuGTS.","authors":["Yankai Jiang","Peng Zhang","Donglin Yang","Yuan Tian","Hai Lin","Xiaosong Wang"],"url":"https://arxiv.org/abs/2505.02753"}
{"created":"2025-05-06","title":"Optimistic, Signature-Free Reliable Broadcast and Its Applications","abstract":"Reliable broadcast (RBC) is a key primitive in fault-tolerant distributed systems, and improving its efficiency can benefit a wide range of applications. This work focuses on signature-free RBC protocols, which are particularly attractive due to their computational efficiency. Existing protocols in this setting incur an optimal 3 steps to reach a decision while tolerating up to $f < n/3$ Byzantine faults, where $n$ is the number of parties. In this work, we propose an optimistic RBC protocol that maintains the $f < n/3$ fault tolerance but achieves termination in just 2 steps under certain optimistic conditions--when at least $\\lceil \\frac{n+2f-2}{2} \\rceil$ non-broadcaster parties behave honestly. We also prove a matching lower bound on the number of honest parties required for 2-step termination.","authors":["Nibesh Shrestha","Qianyu Yu","Aniket Kate","Giuliano Losa","Kartik Nayak","Xuechao Wang"],"url":"https://arxiv.org/abs/2505.02761"}
{"created":"2025-05-06","title":"Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models","abstract":"Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.","authors":["Matthew Dahl"],"url":"https://arxiv.org/abs/2505.02763"}
{"created":"2025-05-06","title":"An Almost Tight Lower Bound for Plurality Consensus with Undecided State Dynamics in the Population Protocol Model","abstract":"We revisit the majority problem in the population protocol communication model, as first studied by Angluin et al. (Distributed Computing 2008). We consider a more general version of this problem known as plurality consensus, which has already been studied intensively in the literature. In this problem, each node in a system of $n$ nodes, has initially one of $k$ different opinions, and they need to agree on the (relative) majority opinion. In particular, we consider the important and intensively studied model of Undecided State Dynamics.","authors":["Antoine El-Hayek","Robert Els\\\"asser","Stefan Schmid"],"url":"https://arxiv.org/abs/2505.02765"}
{"created":"2025-05-06","title":"Giving Simulated Cells a Voice: Evolving Prompt-to-Intervention Models for Cellular Control","abstract":"Guiding biological systems toward desired states, such as morphogenetic outcomes, remains a fundamental challenge with far-reaching implications for medicine and synthetic biology. While large language models (LLMs) have enabled natural language as an interface for interpretable control in AI systems, their use as mediators for steering biological or cellular dynamics remains largely unexplored.","authors":["Nam H. Le","Patrick Erikson","Yanbo Zhang","Michael Levin","Josh Bongard"],"url":"https://arxiv.org/abs/2505.02766"}
{"created":"2025-05-06","title":"Teaching the social media generation: rethinking learning without sacrificing quality","abstract":"The rise of social media and AI tools has reshaped how students engage with learning, process information, and build trust in educational content. This generation prefers short, visual materials and fast feedback but often struggles with focus, critical thinking, and deep learning. Educators face the challenge of adapting teaching methods to these habits without lowering academic standards. This study presents a blended learning redesign of a first-year technical course at a Dutch university. Key features included short whiteboard videos before class, hands-on teamwork during class, narrative-style handouts to reinforce learning, in-class draft assignments without AI, and weekly anonymous feedback to adjust in real time. The results were promising: attendance increased by nearly 50%, and none of the regularly attending students failed the exam. Students found the videos useful but emphasized that in-person sessions were essential for understanding the material. While some resisted the shift in expectations, most appreciated the structure, clarity, and opportunities for active learning. This case suggests that combining digital familiarity with clear expectations and active support can help meet students where they are, while still challenging them to grow.","authors":["Sepinoud Azimi"],"url":"https://arxiv.org/abs/2505.02770"}
{"created":"2025-05-06","title":"Courcelle's Theorem Without Logic","abstract":"Courcelle's Theorem states that on graphs $G$ of tree-width at most $k$ with a given tree-decomposition of size $t(G)$, graph properties $\\mathcal{P}$ definable in Monadic Second Order Logic can be checked in linear time in the size of $t(G)$. Inspired by L. Lov\\'asz' work using connection matrices instead of logic, we give a generalized version of Courcelle's theorem which replaces the definability hypothesis by a purely combinatorial hypothesis using a generalization of connection matrices.","authors":["Yuval Filmus","Johann A. Makowsky"],"url":"https://arxiv.org/abs/2505.02771"}
{"created":"2025-05-06","title":"Unsupervised Deep Learning-based Keypoint Localization Estimating Descriptor Matching Performance","abstract":"Retinal image registration, particularly for color fundus images, is a challenging yet essential task with diverse clinical applications. Existing registration methods for color fundus images typically rely on keypoints and descriptors for alignment; however, a significant limitation is their reliance on labeled data, which is particularly scarce in the medical domain.","authors":["David Rivas-Villar","\\'Alvaro S. Hervella","Jos\\'e Rouco","Jorge Novo"],"url":"https://arxiv.org/abs/2505.02779"}
{"created":"2025-05-06","title":"Beyond the Monitor: Mixed Reality Visualization and AI for Enhanced Digital Pathology Workflow","abstract":"Pathologists rely on gigapixel whole-slide images (WSIs) to diagnose diseases like cancer, yet current digital pathology tools hinder diagnosis. The immense scale of WSIs, often exceeding 100,000 X 100,000 pixels, clashes with the limited views traditional monitors offer. This mismatch forces constant panning and zooming, increasing pathologist cognitive load, causing diagnostic fatigue, and slowing pathologists' adoption of digital methods. PathVis, our mixed-reality visualization platform for Apple Vision Pro, addresses these challenges. It transforms the pathologist's interaction with data, replacing cumbersome mouse-and-monitor navigation with intuitive exploration using natural hand gestures, eye gaze, and voice commands in an immersive workspace. PathVis integrates AI to enhance diagnosis. An AI-driven search function instantly retrieves and displays the top five similar patient cases side-by-side, improving diagnostic precision and efficiency through rapid comparison. Additionally, a multimodal conversational AI assistant offers real-time image interpretation support and aids collaboration among pathologists across multiple Apple devices. By merging the directness of traditional pathology with advanced mixed-reality visualization and AI, PathVis improves diagnostic workflows, reduces cognitive strain, and makes pathology practice more effective and engaging. The PathVis source code and a demo video are publicly available at: https://github.com/jaiprakash1824/Path_Vis","authors":["Jai Prakash Veerla","Partha Sai Guttikonda","Helen H. Shang","Mohammad Sadegh Nasr","Cesar Torres","Jacob M. Luber"],"url":"https://arxiv.org/abs/2505.02780"}
{"created":"2025-05-06","title":"Local Markov Equivalence and Local Causal Discovery for Identifying Controlled Direct Effects","abstract":"Understanding and identifying controlled direct effects (CDEs) is crucial across numerous scientific domains, including public health. While existing methods can identify these effects from causal directed acyclic graphs (DAGs), the true underlying structure is often unknown in practice. Essential graphs, which represent a Markov equivalence class of DAGs characterized by the same set of d-separations, provide a more practical and realistic alternative. However, learning the full essential graph is computationally intensive and typically depends on strong, untestable assumptions. In this work, we characterize a local class of graphs, defined relative to a target variable, that share a specific subset of d-separations, and introduce a graphical representation of this class, called the local essential graph (LEG). We then present LocPC, a novel algorithm designed to recover the LEG from an observed distribution using only local conditional independence tests. Building on LocPC, we propose LocPC-CDE, an algorithm that discovers the portion of the LEG that is sufficient to identify a CDE, bypassing the need of retrieving the full essential graph. Compared to global methods, our algorithms require less conditional independence tests and operate under weaker assumptions while maintaining theoretical guarantees.","authors":["Timoth\\'ee Loranchet","Charles K. Assaad"],"url":"https://arxiv.org/abs/2505.02781"}
{"created":"2025-05-06","title":"Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge","abstract":"Accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. The FeTA Challenge 2024 advanced automated fetal brain MRI analysis by introducing biometry prediction as a new task alongside tissue segmentation. For the first time, our diverse multi-centric test set included data from a new low-field (0.55T) MRI dataset. Evaluation metrics were also expanded to include the topology-specific Euler characteristic difference (ED). Sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. However, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. The ED metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. Seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. Domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. Other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. Overall, FeTA 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain MRI, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable AI tools.","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Misha Kaandorp","Margaux Roulet","Diego Fajardo-Rojas","Liu Li","Jana Hutter","Hongwei Bran Li","Matthew Barkovich","Hui Ji","Luca Wilhelmi","Aline D\\\"andliker","C\\'eline Steger","M\\'eriam Koob","Yvan Gomez","Anton Jakov\\v{c}i\\'c","Melita Klai\\'c","Ana Ad\\v{z}i\\'c","Pavel Markovi\\'c","Gracia Grabari\\'c","Milan Rados","Jordina Aviles Verdera","Gregor Kasprian","Gregor Dovjak","Raphael Gaubert-Rachm\\\"uhl","Maurice Aschwanden","Qi Zeng","Davood Karimi","Denis Peruzzo","Tommaso Ciceri","Giorgio Longari","Rachika E. Hamadache","Amina Bouzid","Xavier Llad\\'o","Simone Chiarella","Gerard Mart\\'i-Juan","Miguel \\'Angel Gonz\\'alez Ballester","Marco Castellaro","Marco Pinamonti","Valentina Visani","Robin Cremese","Ke\\\"in Sam","Fleur Gaudfernau","Param Ahir","Mehul Parikh","Maximilian Zenk","Michael Baumgartner","Klaus Maier-Hein","Li Tianhong","Yang Hong","Zhao Longfei","Domen Preloznik","\\v{Z}iga \\v{S}piclin","Jae Won Choi","Muyang Li","Jia Fu","Guotai Wang","Jingwen Jiang","Lyuyang Tong","Bo Du","Andrea Gondova","Sungmin You","Kiho Im","Abdul Qayyum","Moona Mazher","Steven A Niederer","Maya Yanko","Bella Specktor-Fadida","Dafna Ben Bashat","Andras Jakab","Roxane Licandro","Kelly Payette","Meritxell Bach Cuadra"],"url":"https://arxiv.org/abs/2505.02784"}
{"created":"2025-05-06","title":"Brief Announcement: Minimizing Energy Solves Relative Majority with a Cubic Number of States in Population Protocols","abstract":"This paper revisits a fundamental distributed computing problem in the population protocol model.","authors":["Tom-Lukas Breitkopf","Julien Dallot","Antoine El-Hayek","Stefan Schmid"],"url":"https://arxiv.org/abs/2505.02785"}
{"created":"2025-05-06","title":"Unsupervised training of keypoint-agnostic descriptors for flexible retinal image registration","abstract":"Current color fundus image registration approaches are limited, among other things, by the lack of labeled data, which is even more significant in the medical domain, motivating the use of unsupervised learning. Therefore, in this work, we develop a novel unsupervised descriptor learning method that does not rely on keypoint detection. This enables the resulting descriptor network to be agnostic to the keypoint detector used during the registration inference.","authors":["David Rivas-Villar","\\'Alvaro S. Hervella","Jos\\'e Rouco","Jorge Novo"],"url":"https://arxiv.org/abs/2505.02787"}
{"created":"2025-05-06","title":"Recolorable Graph Exploration by an Oblivious Agent with Fewer Colors","abstract":"Recently, B\\\"ockenhauer, Frei, Unger, and Wehner (SIROCCO 2023) introduced a novel variant of the graph exploration problem in which a single memoryless agent must visit all nodes of an unknown, undirected, and connected graph before returning to its starting node. Unlike the standard model for mobile agents, edges are not labeled with port numbers. Instead, the agent can color its current node and observe the color of each neighboring node. To move, it specifies a target color and then moves to an adversarially chosen neighbor of that color. B\\\"ockenhauer~et al.~analyzed the minimum number of colors required for successful exploration and proposed an elegant algorithm that enables the agent to explore an arbitrary graph using only eight colors. In this paper, we present a novel graph exploration algorithm that requires only six colors. Furthermore, we prove that five colors are sufficient if we consider only a restricted class of graphs, which we call the $\\varphi$-free graphs, a class that includes every graph with maximum degree at most three and every cactus.","authors":["Shota Takahashi","Haruki Kanaya","Shoma Hiraoka","Ryota Eguchi","Yuichi Sudo"],"url":"https://arxiv.org/abs/2505.02789"}
{"created":"2025-05-06","title":"Scoring the European Citizen in the AI Era","abstract":"Social scoring is one of the AI practices banned by the AI Act. This ban is explicitly inspired by China, which in 2014 announced its intention to set up a large-scale government project - the Social Credit System - aiming to rate every Chinese citizen according to their good behaviour, using digital technologies and AI. But in Europe, individuals are also scored by public and private bodies in a variety of contexts, such as assessing creditworthiness, monitoring employee productivity, detecting social fraud or terrorist risks, and so on. However, the AI Act does not intend to prohibit these types of scoring, as they would qualify as 'high-risk AI systems', which are authorised while subject to various requirements. One might therefore think that the ban on social scoring will have no practical effect on the scoring practices already in use in Europe, and that it is merely a vague safeguard in case an authoritarian power is tempted to set up such a system on European territory. Contrary to this view, this article argues that the ban has been drafted in a way that is flexible and therefore likely to make it a useful tool, similar and complementary to Article 22 of the General Data Protection Regulation, to protect individuals against certain forms of disproportionate use of AI-based scoring.","authors":["Nathan Genicot"],"url":"https://arxiv.org/abs/2505.02791"}
{"created":"2025-05-06","title":"HSplitLoRA: A Heterogeneous Split Parameter-Efficient Fine-Tuning Framework for Large Language Models","abstract":"Recently, large language models (LLMs) have achieved remarkable breakthroughs, revolutionizing the natural language processing domain and beyond. Due to immense parameter sizes, fine-tuning these models with private data for diverse downstream tasks has become mainstream. Though federated learning (FL) offers a promising solution for fine-tuning LLMs without sharing raw data, substantial computing costs hinder its democratization. Moreover, in real-world scenarios, private client devices often possess heterogeneous computing resources, further complicating LLM fine-tuning. To combat these challenges, we propose HSplitLoRA, a heterogeneous parameter-efficient fine-tuning (PEFT) framework built on split learning (SL) and low-rank adaptation (LoRA) fine-tuning, for efficiently fine-tuning LLMs on heterogeneous client devices. HSplitLoRA first identifies important weights based on their contributions to LLM training. It then dynamically configures the decomposition ranks of LoRA adapters for selected weights and determines the model split point according to varying computing budgets of client devices. Finally, a noise-free adapter aggregation mechanism is devised to support heterogeneous adapter aggregation without introducing noise. Extensive experiments demonstrate that HSplitLoRA outperforms state-of-the-art benchmarks in training accuracy and convergence speed.","authors":["Zheng Lin","Yuxin Zhang","Zhe Chen","Zihan Fang","Xianhao Chen","Praneeth Vepakomma","Wei Ni","Jun Luo","Yue Gao"],"url":"https://arxiv.org/abs/2505.02795"}
{"created":"2025-05-06","title":"Adaptive Bidding Policies for First-Price Auctions with Budget Constraints under Non-stationarity","abstract":"We study how a budget-constrained bidder should learn to adaptively bid in repeated first-price auctions to maximize her cumulative payoff. This problem arose due to an industry-wide shift from second-price auctions to first-price auctions in display advertising recently, which renders truthful bidding (i.e., always bidding one's private value) no longer optimal. We propose a simple dual-gradient-descent-based bidding policy that maintains a dual variable for budget constraint as the bidder consumes her budget. In analysis, we consider two settings regarding the bidder's knowledge of her private values in the future: (i) an uninformative setting where all the distributional knowledge (can be non-stationary) is entirely unknown to the bidder, and (ii) an informative setting where a prediction of the budget allocation in advance. We characterize the performance loss (or regret) relative to an optimal policy with complete information on the stochasticity. For uninformative setting, We show that the regret is \\tilde{O}(\\sqrt{T}) plus a variation term that reflects the non-stationarity of the value distributions, and this is of optimal order. We then show that we can get rid of the variation term with the help of the prediction; specifically, the regret is \\tilde{O}(\\sqrt{T}) plus the prediction error term in the informative setting.","authors":["Yige Wang","Jiashuo Jiang"],"url":"https://arxiv.org/abs/2505.02796"}
{"created":"2025-05-06","title":"DPNet: Dynamic Pooling Network for Tiny Object Detection","abstract":"In unmanned aerial systems, especially in complex environments, accurately detecting tiny objects is crucial. Resizing images is a common strategy to improve detection accuracy, particularly for small objects. However, simply enlarging images significantly increases computational costs and the number of negative samples, severely degrading detection performance and limiting its applicability. This paper proposes a Dynamic Pooling Network (DPNet) for tiny object detection to mitigate these issues. DPNet employs a flexible down-sampling strategy by introducing a factor (df) to relax the fixed downsampling process of the feature map to an adjustable one. Furthermore, we design a lightweight predictor to predict df for each input image, which is used to decrease the resolution of feature maps in the backbone. Thus, we achieve input-aware downsampling. We also design an Adaptive Normalization Module (ANM) to make a unified detector compatible with different dfs. A guidance loss supervises the predictor's training. DPNet dynamically allocates computing resources to trade off between detection accuracy and efficiency. Experiments on the TinyCOCO and TinyPerson datasets show that DPNet can save over 35% and 25% GFLOPs, respectively, while maintaining comparable detection performance. The code will be made publicly available.","authors":["Luqi Gong","Haotian Chen","Yikun Chen","Tianliang Yao","Chao Li","Shuai Zhao","Guangjie Han"],"url":"https://arxiv.org/abs/2505.02797"}
{"created":"2025-05-06","title":"Unifying Laplace Mechanism with Instance Optimality in Differential Privacy","abstract":"We adapt the canonical Laplace mechanism, widely used in differentially private data analysis, to achieve near instance optimality with respect to the hardness of the underlying dataset. In particular, we construct a piecewise Laplace distribution whereby we defy traditional assumptions and show that Laplace noise can in fact be drawn proportional to the local sensitivity when done in a piecewise manner. While it may initially seem counterintuitive that this satisfies (pure) differential privacy and can be sampled, we provide both through a simple connection to the exponential mechanism and inverse sensitivity along with the fact that the Laplace distribution is a two-sided exponential distribution. As a result, we prove that in the continuous setting our \\textit{piecewise Laplace mechanism} strictly dominates the inverse sensitivity mechanism, which was previously shown to both be nearly instance optimal and uniformly outperform the smooth sensitivity framework. Furthermore, in the worst-case where all local sensitivities equal the global sensitivity, our method simply reduces to a Laplace mechanism. We also complement this with an approximate local sensitivity variant to potentially ease the computational cost, which can also extend to higher dimensions.","authors":["David Durfee"],"url":"https://arxiv.org/abs/2505.02798"}
{"created":"2025-05-06","title":"Generating HomeAssistant Automations Using an LLM-based Chatbot","abstract":"To combat climate change, individuals are encouraged to adopt sustainable habits, in particular, with their household, optimizing their electrical consumption. Conversational agents, such as Smart Home Assistants, hold promise as effective tools for promoting sustainable practices within households. Our research investigated the application of Large Language Models (LLM) in enhancing smart home automation and promoting sustainable household practices, specifically using the HomeAssistant framework. In particular, it highlights the potential of GPT models in generating accurate automation routines. While the LLMs showed proficiency in understanding complex commands and creating valid JSON outputs, challenges such as syntax errors and message malformations were noted, indicating areas for further improvement. Still, despite minimal quantitative differences between \"green\" and \"no green\" prompts, qualitative feedback highlighted a positive shift towards sustainability in the routines generated with environmentally focused prompts. Then, an empirical evaluation (N=56) demonstrated that the system was well-received and found engaging by users compared to its traditional rule-based counterpart. Our findings highlight the role of LLMs in advancing smart home technologies and suggest further research to refine these models for broader, real-world applications to support sustainable living.","authors":["Mathyas Giudici","Alessandro Sironi","Ismaele Villa","Samuele Scherini","Franca Garzotto"],"url":"https://arxiv.org/abs/2505.02802"}
{"created":"2025-05-06","title":"Cell-Free Massive MIMO-Assisted SWIPT for IoT Networks","abstract":"This paper studies cell-free massive multiple-input multiple-output (CF-mMIMO) systems that underpin simultaneous wireless information and power transfer (SWIPT) for separate information users (IUs) and energy users (EUs) in Internet of Things (IoT) networks. We propose a joint access point (AP) operation mode selection and power control design, wherein certain APs are designated for energy transmission to EUs, while others are dedicated to information transmission to IUs. The performance of the system, from both a spectral efficiency (SE) and energy efficiency (EE) perspective, is comprehensively analyzed. Specifically, we formulate two mixed-integer nonconvex optimization problems for maximizing the average sum-SE and EE, under realistic power consumption models and constraints on the minimum individual SE requirements for individual IUs, minimum HE for individual EUs, and maximum transmit power at each AP. The challenging optimization problems are solved using successive convex approximation (SCA) techniques. The proposed framework design is further applied to the average sum-HE maximization and energy harvesting fairness problems. Our numerical results demonstrate that the proposed joint AP operation mode selection and power control algorithm can achieve EE performance gains of up to $4$-fold and $5$-fold over random AP operation mode selection, with and without power control respectively.","authors":["Mohammadali Mohammadi","Le-Nam Tran","Zahra Mobini","Hien Quoc Ngo","Michail Matthaiou"],"url":"https://arxiv.org/abs/2505.02806"}
{"created":"2025-05-06","title":"Towards Quantifying the Hessian Structure of Neural Networks","abstract":"Empirical studies reported that the Hessian matrix of neural networks (NNs) exhibits a near-block-diagonal structure, yet its theoretical foundation remains unclear. In this work, we reveal two forces that shape the Hessian structure: a ``static force'' rooted in the architecture design, and a ``dynamic force'' arisen from training. We then provide a rigorous theoretical analysis of ``static force'' at random initialization. We study linear models and 1-hidden-layer networks with the mean-square (MSE) loss and the Cross-Entropy (CE) loss for classification tasks. By leveraging random matrix theory, we compare the limit distributions of the diagonal and off-diagonal Hessian blocks and find that the block-diagonal structure arises as $C \\rightarrow \\infty$, where $C$ denotes the number of classes. Our findings reveal that $C$ is a primary driver of the near-block-diagonal structure. These results may shed new light on the Hessian structure of large language models (LLMs), which typically operate with a large $C$ exceeding $10^4$ or $10^5$.","authors":["Zhaorui Dong","Yushun Zhang","Zhi-Quan Luo","Jianfeng Yao","Ruoyu Sun"],"url":"https://arxiv.org/abs/2505.02809"}
{"created":"2025-05-06","title":"Knowing You Don't Know: Learning When to Continue Search in Multi-round RAG through Self-Practicing","abstract":"Retrieval Augmented Generation (RAG) has shown strong capability in enhancing language models' knowledge and reducing AI generative hallucinations, driving its widespread use. However, complex tasks requiring multi-round retrieval remain challenging, and early attempts tend to be overly optimistic without a good sense of self-skepticism. Current multi-round RAG systems may continue searching even when enough information has already been retrieved, or they may provide incorrect answers without having sufficient information or knowledge. Existing solutions either require large amounts of expensive human-labeled process supervision data or lead to subpar performance.","authors":["Diji Yang","Linda Zeng","Jinmeng Rao","Yi Zhang"],"url":"https://arxiv.org/abs/2505.02811"}
{"created":"2025-05-06","title":"Structured Estimators: A New Perspective on Information Freshness","abstract":"In recent literature, when modeling for information freshness in remote estimation settings, estimators have been mainly restricted to the class of martingale estimators, meaning the remote estimate at any time is equal to the most recently received update. This is mainly due to its simplicity and ease of analysis. However, these martingale estimators are far from optimal in some cases, especially in pull-based update systems. For such systems, maximum aposteriori probability (MAP) estimators are optimum, but can be challenging to analyze. Here, we introduce a new class of estimators, called structured estimators, which retain useful characteristics from a MAP estimate while still being analytically tractable. Our proposed estimators move seamlessly from a martingale estimator to a MAP estimator.","authors":["Sahan Liyanaarachchi","Sennur Ulukus","Nail Akar"],"url":"https://arxiv.org/abs/2505.02813"}
{"created":"2025-05-06","title":"Database-Agnostic Gait Enrollment using SetTransformers","abstract":"Gait recognition has emerged as a powerful tool for unobtrusive and long-range identity analysis, with growing relevance in surveillance and monitoring applications. Although recent advances in deep learning and large-scale datasets have enabled highly accurate recognition under closed-set conditions, real-world deployment demands open-set gait enrollment, which means determining whether a new gait sample corresponds to a known identity or represents a previously unseen individual. In this work, we introduce a transformer-based framework for open-set gait enrollment that is both dataset-agnostic and recognition-architecture-agnostic. Our method leverages a SetTransformer to make enrollment decisions based on the embedding of a probe sample and a context set drawn from the gallery, without requiring task-specific thresholds or retraining for new environments. By decoupling enrollment from the main recognition pipeline, our model is generalized across different datasets, gallery sizes, and identity distributions. We propose an evaluation protocol that uses existing datasets in different ratios of identities and walks per identity. We instantiate our method using skeleton-based gait representations and evaluate it on two benchmark datasets (CASIA-B and PsyMo), using embeddings from three state-of-the-art recognition models (GaitGraph, GaitFormer, and GaitPT). We show that our method is flexible, is able to accurately perform enrollment in different scenarios, and scales better with data compared to traditional approaches. We will make the code and dataset scenarios publicly available.","authors":["Nicoleta Basoc","Adrian Cosma","Andy C\\v{a}trun\\v{a}","Emilian R\\v{a}doi"],"url":"https://arxiv.org/abs/2505.02815"}
{"created":"2025-05-06","title":"ReplaceMe: Network Simplification via Layer Pruning and Linear Transformations","abstract":"We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation to approximate the pruned blocks. This estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25% pruning while retaining approximately 90% of the original model's performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead (see Fig.1). We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at this repository.","authors":["Dmitriy Shopkhoev","Ammar Ali","Magauiya Zhussip","Valentin Malykh","Stamatios Lefkimmiatis","Nikos Komodakis","Sergey Zagoruyko"],"url":"https://arxiv.org/abs/2505.02819"}
{"created":"2025-05-06","title":"AutoLibra: Agent Metric Induction from Open-Ended Feedback","abstract":"Agents are predominantly evaluated and optimized via task success metrics, which are coarse, rely on manual design from experts, and fail to reward intermediate emergent behaviors. We propose AutoLibra, a framework for agent evaluation, that transforms open-ended human feedback, e.g., \"If you find that the button is disabled, don't click it again\", or \"This agent has too much autonomy to decide what to do on its own\", into metrics for evaluating fine-grained behaviors in agent trajectories. AutoLibra accomplishes this by grounding feedback to an agent's behavior, clustering similar positive and negative behaviors, and creating concrete metrics with clear definitions and concrete examples, which can be used for prompting LLM-as-a-Judge as evaluators. We further propose two meta-metrics to evaluate the alignment of a set of (induced) metrics with open feedback: \"coverage\" and \"redundancy\". Through optimizing these meta-metrics, we experimentally demonstrate AutoLibra's ability to induce more concrete agent evaluation metrics than the ones proposed in previous agent evaluation benchmarks and discover new metrics to analyze agents. We also present two applications of AutoLibra in agent improvement: First, we show that AutoLibra-induced metrics serve as better prompt-engineering targets than the task success rate on a wide range of text game tasks, improving agent performance over baseline by a mean of 20%. Second, we show that AutoLibra can iteratively select high-quality fine-tuning data for web navigation agents. Our results suggest that AutoLibra is a powerful task-agnostic tool for evaluating and improving language agents.","authors":["Hao Zhu","Phil Cuvin","Xinkai Yu","Charlotte Ka Yee Yan","Jason Zhang","Diyi Yang"],"url":"https://arxiv.org/abs/2505.02820"}
{"created":"2025-05-06","title":"MUSAR: Exploring Multi-Subject Customization from Single-Subject Dataset via Attention Routing","abstract":"Current multi-subject customization approaches encounter two critical challenges: the difficulty in acquiring diverse multi-subject training data, and attribute entanglement across different subjects. To bridge these gaps, we propose MUSAR - a simple yet effective framework to achieve robust multi-subject customization while requiring only single-subject training data. Firstly, to break the data limitation, we introduce debiased diptych learning. It constructs diptych training pairs from single-subject images to facilitate multi-subject learning, while actively correcting the distribution bias introduced by diptych construction via static attention routing and dual-branch LoRA. Secondly, to eliminate cross-subject entanglement, we introduce dynamic attention routing mechanism, which adaptively establishes bijective mappings between generated images and conditional subjects. This design not only achieves decoupling of multi-subject representations but also maintains scalable generalization performance with increasing reference subjects. Comprehensive experiments demonstrate that our MUSAR outperforms existing methods - even those trained on multi-subject dataset - in image quality, subject consistency, and interaction naturalness, despite requiring only single-subject dataset.","authors":["Zinan Guo","Pengze Zhang","Yanze Wu","Chong Mou","Songtao Zhao","Qian He"],"url":"https://arxiv.org/abs/2505.02823"}
{"created":"2025-05-06","title":"Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models","abstract":"Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.","authors":["Kuofeng Gao","Yufei Zhu","Yiming Li","Jiawang Bai","Yong Yang","Zhifeng Li","Shu-Tao Xia"],"url":"https://arxiv.org/abs/2505.02824"}
{"created":"2025-05-06","title":"Towards Application-Specific Evaluation of Vision Models: Case Studies in Ecology and Biology","abstract":"Computer vision methods have demonstrated considerable potential to streamline ecological and biological workflows, with a growing number of datasets and models becoming available to the research community. However, these resources focus predominantly on evaluation using machine learning metrics, with relatively little emphasis on how their application impacts downstream analysis. We argue that models should be evaluated using application-specific metrics that directly represent model performance in the context of its final use case. To support this argument, we present two disparate case studies: (1) estimating chimpanzee abundance and density with camera trap distance sampling when using a video-based behaviour classifier and (2) estimating head rotation in pigeons using a 3D posture estimator. We show that even models with strong machine learning performance (e.g., 87% mAP) can yield data that leads to discrepancies in abundance estimates compared to expert-derived data. Similarly, the highest-performing models for posture estimation do not produce the most accurate inferences of gaze direction in pigeons. Motivated by these findings, we call for researchers to integrate application-specific metrics in ecological/biological datasets, allowing for models to be benchmarked in the context of their downstream application and to facilitate better integration of models into application workflows.","authors":["Alex Hoi Hang Chan","Otto Brookes","Urs Waldmann","Hemal Naik","Iain D. Couzin","Majid Mirmehdi","No\\\"el Adiko Houa","Emmanuelle Normand","Christophe Boesch","Lukas Boesch","Mimi Arandjelovic","Hjalmar K\\\"uhl","Tilo Burghardt","Fumihiro Kano"],"url":"https://arxiv.org/abs/2505.02825"}
{"created":"2025-05-06","title":"Privacy Risks and Preservation Methods in Explainable Artificial Intelligence: A Scoping Review","abstract":"Explainable Artificial Intelligence (XAI) has emerged as a pillar of Trustworthy AI and aims to bring transparency in complex models that are opaque by nature. Despite the benefits of incorporating explanations in models, an urgent need is found in addressing the privacy concerns of providing this additional information to end users. In this article, we conduct a scoping review of existing literature to elicit details on the conflict between privacy and explainability. Using the standard methodology for scoping review, we extracted 57 articles from 1,943 studies published from January 2019 to December 2024. The review addresses 3 research questions to present readers with more understanding of the topic: (1) what are the privacy risks of releasing explanations in AI systems? (2) what current methods have researchers employed to achieve privacy preservation in XAI systems? (3) what constitutes a privacy preserving explanation? Based on the knowledge synthesized from the selected studies, we categorize the privacy risks and preservation methods in XAI and propose the characteristics of privacy preserving explanations to aid researchers and practitioners in understanding the requirements of XAI that is privacy compliant. Lastly, we identify the challenges in balancing privacy with other system desiderata and provide recommendations for achieving privacy preserving XAI. We expect that this review will shed light on the complex relationship of privacy and explainability, both being the fundamental principles of Trustworthy AI.","authors":["Sonal Allana","Mohan Kankanhalli","Rozita Dara"],"url":"https://arxiv.org/abs/2505.02828"}
{"created":"2025-05-06","title":"LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery","abstract":"Segmentation models can recognize a pre-defined set of objects in images. However, models that can reason over complex user queries that implicitly refer to multiple objects of interest are still in their infancy. Recent advances in reasoning segmentation--generating segmentation masks from complex, implicit query text--demonstrate that vision-language models can operate across an open domain and produce reasonable outputs. However, our experiments show that such models struggle with complex remote-sensing imagery. In this work, we introduce LISAt, a vision-language model designed to describe complex remote-sensing scenes, answer questions about them, and segment objects of interest. We trained LISAt on a new curated geospatial reasoning-segmentation dataset, GRES, with 27,615 annotations over 9,205 images, and a multimodal pretraining dataset, PreGRES, containing over 1 million question-answer pairs. LISAt outperforms existing geospatial foundation models such as RS-GPT4V by over 10.04 % (BLEU-4) on remote-sensing description tasks, and surpasses state-of-the-art open-domain models on reasoning segmentation tasks by 143.36 % (gIoU). Our model, datasets, and code are available at https://lisat-bair.github.io/LISAt/","authors":["Jerome Quenum","Wen-Han Hsieh","Tsung-Han Wu","Ritwik Gupta","Trevor Darrell","David M. Chan"],"url":"https://arxiv.org/abs/2505.02829"}
{"created":"2025-05-06","title":"AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation","abstract":"Chest X-rays (CXRs) are the most frequently performed imaging examinations in clinical settings. Recent advancements in Large Multimodal Models (LMMs) have enabled automated CXR interpretation, enhancing diagnostic accuracy and efficiency. However, despite their strong visual understanding, current Medical LMMs (MLMMs) still face two major challenges: (1) Insufficient region-level understanding and interaction, and (2) Limited accuracy and interpretability due to single-step reasoning. In this paper, we empower MLMMs with anatomy-centric reasoning capabilities to enhance their interactivity and explainability. Specifically, we first propose an Anatomical Ontology-Guided Reasoning (AOR) framework, which centers on cross-modal region-level information to facilitate multi-step reasoning. Next, under the guidance of expert physicians, we develop AOR-Instruction, a large instruction dataset for MLMMs training. Our experiments demonstrate AOR's superior performance in both VQA and report generation tasks.","authors":["Qingqiu Li","Zihang Cui","Seongsu Bae","Jilan Xu","Runtian Yuan","Yuejie Zhang","Rui Feng","Quanli Shen","Xiaobo Zhang","Junjun He","Shujun Wang"],"url":"https://arxiv.org/abs/2505.02830"}
{"created":"2025-05-06","title":"No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves","abstract":"Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance generation quality of the diffusion transformers. However, existing approaches necessitate to either introduce an additional and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation A}lignment (SRA), a simple yet straightforward method that obtain representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in earlier layer with higher noise to that in later layer with lower noise to progressively enhance the overall representation learning during only generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that heavily dependent on powerful external representation priors.","authors":["Dengyang Jiang","Mengmeng Wang","Liuzhuozheng Li","Lei Zhang","Haoyu Wang","Wei Wei","Guang Dai","Yanning Zhang","Jingdong Wang"],"url":"https://arxiv.org/abs/2505.02831"}
{"created":"2025-05-06","title":"TWIST: Teleoperated Whole-Body Imitation System","abstract":"Teleoperating humanoid robots in a whole-body manner marks a fundamental step toward developing general-purpose robotic intelligence, with human motion providing an ideal interface for controlling all degrees of freedom. Yet, most current humanoid teleoperation systems fall short of enabling coordinated whole-body behavior, typically limiting themselves to isolated locomotion or manipulation tasks. We present the Teleoperated Whole-Body Imitation System (TWIST), a system for humanoid teleoperation through whole-body motion imitation. We first generate reference motion clips by retargeting human motion capture data to the humanoid robot. We then develop a robust, adaptive, and responsive whole-body controller using a combination of reinforcement learning and behavior cloning (RL+BC). Through systematic analysis, we demonstrate how incorporating privileged future motion frames and real-world motion capture (MoCap) data improves tracking accuracy. TWIST enables real-world humanoid robots to achieve unprecedented, versatile, and coordinated whole-body motor skills--spanning whole-body manipulation, legged manipulation, locomotion, and expressive movement--using a single unified neural network controller. Our project website: https://humanoid-teleop.github.io","authors":["Yanjie Ze","Zixuan Chen","Jo\\~ao Pedro Ara\\'ujo","Zi-ang Cao","Xue Bin Peng","Jiajun Wu","C. Karen Liu"],"url":"https://arxiv.org/abs/2505.02833"}
{"created":"2025-05-06","title":"R1-Reward: Training Multimodal Reward Model Through Stable Reinforcement Learning","abstract":"Multimodal Reward Models (MRMs) play a crucial role in enhancing the performance of Multimodal Large Language Models (MLLMs). While recent advancements have primarily focused on improving the model structure and training data of MRMs, there has been limited exploration into the effectiveness of long-term reasoning capabilities for reward modeling and how to activate these capabilities in MRMs. In this paper, we explore how Reinforcement Learning (RL) can be used to improve reward modeling. Specifically, we reformulate the reward modeling problem as a rule-based RL task. However, we observe that directly applying existing RL algorithms, such as Reinforce++, to reward modeling often leads to training instability or even collapse due to the inherent limitations of these algorithms. To address this issue, we propose the StableReinforce algorithm, which refines the training loss, advantage estimation strategy, and reward design of existing RL methods. These refinements result in more stable training dynamics and superior performance. To facilitate MRM training, we collect 200K preference data from diverse datasets. Our reward model, R1-Reward, trained using the StableReinforce algorithm on this dataset, significantly improves performance on multimodal reward modeling benchmarks. Compared to previous SOTA models, R1-Reward achieves a $8.4\\%$ improvement on the VL Reward-Bench and a $14.3\\%$ improvement on the Multimodal Reward Bench. Moreover, with more inference compute, R1-Reward's performance is further enhanced, highlighting the potential of RL algorithms in optimizing MRMs.","authors":["Yi-Fan Zhang","Xingyu Lu","Xiao Hu","Chaoyou Fu","Bin Wen","Tianke Zhang","Changyi Liu","Kaiyu Jiang","Kaibing Chen","Kaiyu Tang","Haojie Ding","Jiankang Chen","Fan Yang","Zhang Zhang","Tingting Gao","Liang Wang"],"url":"https://arxiv.org/abs/2505.02835"}
{"created":"2025-05-06","title":"Scenethesis: A Language and Vision Agentic Framework for 3D Scene Generation","abstract":"Synthesizing interactive 3D scenes from text is essential for gaming, virtual reality, and embodied AI. However, existing methods face several challenges. Learning-based approaches depend on small-scale indoor datasets, limiting the scene diversity and layout complexity. While large language models (LLMs) can leverage diverse text-domain knowledge, they struggle with spatial realism, often producing unnatural object placements that fail to respect common sense. Our key insight is that vision perception can bridge this gap by providing realistic spatial guidance that LLMs lack. To this end, we introduce Scenethesis, a training-free agentic framework that integrates LLM-based scene planning with vision-guided layout refinement. Given a text prompt, Scenethesis first employs an LLM to draft a coarse layout. A vision module then refines it by generating an image guidance and extracting scene structure to capture inter-object relations. Next, an optimization module iteratively enforces accurate pose alignment and physical plausibility, preventing artifacts like object penetration and instability. Finally, a judge module verifies spatial coherence. Comprehensive experiments show that Scenethesis generates diverse, realistic, and physically plausible 3D interactive scenes, making it valuable for virtual content creation, simulation environments, and embodied AI research.","authors":["Lu Ling","Chen-Hsuan Lin","Tsung-Yi Lin","Yifan Ding","Yu Zeng","Yichen Sheng","Yunhao Ge","Ming-Yu Liu","Aniket Bera","Zhaoshuo Li"],"url":"https://arxiv.org/abs/2505.02836"}
{"created":"2025-05-06","title":"LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset","abstract":"Gas leakage poses a significant hazard that requires prevention. Traditionally, human inspection has been used for detection, a slow and labour-intensive process. Recent research has applied machine learning techniques to this problem, yet there remains a shortage of high-quality, publicly available datasets. This paper introduces a synthetic dataset, SimGas, featuring diverse backgrounds, interfering foreground objects, diverse leak locations, and precise segmentation ground truth. We propose a zero-shot method that combines background subtraction, zero-shot object detection, filtering, and segmentation to leverage this dataset. Experimental results indicate that our approach significantly outperforms baseline methods based solely on background subtraction and zero-shot object detection with segmentation, reaching an IoU of 69%. We also present an analysis of various prompt configurations and threshold settings to provide deeper insights into the performance of our method. Finally, we qualitatively (because of the lack of ground truth) tested our performance on GasVid and reached decent results on the real-world dataset. The dataset, code, and full qualitative results are available at https://github.com/weathon/Lang-Gas.","authors":["Wenqi Guo","Yiyang Du","Shan Du"],"url":"https://arxiv.org/abs/2503.02910"}
{"created":"2025-05-06","title":"Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations","abstract":"The rapid adoption of AI-powered applications demands high-performance, scalable, and efficient cloud database solutions, as traditional architectures often struggle with AI-driven workloads requiring real-time data access, vector search, and low-latency queries. This paper explores how cloud-native databases enable AI-driven applications by leveraging purpose-built technologies such as vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores (Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and PostgreSQL). It presents architectural patterns for integrating AI workloads with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with LLMs, real-time data pipelines, AI-driven query optimization, and embeddings-based search. Performance benchmarks, scalability considerations, and cost-efficient strategies are evaluated to guide the design of AI-enabled applications. Real-world case studies from industries such as healthcare, finance, and customer experience illustrate how enterprises utilize cloud databases to enhance AI capabilities while ensuring security, governance, and compliance with enterprise and regulatory standards. By providing a comprehensive analysis of AI and cloud database integration, this paper serves as a practical guide for researchers, architects, and enterprises to build next-generation AI applications that optimize performance, scalability, and cost efficiency in cloud environments.","authors":["Santosh Bhupathi"],"url":"https://arxiv.org/abs/2504.18793"}
{"created":"2025-05-06","title":"Enhancing TCR-Peptide Interaction Prediction with Pretrained Language Models and Molecular Representations","abstract":"Understanding the binding specificity between T-cell receptors (TCRs) and peptide-major histocompatibility complexes (pMHCs) is central to immunotherapy and vaccine development. However, current predictive models struggle with generalization, especially in data-scarce settings and when faced with novel epitopes. We present LANTERN (Large lAnguage model-powered TCR-Enhanced Recognition Network), a deep learning framework that combines large-scale protein language models with chemical representations of peptides. By encoding TCR \\b{eta}-chain sequences using ESM-1b and transforming peptide sequences into SMILES strings processed by MolFormer, LANTERN captures rich biological and chemical features critical for TCR-peptide recognition. Through extensive benchmarking against existing models such as ChemBERTa, TITAN, and NetTCR, LANTERN demonstrates superior performance, particularly in zero-shot and few-shot learning scenarios. Our model also benefits from a robust negative sampling strategy and shows significant clustering improvements via embedding analysis. These results highlight the potential of LANTERN to advance TCR-pMHC binding prediction and support the development of personalized immunotherapies.","authors":["Cong Qi","Hanzhang Fang","Siqi jiang","Tianxing Hu","Wei Zhi"],"url":"https://arxiv.org/abs/2505.01433"}
{"created":"2025-05-06","title":"Supervisory Control of a Flexible Manufacturing Unit for the Production of Two Products","abstract":"In this diploma thesis, the mathematical model of a multi-product manufacturing unit will be presented. The unit consists of a set of three conveyors, a robot, a lathe, a milling machine, an assembly machine, and a painting machine. Finally, the connection of the above elements is carried out via one slot buffer. The products can be divided into two distinct sets according to the route they will follow through the machines of the unit. The mathematical models of the individual subsystems of the plant using finite deterministic automata will be presented. The two different routes that the products can follow will be presented. The desired product flow will be presented in the form of desired regular languages. The properties of the desired languages with respect the overall automaton of the system will be investigated. A supervisory architecture will be designed based on the desired regular languages.","authors":["Kyriakos Giagiakos"],"url":"https://arxiv.org/abs/2505.01434"}
{"created":"2025-05-06","title":"Seasonal Prediction with Neural GCM and Simplified Boundary Forcings: Large-scale Atmospheric Variability and Tropical Cyclone Activity","abstract":"Machine learning (ML) models are successful with weather forecasting and have shown progress in climate simulations, yet leveraging them for useful climate predictions needs exploration. Here we show this feasibility using NeuralGCM, a hybrid ML-physics atmospheric model, for seasonal predictions of large-scale atmospheric variability and Northern Hemisphere tropical cyclone (TC) activity. Inspired by physical model studies, we simplify boundary conditions, assuming sea surface temperature (SST) and sea ice follow their climatological cycle but persist anomalies present at initialization. With such forcings, NeuralGCM simulates realistic atmospheric circulation and TC climatology patterns. Furthermore, this configuration yields useful seasonal predictions (July-November) for the tropical atmosphere and various TC activity metrics. Notably, the prediction skill for TC frequency in the North Atlantic and East Pacific basins is comparable to existing physical models. These findings highlight the promise of leveraging ML models with physical insights to model TC risks and deliver seamless weather-climate predictions.","authors":["Gan Zhang","Megha Rao","Janni Yuval","Ming Zhao"],"url":"https://arxiv.org/abs/2505.01455"}
{"created":"2025-05-06","title":"CostFilter-AD: Enhancing Anomaly Detection through Matching Cost Filtering","abstract":"Unsupervised anomaly detection (UAD) seeks to localize the anomaly mask of an input image with respect to normal samples. Either by reconstructing normal counterparts (reconstruction-based) or by learning an image feature embedding space (embedding-based), existing approaches fundamentally rely on image-level or feature-level matching to derive anomaly scores. Often, such a matching process is inaccurate yet overlooked, leading to sub-optimal detection. To address this issue, we introduce the concept of cost filtering, borrowed from classical matching tasks, such as depth and flow estimation, into the UAD problem. We call this approach {\\em CostFilter-AD}. Specifically, we first construct a matching cost volume between the input and normal samples, comprising two spatial dimensions and one matching dimension that encodes potential matches. To refine this, we propose a cost volume filtering network, guided by the input observation as an attention query across multiple feature layers, which effectively suppresses matching noise while preserving edge structures and capturing subtle anomalies. Designed as a generic post-processing plug-in, CostFilter-AD can be integrated with either reconstruction-based or embedding-based methods. Extensive experiments on MVTec-AD and VisA benchmarks validate the generic benefits of CostFilter-AD for both single- and multi-class UAD tasks. Code and models will be released at https://github.com/ZHE-SAPI/CostFilter-AD.","authors":["Zhe Zhang","Mingxiu Cai","Hanxiao Wang","Gaochang Wu","Tianyou Chai","Xiatian Zhu"],"url":"https://arxiv.org/abs/2505.01476"}
{"created":"2025-05-06","title":"Nystr\\\"om Type Exponential Integrators for Strongly Magnetized Charged Particle Dynamics","abstract":"Calculating the dynamics of charged particles in electromagnetic fields (i.e. the particle pushing problem) is one of the most computationally intensive components of particle-in-cell (PIC) methods for plasma physics simulations. This task is especially challenging when the plasma is strongly magnetized, since in this case the particle motion consists of a wide range of temporal scales from highly oscillatory fast gyromotion to slow macroscopic behavior and the resulting numerical model is very stiff. Current state-of-the-art time integrators used to simulate particle motion have limitations given the severe numerical stiffness of the problem and more efficient methods are of interest. Recently, exponential integrators have been proposed as a promising new approach for these simulations and shown to offer computational advantages over commonly used schemes. Exponential methods can solve linear problems exactly and are A-stable. In this paper, the standard exponential algorithms framework is extended to derive Nystr\\\"om-type exponential methods that integrate the Newtonian equations of motion as a second-order differential equation. Specific Nystr\\\"om-type schemes of second and third orders are derived and applied to strongly magnetized particle pushing problems. Numerical experiments are presented to demonstrate that the Nystr\\\"om-type exponential integrators can provide significant improvement in computational efficiency over the standard exponential methods.","authors":["Tri P. Nguyen","Ilon Joseph","Mayya Tokman"],"url":"https://arxiv.org/abs/2505.01525"}
{"created":"2025-05-06","title":"An Adaptive Framework for Autoregressive Forecasting in CFD Using Hybrid Modal Decomposition and Deep Learning","abstract":"This work presents, to the best of the authors' knowledge, the first generalizable and fully data-driven adaptive framework designed to stabilize deep learning (DL) autoregressive forecasting models over long time horizons, with the goal of reducing the computational cost required in computational fluid dynamics (CFD) simulations.The proposed methodology alternates between two phases: (i) predicting the evolution of the flow field over a selected time interval using a trained DL model, and (ii) updating the model with newly generated CFD data when stability degrades, thus maintaining accurate long-term forecasting. This adaptive retraining strategy ensures robustness while avoiding the accumulation of predictive errors typical in autoregressive models. The framework is validated across three increasingly complex flow regimes, from laminar to turbulent, demonstrating from 30 \\% to 95 \\% reduction in computational cost without compromising physical consistency or accuracy. Its entirely data-driven nature makes it easily adaptable to a wide range of time-dependent simulation problems. The code implementing this methodology is available as open-source and it will be integrated into the upcoming release of the ModelFLOWs-app.","authors":["Rodrigo Abad\\'ia-Heredia","Manuel Lopez-Martin","Soledad Le Clainche"],"url":"https://arxiv.org/abs/2505.01531"}
{"created":"2025-05-06","title":"Bilateral base-extension semantics","abstract":"Bilateralism is the position according to which assertion and rejection are conceptually independent speech acts. Logical bilateralism demands that systems of logic provide conditions for assertion and rejection that are not reducible to each other, which often leads to independent definitions of proof rules (for assertion) and dual proof rules, also called refutation rules (for rejection). Since it provides a critical account of what it means for something to be a proof or a refutation, bilateralism is often studied in the context of proof-theoretic semantics, an approach that aims to elucidate both the meaning of proofs (and refutations) and what kinds of semantics can be given if proofs (and refutations) are considered as basic semantic notions. The recent literature on bilateral proof-theoretic semantics has only dealt with the semantics of proofs and refutations, whereas we deal with semantics in terms of proofs and refutations. In this paper we present a bilateral version of base-extension semantics - one of the most widely studied proof-theoretic semantics - by allowing atomic bases to contain both atomic proof rules and atomic refutation rules. The semantics is shown to be sound and complete with respect to the bilateral dual intuitionistic logic 2Int. Structural similarities between atomic proofs and refutations also allow us to define duality notions for atomic rules, deductions and bases, which may then be used for the proof of bilateral semantic harmony results. Aside from enabling embeddings between different fragments of the language, bilateral semantic harmony is shown to be a restatement of the syntactic horizontal inversion principle, whose meaning-conferring character may now be interpreted as the requirement of preservation of harmony notions already present at the core of the semantics by inferences.","authors":["Victor Barroso-Nascimento","Maria Os\\'orio"],"url":"https://arxiv.org/abs/2505.01593"}
{"created":"2025-05-06","title":"Quantum-Assisted Vehicle Routing: Realizing QAOA-based Approach on Gate-Based Quantum Computer","abstract":"The Vehicle Routing Problem (VRP) is a crucial optimization challenge with significant economic and environmental implications, particularly in logistics and transportation planning. While classical algorithms struggle to efficiently solve large-scale instances of VRP due to its combinatorial complexity, quantum computing presents a promising alternative for tackling such problems. In this work, we explore the application of the Quantum Approximate Optimization Algorithm (QAOA) to solve instances of VRP, analyzing its effectiveness and scalability. We formulate VRP as a Quadratic Unconstrained Binary Optimization (QUBO) problem by encoding the constraints into a single cost function suitable for QAOA. Our study investigates the impact of problem size on quantum circuit complexity and evaluate the feasibility of executing QAOA-based VRP solutions on near-term quantum hardware. The results indicate that while QAOA demonstrates potential for solving VRP, the primary limitation lies in circuit depth and noise-induced errors, which critically affect performance on current quantum processors. Overcoming these challenges will require advancements in error mitigation techniques and more efficient quantum circuit designs to realize the full potential of quantum computing for combinatorial optimization.","authors":["Talha Azfar","Ruimin Ke","Osama Muhammad Raisuddin","Jose Holguin-Veras"],"url":"https://arxiv.org/abs/2505.01614"}
{"created":"2025-05-06","title":"Divide-and-Conquer Simulation of Open Quantum Systems","abstract":"One of the promises of quantum computing is to simulate physical systems efficiently. However, the simulation of open quantum systems - where interactions with the environment play a crucial role - remains challenging for quantum computing, as it is impossible to implement deterministically non-unitary operators on a quantum computer without auxiliary qubits. The Stinespring dilation can simulate an open dynamic but requires a high circuit depth, which is impractical for NISQ devices. An alternative approach is parallel probabilistic block-encoding methods, such as the Sz.-Nagy and Singular Value Decomposition dilations. These methods result in shallower circuits but are hybrid methods, and we do not simulate the quantum dynamic on the quantum computer. In this work, we describe a divide-and-conquer strategy for preparing mixed states to combine the output of each Kraus operator dilation and obtain the complete dynamic on quantum hardware with a lower circuit depth. The work also introduces a balanced strategy that groups the original Kraus operators into an expanded operator, leading to a trade-off between circuit depth, CNOT count, and number of qubits. We perform a computational analysis to demonstrate the advantages of the new method and present a proof-of-concept simulation of the Fenna-Matthews-Olson dynamic on current quantum hardware.","authors":["Thiago Melo D. Azevedo","Caio Almeida","Pedro Linck","Adenilton J. da Silva","Nadja K. Bernardes"],"url":"https://arxiv.org/abs/2505.01623"}
{"created":"2025-05-06","title":"Transfer Learning-Based Deep Residual Learning for Speech Recognition in Clean and Noisy Environments","abstract":"Addressing the detrimental impact of non-stationary environmental noise on automatic speech recognition (ASR) has been a persistent and significant research focus. Despite advancements, this challenge continues to be a major concern. Recently, data-driven supervised approaches, such as deep neural networks, have emerged as promising alternatives to traditional unsupervised methods. With extensive training, these approaches have the potential to overcome the challenges posed by diverse real-life acoustic environments. In this light, this paper introduces a novel neural framework that incorporates a robust frontend into ASR systems in both clean and noisy environments. Utilizing the Aurora-2 speech database, the authors evaluate the effectiveness of an acoustic feature set for Mel-frequency, employing the approach of transfer learning based on Residual neural network (ResNet). The experimental results demonstrate a significant improvement in recognition accuracy compared to convolutional neural networks (CNN) and long short-term memory (LSTM) networks. They achieved accuracies of 98.94% in clean and 91.21% in noisy mode.","authors":["Noussaiba Djeffal","Djamel Addou","Hamza Kheddar","Sid Ahmed Selouani"],"url":"https://arxiv.org/abs/2505.01632"}
{"created":"2025-05-06","title":"Seeing Heat with Color -- RGB-Only Wildfire Temperature Inference from SAM-Guided Multimodal Distillation using Radiometric Ground Truth","abstract":"High-fidelity wildfire monitoring using Unmanned Aerial Vehicles (UAVs) typically requires multimodal sensing - especially RGB and thermal imagery - which increases hardware cost and power consumption. This paper introduces SAM-TIFF, a novel teacher-student distillation framework for pixel-level wildfire temperature prediction and segmentation using RGB input only. A multimodal teacher network trained on paired RGB-Thermal imagery and radiometric TIFF ground truth distills knowledge to a unimodal RGB student network, enabling thermal-sensor-free inference. Segmentation supervision is generated using a hybrid approach of segment anything (SAM)-guided mask generation, and selection via TOPSIS, along with Canny edge detection and Otsu's thresholding pipeline for automatic point prompt selection. Our method is the first to perform per-pixel temperature regression from RGB UAV data, demonstrating strong generalization on the recent FLAME 3 dataset. This work lays the foundation for lightweight, cost-effective UAV-based wildfire monitoring systems without thermal sensors.","authors":["Michael Marinaccio","Fatemeh Afghah"],"url":"https://arxiv.org/abs/2505.01638"}
{"created":"2025-05-06","title":"Fast Likelihood-Free Parameter Estimation for L\\'evy Processes","abstract":"L\\'evy processes are widely used in financial modeling due to their ability to capture discontinuities and heavy tails, which are common in high-frequency asset return data. However, parameter estimation remains a challenge when associated likelihoods are unavailable or costly to compute. We propose a fast and accurate method for L\\'evy parameter estimation using the neural Bayes estimation (NBE) framework -- a simulation-based, likelihood-free approach that leverages permutation-invariant neural networks to approximate Bayes estimators. Through extensive simulations across several L\\'evy models, we show that NBE outperforms traditional methods in both accuracy and runtime, while also enabling rapid bootstrap-based uncertainty quantification. We illustrate our approach on a challenging high-frequency cryptocurrency return dataset, where the method captures evolving parameter dynamics and delivers reliable and interpretable inference at a fraction of the computational cost of traditional methods. NBE provides a scalable and practical solution for inference in complex financial models, enabling parameter estimation and uncertainty quantification over an entire year of data in just seconds. We additionally investigate nearly a decade of high-frequency Bitcoin returns, requiring less than one minute to estimate parameters under the proposed approach.","authors":["Nicolas Coloma","William Kleiber"],"url":"https://arxiv.org/abs/2505.01639"}
{"created":"2025-05-06","title":"Identifying Doppelganger Active Galactic Nuclei across redshifts from spectroscopic surveys","abstract":"Active Galactic Nuclei (AGNs) are among the most luminous objects in the universe, making them valuable probes for studying galaxy evolution. However, understanding how AGN properties evolve over cosmic time remains a fundamental challenge. This study investigates whether AGNs at low redshift (nearby) can serve as proxies for their high-redshift (distant) counterparts by identifying spectral 'doppelg\\\"angers', AGNs with remarkably similar emission line properties despite being separated by vast cosmic distances. We analyze key spectral features of bona fide AGNs using the Sloan Digital Sky Survey's Data Release 16, including continuum and emission lines: Nitrogen (N V), Carbon (C IV), Magnesium (Mg II), Hydrogen-beta (H$\\beta$), and Iron (Fe II - optical and UV) emission lines. We incorporated properties such as equivalent width, velocity dispersion in the form of full width at half maximum (FWHM), and continuum luminosities (135nm, 300nm, and 510nm) closest to these prominent lines. Our initial findings suggest the existence of multiple AGNs with highly similar spectra, hinting at the possibility that local AGNs may indeed share intrinsic properties with high-redshift ones. We showcase here one of the better candidate pairs of AGNs resulting from our analyses.","authors":["Shreya Sareen","Swayamtrupta Panda"],"url":"https://arxiv.org/abs/2505.01642"}
{"created":"2025-05-06","title":"A Dual-Task Synergy-Driven Generalization Framework for Pancreatic Cancer Segmentation in CT Scans","abstract":"Pancreatic cancer, characterized by its notable prevalence and mortality rates, demands accurate lesion delineation for effective diagnosis and therapeutic interventions. The generalizability of extant methods is frequently compromised due to the pronounced variability in imaging and the heterogeneous characteristics of pancreatic lesions, which may mimic normal tissues and exhibit significant inter-patient variability. Thus, we propose a generalization framework that synergizes pixel-level classification and regression tasks, to accurately delineate lesions and improve model stability. This framework not only seeks to align segmentation contours with actual lesions but also uses regression to elucidate spatial relationships between diseased and normal tissues, thereby improving tumor localization and morphological characterization. Enhanced by the reciprocal transformation of task outputs, our approach integrates additional regression supervision within the segmentation context, bolstering the model's generalization ability from a dual-task perspective. Besides, dual self-supervised learning in feature spaces and output spaces augments the model's representational capability and stability across different imaging views. Experiments on 594 samples composed of three datasets with significant imaging differences demonstrate that our generalized pancreas segmentation results comparable to mainstream in-domain validation performance (Dice: 84.07%). More importantly, it successfully improves the results of the highly challenging cross-lesion generalized pancreatic cancer segmentation task by 9.51%. Thus, our model constitutes a resilient and efficient foundational technological support for pancreatic disease management and wider medical applications. The codes will be released at https://github.com/SJTUBME-QianLab/Dual-Task-Seg.","authors":["Jun Li","Yijue Zhang","Haibo Shi","Minhong Li","Qiwei Li","Xiaohua Qian"],"url":"https://arxiv.org/abs/2505.01644"}
{"created":"2025-05-06","title":"Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations","abstract":"This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that the brain signals of the subjects can be aligned in this common space during training to form a semantically aligned common brain. This is leveraged to demonstrate that aligning subject-specific lightweight modules to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios. We evaluate our methods on different datasets, demonstrating that the common space is subject and dataset-agnostic.","authors":["Christos Zangos","Danish Ebadulla","Thomas Christopher Sprague","Ambuj Singh"],"url":"https://arxiv.org/abs/2505.01670"}
{"created":"2025-05-06","title":"A brain-inspired generative model for EEG-based cognitive state identification","abstract":"This article proposes a brain-inspired generative (BIG) model that merges an impulsive-attention neural network and a variational autoencoder (VAE) for identifying cognitive states based on electroencephalography (EEG) data. A hybrid learning method is presented for training the model by integrating gradient-based learning and heteroassociative memory. The BIG model is capable of achieving multi-task objectives: classification, generating new EEG, and brain network interpretation, alleviating the limitations of excessive data training and high computational cost in conventional approaches. Experimental results on two public EEG datasets demonstrate that the BIG model achieves a classification accuracy above 89\\%, comparable with state-of-the-art methods, while reducing computational cost by nearly 11\\% over the baseline EEGNet. Incorporating the generated EEG data for training, the BIG model exhibits comparative performance in a few-shot pattern.Ablation studies justify the poised brain-inspired characteristic regarding the impulsive-attention module and the hybrid learning method. Thanks to the performance advantages with interpretable outputs, this BIG model has application potential for building digital twins of the brain.","authors":["Bin Hu","Zhi-Hong Guan"],"url":"https://arxiv.org/abs/2505.01685"}
{"created":"2025-05-06","title":"Speculative Evolution Through 3D Cellular Automata","abstract":"This project explores speculative evolution through a 3D implementation of Conway's Game of Life, using procedural simulation to generate unfamiliar extraterrestrial organic forms. By applying a volumetric optimized workflow, the raw cellular structures are smoothed into unified, bone-like geometries that resemble hypothetical non-terrestrial morphologies. The resulting forms, strange yet organic, are 3D printed as fossil-like artifacts, presenting a tangible representation of generative structures. This process situates the work at the intersection of artificial life, evolutionary modeling, and digital fabrication, illustrating how simple rules can simulate complex biological emergence and challenge conventional notions of organic form.","authors":["Amir Hossein Khazaei"],"url":"https://arxiv.org/abs/2505.01692"}
{"created":"2025-05-06","title":"Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking","abstract":"Integrating heterogeneous biomedical data including imaging, omics, and clinical records supports accurate diagnosis and personalised care. Graph-based models fuse such non-Euclidean data by capturing spatial and relational structure, yet clinical uptake requires regulator-ready interpretability. We present the first technical survey of interpretable graph based models for multimodal biomedical data, covering 26 studies published between Jan 2019 and Sep 2024. Most target disease classification, notably cancer and rely on static graphs from simple similarity measures, while graph-native explainers are rare; post-hoc methods adapted from non-graph domains such as gradient saliency, and SHAP predominate. We group existing approaches into four interpretability families, outline trends such as graph-in-graph hierarchies, knowledge-graph edges, and dynamic topology learning, and perform a practical benchmark. Using an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient Saliency and Graph Masking surface complementary metabolic and transport signatures. Permutation tests show all four beat random gene sets, but with distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher compute cost, while Gradient Saliency and Sensitivity Analysis are quicker though coarser. We also provide a step-by-step flowchart covering graph construction, explainer choice and resource budgeting to help researchers balance transparency and performance. This review synthesises the state of interpretable graph learning for multimodal medicine, benchmarks leading techniques, and charts future directions, from advanced XAI tools to under-studied diseases, serving as a concise reference for method developers and translational scientists.","authors":["Alireza Sadeghi","Farshid Hajati","Ahmadreza Argha","Nigel H Lovell","Min Yang","Hamid Alinejad-Rokny"],"url":"https://arxiv.org/abs/2505.01696"}
{"created":"2025-05-06","title":"Real-Time, Single-Ear, Wearable ECG Reconstruction, R-Peak Detection, and HR/HRV Monitoring","abstract":"Biosignal monitoring, in particular heart activity through heart rate (HR) and heart rate variability (HRV) tracking, is vital in enabling continuous, non-invasive tracking of physiological and cognitive states. Recent studies have explored compact, head-worn devices for HR and HRV monitoring to improve usability and reduce stigma. However, this approach is challenged by the current reliance on wet electrodes, which limits usability, the weakness of ear-derived signals, making HR/HRV extraction more complex, and the incompatibility of current algorithms for embedded deployment. This work introduces a single-ear wearable system for real-time ECG (Electrocardiogram) parameter estimation, which directly runs on BioGAP, an energy-efficient device for biosignal acquisition and processing. By combining SoA in-ear electrode technology, an optimized DeepMF algorithm, and BioGAP, our proposed subject-independent approach allows for robust extraction of HR/HRV parameters directly on the device with just 36.7 uJ/inference at comparable performance with respect to the current state-of-the-art architecture, achieving 0.49 bpm and 25.82 ms for HR/HRV mean errors, respectively and an estimated battery life of 36h with a total system power consumption of 7.6 mW. Clinical relevance: The ability to reconstruct ECG signals and extract HR and HRV paves the way for continuous, unobtrusive cardiovascular monitoring with head-worn devices. In particular, the integration of cardiovascular measurements in everyday-use devices (such as earbuds) has potential in continuous at-home monitoring to enable early detection of cardiovascular irregularities.","authors":["Carlos Santos","Sebastian Frey","Andrea Cossettini","Luca Benini","Victor Kartsch"],"url":"https://arxiv.org/abs/2505.01738"}
{"created":"2025-05-06","title":"CLOG-CD: Curriculum Learning based on Oscillating Granularity of Class Decomposed Medical Image Classification","abstract":"Curriculum learning strategies have been proven to be effective in various applications and have gained significant interest in the field of machine learning. It has the ability to improve the final model's performance and accelerate the training process. However, in the medical imaging domain, data irregularities can make the recognition task more challenging and usually result in misclassification between the different classes in the dataset. Class-decomposition approaches have shown promising results in solving such a problem by learning the boundaries within the classes of the data set. In this paper, we present a novel convolutional neural network (CNN) training method based on the curriculum learning strategy and the class decomposition approach, which we call CLOG-CD, to improve the performance of medical image classification. We evaluated our method on four different imbalanced medical image datasets, such as Chest X-ray (CXR), brain tumour, digital knee X-ray, and histopathology colorectal cancer (CRC). CLOG-CD utilises the learnt weights from the decomposition granularity of the classes, and the training is accomplished from descending to ascending order (i.e., anti-curriculum technique). We also investigated the classification performance of our proposed method based on different acceleration factors and pace function curricula. We used two pre-trained networks, ResNet-50 and DenseNet-121, as the backbone for CLOG-CD. The results with ResNet-50 show that CLOG-CD has the ability to improve classification performance with an accuracy of 96.08% for the CXR dataset, 96.91% for the brain tumour dataset, 79.76% for the digital knee X-ray, and 99.17% for the CRC dataset, compared to other training strategies. In addition, with DenseNet-121, CLOG-CD has achieved 94.86%, 94.63%, 76.19%, and 99.45% for CXR, brain tumour, digital knee X-ray, and CRC datasets, respectively","authors":["Asmaa Abbas","Mohamed Gaber","Mohammed M. Abdelsamea"],"url":"https://arxiv.org/abs/2505.01741"}
{"created":"2025-05-06","title":"Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs","abstract":"Neural image compression, necessary in various machine-to-machine communication scenarios, suffers from its heavy encode-decode structures and inflexibility in switching between different compression levels. Consequently, it raises significant challenges in applying the neural image compression to edge devices that are developed for powerful servers with high computational and storage capacities. We take a step to solve the challenges by proposing a new transformer-based edge-compute-free image coding framework called Easz. Easz shifts the computational overhead to the server, and hence avoids the heavy encoding and model switching overhead on the edge. Easz utilizes a patch-erase algorithm to selectively remove image contents using a conditional uniform-based sampler. The erased pixels are reconstructed on the receiver side through a transformer-based framework. To further reduce the computational overhead on the receiver, we then introduce a lightweight transformer-based reconstruction structure to reduce the reconstruction load on the receiver side. Extensive evaluations conducted on a real-world testbed demonstrate multiple advantages of Easz over existing compression approaches, in terms of adaptability to different compression levels, computational efficiency, and image reconstruction quality.","authors":["Yu Mao","Jingzong Li","Jun Wang","Hong Xu","Tei-Wei Kuo","Nan Guan","Chun Jason Xue"],"url":"https://arxiv.org/abs/2505.01742"}
{"created":"2025-05-06","title":"Low-Complexity Acoustic Scene Classification with Device Information in the DCASE 2025 Challenge","abstract":"This paper presents the Low-Complexity Acoustic Scene Classification with Device Information Task of the DCASE 2025 Challenge and its baseline system. Continuing the focus on low-complexity models, data efficiency, and device mismatch from previous editions (2022--2024), this year's task introduces a key change: recording device information is now provided at inference time. This enables the development of device-specific models that leverage device characteristics -- reflecting real-world deployment scenarios in which a model is designed with awareness of the underlying hardware. The training set matches the 25% subset used in the corresponding DCASE 2024 challenge, with no restrictions on external data use, highlighting transfer learning as a central topic. The baseline achieves 50.72% accuracy on this ten-class problem with a device-general model, improving to 51.89% when using the available device information.","authors":["Florian Schmid","Paul Primus","Toni Heittola","Annamaria Mesaros","Irene Mart\\'in-Morat\\'o","Gerhard Widmer"],"url":"https://arxiv.org/abs/2505.01747"}
{"created":"2025-05-06","title":"FLOWER: Flow-Based Estimated Gaussian Guidance for General Speech Restoration","abstract":"We introduce FLOWER, a novel conditioning method designed for speech restoration that integrates Gaussian guidance into generative frameworks. By transforming clean speech into a predefined prior distribution (e.g., Gaussian distribution) using a normalizing flow network, FLOWER extracts critical information to guide generative models. This guidance is incorporated into each block of the generative network, enabling precise restoration control. Experimental results demonstrate the effectiveness of FLOWER in improving performance across various general speech restoration tasks.","authors":["Da-Hee Yang","Jaeuk Lee","Joon-Hyuk Chang"],"url":"https://arxiv.org/abs/2505.01750"}
{"created":"2025-05-06","title":"A dynamic view of the double descent","abstract":"It has been observed by Belkin et al.\\ that overparametrized neural networks exhibit a `double descent' phenomenon. That is, as the model complexity, as reflected in the number of features, increases, the training error initially decreases, then increases, and then decreases again. A counterpart of this phenomenon in the time domain has been noted in the context of epoch-wise training, viz., that the training error decreases with time, then increases, then decreases again. This note presents a plausible explanation for this phenomenon by using the theory of two time scale stochastic approximation and singularly perturbed differential equations, applied to the continuous time limit of the gradient dynamics. This adds a `dynamic' angle to an already well studied theme.","authors":["Vivek Shripad Borkar"],"url":"https://arxiv.org/abs/2505.01751"}
{"created":"2025-05-06","title":"LensNet: An End-to-End Learning Framework for Empirical Point Spread Function Modeling and Lensless Imaging Reconstruction","abstract":"Lensless imaging stands out as a promising alternative to conventional lens-based systems, particularly in scenarios demanding ultracompact form factors and cost-effective architectures. However, such systems are fundamentally governed by the Point Spread Function (PSF), which dictates how a point source contributes to the final captured signal. Traditional lensless techniques often require explicit calibrations and extensive pre-processing, relying on static or approximate PSF models. These rigid strategies can result in limited adaptability to real-world challenges, including noise, system imperfections, and dynamic scene variations, thus impeding high-fidelity reconstruction. In this paper, we propose LensNet, an end-to-end deep learning framework that integrates spatial-domain and frequency-domain representations in a unified pipeline. Central to our approach is a learnable Coded Mask Simulator (CMS) that enables dynamic, data-driven estimation of the PSF during training, effectively mitigating the shortcomings of fixed or sparsely calibrated kernels. By embedding a Wiener filtering component, LensNet refines global structure and restores fine-scale details, thus alleviating the dependency on multiple handcrafted pre-processing steps. Extensive experiments demonstrate LensNet's robust performance and superior reconstruction quality compared to state-of-the-art methods, particularly in preserving high-frequency details and attenuating noise. The proposed framework establishes a novel convergence between physics-based modeling and data-driven learning, paving the way for more accurate, flexible, and practical lensless imaging solutions for applications ranging from miniature sensors to medical diagnostics. The link of code is https://github.com/baijiesong/Lensnet.","authors":["Jiesong Bai","Yuhao Yin","Yihang Dong","Xiaofeng Zhang","Chi-Man Pun","Xuhang Chen"],"url":"https://arxiv.org/abs/2505.01755"}
{"created":"2025-05-06","title":"Quantum Speedup for Hypergraph Sparsification","abstract":"Graph sparsification serves as a foundation for many algorithms, such as approximation algorithms for graph cuts and Laplacian system solvers. As its natural generalization, hypergraph sparsification has recently gained increasing attention, with broad applications in graph machine learning and other areas. In this work, we propose the first quantum algorithm for hypergraph sparsification, addressing an open problem proposed by Apers and de Wolf (FOCS'20). For a weighted hypergraph with $n$ vertices, $m$ hyperedges, and rank $r$, our algorithm outputs a near-linear size $\\varepsilon$-spectral sparsifier in time $\\widetilde O(r\\sqrt{mn}/\\varepsilon)$. This algorithm matches the quantum lower bound for constant $r$ and demonstrates quantum speedup when compared with the state-of-the-art $\\widetilde O(mr)$-time classical algorithm. As applications, our algorithm implies quantum speedups for computing hypergraph cut sparsifiers, approximating hypergraph mincuts and hypergraph $s$-$t$ mincuts.","authors":["Chenghua Liu","Minbo Gao","Zhengfeng Ji","Mingsheng Ying"],"url":"https://arxiv.org/abs/2505.01763"}
{"created":"2025-05-06","title":"Continuous Filtered Backprojection by Learnable Interpolation Network","abstract":"Accurate reconstruction of computed tomography (CT) images is crucial in medical imaging field. However, there are unavoidable interpolation errors in the backprojection step of the conventional reconstruction methods, i.e., filtered-back-projection based methods, which are detrimental to the accurate reconstruction. In this study, to address this issue, we propose a novel deep learning model, named Leanable-Interpolation-based FBP or LInFBP shortly, to enhance the reconstructed CT image quality, which achieves learnable interpolation in the backprojection step of filtered backprojection (FBP) and alleviates the interpolation errors. Specifically, in the proposed LInFBP, we formulate every local piece of the latent continuous function of discrete sinogram data as a linear combination of selected basis functions, and learn this continuous function by exploiting a deep network to predict the linear combination coefficients. Then, the learned latent continuous function is exploited for interpolation in backprojection step, which first time takes the advantage of deep learning for the interpolation in FBP. Extensive experiments, which encompass diverse CT scenarios, demonstrate the effectiveness of the proposed LInFBP in terms of enhanced reconstructed image quality, plug-and-play ability and generalization capability.","authors":["Hui Lin","Dong Zeng","Qi Xie","Zerui Mao","Jianhua Ma","Deyu Meng"],"url":"https://arxiv.org/abs/2505.01768"}
{"created":"2025-05-06","title":"Polar Interpolants for Thin-Shell Microstructure Homogenization","abstract":"This paper introduces a new formulation for material homogenization of thin-shell microstructures. It addresses important challenges that limit the quality of previous approaches: methods that fit the energy response neglect visual impact, methods that fit the stress response are not conservative, and all of them are limited to a low-dimensional interplay between deformation modes. The new formulation is rooted on the following design principles: the material energy functions are conservative by definition, they are formulated on the high-dimensional membrane and bending domain to capture the complex interplay of the different deformation modes, the material function domain is maximally aligned with the training data, and the material parameters and the optimization are formulated on stress instead of energy for better correlation with visual impact. The key novelty of our formulation is a new type of high-order RBF interpolant for polar coordinates, which allows us to fulfill all the design principles. We design a material function using this novel interpolant, as well as an overall homogenization workflow. Our results demonstrate very accurate fitting of diverse microstructure behaviors, both quantitatively and qualitatively superior to previous work.","authors":["Antoine Chan-Lock","Miguel Otaduy"],"url":"https://arxiv.org/abs/2505.01779"}
{"created":"2025-05-06","title":"Rate-Limited Closed-Loop Distributed ISAC Systems: An Autoencoder Approach","abstract":"In closed-loop distributed multi-sensor integrated sensing and communication (ISAC) systems, performance often hinges on transmitting high-dimensional sensor observations over rate-limited networks. In this paper, we first present a general framework for rate-limited closed-loop distributed ISAC systems, and then propose an autoencoder-based observation compression method to overcome the constraints imposed by limited transmission capacity. Building on this framework, we conduct a case study using a closed-loop linear quadratic regulator (LQR) system to analyze how the interplay among observation, compression, and state dimensions affects reconstruction accuracy, state estimation error, and control performance. In multi-sensor scenarios, our results further show that optimal resource allocation initially prioritizes low-noise sensors until the compression becomes lossless, after which resources are reallocated to high-noise sensors.","authors":["Guangjin Pan","Zhixing Li","Ay\\c{c}a \\\"Oz\\c{c}elikkale","Christian H\\\"ager","Musa Furkan Keskin","Henk Wymeersch"],"url":"https://arxiv.org/abs/2505.01780"}
{"created":"2025-05-06","title":"TV-SurvCaus: Dynamic Representation Balancing for Causal Survival Analysis","abstract":"Estimating the causal effect of time-varying treatments on survival outcomes is a challenging task in many domains, particularly in medicine where treatment protocols adapt over time. While recent advances in representation learning have improved causal inference for static treatments, extending these methods to dynamic treatment regimes with survival outcomes remains under-explored. In this paper, we introduce TV-SurvCaus, a novel framework that extends representation balancing techniques to the time-varying treatment setting for survival analysis. We provide theoretical guarantees through (1) a generalized bound for time-varying precision in estimation of heterogeneous effects, (2) variance control via sequential balancing weights, (3) consistency results for dynamic treatment regimes, (4) convergence rates for representation learning with temporal dependencies, and (5) a formal bound on the bias due to treatment-confounder feedback. Our neural architecture incorporates sequence modeling to handle temporal dependencies while balancing time-dependent representations. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that TV-SurvCaus outperforms existing methods in estimating individualized treatment effects with time-varying covariates and treatments. Our framework advances the field of causal inference by enabling more accurate estimation of treatment effects in dynamic, longitudinal settings with survival outcomes.","authors":["Ayoub Abraich"],"url":"https://arxiv.org/abs/2505.01785"}
{"created":"2025-05-06","title":"Switched Systems Control via Discreteness-Promoting Regularization","abstract":"This paper proposes a novel method for designing finite-horizon discrete-valued switching signals in linear switched systems based on discreteness-promoting regularization. The inherent combinatorial optimization problem is reformulated as a continuous optimization problem with a non-convex regularization term that promotes discreteness of the control. We prove that any solution obtained from the relaxed problem is also a solution to the original problem. The resulting non-convex optimization problem is efficiently solved through time discretization. Numerical examples demonstrate the effectiveness of the proposed method.","authors":["Masaaki Nagahara","Takuya Ikeda","Ritsuki Hoshimoto"],"url":"https://arxiv.org/abs/2505.01803"}
{"created":"2025-05-06","title":"Smoothness of the Augmented Lagrangian Dual in Convex Optimization","abstract":"This paper investigates the general linearly constrained optimization problem: $\\min_{x \\in \\R^d} f(x) \\ \\st \\ A x = b$, where $f: \\R^n \\rightarrow \\exs$ is a closed proper convex function, $A \\in \\R^{p \\times d}$, and $b \\in \\R^p$. We establish the following results without requiring additional regularity conditions: (1) the augmented Lagrangian dual function $\\phi_{\\rho}(\\lambda) = \\inf_x \\cL_{\\rho}(x, \\lambda)$ is $\\frac{1}{\\rho}$-smooth everywhere; and (2) the solution to $\\min_{x \\in \\R^d} \\cL_{\\rho}(x, \\lambda)$ exists for any dual variable $\\lambda \\in \\R^p$, where $\\rho > 0$ is the augmented parameter and $\\cL_{\\rho}(x, \\lambda) = f(x) + \\dotprod{\\lambda, A x - b} + \\frac{\\rho}{2}\\norm{A x - b}^2$ is the augmented Lagrangian. These findings significantly relax the strong assumptions commonly imposed in existing literature to guarantee similar properties.","authors":["Jingwang Li","Vincent Lau"],"url":"https://arxiv.org/abs/2505.01824"}
{"created":"2025-05-06","title":"Rank-One Modified Value Iteration","abstract":"In this paper, we provide a novel algorithm for solving planning and learning problems of Markov decision processes. The proposed algorithm follows a policy iteration-type update by using a rank-one approximation of the transition probability matrix in the policy evaluation step. This rank-one approximation is closely related to the stationary distribution of the corresponding transition probability matrix, which is approximated using the power method. We provide theoretical guarantees for the convergence of the proposed algorithm to optimal (action-)value function with the same rate and computational complexity as the value iteration algorithm in the planning problem and as the Q-learning algorithm in the learning problem. Through our extensive numerical simulations, however, we show that the proposed algorithm consistently outperforms first-order algorithms and their accelerated versions for both planning and learning problems.","authors":["Arman Sharifi Kolarijani","Tolga Ok","Peyman Mohajerin Esfahani","Mohamad Amin Sharif Kolarijani"],"url":"https://arxiv.org/abs/2505.01828"}
{"created":"2025-05-06","title":"Multi-Scale Target-Aware Representation Learning for Fundus Image Enhancement","abstract":"High-quality fundus images provide essential anatomical information for clinical screening and ophthalmic disease diagnosis. Yet, due to hardware limitations, operational variability, and patient compliance, fundus images often suffer from low resolution and signal-to-noise ratio. Recent years have witnessed promising progress in fundus image enhancement. However, existing works usually focus on restoring structural details or global characteristics of fundus images, lacking a unified image enhancement framework to recover comprehensive multi-scale information. Moreover, few methods pinpoint the target of image enhancement, e.g., lesions, which is crucial for medical image-based diagnosis. To address these challenges, we propose a multi-scale target-aware representation learning framework (MTRL-FIE) for efficient fundus image enhancement. Specifically, we propose a multi-scale feature encoder (MFE) that employs wavelet decomposition to embed both low-frequency structural information and high-frequency details. Next, we design a structure-preserving hierarchical decoder (SHD) to fuse multi-scale feature embeddings for real fundus image restoration. SHD integrates hierarchical fusion and group attention mechanisms to achieve adaptive feature fusion while retaining local structural smoothness. Meanwhile, a target-aware feature aggregation (TFA) module is used to enhance pathological regions and reduce artifacts. Experimental results on multiple fundus image datasets demonstrate the effectiveness and generalizability of MTRL-FIE for fundus image enhancement. Compared to state-of-the-art methods, MTRL-FIE achieves superior enhancement performance with a more lightweight architecture. Furthermore, our approach generalizes to other ophthalmic image processing tasks without supervised fine-tuning, highlighting its potential for clinical applications.","authors":["Haofan Wu","Yin Huang","Yuqing Wu","Qiuyu Yang","Bingfang Wang","Li Zhang","Muhammad Fahadullah Khan","Ali Zia","M. Saleh Memon","Syed Sohail Bukhari","Abdul Fattah Memon","Daizong Ji","Ya Zhang","Ghulam Mustafa","Yin Fang"],"url":"https://arxiv.org/abs/2505.01831"}
{"created":"2025-05-06","title":"Accelerating Volumetric Medical Image Annotation via Short-Long Memory SAM 2","abstract":"Manual annotation of volumetric medical images, such as magnetic resonance imaging (MRI) and computed tomography (CT), is a labor-intensive and time-consuming process. Recent advancements in foundation models for video object segmentation, such as Segment Anything Model 2 (SAM 2), offer a potential opportunity to significantly speed up the annotation process by manually annotating one or a few slices and then propagating target masks across the entire volume. However, the performance of SAM 2 in this context varies. Our experiments show that relying on a single memory bank and attention module is prone to error propagation, particularly at boundary regions where the target is present in the previous slice but absent in the current one. To address this problem, we propose Short-Long Memory SAM 2 (SLM-SAM 2), a novel architecture that integrates distinct short-term and long-term memory banks with separate attention modules to improve segmentation accuracy. We evaluate SLM-SAM 2 on three public datasets covering organs, bones, and muscles across MRI and CT modalities. We show that the proposed method markedly outperforms the default SAM 2, achieving average Dice Similarity Coefficient improvement of 0.14 and 0.11 in the scenarios when 5 volumes and 1 volume are available for the initial adaptation, respectively. SLM-SAM 2 also exhibits stronger resistance to over-propagation, making a notable step toward more accurate automated annotation of medical images for segmentation model development.","authors":["Yuwen Chen","Zafer Yildiz","Qihang Li","Yaqian Chen","Haoyu Dong","Hanxue Gu","Nicholas Konz","Maciej A. Mazurowski"],"url":"https://arxiv.org/abs/2505.01854"}
{"created":"2025-05-06","title":"Bayesian learning of the optimal action-value function in a Markov decision process","abstract":"The Markov Decision Process (MDP) is a popular framework for sequential decision-making problems, and uncertainty quantification is an essential component of it to learn optimal decision-making strategies. In particular, a Bayesian framework is used to maintain beliefs about the optimal decisions and the unknown ingredients of the model, which are also to be learned from the data, such as the rewards and state dynamics. However, many existing Bayesian approaches for learning the optimal decision-making strategy are based on unrealistic modelling assumptions and utilise approximate inference techniques. This raises doubts whether the benefits of Bayesian uncertainty quantification are fully realised or can be relied upon.","authors":["Jiaqi Guo","Chon Wai Ho","Sumeetpal S. Singh"],"url":"https://arxiv.org/abs/2505.01859"}
{"created":"2025-05-06","title":"Adversarial Robustness of Deep Learning Models for Inland Water Body Segmentation from SAR Images","abstract":"Inland water body segmentation from Synthetic Aperture Radar (SAR) images is an important task needed for several applications, such as flood mapping. While SAR sensors capture data in all-weather conditions as high-resolution images, differentiating water and water-like surfaces from SAR images is not straightforward. Inland water bodies, such as large river basins, have complex geometry, which adds to the challenge of segmentation. U-Net is a widely used deep learning model for land-water segmentation of SAR images. In practice, manual annotation is often used to generate the corresponding water masks as ground truth. Manual annotation of the images is prone to label noise owing to data poisoning attacks, especially due to complex geometry. In this work, we simulate manual errors in the form of adversarial attacks on the U-Net model and study the robustness of the model to human errors in annotation. Our results indicate that U-Net can tolerate a certain level of corruption before its performance drops significantly. This finding highlights the crucial role that the quality of manual annotations plays in determining the effectiveness of the segmentation model. The code and the new dataset, along with adversarial examples for robust training, are publicly available. (Github link - https://github.com/GVCL/IWSeg-SAR-Poison.git)","authors":["Siddharth Kothari","Srinivasan Murali","Sankalp Kothari","Ujjwal Verma","Jaya Sreevalsan-Nair"],"url":"https://arxiv.org/abs/2505.01884"}
{"created":"2025-05-06","title":"Parameter Sensitivity Analysis in Zinc-Ion Batteries: A Study on Ionic Conductivity, Current Density, and Electrode Properties","abstract":"This study presents a comprehensive Multiphysics model for zinc-ion batteries (ZIBs), incorporating electrochemical aspects. The model integrates the mass transport of Zn2+ ions, charge transfer, and solid diffusion to predict performance parameters like cell potential, and energy density. Significant research has focused on enhancing battery performance by optimizing components of battery to improve parameters such as ionic conductivity and exchange current density and capacity. In this study, we present a model-based investigation of zinc-ion batteries, examining the impact of these parameters. Our findings reveal that at low current densities, raising of ionic conductivity beyond 1.3 S/m and exchange current density above 0.13 mA/cm2 do not yield substantial improvements in capacity. These insights underscore the importance of identifying performance thresholds in the development of next-generation batteries.","authors":["Roya Rajabi","Shichen Sun","Booker Wu","Jamil Khan","Kevin Huang"],"url":"https://arxiv.org/abs/2505.01887"}
{"created":"2025-05-06","title":"Towards Sustainable Energy Storage: Evaluating Polymer Electrolytes for Zinc Ion Batteries","abstract":"Polymer electrolytes present a promising solution to the challenges posed by aqueous electrolytes in energy storage systems, offering the flexibility needed for wearable electronics. Despite the increasing interest in polymer electrolyte-based zinc ion batteries (ZIBs), their development is still in its early stages due to various challenges. In this study, we fabricated three promising polymer electrolytes: CSAM (carboxyl methyl chitosan with acrylamide monomer), PAM (polyacrylamide monomer hydrogel electrolyte), and p-PBI (Phosphoric acid (PA)-doped polybenzimidazole) with Zn(ClO4)2 and Zn(OTf)2, for their application in zinc ion batteries. Our results demonstrated that PAM hydrogel electrolyte exhibited very low LDH formation after a long cycle, demonstrating effective protection for zinc foil, and the high mechanical stability of the p-PBI membrane provided prolonged durability against short circuits through the formation of LDH. The presence of carboxyl groups in CSAM and the formation of O-H bonding facilitated ion movement, resulting in enhanced ionic conductivity, and preventing dendrite formation. Incorporating these hydrogels with high-performance zinc salts, such as zinc triflate (Zn(OTf)2), resulted in impressive stability, with the symmetric cell demonstrating over 4000 hours of uniform and stable voltage profile under 1 mA/cm2 and low overpotential of around 53 mV cycling with CSAM. The full-cell battery with PBI-T membrane showed the highest durability and capacity compared to CSAM-T and PAM-T, due to the greater availability of free protons for storing zinc in the cathode.","authors":["Roya Rajabi","Shichen Sun","Booker Wu","Jamil Khan","Kevin Huang"],"url":"https://arxiv.org/abs/2505.01890"}
{"created":"2025-05-06","title":"3D neuron growth and neurodevelopmental disorder modeling based on truncated hierarchical B-splines with multi-level local refinements","abstract":"3D neuron growth and neurodevelopmental disorders (NDDs) deterioration exhibit complex morphological transformations as neurites differentiate into axons and dendrites, forming intricate networks driven by tubulin concentrations and neurotrophin signals. Conventional 2D models fall short of capturing such morphological complexity, prompting the need and development of advanced 3D computational approaches. In this paper, we present a complex 3D neuron growth model based on isogeometric analysis (IGA) and the phase field method, utilizing locally refined truncated hierarchical B-splines (THB-splines). IGA offers isoparametric representation and higher-order continuity, which are essential for simulating the smooth, evolving interfaces of phase field neurites. In contrast, the phase field method can automatically handle diffuse interfaces and complex topological changes without explicit boundary tracking. This IGA-based phase field method enables accurate and efficient simulation of neurite extensions, branching, and retraction in a fully 3D setting. The THB-spline implementation supports multi-level local refinement, focusing computational resources on regions of active growth, while dynamic domain expansion adapts the simulation domain to extend with growing neurites. KD-tree-based interpolation ensures that phase field variables are accurately transferred onto newly refined meshes. NDDs associated neurite deterioration is simulated by modulating the driving force term within the phase field model to induce interface retraction. This comprehensive 3D framework enhances the accuracy of neurite morphology simulations, advancing the study of complex neuron development, network formation and NDDs.","authors":["Kuanren Qian","Yongjie Jessica Zhang"],"url":"https://arxiv.org/abs/2505.01940"}
{"created":"2025-05-06","title":"Optimization over Trained (and Sparse) Neural Networks: A Surrogate within a Surrogate","abstract":"We can approximate a constraint or an objective function that is uncertain or nonlinear with a neural network that we embed in the optimization model. This approach, which is known as constraint learning, faces the challenge that optimization models with neural network surrogates are harder to solve. Such difficulties have motivated studies on model reformulation, specialized optimization algorithms, and - to a lesser extent - pruning of the embedded networks. In this work, we double down on the use of surrogates by applying network pruning to produce a surrogate of the neural network itself. In the context of using a Mixed-Integer Linear Programming (MILP) solver to verify neural networks, we obtained faster adversarial perturbations for dense neural networks by using sparse surrogates, especially - and surprisingly - if not taking the time to finetune the sparse network to make up for the loss in accuracy. In other words, we show that a pruned network with bad classification performance can still be a good - and more efficient - surrogate.","authors":["Hung Pham","Aiden Ren","Ibrahim Tahir","Jiatai Tong","Thiago Serra"],"url":"https://arxiv.org/abs/2505.01985"}
{"created":"2025-05-06","title":"Extended Fiducial Inference for Individual Treatment Effects via Deep Neural Networks","abstract":"Individual treatment effect estimation has gained significant attention in recent data science literature. This work introduces the Double Neural Network (Double-NN) method to address this problem within the framework of extended fiducial inference (EFI). In the proposed method, deep neural networks are used to model the treatment and control effect functions, while an additional neural network is employed to estimate their parameters. The universal approximation capability of deep neural networks ensures the broad applicability of this method. Numerical results highlight the superior performance of the proposed Double-NN method compared to the conformal quantile regression (CQR) method in individual treatment effect estimation. From the perspective of statistical inference, this work advances the theory and methodology for statistical inference of large models. Specifically, it is theoretically proven that the proposed method permits the model size to increase with the sample size $n$ at a rate of $O(n^{\\zeta})$ for some $0 \\leq \\zeta<1$, while still maintaining proper quantification of uncertainty in the model parameters. This result marks a significant improvement compared to the range $0\\leq \\zeta < \\frac{1}{2}$ required by the classical central limit theorem. Furthermore, this work provides a rigorous framework for quantifying the uncertainty of deep neural networks under the neural scaling law, representing a substantial contribution to the statistical understanding of large-scale neural network models.","authors":["Sehwan Kim","Faming Liang"],"url":"https://arxiv.org/abs/2505.01995"}
{"created":"2025-05-06","title":"Hybrid Image Resolution Quality Metric (HIRQM):A Comprehensive Perceptual Image Quality Assessment Framework","abstract":"Traditional image quality assessment metrics like Mean Squared Error and Structural Similarity Index often fail to reflect perceptual quality under complex distortions. We propose the Hybrid Image Resolution Quality Metric (HIRQM), integrating statistical, multi-scale, and deep learning-based methods for a comprehensive quality evaluation. HIRQM combines three components: Probability Density Function for local pixel distribution analysis, Multi-scale Feature Similarity for structural integrity across resolutions, and Hierarchical Deep Image Features using a pre-trained VGG16 network for semantic alignment with human perception. A dynamic weighting mechanism adapts component contributions based on image characteristics like brightness and variance, enhancing flexibility across distortion types. Our contributions include a unified metric and dynamic weighting for better perceptual alignment. Evaluated on TID2013 and LIVE datasets, HIRQM achieves Pearson and Spearman correlations of 0.92 and 0.90, outperforming traditional metrics. It excels in handling noise, blur, and compression artifacts, making it valuable for image processing applications like compression and restoration.","authors":["Vineesh Kumar Reddy Mondem"],"url":"https://arxiv.org/abs/2505.02001"}
{"created":"2025-05-06","title":"Learning the Simplest Neural ODE","abstract":"Since the advent of the ``Neural Ordinary Differential Equation (Neural ODE)'' paper, learning ODEs with deep learning has been applied to system identification, time-series forecasting, and related areas. Exploiting the diffeomorphic nature of ODE solution maps, neural ODEs has also enabled their use in generative modeling. Despite the rich potential to incorporate various kinds of physical information, training Neural ODEs remains challenging in practice. This study demonstrates, through the simplest one-dimensional linear model, why training Neural ODEs is difficult. We then propose a new stabilization method and provide an analytical convergence analysis. The insights and techniques presented here serve as a concise tutorial for researchers beginning work on Neural ODEs.","authors":["Yuji Okamoto","Tomoya Takeuchi","Yusuke Sakemi"],"url":"https://arxiv.org/abs/2505.02019"}
{"created":"2025-05-06","title":"Efficient computation of soliton gas primitive potentials","abstract":"We consider the problem of computing a class of soliton gas primitive potentials for the Korteweg--de Vries equation that arise from the accumulation of solitons on an infinite interval in the physical domain, extending to $-\\infty$. This accumulation results in an associated Riemann--Hilbert problem on a number of disjoint intervals. In the case where the jump matrices have specific square-root behavior, we describe an efficient and accurate numerical method to solve this Riemann--Hilbert problem and extract the potential. The keys to the method are, first, the deformation of the Riemann--Hilbert problem, making numerical use of the so-called $g$-function, and, second, the incorporation of endpoint singularities into the chosen basis to discretize and solve the associated singular integral equation.","authors":["Cade Ballew","Deniz Bilman","Thomas Trogdon"],"url":"https://arxiv.org/abs/2505.02029"}
{"created":"2025-05-06","title":"Efficient Curvature-Aware Hypergradient Approximation for Bilevel Optimization","abstract":"Bilevel optimization is a powerful tool for many machine learning problems, such as hyperparameter optimization and meta-learning. Estimating hypergradients (also known as implicit gradients) is crucial for developing gradient-based methods for bilevel optimization. In this work, we propose a computationally efficient technique for incorporating curvature information into the approximation of hypergradients and present a novel algorithmic framework based on the resulting enhanced hypergradient computation. We provide convergence rate guarantees for the proposed framework in both deterministic and stochastic scenarios, particularly showing improved computational complexity over popular gradient-based methods in the deterministic setting. This improvement in complexity arises from a careful exploitation of the hypergradient structure and the inexact Newton method. In addition to the theoretical speedup, numerical experiments demonstrate the significant practical performance benefits of incorporating curvature information.","authors":["Youran Dong","Junfeng Yang","Wei Yao","Jin Zhang"],"url":"https://arxiv.org/abs/2505.02101"}
{"created":"2025-05-06","title":"Cognition without neurons: modelling anticipation in a basal reservoir computer","abstract":"How do non-neural organisms, such as the slime mould \\textit{Physarum polycephalum}, anticipate periodic events in their environment? We present a minimal, biologically inspired reservoir model that demonstrates simple temporal anticipation without neurons, spikes, or trained readouts. The model consists of a spatially embedded hexagonal network in which nodes regulate their energy through local, allostatic adaptation. Input perturbations shape energy dynamics over time, allowing the system to internalize temporal regularities into its structure. After being exposed to a periodic input signal, the model spontaneously re-enacts those dynamics even in the absence of further input -- a form of unsupervised temporal pattern completion. This behaviour emerges from internal homeodynamic regulation, without supervised learning or symbolic processing. Our results show that simple homeodynamic regulation can support unsupervised prediction, suggesting a pathway to memory and anticipation in basal organisms.","authors":["Polyphony Bruna","Linn\\'ea Gyllingberg"],"url":"https://arxiv.org/abs/2505.02114"}
{"created":"2025-05-06","title":"Dual Acceleration for Minimax Optimization: Linear Convergence Under Relaxed Assumptions","abstract":"This paper addresses the bilinearly coupled minimax optimization problem: $\\min_{x \\in \\R^{d_x}}\\max_{y \\in \\R^{d_y}} \\ f_1(x) + f_2(x) + y\\T Bx - g_1(y) - g_2(y)$, where $f_1$ and $g_1$ are smooth convex functions, $f_2$ and $g_2$ are potentially nonsmooth convex functions, and $B$ is a coupling matrix. Existing algorithms for solving this problem achieve linear convergence only under stronger conditions, which may not be met in many scenarios. We first introduce the Primal-Dual Proximal Gradient (PDPG) method and demonstrate that it converges linearly under an assumption where existing algorithms fail to achieve linear convergence. Building on insights gained from analyzing the convergence conditions of existing algorithms and PDPG, we further propose the inexact Dual Accelerated Proximal Gradient (iDAPG) method. This method achieves linear convergence under weaker conditions than those required by existing approaches. Moreover, even in cases where existing methods guarantee linear convergence, iDAPG can still provide superior theoretical performance in certain scenarios.","authors":["Jingwang Li","Xiao Li"],"url":"https://arxiv.org/abs/2505.02115"}
{"created":"2025-05-06","title":"Boundary value problem of magnetically insulated diode: existence of solutions and complex bifurcation","abstract":"The paper focuses on the stationary self-consistent problem of magnetic insulation for a vacuum diode with space-charge limitation, described by a singularly perturbed Vlasov-Maxwell system of dimension 1.5. The case of insulated diode when the electrons are deflected back towards the cathode at the point $x^{*}$ is considered. First, the initial VM system is reduced to the nonlinear singular limit system of ODEs for the potentials of electric and magnetic fields. The second step deals with the limit system's reduction to the new nonlinear singular ODE equation for effective potential $\\Theta(x)$. The existence of non-negative solutions is proved for the last equation on the interval $[0, x^{*})$ where $\\Theta(x)>0$. The most interesting and unexplored case is when $\\Theta(x)<0$ on the interval $(x^{*}, 1]$ and corresponds to the case of an insulated diode. For the first time, a numerical analysis of complex bifurcation of solutions in insulated diode is considered for $\\Theta(x)<0$ depending on parameters and boundary conditions. Bifurcation diagrams of the dependence of solution $\\Theta(x)$ on a free point (free boundary) $x^{*}$ were constructed. Insulated diode spacing is found.","authors":["Denis Sidorov","Alexander Sinitsyn","David Leguizamon","Liguo Wang"],"url":"https://arxiv.org/abs/2505.02155"}
{"created":"2025-05-06","title":"Pickup & Delivery with Time Windows and Transfers: combining decomposition with metaheuristics","abstract":"This paper examines the generalisation of the Pickup and Delivery Problem that allows mid-route load exchanges among vehicles and obeys strict time-windows at all locations. We propose a novel Logic-Based Benders Decomposition (LBBD) that improves optimality gaps for all benchmarks in the literature and scales up to handle larger ones. To tackle even larger instances, we introduce a refined Large Neighborhood Search (LNS) algorithm that improves the adaptability of LNS beyond case-specific configurations appearing in related literature.","authors":["Ioannis Avgerinos","Ioannis Mourtos","Nikolaos Tsompanidis","Georgios Zois"],"url":"https://arxiv.org/abs/2505.02158"}
{"created":"2025-05-06","title":"Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering","abstract":"Time series clustering is an unsupervised learning method for classifying time series data into groups with similar behavior. It is used in applications such as healthcare, finance, economics, energy, and climate science. Several time series clustering methods have been introduced and used for over four decades. Most of them focus on measuring either Euclidean distances or association dissimilarities between time series. In this work, we propose a new dissimilarity measure called ranked Pearson correlation dissimilarity (RDPC), which combines a weighted average of a specified fraction of the largest element-wise differences with the well-known Pearson correlation dissimilarity. It is incorporated into hierarchical clustering. The performance is evaluated and compared with existing clustering algorithms. The results show that the RDPC algorithm outperforms others in complicated cases involving different seasonal patterns, trends, and peaks. Finally, we demonstrate our method by clustering a random sample of customers from a Thai electricity consumption time series dataset into seven groups with unique characteristics.","authors":["Chutiphan Charoensuk","Nathakhun Wiroonsri"],"url":"https://arxiv.org/abs/2505.02173"}
{"created":"2025-05-06","title":"Latent Variable Estimation in Bayesian Black-Litterman Models","abstract":"We revisit the Bayesian Black-Litterman (BL) portfolio model and remove its reliance on subjective investor views. Classical BL requires an investor \"view\": a forecast vector $q$ and its uncertainty matrix $\\Omega$ that describe how much a chosen portfolio should outperform the market. Our key idea is to treat $(q,\\Omega)$ as latent variables and learn them from market data within a single Bayesian network. Consequently, the resulting posterior estimation admits closed-form expression, enabling fast inference and stable portfolio weights. Building on these, we propose two mechanisms to capture how features interact with returns: shared-latent parametrization and feature-influenced views; both recover classical BL and Markowitz portfolios as special cases. Empirically, on 30-year Dow-Jones and 20-year sector-ETF data, we improve Sharpe ratios by 50% and cut turnover by 55% relative to Markowitz and the index baselines. This work turns BL into a fully data-driven, view-free, and coherent Bayesian framework for portfolio optimization.","authors":["Thomas Y. L. Lin","Jerry Yao-Chieh Hu","Paul W. Chiou","Peter Lin"],"url":"https://arxiv.org/abs/2505.02185"}
{"created":"2025-05-06","title":"Packaged Quantum States for Gauge-Invariant Quantum Computation and Communication","abstract":"Packaged quantum states are gauge-invariant states in which all internal quantum numbers (IQNs) form an inseparable block. This feature gives rise to novel packaged entanglements that encompass all IQNs, which is important both for fundamental physics and for quantum technology. Here we develop a framework for gauge-invariant quantum information processing based on packaged quantum states. We propose the necessary and sufficient conditions for a valid packaged superposition state of a single particle and multi-particle. We then present the details of constructing gauge-invariant packaged qubits (or qudits), packaged gates, and packaged circuits (which commute with the total charge operator). These serve as alternative foundation for gauge-invariant quantum information science. We then adapt conventional quantum error-correction codes, quantum algorithms, and quantum communication protocols to the ($d \\times D$)-dimensional hybrid-packaged subspace. This high-dimensional hybrid-packaged subspace is flexible for pruning and scaling to match available physics systems. Thus, packaged quantum information processing becomes feasible and testable. ","authors":["Rongchao Ma"],"url":"https://arxiv.org/abs/2505.02205"}
{"created":"2025-05-06","title":"CSASN: A Multitask Attention-Based Framework for Heterogeneous Thyroid Carcinoma Classification in Ultrasound Images","abstract":"Heterogeneous morphological features and data imbalance pose significant challenges in rare thyroid carcinoma classification using ultrasound imaging. To address this issue, we propose a novel multitask learning framework, Channel-Spatial Attention Synergy Network (CSASN), which integrates a dual-branch feature extractor - combining EfficientNet for local spatial encoding and ViT for global semantic modeling, with a cascaded channel-spatial attention refinement module. A residual multiscale classifier and dynamically weighted loss function further enhance classification stability and accuracy. Trained on a multicenter dataset comprising more than 2000 patients from four clinical institutions, our framework leverages a residual multiscale classifier and dynamically weighted loss function to enhance classification stability and accuracy. Extensive ablation studies demonstrate that each module contributes significantly to model performance, particularly in recognizing rare subtypes such as FTC and MTC carcinomas. Experimental results show that CSASN outperforms existing single-stream CNN or Transformer-based models, achieving a superior balance between precision and recall under class-imbalanced conditions. This framework provides a promising strategy for AI-assisted thyroid cancer diagnosis.","authors":["Peiqi Li","Yincheng Gao","Renxing Li","Haojie Yang","Yunyun Liu","Boji Liu","Jiahui Ni","Ying Zhang","Yulu Wu","Xiaowei Fang","Lehang Guo","Liping Sun","Jiangang Chen"],"url":"https://arxiv.org/abs/2505.02211"}
{"created":"2025-05-06","title":"CONQURE: A Co-Execution Environment for Quantum and Classical Resources","abstract":"Cutting edge classical computing today relies on a combination of CPU-based computing with a strong reliance on accelerators. In particular, high-performance computing (HPC) and machine learning (ML) rely heavily on acceleration via GPUs for numerical kernels. In the future, acceleration via quantum devices may complement GPUs for kernels where algorithms provide quantum advantage, i.e., significant speedups over classical algorithms. Computing with quantum kernels mapped onto quantum processing units (QPUs) requires seamless integration into HPC and ML. However, quantum offloading onto HPC/cloud lacks open-source software infrastructure. For classical algorithms, parallelization standards, such as OpenMP, MPI, or CUDA exist. In contrast, a lack of quantum abstractions currently limits the adoption of quantum acceleration in practical applications creating a gap between quantum algorithm development and practical HPC integration. Such integration needs to extend to efficient quantum offloading of kernels, which further requires scheduling of quantum resources, control of QPU kernel execution, tracking of QPU results, providing results to classical calling contexts and coordination with HPC scheduling. This work proposes CONQURE, a co-execution environment for quantum and classical resources. CONQURE is a fully open-source cloud queue framework that presents a novel modular scheduling framework allowing users to offload OpenMP quantum kernels to QPUs as quantum circuits, to relay results back to calling contexts in classical computing, and to schedule quantum resources via our CONQURE API. We show our API has a low overhead averaging 12.7ms in our tests, and we demonstrate functionality on an ion-trap device. Our OpenMP extension enables the parallelization of VQE runs with a 3.1X reduction in runtime.","authors":["Atulya Mahesh","Swastik Mittal","Frank Mueller"],"url":"https://arxiv.org/abs/2505.02241"}
{"created":"2025-05-06","title":"Heterosynaptic Circuits Are Universal Gradient Machines","abstract":"We propose a design principle for the learning circuits of the biological brain. The principle states that almost any dendritic weights updated via heterosynaptic plasticity can implement a generalized and efficient class of gradient-based meta-learning. The theory suggests that a broad class of biologically plausible learning algorithms, together with the standard machine learning optimizers, can be grounded in heterosynaptic circuit motifs. This principle suggests that the phenomenology of (anti-) Hebbian (HBP) and heterosynaptic plasticity (HSP) may emerge from the same underlying dynamics, thus providing a unifying explanation. It also suggests an alternative perspective of neuroplasticity, where HSP is promoted to the primary learning and memory mechanism, and HBP is an emergent byproduct. We present simulations that show that (a) HSP can explain the metaplasticity of neurons, (b) HSP can explain the flexibility of the biology circuits, and (c) gradient learning can arise quickly from simple evolutionary dynamics that do not compute any explicit gradient. While our primary focus is on biology, the principle also implies a new approach to designing AI training algorithms and physically learnable AI hardware. Conceptually, our result demonstrates that contrary to the common belief, gradient computation may be extremely easy and common in nature.","authors":["Liu Ziyin","Isaac Chuang","Tomaso Poggio"],"url":"https://arxiv.org/abs/2505.02248"}
{"created":"2025-05-06","title":"Bayesian Federated Cause-of-Death Classification and Quantification Under Distribution Shift","abstract":"In regions lacking medically certified causes of death, verbal autopsy (VA) is a critical and widely used tool to ascertain the cause of death through interviews with caregivers. Data collected by VAs are often analyzed using probabilistic algorithms. The performance of these algorithms often degrades due to distributional shift across populations. Most existing VA algorithms rely on centralized training, requiring full access to training data for joint modeling. This is often infeasible due to privacy and logistical constraints. In this paper, we propose a novel Bayesian Federated Learning (BFL) framework that avoids data sharing across multiple training sources. Our method enables reliable individual-level cause-of-death classification and population-level quantification of cause-specific mortality fractions (CSMFs), in a target domain with limited or no local labeled data. The proposed framework is modular, computationally efficient, and compatible with a wide range of existing VA algorithms as candidate models, facilitating flexible deployment in real-world mortality surveillance systems. We validate the performance of BFL through extensive experiments on two real-world VA datasets under varying levels of distribution shift. Our results show that BFL significantly outperforms the base models built on a single domain and achieves comparable or better performance compared to joint modeling.","authors":["Yu Zhu","Zehang Richard Li"],"url":"https://arxiv.org/abs/2505.02257"}
{"created":"2025-05-06","title":"Smooth Integer Encoding via Integral Balance","abstract":"We introduce a novel method for encoding integers using smooth real-valued functions whose integral properties implicitly reflect discrete quantities. In contrast to classical representations, where the integer appears as an explicit parameter, our approach encodes the number N in the set of natural numbers through the cumulative balance of a smooth function f_N(t), constructed from localized Gaussian bumps with alternating and decaying coefficients. The total integral I(N) converges to zero as N tends to infinity, and the integer can be recovered as the minimal point of near-cancellation.","authors":["Stanislav Semenov"],"url":"https://arxiv.org/abs/2505.02259"}
{"created":"2025-05-06","title":"Minimisation of Quasar-Convex Functions Using Random Zeroth-Order Oracles","abstract":"This study explores the performance of a random Gaussian smoothing zeroth-order (ZO) scheme for minimising quasar-convex (QC) and strongly quasar-convex (SQC) functions in both unconstrained and constrained settings. For the unconstrained problem, we establish the ZO algorithm's convergence to a global minimum along with its complexity when applied to both QC and SQC functions. For the constrained problem, we introduce the new notion of proximal-quasar-convexity and prove analogous results to the unconstrained case. Specifically, we show the complexity bounds and the convergence of the algorithm to a neighbourhood of a global minimum whose size can be controlled under a variance reduction scheme. Theoretical findings are illustrated through investigating the performance of the algorithm applied to a range of problems in machine learning and optimisation. Specifically, we observe scenarios where the ZO method outperforms gradient descent. We provide a possible explanation for this phenomenon.","authors":["Amir Ali Farzin","Yuen-Man Pun","Iman Shames"],"url":"https://arxiv.org/abs/2505.02281"}
{"created":"2025-05-06","title":"Optimally accurate operators for partial differential equations","abstract":"In this contribution, we generalize the concept of \\textit{optimally accurate operators} proposed and used in a series of studies on the simulation of seismic wave propagation, particularly based on Geller \\& Takeuchi (1995). Although these operators have been mathematically and numerically proven to be more accurate than conventional methods, the theory was specifically developed for the equations of motion in linear elastic continuous media. Furthermore, the original theory requires compensation for errors from each term due to truncation at low orders during the error estimation, which has limited its application to other types of physics described by partial differential equations.","authors":["Nobuaki Fuji","Thibault Duretz"],"url":"https://arxiv.org/abs/2505.02320"}
{"created":"2025-05-06","title":"Temporal Robustness in Discrete Time Linear Dynamical Systems","abstract":"Discrete time linear dynamical systems, including Markov chains, have found many applications. However, in some problems, there is uncertainty about the time horizon for which the system runs. This creates uncertainty about the cost (or reward) incurred based on the state distribution when the system stops. Given past data samples of how long a system ran, we propose to theoretically analyze a distributional robust cost estimation task in a Wasserstein ambiguity set, instead of learning a probability distribution from a few samples. Towards this, we show an equivalence between a discrete time Markov Chain on a probability simplex and a global asymptotic stable (GAS) discrete time linear dynamical system, allowing us to base our study on a GAS system only. Then, we provide various polynomial time algorithms and hardness results for different cases in our theoretical study, including a fundamental result about Wasserstein distance based polytope.","authors":["Nilava Metya","Arunesh Sinha"],"url":"https://arxiv.org/abs/2505.02347"}
{"created":"2025-05-06","title":"Learning simple heuristic rules for classifying materials based on chemical composition","abstract":"In the past decade, there has been a significant interest in the use of machine learning approaches in materials science research. Conventional deep learning approaches that rely on complex, nonlinear models have become increasingly important in computational materials science due to their high predictive accuracy. In contrast to these approaches, we have shown in a recent work that a remarkably simple learned heuristic rule -- based on the concept of topogivity -- can classify whether a material is topological using only its chemical composition. In this paper, we go beyond the topology classification scenario by also studying the use of machine learning to develop simple heuristic rules for classifying whether a material is a metal based on chemical composition. Moreover, we present a framework for incorporating chemistry-informed inductive bias based on the structure of the periodic table. For both the topology classification and the metallicity classification tasks, we empirically characterize the performance of simple heuristic rules fit with and without chemistry-informed inductive bias across a wide range of training set sizes. We find evidence that incorporating chemistry-informed inductive bias can reduce the amount of training data required to reach a given level of test accuracy.","authors":["Andrew Ma","Marin Solja\\v{c}i\\'c"],"url":"https://arxiv.org/abs/2505.02361"}
{"created":"2025-05-06","title":"An Arbitrary-Modal Fusion Network for Volumetric Cranial Nerves Tract Segmentation","abstract":"The segmentation of cranial nerves (CNs) tract provides a valuable quantitative tool for the analysis of the morphology and trajectory of individual CNs. Multimodal CNs tract segmentation networks, e.g., CNTSeg, which combine structural Magnetic Resonance Imaging (MRI) and diffusion MRI, have achieved promising segmentation performance. However, it is laborious or even infeasible to collect complete multimodal data in clinical practice due to limitations in equipment, user privacy, and working conditions. In this work, we propose a novel arbitrary-modal fusion network for volumetric CNs tract segmentation, called CNTSeg-v2, which trains one model to handle different combinations of available modalities. Instead of directly combining all the modalities, we select T1-weighted (T1w) images as the primary modality due to its simplicity in data acquisition and contribution most to the results, which supervises the information selection of other auxiliary modalities. Our model encompasses an Arbitrary-Modal Collaboration Module (ACM) designed to effectively extract informative features from other auxiliary modalities, guided by the supervision of T1w images. Meanwhile, we construct a Deep Distance-guided Multi-stage (DDM) decoder to correct small errors and discontinuities through signed distance maps to improve segmentation accuracy. We evaluate our CNTSeg-v2 on the Human Connectome Project (HCP) dataset and the clinical Multi-shell Diffusion MRI (MDM) dataset. Extensive experimental results show that our CNTSeg-v2 achieves state-of-the-art segmentation performance, outperforming all competing methods.","authors":["Lei Xie","Huajun Zhou","Junxiong Huang","Jiahao Huang","Qingrun Zeng","Jianzhong He","Jiawei Zhang","Baohua Fan","Mingchu Li","Guoqiang Xie","Hao Chen","Yuanjing Feng"],"url":"https://arxiv.org/abs/2505.02385"}
{"created":"2025-05-06","title":"Diagnostic Uncertainty in Pneumonia Detection using CNN MobileNetV2 and CNN from Scratch","abstract":"Pneumonia Diagnosis, though it is crucial for an effective treatment, it can be hampered by uncertainty. This uncertainty starts to arise due to some factors like atypical presentations, limitations of diagnostic tools such as chest X-rays, and the presence of co-existing respiratory conditions. This research proposes one of the supervised learning methods, CNN. Using MobileNetV2 as the pre-trained one with ResNet101V2 architecture and using Keras API as the built from scratch model, for identifying lung diseases especially pneumonia. The datasets used in this research were obtained from the website through Kaggle. The result shows that by implementing CNN MobileNetV2 and CNN from scratch the result is promising. While validating data, MobileNetV2 performs with stability and minimal overfitting, while the training accuracy increased to 84.87% later it slightly decreased to 78.95%, with increasing validation loss from 0.499 to 0.6345. Nonetheless, MobileNetV2 is more stable. Although it takes more time to train each epoch. Meanwhile, after the 10th epoch, the Scratch model displayed more instability and overfitting despite having higher validation accuracy, training accuracy decreased significantly to 78.12% and the validation loss increased from 0.5698 to 1.1809. With these results, ResNet101V2 offers stability, and the Scratch model offers high accuracy.","authors":["Kennard Norbert Sudiardjo","Islam Nur Alam","Wilson Wijaya","Lili Ayu Wulandhari"],"url":"https://arxiv.org/abs/2505.02396"}
{"created":"2025-05-06","title":"Efficient Classical Algorithms for Simulating Gaussian Boson Sampling on Graphs","abstract":"Gaussian Boson Sampling (GBS) is a promising candidate for demonstrating quantum computational advantage and can be applied to solving graph-related problems. In this work, we propose Markov chain Monte Carlo-based algorithms to simulate GBS on undirected, unweighted graphs. Our main contribution is a double-loop variant of Glauber dynamics, whose stationary distribution matches the GBS distribution. We further prove that it mixes in polynomial time for dense graphs using a refined canonical path argument. Numerically, we conduct experiments on graphs with 256 vertices, larger than the scales in former GBS experiments as well as classical simulations. In particular, we show that both the single-loop and double-loop Glauber dynamics improve the performance of original random search and simulated annealing algorithms for the max-Hafnian and densest $k$-subgraph problems up to 10x. Overall, our approach offers both theoretical guarantees and practical advantages for classical simulations of GBS on graphs.","authors":["Yexin Zhang","Shuo Zhou","Xinzhao Wang","Ziruo Wang","Ziyi Yang","Rui Yang","Yecheng Xue","Tongyang Li"],"url":"https://arxiv.org/abs/2505.02445"}
{"created":"2025-05-06","title":"Deep learning of personalized priors from past MRI scans enables fast, quality-enhanced point-of-care MRI with low-cost systems","abstract":"Magnetic resonance imaging (MRI) offers superb-quality images, but its accessibility is limited by high costs, posing challenges for patients requiring longitudinal care. Low-field MRI provides affordable imaging with low-cost devices but is hindered by long scans and degraded image quality, including low signal-to-noise ratio (SNR) and tissue contrast. We propose a novel healthcare paradigm: using deep learning to extract personalized features from past standard high-field MRI scans and harnessing them to enable accelerated, enhanced-quality follow-up scans with low-cost systems. To overcome the SNR and contrast differences, we introduce ViT-Fuser, a feature-fusion vision transformer that learns features from past scans, e.g. those stored in standard DICOM CDs. We show that \\textit{a single prior scan is sufficient}, and this scan can come from various MRI vendors, field strengths, and pulse sequences. Experiments with four datasets, including glioblastoma data, low-field ($50mT$), and ultra-low-field ($6.5mT$) data, demonstrate that ViT-Fuser outperforms state-of-the-art methods, providing enhanced-quality images from accelerated low-field scans, with robustness to out-of-distribution data. Our freely available framework thus enables rapid, diagnostic-quality, low-cost imaging for wide healthcare applications.","authors":["Tal Oved","Beatrice Lena","Chlo\\'e F. Najac","Sheng Shen","Matthew S. Rosen","Andrew Webb","Efrat Shimron"],"url":"https://arxiv.org/abs/2505.02470"}
{"created":"2025-05-06","title":"Integrating Column Generation and Large Neighborhood Search for Bus Driver Scheduling with Complex Break Constraints","abstract":"The Bus Driver Scheduling Problem (BDSP) is a combinatorial optimization problem with the goal to design shifts to cover prearranged bus tours. The objective takes into account the operational cost as well as the satisfaction of drivers. This problem is heavily constrained due to strict legal rules and collective agreements. The objective of this article is to provide state-of-the-art exact and hybrid solution methods that can provide high-quality solutions for instances of different sizes. This work presents a comprehensive study of both an exact method, Branch and Price (B&amp;P), as well as a Large Neighborhood Search (LNS) framework which uses B&amp;P or Column Generation (CG) for the repair phase to solve the BDSP. It further proposes and evaluates a novel deeper integration of B&amp;P and LNS, storing the generated columns from the LNS subproblems and reusing them for other subproblems, or to find better global solutions. The article presents a detailed analysis of several components of the solution methods and their impact, including general improvements for the B&amp;P subproblem, which is a high-dimensional Resource Constrained Shortest Path Problem (RCSPP), and the components of the LNS. The evaluation shows that our approach provides new state-of-the-art results for instances of all sizes, including exact solutions for small instances, and low gaps to a known lower bound for mid-sized instances. Conclusions: We observe that B&amp;P provides the best results for small instances, while the tight integration of LNS and CG can provide high-quality solutions for larger instances, further improving over LNS which just uses CG as a black box. The proposed methods are general and can also be applied to other rule sets and related optimization problems","authors":["Lucas Kletzander","Tommaso Mannelli Mazzoli","Nysret Musliu","Pascal Van Hentenryck"],"url":"https://arxiv.org/abs/2505.02485"}
{"created":"2025-05-06","title":"Resolving Memorization in Empirical Diffusion Model for Manifold Data in High-Dimensional Spaces","abstract":"Diffusion models is a popular computational tool to generate new data samples. It utilizes a forward diffusion process that add noise to the data distribution and then use a reverse process to remove noises to produce samples from the data distribution. However, when the empirical data distribution consists of $n$ data point, using the empirical diffusion model will necessarily produce one of the existing data points. This is often referred to as the memorization effect, which is usually resolved by sophisticated machine learning procedures in the current literature. This work shows that the memorization problem can be resolved by a simple inertia update step at the end of the empirical diffusion model simulation. Our inertial diffusion model requires only the empirical diffusion model score function and it does not require any further training. We show that choosing the inertia diffusion model sample distribution is an $O\\left(n^{-\\frac{2}{d+4}}\\right)$ Wasserstein-1 approximation of a data distribution lying on a $C^2$ manifold of dimension $d$. Since this estimate is significant smaller the Wasserstein1 distance between population and empirical distributions, it rigorously shows the inertial diffusion model produces new data samples. Remarkably, this upper bound is completely free of the ambient space dimension, since there is no training involved. Our analysis utilizes the fact that the inertial diffusion model samples are approximately distributed as the Gaussian kernel density estimator on the manifold. This reveals an interesting connection between diffusion model and manifold learning.","authors":["Yang Lyu","Yuchun Qian","Tan Minh Nguyen","Xin T. Tong"],"url":"https://arxiv.org/abs/2505.02508"}
{"created":"2025-05-06","title":"Energy Efficiency Maximization for CR-NOMA based Smart Grid Communication Network","abstract":"Managing massive data flows effectively and resolving spectrum shortages are two challenges that Smart Grid Communication Networks (SGCN) must overcome. To address these problems, we provide a combined optimization approach that makes use of Cognitive Radio (CR) and Non-Orthogonal Multiple Access (NOMA) technologies. Our work focuses on using user pairing (UP) and power allocation (PA) techniques to maximize energy efficiency (EE) in SGCN, particularly within Neighbourhood Area Networks (NANs). We develop a joint optimization problem that takes into account the real-world limitations of a CR-NOMA setting. This problem is NP-hard, nonlinear, and nonconvex by nature. To address the computational complexity of the problem, we use the Block Coordinate Descent (BCD) method, which breaks the problem into UP and PA subproblems. Initially, we proposed the Zebra-Optimization User Pairing (ZOUP) algorithm to tackle the UP problem, which outperforms both Orthogonal Multiple Access (OMA) and non-optimized NOMA (UPWO) by 78.8\\% and 13.6\\%, respectively, at a SNR of 15 dB. Based on the ZOUP pairs, we subsequently proposed the PA approach, i.e., ZOUPPA, which significantly outperforms UPWO and ZOUP by 53.2\\% and 25.4\\%, respectively, at an SNR of 15 dB. A detailed analysis of key parameters, including varying SNRs, power allocation constants, path loss exponents, user density, channel availability, and coverage radius, underscores the superiority of our approach. By facilitating the effective use of communication resources in SGCN, our research opens the door to more intelligent and energy-efficient grid systems. Our work tackles important issues in SGCN and lays the groundwork for future developments in smart grid communication technologies by combining modern optimization approaches with CR-NOMA.","authors":["Mubashar Sarfraz","Sheraz Alam","Sajjad A. Ghauri","Asad Mahmood"],"url":"https://arxiv.org/abs/2505.02530"}
{"created":"2025-05-06","title":"On APN functions in odd characteristic, the disproof of a conjecture and related problems","abstract":"In this paper disprove a conjecture by Pal and Budaghyan (DCC, 2024) on the existence of a family of APN permutations, but showing that if the field's cardinality $q$ is larger than~$9587$, then those functions will never be APN. Moreover, we discuss other connected families of functions, for potential APN functions, but we show that they are not good candidates for APNess if the underlying field is large, in spite of the fact that they though they are APN for small environments.","authors":["Daniele Bartoli","Pantelimon Stanica"],"url":"https://arxiv.org/abs/2505.02585"}
{"created":"2025-05-06","title":"Lane-Wise Highway Anomaly Detection","abstract":"This paper proposes a scalable and interpretable framework for lane-wise highway traffic anomaly detection, leveraging multi-modal time series data extracted from surveillance cameras. Unlike traditional sensor-dependent methods, our approach uses AI-powered vision models to extract lane-specific features, including vehicle count, occupancy, and truck percentage, without relying on costly hardware or complex road modeling. We introduce a novel dataset containing 73,139 lane-wise samples, annotated with four classes of expert-validated anomalies: three traffic-related anomalies (lane blockage and recovery, foreign object intrusion, and sustained congestion) and one sensor-related anomaly (camera angle shift). Our multi-branch detection system integrates deep learning, rule-based logic, and machine learning to improve robustness and precision. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods in precision, recall, and F1-score, providing a cost-effective and scalable solution for real-world intelligent transportation systems.","authors":["Mei Qiu","William Lorenz Reindl","Yaobin Chen","Stanley Chien","Shu Hu"],"url":"https://arxiv.org/abs/2505.02613"}
{"created":"2025-05-06","title":"Entropic Mirror Descent for Linear Systems: Polyak's Stepsize and Implicit Bias","abstract":"This paper focuses on applying entropic mirror descent to solve linear systems, where the main challenge for the convergence analysis stems from the unboundedness of the domain. To overcome this without imposing restrictive assumptions, we introduce a variant of Polyak-type stepsizes. Along the way, we strengthen the bound for $\\ell_1$-norm implicit bias, obtain sublinear and linear convergence results, and generalize the convergence result to arbitrary convex $L$-smooth functions. We also propose an alternative method that avoids exponentiation, resembling the original Hadamard descent, but with provable convergence.","authors":["Yura Malitsky","Alexander Posch"],"url":"https://arxiv.org/abs/2505.02614"}
{"created":"2025-05-06","title":"DeepSparse: A Foundation Model for Sparse-View CBCT Reconstruction","abstract":"Cone-beam computed tomography (CBCT) is a critical 3D imaging technology in the medical field, while the high radiation exposure required for high-quality imaging raises significant concerns, particularly for vulnerable populations. Sparse-view reconstruction reduces radiation by using fewer X-ray projections while maintaining image quality, yet existing methods face challenges such as high computational demands and poor generalizability to different datasets. To overcome these limitations, we propose DeepSparse, the first foundation model for sparse-view CBCT reconstruction, featuring DiCE (Dual-Dimensional Cross-Scale Embedding), a novel network that integrates multi-view 2D features and multi-scale 3D features. Additionally, we introduce the HyViP (Hybrid View Sampling Pretraining) framework, which pretrains the model on large datasets with both sparse-view and dense-view projections, and a two-step finetuning strategy to adapt and refine the model for new datasets. Extensive experiments and ablation studies demonstrate that our proposed DeepSparse achieves superior reconstruction quality compared to state-of-the-art methods, paving the way for safer and more efficient CBCT imaging.","authors":["Yiqun Lin","Hualiang Wang","Jixiang Chen","Jiewen Yang","Jiarong Guo","Xiaomeng Li"],"url":"https://arxiv.org/abs/2505.02628"}
{"created":"2025-05-06","title":"Multi-View Learning with Context-Guided Receptance for Image Denoising","abstract":"Image denoising is essential in low-level vision applications such as photography and automated driving. Existing methods struggle with distinguishing complex noise patterns in real-world scenes and consume significant computational resources due to reliance on Transformer-based models. In this work, the Context-guided Receptance Weighted Key-Value (\\M) model is proposed, combining enhanced multi-view feature integration with efficient sequence modeling. Our approach introduces the Context-guided Token Shift (CTS) paradigm, which effectively captures local spatial dependencies and enhance the model's ability to model real-world noise distributions. Additionally, the Frequency Mix (FMix) module extracting frequency-domain features is designed to isolate noise in high-frequency spectra, and is integrated with spatial representations through a multi-view learning process. To improve computational efficiency, the Bidirectional WKV (BiWKV) mechanism is adopted, enabling full pixel-sequence interaction with linear complexity while overcoming the causal selection constraints. The model is validated on multiple real-world image denoising datasets, outperforming the existing state-of-the-art methods quantitatively and reducing inference time up to 40\\%. Qualitative results further demonstrate the ability of our model to restore fine details in various scenes.","authors":["Binghong Chen","Tingting Chai","Wei Jiang","Yuanrong Xu","Guanglu Zhou","Xiangqian Wu"],"url":"https://arxiv.org/abs/2505.02705"}
{"created":"2025-05-06","title":"Linear colorings of graphs","abstract":"Motivated by algorithmic applications, Kun, O'Brien, Pilipczuk, and Sullivan introduced the parameter linear chromatic number as a relaxation of treedepth and proved that the two parameters are polynomially related. They conjectured that treedepth could be bounded from above by twice the linear chromatic number.","authors":["Claire Hilaire","Matja\\v{z} Krnc","Martin Milani\\v{c}","Jean-Florent Raymond"],"url":"https://arxiv.org/abs/2505.02768"}
{"created":"2025-05-06","title":"i-QLS: Quantum-supported Algorithm for Least Squares Optimization in Non-Linear Regression","abstract":"We propose an iterative quantum-assisted least squares (i-QLS) optimization method that leverages quantum annealing to overcome the scalability and precision limitations of prior quantum least squares approaches. Unlike traditional QUBO-based formulations, which suffer from a qubit overhead due to fixed discretization, our approach refines the solution space iteratively, enabling exponential convergence while maintaining a constant qubit requirement per iteration. This iterative refinement transforms the problem into an anytime algorithm, allowing for flexible computational trade-offs. Furthermore, we extend our framework beyond linear regression to non-linear function approximation via spline-based modeling, demonstrating its adaptability to complex regression tasks. We empirically validate i-QLS on the D-Wave quantum annealer, showing that our method efficiently scales to high-dimensional problems, achieving competitive accuracy with classical solvers while outperforming prior quantum approaches. Experiments confirm that i-QLS enables near-term quantum hardware to perform regression tasks with improved precision and scalability, paving the way for practical quantum-assisted machine learning applications.","authors":["Supreeth Mysore Venkatesh","Antonio Macaluso","Diego Arenas","Matthias Klusch","Andreas Dengel"],"url":"https://arxiv.org/abs/2505.02788"}
{"created":"2025-05-06","title":"Security Analysis of the Open Banking Account and Transaction API Protocol","abstract":"The Second Payment Services Directive (PSD2) of the European Union aims to create a consumer-friendly financial market by mandating secure and standardised data sharing between banking operators and third parties. Consequently, EU countries and the United Kingdom have adopted Open Banking, a standardised data-sharing API. This paper presents a formal modelling and security analysis of the UK Open Banking Standard's APIs, with a specific focus on the Account and Transaction API protocol. Our methodology employs the extended Alice and Bob notation (AnBx) to create a formal model of the protocol, which is then verified using the OFMC symbolic model checker and the Proverif cryptographic protocol verifier. We extend previous work by enabling verification for unlimited sessions with a strongly typed model. Additionally, we integrate our formal analysis with practical security testing of some necessary conditions to demonstrate verified security-goals in the NatWest Open Banking sandbox, evaluating mechanisms such as authorisation and authentication procedures.","authors":["Paolo Modesti","Leo Freitas","Qudus Shotomiwa","Abdulaziz Almehrej"],"url":"https://arxiv.org/abs/2003.12776"}
{"created":"2025-05-06","title":"From Attack to Protection: Leveraging Watermarking Attack Network for Advanced Add-on Watermarking","abstract":"Multi-bit watermarking (MW) has been designed to enhance resistance against watermarking attacks, such as signal processing operations and geometric distortions. Various benchmark tools exist to assess this robustness through simulated attacks on watermarked images. However, these tools often fail to capitalize on the unique attributes of the targeted MW and typically neglect the aspect of visual quality, a critical factor in practical applications. To overcome these shortcomings, we introduce a watermarking attack network (WAN), a fully trainable watermarking benchmark tool designed to exploit vulnerabilities within MW systems and induce watermark bit inversions, significantly diminishing watermark extractability. The proposed WAN employs an architecture based on residual dense blocks, which is adept at both local and global feature learning, thereby maintaining high visual quality while obstructing the extraction of embedded information. Our empirical results demonstrate that the WAN effectively undermines various block-based MW systems while minimizing visual degradation caused by attacks. This is facilitated by our novel watermarking attack loss, which is specifically crafted to compromise these systems. The WAN functions not only as a benchmarking tool but also as an add-on watermarking (AoW) mechanism, augmenting established universal watermarking schemes by enhancing robustness or imperceptibility without requiring detailed method context and adapting to dynamic watermarking requirements. Extensive experimental results show that AoW complements the performance of the targeted MW system by independently enhancing both imperceptibility and robustness.","authors":["Seung-Hun Nam","Jihyeon Kang","Daesik Kim","Namhyuk Ahn","Wonhyuk Ahn"],"url":"https://arxiv.org/abs/2008.06255"}
{"created":"2025-05-06","title":"Minimax Strikes Back","abstract":"Deep Reinforcement Learning reaches a superhuman level of play in many complete information games. The state of the art algorithm for learning with zero knowledge is AlphaZero. We take another approach, Ath\\'enan, which uses a different, Minimax-based, search algorithm called Descent, as well as different learning targets and that does not use a policy. We show that for multiple games it is much more efficient than the reimplementation of AlphaZero: Polygames. It is even competitive with Polygames when Polygames uses 100 times more GPU (at least for some games). One of the keys to the superior performance is that the cost of generating state data for training is approximately 296 times lower with Ath\\'enan. With the same reasonable ressources, Ath\\'enan without reinforcement heuristic is at least 7 times faster than Polygames and much more than 30 times faster with reinforcement heuristic.","authors":["Quentin Cohen-Solal","Tristan Cazenave"],"url":"https://arxiv.org/abs/2012.10700"}
{"created":"2025-05-06","title":"Compressing integer lists with Contextual Arithmetic Trits","abstract":"Inverted indexes allow to query large databases without needing to search in the database at each query. An important line of research is to construct the most efficient inverted indexes, both in terms of compression ratio and time efficiency. In this article, we show how to use trit encoding, combined with contextual methods for computing inverted indexes. We perform an extensive study of different variants of these methods and show that our method consistently outperforms the Binary Interpolative Method -- which is one of the golden standards in this topic -- with respect to compression size. We apply our methods to a variety of datasets and make available the source code that produced the results, together with all our datasets.","authors":["Yann Barsamian","Andr\\'e Chailloux"],"url":"https://arxiv.org/abs/2209.02089"}
{"created":"2025-05-06","title":"Enhanced RMT estimator for signal number estimation in the presence of colored noise","abstract":"The subspace-based techniques are widely utilized in various scientific fields, and they need accurate estimation of the signal subspace dimension. The classic RMT estimator for model order estimation based on random matrix theory assumes that the noise is white Gaussian, and performs poorly in the presence of colored noise with unknown covariance matrix. In the presence of colored noise, the multivariate regression (MV-R) algorithm models the source detection as a multivariate regression problem and infers the model order from the covariance matrix of the residual error. However, the MV-R algorithm requires that the noise is sufficiently weaker than the signal. In order to deal with these problems, this paper proposes a novel signal number estimation algorithm in the presence of colored noise based on the analysis of the behavior of information theoretic criteria. Firstly, a first criterion is defined as the ratio of the current eigenvalue and the mean of the next ones, and its properties is analyzed with respect to the over-modeling and under-modeling. Moreover, a second criterion is designed as the ratio of the current value and the next value of the first criterion, and its properties is analyzed with respect to the over-modeling and under-modeling. Then, a novel enhanced RMT estimator is proposed for signal number estimation by analyzing the detection properties among the signal number estimates obtained by these two criteria, the MV-R estimator and the RMT estimator to sequentially determine whether the eigenvalue being tested is arising from a signal or from noise. Finally, simulation results are presented to illustrate that the proposed enhanced RMT estimator has better estimation performance than the existing methods.","authors":["Huiyue Yi","Wuxiong Zhang","Hui Xu"],"url":"https://arxiv.org/abs/2211.12942"}
{"created":"2025-05-06","title":"One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data","abstract":"There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.","authors":["Simone Luetto","Fabrizio Garuti","Enver Sangineto","Lorenzo Forni","Rita Cucchiara"],"url":"https://arxiv.org/abs/2302.06375"}
{"created":"2025-05-06","title":"Transformadores: Fundamentos teoricos y Aplicaciones","abstract":"Transformers are a neural network architecture originally developed for natural language processing, which have since become a foundational tool for solving a wide range of problems, including text, audio, image processing, reinforcement learning, and other tasks involving heterogeneous input data. Their hallmark is the self-attention mechanism, which allows the model to weigh different parts of the input sequence dynamically, and is an evolution of earlier attention-based approaches. This article provides readers with the necessary background to understand recent research on transformer models, and presents the mathematical and algorithmic foundations of their core components. It also explores the architecture's various elements, potential modifications, and some of the most relevant applications. The article is written in Spanish to help make this scientific knowledge more accessible to the Spanish-speaking community.","authors":["Jordi de la Torre"],"url":"https://arxiv.org/abs/2302.09327"}
{"created":"2025-05-06","title":"Constrained Adversarial Learning for Automated Software Testing: a literature review","abstract":"It is imperative to safeguard computer applications and information systems against the growing number of cyber-attacks. Automated software testing tools can be developed to quickly analyze many lines of code and detect vulnerabilities by generating function-specific testing data. This process draws similarities to the constrained adversarial examples generated by adversarial machine learning methods, so there could be significant benefits to the integration of these methods in testing tools to identify possible attack vectors. Therefore, this literature review is focused on the current state-of-the-art of constrained data generation approaches applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance their software testing tools with adversarial testing methods and improve the resilience and robustness of their information systems. The found approaches were systematized, and the advantages and limitations of those specific for white-box, grey-box, and black-box testing were analyzed, identifying research gaps and opportunities to automate the testing tools with data generated by adversarial attacks.","authors":["Jo\\~ao Vitorino","Tiago Dias","Tiago Fonseca","Eva Maia","Isabel Pra\\c{c}a"],"url":"https://arxiv.org/abs/2303.07546"}
{"created":"2025-05-06","title":"When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions","abstract":"The intersection of Foundation Model (FM) and Federated Learning (FL) presents a unique opportunity to unlock new possibilities for real-world applications. On the one hand, FL, as a collaborative learning paradigm, help address challenges in FM development by expanding data availability, enabling computation sharing, facilitating the collaborative development of FMs, tackling continuous data update, avoiding FM monopoly, response delay and FM service down. On the other hand, FM, equipped with pre-trained knowledge and exceptional performance, can serve as a robust starting point for FL. It can also generate synthetic data to enrich data diversity and enhance overall performance of FL. Meanwhile, FM unlocks new sharing paradigm and multi-task and multi-modality capabilities for FL. By examining the interplay between FL and FM, this paper presents the motivations, challenges, and future directions of empowering FL with FM and empowering FM with FL. We hope that this work provides a good foundation to inspire future research efforts to drive advancements in both fields.","authors":["Weiming Zhuang","Chen Chen","Jingtao Li","Chaochao Chen","Yaochu Jin","Lingjuan Lyu"],"url":"https://arxiv.org/abs/2306.15546"}
{"created":"2025-05-06","title":"A Model Predictive Capture Point Control Framework for Robust Humanoid Balancing via Ankle, Hip, and Stepping Strategies","abstract":"The robust balancing capability of humanoids is essential for mobility in real environments. Many studies focus on implementing human-inspired ankle, hip, and stepping strategies to achieve human-level balance. In this paper, a robust balance control framework for humanoids is proposed. Firstly, a Model Predictive Control (MPC) framework is proposed for Capture Point (CP) tracking control, enabling the integration of ankle, hip, and stepping strategies within a single framework. Additionally, a variable weighting method is introduced that adjusts the weighting parameters of the Centroidal Angular Momentum damping control. Secondly, a hierarchical structure of the MPC and a stepping controller was proposed, allowing for the step time optimization. The robust balancing performance of the proposed method is validated through simulations and real robot experiments. Furthermore, a superior balancing performance is demonstrated compared to a state-of-the-art Quadratic Programming-based CP controller that employs the ankle, hip, and stepping strategies.","authors":["Myeong-Ju Kim","Daegyu Lim","Gyeongjae Park","Kwanwoo Lee","Jaeheung Park"],"url":"https://arxiv.org/abs/2307.13243"}
{"created":"2025-05-06","title":"Improving Audio-Text Retrieval via Hierarchical Cross-Modal Interaction and Auxiliary Captions","abstract":"Most existing audio-text retrieval (ATR) methods focus on constructing contrastive pairs between whole audio clips and complete caption sentences, while ignoring fine-grained cross-modal relationships, e.g., short segments and phrases or frames and words. In this paper, we introduce a hierarchical cross-modal interaction (HCI) method for ATR by simultaneously exploring clip-sentence, segment-phrase, and frame-word relationships, achieving a comprehensive multi-modal semantic comparison. Besides, we also present a novel ATR framework that leverages auxiliary captions (AC) generated by a pretrained captioner to perform feature interaction between audio and generated captions, which yields enhanced audio representations and is complementary to the original ATR matching branch. The audio and generated captions can also form new audio-text pairs as data augmentation for training. Experiments show that our HCI significantly improves the ATR performance. Moreover, our AC framework also shows stable performance gains on multiple datasets.","authors":["Yifei Xin","Yuexian Zou"],"url":"https://arxiv.org/abs/2307.15344"}
{"created":"2025-05-06","title":"SSTFormer: Bridging Spiking Neural Network and Memory Support Transformer for Frame-Event based Recognition","abstract":"Event camera-based pattern recognition is a newly arising research topic in recent years. Current researchers usually transform the event streams into images, graphs, or voxels, and adopt deep neural networks for event-based classification. Although good performance can be achieved on simple event recognition datasets, however, their results may be still limited due to the following two issues. Firstly, they adopt spatial sparse event streams for recognition only, which may fail to capture the color and detailed texture information well. Secondly, they adopt either Spiking Neural Networks (SNN) for energy-efficient recognition with suboptimal results, or Artificial Neural Networks (ANN) for energy-intensive, high-performance recognition. However, seldom of them consider achieving a balance between these two aspects. In this paper, we formally propose to recognize patterns by fusing RGB frames and event streams simultaneously and propose a new RGB frame-event recognition framework to address the aforementioned issues. The proposed method contains four main modules, i.e., memory support Transformer network for RGB frame encoding, spiking neural network for raw event stream encoding, multi-modal bottleneck fusion module for RGB-Event feature aggregation, and prediction head. Due to the scarce of RGB-Event based classification dataset, we also propose a large-scale PokerEvent dataset which contains 114 classes, and 27102 frame-event pairs recorded using a DVS346 event camera. Extensive experiments on two RGB-Event based classification datasets fully validated the effectiveness of our proposed framework. We hope this work will boost the development of pattern recognition by fusing RGB frames and event streams. Both our dataset and source code of this work will be released at https://github.com/Event-AHU/SSTFormer","authors":["Xiao Wang","Yao Rong","Zongzhen Wu","Lin Zhu","Bo Jiang","Jin Tang","Yonghong Tian"],"url":"https://arxiv.org/abs/2308.04369"}
{"created":"2025-05-06","title":"Inverse Dynamics Trajectory Optimization for Contact-Implicit Model Predictive Control","abstract":"Robots must make and break contact with the environment to perform useful tasks, but planning and control through contact remains a formidable challenge. In this work, we achieve real-time contact-implicit model predictive control with a surprisingly simple method: inverse dynamics trajectory optimization. While trajectory optimization with inverse dynamics is not new, we introduce a series of incremental innovations that collectively enable fast model predictive control on a variety of challenging manipulation and locomotion tasks. We implement these innovations in an open-source solver and present simulation examples to support the effectiveness of the proposed approach. Additionally, we demonstrate contact-implicit model predictive control on hardware at over 100 Hz for a 20-degree-of-freedom bi-manual manipulation task. Video and code are available at https://idto.github.io.","authors":["Vince Kurtz","Alejandro Castro","Aykut \\\"Ozg\\\"un \\\"Onol","Hai Lin"],"url":"https://arxiv.org/abs/2309.01813"}
{"created":"2025-05-06","title":"MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection","abstract":"In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT), a well-known methodology of transformers, have been shown the best results of all methods implemented. Finally, a web application has been developed to demonstrate the performance of the pre-trained top-performer model (BERT) for multi-label ER in Bangla.","authors":["Sumit Kumar Banshal","Sajal Das","Shumaiya Akter Shammi","Narayan Ranjan Chakraborty","Aulia Luqman Aziz","Mohammed Aljuaid","Fazla Rabby","Rohit Bansal"],"url":"https://arxiv.org/abs/2309.15670"}
{"created":"2025-05-06","title":"Bandwidth Parameterized by Cluster Vertex Deletion Number","abstract":"Given a graph $G$ and an integer $b$, Bandwidth asks whether there exists a bijection $\\pi$ from $V(G)$ to $\\{1, \\ldots, |V(G)|\\}$ such that $\\max_{\\{u, v \\} \\in E(G)} | \\pi(u) - \\pi(v) | \\leq b$. This is a classical NP-complete problem, known to remain NP-complete even on very restricted classes of graphs, such as trees of maximum degree 3 and caterpillars of hair length 3. In the realm of parameterized complexity, these results imply that the problem remains NP-hard on graphs of bounded pathwidth, while it is additionally known to be W[1]-hard when parameterized by the tree-depth of the input graph. In contrast, the problem does become FPT when parameterized by the vertex cover number. In this paper we make progress in understanding the parameterized (in)tractability of Bandwidth. We first show that it is FPT when parameterized by the cluster vertex deletion number cvd plus the clique number $\\omega$, thus significantly strengthening the previously mentioned result for vertex cover number. On the other hand, we show that Bandwidth is W[1]-hard when parameterized only by cvd. Our results develop and generalize some of the methods of argumentation of the previous results and narrow some of the complexity gaps.","authors":["Tatsuya Gima","Eun Jung Kim","Noleen K\\\"ohler","Nikolaos Melissinos","Manolis Vasilakis"],"url":"https://arxiv.org/abs/2309.17204"}
{"created":"2025-05-06","title":"Generalized Animal Imitator: Agile Locomotion with Versatile Motion Prior","abstract":"The agility of animals, particularly in complex activities such as running, turning, jumping, and backflipping, stands as an exemplar for robotic system design. Transferring this suite of behaviors to legged robotic systems introduces essential inquiries: How can a robot learn multiple locomotion behaviors simultaneously? How can the robot execute these tasks with a smooth transition? How to integrate these skills for wide-range applications? This paper introduces the Versatile Instructable Motion prior (VIM) - a Reinforcement Learning framework designed to incorporate a range of agile locomotion tasks suitable for advanced robotic applications. Our framework enables legged robots to learn diverse agile low-level skills by imitating animal motions and manually designed motions. Our Functionality reward guides the robot's ability to adopt varied skills, and our Stylization reward ensures that robot motions align with reference motions. Our evaluations of the VIM framework span both simulation and the real world. Our framework allows a robot to concurrently learn diverse agile locomotion skills using a single learning-based controller in the real world. Videos can be found on our website: https://rchalyang.github.io/VIM/","authors":["Ruihan Yang","Zhuoqun Chen","Jianhan Ma","Chongyi Zheng","Yiyu Chen","Quan Nguyen","Xiaolong Wang"],"url":"https://arxiv.org/abs/2310.01408"}
{"created":"2025-05-06","title":"A simple connection from loss flatness to compressed neural representations","abstract":"Sharpness, a geometric measure in the parameter space that reflects the flatness of the loss landscape, has long been studied for its potential connections to neural network behavior. While sharpness is often associated with generalization, recent work highlights inconsistencies in this relationship, leaving its true significance unclear. In this paper, we investigate how sharpness influences the local geometric features of neural representations in feature space, offering a new perspective on its role. We introduce this problem and study three measures for compression: the Local Volumetric Ratio (LVR), based on volume compression, the Maximum Local Sensitivity (MLS), based on sensitivity to input changes, and the Local Dimensionality, based on how uniform the sensitivity is on different directions. We show that LVR and MLS correlate with the flatness of the loss around the local minima; and that this correlation is predicted by a relatively simple mathematical relationship: a flatter loss corresponds to a lower upper bound on the compression metrics of neural representations. Our work builds upon the linear stability insight by Ma and Ying, deriving inequalities between various compression metrics and quantities involving sharpness. Our inequalities readily extend to reparametrization-invariant sharpness as well. Through empirical experiments on various feedforward, convolutional, and transformer architectures, we find that our inequalities predict a consistently positive correlation between local representation compression and sharpness.","authors":["Shirui Chen","Stefano Recanatesi","Eric Shea-Brown"],"url":"https://arxiv.org/abs/2310.01770"}
{"created":"2025-05-06","title":"On a Hierarchy of Spectral Invariants for Graphs","abstract":"We consider a hierarchy of graph invariants that naturally extends the spectral invariants defined by F\\\"urer (Lin. Alg. Appl. 2010) based on the angles formed by the set of standard basis vectors and their projections onto eigenspaces of the adjacency matrix. We provide a purely combinatorial characterization of this hierarchy in terms of the walk counts. This allows us to give a complete answer to F\\\"urer's question about the strength of his invariants in distinguishing non-isomorphic graphs in comparison to the 2-dimensional Weisfeiler-Leman algorithm, extending the recent work of Rattan and Seppelt (SODA 2023). As another application of the characterization, we prove that almost all graphs are determined up to isomorphism in terms of the spectrum and the angles, which is of interest in view of the long-standing open problem whether almost all graphs are determined by their eigenvalues alone. Finally, we describe the exact relationship between the hierarchy and the Weisfeiler-Leman algorithms for small dimensions, as also some other important spectral characteristics of a graph such as the generalized and the main spectra.","authors":["V. Arvind","Frank Fuhlbr\\\"uck","Johannes K\\\"obler","Oleg Verbitsky"],"url":"https://arxiv.org/abs/2310.04391"}
{"created":"2025-05-06","title":"Supercharging Graph Transformers with Advective Diffusion","abstract":"The capability of generalization is a cornerstone for the success of modern learning systems. For non-Euclidean data, e.g., graphs, that particularly involves topological structures, one important aspect neglected by prior studies is how machine learning models generalize under topological shifts. This paper proposes AdvDIFFormer, a physics-inspired graph Transformer model designed to address this challenge. The model is derived from advective diffusion equations which describe a class of continuous message passing process with observed and latent topological structures. We show that AdvDIFFormer has provable capability for controlling generalization error with topological shifts, which in contrast cannot be guaranteed by graph diffusion models. Empirically, the model demonstrates superiority in various predictive tasks across information networks, molecular screening and protein interactions.","authors":["Qitian Wu","Chenxiao Yang","Kaipeng Zeng","Michael Bronstein"],"url":"https://arxiv.org/abs/2310.06417"}
{"created":"2025-05-06","title":"OptiReduce: Resilient and Tail-Optimal AllReduce for Distributed Deep Learning in the Cloud","abstract":"We present OptiReduce, a new collective-communication system for the cloud with bounded, predictable completion times for deep-learning jobs in the presence of varying computation (stragglers) and communication (congestion and gradient drops) variabilities. OptiReduce exploits the inherent resiliency and the stochastic nature of distributed deep-learning (DDL) training and fine-tuning to work with approximated (or lost) gradients -- providing an efficient balance between (tail) performance and the resulting accuracy of the trained models.","authors":["Ertza Warraich","Omer Shabtai","Khalid Manaa","Shay Vargaftik","Yonatan Piasetzky","Matty Kadosh","Lalith Suresh","Muhammad Shahbaz"],"url":"https://arxiv.org/abs/2310.06993"}
{"created":"2025-05-06","title":"Bandwidth Efficient Livestreaming in Mobile Wireless Networks: A Peer-to-Peer ACIDE Solution","abstract":"In mobile wireless networks, livestreaming in high user density areas presents two typical challenges: the wireless bandwidth is depleted and the number of users is limited. In this study, a media distribution model utilizing peer to peer communications, Active Control in an Intelligent and Distributed Environment, is proposed for bandwidth efficient livestreaming. The basic idea is to group users with identical livestream interest in a cluster of n peers. Instead of sending n copies of a livestream package, only one copy is sent to the cluster. A package is divided into n blocks. Each user receives one block from the base station and the remaining n-1 blocks from the other peers. Two optimization problems are addressed. The first problem is minimizing the bandwidth needed to guarantee a continuous live media play on all peers. A solution is proposed to find the optimal block sizes such that the wireless bandwidth is minimized. The second problem is maximizing the number of peers admitted to a cluster, given a fixed wireless bandwidth. This problem is NP-complete and a greedy strategy is proposed to calculate a feasible solution for peer selection. The proposed model improves the bandwidth efficiency and allows more users to be served.","authors":["Andrei Negulescu","Weijia Shang"],"url":"https://arxiv.org/abs/2310.14283"}
{"created":"2025-05-06","title":"Privacy Preserving Event Detection","abstract":"This paper presents a privacy-preserving event detection scheme based on measurements made by a network of sensors. A diameter-like decision statistic made up of the marginal types of the measurements observed by the sensors is employed. The proposed detection scheme can achieve the best type-I error exponent as the type-II error rate is required to be negligible. Detection performance with finite-length observations is also demonstrated through a simulation example of spectrum sensing. Privacy protection is achieved by obfuscating the sensors' marginal types with random zero-modulo-sum numbers that are generated and distributed via the exchange of encrypted messages among the sensors. The privacy-preserving performance against \"honest but curious\" adversaries, including colluding sensors, the fusion center, and external eavesdroppers, is analyzed through a series of cryptographic games. It is shown that the probability that any probabilistic polynomial time adversary successfully estimates the sensors' measured types cannot be much better than independent guessing, when there are at least two non-colluding sensors.","authors":["Xiaoshan Wang","Tan F. Wong"],"url":"https://arxiv.org/abs/2312.00933"}
{"created":"2025-05-06","title":"Improving Code Reviewer Recommendation: Accuracy, Latency, Workload, and Bystanders","abstract":"The code review team at Meta is continuously improving the code review process. To evaluate the new recommenders, we conduct three A/B tests which are a type of randomized controlled experimental trial.","authors":["Peter C. Rigby","Seth Rogers","Sadruddin Saleem","Parth Suresh","Daniel Suskin","Patrick Riggs","Chandra Maddila","Nachiappan Nagappan"],"url":"https://arxiv.org/abs/2312.17169"}
{"created":"2025-05-06","title":"Inference and Visualization of Community Structure in Attributed Hypergraphs Using Mixed-Membership Stochastic Block Models","abstract":"Hypergraphs represent complex systems involving interactions among more than two entities and allow the investigation of higher-order structure and dynamics in complex systems. Node attribute data, which often accompanies network data, can enhance the inference of community structure in complex systems. While mixed-membership stochastic block models have been employed to infer community structure in hypergraphs, they complicate the visualization and interpretation of inferred community structure by assuming that nodes may possess soft community memberships. In this study, we propose a framework, HyperNEO, that combines mixed-membership stochastic block models for hypergraphs with dimensionality reduction methods. Our approach generates a node layout that largely preserves the community memberships of nodes. We evaluate our framework on both synthetic and empirical hypergraphs with node attributes. We expect our framework will broaden the investigation and understanding of higher-order community structure in complex systems.","authors":["Kazuki Nakajima","Takeaki Uno"],"url":"https://arxiv.org/abs/2401.00688"}
{"created":"2025-05-06","title":"Conjugate Direction Methods Under Inconsistent Systems","abstract":"Since the development of the conjugate gradient (CG) method in 1952 by Hestenes and Stiefel, CG, has become an indispensable tool in computational mathematics for solving positive definite linear systems. On the other hand, the conjugate residual (CR) method, closely related CG and introduced by Stiefel in 1955 for the same settings, remains relatively less known outside the numerical linear algebra community. Since their inception, these methods -- henceforth collectively referred to as conjugate direction methods -- have been extended beyond positive definite to indefinite, albeit consistent, settings. Going one step further, in this paper, we investigate the theoretical and empirical properties of these methods under inconsistent systems. Among other things, we show that small modifications to the original algorithms allow for the pseudo-inverse solution. Furthermore, we show that CR is essentially equivalent to the minimum residual method, proposed by Paige and Saunders in 1975, in such contexts. Lastly, we conduct a series of numerical experiments to shed lights on their numerical stability (or lack thereof) and their performance for inconsistent systems. Surprisingly, we will demonstrate that, unlike CR and contrary to popular belief, CG can exhibit significant numerical instability, bordering on catastrophe in some instances.","authors":["Alexander Lim","Yang Liu","Fred Roosta"],"url":"https://arxiv.org/abs/2401.11714"}
{"created":"2025-05-06","title":"HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable In-The-Wild Navigation","abstract":"Routes represent an integral part of triggering emotions in drivers. Navigation systems allow users to choose a navigation strategy, such as the fastest or shortest route. However, they do not consider the driver's emotional well-being. We present HappyRouting, a novel navigation-based empathic car interface guiding drivers through real-world traffic while evoking positive emotions. We propose design considerations, derive a technical architecture, and implement a routing optimization framework. Our contribution is a machine learning-based generated emotion map layer, predicting emotions along routes based on static and dynamic contextual data. We evaluated HappyRouting in a real-world driving study (N=13), finding that happy routes increase subjectively perceived valence by 11% (p=.007). Although happy routes take 1.25 times longer on average, participants perceived the happy route as shorter, presenting an emotion-enhanced alternative to today's fastest routing mechanisms. We discuss how emotion-based routing can be integrated into navigation apps, promoting emotional well-being for mobility use.","authors":["David Bethge","Daniel Bulanda","Adam Kozlowski","Thomas Kosch","Albrecht Schmidt","Tobias Grosse-Puppendahl"],"url":"https://arxiv.org/abs/2401.15695"}
{"created":"2025-05-06","title":"SMUTF: Schema Matching Using Generative Tags and Hybrid Features","abstract":"We introduce SMUTF (Schema Matching Using Generative Tags and Hybrid Features), a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy \"generative tags\" for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.","authors":["Yu Zhang","Mei Di","Haozheng Luo","Chenwei Xu","Richard Tzong-Han Tsai"],"url":"https://arxiv.org/abs/2402.01685"}
{"created":"2025-05-06","title":"Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off","abstract":"To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This capability implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by order of $\\sqrt{d}$, where $d$ is the input dimension. We illustrate our theoretical results with experiments on representative datasets and observe significant performance improvements and strict privacy guarantees under different privacy settings. The code is available at https://github.com/6lyc/FedCEO_Collaborate-with-Each-Other.","authors":["Yuecheng Li","Lele Fu","Tong Wang","Jian Lou","Bin Chen","Lei Yang","Jian Shen","Zibin Zheng","Chuan Chen"],"url":"https://arxiv.org/abs/2402.07002"}
{"created":"2025-05-06","title":"Conformal Predictive Programming for Chance Constrained Optimization","abstract":"We propose conformal predictive programming (CPP), a framework to solve chance constrained optimization problems, i.e., optimization problems with constraints that are functions of random variables. CPP utilizes samples from these random variables along with the quantile lemma - central to conformal prediction - to transform the chance constrained optimization problem into a deterministic problem with a quantile reformulation. CPP inherits a priori guarantees on constraint satisfaction from existing sample average approximation approaches for a class of chance constrained optimization problems, and it provides a posteriori guarantees that are of conditional and marginal nature otherwise. The strength of CPP is that it can easily support different variants of conformal prediction which have been (or will be) proposed within the conformal prediction community. To illustrate this, we present robust CPP to deal with distribution shifts in the random variables and Mondrian CPP to deal with class conditional chance constraints. To enable tractable solutions to the quantile reformulation, we present a mixed integer programming method (CPP-MIP) encoding, a bilevel optimization strategy (CPP-Bilevel), and a sampling-and-discarding optimization strategy (CPP-Discarding). We also extend CPP to deal with joint chance constrained optimization (JCCO). In a series of case studies, we show the validity of the aforementioned approaches, empirically compare CPP-MIP, CPP-Bilevel, as well as CPP-Discarding, and illustrate the advantage of CPP as compared to scenario approach.","authors":["Yiqi Zhao","Xinyi Yu","Matteo Sesia","Jyotirmoy V. Deshmukh","Lars Lindemann"],"url":"https://arxiv.org/abs/2402.07407"}
{"created":"2025-05-06","title":"Numerical Exploration of Nonlinear Dispersion Effects via a Strongly Coupled Two-scale System","abstract":"The effective, fast transport of matter through porous media is often characterized by complex dispersion effects. To describe in mathematical terms such situations, instead of a simple macroscopic equation (as in the classical Darcy's law), one may need to consider two-scale boundary-value problems with full coupling between the scales where the macroscopic transport depends non-linearly on local (i.e. microscopic) drift interactions, which are again influenced by local concentrations. Such two-scale problems are computationally very expensive as numerous elliptic partial differential equations (cell problems) have to constantly be recomputed. In this work, we investigate such an effective two-scale model involving a suitable nonlinear dispersion term and explore numerically the behavior of its weak solutions. We introduce two distinct numerical schemes dealing with the same non-linear scale-coupling: (i) a Picard-type iteration and (ii) a time discretization decoupling. In addition, we propose a precomputing strategy where the calculations of cell problems are pushed into an offline phase. Our approach works for both schemes and significantly reduces computation times. We prove that the proposed precomputing strategy converges to the exact solution. Finally, we test our schemes via several numerical experiments that illustrate dispersion effects introduced by specific choices of microstructure and model ingredients.","authors":["Surendra Nepal","Vishnu Raveendran","Michael Eden","Rainey Lyons","Adrian Muntean"],"url":"https://arxiv.org/abs/2402.09607"}
{"created":"2025-05-06","title":"Mochi: Fast \\& Exact Collision Detection","abstract":"Collision Detection (CD) has several applications across the domains such as robotics, visual graphics, and fluid mechanics. Finding exact collisions between the objects in the scene is quite computationally intensive. To quickly filter the object pairs that do not result in a collision, bounding boxes are built on the objects, indexed using a Bounding Volume Hierarchy(BVH), and tested for intersection before performing the expensive object-object intersection tests. In state-of-the-art CD libraries, accelerators such as GPUs are used to accelerate BVH traversal by building specialized data structures. The recent addition of ray tracing architecture to GPU hardware is designed to do the same but in the context of implementing a Ray Tracing algorithm to render a graphical scene in real-time. We present Mochi, a fast and exact collision detection engine that accelerates both the broad and narrow phases by taking advantage of the capabilities of Ray Tracing cores. We introduce multiple new reductions to perform generic CD to support three types of objects for CD: simple spherical particles, objects describable by mathematical equations, and complex objects composed of a triangle mesh. By implementing our reductions, Mochi achieves several orders of magnitude speedups on synthetic datasets and 5x-28x speedups on real-world triangle mesh datasets. We further evaluate our reductions thoroughly and provide several architectural insights on the ray tracing cores that are otherwise unknown due to their proprietorship.","authors":["Durga Keerthi Mandarapu","Nicholas James","Milind Kulkarni"],"url":"https://arxiv.org/abs/2402.14801"}
{"created":"2025-05-06","title":"Efficient State Space Model via Fast Tensor Convolution and Block Diagonalization","abstract":"Existing models encounter bottlenecks in balancing performance and computational efficiency when modeling long sequences. Although the state space model (SSM) has achieved remarkable success in handling long sequence tasks, it still faces the problem of large number of parameters. In order to further improve the efficiency of SSM, we propose a new state space layer based on multiple-input multiple-output SSM, called efficient SSM (eSSM). Our eSSM is built on the convolutional representation of multi-input and multi-input (MIMO) SSM. We propose a variety of effective strategies to improve the computational efficiency. The diagonalization of the system matrix first decouples the original system. Then a fast tensor convolution is proposed based on the fast Fourier transform. In addition, the block diagonalization of the SSM further reduces the model parameters and improves the model flexibility. Extensive experimental results show that the performance of the proposed model on multiple databases matches the performance of state-of-the-art models, such as S4, and is significantly better than Transformers and LSTM. In the model efficiency benchmark, the parameters of eSSM are only 12.89\\% of LSTM and 13.24\\% of Mamba. The training speed of eSSM is 3.94 times faster than LSTM and 1.35 times faster than Mamba. Code is available at: \\href{https://github.com/leonty1/essm}{https://github.com/leonty1/essm}.","authors":["Tongyi Liang","Han-Xiong Li"],"url":"https://arxiv.org/abs/2402.15290"}
{"created":"2025-05-06","title":"REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories","abstract":"Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, REPLAY not only resorts to the spatiotemporal distances in sparse trajectories to search for the informative past hidden states, but also accommodates the time-varying temporal regularities by incorporating smoothed timestamp embeddings using Gaussian weighted averaging with timestamp-specific learnable bandwidths, which can flexibly adapt to the temporal regularities of different strengths across different timestamps. Our extensive evaluation compares REPLAY against a sizable collection of state-of-the-art techniques on two real-world datasets. Results show that REPLAY consistently and significantly outperforms state-of-the-art methods by 7.7\\%-10.5\\% in the location prediction task, and the bandwidths reveal interesting patterns of the time-varying temporal regularities.","authors":["Bangchao Deng","Bingqing Qu","Pengyang Wang","Dingqi Yang","Benjamin Fankhauser","Philippe Cudre-Mauroux"],"url":"https://arxiv.org/abs/2402.16310"}
{"created":"2025-05-06","title":"GenAINet: Enabling Wireless Collective Intelligence via Knowledge Transfer and Reasoning","abstract":"Generative Artificial Intelligence (GenAI) and communication networks are expected to have groundbreaking synergies for 6G. Connecting GenAI agents via a wireless network can potentially unleash the power of Collective Intelligence (CI) and pave the way for Artificial General Intelligence (AGI). However, current wireless networks are designed as a \"data pipe\" and are not suited to accommodate and leverage the power of GenAI. In this paper, we propose the GenAINet framework in which distributed GenAI agents communicate knowledge (facts, experiences, and methods) to accomplish arbitrary tasks. We first propose an architecture for a single GenAI agent and then provide a network architecture integrating GenAI capabilities to manage both network protocols and applications. Building on this, we investigate effective communication and reasoning problems by proposing a semantic-native GenAINet. Specifically, GenAI agents extract semantics from heterogeneous raw data, build and maintain a knowledge model representing the semantic relationships among pieces of knowledge, which is retrieved by GenAI models for planning and reasoning. Under this paradigm, different levels of collaboration can be achieved flexibly depending on the complexity of targeted tasks. Furthermore, we conduct two case studies in which, through wireless device queries, we demonstrate that extracting, compressing and transferring common knowledge can improve query accuracy while reducing communication costs; and in the wireless power control problem, we show that distributed agents can complete general tasks independently through collaborative reasoning without predefined communication protocols. Finally, we discuss challenges and future research directions in applying Large Language Models (LLMs) in 6G networks.","authors":["Hang Zou","Qiyang Zhao","Samson Lasaulce","Lina Bariah","Mehdi Bennis","Merouane Debbah"],"url":"https://arxiv.org/abs/2402.16631"}
{"created":"2025-05-06","title":"Finite element schemes with tangential motion for fourth order geometric curve evolutions in arbitrary codimension","abstract":"We introduce novel finite element schemes for curve diffusion and elastic flow in arbitrary codimension. The schemes are based on a variational form of a system that includes a specifically chosen tangential motion. We derive optimal $L^2$- and $H^1$-error bounds for continuous-in-time semidiscrete finite element approximations that use piecewise linear elements. In addition, we consider fully discrete schemes and, in the case of curve diffusion, prove unconditional stability for it. Finally, we present several numerical simulations, including some convergence experiments that confirm the derived error bounds. The presented simulations suggest that the tangential motion leads to equidistribution in practice.","authors":["Klaus Deckelnick","Robert N\\\"urnberg"],"url":"https://arxiv.org/abs/2402.16799"}
{"created":"2025-05-06","title":"Scalable and Interpretable Identification of Minimal Undesignable RNA Structure Motifs with Rotational Invariance","abstract":"RNA design aims to find a sequence that folds with highest probability into a designated target structure. However, certain structures are undesignable, meaning no sequence can fold into the target structure under the default (Turner) RNA folding model. Understanding the specific local structures (i.e., \"motifs\") that contribute to undesignability is crucial for refining RNA folding models and determining the limits of RNA designability. Despite its importance, this problem has received very little attention, and previous efforts are neither scalable nor interpretable. We develop a new theoretical framework for motif (un-)designability, and design scalable and interpretable algorithms to identify minimal undesignable motifs within a given RNA secondary structure. Our approach establishes motif undesignability by searching for rival motifs, rather than exhaustively enumerating all (partial) sequences that could potentially fold into the motif. Furthermore, we exploit rotational invariance in RNA structures to detect, group, and reuse equivalent motifs and to construct a database of unique minimal undesignable motifs. To achieve that, we propose a loop-pair graph representation for motifs and a recursive graph isomorphism algorithm for motif equivalence. Our algorithms successfully identify 24 unique minimal undesignable motifs among 18 undesignable puzzles from the Eterna100 benchmark. Surprisingly, we also find over 350 unique minimal undesignable motifs and 663 undesignable native structures in the ArchiveII dataset, drawn from a diverse set of RNA families. Our source code is available at https://github.com/shanry/RNA-Undesign and our web server is available at http://linearfold.org/motifs.","authors":["Tianshuo Zhou","Wei Yu Tang","Apoorv Malik","David H. Mathews","Liang Huang"],"url":"https://arxiv.org/abs/2402.17206"}
{"created":"2025-05-06","title":"DECIDER: A Dual-System Rule-Controllable Decoding Framework for Language Generation","abstract":"Constrained decoding approaches aim to control the meaning or style of text generated by the pre-trained large language models (LLMs or also PLMs) for various tasks at inference time. However, these methods often guide plausible continuations by greedily and explicitly selecting targets. Though fulfilling the task requirements, these methods may overlook certain general and natural logics that humans would implicitly follow towards such targets. Inspired by cognitive dual-process theory, in this work, we propose a novel decoding framework DECIDER where the base LLMs are equipped with a First-Order Logic (FOL) reasoner to express and evaluate the rules, along with a decision function that merges the outputs of both systems to guide the generation. Unlike previous constrained decodings, DECIDER transforms the encouragement of target-specific words into all words that satisfy several high-level rules, enabling us to programmatically integrate our logic into LLMs. Experiments on CommonGen and PersonaChat demonstrate that DECIDER effectively follows given FOL rules to guide LLMs in a more human-like and logic-controlled manner.","authors":["Chen Xu","Tian Lan","Yu Ji","Changlong Yu","Wei Wang","Jun Gao","Qunxi Dong","Kun Qian","Piji Li","Wei Bi","Bin Hu"],"url":"https://arxiv.org/abs/2403.01954"}
{"created":"2025-05-06","title":"Optimizing Inventory Placement for a Downstream Online Matching Problem","abstract":"We study the inventory placement problem of splitting $Q$ units of a single item across warehouses in advance of a downstream online matching problem that represents the dynamic fulfillment decisions of an e-commerce retailer. This is a challenging problem both theoretically, due to the computational complexity of the downstream matching problem, and practically, as the fulfillment team continuously updates its algorithm while the placement team lacks direct evaluation of placement decisions.","authors":["Boris Epstein (Columbia University)","Will Ma (Columbia University)"],"url":"https://arxiv.org/abs/2403.04598"}
{"created":"2025-05-06","title":"Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain","abstract":"Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructing dynamic graphs, which enables the learning of higher-level semantic representations considering temporal perspectives, addressing the challenges in fMRI data representation learning. Utilizing the large-scale UK Biobank dataset for self-supervised learning, ST-JEMA shows exceptional representation learning performance on dynamic functional connectivity demonstrating superiority over previous methods in predicting phenotypes and psychiatric diagnoses across eight benchmark fMRI datasets even with limited samples and effectiveness of temporal reconstruction on missing data scenarios. These findings highlight the potential of our approach as a robust representation learning method for leveraging label-scarce fMRI data.","authors":["Jungwon Choi","Hyungi Lee","Byung-Hoon Kim","Juho Lee"],"url":"https://arxiv.org/abs/2403.06432"}
{"created":"2025-05-06","title":"Impact of Noisy Supervision in Foundation Model Learning","abstract":"Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are significantly different. These observations are agnostic to scales of pre-training datasets, pre-training noise types, model architectures, pre-training objectives, downstream tuning methods, and downstream applications. We empirically ascertain that the reason behind this is that the pre-training noise shapes the feature space differently. We then propose a tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization, which is applicable in both parameter-efficient and black-box tuning manners. We additionally conduct extensive experiments on popular vision and language models, including APIs, which are supervised and self-supervised pre-trained on realistic noisy data for evaluation. Our analysis and results demonstrate the importance of this novel and fundamental research direction, which we term as Noisy Model Learning.","authors":["Hao Chen","Zihan Wang","Ran Tao","Hongxin Wei","Xing Xie","Masashi Sugiyama","Bhiksha Raj","Jindong Wang"],"url":"https://arxiv.org/abs/2403.06869"}
{"created":"2025-05-06","title":"Time-Constrained Erasure Correction for Data Recovery in UAV-LoRa-WuR Networks","abstract":"We described two erasure-correction schemes for data recovery in UAV-LoRa-WuR networks. Our results show that unless the maximum number for redundant frames a sensor can send per data-collection cycle is very small, erasure coding provides noticeable improvements over an uncoded transmissions. Whether to employ coding -- and if so, which type -- should be determined based on the sensors' energy budget (which dictates the maximum redundancy), the UAV's hovering time, and the node density. The analytical framework presented above aids in this decision making.","authors":["Kushwanth Sistu","Siddhartha S. Borkotoky"],"url":"https://arxiv.org/abs/2403.09782"}
{"created":"2025-05-06","title":"DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers","abstract":"Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.","authors":["Xuanlei Zhao","Shenggan Cheng","Chang Chen","Zangwei Zheng","Ziming Liu","Zheming Yang","Yang You"],"url":"https://arxiv.org/abs/2403.10266"}
{"created":"2025-05-06","title":"ParaICL: Towards Parallel In-Context Learning","abstract":"Large language models (LLMs) have become the norm in natural language processing (NLP), excelling in few-shot in-context learning (ICL) with their remarkable abilities. Nonetheless, the success of ICL largely hinges on the choice of few-shot demonstration examples, making the selection process increasingly crucial. Existing methods have delved into optimizing the quantity and semantic similarity of these examples to improve ICL performances. However, our preliminary experiments indicate that the effectiveness of ICL is limited by the length of the input context. Moreover, varying combinations of few-shot demonstration examples can significantly boost accuracy across different test samples. To address this, we propose a novel method named parallel in-context learning (ParaICL) that effectively utilizes all demonstration examples without exceeding the manageable input context length. ParaICL employs parallel batching to distribute demonstration examples into different batches according to the semantic similarities of the questions in the demonstrations to the test question. It then computes normalized batch semantic scores for each batch. A weighted average semantic objective, constrained by adaptive plausibility, is applied to select the most appropriate tokens. Through extensive experiments, we validate the effectiveness of ParaICL and conduct ablation studies to underscore its design rationale. We further demonstrate that ParaICL can seamlessly integrate with existing methods.","authors":["Xingxuan Li","Xuan-Phi Nguyen","Shafiq Joty","Lidong Bing"],"url":"https://arxiv.org/abs/2404.00570"}
{"created":"2025-05-06","title":"FireANTs: Adaptive Riemannian Optimization for Multi-Scale Diffeomorphic Matching","abstract":"The paper proposes FireANTs, the first multi-scale Adaptive Riemannian Optimization algorithm for dense diffeomorphic image matching. One of the most critical and understudied aspects of diffeomorphic image matching algorithms are its highly ill-conditioned nature. We quantitatively capture the extent of ill-conditioning in a typical MRI matching task, motivating the need for an adaptive optimization algorithm for diffeomorphic matching. To this end, FireANTs generalizes the concept of momentum and adaptive estimates of the Hessian to mitigate this ill-conditioning in the non-Euclidean space of diffeomorphisms. Unlike common non-Euclidean manifolds, we also formalize considerations for multi-scale optimization of diffeomorphisms. Our rigorous mathematical results and operational contributions lead to a state-of-the-art dense matching algorithm that can be applied to generic image data with remarkable accuracy and robustness. We demonstrate consistent improvements in image matching performance across a spectrum of community-standard medical and biological correspondence matching challenges spanning a wide variety of image modalities, anatomies, resolutions, acquisition protocols, and preprocessing pipelines. This improvement is supplemented by 300x to 3200x speedup over existing CPU-based state-of-the-art algorithms. For the first time, we perform diffeomorphic matching of sub-micron mouse isocortex volumes at native resolution, and generate a 25{\\mu}m mouse brain atlas in under 25 minutes. Our fast implementation also enables hyperparameter studies that were intractable with existing correspondence matching algorithms.","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"url":"https://arxiv.org/abs/2404.01249"}
{"created":"2025-05-06","title":"CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement","abstract":"We propose CHOSEN, a simple yet flexible, robust and effective multi-view depth refinement framework. It can be employed in any existing multi-view stereo pipeline, with straightforward generalization capability for different multi-view capture systems such as camera relative positioning and lenses. Given an initial depth estimation, CHOSEN iteratively re-samples and selects the best hypotheses, and automatically adapts to different metric or intrinsic scales determined by the capture system. The key to our approach is the application of contrastive learning in an appropriate solution space and a carefully designed hypothesis feature, based on which positive and negative hypotheses can be effectively distinguished. Integrated in a simple baseline multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of depth and normal accuracy compared to many current deep learning based multi-view stereo pipelines.","authors":["Di Qiu","Yinda Zhang","Thabo Beeler","Vladimir Tankovich","Christian H\\\"ane","Sean Fanello","Christoph Rhemann","Sergio Orts Escolano"],"url":"https://arxiv.org/abs/2404.02225"}
{"created":"2025-05-06","title":"Generative-Contrastive Heterogeneous Graph Neural Network","abstract":"Heterogeneous Graphs (HGs) effectively model complex relationships in the real world through multi-type nodes and edges. In recent years, inspired by self-supervised learning (SSL), contrastive learning (CL)-based Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential in utilizing data augmentation and contrastive discriminators for downstream tasks. However, data augmentation remains limited due to the graph data's integrity. Furthermore, the contrastive discriminators suffer from sampling bias and lack local heterogeneous information. To tackle the above limitations, we propose a novel Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN). Specifically, we propose a heterogeneous graph generative learning method that enhances CL-based paradigm. This paradigm includes: 1) A contrastive view augmentation strategy using a masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generating hard negative samples. 3) A hierarchical contrastive learning strategy aimed at capturing local and global information. Furthermore, the hierarchical contrastive learning and sampling strategies aim to constitute an enhanced contrastive discriminator under the generative-contrastive perspective. Finally, we compare our model with seventeen baselines on eight real-world datasets. Our model outperforms the latest baselines on node classification and link prediction tasks.","authors":["Yu Wang","Lei Sang","Yi Zhang","Yiwen Zhang","Xindong Wu"],"url":"https://arxiv.org/abs/2404.02810"}
{"created":"2025-05-06","title":"Multilingual Brain Surgeon: Large Language Models Can be Compressed Leaving No Language Behind","abstract":"Large Language Models (LLMs) have ushered in a new era in Natural Language Processing, but their massive size demands effective compression techniques for practicality. Although numerous model compression techniques have been investigated, they typically rely on a calibration set that overlooks the multilingual context and results in significant accuracy degradation for low-resource languages. This paper introduces Multilingual Brain Surgeon (MBS), a novel calibration data sampling method for multilingual LLMs compression. MBS overcomes the English-centric limitations of existing methods by sampling calibration data from various languages proportionally to the language distribution of the model training datasets. Our experiments, conducted on the BLOOM multilingual LLM, demonstrate that MBS improves the performance of existing English-centric compression methods, especially for low-resource languages. We also uncover the dynamics of language interaction during compression, revealing that the larger the proportion of a language in the training set and the more similar the language is to the calibration language, the better performance the language retains after compression. In conclusion, MBS presents an innovative approach to compressing multilingual LLMs, addressing the performance disparities and improving the language inclusivity of existing compression techniques.","authors":["Hongchuan Zeng","Hongshen Xu","Lu Chen","Kai Yu"],"url":"https://arxiv.org/abs/2404.04748"}
{"created":"2025-05-06","title":"LeapFrog: The Rowhammer Instruction Skip Attack","abstract":"Since its inception, Rowhammer exploits have rapidly evolved into increasingly sophisticated threats compromising data integrity and the control flow integrity of victim processes. Nevertheless, it remains a challenge for an attacker to identify vulnerable targets (i.e., Rowhammer gadgets), understand the outcome of the attempted fault, and formulate an attack that yields useful results.","authors":["Andrew Adiletta","M. Caner Tol","Kemal Derya","Berk Sunar","Saad Islam"],"url":"https://arxiv.org/abs/2404.07878"}
{"created":"2025-05-06","title":"MOWA: Multiple-in-One Image Warping Model","abstract":"While recent image warping approaches achieved remarkable success on existing benchmarks, they still require training separate models for each specific task and cannot generalize well to different camera models or customized manipulations. To address diverse types of warping in practice, we propose a Multiple-in-One image WArping model (named MOWA) in this work. Specifically, we mitigate the difficulty of multi-task learning by disentangling the motion estimation at both the region level and pixel level. To further enable dynamic task-aware image warping, we introduce a lightweight point-based classifier that predicts the task type, serving as prompts to modulate the feature maps for more accurate estimation. To our knowledge, this is the first work that solves multiple practical warping tasks in one single model. Extensive experiments demonstrate that our MOWA, which is trained on six tasks for multiple-in-one image warping, outperforms state-of-the-art task-specific models across most tasks. Moreover, MOWA also exhibits promising potential to generalize into unseen scenes, as evidenced by cross-domain and zero-shot evaluations. The code and more visual results can be found on the project page: https://kangliao929.github.io/projects/mowa/.","authors":["Kang Liao","Zongsheng Yue","Zhonghua Wu","Chen Change Loy"],"url":"https://arxiv.org/abs/2404.10716"}
{"created":"2025-05-06","title":"Proactive Software Supply Chain Risk Management Framework (P-SSCRM)","abstract":"The Proactive Software Supply Chain Risk Management Framework (P SSCRM) described in this document is designed to help you understand and plan a secure software supply chain risk management initiative. P SSCRM was created through a process of understanding and analyzing real world data from nine industry leading software supply chain risk management initiatives as well as through the analysis and unification of ten government and industry documents, frameworks, and standards. Although individual methodologies and standards differ, many initiatives and standards share common ground. P SSCRM describes this common ground and presents a model for understanding, quantifying, and developing a secure software supply chain risk management program and determining where your organization's existing efforts stand when contrasted with other real world software supply chain risk management initiatives.","authors":["Laurie Williams (North Carolina State University)","Sammy Migues (Imbricate Security)","Jamie Boote (Synopsys)","Ben Hutchison (Synopsys)"],"url":"https://arxiv.org/abs/2404.12300"}
{"created":"2025-05-06","title":"Strengthening Infrastructure Resilience to Hurricanes by Modeling Transportation and Electric Power Network Interdependencies","abstract":"This study presents an agent-based model (ABM) developed to simulate the resilience of a community to hurricane-induced infrastructure disruptions, focusing on the interdependencies between electric power and transportation networks. In this ABM approach, agents represent the components of a system, where interactions within a system shape intra-dependency of a system and interactions among systems shape interdependencies. To study household resilience subject to a hurricane, a library of agents has been created including electric power network, transportation network, wind/flooding hazards, and household agents. The ABM is applied over the household and infrastructure data from a community (Zip code 33147) in Miami-Dade County, Florida. Interdependencies between the two networks are modeled in two ways, (i) representing the role of transportation in fuel delivery to power plants and restoration teams' access, (ii) impact of power outage on transportation network components. Restoring traffic signals quickly is crucial as their outage can slow down traffic and increase the chance of crashes. We simulate three restoration strategies: component based, distance based, and traffic lights based restoration. The model is validated against Hurricane Irma data, showing consistent behavior with varying hazard intensities. Scenario analyses explore the impact of restoration strategies, road accessibility, and wind speed intensities on power restoration. Results demonstrate that a traffic lights based restoration strategy efficiently prioritizes signal recovery without delaying household power restoration time. Restoration of power services will be faster if restoration teams do not need to wait due to inaccessible roads and fuel transportation to power plants is not delayed.","authors":["Tasnuba Binte Jamal","Samiul Hasan","Omar I. Abdul-Aziz","Pallab Mozumder","Rounak Meyur"],"url":"https://arxiv.org/abs/2404.12978"}
{"created":"2025-05-06","title":"Covariant spatio-temporal receptive fields for spiking neural networks","abstract":"Biological nervous systems constitute important sources of inspiration towards computers that are faster, cheaper, and more energy efficient. Neuromorphic disciplines view the brain as a coevolved system, simultaneously optimizing the hardware and the algorithms running on it. There are clear efficiency gains when bringing the computations into a physical substrate, but we presently lack theories to guide efficient implementations. Here, we present a principled computational model for neuromorphic systems in terms of spatio-temporal receptive fields, based on affine Gaussian kernels over space and leaky-integrator and leaky integrate-and-fire models over time. Our theory is provably covariant to spatial affine and temporal scaling transformations, and with close similarities to the visual processing in mammalian brains. We use these spatio-temporal receptive fields as a prior in an event-based vision task, and show that this improves the training of spiking networks, which otherwise is known as problematic for event-based vision. This work combines efforts within scale-space theory and computational neuroscience to identify theoretically well-founded ways to process spatio-temporal signals in neuromorphic systems. Our contributions are immediately relevant for signal processing and event-based vision, and can be extended to other processing tasks over space and time, such as memory and control.","authors":["Jens Egholm Pedersen","J\\\"org Conradt","Tony Lindeberg"],"url":"https://arxiv.org/abs/2405.00318"}
{"created":"2025-05-06","title":"Enhancing person re-identification via Uncertainty Feature Fusion Method and Auto-weighted Measure Combination","abstract":"Person re-identification (Re-ID) is a challenging task that involves identifying the same person across different camera views in surveillance systems. Current methods usually rely on features from single-camera views, which can be limiting when dealing with multiple cameras and challenges such as changing viewpoints and occlusions. In this paper, a new approach is introduced that enhances the capability of ReID models through the Uncertain Feature Fusion Method (UFFM) and Auto-weighted Measure Combination (AMC). UFFM generates multi-view features using features extracted independently from multiple images to mitigate view bias. However, relying only on similarity based on multi-view features is limited because these features ignore the details represented in single-view features. Therefore, we propose the AMC method to generate a more robust similarity measure by combining various measures. Our method significantly improves Rank@1 accuracy and Mean Average Precision (mAP) when evaluated on person re-identification datasets. Combined with the BoT Baseline on challenging datasets, we achieve impressive results, with a 7.9% improvement in Rank@1 and a 12.1% improvement in mAP on the MSMT17 dataset. On the Occluded-DukeMTMC dataset, our method increases Rank@1 by 22.0% and mAP by 18.4%. Code is available: https://github.com/chequanghuy/Enhancing-Person-Re-Identification-via-UFFM-and-AMC","authors":["Quang-Huy Che","Le-Chuong Nguyen","Duc-Tuan Luu","Vinh-Tiep Nguyen"],"url":"https://arxiv.org/abs/2405.01101"}
{"created":"2025-05-06","title":"Updating Windows Malware Detectors: Balancing Robustness and Regression against Adversarial EXEmples","abstract":"Adversarial EXEmples are carefully-perturbed programs tailored to evade machine learning Windows malware detectors, with an ongoing effort to develop robust models able to address detection effectiveness. However, even if robust models can prevent the majority of EXEmples, to maintain predictive power over time, models are fine-tuned to newer threats, leading either to partial updates or time-consuming retraining from scratch. Thus, even if the robustness against adversarial EXEmples is higher, the new models might suffer a regression in performance by misclassifying threats that were previously correctly detected. For these reasons, we study the trade-off between accuracy and regression when updating Windows malware detectors by proposing EXE-scanner, a plugin that can be chained to existing detectors to promptly stop EXEmples without causing regression. We empirically show that previously proposed hardening techniques suffer a regression of accuracy when updating non-robust models, exacerbating the gap when considering low false positives regimes and temporal drifts affecting data. Also, through EXE-scanner we gain evidence on the detectability of adversarial EXEmples, showcasing the presence of artifacts left inside while creating them. Due to its design, EXE-scanner can be chained to any classifier to obtain the best performance without the need for costly retraining. To foster reproducibility, we openly release the source code, along with the dataset of adversarial EXEmples based on state-of-the-art perturbation algorithms.","authors":["Matous Kozak","Luca Demetrio","Dmitrijs Trizna","Fabio Roli"],"url":"https://arxiv.org/abs/2405.02646"}
{"created":"2025-05-06","title":"Fast Online Movement Optimization of Aerial Base Stations Based on Global Connectivity Map","abstract":"Aerial base stations (ABSs) mounted on unmanned aerial vehicles (UAVs) are capable of extending wireless connectivity to ground users (GUs) across a variety of scenarios. However, it is an NP-hard problem with exponential complexity in $M$ and $N$, in order to maximize the coverage rate (CR) of $M$ GUs by jointly placing $N$ ABSs with limited coverage range. The complexity of the problem escalates in environments where the signal propagation is obstructed by localized obstacles such as buildings, and is further compounded by the dynamic GU positions. In response to these challenges, this paper focuses on the optimization of a multi-ABS movement problem, aiming to improve the mean CR for mobile GUs within a site-specific environment. Our proposals include 1) introducing the concept of global connectivity map (GCM) which contains the connectivity information between given pairs of ABS/GU locations; 2) partitioning the ABS movement problem into ABS placement sub-problems and formulate each sub-problem into a binary integer linear programming (BILP) problem based on GCM; 3) and proposing a fast online algorithm to execute (one-pass) projected stochastic subgradient descent within the dual space to rapidly solve the BILP problem with near-optimal performance. Numerical results demonstrate that our proposed method achieves a high CR performance close to the upper bound obtained by the open-source solver (SCIP), yet with significantly reduced running time. Moreover, our method also outperforms common benchmarks in the literature such as the K-means initiated evolutionary algorithm or the ones based on deep reinforcement learning (DRL), in terms of CR performance and/or time efficiency.","authors":["Yiling Wang","Jiangbin Lyu","Liqun Fu"],"url":"https://arxiv.org/abs/2405.02655"}
{"created":"2025-05-06","title":"AFDM Channel Estimation in Multi-Scale Multi-Lag Channels","abstract":"Affine Frequency Division Multiplexing (AFDM) is a brand new chirp-based multi-carrier (MC) waveform for high mobility communications, with promising advantages over Orthogonal Frequency Division Multiplexing (OFDM) and other MC waveforms. Existing AFDM research focuses on wireless communication at high carrier frequency (CF), which typically considers only Doppler frequency shift (DFS) as a result of mobility, while ignoring the accompanied Doppler time scaling (DTS) on waveform. However, for underwater acoustic (UWA) communication at much lower CF and propagating at speed of sound, the DTS effect could not be ignored and poses significant challenges for channel estimation. This paper analyzes the channel frequency response (CFR) of AFDM under multi-scale multi-lag (MSML) channels, where each propagating path could have different delay and DFS/DTS. Based on the newly derived input-output formula and its characteristics, two new channel estimation methods are proposed, i.e., AFDM with iterative multi-index (AFDM-IMI) estimation under low to moderate DTS, and AFDM with orthogonal matching pursuit (AFDM-OMP) estimation under high DTS. Numerical results confirm the effectiveness of the proposed methods against the original AFDM channel estimation method. Moreover, the resulted AFDM system outperforms OFDM as well as Orthogonal Chirp Division Multiplexing (OCDM) in terms of channel estimation accuracy and bit error rate (BER), which is consistent with our theoretical analysis based on CFR overlap probability (COP), mutual incoherent property (MIP) and channel diversity gain under MSML channels.","authors":["Rongyou Cao","Yuheng Zhong","Jiangbin Lyu","Deqing Wang","Liqun Fu"],"url":"https://arxiv.org/abs/2405.02660"}
{"created":"2025-05-06","title":"Outlier Gradient Analysis: Efficiently Identifying Detrimental Training Samples for Deep Learning Models","abstract":"A core data-centric learning challenge is the identification of training samples that are detrimental to model performance. Influence functions serve as a prominent tool for this task and offer a robust framework for assessing training data influence on model predictions. Despite their widespread use, their high computational cost associated with calculating the inverse of the Hessian matrix pose constraints, particularly when analyzing large-sized deep models. In this paper, we establish a bridge between identifying detrimental training samples via influence functions and outlier gradient detection. This transformation not only presents a straightforward and Hessian-free formulation but also provides insights into the role of the gradient in sample impact. Through systematic empirical evaluations, we first validate the hypothesis of our proposed outlier gradient analysis approach on synthetic datasets. We then demonstrate its effectiveness in detecting mislabeled samples in vision models and selecting data samples for improving performance of natural language processing transformer models. We also extend its use to influential sample identification for fine-tuning Large Language Models.","authors":["Anshuman Chhabra","Bo Li","Jian Chen","Prasant Mohapatra","Hongfu Liu"],"url":"https://arxiv.org/abs/2405.03869"}
{"created":"2025-05-06","title":"From Human Judgements to Predictive Models: Unravelling Acceptability in Code-Mixed Sentences","abstract":"Current computational approaches for analysing or generating code-mixed sentences do not explicitly model ``naturalness'' or ``acceptability'' of code-mixed sentences, but rely on training corpora to reflect distribution of acceptable code-mixed sentences. Modelling human judgement for the acceptability of code-mixed text can help in distinguishing natural code-mixed text and enable quality-controlled generation of code-mixed text. To this end, we construct Cline - a dataset containing human acceptability judgements for English-Hindi~(en-hi) code-mixed text. Cline is the largest of its kind with 16,642 sentences, consisting of samples sourced from two sources: synthetically generated code-mixed text and samples collected from online social media. Our analysis establishes that popular code-mixing metrics such as CMI, Number of Switch Points, Burstines, which are used to filter/curate/compare code-mixed corpora have low correlation with human acceptability judgements, underlining the necessity of our dataset. Experiments using Cline demonstrate that simple Multilayer Perceptron (MLP) models when trained solely using code-mixing metrics as features are outperformed by fine-tuned pre-trained Multilingual Large Language Models (MLLMs). Specifically, among Encoder models XLM-Roberta and Bernice outperform IndicBERT across different configurations. Among Encoder-Decoder models, mBART performs better than mT5, however Encoder-Decoder models are not able to outperform Encoder-only models. Decoder-only models perform the best when compared to all other MLLMS, with Llama 3.2 - 3B models outperforming similarly sized Qwen, Phi models. Comparison with zero and fewshot capabilitites of ChatGPT show that MLLMs fine-tuned on larger data outperform ChatGPT, providing scope for improvement in code-mixed tasks. Zero-shot transfer from En-Hi to En-Te acceptability judgments are better than random baselines.","authors":["Prashant Kodali","Anmol Goel","Likhith Asapu","Vamshi Krishna Bonagiri","Anirudh Govil","Monojit Choudhury","Ponnurangam Kumaraguru","Manish Shrivastava"],"url":"https://arxiv.org/abs/2405.05572"}
{"created":"2025-05-06","title":"A Comprehensive Survey on Data Augmentation","abstract":"Data augmentation is a series of techniques that generate high-quality artificial data by manipulating existing data samples. By leveraging data augmentation techniques, AI models can achieve significantly improved applicability in tasks involving scarce or imbalanced datasets, thereby substantially enhancing AI models' generalization capabilities. Existing literature surveys only focus on a certain type of specific modality data, and categorize these methods from modality-specific and operation-centric perspectives, which lacks a consistent summary of data augmentation methods across multiple modalities and limits the comprehension of how existing data samples serve the data augmentation process. To bridge this gap, we propose a more enlightening taxonomy that encompasses data augmentation techniques for different common data modalities. Specifically, from a data-centric perspective, this survey proposes a modality-independent taxonomy by investigating how to take advantage of the intrinsic relationship between data samples, including single-wise, pair-wise, and population-wise sample data augmentation methods. Additionally, we categorize data augmentation methods across five data modalities through a unified inductive approach.","authors":["Zaitian Wang","Pengfei Wang","Kunpeng Liu","Pengyang Wang","Yanjie Fu","Chang-Tien Lu","Charu C. Aggarwal","Jian Pei","Yuanchun Zhou"],"url":"https://arxiv.org/abs/2405.09591"}
{"created":"2025-05-06","title":"NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation","abstract":"Quadrilateral mesh generation plays a crucial role in numerical simulations within Computer-Aided Design and Engineering (CAD/E). Producing high-quality quadrangulation typically requires satisfying four key criteria. First, the quadrilateral mesh should closely align with principal curvature directions. Second, singular points should be strategically placed and effectively minimized. Third, the mesh should accurately conform to sharp feature edges. Lastly, quadrangulation results should exhibit robustness against noise and minor geometric variations. Existing methods generally involve first computing a regular cross field to represent quad element orientations across the surface, followed by extracting a quadrilateral mesh aligned closely with this cross field. A primary challenge with this approach is balancing the smoothness of the cross field with its alignment to pre-computed principal curvature directions, which are sensitive to small surface perturbations and often ill-defined in spherical or planar regions.","authors":["Qiujie Dong","Huibiao Wen","Rui Xu","Shuangmin Chen","Jiaran Zhou","Shiqing Xin","Changhe Tu","Taku Komura","Wenping Wang"],"url":"https://arxiv.org/abs/2405.13745"}
{"created":"2025-05-06","title":"Delta-modular ILP Problems of Bounded Codimension, Discrepancy, and Convolution (new version)","abstract":"For integers $k,n \\geq 0$ and a cost vector $c \\in Z^n$, we study two fundamental integer linear programming (ILP) problems: \\[","authors":["D. Gribanov","D. Malyshev","P. M. Pardalos"],"url":"https://arxiv.org/abs/2405.17001"}
{"created":"2025-05-06","title":"ParallelEdits: Efficient Multi-object Image Editing","abstract":"Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses. In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. Additionally, we introduce the PIE-Bench++ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios.","authors":["Mingzhen Huang","Jialing Cai","Shan Jia","Vishnu Suresh Lokhande","Siwei Lyu"],"url":"https://arxiv.org/abs/2406.00985"}
{"created":"2025-05-06","title":"Automatic Input Feature Relevance via Spectral Neural Networks","abstract":"In machine learning practice it is often useful to identify relevant input features, so as to obtain compact dataset for more efficient numerical handling. On the other hand, by isolating key input elements, ranked according their respective degree of relevance, can help to elaborate on the process of decision making. Here, we propose a novel method to estimate the relative importance of the input components for a Deep Neural Network. This is achieved by leveraging on a spectral re-parametrization of the optimization process. Eigenvalues associated to input nodes provide in fact a robust proxy to gauge the relevance of the supplied entry features. Notably, the spectral features ranking is performed automatically, as a byproduct of the network training, with no additional processing to be carried out. The technique is successfully challenged against both synthetic and real data.","authors":["Lorenzo Chicchi","Lorenzo Buffoni","Diego Febbe","Lorenzo Giambagli","Raffaele Marino","Duccio Fanelli"],"url":"https://arxiv.org/abs/2406.01183"}
{"created":"2025-05-06","title":"Large Language Models as Carriers of Hidden Messages","abstract":"Simple fine-tuning can embed hidden text into large language models (LLMs), which is revealed only when triggered by a specific query. Applications include LLM fingerprinting, where a unique identifier is embedded to verify licensing compliance, and steganography, where the LLM carries hidden messages disclosed through a trigger query.","authors":["Jakub Hoscilowicz","Pawel Popiolek","Jan Rudkowski","Jedrzej Bieniasz","Artur Janicki"],"url":"https://arxiv.org/abs/2406.02481"}
{"created":"2025-05-06","title":"Deep Implicit Optimization enables Robust Learnable Features for Deformable Image Registration","abstract":"Deep Learning in Image Registration (DLIR) methods have been tremendously successful in image registration due to their speed and ability to incorporate weak label supervision at training time. However, existing DLIR methods forego many of the benefits and invariances of optimization methods. The lack of a task-specific inductive bias in DLIR methods leads to suboptimal performance, especially in the presence of domain shift. Our method aims to bridge this gap between statistical learning and optimization by explicitly incorporating optimization as a layer in a deep network. A deep network is trained to predict multi-scale dense feature images that are registered using a black box iterative optimization solver. This optimal warp is then used to minimize image and label alignment errors. By implicitly differentiating end-to-end through an iterative optimization solver, we explicitly exploit invariances of the correspondence matching problem induced by the optimization, while learning registration and label-aware features, and guaranteeing the warp functions to be a local minima of the registration objective in the feature space. Our framework shows excellent performance on in-domain datasets, and is agnostic to domain shift such as anisotropy and varying intensity profiles. For the first time, our method allows switching between arbitrary transformation representations (free-form to diffeomorphic) at test time with zero retraining. End-to-end feature learning also facilitates interpretability of features and arbitrary test-time regularization, which is not possible with existing DLIR methods.","authors":["Rohit Jena","Pratik Chaudhari","James C. Gee"],"url":"https://arxiv.org/abs/2406.07361"}
{"created":"2025-05-06","title":"A Comprehensive Survey on Machine Learning Driven Material Defect Detection","abstract":"Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavors in addressing contemporary challenges associated with MD. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This survey consolidates ML-based MDD literature and provides a foundation for future research and practice.","authors":["Jun Bai","Di Wu","Tristan Shelley","Peter Schubel","David Twine","John Russell","Xuesen Zeng","Ji Zhang"],"url":"https://arxiv.org/abs/2406.07880"}
{"created":"2025-05-06","title":"Griesmer type bounds for additive codes over finite fields, integral and fractional MDS codes","abstract":"In this article we prove Griesmer type bounds for additive codes over finite fields. These new bounds give upper bounds on the length of maximum distance separable (MDS) codes, codes which attain the Singleton bound. We will also consider codes to be MDS if they attain the fractional Singleton bound, due to Huffman. We prove that this bound in the fractional case can be obtained by codes whose length surpasses the length of the longest known codes in the integral case. For small parameters, we provide exhaustive computational results for additive MDS codes, by classifying the corresponding (fractional) subspace-arcs. This includes a complete classification of fractional additive MDS codes of size 243 over the field of order 9.","authors":["Simeon Ball","Michel Lavrauw","Tabriz Popatia"],"url":"https://arxiv.org/abs/2406.08916"}
{"created":"2025-05-06","title":"Beyond the Calibration Point: Mechanism Comparison in Differential Privacy","abstract":"In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\\varepsilon, \\delta)$-pair. This practice overlooks that DP guarantees can vary substantially even between mechanisms sharing a given $(\\varepsilon, \\delta)$, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\\varepsilon, \\delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities.","authors":["Georgios Kaissis","Stefan Kolek","Borja Balle","Jamie Hayes","Daniel Rueckert"],"url":"https://arxiv.org/abs/2406.08918"}
{"created":"2025-05-06","title":"Can't Hide Behind the API: Stealing Black-Box Commercial Embedding Models","abstract":"Embedding models that generate dense vector representations of text are widely used and hold significant commercial value. Companies such as OpenAI and Cohere offer proprietary embedding models via paid APIs, but despite being \"hidden\" behind APIs, these models are not protected from theft. We present, to our knowledge, the first effort to \"steal\" these models for retrieval by training thief models on text-embedding pairs obtained from the APIs. Our experiments demonstrate that it is possible to replicate the retrieval effectiveness of commercial embedding models with a cost of under $300. Notably, our methods allow for distilling from multiple teachers into a single robust student model, and for distilling into presumably smaller models with fewer dimension vectors, yet competitive retrieval effectiveness. Our findings raise important considerations for deploying commercial embedding models and suggest measures to mitigate the risk of model theft.","authors":["Manveer Singh Tamber","Jasper Xian","Jimmy Lin"],"url":"https://arxiv.org/abs/2406.09355"}
{"created":"2025-05-06","title":"tPARAFAC2: Tracking evolving patterns in (incomplete) temporal data","abstract":"Tensor factorizations have been widely used for the task of uncovering patterns in various domains. Often, the input is time-evolving, shifting the goal to tracking the evolution of the underlying patterns instead. To adapt to this more complex setting, existing methods incorporate temporal regularization but they either have overly constrained structural requirements or lack uniqueness which is crucial for interpretation. In this paper, in order to capture the underlying evolving patterns, we introduce t(emporal)PARAFAC2, which utilizes temporal smoothness regularization on the evolving factors. Previously, Alternating Optimization (AO) and Alternating Direction Method of Multipliers (ADMM)-based algorithmic approach has been introduced to fit the PARAFAC2 model to fully observed data. In this paper, we extend this algorithmic framework to the case of partially observed data and use it to fit the tPARAFAC2 model to complete and incomplete datasets with the goal of revealing evolving patterns. Our numerical experiments on simulated datasets demonstrate that tPARAFAC2 can extract the underlying evolving patterns more accurately compared to the state-of-the-art in the presence of high amounts of noise and missing data. Using two real datasets, we also demonstrate the effectiveness of the algorithmic approach in terms of handling missing data and tPARAFAC2 model in terms of revealing evolving patterns. The paper provides an extensive comparison of different approaches for handling missing data within the proposed framework, and discusses both the advantages and limitations of tPARAFAC2 model.","authors":["Christos Chatzis","Carla Schenker","Max Pfeffer","Evrim Acar"],"url":"https://arxiv.org/abs/2407.01356"}
{"created":"2025-05-06","title":"LASSI: An LLM-based Automated Self-Correcting Pipeline for Translating Parallel Scientific Codes","abstract":"This paper addresses the problem of providing a novel approach to sourcing significant training data for LLMs focused on science and engineering. In particular, a crucial challenge is sourcing parallel scientific codes in the ranges of millions to billions of codes. To tackle this problem, we propose an automated pipeline framework called LASSI, designed to translate between parallel programming languages by bootstrapping existing closed- or open-source LLMs. LASSI incorporates autonomous enhancement through self-correcting loops where errors encountered during the compilation and execution of generated code are fed back to the LLM through guided prompting for debugging and refactoring. We highlight the bi-directional translation of existing GPU benchmarks between OpenMP target offload and CUDA to validate LASSI. The results of evaluating LASSI with different application codes across four LLMs demonstrate the effectiveness of LASSI for generating executable parallel codes, with 80% of OpenMP to CUDA translations and 85% of CUDA to OpenMP translations producing the expected output. We also observe approximately 78% of OpenMP to CUDA translations and 62% of CUDA to OpenMP translations execute within 10% of or at a faster runtime than the original benchmark code in the same language.","authors":["Matthew T. Dearing","Yiheng Tao","Xingfu Wu","Zhiling Lan","Valerie Taylor"],"url":"https://arxiv.org/abs/2407.01638"}
{"created":"2025-05-06","title":"MIGC++: Advanced Multi-Instance Generation Controller for Image Synthesis","abstract":"We introduce the Multi-Instance Generation (MIG) task, which focuses on generating multiple instances within a single image, each accurately placed at predefined positions with attributes such as category, color, and shape, strictly following user specifications. MIG faces three main challenges: avoiding attribute leakage between instances, supporting diverse instance descriptions, and maintaining consistency in iterative generation. To address attribute leakage, we propose the Multi-Instance Generation Controller (MIGC). MIGC generates multiple instances through a divide-and-conquer strategy, breaking down multi-instance shading into single-instance tasks with singular attributes, later integrated. To provide more types of instance descriptions, we developed MIGC++. MIGC++ allows attribute control through text \\& images and position control through boxes \\& masks. Lastly, we introduced the Consistent-MIG algorithm to enhance the iterative MIG ability of MIGC and MIGC++. This algorithm ensures consistency in unmodified regions during the addition, deletion, or modification of instances, and preserves the identity of instances when their attributes are changed. We introduce the COCO-MIG and Multimodal-MIG benchmarks to evaluate these methods. Extensive experiments on these benchmarks, along with the COCO-Position benchmark and DrawBench, demonstrate that our methods substantially outperform existing techniques, maintaining precise control over aspects including position, attribute, and quantity. Project page: https://github.com/limuloo/MIGC.","authors":["Dewei Zhou","You Li","Fan Ma","Zongxin Yang","Yi Yang"],"url":"https://arxiv.org/abs/2407.02329"}
{"created":"2025-05-06","title":"A Pattern Language for Machine Learning Tasks","abstract":"We formalise the essential data of objective functions as equality constraints on composites of learners. We call these constraints \"tasks\", and we investigate the idealised view that such tasks determine model behaviours. We develop a flowchart-like graphical mathematics for tasks that allows us to; (1) offer a unified perspective of approaches in machine learning across domains; (2) design and optimise desired behaviours model-agnostically; and (3) import insights from theoretical computer science into practical machine learning. As a proof-of-concept of the potential practical impact of our theoretical framework, we exhibit and implement a novel \"manipulator\" task that minimally edits input data to have a desired attribute. Our model-agnostic approach achieves this end-to-end, and without the need for custom architectures, adversarial training, random sampling, or interventions on the data, hence enabling capable, small-scale, and training-stable models.","authors":["Benjamin Rodatz","Ian Fan","Tuomas Laakkonen","Neil John Ortega","Thomas Hoffmann","Vincent Wang-Mascianica"],"url":"https://arxiv.org/abs/2407.02424"}
{"created":"2025-05-06","title":"CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive Hand-object Segmentation","abstract":"Egocentric Interactive hand-object segmentation (EgoIHOS) requires the segmentation of hands and interacting objects in egocentric images, which is crucial for understanding human behavior in assistive systems. Previous methods typically recognize hands and interacting objects as distinct semantic categories based solely on visual features, or simply use hand predictions as auxiliary cues for object segmentation. Despite the promising progress achieved by these methods, they fail to adequately model the interactive relationships between hands and objects while ignoring the coupled physical relationships among object categories, ultimately constraining their segmentation performance. To make up for the shortcomings of existing methods, we propose a novel method called CaRe-Ego that achieves state-of-the-art performance by emphasizing the contact between hands and objects from two aspects. First, we introduce a Hand-guided Object Feature Enhancer (HOFE) to establish the hand-object interactive relationships to extract more contact-relevant and discriminative object features. Second, we design the Contact-centric Object Decoupling Strategy (CODS) to explicitly model and disentangle coupling relationships among object categories, thereby emphasizing contact-aware feature learning. Experiments on various in-domain and out-of-domain test sets show that Care-Ego significantly outperforms existing methods with robust generalization capability. Codes are publicly available at https://github.com/yuggiehk/CaRe-Ego/.","authors":["Yuejiao Su","Yi Wang","Lap-Pui Chau"],"url":"https://arxiv.org/abs/2407.05576"}
{"created":"2025-05-06","title":"Smoothing of Headland Path Edges and Headland-to-Mainfield Lane Transitions Based on a Spatial Domain Transformation and Linear Programming","abstract":"Within the context of in-field path planning and under the assumption of nonholonomic vehicle models this paper addresses two tasks: smoothing of headland path edges and smoothing of headland-to-mainfield lane transitions. Both tasks are solved by a two-step hierarchical algorithm. The first step differs for the two tasks generating either a piecewise-affine or a Dubins reference path. The second step leverages a transformation of vehicle dynamics from the time domain into the spatial domain and linear programming. Benefits such as a hyperparameter-free objective function and spatial constraints useful for area coverage gaps avoidance and precision path planning are discussed. The method, which is a deterministic optimisation-based method, is evaluated on 5 real-world fields solving 19 instances of the first task and 84 instances of the second task.","authors":["Mogens Plessen"],"url":"https://arxiv.org/abs/2407.05979"}
{"created":"2025-05-06","title":"Revisit the Arimoto-Blahut algorithm: New Analysis with Approximation","abstract":"By the seminal paper of Claude Shannon \\cite{Shannon48}, the computation of the capacity of a discrete memoryless channel has been considered as one of the most important and fundamental problems in Information Theory. Nearly 50 years ago, Arimoto and Blahut independently proposed identical algorithms to solve this problem in their seminal papers \\cite{Arimoto1972AnAF, Blahut1972ComputationOC}. The Arimoto-Blahut algorithm was proven to converge to the capacity of the channel as $t \\to \\infty$ with the convergence rate upper bounded by $O\\left(\\log(m)/t\\right)$, where $m$ is the size of the input distribution, and being inverse exponential when there is a unique solution in the interior of the input probability simplex \\cite{Arimoto1972AnAF}. Recently it was proved, in \\cite{Nakagawa2020AnalysisOT}, that the convergence rate is at worst inverse linear $O(1/t)$ in some specific cases.","authors":["Michail Fasoulakis","Konstantinos Varsos","Apostolos Traganitis"],"url":"https://arxiv.org/abs/2407.06013"}
{"created":"2025-05-06","title":"Tailored Design of Audio-Visual Speech Recognition Models using Branchformers","abstract":"Recent advances in Audio-Visual Speech Recognition (AVSR) have led to unprecedented achievements in the field, improving the robustness of this type of system in adverse, noisy environments. In most cases, this task has been addressed through the design of models composed of two independent encoders, each dedicated to a specific modality. However, while recent works have explored unified audio-visual encoders, determining the optimal cross-modal architecture remains an ongoing challenge. Furthermore, such approaches often rely on models comprising vast amounts of parameters and high computational cost training processes. In this paper, we aim to bridge this research gap by introducing a novel audio-visual framework. Our proposed method constitutes, to the best of our knowledge, the first attempt to harness the flexibility and interpretability offered by encoder architectures, such as the Branchformer, in the design of parameter-efficient AVSR systems. To be more precise, the proposed framework consists of two steps: first, estimating audio- and video-only systems, and then designing a tailored audio-visual unified encoder based on the layer-level branch scores provided by the modality-specific models. Extensive experiments on English and Spanish AVSR benchmarks covering multiple data conditions and scenarios demonstrated the effectiveness of our proposed method. Even when trained on a moderate scale of data, our models achieve competitive word error rates (WER) of approximately 2.5\\% for English and surpass existing approaches for Spanish, establishing a new benchmark with an average WER of around 9.1\\%. These results reflect how our tailored AVSR system is able to reach state-of-the-art recognition rates while significantly reducing the model complexity w.r.t. the prevalent approach in the field. Code and pre-trained models are available at https://github.com/david-gimeno/tailored-avsr.","authors":["David Gimeno-G\\'omez","Carlos-D. Mart\\'inez-Hinarejos"],"url":"https://arxiv.org/abs/2407.06606"}
{"created":"2025-05-06","title":"Topological Offsets","abstract":"We introduce Topological Offsets, a novel approach to generate manifold and self-intersection-free offset surfaces that are topologically equivalent to an offset infinitesimally close to the surface. Our approach, by construction, creates a manifold, watertight, and self-intersection-free offset surface strictly enclosing the input, while doing a best effort to move it to a prescribed distance from the input. Differently from existing approaches, we embed the input in a background mesh and insert a topological offset around the input with purely combinatorial operations. The topological offset is then inflated/deflated to match the user-prescribed distance while enforcing that no intersections or non-manifold configurations are introduced. We evaluate the effectiveness and robustness of our approach on the Thingi10k dataset, and show that topological offsets are beneficial in multiple graphics applications, including (1) converting non-manifold surfaces to manifold ones, (2) creating layered offsets, and (3) reliably computing finite offsets.","authors":["Daniel Zint","Zhouyuan Chen","Yifei Zhu","Denis Zorin","Teseo Schneider","Daniele Panozzo"],"url":"https://arxiv.org/abs/2407.07725"}
{"created":"2025-05-06","title":"Model-agnostic clean-label backdoor mitigation in cybersecurity environments","abstract":"The training phase of machine learning models is a delicate step, especially in cybersecurity contexts. Recent research has surfaced a series of insidious training-time attacks that inject backdoors in models designed for security classification tasks without altering the training labels. With this work, we propose new techniques that leverage insights in cybersecurity threat models to effectively mitigate these clean-label poisoning attacks, while preserving the model utility. By performing density-based clustering on a carefully chosen feature subspace, and progressively isolating the suspicious clusters through a novel iterative scoring procedure, our defensive mechanism can mitigate the attacks without requiring many of the common assumptions in the existing backdoor defense literature. To show the generality of our proposed mitigation, we evaluate it on two clean-label model-agnostic attacks on two different classic cybersecurity data modalities: network flows classification and malware classification, using gradient boosting and neural network models.","authors":["Giorgio Severi","Simona Boboila","John Holodnak","Kendra Kratkiewicz","Rauf Izmailov","Michael J. De Lucia","Alina Oprea"],"url":"https://arxiv.org/abs/2407.08159"}
{"created":"2025-05-06","title":"Thunderbolt: Concurrent Smart Contract Execution with Nonblocking Reconfiguration for Sharded DAGs","abstract":"Sharding has emerged as a critical technique for enhancing blockchain system scalability. However, existing sharding approaches face unique challenges when applied to Directed Acyclic Graph (DAG)-based protocols that integrate expressive smart contract processing. Current solutions predominantly rely on coordination mechanisms like 2PC and require transaction read/write sets to optimize parallel execution. These requirements introduce two fundamental limitations: 1) additional coordination phases incur latency overhead, and 2) pre-declaration of read/write sets proves impractical for Turing-complete smart contracts with dynamic access patterns.","authors":["Junchao Chen","Alberto Sonnino","Lefteris Kokoris-Kogias","Mohammad Sadoghi"],"url":"https://arxiv.org/abs/2407.09409"}
{"created":"2025-05-06","title":"LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models","abstract":"The advances of large foundation models necessitate wide-coverage, low-cost, and zero-contamination benchmarks. Despite continuous exploration of language model evaluations, comprehensive studies on the evaluation of Large Multi-modal Models (LMMs) remain limited. In this work, we introduce LMMS-EVAL, a unified and standardized multimodal benchmark framework with over 50 tasks and more than 10 models to promote transparent and reproducible evaluations. Although LMMS-EVAL offers comprehensive coverage, we find it still falls short in achieving low cost and zero contamination. To approach this evaluation trilemma, we further introduce LMMS-EVAL LITE, a pruned evaluation toolkit that emphasizes both coverage and efficiency. Additionally, we present Multimodal LIVEBENCH that utilizes continuously updating news and online forums to assess models' generalization abilities in the wild, featuring a low-cost and zero-contamination evaluation approach. In summary, our work highlights the importance of considering the evaluation trilemma and provides practical solutions to navigate the trade-offs in evaluating large multi-modal models, paving the way for more effective and reliable benchmarking of LMMs. We opensource our codebase and maintain leaderboard of LIVEBENCH at https://github.com/EvolvingLMMs-Lab/lmms-eval and https://huggingface.co/spaces/lmms-lab/LiveBench.","authors":["Kaichen Zhang","Bo Li","Peiyuan Zhang","Fanyi Pu","Joshua Adrian Cahyono","Kairui Hu","Shuai Liu","Yuanhan Zhang","Jingkang Yang","Chunyuan Li","Ziwei Liu"],"url":"https://arxiv.org/abs/2407.12772"}
{"created":"2025-05-06","title":"Parallel Split Learning with Global Sampling","abstract":"Distributed deep learning in resource-constrained environments faces scalability and generalization challenges due to large effective batch sizes and non-identically distributed client data. We introduce a server-driven sampling strategy that maintains a fixed global batch size by dynamically adjusting client-side batch sizes. This decouples the effective batch size from the number of participating devices and ensures that global batches better reflect the overall data distribution. Using standard concentration bounds, we establish tighter deviation guarantees compared to existing approaches. Empirical results on a benchmark dataset confirm that the proposed method improves model accuracy, training efficiency, and convergence stability, offering a scalable solution for learning at the network edge.","authors":["Mohammad Kohankhaki","Ahmad Ayad","Mahdi Barhoush","Anke Schmeink"],"url":"https://arxiv.org/abs/2407.15738"}
{"created":"2025-05-06","title":"Commitment Attacks on Ethereum's Reward Mechanism","abstract":"Validators in permissionless, large-scale blockchains, such as Ethereum, are typically payoff-maximizing, rational actors. Ethereum relies on in-protocol incentives, like rewards for correct and timely votes, to induce honest behavior and secure the blockchain. However, external incentives, such as the block proposer's opportunity to capture maximal extractable value (MEV), may tempt validators to deviate from honest protocol participation.","authors":["Roozbeh Sarenche","Ertem Nusret Tas","Barnabe Monnot","Caspar Schwarz-Schilling","Bart Preneel"],"url":"https://arxiv.org/abs/2407.19479"}
{"created":"2025-05-06","title":"AI-Driven Healthcare: A Review on Ensuring Fairness and Mitigating Bias","abstract":"Artificial intelligence (AI) is rapidly advancing in healthcare, enhancing the efficiency and effectiveness of services across various specialties, including cardiology, ophthalmology, dermatology, emergency medicine, etc. AI applications have significantly improved diagnostic accuracy, treatment personalization, and patient outcome predictions by leveraging technologies such as machine learning, neural networks, and natural language processing. However, these advancements also introduce substantial ethical and fairness challenges, particularly related to biases in data and algorithms. These biases can lead to disparities in healthcare delivery, affecting diagnostic accuracy and treatment outcomes across different demographic groups. This review paper examines the integration of AI in healthcare, highlighting critical challenges related to bias and exploring strategies for mitigation. We emphasize the necessity of diverse datasets, fairness-aware algorithms, and regulatory frameworks to ensure equitable healthcare delivery. The paper concludes with recommendations for future research, advocating for interdisciplinary approaches, transparency in AI decision-making, and the development of innovative and inclusive AI applications.","authors":["Sribala Vidyadhari Chinta","Zichong Wang","Avash Palikhe","Xingyu Zhang","Ayesha Kashif","Monique Antoinette Smith","Jun Liu","Wenbin Zhang"],"url":"https://arxiv.org/abs/2407.19655"}
{"created":"2025-05-06","title":"FIVB ranking: Misstep in the right direction","abstract":"This work presents and evaluates the ranking algorithm that has been used by Federation Internationale de Volleyball (FIVB) since 2020. The prominent feature of the FIVB ranking is the use of the probabilistic model, which explicitly calculates the probabilities of the future matches results using the estimated teams' strengths. Such explicit modeling is new in the context of official sport rankings, especially for multi-level outcomes, and we study the optimality of its parameters using both analytical and numerical methods. We conclude that from the modeling perspective, the current thresholds fit well the data but adding the home-field advantage (HFA) would be beneficial. Regarding the algorithm itself, we explain the rationale behind the approximations currently used and show a simple method to find new parameters (numerical score) which improve the performance. We also show that the weighting of the match results is counterproductive.","authors":["Salma Tenni","Daniel Gomes de Pinho Zanco","Leszek Szczecinski"],"url":"https://arxiv.org/abs/2408.01603"}
{"created":"2025-05-06","title":"A Logical Fallacy-Informed Framework for Argument Generation","abstract":"Despite the remarkable performance of Large Language Models (LLMs) in natural language processing tasks, they still struggle with generating logically sound arguments, resulting in potential risks such as spreading misinformation. To address this issue, we introduce FIPO, a fallacy-informed framework that leverages preference optimization methods to steer LLMs toward logically sound arguments. FIPO includes a classification loss, to capture the fine-grained information on fallacy types. Our results on argumentation datasets show that our method reduces the fallacy errors by up to 17.5%. Furthermore, our human evaluation results indicate that the quality of the generated arguments by our method significantly outperforms the fine-tuned baselines, as well as other preference optimization methods, such as DPO. These findings highlight the importance of ensuring models are aware of logical fallacies for effective argument generation. Our code is available at github.com/lucamouchel/Logical-Fallacies.","authors":["Luca Mouchel","Debjit Paul","Shaobo Cui","Robert West","Antoine Bosselut","Boi Faltings"],"url":"https://arxiv.org/abs/2408.03618"}
{"created":"2025-05-06","title":"How Does Audio Influence Visual Attention in Omnidirectional Videos? Database and Model","abstract":"Understanding and predicting viewer attention in omnidirectional videos (ODVs) is crucial for enhancing user engagement in virtual and augmented reality applications. Although both audio and visual modalities are essential for saliency prediction in ODVs, the joint exploitation of these two modalities has been limited, primarily due to the absence of large-scale audio-visual saliency databases and comprehensive analyses. This paper comprehensively investigates audio-visual attention in ODVs from both subjective and objective perspectives. Specifically, we first introduce a new audio-visual saliency database for omnidirectional videos, termed AVS-ODV database, containing 162 ODVs and corresponding eye movement data collected from 60 subjects under three audio modes including mute, mono, and ambisonics. Based on the constructed AVS-ODV database, we perform an in-depth analysis of how audio influences visual attention in ODVs. To advance the research on audio-visual saliency prediction for ODVs, we further establish a new benchmark based on the AVS-ODV database by testing numerous state-of-the-art saliency models, including visual-only models and audio-visual models. In addition, given the limitations of current models, we propose an innovative omnidirectional audio-visual saliency prediction network (OmniAVS), which is built based on the U-Net architecture, and hierarchically fuses audio and visual features from the multimodal aligned embedding space. Extensive experimental results demonstrate that the proposed OmniAVS model outperforms other state-of-the-art models on both ODV AVS prediction and traditional AVS predcition tasks. The AVS-ODV database and OmniAVS model will be released to facilitate future research.","authors":["Yuxin Zhu","Huiyu Duan","Kaiwei Zhang","Yucheng Zhu","Xilei Zhu","Long Teng","Xiongkuo Min","Guangtao Zhai"],"url":"https://arxiv.org/abs/2408.05411"}
{"created":"2025-05-06","title":"LipidBERT: A Lipid Language Model Pre-trained on METiS de novo Lipid Library","abstract":"In this study, we generate and maintain a database of 10 million virtual lipids through METiS's in-house de novo lipid generation algorithms and lipid virtual screening techniques. These virtual lipids serve as a corpus for pre-training, lipid representation learning, and downstream task knowledge transfer, culminating in state-of-the-art LNP property prediction performance. We propose LipidBERT, a BERT-like model pre-trained with the Masked Language Model (MLM) and various secondary tasks. Additionally, we compare the performance of embeddings generated by LipidBERT and PhatGPT, our GPT-like lipid generation model, on downstream tasks. The proposed bilingual LipidBERT model operates in two languages: the language of ionizable lipid pre-training, using in-house dry-lab lipid structures, and the language of LNP fine-tuning, utilizing in-house LNP wet-lab data. This dual capability positions LipidBERT as a key AI-based filter for future screening tasks, including new versions of METiS de novo lipid libraries and, more importantly, candidates for in vivo testing for orgran-targeting LNPs. To the best of our knowledge, this is the first successful demonstration of the capability of a pre-trained language model on virtual lipids and its effectiveness in downstream tasks using web-lab data. This work showcases the clever utilization of METiS's in-house de novo lipid library as well as the power of dry-wet lab integration.","authors":["Tianhao Yu","Cai Yao","Zhuorui Sun","Feng Shi","Lin Zhang","Kangjie Lyu","Xuan Bai","Andong Liu","Xicheng Zhang","Jiali Zou","Wenshou Wang","Chris Lai","Kai Wang"],"url":"https://arxiv.org/abs/2408.06150"}
{"created":"2025-05-06","title":"An improved Shifted CholeskyQR based on columns","abstract":"Among all the deterministic CholeskyQR-type algorithms, Shifted CholeskyQR3 is specifically designed to address the QR factorization of ill-conditioned matrices. This algorithm introduces a shift parameter $s$ to prevent failure during the initial Cholesky factorization step, making the choice of this parameter critical for the algorithm's effectiveness. Our goal is to identify a smaller $s$ compared to the traditional selection based on $\\norm{X}_{2}$. In this research, we propose a new definition for the input matrix $X$ called $[X]_{g}$, which is based on the column properties of $X$. $[X]_{g}$ allows us to obtain a reduced shift parameter $s$ for the Shifted CholeskyQR3 algorithm, thereby improving the sufficient condition of $\\kappa_{2}(X)$ for this method. We provide rigorous proofs of orthogonality and residuals for the improved algorithm using our proposed $s$. Numerical experiments confirm the enhanced numerical stability of orthogonality and residuals with the reduced $s$. We find that Shifted CholeskyQR3 can effectively handle ill-conditioned $X$ with a larger $\\kappa_{2}(X)$ when using our reduced $s$ compared to the original $s$. Furthermore, we compare CPU times with other algorithms to assess performance improvements.","authors":["Yuwei Fan","Haoran Guan","Zhonghua Qiao"],"url":"https://arxiv.org/abs/2408.06311"}
{"created":"2025-05-06","title":"Blessing of Dimensionality for Approximating Sobolev Classes on Manifolds","abstract":"The manifold hypothesis says that natural high-dimensional data lie on or around a low-dimensional manifold. The recent success of statistical and learning-based methods in very high dimensions empirically supports this hypothesis, suggesting that typical worst-case analysis does not provide practical guarantees. A natural step for analysis is thus to assume the manifold hypothesis and derive bounds that are independent of any ambient dimensions that the data may be embedded in. Theoretical implications in this direction have recently been explored in terms of generalization of ReLU networks and convergence of Langevin methods. In this work, we consider optimal uniform approximations with functions of finite statistical complexity. While upper bounds on uniform approximation exist in the literature using ReLU neural networks, we consider the opposite: lower bounds to quantify the fundamental difficulty of approximation on manifolds. In particular, we demonstrate that the statistical complexity required to approximate a class of bounded Sobolev functions on a compact manifold is bounded from below, and moreover that this bound is dependent only on the intrinsic properties of the manifold, such as curvature, volume, and injectivity radius.","authors":["Hong Ye Tan","Subhadip Mukherjee","Junqi Tang","Carola-Bibiane Sch\\\"onlieb"],"url":"https://arxiv.org/abs/2408.06996"}
{"created":"2025-05-06","title":"Enhanced BPINN Training Convergence in Solving General and Multi-scale Elliptic PDEs with Noise","abstract":"Bayesian Physics Informed Neural Networks (BPINN) have attracted considerable attention for inferring the system states and physical parameters of differential equations according to noisy observations. However, in practice, Hamiltonian Monte Carlo (HMC) used to estimate the internal parameters of the solver for BPINN often encounters these troubles including poor performance and awful convergence for a given step size used to adjust the momentum of those parameters. To address the convergence of HMC for the BPINN method and extend its application scope to multi-scale partial differential equations (PDE), we develop a robust multi-scale BPINN (dubbed MBPINN) method by integrating multi-scale deep neural networks (MscaleDNN) and the BPINN framework. In this newly proposed MBPINN method, we reframe HMC with Stochastic Gradient Descent (SGD) to ensure the most ``likely'' estimation is always provided, and we configure its solver as a Fourier feature mapping-induced MscaleDNN. This novel method offers several key advantages: (1) it is more robust than HMC, (2) it incurs less computational cost than HMC, and (3) it is more flexible for complex problems. We demonstrate the applicability and performance of the proposed method through some general Poisson and multi-scale elliptic problems in one and two-dimensional Euclidean spaces. Our findings indicate that the proposed method can avoid HMC failures and provide valid results. Additionally, our method is capable of handling complex elliptic PDE and producing comparable results for general elliptic PDE under the case of lower signal-to-noise rate. These findings suggest that our proposed approach has great potential for physics-informed machine learning for parameter estimation and solution recovery in the case of ill-posed problems.","authors":["Yilong Hou","Xi'an Li","Jinran Wu","You-Gan Wang"],"url":"https://arxiv.org/abs/2408.09340"}
{"created":"2025-05-06","title":"FEDKIM: Adaptive Federated Knowledge Injection into Medical Foundation Models","abstract":"Foundation models have demonstrated remarkable capabilities in handling diverse modalities and tasks, outperforming conventional artificial intelligence (AI) approaches that are highly task-specific and modality-reliant. In the medical domain, however, the development of comprehensive foundation models is constrained by limited access to diverse modalities and stringent privacy regulations. To address these constraints, this study introduces a novel knowledge injection approach, FedKIM, designed to scale the medical foundation model within a federated learning framework. FedKIM leverages lightweight local models to extract healthcare knowledge from private data and integrates this knowledge into a centralized foundation model using a designed adaptive Multitask Multimodal Mixture Of Experts (M3OE) module. This method not only preserves privacy but also enhances the model's ability to handle complex medical tasks involving multiple modalities. Our extensive experiments across twelve tasks in seven modalities demonstrate the effectiveness of FedKIM in various settings, highlighting its potential to scale medical foundation models without direct access to sensitive data.","authors":["Xiaochen Wang","Jiaqi Wang","Houping Xiao","Jinghui Chen","Fenglong Ma"],"url":"https://arxiv.org/abs/2408.10276"}
{"created":"2025-05-06","title":"Kubrick: Multimodal Agent Collaborations for Synthetic Video Generation","abstract":"Text-to-video generation has been dominated by diffusion-based or autoregressive models. These novel models provide plausible versatility, but are criticized for improper physical motion, shading and illumination, camera motion, and temporal consistency. The film industry relies on manually-edited Computer-Generated Imagery (CGI) using 3D modeling software. Human-directed 3D synthetic videos address these shortcomings, but require tight collaboration between movie makers and 3D rendering experts. We introduce an automatic synthetic video generation pipeline based on Vision Large Language Model (VLM) agent collaborations. Given a language description of a video, multiple VLM agents direct various processes of the generation pipeline. They cooperate to create Blender scripts which render a video following the given description. Augmented with Blender-based movie making knowledge, the Director agent decomposes the text-based video description into sub-processes. For each sub-process, the Programmer agent produces Python-based Blender scripts based on function composing and API calling. The Reviewer agent, with knowledge of video reviewing, character motion coordinates, and intermediate screenshots, provides feedback to the Programmer agent. The Programmer agent iteratively improves scripts to yield the best video outcome. Our generated videos show better quality than commercial video generation models in five metrics on video quality and instruction-following performance. Our framework outperforms other approaches in a user study on quality, consistency, and rationality.","authors":["Liu He","Yizhi Song","Hejun Huang","Pinxin Liu","Yunlong Tang","Daniel Aliaga","Xin Zhou"],"url":"https://arxiv.org/abs/2408.10453"}
{"created":"2025-05-06","title":"Leveraging Unlabeled Data Sharing through Kernel Function Approximation in Offline Reinforcement Learning","abstract":"Offline reinforcement learning (RL) learns policies from a fixed dataset, but often requires large amounts of data. The challenge arises when labeled datasets are expensive, especially when rewards have to be provided by human labelers for large datasets. In contrast, unlabelled data tends to be less expensive. This situation highlights the importance of finding effective ways to use unlabelled data in offline RL, especially when labelled data is limited or expensive to obtain. In this paper, we present the algorithm to utilize the unlabeled data in the offline RL method with kernel function approximation and give the theoretical guarantee. We present various eigenvalue decay conditions of $\\mathcal{H}_k$ which determine the complexity of the algorithm. In summary, our work provides a promising approach for exploiting the advantages offered by unlabeled data in offline RL, whilst maintaining theoretical assurances.","authors":["Yen-Ru Lai","Fu-Chieh Chang","Pei-Yuan Wu"],"url":"https://arxiv.org/abs/2408.12307"}
{"created":"2025-05-06","title":"Face Clustering via Early Stopping and Edge Recall","abstract":"Large-scale face clustering has achieved significant progress, with many efforts dedicated to learning to cluster large-scale faces with supervised-learning. However, complex model design and tedious clustering processes are typical in existing methods. Such limitations result in infeasible clustering in real-world applications. Reasonable and efficient model design and training need to be taken into account. Besides, developing unsupervised face clustering algorithms is crucial, which are more realistic in real-world applications. In this paper, we propose a novel unsupervised face clustering algorithm FC-ES and a novel supervised face clustering algorithm FC-ESER to address these issues. An efficient and effective neighbor-based edge probability and a novel early stopping strategy are proposed in FC-ES, guaranteeing the accuracy and recall of large-scale face clustering simultaneously. Furthermore, to take advantage of supervised learning, a novel edge recall strategy is proposed in FC-ESER to further recall the edge connections that are not connected in FC-ES. Extensive experiments on multiple benchmarks for face, person, and vehicle clustering show that our proposed FC-ES and FC-ESER significantly outperform previous state-of-the-art methods. Our code will be available at https://github.com/jumptoliujj/FC-ESER.","authors":["Junjie Liu"],"url":"https://arxiv.org/abs/2408.13431"}
{"created":"2025-05-06","title":"Easy-access online social media metrics can foster the identification of misinformation sharing users","abstract":"Misinformation poses a significant challenge studied extensively by researchers, yet acquiring data to identify primary sharers is time-consuming and challenging. To address this, we propose a low-barrier approach to differentiate social media users who are more likely to share misinformation from those who are less likely. Leveraging insights from previous studies, we demonstrate that easy-access online social network metrics -- average daily tweet count, and account age -- can be leveraged to help identify potential low factuality content spreaders on X (previously known as Twitter). We find that higher tweet frequency is positively associated with low factuality in shared content, while account age is negatively associated with it. We also find that some of the effects, namely the effect of the number of accounts followed and the number of tweets produced, differ depending on the number of followers a user has. Our findings show that relying on these easy-access social network metrics could serve as a low-barrier approach for initial identification of users who are more likely to spread misinformation, and therefore contribute to combating misinformation effectively on social media platforms.","authors":["J\\'ulia Sz\\'amely","Alessandro Galeazzi","J\\'ulia Koltai","Elisa Omodei"],"url":"https://arxiv.org/abs/2408.15186"}
{"created":"2025-05-06","title":"FissionVAE: Federated Non-IID Image Generation with Latent Space and Decoder Decomposition","abstract":"Federated learning is a machine learning paradigm that enables decentralized clients to collaboratively learn a shared model while keeping all the training data local. While considerable research has focused on federated image generation, particularly Generative Adversarial Networks, Variational Autoencoders have received less attention. In this paper, we address the challenges of non-IID (independently and identically distributed) data environments featuring multiple groups of images of different types. Non-IID data distributions can lead to difficulties in maintaining a consistent latent space and can also result in local generators with disparate texture features being blended during aggregation. We thereby introduce FissionVAE that decouples the latent space and constructs decoder branches tailored to individual client groups. This method allows for customized learning that aligns with the unique data distributions of each group. Additionally, we incorporate hierarchical VAEs and demonstrate the use of heterogeneous decoder architectures within FissionVAE. We also explore strategies for setting the latent prior distributions to enhance the decoupling process. To evaluate our approach, we assemble two composite datasets: the first combines MNIST and FashionMNIST; the second comprises RGB datasets of cartoon and human faces, wild animals, marine vessels, and remote sensing images. Our experiments demonstrate that FissionVAE greatly improves generation quality on these datasets compared to baseline federated VAE models.","authors":["Chen Hu","Hanchi Ren","Jingjing Deng","Xianghua Xie","Xiaoke Ma"],"url":"https://arxiv.org/abs/2408.17090"}
{"created":"2025-05-06","title":"Tele-LLMs: A Series of Specialized Large Language Models for Telecommunications","abstract":"The emergence of large language models (LLMs) has significantly impacted various fields, from natural language processing to sectors like medicine and finance. However, despite their rapid proliferation, the applications of LLMs in telecommunications remain limited, often relying on general-purpose models that lack domain-specific specialization. This lack of specialization results in underperformance, particularly when dealing with telecommunications-specific technical terminology and their associated mathematical representations. This paper addresses this gap by first creating and disseminating Tele-Data, a comprehensive dataset of telecommunications material curated from relevant sources, and Tele-Eval, a large-scale question-and-answer dataset tailored to the domain. Through extensive experiments, we explore the most effective training techniques for adapting LLMs to the telecommunications domain, ranging from examining the division of expertise across various telecommunications aspects to employing parameter-efficient techniques. We also investigate how models of different sizes behave during adaptation and analyze the impact of their training data on this behavior. Leveraging these findings, we develop and open-source Tele-LLMs, the first series of language models ranging from 1B to 8B parameters, specifically tailored for telecommunications. Our evaluations demonstrate that these models outperform their general-purpose counterparts on Tele-Eval and telecommunications-related literature tasks while retaining their previously acquired capabilities, thus avoiding the catastrophic forgetting phenomenon.","authors":["Ali Maatouk","Kenny Chirino Ampudia","Rex Ying","Leandros Tassiulas"],"url":"https://arxiv.org/abs/2409.05314"}
{"created":"2025-05-06","title":"M3-Jepa: Multimodal Alignment via Multi-directional MoE based on the JEPA framework","abstract":"Current multimodal alignment strategies primarily use single or unified modality encoders, while optimizing the alignment on the original token space. Such a framework is easy to implement and incorporate with the pretrained knowledge, but might result in information bias. To deal with such issues, the joint encoding predictive architecture (JEPA) learns the alignment loss on the latent space, with a predictor to convert the input encoding to the output latent space. However, the application of JEPA in multimodal scenarios is limited so far. In this paper, we introduce M3-Jepa, a scalable multimodal alignment framework, with the predictor implemented by a multi-directional mixture of experts (MoE). We demonstrate the framework can maximize the mutual information with information theory derivations, by alternating the optimization between different uni-directional tasks. By thoroughly designed experiments, we show that M3-Jepa can obtain state-of-the-art performance on different modalities and tasks, generalize to unseen datasets and domains, and is computationally efficient in training and inference. Our study indicates that M3-Jepa might provide a new paradigm to self-supervised learning and open-world modeling.","authors":["Hongyang Lei","Xiaolong Cheng","Dan Wang","Kun Fan","Qi Qin","Huazhen Huang","Yetao Wu","Qingqing Gu","Zhonglin Jiang","Yong Chen","Luo Ji"],"url":"https://arxiv.org/abs/2409.05929"}
{"created":"2025-05-06","title":"CoDiCast: Conditional Diffusion Model for Global Weather Prediction with Uncertainty Quantification","abstract":"Accurate weather forecasting is critical for science and society. Yet, existing methods have not managed to simultaneously have the properties of high accuracy, low uncertainty, and high computational efficiency. On one hand, to quantify the uncertainty in weather predictions, the strategy of ensemble forecast (i.e., generating a set of diverse predictions) is often employed. However, traditional ensemble numerical weather prediction (NWP) is computationally intensive. On the other hand, most existing machine learning-based weather prediction (MLWP) approaches are efficient and accurate. Nevertheless, they are deterministic and cannot capture the uncertainty of weather forecasting. In this work, we propose CoDiCast, a conditional diffusion model to generate accurate global weather prediction, while achieving uncertainty quantification with ensemble forecasts and modest computational cost. The key idea is to simulate a conditional version of the reverse denoising process in diffusion models, which starts from pure Gaussian noise to generate realistic weather scenarios for a future time point. Each denoising step is conditioned on observations from the recent past. Ensemble forecasts are achieved by repeatedly sampling from stochastic Gaussian noise to represent uncertainty quantification. CoDiCast is trained on a decade of ERA5 reanalysis data from the European Centre for Medium-Range Weather Forecasts (ECMWF). Experimental results demonstrate that our approach outperforms several existing data-driven methods in accuracy. Our conditional diffusion model, CoDiCast, can generate 6-day global weather forecasts, at 6-hour steps and $5.625^\\circ$ latitude-longitude resolution, for over 5 variables, in about 12 minutes on a commodity A100 GPU machine with 80GB memory. The open-souced code is provided at https://github.com/JimengShi/CoDiCast.","authors":["Jimeng Shi","Bowen Jin","Jiawei Han","Sundararaman Gopalakrishnan","Giri Narasimhan"],"url":"https://arxiv.org/abs/2409.05975"}
{"created":"2025-05-06","title":"CFCPalsy: Facial Image Synthesis with Cross-Fusion Cycle Diffusion Model for Facial Paralysis Individuals","abstract":"Currently, the diagnosis of facial paralysis remains a challenging task, often relying heavily on the subjective judgment and experience of clinicians, which can introduce variability and uncertainty in the assessment process. One promising application in real-life situations is the automatic estimation of facial paralysis. However, the scarcity of facial paralysis datasets limits the development of robust machine learning models for automated diagnosis and therapeutic interventions. To this end, this study aims to synthesize a high-quality facial paralysis dataset to address this gap, enabling more accurate and efficient algorithm training. Specifically, a novel Cross-Fusion Cycle Palsy Expression Generative Model (CFCPalsy) based on the diffusion model is proposed to combine different features of facial information and enhance the visual details of facial appearance and texture in facial regions, thus creating synthetic facial images that accurately represent various degrees and types of facial paralysis. We have qualitatively and quantitatively evaluated the proposed method on the commonly used public clinical datasets of facial paralysis to demonstrate its effectiveness. Experimental results indicate that the proposed method surpasses state-of-the-art methods, generating more realistic facial images and maintaining identity consistency.","authors":["Weixiang Gao","Yating Zhang","Yifan Xia"],"url":"https://arxiv.org/abs/2409.07271"}
{"created":"2025-05-06","title":"Maximum And- vs. Even-SAT","abstract":"A (multi)set of literals, called a clause, is strongly satisfied by an assignment if no literal evaluates to false. Finding an assignment that maximises the number of strongly satisfied clauses is NP-hard. We present a simple algorithm that finds, given a set of clauses that admits an assignment that strongly satisfies a $\\rho$-fraction of the clauses, an assignment in which at least a $\\rho$-fraction of the clauses is weakly satisfied, in the sense that an even number of literals evaluates to false. In particular, this implies an efficient algorithm for finding an undirected cut of value $\\rho$ in a graph $G$ given that a directed cut of value $\\rho$ in $G$ is promised to exist. A similar argument also gives an efficient algorithm for finding an acyclic subgraph of $G$ with $\\rho$ edges under the same promise.","authors":["Tamio-Vesa Nakajima","Stanislav \\v{Z}ivn\\'y"],"url":"https://arxiv.org/abs/2409.07837"}
{"created":"2025-05-06","title":"Applications of multiscale hierarchical decomposition to blind deconvolution","abstract":"The blind image deconvolution is a challenging, highly ill-posed nonlinear inverse problem. We introduce a Multiscale Hierarchical Decomposition Method (MHDM) that is iteratively solving variational problems with adaptive data and regularization parameters, towards obtaining finer and finer details of the unknown kernel and image. We establish convergence of the residual in the noise-free data case, and then in the noisy data case when the algorithm is stopped early by means of a discrepancy principle. Fractional Sobolev norms are employed as regularizers for both kernel and image, with the advantage of computing the minimizers explicitly in a pointwise manner. In order to break the notorious symmetry occurring during each minimization step, we enforce a positivity constraint on the Fourier transform of the kernels. Numerical comparisons with a single-step variational method and a non-blind MHDM show that our approach produces comparable results, while less laborious parameter tuning is necessary at the price of more computations. Additionally, the scale decomposition of both reconstructed kernel and image provides a meaningful interpretation of the involved iteration steps.","authors":["Tobias Wolf","Stefan Kindermann","Elena Resmerita","Luminita Vese"],"url":"https://arxiv.org/abs/2409.08734"}
{"created":"2025-05-06","title":"Audio-text Retrieval with Transformer-based Hierarchical Alignment and Disentangled Cross-modal Representation","abstract":"Most existing audio-text retrieval (ATR) approaches typically rely on a single-level interaction to associate audio and text, limiting their ability to align different modalities and leading to suboptimal matches. In this work, we present a novel ATR framework that leverages two-stream Transformers in conjunction with a Hierarchical Alignment (THA) module to identify multi-level correspondences of different Transformer blocks between audio and text. Moreover, current ATR methods mainly focus on learning a global-level representation, missing out on intricate details to capture audio occurrences that correspond to textual semantics. To bridge this gap, we introduce a Disentangled Cross-modal Representation (DCR) approach that disentangles high-dimensional features into compact latent factors to grasp fine-grained audio-text semantic correlations. Additionally, we develop a confidence-aware (CA) module to estimate the confidence of each latent factor pair and adaptively aggregate cross-modal latent factors to achieve local semantic alignment. Experiments show that our THA effectively boosts ATR performance, with the DCR approach further contributing to consistent performance gains.","authors":["Yifei Xin","Zhihong Zhu","Xuxin Cheng","Xusheng Yang","Yuexian Zou"],"url":"https://arxiv.org/abs/2409.09256"}
{"created":"2025-05-06","title":"Constructive Approach to Bidirectional Influence between Qualia Structure and Language Emergence","abstract":"This perspective paper explores the bidirectional influence between language emergence and the relational structure of subjective experiences, termed qualia structure, and lays out a constructive approach to the intricate dependency between the two. We hypothesize that the emergence of languages with distributional semantics (e.g., syntactic-semantic structures) is linked to the coordination of internal representations shaped by experience, potentially facilitating more structured language through reciprocal influence. This hypothesized mutual dependency connects to recent advancements in AI and symbol emergence robotics, and is explored within this paper through theoretical frameworks such as the collective predictive coding. Computational studies show that neural network-based language models form systematically structured internal representations, and multimodal language models can share representations between language and perceptual information. This perspective suggests that language emergence serves not only as a mechanism creating a communication tool but also as a mechanism for allowing people to realize shared understanding of qualitative experiences. The paper discusses the implications of this bidirectional influence in the context of consciousness studies, linguistics, and cognitive science, and outlines future constructive research directions to further explore this dynamic relationship between language emergence and qualia structure.","authors":["Tadahiro Taniguchi","Masafumi Oizumi","Noburo Saji","Takato Horii","Naotsugu Tsuchiya"],"url":"https://arxiv.org/abs/2409.09413"}
{"created":"2025-05-06","title":"MFCLIP: Multi-modal Fine-grained CLIP for Generalizable Diffusion Face Forgery Detection","abstract":"The rapid development of photo-realistic face generation methods has raised significant concerns in society and academia, highlighting the urgent need for robust and generalizable face forgery detection (FFD) techniques. Although existing approaches mainly capture face forgery patterns using image modality, other modalities like fine-grained noises and texts are not fully explored, which limits the generalization capability of the model. In addition, most FFD methods tend to identify facial images generated by GAN, but struggle to detect unseen diffusion-synthesized ones. To address the limitations, we aim to leverage the cutting-edge foundation model, contrastive language-image pre-training (CLIP), to achieve generalizable diffusion face forgery detection (DFFD). In this paper, we propose a novel multi-modal fine-grained CLIP (MFCLIP) model, which mines comprehensive and fine-grained forgery traces across image-noise modalities via language-guided face forgery representation learning, to facilitate the advancement of DFFD. Specifically, we devise a fine-grained language encoder (FLE) that extracts fine global language features from hierarchical text prompts. We design a multi-modal vision encoder (MVE) to capture global image forgery embeddings as well as fine-grained noise forgery patterns extracted from the richest patch, and integrate them to mine general visual forgery traces. Moreover, we build an innovative plug-and-play sample pair attention (SPA) method to emphasize relevant negative pairs and suppress irrelevant ones, allowing cross-modality sample pairs to conduct more flexible alignment. Extensive experiments and visualizations show that our model outperforms the state of the arts on different settings like cross-generator, cross-forgery, and cross-dataset evaluations.","authors":["Yaning Zhang","Tianyi Wang","Zitong Yu","Zan Gao","Linlin Shen","Shengyong Chen"],"url":"https://arxiv.org/abs/2409.09724"}
{"created":"2025-05-06","title":"Underwater Image Enhancement via Dehazing and Color Restoration","abstract":"Underwater visual imaging is crucial for marine engineering, but it suffers from low contrast, blurriness, and color degradation, which hinders downstream analysis. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process, neglecting their inherent independence while overlooking their synergistic relationship. To overcome this limitation, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) that dynamically integrates these decoupled features to achieve comprehensive enhancement. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to respectively preserve color fidelity and enhance structural details during network training. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images.","authors":["Chengqin Wu","Shuai Yu","Tuyan Luo","Qiuhua Rao","Qingson Hu","Jingxiang Xu","Lijun Zhang"],"url":"https://arxiv.org/abs/2409.09779"}
{"created":"2025-05-06","title":"Adaptive Anomaly Detection in Network Flows with Low-Rank Tensor Decompositions and Deep Unrolling","abstract":"Anomaly detection (AD) is increasingly recognized as a key component for ensuring the resilience of future communication systems. While deep learning has shown state-of-the-art AD performance, its application in critical systems is hindered by concerns regarding training data efficiency, domain adaptation and interpretability. This work considers AD in network flows using incomplete measurements, leveraging a robust tensor decomposition approach and deep unrolling techniques to address these challenges. We first propose a novel block-successive convex approximation algorithm based on a regularized model-fitting objective where the normal flows are modeled as low-rank tensors and anomalies as sparse. An augmentation of the objective is introduced to decrease the computational cost. We apply deep unrolling to derive a novel deep network architecture based on our proposed algorithm, treating the regularization parameters as learnable weights. Inspired by Bayesian approaches, we extend the model architecture to perform online adaptation to per-flow and per-time-step statistics, improving AD performance while maintaining a low parameter count and preserving the problem's permutation equivariances. To optimize the deep network weights for detection performance, we employ a homotopy optimization approach based on an efficient approximation of the area under the receiver operating characteristic curve. Extensive experiments on synthetic and real-world data demonstrate that our proposed deep network architecture exhibits a high training data efficiency, outperforms reference methods, and adapts seamlessly to varying network topologies.","authors":["Lukas Schynol","Marius Pesavento"],"url":"https://arxiv.org/abs/2409.11529"}
{"created":"2025-05-06","title":"Hyper-parameter Optimization for Wireless Network Traffic Prediction Models with A Novel Meta-Learning Framework","abstract":"This paper proposes a novel meta-learning based hyper-parameter optimization framework for wireless network traffic prediction (NTP) models. The primary objective is to accumulate and leverage the acquired hyper-parameter optimization experience, enabling the rapid determination of optimal hyper-parameters for new tasks. In this paper, an attention-based deep neural network (ADNN) is employed as the base-learner to address specific NTP tasks. The meta-learner is an innovative framework that integrates meta-learning with the k-nearest neighbor algorithm (KNN), genetic algorithm (GA), and gated residual network (GRN). Specifically, KNN is utilized to identify a set of candidate hyper-parameter selection strategies for a new task, which then serves as the initial population for GA, while a GRN-based chromosome screening module accelerates the validation of offspring chromosomes, ultimately determining the optimal hyper-parameters. Experimental results demonstrate that, compared to traditional methods such as Bayesian optimization (BO), GA, and particle swarm optimization (PSO), the proposed framework determines optimal hyper-parameters more rapidly, significantly reduces optimization time, and enhances the performance of the base-learner. It achieves an optimal balance between optimization efficiency and prediction accuracy.","authors":["Liangzhi Wang","Jie Zhang","Yuan Gao","Jiliang Zhang","Guiyi Wei","Haibo Zhou","Bin Zhuge","Zitian Zhang"],"url":"https://arxiv.org/abs/2409.14535"}
{"created":"2025-05-06","title":"Spatio-Tempora Metric-Semantic Mapping for Persistent Orchard Monitoring: Method and Dataset","abstract":"Monitoring orchards at the individual tree or fruit level throughout the growth season is crucial for plant phenotyping and horticultural resource optimization, such as chemical use and yield estimation. We present a 4D spatio-temporal metric-semantic mapping system that integrates multi-session measurements to track fruit growth over time. Our approach combines a LiDAR-RGB fusion module for 3D fruit localization with a 4D fruit association method leveraging positional, visual, and topology information for improved data association precision. Evaluated on real orchard data, our method achieves a 96.9% fruit counting accuracy for 1,790 apples across 60 trees, a mean fruit size estimation error of 1.1 cm, and a 23.7% improvement in 4D data association precision over baselines. We publicly release a multimodal dataset covering five fruit species across their growth seasons. https://4d-metric-semantic-mapping.org/","authors":["Jiuzhou Lei","Ankit Prabhu","Xu Liu","Fernando Cladera","Mehrad Mortazavi","Reza Ehsani","Pratik Chaudhari","Vijay Kumar"],"url":"https://arxiv.org/abs/2409.19786"}
{"created":"2025-05-06","title":"MNT Elliptic Curves with Non-Prime Order","abstract":"Miyaji, Nakabayashi, and Takano proposed the algorithm for the construction of prime order pairing-friendly elliptic curves with embedding degrees $k=3,4,6$. We present a method for generating generalized MNT curves. The order of such pairing-friendly curves is the product of two prime numbers.","authors":["Maciej Grze\\'skowiak"],"url":"https://arxiv.org/abs/2409.20254"}
{"created":"2025-05-06","title":"Object-Centric Kinodynamic Planning for Nonprehensile Robot Rearrangement Manipulation","abstract":"Nonprehensile actions such as pushing are crucial for addressing multi-object rearrangement problems. To date, existing nonprehensile solutions are all robot-centric, i.e., the manipulation actions are generated with robot-relevant intent and their outcomes are passively evaluated afterwards. Such pipelines are very different from human strategies and are typically inefficient. To this end, this work proposes a novel object-centric planning paradigm and develops the first object-centric planner for general nonprehensile rearrangement problems. By assuming that each object can actively move without being driven by robot interactions, the object-centric planner focuses on planning desired object motions, which are realized via robot actions generated online via a closed-loop pushing strategy. Through extensive experiments and in comparison with state-of-the-art baselines in both simulation and on a physical robot, we show that our object-centric paradigm can generate more intuitive and task-effective robot actions with significantly improved efficiency. In addition, we propose a benchmarking protocol to standardize and facilitate future research in nonprehensile rearrangement.","authors":["Kejia Ren","Gaotian Wang","Andrew S. Morgan","Lydia E. Kavraki","Kaiyu Hang"],"url":"https://arxiv.org/abs/2410.00261"}
{"created":"2025-05-06","title":"Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport","abstract":"Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learned directly from data. Theoretically, we explore the connections between the RUOT and Schr\\\"odinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape. Our code is available at: https://github.com/zhenyiizhang/DeepRUOT.","authors":["Zhenyi Zhang","Tiejun Li","Peijie Zhou"],"url":"https://arxiv.org/abs/2410.00844"}
{"created":"2025-05-06","title":"High and Low Resolution Tradeoffs in Roadside Multimodal Sensing","abstract":"Balancing cost and performance is crucial when choosing high- versus low-resolution point-cloud roadside sensors. For example, LiDAR delivers dense point cloud, while 4D millimeter-wave radar, though spatially sparser, embeds velocity cues that help distinguish objects and come at a lower price. Unfortunately, the sensor placement strategies will influence point cloud density and distribution across the coverage area. Compounding the first challenge is the fact that different sensor mixtures often demand distinct neural network architectures to maximize their complementary strengths. Without an evaluation framework that establishes a benchmark for comparison, it is imprudent to make claims regarding whether marginal gains result from higher resolution and new sensing modalities or from the algorithms. We present an ex-ante evaluation that addresses the two challenges. First, we realized a simulation tool that builds on integer programming to automatically compare different sensor placement strategies against coverage and cost jointly. Additionally, inspired by human multi-sensory integration, we propose a modular framework to assess whether reductions in spatial resolution can be compensated by informational richness in detecting traffic participants. Extensive experimental testing on the proposed framework shows that fusing velocity-encoded radar with low-resolution LiDAR yields marked gains (14 percent AP for pedestrians and an overall mAP improvement of 1.5 percent across six categories) at lower cost than high-resolution LiDAR alone. Notably, these marked gains hold regardless of the specific deep neural modules employed in our frame. The result challenges the prevailing assumption that high resolution are always superior to low-resolution alternatives.","authors":["Shaozu Ding","Yihong Tang","Marco De Vincenzi","Dajiang Suo"],"url":"https://arxiv.org/abs/2410.01250"}
{"created":"2025-05-06","title":"HAINAN: Fast and Accurate Transducer for Hybrid-Autoregressive ASR","abstract":"We present Hybrid-Autoregressive INference TrANsducers (HAINAN), a novel architecture for speech recognition that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAINAN supports both autoregressive inference with all network components and non-autoregressive inference without the predictor. Additionally, we propose a novel semi-autoregressive inference paradigm that first generates an initial hypothesis using non-autoregressive inference, followed by refinement steps where each token prediction is regenerated using parallelized autoregression on the initial hypothesis. Experiments on multiple datasets across different languages demonstrate that HAINAN achieves efficiency parity with CTC in non-autoregressive mode and with TDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN outperforms TDT and RNN-T, while non-autoregressive HAINAN significantly outperforms CTC. Semi-autoregressive inference further enhances the model's accuracy with minimal computational overhead, and even outperforms TDT results in some cases. These results highlight HAINAN's flexibility in balancing accuracy and speed, positioning it as a strong candidate for real-world speech recognition applications.","authors":["Hainan Xu","Travis M. Bartley","Vladimir Bataev","Boris Ginsburg"],"url":"https://arxiv.org/abs/2410.02597"}
{"created":"2025-05-06","title":"Understanding Large Language Models in Your Pockets: Performance Study on COTS Mobile Devices","abstract":"As large language models (LLMs) increasingly integrate into every aspect of our work and daily lives, there are growing concerns about user privacy, which push the trend toward local deployment of these models. There are a number of lightweight LLMs (e.g., Gemini Nano, LLAMA2 7B) that can run locally on smartphones, providing users with greater control over their personal data. As a rapidly emerging application, we are concerned about their performance on commercial-off-the-shelf mobile devices. To fully understand the current landscape of LLM deployment on mobile platforms, we conduct a comprehensive measurement study on mobile devices. We evaluate both metrics that affect user experience, including token throughput, latency, and battery consumption, as well as factors critical to developers, such as resource utilization, DVFS strategies, and inference engines. In addition, we provide a detailed analysis of how these hardware capabilities and system dynamics affect on-device LLM performance, which may help developers identify and address bottlenecks for mobile LLM applications. We also provide comprehensive comparisons across the mobile system-on-chips (SoCs) from major vendors, highlighting their performance differences in handling LLM workloads. We hope that this study can provide insights for both the development of on-device LLMs and the design for future mobile system architecture.","authors":["Jie Xiao","Qianyi Huang","Xu Chen","Chen Tian"],"url":"https://arxiv.org/abs/2410.03613"}
{"created":"2025-05-06","title":"SDA-GRIN for Adaptive Spatial-Temporal Multivariate Time Series Imputation","abstract":"In various applications, the multivariate time series often suffers from missing data. This issue can significantly disrupt systems that rely on the data. Spatial and temporal dependencies can be leveraged to impute the missing samples. Existing imputation methods often ignore dynamic changes in spatial dependencies. We propose a Spatial Dynamic Aware Graph Recurrent Imputation Network (SDA-GRIN) which is capable of capturing dynamic changes in spatial dependencies.SDA-GRIN leverages a multi-head attention mechanism to adapt graph structures with time. SDA-GRIN models multivariate time series as a sequence of temporal graphs and uses a recurrent message-passing architecture for imputation. We evaluate SDA-GRIN on four real-world datasets: SDA-GRIN improves MSE by 9.51% for the AQI and 9.40% for AQI-36. On the PEMS-BAY dataset, it achieves a 1.94% improvement in MSE. Detailed ablation study demonstrates the effect of window sizes and missing data on the performance of the method. Project page:https://ameskandari.github.io/sda-grin/","authors":["Amir Eskandari","Aman Anand","Drishti Sharma","Farhana Zulkernine"],"url":"https://arxiv.org/abs/2410.03954"}
{"created":"2025-05-06","title":"T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data","abstract":"Self-supervision is often used for pre-training to foster performance on a downstream task by constructing meaningful representations of samples. Self-supervised learning (SSL) generally involves generating different views of the same sample and thus requires data augmentations that are challenging to construct for tabular data. This constitutes one of the main challenges of self-supervision for structured data. In the present work, we propose a novel augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on a Joint Embedding Predictive Architecture (JEPA) and is akin to mask reconstruction in the latent space. It involves predicting the latent representation of one subset of features from the latent representation of a different subset within the same sample, thereby learning rich representations without augmentations. We use our method as a pre-training technique and train several deep classifiers on the obtained representation. Our experimental results demonstrate a substantial improvement in both classification and regression tasks, outperforming models trained directly on samples in their original data space. Moreover, T-JEPA enables some methods to consistently outperform or match the performance of traditional methods likes Gradient Boosted Decision Trees. To understand why, we extensively characterize the obtained representations and show that T-JEPA effectively identifies relevant features for downstream tasks without access to the labels. Additionally, we introduce regularization tokens, a novel regularization method critical for training of JEPA-based models on structured data.","authors":["Hugo Thimonier","Jos\\'e Lucas De Melo Costa","Fabrice Popineau","Arpad Rimmel","Bich-Li\\^en Doan"],"url":"https://arxiv.org/abs/2410.05016"}
{"created":"2025-05-06","title":"Shifted CholeskyQR for sparse matrices","abstract":"In this work, we focus on Shifted CholeskyQR for sparse matrices, which is widely used in real applications. We introduce a new model for sparse matrices, categorizing them into two types, $T_{1}$ matrices and $T_{2}$ matrices, based on the presence of dense columns. We provide an alternative choice of the shifted item $s$ for Shifted CholeskyQR3 \\cite{Shifted} based on the structure and the key element of the input sparse $X$. We do rounding error analysis of Shifted CholeskyQR3 with such an $s$ and show that it is optimal compared to the original one in \\cite{New} with proper element-norm conditions (ENCs) for $T_{1}$ matrices, improving the applicability while maintaining numerical stability. Our theoretical analysis utilizes the properties of $[\\cdot]_{g}$ proposed in \\cite{New}, which is the first to build connections between rounding error analysis and sparse matrices. Numerical experiments confirm our findings for $T_{1}$ matrices. Additionally, Shifted CholeskyQR3 with the alternative choice $s$ is applicable to $T_{2}$ matrices, which are more ill-conditioned than dense cases. Furthermore, Shifted CholeskyQR3 with our alternative $s$ shows good efficiency for both $T_{1}$ and $T_{2}$ matrices.","authors":["Haoran Guan","Yuwei Fan"],"url":"https://arxiv.org/abs/2410.06525"}
{"created":"2025-05-06","title":"Bayes-Nash Generative Privacy Against Membership Inference Attacks","abstract":"Membership inference attacks (MIAs) expose significant privacy risks by determining whether an individual's data is in a dataset. While differential privacy (DP) mitigates such risks, it has several limitations in achieving an optimal balance between utility and privacy, include limited resolution in expressing this tradeoff in only a few privacy parameters, and intractable sensitivity calculations that may be necessary to provide tight privacy guarantees. We propose a game-theoretic framework that models privacy protection from MIA as a Bayesian game between a defender and an attacker. In this game, a dataset is the defender's private information, with privacy loss to the defender (which is gain to the attacker) captured in terms of the attacker's ability to infer membership of individuals in the dataset. To address the strategic complexity of this game, we represent the mixed strategy of the defender as a neural network generator which maps a private dataset to its public representation (for example, noisy summary statistics), while the mixed strategy of the attacker is captured by a discriminator which makes membership inference claims. We refer to the resulting computational approach as a general-sum Generative Adversarial Network, which is trained iteratively by alternating generator and discriminator updates akin to conventional GANs. We call the defender's data sharing policy thereby obtained Bayes-Nash Generative Privacy (BNGP). The BNGP strategy avoids sensitivity calculations, supports compositions of correlated mechanisms, is robust to the attacker's heterogeneous preferences over true and false positives, and yields provable differential privacy guarantees, albeit in an idealized setting.","authors":["Tao Zhang","Rajagopal Venkatesaramani","Rajat K. De","Bradley A. Malin","Yevgeniy Vorobeychik"],"url":"https://arxiv.org/abs/2410.07414"}
{"created":"2025-05-06","title":"3D Vision-Language Gaussian Splatting","abstract":"Recent advancements in 3D reconstruction methods and vision-language models have propelled the development of multi-modal 3D scene understanding, which has vital applications in robotics, autonomous driving, and virtual/augmented reality. However, current multi-modal scene understanding approaches have naively embedded semantic representations into 3D reconstruction methods without striking a balance between visual and language modalities, which leads to unsatisfying semantic rasterization of translucent or reflective objects, as well as over-fitting on color modality. To alleviate these limitations, we propose a solution that adequately handles the distinct visual and semantic modalities, i.e., a 3D vision-language Gaussian splatting model for scene understanding, to put emphasis on the representation learning of language modality. We propose a novel cross-modal rasterizer, using modality fusion along with a smoothed semantic indicator for enhancing semantic rasterization. We also employ a camera-view blending technique to improve semantic consistency between existing and synthesized views, thereby effectively mitigating over-fitting. Extensive experiments demonstrate that our method achieves state-of-the-art performance in open-vocabulary semantic segmentation, surpassing existing methods by a significant margin.","authors":["Qucheng Peng","Benjamin Planche","Zhongpai Gao","Meng Zheng","Anwesa Choudhuri","Terrence Chen","Chen Chen","Ziyan Wu"],"url":"https://arxiv.org/abs/2410.07577"}
{"created":"2025-05-06","title":"Probabilistic error analysis of CholeskyQR based on columns","abstract":"In this work, we utilize the randomized models presented in \\cite{New} and do probabilistic error analysis of CholeskyQR2 and Shifted CholeskyQR3. We integrate the theoretical analysis with $[\\cdot]_{g}$ defined in \\cite{Columns}, providing sharper upper bounds of accuracy and better sufficient conditions for CholeskyQR2 and Shifted CholeskyQR3 along with the corresponding probabilities. Moreover, a probabilistic shifted item $s$ for Shifted CholeskyQR3 is received, improving the applicability of the algorithm while maintaining numerical stability. Numerical experiments confirm our findings and show that such a probabilistic $s$ has good robustness in ill-conditioned cases.","authors":["Haoran Guan","Yuwei Fan"],"url":"https://arxiv.org/abs/2410.09389"}
{"created":"2025-05-06","title":"AI-based particle track identification in scintillating fibres read out with imaging sensors","abstract":"This paper presents the development and application of an AI-based method for particle track identification using scintillating fibres read out with imaging sensors. We propose a variational autoencoder (VAE) to efficiently filter and identify frames containing signal from the substantial data generated by SPAD array sensors. Our VAE model, trained on purely background frames, demonstrated a high capability to distinguish frames containing particle tracks from background noise. The performance of the VAE-based anomaly detection was validated with experimental data, demonstrating the method's ability to efficiently identify relevant events with rapid processing time, suggesting a solid prospect for deployment as a fast inference tool on hardware for real-time anomaly detection. This work highlights the potential of combining advanced sensor technology with machine learning techniques to enhance particle detection and tracking.","authors":["Noemi B\\\"uhrer","Sa\\'ul Alonso-Monsalve","Matthew Franks","Till Dieminger","Davide Sgalaberna"],"url":"https://arxiv.org/abs/2410.10519"}
{"created":"2025-05-06","title":"Hard-Constrained Neural Networks with Universal Approximation Guarantees","abstract":"Incorporating prior knowledge or specifications of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and leads to conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction--an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Unlike approaches that modify outputs only at inference time, HardNet enables end-to-end training with hard constraint guarantees, leading to improved performance. To the best of our knowledge, HardNet is the first method with an efficient forward pass to enforce more than one input-dependent inequality constraint. It allows unconstrained optimization of the network parameters using standard algorithms by appending a differentiable closed-form enforcement layer to the network's output. Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: learning with piecewise constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems.","authors":["Youngjae Min","Navid Azizan"],"url":"https://arxiv.org/abs/2410.10807"}
{"created":"2025-05-06","title":"Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models","abstract":"3D meshes are widely used in computer vision and graphics for their efficiency in animation and minimal memory use, playing a crucial role in movies, games, AR, and VR. However, creating temporally consistent and realistic textures for mesh sequences remains labor-intensive for professional artists. On the other hand, while video diffusion models excel at text-driven video generation, they often lack 3D geometry awareness and struggle with achieving multi-view consistent texturing for 3D meshes. In this work, we present Tex4D, a zero-shot approach that integrates inherent 3D geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4D textures. Given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the UV space. To ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis. However, straightforwardly combining the video diffusion model and the UV texture aggregation leads to blurry results. We analyze the underlying causes and propose a simple yet effective modification to the DDIM sampling process to address this issue. Additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process. To the best of our knowledge, Tex4D is the first method specifically designed for 4D scene texturing. Extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences.","authors":["Jingzhi Bao","Xueting Li","Ming-Hsuan Yang"],"url":"https://arxiv.org/abs/2410.10821"}
{"created":"2025-05-06","title":"MF-LAL: Drug Compound Generation Using Multi-Fidelity Latent Space Active Learning","abstract":"Current generative models for drug discovery primarily use molecular docking as an oracle to guide the generation of active compounds. However, such models are often not useful in practice because even compounds with high docking scores do not consistently show real-world experimental activity. More accurate methods for activity prediction exist, such as molecular dynamics based binding free energy calculations, but they are too computationally expensive to use in a generative model. To address this challenge, we propose Multi-Fidelity Latent space Active Learning (MF-LAL), a generative modeling framework that integrates a set of oracles with varying cost-accuracy tradeoffs. We train a surrogate model for each oracle and use these surrogates to guide generation of compounds with high predicted activity. Unlike previous approaches that separately learn the surrogate model and generative model, MF-LAL combines the generative and multi-fidelity surrogate models into a single framework, allowing for more accurate activity prediction and higher quality samples. We train MF-LAL with a novel active learning algorithm to further reduce computational cost. Our experiments on two disease-relevant proteins show that MF-LAL produces compounds with significantly better binding free energy scores than other single and multi-fidelity approaches (~50% improvement in mean binding free energy score).","authors":["Peter Eckmann","Dongxia Wu","Germano Heinzelmann","Michael K. Gilson","Rose Yu"],"url":"https://arxiv.org/abs/2410.11226"}
{"created":"2025-05-06","title":"Algebraic Language Theory with Effects","abstract":"Regular languages -- the languages accepted by deterministic finite automata -- are known to be precisely the languages recognized by finite monoids. This characterization is the origin of algebraic language theory. In this paper, we generalize the correspondence between automata and monoids to automata with generic computational effects given by a monad, providing the foundations of an effectful algebraic language theory. We show that, under suitable conditions on the monad, a language is computable by an effectful automaton precisely when it is recognizable by (1) an effectful monoid morphism into an effect-free finite monoid, and (2) a monoid morphism into a monad-monoid bialgebra whose carrier is a finitely generated algebra for the monad, the former mode of recognition being conceptually completely new. Our prime application is a novel algebraic approach to languages computed by probabilistic finite automata. Additionally, we derive new algebraic characterizations for nondeterministic probabilistic finite automata and for weighted finite automata over unrestricted semirings, generalizing previous results on weighted algebraic recognition over commutative rings.","authors":["Fabian Lenke","Stefan Milius","Henning Urbat","Thorsten Wi{\\ss}mann"],"url":"https://arxiv.org/abs/2410.12569"}
{"created":"2025-05-06","title":"Mimetic Metrics for the DGSEM","abstract":"Free-stream preservation is an essential property for numerical solvers on curvilinear grids. Key to this property is that the metric terms of the curvilinear mapping satisfy discrete metric identities, i.e., have zero divergence. Divergence-free metric terms are furthermore essential for entropy stability on curvilinear grids. We present a new way to compute the metric terms for discontinuous Galerkin spectral element methods (DGSEMs) that guarantees they are divergence-free. Our proposed mimetic approach uses projections that fit within the de Rham Cohomology.","authors":["Daniel Bach","Andr\\'es Rueda-Ram\\'irez","David A. Kopriva","Gregor J. Gassner"],"url":"https://arxiv.org/abs/2410.14502"}
{"created":"2025-05-06","title":"ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions","abstract":"Retrieval-augmented generation (RAG) has become integral to large language models (LLMs), particularly for conversational AI systems where user questions may reference knowledge beyond the LLMs' training cutoff. However, many natural user questions lack well-defined answers, either due to limited domain knowledge or because the retrieval system returns documents that are relevant in appearance but uninformative in content. In such cases, LLMs often produce hallucinated answers without flagging them. While recent work has largely focused on questions with false premises, we study out-of-scope questions, where the retrieved document appears semantically similar to the question but lacks the necessary information to answer it. In this paper, we propose a guided hallucination-based approach ELOQ to automatically generate a diverse set of out-of-scope questions from post-cutoff documents, followed by human verification to ensure quality. We use this dataset to evaluate several LLMs on their ability to detect out-of-scope questions and generate appropriate responses. Finally, we introduce an improved detection method that enhances the reliability of LLM-based question-answering systems in handling out-of-scope questions.","authors":["Zhiyuan Peng","Jinming Nian","Alexandre Evfimievski","Yi Fang"],"url":"https://arxiv.org/abs/2410.14567"}
{"created":"2025-05-06","title":"On the Vulnerability of Text Sanitization","abstract":"Text sanitization, which employs differential privacy to replace sensitive tokens with new ones, represents a significant technique for privacy protection. Typically, its performance in preserving privacy is evaluated by measuring the attack success rate (ASR) of reconstruction attacks, where attackers attempt to recover the original tokens from the sanitized ones. However, current reconstruction attacks on text sanitization are developed empirically, making it challenging to accurately assess the effectiveness of sanitization. In this paper, we aim to provide a more accurate evaluation of sanitization effectiveness. Inspired by the works of Palamidessi et al., we implement theoretically optimal reconstruction attacks targeting text sanitization. We derive their bounds on ASR as benchmarks for evaluating sanitization performance. For real-world applications, we propose two practical reconstruction attacks based on these theoretical findings. Our experimental results underscore the necessity of reassessing these overlooked risks. Notably, one of our attacks achieves a 46.4% improvement in ASR over the state-of-the-art baseline, with a privacy budget of epsilon=4.0 on the SST-2 dataset. Our code is available at: https://github.com/mengtong0110/On-the-Vulnerability-of-Text-Sanitization.","authors":["Meng Tong","Kejiang Chen","Xiaojian Yuan","Jiayang Liu","Weiming Zhang","Nenghai Yu","Jie Zhang"],"url":"https://arxiv.org/abs/2410.17052"}
{"created":"2025-05-06","title":"LLMs for Extremely Low-Resource Finno-Ugric Languages","abstract":"The advancement of large language models (LLMs) has predominantly focused on high-resource languages, leaving low-resource languages, such as those in the Finno-Ugric family, significantly underrepresented. This paper addresses this gap by focusing on V\\~oro, Livonian, and Komi. We cover almost the entire cycle of LLM creation, from data collection to instruction tuning and evaluation. Our contributions include developing multilingual base and instruction-tuned models; creating evaluation benchmarks, including the smugri-MT-bench multi-turn conversational benchmark; and conducting human evaluation. We intend for this work to promote linguistic diversity, ensuring that lesser-resourced languages can benefit from advancements in NLP.","authors":["Taido Purason","Hele-Andra Kuulmets","Mark Fishel"],"url":"https://arxiv.org/abs/2410.18902"}
{"created":"2025-05-06","title":"Quantum Rationale-Aware Graph Contrastive Learning for Jet Discrimination","abstract":"In high-energy physics, particle jet tagging plays a pivotal role in distinguishing quark from gluon jets using data from collider experiments. While graph-based deep learning methods have advanced this task beyond traditional feature-engineered approaches, the complex data structure and limited labeled samples present ongoing challenges. However, existing contrastive learning (CL) frameworks struggle to leverage rationale-aware augmentations effectively, often lacking supervision signals that guide the extraction of salient features and facing computational efficiency issues such as high parameter counts. In this study, we demonstrate that integrating a quantum rationale generator (QRG) within our proposed Quantum Rationale-aware Graph Contrastive Learning (QRGCL) framework significantly enhances jet discrimination performance, reducing reliance on labeled data and capturing discriminative features. Evaluated on the quark-gluon jet dataset, QRGCL achieves an AUC score of $77.53\\%$ while maintaining a compact architecture of only 45 QRG parameters, outperforming classical, quantum, and hybrid GCL and GNN benchmarks. These results highlight QRGCL's potential to advance jet tagging and other complex classification tasks in high-energy physics, where computational efficiency and feature extraction limitations persist.","authors":["Md Abrar Jahin","Md. Akmol Masud","M. F. Mridha","Nilanjan Dey","Zeyar Aung"],"url":"https://arxiv.org/abs/2411.01642"}
{"created":"2025-05-06","title":"CAD-NeRF: Learning NeRFs from Uncalibrated Few-view Images by CAD Model Retrieval","abstract":"Reconstructing from multi-view images is a longstanding problem in 3D vision, where neural radiance fields (NeRFs) have shown great potential and get realistic rendered images of novel views. Currently, most NeRF methods either require accurate camera poses or a large number of input images, or even both. Reconstructing NeRF from few-view images without poses is challenging and highly ill-posed. To address this problem, we propose CAD-NeRF, a method reconstructed from less than 10 images without any known poses. Specifically, we build a mini library of several CAD models from ShapeNet and render them from many random views. Given sparse-view input images, we run a model and pose retrieval from the library, to get a model with similar shapes, serving as the density supervision and pose initializations. Here we propose a multi-view pose retrieval method to avoid pose conflicts among views, which is a new and unseen problem in uncalibrated NeRF methods. Then, the geometry of the object is trained by the CAD guidance. The deformation of the density field and camera poses are optimized jointly. Then texture and density are trained and fine-tuned as well. All training phases are in self-supervised manners. Comprehensive evaluations of synthetic and real images show that CAD-NeRF successfully learns accurate densities with a large deformation from retrieved CAD models, showing the generalization abilities.","authors":["Xin Wen","Xuening Zhu","Renjiao Yi","Zhifeng Wang","Chenyang Zhu","Kai Xu"],"url":"https://arxiv.org/abs/2411.02979"}
{"created":"2025-05-06","title":"From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice","abstract":"Creative writing is a deeply human craft, yet AI systems using large language models (LLMs) offer the automation of significant parts of the writing process. So why do some creative writers choose to use AI? Through interviews and observed writing sessions with 18 creative writers who already use AI regularly in their writing practice, we find that creative writers are intentional about how they incorporate AI, making many deliberate decisions about when and how to engage AI based on their core values, such as authenticity and craftsmanship. We characterize the interplay between writers' values, their fluid relationships with AI, and specific integration strategies -- ultimately enabling writers to create new AI workflows without compromising their creative values. We provide insight for writing communities, AI developers and future researchers on the importance of supporting transparency of these emerging writing processes and rethinking what AI features can best serve writers.","authors":["Alicia Guo","Shreya Sathyanarayanan","Leijie Wang","Jeffrey Heer","Amy Zhang"],"url":"https://arxiv.org/abs/2411.03137"}
{"created":"2025-05-06","title":"Balancing Pipeline Parallelism with Vocabulary Parallelism","abstract":"Pipeline parallelism is widely used to scale the training of transformer-based large language models, various works have been done to improve its throughput and memory footprint. In this paper, we address a frequently overlooked issue: the vocabulary layers can cause imbalanced computation and memory usage across pipeline stages, worsening pipeline bubbles and the memory bottleneck. To tackle this, we partition the vocabulary layers evenly across pipeline devices and group the computation into pipeline passes. To reduce the activation memory overhead, we propose several algorithms to reduce communication barriers within vocabulary layers. Additionally, we utilize a generalizable method to integrate Vocabulary Parallelism with existing pipeline schedules. By combining these techniques, our methods effectively balance the computation and parameter memory, with only a small constant activation memory overhead. Notably, when combined with activation memory-balanced schedules like V-Half, our approach achieves perfect balance in both memory and computation. Extensive evaluations demonstrate that our method achieves computation and memory balance regardless of the vocabulary size, resulting in a 5% to 51% improvement in throughput compared to naive approaches, meanwhile significantly reducing peak memory usage especially for large vocabulary scenarios. Our implementation is open-sourced at https://github.com/sail-sg/VocabularyParallelism .","authors":["Man Tsung Yeung","Penghui Qi","Min Lin","Xinyi Wan"],"url":"https://arxiv.org/abs/2411.05288"}
{"created":"2025-05-06","title":"Learning-Guided Fuzzing for Testing Stateful SDN Controllers","abstract":"Controllers for software-defined networks (SDNs) are centralised software components that enable advanced network functionalities, such as dynamic traffic engineering and network virtualisation. However, these functionalities increase the complexity of SDN controllers, making thorough testing crucial. SDN controllers are stateful, interacting with multiple network devices through sequences of control messages. Identifying stateful failures in an SDN controller is challenging due to the infinite possible sequences of control messages, which result in an unbounded number of stateful interactions between the controller and network devices. In this article, we propose SeqFuzzSDN, a learning-guided fuzzing method for testing stateful SDN controllers. SeqFuzzSDN aims to (1) efficiently explore the state space of the SDN controller under test, (2) generate effective and diverse tests (i.e., control message sequences) to uncover failures, and (3) infer accurate failure-inducing models that characterise the message sequences leading to failures. In addition, we compare SeqFuzzSDN with three extensions of state-of-the-art (SOTA) methods for fuzzing SDNs. Our findings show that, compared to the extended SOTA methods, SeqFuzzSDN (1) generates more diverse message sequences that lead to failures within the same time budget, and (2) produces more accurate failure-inducing models, significantly outperforming the other extended SOTA methods in terms of sensitivity.","authors":["Rapha\\\"el Ollando","Seung Yeob Shin","Lionel C. Briand"],"url":"https://arxiv.org/abs/2411.08626"}
{"created":"2025-05-06","title":"Unlocking Transfer Learning for Open-World Few-Shot Recognition","abstract":"Few-Shot Open-Set Recognition (FSOSR) targets a critical real-world challenge, aiming to categorize inputs into known categories, termed closed-set classes, while identifying open-set inputs that fall outside these classes. Although transfer learning where a model is tuned to a given few-shot task has become a prominent paradigm in closed-world, we observe that it fails to expand to open-world. To unlock this challenge, we propose a two-stage method which consists of open-set aware meta-learning with open-set free transfer learning. In the open-set aware meta-learning stage, a model is trained to establish a metric space that serves as a beneficial starting point for the subsequent stage. During the open-set free transfer learning stage, the model is further adapted to a specific target task through transfer learning. Additionally, we introduce a strategy to simulate open-set examples by modifying the training dataset or generating pseudo open-set examples. The proposed method achieves state-of-the-art performance on two widely recognized benchmarks, miniImageNet and tieredImageNet, with only a 1.5\\% increase in training effort. Our work demonstrates the effectiveness of transfer learning in FSOSR.","authors":["Byeonggeun Kim","Juntae Lee","Kyuhong Shim","Simyung Chang"],"url":"https://arxiv.org/abs/2411.09986"}
{"created":"2025-05-06","title":"Infinite Width Limits of Self Supervised Neural Networks","abstract":"The NTK is a widely used tool in the theoretical analysis of deep learning, allowing us to look at supervised deep neural networks through the lenses of kernel regression. Recently, several works have investigated kernel models for self-supervised learning, hypothesizing that these also shed light on the behavior of wide neural networks by virtue of the NTK. However, it remains an open question to what extent this connection is mathematically sound -- it is a commonly encountered misbelief that the kernel behavior of wide neural networks emerges irrespective of the loss function it is trained on. In this paper, we bridge the gap between the NTK and self-supervised learning, focusing on two-layer neural networks trained under the Barlow Twins loss. We prove that the NTK of Barlow Twins indeed becomes constant as the width of the network approaches infinity. Our analysis technique is a bit different from previous works on the NTK and may be of independent interest. Overall, our work provides a first justification for the use of classic kernel theory to understand self-supervised learning of wide neural networks. Building on this result, we derive generalization error bounds for kernelized Barlow Twins and connect them to neural networks of finite width.","authors":["Maximilian Fleissner","Gautham Govind Anil","Debarghya Ghoshdastidar"],"url":"https://arxiv.org/abs/2411.11176"}
{"created":"2025-05-06","title":"Distributed Maximum Flow in Planar Graphs","abstract":"The dual of a planar graph $G$ is a planar graph $G^*$ that has a vertex for each face of $G$ and an edge for each pair of adjacent faces of $G$. The profound relationship between a planar graph and its dual has been the algorithmic basis for solving numerous (centralized) classical problems on planar graphs. In the distributed setting however, the only use of planar duality is for finding a recursive decomposition of $G$ [DISC 2017, STOC 2019].","authors":["Yaseen Abd-Elhaleem (University of Haifa)","Michal Dory (University of Haifa)","Merav Parter (Weizmann Institute of Science)","Oren Weimann (University of Haifa)"],"url":"https://arxiv.org/abs/2411.11718"}
{"created":"2025-05-06","title":"Freezing of Gait Detection Using Gramian Angular Fields and Federated Learning from Wearable Sensors","abstract":"Freezing of gait (FOG) is a debilitating symptom of Parkinson's disease that impairs mobility and safety by increasing the risk of falls. An effective FOG detection system must be accurate, real-time, and deployable in free-living environments to enable timely interventions. However, existing detection methods face challenges due to (1) intra- and inter-patient variability, (2) subject-specific training, (3) using multiple sensors in FOG dominant locations (e.g., ankles) leading to high failure points, (4) centralized, non-adaptive learning frameworks that sacrifice patient privacy and prevent collaborative model refinement across populations and disease progression, and (5) most systems are tested in controlled settings, limiting their real-world applicability for continuous in-home monitoring. Addressing these gaps, we present FOGSense, a real-world deployable FOG detection system designed for uncontrolled, free-living conditions using only a single sensor. FOGSense uses Gramian Angular Field (GAF) transformations and privacy-preserving federated deep learning to capture temporal and spatial gait patterns missed by traditional methods with a low false positive rate. We evaluated our system using a public Parkinson's dataset collected in a free-living environment. FOGSense improves accuracy by 10.4% over a single-axis accelerometer, reduces failure points compared to multi-sensor systems, and demonstrates robustness to missing values. The federated architecture allows personalized model adaptation and efficient smartphone synchronization during off-peak hours, making it effective for long-term monitoring as symptoms evolve. Overall, FOGSense achieved a 22.2% improvement in F1-score and a 74.53% reduction in false positive rate compared to state-of-the-art methods, along with enhanced sensitivity for FOG episode detection.","authors":["Shovito Barua Soumma","S M Raihanul Alam","Rudmila Rahman","Umme Niraj Mahi","Abdullah Mamun","Sayyed Mostafa Mostafavi","Hassan Ghasemzadeh"],"url":"https://arxiv.org/abs/2411.11764"}
{"created":"2025-05-06","title":"The Double-Ellipsoid Geometry of CLIP","abstract":"Contrastive Language-Image Pre-Training (CLIP) is highly instrumental in machine learning applications within a large variety of domains. We investigate the geometry of this embedding, which is still not well understood. We examine the raw unnormalized embedding and show that text and image reside on linearly separable ellipsoid shells, not centered at the origin. We explain the benefits of having this structure, allowing to better embed instances according to their uncertainty during contrastive training. Frequent concepts in the dataset yield more false negatives, inducing greater uncertainty. A new notion of conformity is introduced, which measures the average cosine similarity of an instance to any other instance within a representative data set. We show this measure can be accurately estimated by simply computing the cosine similarity to the modality mean vector. Furthermore, we find that CLIP's modality gap optimizes the matching of the conformity distributions of image and text.","authors":["Meir Yossef Levi","Guy Gilboa"],"url":"https://arxiv.org/abs/2411.14517"}
{"created":"2025-05-06","title":"Large Language Model with Region-guided Referring and Grounding for CT Report Generation","abstract":"Computed tomography (CT) report generation is crucial to assist radiologists in interpreting CT volumes, which can be time-consuming and labor-intensive. Existing methods primarily only consider the global features of the entire volume, making it struggle to focus on specific regions and potentially missing abnormalities. To address this issue, we propose Reg2RG, the first region-guided referring and grounding framework for CT report generation, which enhances diagnostic performance by focusing on anatomical regions within the volume. Specifically, we utilize masks from a universal segmentation module to capture local features for each referring region. A local feature decoupling (LFD) strategy is proposed to preserve the local high-resolution details with little computational overhead. Then the local features are integrated with global features to capture inter-regional relationships within a cohesive context. Moreover, we propose a novel region-report alignment (RRA) training strategy. It leverages the recognition of referring regions to guide the generation of region-specific reports, enhancing the model's referring and grounding capabilities while also improving the report's interpretability. A large language model (LLM) is further employed as the language decoder to generate reports from integrated visual features, facilitating region-level comprehension. Extensive experiments on two large-scale chest CT-report datasets demonstrate the superiority of our method, which outperforms several state-of-the-art methods in terms of both natural language generation and clinical efficacy metrics while preserving promising interpretability. The code is available at https://github.com/zhi-xuan-chen/Reg2RG.","authors":["Zhixuan Chen","Yequan Bie","Haibo Jin","Hao Chen"],"url":"https://arxiv.org/abs/2411.15539"}
{"created":"2025-05-06","title":"On the Robustness of the Successive Projection Algorithm","abstract":"The successive projection algorithm (SPA) is a workhorse algorithm to learn the $r$ vertices of the convex hull of a set of $(r-1)$-dimensional data points, a.k.a. a latent simplex, which has numerous applications in data science. In this paper, we revisit the robustness to noise of SPA and several of its variants. In particular, when $r \\geq 3$, we prove the tightness of the existing error bounds for SPA and for two more robust preconditioned variants of SPA. We also provide significantly improved error bounds for SPA, by a factor proportional to the conditioning of the $r$ vertices, in two special cases: for the first extracted vertex, and when $r \\leq 2$. We then provide further improvements for the error bounds of a translated version of SPA proposed by Arora et al. (''A practical algorithm for topic modeling with provable guarantees'', ICML, 2013) in two special cases: for the first two extracted vertices, and when $r \\leq 3$. Finally, we propose a new more robust variant of SPA that first shifts and lifts the data points in order to minimize the conditioning of the problem. We illustrate our results on synthetic data.","authors":["Giovanni Barbarino","Nicolas Gillis"],"url":"https://arxiv.org/abs/2411.16195"}
{"created":"2025-05-06","title":"Context-Aware Input Orchestration for Video Inpainting","abstract":"Traditional neural network-driven inpainting methods struggle to deliver high-quality results within the constraints of mobile device processing power and memory. Our research introduces an innovative approach to optimize memory usage by altering the composition of input data. Typically, video inpainting relies on a predetermined set of input frames, such as neighboring and reference frames, often limited to five-frame sets. Our focus is to examine how varying the proportion of these input frames impacts the quality of the inpainted video. By dynamically adjusting the input frame composition based on optical flow and changes of the mask, we have observed an improvement in various contents including rapid visual context changes.","authors":["Hoyoung Kim","Azimbek Khudoyberdiev","Seonghwan Jeong","Jihoon Ryoo"],"url":"https://arxiv.org/abs/2411.16926"}
{"created":"2025-05-06","title":"Active Data Curation Effectively Distills Large-Scale Multimodal Models","abstract":"Knowledge distillation (KD) is the de facto standard for compressing large-scale models into smaller ones. Prior works have explored ever more complex KD strategies involving different objective functions, teacher-ensembles, and weight inheritance. In this work we explore an alternative, yet simple approach -- active data curation as effective distillation for contrastive multimodal pretraining. Our simple online batch selection method, ACID, outperforms strong KD baselines across various model-, data- and compute-configurations. Further, we find such an active data curation strategy to in fact be complementary to standard KD, and can be effectively combined to train highly performant inference-efficient models. Our simple and scalable pretraining framework, ACED, achieves state-of-the-art results across 27 zero-shot classification and retrieval tasks with upto 11% less inference FLOPs. We further demonstrate that our ACED models yield strong vision-encoders for training generative multimodal models in the LiT-Decoder setting, outperforming larger vision encoders for image-captioning and visual question-answering tasks.","authors":["Vishaal Udandarao","Nikhil Parthasarathy","Muhammad Ferjad Naeem","Talfan Evans","Samuel Albanie","Federico Tombari","Yongqin Xian","Alessio Tonioni","Olivier J. H\\'enaff"],"url":"https://arxiv.org/abs/2411.18674"}
{"created":"2025-05-06","title":"AMO Sampler: Enhancing Text Rendering with Overshooting","abstract":"Achieving precise alignment between textual instructions and generated images in text-to-image generation is a significant challenge, particularly in rendering written text within images. Sate-of-the-art models like Stable Diffusion 3 (SD3), Flux, and AuraFlow still struggle with accurate text depiction, resulting in misspelled or inconsistent text. We introduce a training-free method with minimal computational overhead that significantly enhances text rendering quality. Specifically, we introduce an overshooting sampler for pretrained rectified flow (RF) models, by alternating between over-simulating the learned ordinary differential equation (ODE) and reintroducing noise. Compared to the Euler sampler, the overshooting sampler effectively introduces an extra Langevin dynamics term that can help correct the compounding error from successive Euler steps and therefore improve the text rendering. However, when the overshooting strength is high, we observe over-smoothing artifacts on the generated images. To address this issue, we propose an Attention Modulated Overshooting sampler (AMO), which adaptively controls the strength of overshooting for each image patch according to their attention score with the text content. AMO demonstrates a 32.3% and 35.9% improvement in text rendering accuracy on SD3 and Flux without compromising overall image quality or increasing inference cost. Code available at: https://github.com/hxixixh/amo-release.","authors":["Xixi Hu","Keyang Xu","Bo Liu","Qiang Liu","Hongliang Fei"],"url":"https://arxiv.org/abs/2411.19415"}
{"created":"2025-05-06","title":"LiDAR-EDIT: LiDAR Data Generation by Editing the Object Layouts in Real-World Scenes","abstract":"We present LiDAR-EDIT, a novel paradigm for generating synthetic LiDAR data for autonomous driving. Our framework edits real-world LiDAR scans by introducing new object layouts while preserving the realism of the background environment. Compared to end-to-end frameworks that generate LiDAR point clouds from scratch, LiDAR-EDIT offers users full control over the object layout, including the number, type, and pose of objects, while keeping most of the original real-world background. Our method also provides object labels for the generated data. Compared to novel view synthesis techniques, our framework allows for the creation of counterfactual scenarios with object layouts significantly different from the original real-world scene. LiDAR-EDIT uses spherical voxelization to enforce correct LiDAR projective geometry in the generated point clouds by construction. During object removal and insertion, generative models are employed to fill the unseen background and object parts that were occluded in the original real LiDAR scans. Experimental results demonstrate that our framework produces realistic LiDAR scans with practical value for downstream tasks.","authors":["Shing-Hei Ho","Bao Thach","Minghan Zhu"],"url":"https://arxiv.org/abs/2412.00592"}
{"created":"2025-05-06","title":"Adaptive Informed Deep Neural Networks for Power Flow Analysis","abstract":"This study introduces PINN4PF, an end-to-end deep learning architecture for power flow (PF) analysis that effectively captures the nonlinear dynamics of large-scale modern power systems. The proposed neural network (NN) architecture consists of two important advancements in the training pipeline: (A) a double-head feed-forward NN that aligns with PF analysis, including an activation function that adjusts to the net active and reactive power injections patterns, and (B) a physics-based loss function that partially incorporates power system topology information. The effectiveness of the proposed architecture is illustrated through 4-bus, 15-bus, 290-bus, and 2224-bus test systems and is evaluated against two baselines: a linear regression model (LR) and a black-box NN (MLP). The comparison is based on (i) generalization ability, (ii) robustness, (iii) impact of training dataset size on generalization ability, (iv) accuracy in approximating derived PF quantities (specifically line current, line active power, and line reactive power), and (v) scalability. Results demonstrate that PINN4PF outperforms both baselines across all test systems by up to two orders of magnitude not only in terms of direct criteria, e.g., generalization ability, but also in terms of approximating derived physical quantities.","authors":["Zeynab Kaseb","Stavros Orfanoudakis","Pedro P. Vergara","Peter Palensky"],"url":"https://arxiv.org/abs/2412.02659"}
{"created":"2025-05-06","title":"M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation","abstract":"Multiphysics simulation, which models the interactions between multiple physical processes, and multi-component simulation of complex structures are critical in fields like nuclear and aerospace engineering. Previous studies use numerical solvers or ML-based surrogate models for these simulations. However, multiphysics simulations typically require integrating multiple specialized solvers-each for a specific physical process-into a coupled program, which introduces significant development challenges. Furthermore, existing numerical algorithms struggle with highly complex large-scale structures in multi-component simulations. Here we propose compositional Multiphysics and Multi-component PDE Simulation with Diffusion models (M2PDE) to overcome these challenges. During diffusion-based training, M2PDE learns energy functions modeling the conditional probability of one physical process/component conditioned on other processes/components. In inference, M2PDE generates coupled multiphysics and multi-component solutions by sampling from the joint probability distribution. We evaluate M2PDE on two multiphysics tasks-reaction-diffusion and nuclear thermal coupling-where it achieves more accurate predictions than surrogate models in challenging scenarios. We then apply it to a multi-component prismatic fuel element problem, demonstrating that M2PDE scales from single-component training to a 64-component structure and outperforms existing domain-decomposition and graph-based approaches. The code is available at https://github.com/AI4Science-WestlakeU/M2PDE.","authors":["Tao Zhang","Zhenhai Liu","Feipeng Qi","Yongjun Jiao","Tailin Wu"],"url":"https://arxiv.org/abs/2412.04134"}
{"created":"2025-05-06","title":"Stabilizing and Solving Unique Continuation Problems by Parameterizing Data and Learning Finite Element Solution Operators","abstract":"We consider an inverse problem involving the reconstruction of the solution to a nonlinear partial differential equation (PDE) with unknown boundary conditions. Instead of direct boundary data, we are provided with a large dataset of boundary observations for typical solutions (collective data) and a bulk measurement of a specific realization. To leverage this collective data, we first compress the boundary data using proper orthogonal decomposition (POD) in a linear expansion. Next, we identify a possible nonlinear low-dimensional structure in the expansion coefficients using an autoencoder, which provides a parametrization of the dataset in a lower-dimensional latent space. We then train an operator network to map the expansion coefficients representing the boundary data to the finite element (FE) solution of the PDE. Finally, we connect the autoencoder's decoder to the operator network which enables us to solve the inverse problem by optimizing a data-fitting term over the latent space. We analyze the underlying stabilized finite element method (FEM) in the linear setting and establish an optimal error estimate in the $H^1$-norm. The nonlinear problem is then studied numerically, demonstrating the effectiveness of our approach.","authors":["Erik Burman","Mats G. Larson","Karl Larsson","Carl Lundholm"],"url":"https://arxiv.org/abs/2412.04409"}
{"created":"2025-05-06","title":"Aguvis: Unified Pure Vision Agents for Autonomous GUI Interaction","abstract":"Automating GUI tasks remains challenging due to reliance on textual representations, platform-specific action spaces, and limited reasoning capabilities. We introduce Aguvis, a unified vision-based framework for autonomous GUI agents that directly operates on screen images, standardizes cross-platform interactions and incorporates structured reasoning via inner monologue. To enable this, we construct Aguvis Data Collection, a large-scale dataset with multimodal grounding and reasoning annotations, and develop a two-stage training pipeline that separates GUI grounding from planning and reasoning. Experiments show that Aguvis achieves state-of-the-art performance across offline and real-world online benchmarks, marking the first fully autonomous vision-based GUI agent that operates without closed-source models. We open-source all datasets, models, and training recipes at https://aguvis-project.github.io to advance future research.","authors":["Yiheng Xu","Zekun Wang","Junli Wang","Dunjie Lu","Tianbao Xie","Amrita Saha","Doyen Sahoo","Tao Yu","Caiming Xiong"],"url":"https://arxiv.org/abs/2412.04454"}
{"created":"2025-05-06","title":"KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models","abstract":"Large language models with retrieval-augmented generation encounter a pivotal challenge in intricate retrieval tasks, e.g., multi-hop question answering, which requires the model to navigate across multiple documents and generate comprehensive responses based on fragmented information. To tackle this challenge, we introduce a novel Knowledge Graph-based RAG framework with a hierarchical knowledge retriever, termed KG-Retriever. The retrieval indexing in KG-Retriever is constructed on a hierarchical index graph that consists of a knowledge graph layer and a collaborative document layer. The associative nature of graph structures is fully utilized to strengthen intra-document and inter-document connectivity, thereby fundamentally alleviating the information fragmentation problem and meanwhile improving the retrieval efficiency in cross-document retrieval of LLMs. With the coarse-grained collaborative information from neighboring documents and concise information from the knowledge graph, KG-Retriever achieves marked improvements on five public QA datasets, showing the effectiveness and efficiency of our proposed RAG framework.","authors":["Weijie Chen","Ting Bai","Jinbo Su","Jian Luan","Wei Liu","Chuan Shi"],"url":"https://arxiv.org/abs/2412.05547"}
{"created":"2025-05-06","title":"AC-LIO: Towards Asymptotic and Consistent Convergence in LiDAR-Inertial Odometry","abstract":"Existing LiDAR-Inertial Odometry (LIO) methods typically utilize the prior state trajectory derived from the IMU integration to compensate for the motion distortion within LiDAR frames. However, discrepancies between the prior and actual trajectory can lead to residual distortions that compromise the consistency of the LiDAR frame with its corresponding geometric environment. This imbalance may result in pointcloud registration becoming trapped in local optima, thereby exacerbating drift during long-term and large-scale localization. To address the issue, we propose a novel asymptotically and consistently converging LIO framework dubbed AC-LIO. Our key idea is to back propagate current update term based on the prior state chain, and asymptotically compensate for the residual distortion during iteration. Moreover, considering the weak correlation between previous error and current distortion, we establish convergence criteria based on the pointcloud constraints to regulate the backpropagation. This method of guiding asymptotic distortion compensation using convergence criteria subtly enhances the consistent convergence of pointcloud registration, futher improving the accuracy and robustness of LIO system. Extensive experiments demonstrate that our AC-LIO framework significantly promotes consistent convergence in state estimation compared to prior arts, with about 30.4% reduction in average RMSE over the second best result, leading to marked improvements in the accuracy of long-term and large-scale localization and mapping.","authors":["Tianxiang Zhang","Xuanxuan Zhang","Wenlei Fan","Xin Xia","Huai Yu","Lin Wang","You Li"],"url":"https://arxiv.org/abs/2412.05873"}
{"created":"2025-05-06","title":"Ground Perturbation Detection via Lower-Limb Kinematic States During Locomotion","abstract":"Falls during daily ambulation activities are a leading cause of injury in older adults due to delayed physiological responses to disturbances of balance. Lower-limb exoskeletons have the potential to mitigate fall incidents by detecting and reacting to perturbations before the user. Although commonly used, the standard metric for perturbation detection, whole-body angular momentum, is poorly suited for exoskeleton applications due to computational delays and additional tunings. To address this, we developed a novel ground perturbation detector using lower-limb kinematic states during locomotion. To identify perturbations, we tracked deviations in the kinematic states from their nominal steady-state trajectories. Using a data-driven approach, we further optimized our detector with an open-source ground perturbation biomechanics dataset. A pilot experimental validation with five able-bodied subjects demonstrated that our model distinguished perturbed from unperturbed gait cycles with 98.8% accuracy and only a delay of 23.1% within the gait cycle, outperforming the benchmark by 47.7% in detection accuracy. The results of our study offer exciting promise for our detector and its potential utility to enhance the controllability of robotic assistive exoskeletons.","authors":["Maria T. Tagliaferri","Leonardo Campeggi","Owen N. Beck","Inseung Kang"],"url":"https://arxiv.org/abs/2412.06985"}
{"created":"2025-05-06","title":"GDSG: Graph Diffusion-based Solution Generator for Optimization Problems in MEC Networks","abstract":"Optimization is crucial for MEC networks to function efficiently and reliably, most of which are NP-hard and lack efficient approximation algorithms. This leads to a paucity of optimal solution, constraining the effectiveness of conventional deep learning approaches. Most existing learning-based methods necessitate extensive optimal data and fail to exploit the potential benefits of suboptimal data that can be obtained with greater efficiency and effectiveness. Taking the multi-server multi-user computation offloading (MSCO) problem, which is widely observed in systems like Internet-of-Vehicles (IoV) and Unmanned Aerial Vehicle (UAV) networks, as a concrete scenario, we present a Graph Diffusion-based Solution Generation (GDSG) method. This approach is designed to work with suboptimal datasets while converging to the optimal solution large probably. We transform the optimization issue into distribution-learning and offer a clear explanation of learning from suboptimal training datasets. We build GDSG as a multi-task diffusion model utilizing a Graph Neural Network (GNN) to acquire the distribution of high-quality solutions. We use a simple and efficient heuristic approach to obtain a sufficient amount of training data composed entirely of suboptimal solutions. In our implementation, we enhance the backbone GNN and achieve improved generalization. GDSG also reaches nearly 100\\% task orthogonality, ensuring no interference between the discrete and continuous generation tasks. We further reveal that this orthogonality arises from the diffusion-related training loss, rather than the neural network architecture itself. The experiments demonstrate that GDSG surpasses other benchmark methods on both the optimal and suboptimal training datasets. The MSCO datasets has open-sourced at this http URL, as well as the GDSG algorithm codes at https://github.com/qiyu3816/GDSG.","authors":["Ruihuai Liang","Bo Yang","Pengyu Chen","Xuelin Cao","Zhiwen Yu","M\\'erouane Debbah","Dusit Niyato","H. Vincent Poor","Chau Yuen"],"url":"https://arxiv.org/abs/2412.08296"}
{"created":"2025-05-06","title":"BrushEdit: All-In-One Image Inpainting and Editing","abstract":"Image editing has advanced significantly with the development of diffusion models using both inversion-based and instruction-based methods. However, current inversion-based approaches struggle with big modifications (e.g., adding or removing objects) due to the structured nature of inversion noise, which hinders substantial changes. Meanwhile, instruction-based methods often constrain users to black-box operations, limiting direct interaction for specifying editing regions and intensity. To address these limitations, we propose BrushEdit, a novel inpainting-based instruction-guided image editing paradigm, which leverages multimodal large language models (MLLMs) and image inpainting models to enable autonomous, user-friendly, and interactive free-form instruction editing. Specifically, we devise a system enabling free-form instruction editing by integrating MLLMs and a dual-branch image inpainting model in an agent-cooperative framework to perform editing category classification, main object identification, mask acquisition, and editing area inpainting. Extensive experiments show that our framework effectively combines MLLMs and inpainting models, achieving superior performance across seven metrics including mask region preservation and editing effect coherence.","authors":["Yaowei Li","Yuxuan Bian","Xuan Ju","Zhaoyang Zhang","Junhao Zhuang","Ying Shan","Yuexian Zou","Qiang Xu"],"url":"https://arxiv.org/abs/2412.10316"}
{"created":"2025-05-06","title":"AD-LLM: Benchmarking Large Language Models for Anomaly Detection","abstract":"Anomaly detection (AD) is an important machine learning task with many real-world uses, including fraud detection, medical diagnosis, and industrial monitoring. Within natural language processing (NLP), AD helps detect issues like spam, misinformation, and unusual user activity. Although large language models (LLMs) have had a strong impact on tasks such as text generation and summarization, their potential in AD has not been studied enough. This paper introduces AD-LLM, the first benchmark that evaluates how LLMs can help with NLP anomaly detection. We examine three key tasks: (i) zero-shot detection, using LLMs' pre-trained knowledge to perform AD without tasks-specific training; (ii) data augmentation, generating synthetic data and category descriptions to improve AD models; and (iii) model selection, using LLMs to suggest unsupervised AD models. Through experiments with different datasets, we find that LLMs can work well in zero-shot AD, that carefully designed augmentation methods are useful, and that explaining model selection for specific datasets remains challenging. Based on these results, we outline six future research directions on LLMs for AD.","authors":["Tiankai Yang","Yi Nian","Shawn Li","Ruiyao Xu","Yuangang Li","Jiaqi Li","Zhuo Xiao","Xiyang Hu","Ryan Rossi","Kaize Ding","Xia Hu","Yue Zhao"],"url":"https://arxiv.org/abs/2412.11142"}
{"created":"2025-05-06","title":"Decouple and Decompose: Scaling Resource Allocation with DeDe","abstract":"Resource allocation is fundamental for cloud systems to ensure efficient resource sharing among tenants. However, the scale of such optimization problems has outgrown the capabilities of commercial solvers traditionally employed in production. To scale up resource allocation, prior approaches either tailor solutions to specific problems or rely on assumptions tied to particular workloads. In this work, we revisit real-world resource allocation problems and uncover a common underlying structure: a vast majority of these problems are inherently separable, i.e., they optimize the aggregate utility of individual resource and demand allocations, under separate constraints for each resource and each demand. Building on this insight, we develop DeDe, a general, scalable, and theoretically grounded framework for accelerating resource allocation through a \"decouple and decompose\" approach. DeDe systematically decouples entangled resource and demand constraints, thereby decomposing the overall optimization into alternating per-resource and per-demand allocations, which can then be solved efficiently and in parallel. We have implemented DeDe as a library extension to an open-source solver, maintaining a familiar user interface. Experimental results across three prominent resource allocation tasks -- traffic engineering, cluster scheduling, and load balancing -- demonstrate DeDe's substantial speedups and robust allocation quality.","authors":["Zhiying Xu","Minlan Yu","Francis Y. Yan"],"url":"https://arxiv.org/abs/2412.11447"}
{"created":"2025-05-06","title":"ColorFlow: Retrieval-Augmented Image Sequence Colorization","abstract":"Automatic black-and-white image sequence colorization while preserving character and object identity (ID) is a complex task with significant market demand, such as in cartoon or comic series colorization. Despite advancements in visual colorization using large-scale generative models like diffusion models, challenges with controllability and identity consistency persist, making current solutions unsuitable for industrial application.To address this, we propose ColorFlow, a three-stage diffusion-based framework tailored for image sequence colorization in industrial applications. Unlike existing methods that require per-ID finetuning or explicit ID embedding extraction, we propose a novel robust and generalizable Retrieval Augmented Colorization pipeline for colorizing images with relevant color references. Our pipeline also features a dual-branch design: one branch for color identity extraction and the other for colorization, leveraging the strengths of diffusion models. We utilize the self-attention mechanism in diffusion models for strong in-context learning and color identity matching. To evaluate our model, we introduce ColorFlow-Bench, a comprehensive benchmark for reference-based colorization. Results show that ColorFlow outperforms existing models across multiple metrics, setting a new standard in sequential image colorization and potentially benefiting the art industry. We release our codes and models on our project page: https://zhuang2002.github.io/ColorFlow/.","authors":["Junhao Zhuang","Xuan Ju","Zhaoyang Zhang","Yong Liu","Shiyi Zhang","Chun Yuan","Ying Shan"],"url":"https://arxiv.org/abs/2412.11815"}
{"created":"2025-05-06","title":"Robust Contact-rich Manipulation through Implicit Motor Adaptation","abstract":"Contact-rich manipulation plays an important role in daily human activities. However, uncertain physical parameters often pose significant challenges for both planning and control. A promising strategy is to develop policies that are robust across a wide range of parameters. Domain adaptation and domain randomization are widely used, but they tend to either limit generalization to new instances or perform conservatively due to neglecting instance-specific information. \\textit{Explicit motor adaptation} addresses these issues by estimating system parameters online and then retrieving the parameter-conditioned policy from a parameter-augmented base policy. However, it typically requires precise system identification or additional training of a student policy, both of which are challenging in contact-rich manipulation tasks with diverse physical parameters. In this work, we propose \\textit{implicit motor adaptation}, which enables parameter-conditioned policy retrieval given a roughly estimated parameter distribution instead of a single estimate. We leverage tensor train as an implicit representation of the base policy, facilitating efficient retrieval of the parameter-conditioned policy by exploiting the separable structure of tensor cores. This framework eliminates the need for precise system estimation and policy retraining while preserving optimal behavior and strong generalization. We provide a theoretical analysis to validate the approach, supported by numerical evaluations on three contact-rich manipulation primitives. Both simulation and real-world experiments demonstrate its ability to generate robust policies across diverse instances.","authors":["Teng Xue","Amirreza Razmjoo","Suhan Shetty","Sylvain Calinon"],"url":"https://arxiv.org/abs/2412.11829"}
{"created":"2025-05-06","title":"AoI in Context-Aware Hybrid Radio-Optical IoT Networks","abstract":"With the surge in IoT devices ranging from wearables to smart homes, prompt transmission is crucial. The Age of Information (AoI) emerges as a critical metric in this context, representing the freshness of the information transmitted across the network. This paper studies hybrid IoT networks that employ Optical Communication (OC) as a reinforcement medium to Radio Frequency (RF). We formulate a non-linear convex optimization that adopts a multi-objective optimization strategy to dynamically schedule the communication between devices and select their corresponding communication technology, aiming to balance the maximization of network throughput with the minimization of energy usage and the frequency of switching between technologies. To mitigate the impact of dominant sub-objectives and their scale disparity, the designed approach employs a regularization method that approximates adequate sub-objective scaling weights. Simulation results show that the OC supplementary integration alongside RF enhances the network's overall performances and significantly reduces the Mean AoI and Peak AoI, allowing the collection of the freshest possible data using the best available communication technology.","authors":["Aymen Hamrouni","Sofie Pollin","Hazem Sallouha"],"url":"https://arxiv.org/abs/2412.12914"}
{"created":"2025-05-06","title":"A tensor-train reduced basis solver for parameterized partial differential equations on Cartesian grids","abstract":"In this manuscript, we introduce the tensor-train reduced basis method, a novel projection-based reduced-order model designed for the efficient solution of parameterized partial differential equations. While reduced-order models are widely used for their computational efficiency compared to full-order models, they often involve significant offline computational costs. Our proposed approach mitigates this limitation by leveraging the tensor train format to efficiently represent high-dimensional finite element quantities. This method offers several advantages, including a reduced number of operations for constructing the reduced subspaces, a cost-effective hyper-reduction strategy for assembling the PDE residual and Jacobian, and a lower dimensionality of the projection subspaces for a given accuracy. We provide a posteriori error estimates to validate the accuracy of the method and evaluate its computational performance on benchmark problems, including the Poisson equation, heat equation, and transient linear elasticity in two- and three-dimensional domains. Although the current framework is restricted to problems defined on Cartesian grids, we anticipate that it can be extended to arbitrary shapes by integrating the tensor-train reduced basis method with unfitted finite element techniques.","authors":["Nicholas Mueller","Yiran Zhao","Santiago Badia","Tiangang Cui"],"url":"https://arxiv.org/abs/2412.14460"}
{"created":"2025-05-06","title":"Video Prediction Policy: A Generalist Robot Policy with Predictive Visual Representations","abstract":"Visual representations play a crucial role in developing generalist robotic policies. Previous vision encoders, typically pre-trained with single-image reconstruction or two-image contrastive learning, tend to capture static information, often neglecting the dynamic aspects vital for embodied tasks. Recently, video diffusion models (VDMs) demonstrate the ability to predict future frames and showcase a strong understanding of physical world. We hypothesize that VDMs inherently produce visual representations that encompass both current static information and predicted future dynamics, thereby providing valuable guidance for robot action learning. Based on this hypothesis, we propose the Video Prediction Policy (VPP), which learns implicit inverse dynamics model conditioned on predicted future representations inside VDMs. To predict more precise future, we fine-tune pre-trained video foundation model on robot datasets along with internet human manipulation data. In experiments, VPP achieves a 18.6\\% relative improvement on the Calvin ABC-D generalization benchmark compared to the previous state-of-the-art, and demonstrates a 31.6\\% increase in success rates for complex real-world dexterous manipulation tasks. Project page at https://video-prediction-policy.github.io","authors":["Yucheng Hu","Yanjiang Guo","Pengchao Wang","Xiaoyu Chen","Yen-Jen Wang","Jianke Zhang","Koushil Sreenath","Chaochao Lu","Jianyu Chen"],"url":"https://arxiv.org/abs/2412.14803"}
{"created":"2025-05-06","title":"FolAI: Synchronized Foley Sound Generation with Semantic and Temporal Alignment","abstract":"Traditional sound design workflows rely on manual alignment of audio events to visual cues, as in Foley sound design, where everyday actions like footsteps or object interactions are recreated to match the on-screen motion. This process is time-consuming, difficult to scale, and lacks automation tools that preserve creative intent. Despite recent advances in vision-to-audio generation, producing temporally coherent and semantically controllable sound effects from video remains a major challenge. To address these limitations, we introduce FolAI, a two-stage generative framework that decouples the when and the what of sound synthesis, i.e., the temporal structure extraction and the semantically guided generation, respectively. In the first stage, we estimate a smooth control signal from the video that captures the motion intensity and rhythmic structure over time, serving as a temporal scaffold for the audio. In the second stage, a diffusion-based generative model produces sound effects conditioned both on this temporal envelope and on high-level semantic embeddings, provided by the user, that define the desired auditory content (e.g., material or action type). This modular design enables precise control over both timing and timbre, streamlining repetitive tasks while preserving creative flexibility in professional Foley workflows. Results on diverse visual contexts, such as footstep generation and action-specific sonorization, demonstrate that our model reliably produces audio that is temporally aligned with visual motion, semantically consistent with user intent, and perceptually realistic. These findings highlight the potential of FolAI as a controllable and modular solution for scalable, high-quality Foley sound synthesis in professional and interactive settings. Supplementary materials are accessible on our dedicated demo page at https://ispamm.github.io/FolAI.","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunit\\`a","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"url":"https://arxiv.org/abs/2412.15023"}
{"created":"2025-05-06","title":"A finite strain model for fiber angle plasticity of textile fabrics based on isogeometric shell finite elements","abstract":"This work presents a shear elastoplasticity model for textile fabrics within the theoretical framework of anisotropic Kirchhoff-Love shells with bending of embedded fibers proposed by Duong et al. (2023). The plasticity model aims at capturing the rotational inter-ply frictional sliding between fiber families in textile composites undergoing large deformation. Such effects are usually dominant in dry textile fabrics such as woven and non-crimp fabrics. The model explicitly uses relative angles between fiber families as strain measures for the kinematics. The plasticity model is formulated directly with surface invariants without resorting to thickness integration. Motivated by experimental observations from the picture frame test, a yield function is proposed with isotropic hardening and a simple evolution equation. A classical return mapping algorithm is employed to solve the elastoplastic problem within the isogeometric finite shell element formulation of Duong et al. (2022). The verification of the implementation is facilitated by the analytical solution for the picture frame test. The proposed plasticity model is calibrated from the picture frame test and is then validated by the bias extension test, considering available experimental data for different samples from the literature. Good agreement between model prediction and experimental data is obtained. Finally, the applicability of the elastoplasticity model to 3D shell problems is demonstrated.","authors":["Thang Xuan Duong","Roger Andrew Sauer"],"url":"https://arxiv.org/abs/2412.20131"}
{"created":"2025-05-06","title":"ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis","abstract":"Bidirectional transformers excel at sentiment analysis, and Large Language Models (LLM) are effective zero-shot learners. Might they perform better as a team? This paper explores collaborative approaches between ELECTRA and GPT-4o for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as: predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT predictions with GPT-4o-mini significantly improved performance over either model alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and yielded the lowest cost/performance ratio (\\$0.12/F1 point). However, when GPT models were fine-tuned, including predictions decreased performance. GPT-4o FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at much less cost (\\$0.38 vs. \\$1.59/F1 point). Our results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost. Both are affordable options for projects with limited resources.","authors":["James P. Beno"],"url":"https://arxiv.org/abs/2501.00062"}
{"created":"2025-05-06","title":"LUSIFER: Language Universal Space Integration for Enhanced Multilingual Embeddings with Large Language Models","abstract":"Recent advancements in large language models (LLMs) based embedding models have established new state-of-the-art benchmarks for text embedding tasks, particularly in dense vector-based retrieval. However, these models predominantly focus on English, leaving multilingual embedding capabilities largely unexplored. To address this limitation, we present LUSIFER, a novel zero-shot approach that adapts LLM-based embedding models for multilingual tasks without requiring multilingual supervision. LUSIFER's architecture combines a multilingual encoder, serving as a language-universal learner, with an LLM-based embedding model optimized for embedding-specific tasks. These components are seamlessly integrated through a minimal set of trainable parameters that act as a connector, effectively transferring the multilingual encoder's language understanding capabilities to the specialized embedding model. Additionally, to comprehensively evaluate multilingual embedding performance, we introduce a new benchmark encompassing 5 primary embedding tasks, 123 diverse datasets, and coverage across 14 languages. Extensive experimental results demonstrate that LUSIFER significantly enhances the multilingual performance across various embedding tasks, particularly for medium and low-resource languages, without requiring explicit multilingual training data.","authors":["Hieu Man","Nghia Trung Ngo","Viet Dac Lai","Ryan A. Rossi","Franck Dernoncourt","Thien Huu Nguyen"],"url":"https://arxiv.org/abs/2501.00874"}
{"created":"2025-05-06","title":"Defense Strategies for Autonomous Multi-agent Systems: Ensuring Safety and Resilience Under Exponentially Unbounded FDI Attacks","abstract":"False data injection attacks pose a significant threat to autonomous multi-agent systems (MASs). Existing attack-resilient control strategies generally have strict assumptions on the attack signals and overlook safety constraints, such as collision avoidance. In practical applications, leader agents equipped with advanced sensors or weaponry span a safe region to guide heterogeneous follower agents, ensuring coordinated operations while addressing collision avoidance to prevent financial losses and mission failures. This letter addresses these gaps by introducing and solving the safety-aware and attack-resilient (SAAR) control problem under exponentially unbounded false data injection (EU-FDI) attacks. Specifically, a novel attack-resilient observer layer (OL) is first designed to defend against EU-FDI attacks on the OL. Then, an attack-resilient compensational signal is designed to mitigate the adverse effects caused by the EU-FDI attack on control input layer (CIL). Finally, a SAAR controller is designed by solving a quadratic programming (QP) problem integrating control barrier function (CBF) certified collision-free safety constraints. Rigorous Lyapunov-based stability analysis certifies the SAAR controller's effectiveness in ensuring both safety and resilience. This study also pioneers a three-dimensional (3D) simulation of the SAAR containment control problem for heterogeneous MASs, demonstrating its applicability in realistic multi-agent scenarios.","authors":["Yichao Wang","Mohamadamin Rajabinezhad","Dimitra Panagou","Shan Zuo"],"url":"https://arxiv.org/abs/2501.00973"}
{"created":"2025-05-06","title":"CryptoMamba: Leveraging State Space Models for Accurate Bitcoin Price Prediction","abstract":"Predicting Bitcoin price remains a challenging problem due to the high volatility and complex non-linear dynamics of cryptocurrency markets. Traditional time-series models, such as ARIMA and GARCH, and recurrent neural networks, like LSTMs, have been widely applied to this task but struggle to capture the regime shifts and long-range dependencies inherent in the data. In this work, we propose CryptoMamba, a novel Mamba-based State Space Model (SSM) architecture designed to effectively capture long-range dependencies in financial time-series data. Our experiments show that CryptoMamba not only provides more accurate predictions but also offers enhanced generalizability across different market conditions, surpassing the limitations of previous models. Coupled with trading algorithms for real-world scenarios, CryptoMamba demonstrates its practical utility by translating accurate forecasts into financial outcomes. Our findings signal a huge advantage for SSMs in stock and cryptocurrency price forecasting tasks.","authors":["Mohammad Shahab Sepehri","Asal Mehradfar","Mahdi Soltanolkotabi","Salman Avestimehr"],"url":"https://arxiv.org/abs/2501.01010"}
{"created":"2025-05-06","title":"Towards the Anonymization of the Language Modeling","abstract":"Rapid advances in Natural Language Processing (NLP) have revolutionized many fields, including healthcare. However, these advances raise significant privacy concerns, especially when pre-trained models fine-tuned and specialized on sensitive data can memorize and then expose and regurgitate personal information. This paper presents a privacy-preserving language modeling approach to address the problem of language models anonymization, and thus promote their sharing. Specifically, we propose both a Masking Language Modeling (MLM) methodology to specialize a BERT-like language model, and a Causal Language Modeling (CLM) methodology to specialize a GPT-like model that avoids the model from memorizing direct and indirect identifying information present in the training data. We have comprehensively evaluated our approaches using a medical dataset and compared them against different baselines. Our results indicate that by avoiding memorizing both direct and indirect identifiers during model specialization, our masking and causal language modeling schemes offer a good tradeoff for maintaining high privacy while retaining high utility.","authors":["Antoine Boutet","Lucas Magnana","Juliette S\\'en\\'echal","Helain Zimmermann"],"url":"https://arxiv.org/abs/2501.02407"}
{"created":"2025-05-06","title":"Truthful mechanisms for linear bandit games with private contexts","abstract":"The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes. However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system. For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments. We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents. We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\\epsilon$-greedy algorithm incur sublinear but high regret. We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\\ln T)$ frequentist regret. Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families.","authors":["Yiting Hu","Lingjie Duan"],"url":"https://arxiv.org/abs/2501.03865"}
{"created":"2025-05-06","title":"Developing a Foundation of Vector Symbolic Architectures Using Category Theory","abstract":"Connectionist approaches to machine learning, \\emph{i.e.} neural networks, are enjoying a considerable vogue right now. However, these methods require large volumes of data and produce models that are uninterpretable to humans. An alternative framework that is compatible with neural networks and gradient-based learning, but explicitly models compositionality, is Vector Symbolic Architectures (VSAs). VSAs are a family of algebras on high-dimensional vector representations. They arose in cognitive science from the need to unify neural processing and the kind of symbolic reasoning that humans perform. While machine learning methods have benefited from category-theoretical analyses, VSAs have not yet received similar treatment. In this paper, we present a first attempt at applying category theory to VSAs. Specifically, We generalise from vectors to co-presheaves, and describe VSA operations as the right Kan extensions of the external tensor product. This formalisation involves a proof that the right Kan extension in such cases can be expressed as simple, element-wise operations. We validate our formalisation with worked examples that connect to current VSA implementations, while suggesting new possible designs for VSAs.","authors":["Nolan P Shaw","P Michael Furlong","Britt Anderson","Jeff Orchard"],"url":"https://arxiv.org/abs/2501.05368"}
{"created":"2025-05-06","title":"RoboPanoptes: The All-seeing Robot with Whole-body Dexterity","abstract":"We present RoboPanoptes, a capable yet practical robot system that achieves whole-body dexterity through whole-body vision. Its whole-body dexterity allows the robot to utilize its entire body surface for manipulation, such as leveraging multiple contact points or navigating constrained spaces. Meanwhile, whole-body vision uses a camera system distributed over the robot's surface to provide comprehensive, multi-perspective visual feedback of its own and the environment's state. At its core, RoboPanoptes uses a whole-body visuomotor policy that learns complex manipulation skills directly from human demonstrations, efficiently aggregating information from the distributed cameras while maintaining resilience to sensor failures. Together, these design aspects unlock new capabilities and tasks, allowing RoboPanoptes to unbox in narrow spaces, sweep multiple or oversized objects, and succeed in multi-step stowing in cluttered environments, outperforming baselines in adaptability and efficiency. Results are best viewed on https://robopanoptes.github.io.","authors":["Xiaomeng Xu","Dominik Bauer","Shuran Song"],"url":"https://arxiv.org/abs/2501.05420"}
{"created":"2025-05-06","title":"landmarker: a Toolkit for Anatomical Landmark Localization in 2D/3D Images","abstract":"Anatomical landmark localization in 2D/3D images is a critical task in medical imaging. Although many general-purpose tools exist for landmark localization in classical computer vision tasks, such as pose estimation, they lack the specialized features and modularity necessary for anatomical landmark localization applications in the medical domain. Therefore, we introduce landmarker, a Python package built on PyTorch. The package provides a comprehensive, flexible toolkit for developing and evaluating landmark localization algorithms, supporting a range of methodologies, including static and adaptive heatmap regression. landmarker enhances the accuracy of landmark identification, streamlines research and development processes, and supports various image formats and preprocessing pipelines. Its modular design allows users to customize and extend the toolkit for specific datasets and applications, accelerating innovation in medical imaging. landmarker addresses a critical need for precision and customization in landmark localization tasks not adequately met by existing general-purpose pose estimation tools.","authors":["Jef Jonkers","Luc Duchateau","Glenn Van Wallendael","Sofie Van Hoecke"],"url":"https://arxiv.org/abs/2501.10098"}
{"created":"2025-05-06","title":"DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference","abstract":"Mixture-of-Experts (MoE) models, though highly effective for various machine learning tasks, face significant deployment challenges on memory-constrained devices. While GPUs offer fast inference, their limited memory compared to CPUs means not all experts can be stored on the GPU simultaneously, necessitating frequent, costly data transfers from CPU memory, often negating GPU speed advantages. To address this, we present DAOP, an on-device MoE inference engine to optimize parallel GPU-CPU execution. DAOP dynamically allocates experts between CPU and GPU based on per-sequence activation patterns, and selectively pre-calculates predicted experts on CPUs to minimize transfer latency. This approach enables efficient resource utilization across various expert cache ratios while maintaining model accuracy through a novel graceful degradation mechanism. Comprehensive evaluations across various datasets show that DAOP outperforms traditional expert caching and prefetching methods by up to 8.20x and offloading techniques by 1.35x while maintaining accuracy.","authors":["Yujie Zhang","Shivam Aggarwal","Tulika Mitra"],"url":"https://arxiv.org/abs/2501.10375"}
{"created":"2025-05-06","title":"SMARTe-VR: Student Monitoring and Adaptive Response Technology for e-Learning in Virtual Reality","abstract":"This work introduces SMARTe-VR, a platform for student monitoring in an immersive virtual reality environment designed for online education. SMARTe-VR aims to collect data for adaptive learning, focusing on facial biometrics and learning metadata. The platform allows instructors to create customized learning sessions with video lectures, featuring an interface with an AutoQA system to evaluate understanding, interaction tools (e.g., textbook highlighting and lecture tagging), and real-time feedback. Furthermore, we released a dataset that contains 5 research challenges with data from 10 users in VR-based TOEIC sessions. This data set, which spans more than 25 hours, includes facial features, learning metadata, 450 responses, difficulty levels of the questions, concept tags, and understanding labels. Alongside the database, we present preliminary experiments using Item Response Theory models, adapted for understanding detection using facial features. Two architectures were explored: a Temporal Convolutional Network for local features and a Multilayer Perceptron for global features.","authors":["Roberto Daza","Lin Shengkai","Aythami Morales","Julian Fierrez","Katashi Nagao"],"url":"https://arxiv.org/abs/2501.10977"}
{"created":"2025-05-06","title":"TigerVector: Supporting Vector Search in Graph Databases for Advanced RAGs","abstract":"In this paper, we introduce TigerVector, a system that integrates vector search and graph query within TigerGraph, a Massively Parallel Processing (MPP) native graph database. We extend the vertex attribute type with the embedding type. To support fast vector search, we devise an MPP index framework that interoperates efficiently with the graph engine. The graph query language GSQL is enhanced to support vector type expressions and enable query compositions between vector search results and graph query blocks. These advancements elevate the expressive power and analytical capabilities of graph databases, enabling seamless fusion of unstructured and structured data in ways previously unattainable. Through extensive experiments, we demonstrate TigerVector's hybrid search capability, scalability, and superior performance compared to other graph databases (including Neo4j and Amazon Neptune) and a highly optimized specialized vector database (Milvus). TigerVector was integrated into TigerGraph v4.2, the latest release of TigerGraph, in December 2024.","authors":["Shige Liu","Zhifang Zeng","Li Chen","Adil Ainihaer","Arun Ramasami","Songting Chen","Yu Xu","Mingxi Wu","Jianguo Wang"],"url":"https://arxiv.org/abs/2501.11216"}
{"created":"2025-05-06","title":"Dynamic Scene Understanding from Vision-Language Representations","abstract":"Images depicting complex, dynamic scenes are challenging to parse automatically, requiring both high-level comprehension of the overall situation and fine-grained identification of participating entities and their interactions. Current approaches use distinct methods tailored to sub-tasks such as Situation Recognition and detection of Human-Human and Human-Object Interactions. However, recent advances in image understanding have often leveraged web-scale vision-language (V&amp;L) representations to obviate task-specific engineering. In this work, we propose a framework for dynamic scene understanding tasks by leveraging knowledge from modern, frozen V&amp;L representations. By framing these tasks in a generic manner - as predicting and parsing structured text, or by directly concatenating representations to the input of existing models - we achieve state-of-the-art results while using a minimal number of trainable parameters relative to existing approaches. Moreover, our analysis of dynamic knowledge of these representations shows that recent, more powerful representations effectively encode dynamic scene semantics, making this approach newly possible.","authors":["Shahaf Pruss","Morris Alper","Hadar Averbuch-Elor"],"url":"https://arxiv.org/abs/2501.11653"}
{"created":"2025-05-06","title":"Deflation-based certified greedy algorithm and adaptivity for bifurcating nonlinear PDEs","abstract":"This work deals with tailored reduced order models for bifurcating nonlinear parametric partial differential equations, where multiple coexisting solutions arise for a given parametric instance. Approaches based on proper orthogonal decomposition have been widely investigated in the literature, but they usually rely on some \\emph{a-priori} knowledge about the bifurcating model and lack any error estimation. On the other hand, standard certified reduced basis techniques fail to represent correctly the branching behavior, since the error estimator is no longer reliable. The main goal of the contribution is to overcome these limitations by introducing two novel algorithms: (i) the adaptive-greedy, detecting the bifurcation point starting from scarce information over the parametric space, and (ii) the deflated-greedy, certifying multiple coexisting branches simultaneously. The former approach takes advantage of the features of the reduced manifold to detect the bifurcation, while the latter exploits the deflation and continuation methods to discover the bifurcating solutions and enrich the reduced space. We test the two strategies for the Coanda effect held by the Navier-Stokes equations in a sudden-expansion channel. The accuracy of the approach and the error certification are compared with vanilla-greedy and proper orthogonal decomposition.","authors":["Federico Pichi","Maria Strazzullo"],"url":"https://arxiv.org/abs/2501.12361"}
{"created":"2025-05-06","title":"Explicit Construction of Quantum Quasi-Cyclic Low-Density Parity-Check Codes with Column Weight 2 and Girth 12","abstract":"This study proposes an explicit construction method for quantum quasi-cyclic low-density parity-check (QC-LDPC) codes with a girth of 12. The proposed method designs parity-check matrices that maximize the girth while maintaining an orthogonal structure suitable for quantum error correction. By utilizing algebraic techniques, short cycles are eliminated, which improves error correction performance. Additionally, this method is extended to non-binary LDPC codes and spatially-coupled LDPC codes, demonstrating that both the girth and orthogonality can be preserved. The results of this study enable the design of high-performance quantum error-correcting codes without the need for random search.","authors":["Daiki Komoto","Kenta Kasai"],"url":"https://arxiv.org/abs/2501.13444"}
{"created":"2025-05-06","title":"A Cognitive Paradigm Approach to Probe the Perception-Reasoning Interface in VLMs","abstract":"A fundamental challenge in artificial intelligence involves understanding the cognitive mechanisms underlying visual reasoning in sophisticated models like Vision-Language Models (VLMs). How do these models integrate visual perception with abstract thought, especially when reasoning across multiple images or requiring fine-grained compositional understanding? Drawing inspiration from cognitive science, this paper introduces a structured evaluation framework using diverse visual reasoning tasks-Bongard Problems (BPs) and Winoground-to dissect the perception-reasoning interface in VLMs. We propose three distinct evaluation paradigms, mirroring human problem-solving strategies: Direct Visual Rule Learning (DVRL; holistic processing), Deductive Rule Learning (DRL; rule extraction and application), and Componential Analysis (CA; analytical decomposition via task-agnostic textual descriptions). These paradigms systematically vary cognitive load and probe processing stages. Notably, CA enables multi-image reasoning evaluation even for single-image architectures and isolates reasoning from perception by operating on textual descriptions. Applying this framework, we demonstrate that CA, leveraging powerful language models for reasoning over rich, independently generated descriptions, achieves new state-of-the-art (SOTA) performance on challenging benchmarks including Bongard-OpenWorld, Bongard-HOI, and Winoground. Ablation studies confirm reasoning improves significantly when perceptual challenges are mitigated, revealing a critical perception bottleneck. Our framework provides a valuable diagnostic tool and suggests that decoupling perception (via rich, task-agnostic description) from reasoning is a promising direction for robust and general visual intelligence.","authors":["Mohit Vaishnav","Tanel Tammet"],"url":"https://arxiv.org/abs/2501.13620"}
{"created":"2025-05-06","title":"Flexible 3D Cage-based Deformation via Green Coordinates on B\\'{e}zier Patches","abstract":"Cage-based deformation is a fundamental problem in geometry processing, where a cage, a user-specified boundary of a region, is used to deform the ambient space of a given mesh. Traditional 3D cages are typically composed of triangles and quads. While quads can represent non-planar regions when their four corners are not coplanar, they form ruled surfaces with straight isoparametric curves, which limits their ability to handle curved and high-curvature deformations. In this work, we extend the cage for curved boundaries using B\\'{e}zier patches, enabling flexible and high-curvature deformations with only a few control points. The higher-order structure of the B\\'{e}zier patch also allows for the creation of a more compact and precise curved cage for the input model. Based on Green's third identity, we derive the Green coordinates for the B\\'{e}zier cage, achieving shape-preserving deformation with smooth surface boundaries. These coordinates are defined based on the vertex positions and normals of the B\\'{e}zier control net. Given that the coordinates are approximately calculated through the Riemann summation, we propose a global projection technique to ensure that the coordinates accurately conform to the linear reproduction property. Experimental results show that our method achieves high performance in handling curved and high-curvature deformations.","authors":["Dong Xiao","Renjie Chen"],"url":"https://arxiv.org/abs/2501.14068"}
{"created":"2025-05-06","title":"I Know What You Did Last Summer: Identifying VR User Activity Through VR Network Traffic","abstract":"Virtual Reality (VR) technology has gained substantial traction and has the potential to transform a number of industries, including education, entertainment, and professional sectors. Nevertheless, concerns have arisen about the security and privacy implications of VR applications and the impact that they might have on users. In this paper, we investigate the following overarching research question: can VR applications and VR user activities in the context of such applications (e.g., manipulating virtual objects, walking, talking, flying) be identified based on the (potentially encrypted) network traffic that is generated by VR headsets during the operation of VR applications? To answer this question, we collect network traffic data from 25 VR applications running on the Meta Quest Pro headset and identify characteristics of the generated network traffic, which we subsequently use to train off-the-shelf Machine Learning (ML) models. Our results indicate that through the use of ML models, we can identify the VR applications being used with an accuracy of 92.4% and the VR user activities performed with an accuracy of 91%. Furthermore, our results demonstrate that an attacker does not need to collect large amounts of network traffic data for each VR application to carry out such an attack. Specifically, an attacker only needs to collect less than 10 minutes of network traffic data for each VR application in order to identify applications with an accuracy higher than 90% and VR user activities with an accuracy higher than 88%.","authors":["Sheikh Samit Muhaimin","Spyridon Mastorakis"],"url":"https://arxiv.org/abs/2501.15313"}
{"created":"2025-05-06","title":"Rethinking the Bias of Foundation Model under Long-tailed Distribution","abstract":"Long-tailed learning has garnered increasing attention due to its practical significance. Among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. However, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. In this paper, we examine how such imbalances from pre-training affect long-tailed downstream tasks. Specifically, we find the imbalance biases inherited in foundation models on downstream task as parameter imbalance and data imbalance. During fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. Moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. To tackle both imbalances simultaneously, we build our method on causal learning and view the incomplete semantic factor as the confounder, which brings spurious correlations between input samples and labels. To resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data. Notably, we achieve an average performance increase of about $1.67\\%$ on each dataset.","authors":["Jiahao Chen","Bin Qin","Jiangmeng Li","Hao Chen","Bing Su"],"url":"https://arxiv.org/abs/2501.15955"}
{"created":"2025-05-06","title":"Symbolic Mathematical Computation 1965--1975: The View from a Half-Century Perspective","abstract":"The 2025 ISSAC conference in Guanajuato, Mexico, marks the 50th event in this significant series, making it an ideal moment to reflect on the field's history. This paper reviews the formative years of symbolic computation up to 1975, fifty years ago. By revisiting a period unfamiliar to most current participants, this survey aims to shed light on once-pressing issues that are now largely resolved and to highlight how some of today's challenges were recognized earlier than expected.","authors":["Robert M. Corless","Arthur C. Norman","Tomas Recio","William J. Turkel","Stephen M. Watt"],"url":"https://arxiv.org/abs/2501.16457"}
{"created":"2025-05-06","title":"Prompt-Based Cost-Effective Evaluation and Operation of ChatGPT as a Computer Programming Teaching Assistant","abstract":"The dream of achieving a student-teacher ratio of 1:1 is closer than ever thanks to the emergence of large language models (LLMs). One potential application of these models in the educational field would be to provide feedback to students in university introductory programming courses, so that a student struggling to solve a basic implementation problem could seek help from an LLM available 24/7. This article focuses on studying three aspects related to such an application. First, the performance of two well-known models, GPT-3.5T and GPT-4T, in providing feedback to students is evaluated. The empirical results showed that GPT-4T performs much better than GPT-3.5T, however, it is not yet ready for use in a real-world scenario. This is due to the possibility of generating incorrect information that potential users may not always be able to detect. Second, the article proposes a carefully designed prompt using in-context learning techniques that allows automating important parts of the evaluation process, as well as providing a lower bound for the fraction of feedbacks containing incorrect information, saving time and effort. This was possible because the resulting feedback has a programmatically analyzable structure that incorporates diagnostic information about the LLM's performance in solving the requested task. Third, the article also suggests a possible strategy for implementing a practical learning tool based on LLMs, which is rooted on the proposed prompting techniques. This strategy opens up a whole range of interesting possibilities from a pedagogical perspective.","authors":["Marc Ballestero-Rib\\'o","Daniel Ortiz-Mart\\'inez"],"url":"https://arxiv.org/abs/2501.17176"}
{"created":"2025-05-06","title":"Private Information Retrieval on Multigraph-Based Replicated Storage","abstract":"We consider the private information retrieval (PIR) problem for a multigraph-based replication system, where each set of $r$ files is stored on two of the servers according to an underlying $r$-multigraph. Our goal is to establish upper and lower bounds on the PIR capacity of the $r$-multigraph. Specifically, we first propose a construction for multigraph-based PIR systems that leverages the symmetry of the underlying graph-based PIR scheme, deriving a capacity lower bound for such multigraphs. Then, we establish a general upper bound using linear programming, expressed as a function of the underlying graph parameters. Our bounds are demonstrated to be tight for PIR systems on multipaths for even number of vertices.","authors":["Shreya Meel","Xiangliang Kong","Thomas Jacob Maranzatto","Itzhak Tamo","Sennur Ulukus"],"url":"https://arxiv.org/abs/2501.17845"}
{"created":"2025-05-06","title":"Optimal sensor placement under model uncertainty in the weak-constraint 4D-Var framework","abstract":"In data assimilation, the model may be subject to uncertainties and errors. The weak-constraint data assimilation framework enables incorporating model uncertainty in the dynamics of the governing equations. We propose a new framework for near-optimal sensor placement in the weak-constrained setting. This is achieved by first deriving a design criterion based on the expected information gain, which involves the Kullback-Leibler divergence from the forecast prior to the posterior distribution. An explicit formula for this criterion is provided, assuming that the model error and background are independent and Gaussian and the dynamics are linear. We discuss algorithmic approaches to efficiently evaluate this criterion through randomized approximations. To provide further insight and flexibility in computations, we also provide alternative expressions for the criteria. We provide an algorithm to find near-optimal experimental designs using column subset selection, including a randomized algorithm that avoids computing the adjoint of the forward operator. Through numerical experiments in one and two spatial dimensions, we show the effectiveness of our proposed methods.","authors":["Alen Alexanderian","Hugo D\\'iaz","Vishwas Rao","Arvind K. Saibaba"],"url":"https://arxiv.org/abs/2502.00150"}
{"created":"2025-05-06","title":"PAC Learning is just Bipartite Matching (Sort of)","abstract":"The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning.","authors":["Shaddin Dughmi"],"url":"https://arxiv.org/abs/2502.00607"}
{"created":"2025-05-06","title":"CoDe: Blockwise Control for Denoising Diffusion Models","abstract":"Aligning diffusion models to downstream tasks often requires finetuning new models or gradient-based guidance at inference time to enable sampling from the reward-tilted posterior. In this work, we explore a simple inference-time gradient-free guidance approach, called controlled denoising (CoDe), that circumvents the need for differentiable guidance functions and model finetuning. CoDe is a blockwise sampling method applied during intermediate denoising steps, allowing for alignment with downstream rewards. Our experiments demonstrate that, despite its simplicity, CoDe offers a favorable trade-off between reward alignment, prompt instruction following, and inference cost, achieving a competitive performance against the state-of-the-art baselines. Our code is available at: https://github.com/anujinho/code.","authors":["Anuj Singh","Sayak Mukherjee","Ahmad Beirami","Hadi Jamali-Rad"],"url":"https://arxiv.org/abs/2502.00968"}
{"created":"2025-05-06","title":"Improving Efficiency in Near-State and State-Optimal Self-Stabilising Leader Election Population Protocols","abstract":"We investigate leader election problem via ranking within self-stabilising population protocols. In this scenario, the agent's state space comprises $n$ rank states and $x$ extra states. The initial configuration of $n$ agents consists of arbitrary arrangements of rank and extra states, with the objective of self-ranking. Specifically, each agent is tasked with stabilising in a unique rank state silently, implying that after stabilisation, each agent remains in its designated state indefinitely.","authors":["Leszek G\\k{a}sieniec","Tytus Grodzicki","Grzegorz Stachowiak"],"url":"https://arxiv.org/abs/2502.01227"}
{"created":"2025-05-06","title":"Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding","abstract":"Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM.","authors":["Mingyu Jin","Kai Mei","Wujiang Xu","Mingjie Sun","Ruixiang Tang","Mengnan Du","Zirui Liu","Yongfeng Zhang"],"url":"https://arxiv.org/abs/2502.01563"}
{"created":"2025-05-06","title":"DAGNet: A Dual-View Attention-Guided Network for Efficient X-ray Security Inspection","abstract":"With the rapid development of modern transportation systems and the exponential growth of logistics volumes, intelligent X-ray-based security inspection systems play a crucial role in public safety. Although single-view X-ray baggage scanner is widely deployed, they struggles to accurately identify contraband in complex stacking scenarios due to strong viewpoint dependency and inadequate feature representation. To address this, we propose a Dual-View Attention-Guided Network for Efficient X-ray Security Inspection (DAGNet). This study builds on a shared-weight backbone network as the foundation and constructs three key modules that work together: (1) Frequency Domain Interaction Module (FDIM) dynamically enhances features by adjusting frequency components based on inter-view relationships; (2) Dual-View Hierarchical Enhancement Module (DVHEM) employs cross-attention to align features between views and capture hierarchical associations; (3) Convolutional Guided Fusion Module (CGFM) fuses features to suppress redundancy while retaining critical discriminative information. Collectively, these modules substantially improve the performance of dual-view X-ray security inspection. Experimental results demonstrate that DAGNet outperforms existing state-of-the-art approaches across multiple backbone architectures. The code is available at:https://github.com/ShilongHong/DAGNet.","authors":["Shilong Hong","Yanzhou Zhou","Weichao Xu"],"url":"https://arxiv.org/abs/2502.01710"}
{"created":"2025-05-06","title":"Auditing a Dutch Public Sector Risk Profiling Algorithm Using an Unsupervised Bias Detection Tool","abstract":"Algorithms are increasingly used to automate or aid human decisions, yet recent research shows that these algorithms may exhibit bias across legally protected demographic groups. However, data on these groups may be unavailable to organizations or external auditors due to privacy legislation. This paper studies bias detection using an unsupervised clustering tool when data on demographic groups are unavailable. We collaborate with the Dutch Executive Agency for Education to audit an algorithm that was used to assign risk scores to college students at the national level in the Netherlands between 2012-2023. Our audit covers more than 250,000 students from the whole country. The unsupervised clustering tool highlights known disparities between students with a non-European migration background and Dutch origin. Our contributions are three-fold: (1) we assess bias in a real-world, large-scale and high-stakes decision-making process by a governmental organization; (2) we use simulation studies to highlight potential pitfalls of using the unsupervised clustering tool to detect true bias when demographic group data are unavailable and provide recommendations for valid inferences; (3) we provide the unsupervised clustering tool in an open-source library. Our work serves as a starting point for a deliberative assessment by human experts to evaluate potential discrimination in algorithmic-supported decision-making processes.","authors":["Floris Holstege","Mackenzie Jorgensen","Kirtan Padh","Jurriaan Parie","Joel Persson","Krsto Prorokovic","Lukas Snoek"],"url":"https://arxiv.org/abs/2502.01713"}
{"created":"2025-05-06","title":"SE Arena: An Interactive Platform for Evaluating Foundation Models in Software Engineering","abstract":"Foundation models (FMs), particularly large language models (LLMs), have shown significant promise in various software engineering (SE) tasks, including code generation, debugging, and requirement refinement. Despite these advances, existing evaluation frameworks are insufficient for assessing model performance in iterative, context-rich workflows characteristic of SE activities. To address this limitation, we introduce SE Arena, an interactive platform designed to evaluate FMs in SE tasks. SE Arena provides a transparent, open-source leaderboard, supports multi-round conversational workflows, and enables end-to-end model comparisons. The platform introduces novel metrics, including model consistency score that measures the consistency of model outputs through self-play matches, and conversation efficiency index that evaluates model performance while accounting for the number of interaction rounds required to reach conclusions. Moreover, SE Arena incorporates a new feature called RepoChat, which automatically injects repository-related context (e.g., issues, commits, pull requests) into the conversation, further aligning evaluations with real-world development processes. This paper outlines the design and capabilities of SE Arena, emphasizing its potential to advance the evaluation and practical application of FMs in software engineering.","authors":["Zhimin Zhao"],"url":"https://arxiv.org/abs/2502.01860"}
{"created":"2025-05-06","title":"Extending Asynchronous Byzantine Agreement with Crusader Agreement","abstract":"In this work, we study multivalued byzantine agreement (BA) in an asynchronous network of $n$ parties where up to $t < \\frac{n}{3}$ parties are byzantine. We present a new reduction from multivalued BA to binary BA. It allows one to achieve BA on $\\ell$-bit inputs with one instance of binary BA, one instance of crusader agreement (CA) on $\\ell$-bit inputs and $\\Theta(\\ell n + n^2)$ bits of additional communication.","authors":["Mose Mizrahi Erbes","Roger Wattenhofer"],"url":"https://arxiv.org/abs/2502.02320"}
{"created":"2025-05-06","title":"Global Contact-Rich Planning with Sparsity-Rich Semidefinite Relaxations","abstract":"We show that contact-rich motion planning is also sparsity-rich when viewed as polynomial optimization (POP). We can exploit not only the correlative and term sparsity patterns that are general to all POPs, but also specialized sparsity patterns from the robot kinematic structure and the separability of contact modes. Such sparsity enables the design of high-order but sparse semidefinite programming (SDPs) relaxations--building upon Lasserre's moment and sums of squares hierarchy--that (i) can be solved in seconds by off-the-shelf SDP solvers, and (ii) compute near globally optimal solutions to the nonconvex contact-rich planning problems with small certified suboptimality. Through extensive experiments both in simulation (Push Bot, Push Box, Push Box with Obstacles, and Planar Hand) and real world (Push T), we demonstrate the power of using convex SDP relaxations to generate global contact-rich motion plans. As a contribution of independent interest, we release the Sparse Polynomial Optimization Toolbox (SPOT)--implemented in C++ with interfaces to both Python and Matlab--that automates sparsity exploitation for robotics and beyond.","authors":["Shucheng Kang","Guorui Liu","Heng Yang"],"url":"https://arxiv.org/abs/2502.02829"}
{"created":"2025-05-06","title":"How do Humans and Language Models Reason About Creativity? A Comparative Analysis","abstract":"Creativity assessment in science and engineering is increasingly based on both human and AI judgment, but the cognitive processes and biases behind these evaluations remain poorly understood. We conducted two experiments examining how including example solutions with ratings impact creativity evaluation, using a finegrained annotation protocol where raters were tasked with explaining their originality scores and rating for the facets of remoteness (whether the response is \"far\" from everyday ideas), uncommonness (whether the response is rare), and cleverness. In Study 1, we analyzed creativity ratings from 72 experts with formal science or engineering training, comparing those who received example solutions with ratings (example) to those who did not (no example). Computational text analysis revealed that, compared to experts with examples, no-example experts used more comparative language (e.g., \"better/worse\") and emphasized solution uncommonness, suggesting they may have relied more on memory retrieval for comparisons. In Study 2, parallel analyses with state-of-the-art LLMs revealed that models prioritized uncommonness and remoteness of ideas when rating originality, suggesting an evaluative process rooted around the semantic similarity of ideas. In the example condition, while LLM accuracy in predicting the true originality scores improved, the correlations of remoteness, uncommonness, and cleverness with originality also increased substantially -- to upwards of $0.99$ -- suggesting a homogenization in the LLMs evaluation of the individual facets. These findings highlight important implications for how humans and AI reason about creativity and suggest diverging preferences for what different populations prioritize when rating.","authors":["Antonio Laverghetta Jr.","Tuhin Chakrabarty","Tom Hope","Jimmy Pronchick","Krupa Bhawsar","Roger E. Beaty"],"url":"https://arxiv.org/abs/2502.03253"}
{"created":"2025-05-06","title":"When Pre-trained Visual Representations Fall Short: Limitations in Visuo-Motor Robot Learning","abstract":"The integration of pre-trained visual representations (PVRs) into visuo-motor robot learning has emerged as a promising alternative to training visual encoders from scratch. However, PVRs face critical challenges in the context of policy learning, including temporal entanglement and an inability to generalise even in the presence of minor scene perturbations. These limitations hinder performance in tasks requiring temporal awareness and robustness to scene changes. This work identifies these shortcomings and proposes solutions to address them. First, we augment PVR features with temporal perception and a sense of task completion, effectively disentangling them in time. Second, we introduce a module that learns to selectively attend to task-relevant local features, enhancing robustness when evaluated on out-of-distribution scenes. Our experiments demonstrate significant performance improvements, particularly in PVRs trained with masking objectives, and validate the effectiveness of our enhancements in addressing PVR-specific limitations.","authors":["Nikolaos Tsagkas","Andreas Sochopoulos","Duolikun Danier","Sethu Vijayakumar","Chris Xiaoxuan Lu","Oisin Mac Aodha"],"url":"https://arxiv.org/abs/2502.03270"}
{"created":"2025-05-06","title":"Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach","abstract":"As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization algorithm, significantly improves the global system energy efficiency and surpasses the popular MARL and other conventional schemes.","authors":["Abdullahi Isa Ahmed","Jamal Bentahar","El Mehdi Amhoud"],"url":"https://arxiv.org/abs/2502.03377"}
{"created":"2025-05-06","title":"Quantifying imperfect cognition via achieved information gain","abstract":"Cognition, information processing in form of inference, communication, and memorization, is the central activity of any intelligence. Its physical realization in a brain, computer, or in any other intelligent system requires resources like time, energy, memory, bandwidth, money, and others. Due to limited resources, many real world intelligent systems perform only imperfect cognition. To understand the trade-off between accuracy and resource investments in existing systems, e.g. in biology, as well as for the resource-aware optimal design of information processing systems, like computer algorithms and artificial neural networks, a quantification of information obtained in an imperfect cognitive operation is desirable. To this end, we propose the concept of the achieved information gain (AIG) of a belief update, which is given by the amount of information obtained by updating from the initial state of knowledge to the ideal state, minus the amount that a change from the imperfect to the ideal state would yield. AIG has many desirable properties for quantifying imperfect cognition. The ratio of achieved to ideally obtainable information measures cognitive fidelity and that of AIG to the necessary cognitive effort measures cognitive efficiency. We provide an axiomatic derivation of AIG, relate it to other information measures, illustrate its application to common scenarios of posterior inaccuracies, and discuss the implication of cognitive efficiency for sustainable resource allocation in computational inference.","authors":["Torsten En{\\ss}lin"],"url":"https://arxiv.org/abs/2502.04088"}
{"created":"2025-05-06","title":"Unveiling the Mechanisms of Explicit CoT Training: How CoT Enhances Reasoning Generalization","abstract":"The integration of explicit Chain-of-Thought (CoT) reasoning into training large language models (LLMs) has advanced their reasoning capabilities, yet the mechanisms by which CoT enhances generalization remain poorly understood. This work investigates (1) \\textit{how} CoT training reshapes internal model representations and (2) \\textit{why} it improves both in-distribution (ID) and out-of-distribution (OOD) reasoning generalization. Through controlled experiments and theoretical analysis, we derive the following key insights. \\textbf{1)} Structural Advantage: CoT training internalizes reasoning into a two-stage generalizing circuit, where the number of stages corresponds to the explicit reasoning steps during training. Notably, CoT-trained models resolve intermediate results at shallower layers compared to non-CoT counterparts, freeing up deeper layers to specialize in subsequent reasoning steps. \\textbf{2)} Theoretical Analysis: the information-theoretic generalization bounds via distributional divergence can be decomposed into ID and OOD components. While ID error diminishes with sufficient training regardless of CoT, OOD error critically depends on CoT: Non-CoT training fails to generalize to OOD samples due to unseen reasoning patterns, whereas CoT training achieves near-perfect OOD generalization by mastering subtasks and reasoning compositions during training. The identified mechanisms explain our experimental results: CoT training accelerates convergence and enhances generalization from ID to both ID and OOD scenarios while maintaining robust performance even with tolerable noise. These findings are further validated on complex real-world datasets. This paper offers valuable insights for designing CoT strategies to enhance LLM reasoning robustness.","authors":["Xinhao Yao","Ruifeng Ren","Yun Liao","Yong Liu"],"url":"https://arxiv.org/abs/2502.04667"}
{"created":"2025-05-06","title":"The Human Labour of Data Work: Capturing Cultural Diversity through World Wide Dishes","abstract":"This paper provides guidance for building and maintaining infrastructure for participatory AI efforts by sharing reflections on building World Wide Dishes (WWD), a bottom-up, community-led image and text dataset of culinary dishes and associated cultural customs. We present WWD as an example of participatory dataset creation, where community members both guide the design of the research process and contribute to the crowdsourced dataset. This approach incorporates localised expertise and knowledge to address the limitations of web-scraped Internet datasets acknowledged in the Participatory AI discourse. We show that our approach can result in curated, high-quality data that supports decentralised contributions from communities that do not typically contribute to datasets due to a variety of systemic factors. Our project demonstrates the importance of participatory mediators in supporting community engagement by identifying the kinds of labour they performed to make WWD possible. We surface three dimensions of labour performed by participatory mediators that are crucial for participatory dataset construction: building trust with community members, making participation accessible, and contextualising community values to support meaningful data collection. Drawing on our findings, we put forth five lessons for building infrastructure to support future participatory AI efforts.","authors":["Siobhan Mackenzie Hall","Samantha Dalal","Raesetje Sefala","Foutse Yuehgoh","Aisha Alaagib","Imane Hamzaoui","Shu Ishida","Jabez Magomere","Lauren Crais","Aya Salama","Tejumade Afonja"],"url":"https://arxiv.org/abs/2502.05961"}
{"created":"2025-05-06","title":"Model-Based Offline Reinforcement Learning with Reliability-Guaranteed Sequence Modeling","abstract":"Model-based offline reinforcement learning (MORL) aims to learn a policy by exploiting a dynamics model derived from an existing dataset. Applying conservative quantification to the dynamics model, most existing works on MORL generate trajectories that approximate the real data distribution to facilitate policy learning by using current information (e.g., the state and action at time step $t$). However, these works neglect the impact of historical information on environmental dynamics, leading to the generation of unreliable trajectories that may not align with the real data distribution. In this paper, we propose a new MORL algorithm \\textbf{R}eliability-guaranteed \\textbf{T}ransformer (RT), which can eliminate unreliable trajectories by calculating the cumulative reliability of the generated trajectory (i.e., using a weighted variational distance away from the real data). Moreover, by sampling candidate actions with high rewards, RT can efficiently generate high-return trajectories from the existing offline data. We theoretically prove the performance guarantees of RT in policy learning, and empirically demonstrate its effectiveness against state-of-the-art model-based methods on several benchmark tasks.","authors":["Shenghong He"],"url":"https://arxiv.org/abs/2502.06491"}
{"created":"2025-05-06","title":"Threat Me Right: A Human HARMS Threat Model for Technical Systems","abstract":"Threat modelling is the process of identifying potential vulnerabilities in a system and prioritising them. Existing threat modelling tools focus primarily on technical systems and are not as well suited to interpersonal threats. In this paper, we discuss traditional threat modelling methods and their shortcomings, and propose a new threat modelling framework (HARMS) to identify non-technical and human factors harms. We also cover a case study of applying HARMS when it comes to IoT devices such as smart speakers with virtual assistants.","authors":["Kieron Ivy Turk","Anna Talas","Alice Hutchings"],"url":"https://arxiv.org/abs/2502.07116"}
{"created":"2025-05-06","title":"Towards a Value-Complemented Framework for Enabling Human Monitoring in Cyber-Physical Systems","abstract":"[Context and Motivation]: Cyber-Physical Systems (CPS) have become relevant in a wide variety of different domains, integrating hardware and software, often operating in an emerging and uncertain environment where human actors actively or passively engage with the CPS. To ensure correct and safe operation, and self-adaptation, monitors are used for collecting and analyzing diverse runtime information. [Problem]: However, monitoring humans at runtime, collecting potentially sensitive information about their actions and behavior, comes with significant ramifications that can severely hamper the successful integration of human-machine collaboration. Requirements engineering (RE) activities must integrate diverse human values, including Privacy, Security, and Self-Direction during system design, to avoid involuntary data sharing or misuse. [Principal Ideas]: In this research preview, we focus on the importance of incorporating these aspects in the RE lifecycle of eliciting and creating runtime monitors. [Contribution]: We derived an initial conceptual framework, building on the value taxonomy introduced by Schwartz and human value integrated Software Engineering by Whittle, further leveraging the concept of value tactics. The goal is to tie functional and non-functional monitoring requirements to human values and establish traceability between values, requirements, and actors. Based on this, we lay out a research roadmap guiding our ongoing work in this area.","authors":["Zoe Pfister","Michael Vierhauser","Rebekka Wohlrab","Ruth Breu"],"url":"https://arxiv.org/abs/2502.07502"}
{"created":"2025-05-06","title":"Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems","abstract":"Inspired by recent findings on the fractal geometry of language, we introduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe for scaling inference time in language and multimodal systems. RINS is a particular form of recursive depth that significantly outperforms +55 other variants, including the recent \"repeat-all-over\" (RAO) strategy in Mobile LLM (Liu et al., 2024) and latent recurrent thinking (Geiping et al., 2025). Unlike prior works, we carry out our comparisons on a compute-matched regime, and demonstrate that for a fixed model size and training compute budget, RINS substantially improves language modeling performance. It also generalizes beyond pure language tasks, delivering gains in multimodal systems, including a +2% improvement in 0-shot ImageNet accuracy for SigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS improves both the asymptotic performance limits and the scaling exponents. More importantly, with light-weight (linear) adapters (comprising <1% of model parameters) and stochastic dropout, RINS offers a no-regret strategy, meaning that RINS-enabled pretraining improves performance in language modeling even when recursive depth is not applied at inference time. This corresponds to improving performance on a training compute-, parameter-, and inference-matched regime, suggesting its potential as a viable component of LLM pretraining!","authors":["Ibrahim Alabdulmohsin","Xiaohua Zhai"],"url":"https://arxiv.org/abs/2502.07503"}
{"created":"2025-05-06","title":"PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language Model Inference","abstract":"Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced LLMs and makes users pay for expensive compute resources that are poorly utilized for the memory-bound LLM inference tasks.","authors":["Yufeng Gu","Alireza Khadem","Sumanth Umesh","Ning Liang","Xavier Servot","Onur Mutlu","Ravi Iyer","Reetuparna Das"],"url":"https://arxiv.org/abs/2502.07578"}
{"created":"2025-05-06","title":"Robust-Sorting and Applications to Ulam-Median","abstract":"Sorting is one of the most basic primitives in many algorithms and data analysis tasks. Comparison-based sorting algorithms, like quick-sort and merge-sort, are known to be optimal when the outcome of each comparison is error-free. However, many real-world sorting applications operate in scenarios where the outcome of each comparison can be noisy. In this work, we explore settings where a bounded number of comparisons are potentially corrupted by erroneous agents, resulting in arbitrary, adversarial outcomes.","authors":["Ragesh Jaiswal","Amit Kumar","Jatin Yadav"],"url":"https://arxiv.org/abs/2502.07653"}
{"created":"2025-05-06","title":"Predictive Planner for Autonomous Driving with Consistency Models","abstract":"Trajectory prediction and planning are essential for autonomous vehicles to navigate safely and efficiently in dynamic environments. Traditional approaches often treat them separately, limiting the ability for interactive planning. While recent diffusion-based generative models have shown promise in multi-agent trajectory generation, their slow sampling is less suitable for high-frequency planning tasks. In this paper, we leverage the consistency model to build a predictive planner that samples from a joint distribution of ego and surrounding agents, conditioned on the ego vehicle's navigational goal. Trained on real-world human driving datasets, our consistency model generates higher-quality trajectories with fewer sampling steps than standard diffusion models, making it more suitable for real-time deployment. To enforce multiple planning constraints simultaneously on the ego trajectory, a novel online guided sampling approach inspired by the Alternating Direction Method of Multipliers (ADMM) is introduced. Evaluated on the Waymo Open Motion Dataset (WOMD), our method enables proactive behavior such as nudging and yielding, and also demonstrates smoother, safer, and more efficient trajectories and satisfaction of multiple constraints under a limited computational budget.","authors":["Anjian Li","Sangjae Bae","David Isele","Ryne Beeson","Faizan M. Tariq"],"url":"https://arxiv.org/abs/2502.08033"}
{"created":"2025-05-06","title":"CRANE: Reasoning with constrained LLM generation","abstract":"Code generation, symbolic math reasoning, and other tasks require LLMs to produce outputs that are both syntactically and semantically correct. Constrained LLM generation is a promising direction to enforce adherence to formal grammar, but prior works have empirically observed that strict enforcement of formal constraints often diminishes the reasoning capabilities of LLMs. In this work, we first provide a theoretical explanation for why constraining LLM outputs to very restrictive grammars that only allow syntactically valid final answers reduces the reasoning capabilities of the model. Second, we demonstrate that by augmenting the output grammar with carefully designed additional rules, it is always possible to preserve the reasoning capabilities of the LLM while ensuring syntactic and semantic correctness in its outputs. Building on these theoretical insights, we propose a reasoning-augmented constrained decoding algorithm, CRANE, which effectively balances the correctness of constrained generation with the flexibility of unconstrained generation. Experiments on multiple open-source LLMs and benchmarks show that CRANE significantly outperforms both state-of-the-art constrained decoding strategies and standard unconstrained decoding, showing up to 10% points accuracy improvement over baselines on challenging symbolic reasoning benchmarks GSM-symbolic and FOLIO.","authors":["Debangshu Banerjee","Tarun Suresh","Shubham Ugare","Sasa Misailovic","Gagandeep Singh"],"url":"https://arxiv.org/abs/2502.09061"}
{"created":"2025-05-06","title":"Revisiting Euclidean Alignment for Transfer Learning in EEG-Based Brain-Computer Interfaces","abstract":"Due to large intra-subject and inter-subject variabilities of electroencephalogram (EEG) signals, EEG-based brain-computer interfaces (BCIs) usually need subject-specific calibration to tailor the decoding algorithm for each new subject, which is time-consuming and user-unfriendly, hindering their real-world applications. Transfer learning (TL) has been extensively used to expedite the calibration, by making use of EEG data from other subjects/sessions. An important consideration in TL for EEG-based BCIs is to reduce the data distribution discrepancies among different subjects/sessions, to avoid negative transfer. Euclidean alignment (EA) was proposed in 2020 to address this challenge. Numerous experiments from 13 different BCI paradigms demonstrated its effectiveness and efficiency. This paper revisits EA, explaining its procedure and correct usage, introducing its applications and extensions, and pointing out potential new research directions. It should be very helpful to BCI researchers, especially those who are working on EEG signal decoding.","authors":["Dongrui Wu"],"url":"https://arxiv.org/abs/2502.09203"}
{"created":"2025-05-06","title":"SpeechT: Findings of the First Mentorship in Speech Translation","abstract":"This work presents the details and findings of the first mentorship in speech translation (SpeechT), which took place in December 2024 and January 2025. To fulfil the mentorship requirements, the participants engaged in key activities, including data preparation, modelling, and advanced research. The participants explored data augmentation techniques and compared end-to-end and cascaded speech translation systems. The projects covered various languages other than English, including Arabic, Bengali, Galician, Indonesian, Japanese, and Spanish.","authors":["Yasmin Moslem","Juan Juli\\'an Cea Mor\\'an","Mariano Gonzalez-Gomez","Muhammad Hazim Al Farouq","Farah Abdou","Satarupa Deb"],"url":"https://arxiv.org/abs/2502.12050"}
{"created":"2025-05-06","title":"Performance Trade-offs of High Order Meshless Approximation on Distributed Memory Systems","abstract":"Meshless methods approximate operators in a specific node as a weighted sum of values in its neighbours. Higher order approximations of derivatives provide more accurate solutions with better convergence characteristics, but they come at the cost of including more neighbours. On the accuracy-per-compute time basis we know that increasing the approximation order is beneficial for a shared memory computer, but there is additional communication overhead when problems become too large and we have to resort to distributed memory systems. Meshless nodes are divided between systems in spatially coherent subdomains with approximations at their edges requiring neighbouring value exchange. Performance optimization is then a balancing act between minimizing the required number of communicated neighbours by lowering the approximation order or increasing it to enable faster convergence. We use the radial basis function-generated finite difference method (RBF-FD) to approximate the derivatives that we use to solve the Poisson equation with an explicit iterative scheme. Inter-system communication is provided by Open MPI, while OpenMP is used for intra-system parallelisation. We perform the analysis on a homogenous CPU-based cluster where we examine the behaviour and attempt to determine the optimal parameterisation with the goal of minimizing the computational time to reach a desired accuracy.","authors":["Jon Vehovar","Miha Rot","Gregor Kosec"],"url":"https://arxiv.org/abs/2502.12878"}
{"created":"2025-05-06","title":"The Role of GitHub Copilot on Software Development: A Perspective on Productivity, Security, Best Practices and Future Directions","abstract":"GitHub Copilot is transforming software development by automating tasks and boosting productivity through AI driven code generation. In this paper, we conduct a literature survey to synthesize insights on Copilot's impact on productivity and security. We review academic journal databases, industry reports, and official documentation to highlight key findings and challenges. While Copilot accelerates coding and prototyping, concerns over security vulnerabilities and intellectual property risks persist. Drawing from the literature, we provide a perspective on best practices and future directions for responsible AI adoption in software engineering, offering actionable insights for developers and organizations to integrate Copilot effectively while maintaining high standards of quality and security.","authors":["Suresh Babu Nettur","Shanthi Karpurapu","Unnati Nettur","Likhit Sagar Gajja","Sravanthy Myneni","Akhil Dusi"],"url":"https://arxiv.org/abs/2502.13199"}
{"created":"2025-05-06","title":"Reproducing NevIR: Negation in Neural Information Retrieval","abstract":"Negation is a fundamental aspect of human communication, yet it remains a challenge for Language Models (LMs) in Information Retrieval (IR). Despite the heavy reliance of modern neural IR systems on LMs, little attention has been given to their handling of negation. In this study, we reproduce and extend the findings of NevIR, a benchmark study that revealed most IR models perform at or below the level of random ranking when dealing with negation. We replicate NevIR's original experiments and evaluate newly developed state-of-the-art IR models. Our findings show that a recently emerging category-listwise Large Language Model (LLM) re-rankers-outperforms other models but still underperforms human performance. Additionally, we leverage ExcluIR, a benchmark dataset designed for exclusionary queries with extensive negation, to assess the generalisability of negation understanding. Our findings suggest that fine-tuning on one dataset does not reliably improve performance on the other, indicating notable differences in their data distributions. Furthermore, we observe that only cross-encoders and listwise LLM re-rankers achieve reasonable performance across both negation tasks.","authors":["Coen van den Elsen","Francien Barkhof","Thijmen Nijdam","Simon Lupart","Mohammad Aliannejadi"],"url":"https://arxiv.org/abs/2502.13506"}
{"created":"2025-05-06","title":"Formal verification in Solidity and Move: insights from a comparative analysis","abstract":"Formal verification plays a crucial role in making smart contracts safer, being able to find bugs or to guarantee their absence, as well as checking whether the business logic is correctly implemented. For Solidity, even though there already exist several mature verification tools, the semantical quirks of the language can make verification quite hard in practice. Move, on the other hand, has been designed with security and verification in mind, and it has been accompanied since its early stages by a formal verification tool, the Move Prover. In this paper, we investigate through a comparative analysis: 1) how the different designs of the two contract languages impact verification, and 2) what is the state-of-the-art of verification tools for the two languages, and how do they compare on three paradigmatic use cases. Our investigation is supported by an open dataset of verification tasks performed in Certora and in the Aptos Move Prover.","authors":["Massimo Bartoletti","Silvia Crafa","Enrico Lipparini"],"url":"https://arxiv.org/abs/2502.13929"}
{"created":"2025-05-06","title":"LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records","abstract":"Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.","authors":["Sujeong Im","Jungwoo Oh","Edward Choi"],"url":"https://arxiv.org/abs/2502.14259"}
{"created":"2025-05-06","title":"SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training","abstract":"We present a framework for pre-training of 3D hand pose estimation from in-the-wild hand images sharing with similar hand characteristics, dubbed SimHand. Pre-training with large-scale images achieves promising results in various tasks, but prior methods for 3D hand pose pre-training have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our pre-training method with contrastive learning. Specifically, we collect over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands: pairs of non-identical samples with similar hand poses. We then propose a novel contrastive learning method that embeds similar hand pairs closer in the feature space. Our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. Our experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method (PeCLR) in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands.","authors":["Nie Lin","Takehiko Ohkawa","Yifei Huang","Mingfang Zhang","Minjie Cai","Ming Li","Ryosuke Furuta","Yoichi Sato"],"url":"https://arxiv.org/abs/2502.15251"}
{"created":"2025-05-06","title":"Almost AI, Almost Human: The Challenge of Detecting AI-Polished Writing","abstract":"The growing use of large language models (LLMs) for text generation has led to widespread concerns about AI-generated content detection. However, an overlooked challenge is AI-polished text, where human-written content undergoes subtle refinements using AI tools. This raises a critical question: should minimally polished text be classified as AI-generated? Such classification can lead to false plagiarism accusations and misleading claims about AI prevalence in online content. In this study, we systematically evaluate twelve state-of-the-art AI-text detectors using our AI-Polished-Text Evaluation (APT-Eval) dataset, which contains 14.7K samples refined at varying AI-involvement levels. Our findings reveal that detectors frequently flag even minimally polished text as AI-generated, struggle to differentiate between degrees of AI involvement, and exhibit biases against older and smaller models. These limitations highlight the urgent need for more nuanced detection methodologies.","authors":["Shoumik Saha","Soheil Feizi"],"url":"https://arxiv.org/abs/2502.15666"}
{"created":"2025-05-06","title":"Design and Implementation of a Scalable Clinical Data Warehouse for Resource-Constrained Healthcare Systems","abstract":"Centralized electronic health record repositories are critical for advancing disease surveillance, public health research, and evidence-based policymaking. However, developing countries face persistent challenges in achieving this due to fragmented healthcare data sources, inconsistent record-keeping practices, and the absence of standardized patient identifiers, limiting reliable record linkage, compromise data interoperability, and limit scalability-obstacles exacerbated by infrastructural constraints and privacy concerns. To address these barriers, this study proposes a scalable, privacy-preserving clinical data warehouse, NCDW, designed for heterogeneous EHR integration in resource-limited settings and tested with 1.16 million clinical records. The framework incorporates a wrapper-based data acquisition layer for secure, automated ingestion of multisource health data and introduces a soundex algorithm to resolve patient identity mismatches in the absence of unique IDs. A modular data mart is designed for disease-specific analytics, demonstrated through a dengue fever case study in Bangladesh, integrating clinical, demographic, and environmental data for outbreak prediction and resource planning. Quantitative assessment of the data mart underscores its utility in strengthening national decision-support systems, highlighting the model's adaptability for infectious disease management. Comparative evaluation of database technologies reveals NoSQL outperforms relational SQL by 40-69% in complex query processing, while system load estimates validate the architecture's capacity to manage 19 million daily records (34TB over 5 years). The framework can be adapted to various healthcare settings across developing nations by modifying the ingestion layer to accommodate standards like ICD-11 and HL7 FHIR, facilitating interoperability for managing infectious diseases (i.e., COVID, tuberculosis).","authors":["Shovito Barua Soumma","Fahim Shahriar","Umme Niraj Mahi","Md Hasin Abrar","Md Abdur Rahman Fahad","Abu Sayed Md. Latiful Hoque"],"url":"https://arxiv.org/abs/2502.16674"}
{"created":"2025-05-06","title":"Applying LLMs to Active Learning: Towards Cost-Efficient Cross-Task Text Classification without Manually Labeled Data","abstract":"Machine learning-based classifiers have been used for text classification, such as sentiment analysis, news classification, and toxic comment classification. However, supervised machine learning models often require large amounts of labeled data for training, and manual annotation is both labor-intensive and requires domain-specific knowledge, leading to relatively high annotation costs. To address this issue, we propose an approach that integrates large language models (LLMs) into an active learning framework, achieving high cross-task text classification performance without the need for any manually labeled data. Furthermore, compared to directly applying GPT for classification tasks, our approach retains over 93% of its classification performance while requiring only approximately 6% of the computational time and monetary cost, effectively balancing performance and resource efficiency. These findings provide new insights into the efficient utilization of LLMs and active learning algorithms in text classification tasks, paving the way for their broader application.","authors":["Yejian Zhang","Shingo Takada"],"url":"https://arxiv.org/abs/2502.16892"}
{"created":"2025-05-06","title":"A novel approach to navigate the taxonomic hierarchy to address the Open-World Scenarios in Medicinal Plant Classification","abstract":"In this article, we propose a novel approach for plant hierarchical taxonomy classification by posing the problem as an open class problem. It is observed that existing methods for medicinal plant classification often fail to perform hierarchical classification and accurately identifying unknown species, limiting their effectiveness in comprehensive plant taxonomy classification. Thus we address the problem of unknown species classification by assigning it best hierarchical labels. We propose a novel method, which integrates DenseNet121, Multi-Scale Self-Attention (MSSA) and cascaded classifiers for hierarchical classification. The approach systematically categorizes medicinal plants at multiple taxonomic levels, from phylum to species, ensuring detailed and precise classification. Using multi scale space attention, the model captures both local and global contextual information from the images, improving the distinction between similar species and the identification of new ones. It uses attention scores to focus on important features across multiple scales. The proposed method provides a solution for hierarchical classification, showcasing superior performance in identifying both known and unknown species. The model was tested on two state-of-art datasets with and without background artifacts and so that it can be deployed to tackle real word application. We used unknown species for testing our model. For unknown species the model achieved an average accuracy of 83.36%, 78.30%, 60.34% and 43.32% for predicting correct phylum, class, order and family respectively. Our proposed model size is almost four times less than the existing state of the art methods making it easily deploy able in real world application.","authors":["Soumen Sinha","Tanisha Rana","Rahul Roy"],"url":"https://arxiv.org/abs/2502.17289"}
{"created":"2025-05-06","title":"Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs","abstract":"We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.","authors":["Jan Betley","Daniel Tan","Niels Warncke","Anna Sztyber-Betley","Xuchan Bao","Mart\\'in Soto","Nathan Labenz","Owain Evans"],"url":"https://arxiv.org/abs/2502.17424"}
{"created":"2025-05-06","title":"VANPY: Voice Analysis Framework","abstract":"Voice data is increasingly being used in modern digital communications, yet there is still a lack of comprehensive tools for automated voice analysis and characterization. To this end, we developed the VANPY (Voice Analysis in Python) framework for automated pre-processing, feature extraction, and classification of voice data. The VANPY is an open-source end-to-end comprehensive framework that was developed for the purpose of speaker characterization from voice data. The framework is designed with extensibility in mind, allowing for easy integration of new components and adaptation to various voice analysis applications. It currently incorporates over fifteen voice analysis components - including music/speech separation, voice activity detection, speaker embedding, vocal feature extraction, and various classification models.","authors":["Gregory Koushnir","Michael Fire","Galit Fuhrmann Alpert","Dima Kagan"],"url":"https://arxiv.org/abs/2502.17579"}
{"created":"2025-05-06","title":"Simple Sublinear Algorithms for $(\\Delta+1)$ Vertex Coloring via Asymmetric Palette Sparsification","abstract":"The palette sparsification theorem (PST) of Assadi, Chen, and Khanna (SODA 2019) states that in every graph $G$ with maximum degree $\\Delta$, sampling a list of $O(\\log{n})$ colors from $\\{1,\\ldots,\\Delta+1\\}$ for every vertex independently and uniformly, with high probability, allows for finding a $(\\Delta+1)$ vertex coloring of $G$ by coloring each vertex only from its sampled list. PST naturally leads to a host of sublinear algorithms for $(\\Delta+1)$ vertex coloring, including in semi-streaming, sublinear time, and MPC models, which are all proven to be nearly optimal, and in the case of the former two are the only known sublinear algorithms for this problem.","authors":["Sepehr Assadi","Helia Yazdanyar"],"url":"https://arxiv.org/abs/2502.17629"}
{"created":"2025-05-06","title":"Letters from Future Self: Augmenting the Letter-Exchange Exercise with LLM-based Agents to Enhance Young Adults' Career Exploration","abstract":"Young adults often encounter challenges in career exploration. Self-guided interventions, such as the letter-exchange exercise, where participants envision and adopt the perspective of their future selves by exchanging letters with their envisioned future selves, can support career development. However, the broader adoption of such interventions may be limited without structured guidance. To address this, we integrated Large Language Model (LLM)-based agents that simulate participants' future selves into the letter-exchange exercise and evaluated their effectiveness. A one-week experiment (N=36) compared three conditions: (1) participants manually writing replies to themselves from the perspective of their future selves (baseline), (2) future-self agents generating letters to participants, and (3) future-self agents engaging in chat conversations with participants. Results indicated that exchanging letters with future-self agents enhanced participants' engagement during the exercise, while overall benefits of the intervention on future orientation, career self-concept, and psychological support remained comparable across conditions. We discuss design implications for AI-augmented interventions for supporting young adults' career exploration.","authors":["Hayeon Jeon","Suhwoo Yoon","Keyeun Lee","Seo Hyeong Kim","Esther Hehsun Kim","Seonghye Cho","Yena Ko","Soeun Yang","Laura Dabbish","John Zimmerman","Eun-mee Kim","Hajin Lim"],"url":"https://arxiv.org/abs/2502.18881"}
{"created":"2025-05-06","title":"Visual Adaptive Prompting for Compositional Zero-Shot Learning","abstract":"Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results.","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia","Eman El-Sheikh"],"url":"https://arxiv.org/abs/2502.20292"}
{"created":"2025-05-06","title":"EgoNormia: Benchmarking Physical Social Norm Understanding","abstract":"Human activity is moderated by norms. However, machines are often trained without explicit supervision on norm understanding and reasoning, particularly when norms are physically- or socially-grounded. To improve and evaluate the normative reasoning capability of vision-language models (VLMs), we present \\dataset{} $\\|\\epsilon\\|$, consisting of 1,853 challenging, multi-stage MCQ questions based on ego-centric videos of human interactions, evaluating both the prediction and justification of normative actions. The normative actions encompass seven categories: safety, privacy, proxemics, politeness, cooperation, coordination/proactivity, and communication/legibility. To compile this dataset at scale, we propose a novel pipeline leveraging video sampling, automatic answer generation, filtering, and human validation. Our work demonstrates that current state-of-the-art vision-language models lack robust norm understanding, scoring a maximum of 54\\% on \\dataset{} (versus a human bench of 92\\%). Our analysis of performance in each dimension highlights the significant risks of safety, privacy, and the lack of collaboration and communication capability when applied to real-world agents. We additionally show that through a retrieval-based generation (RAG) method, it is possible to use \\dataset{} to enhance normative reasoning in VLMs.","authors":["MohammadHossein Rezaei","Yicheng Fu","Phil Cuvin","Caleb Ziems","Yanzhe Zhang","Hao Zhu","Diyi Yang"],"url":"https://arxiv.org/abs/2502.20490"}
{"created":"2025-05-06","title":"SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models","abstract":"With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from popular distributed inference techniques such as Tensor Parallelism pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD), to reduce communication overheads in tensor parallelism by selectively dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we apply different SPD strategies to attention blocks based on their sensitivity to the model accuracy. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for diverse distributed environments: SPD offered about 20% overall inference latency reduction with < 1% accuracy regression for LLaMA2-70B inference over 8 GPUs.","authors":["Han-Byul Kim","Duc Hoang","Arnav Kundu","Mohammad Samragh","Minsik Cho"],"url":"https://arxiv.org/abs/2502.20727"}
{"created":"2025-05-06","title":"Semantic Volume: Quantifying and Detecting both External and Internal Uncertainty in LLMs","abstract":"Large language models (LLMs) have demonstrated remarkable performance across diverse tasks by encoding vast amounts of factual knowledge. However, they are still prone to hallucinations, generating incorrect or misleading information, often accompanied by high uncertainty. Existing methods for hallucination detection primarily focus on quantifying internal uncertainty, which arises from missing or conflicting knowledge within the model. However, hallucinations can also stem from external uncertainty, where ambiguous user queries lead to multiple possible interpretations. In this work, we introduce Semantic Volume, a novel mathematical measure for quantifying both external and internal uncertainty in LLMs. Our approach perturbs queries and responses, embeds them in a semantic space, and computes the determinant of the Gram matrix of the embedding vectors, capturing their dispersion as a measure of uncertainty. Our framework provides a generalizable and unsupervised uncertainty detection method without requiring internal access to LLMs. We conduct extensive experiments on both external and internal uncertainty detection, demonstrating that our Semantic Volume method consistently outperforms existing baselines in both tasks. Additionally, we provide theoretical insights linking our measure to differential entropy, unifying and extending previous sampling-based uncertainty measures such as the semantic entropy. Semantic Volume is shown to be a robust and interpretable approach to improving the reliability of LLMs by systematically detecting uncertainty in both user queries and model responses.","authors":["Xiaomin Li","Zhou Yu","Ziji Zhang","Yingying Zhuang","Swair Shah","Narayanan Sadagopan","Anurag Beniwal"],"url":"https://arxiv.org/abs/2502.21239"}
{"created":"2025-05-06","title":"Exploration on Real World Assets and Tokenization","abstract":"This study delves into the tokenization of real-world assets (RWAs) on the blockchain with the objective of augmenting liquidity and refining asset management practices. By conducting an exhaustive analysis of the technical procedures implicated and scrutinizing case studies of existing deployments, this research evaluates the advantages, hurdles, and prospective advancements of blockchain technology in reshaping conventional asset management paradigms.","authors":["Ning Xia","Xiaolei Zhao","Yimin Yang","Yixuan Li","Yucong Li"],"url":"https://arxiv.org/abs/2503.01111"}
{"created":"2025-05-06","title":"Frankenstein Optimizer: Harnessing the Potential by Revisiting Optimization Tricks","abstract":"Gradient-based optimization drives the unprecedented performance of modern deep neural network models across diverse applications. Adaptive algorithms have accelerated neural network training due to their rapid convergence rates; however, they struggle to find ``flat minima\" reliably, resulting in suboptimal generalization compared to stochastic gradient descent (SGD). By revisiting various adaptive algorithms' mechanisms, we propose the Frankenstein optimizer, which combines their advantages. The proposed Frankenstein dynamically adjusts first- and second-momentum coefficients according to the optimizer's current state to directly maintain consistent learning dynamics and immediately reflect sudden gradient changes. Extensive experiments across several research domains such as computer vision, natural language processing, few-shot learning, and scientific simulations show that Frankenstein surpasses existing adaptive algorithms and SGD empirically regarding convergence speed and generalization performance. Furthermore, this research deepens our understanding of adaptive algorithms through centered kernel alignment analysis and loss landscape visualization during the learning process. Code is available at https://github.com/acctouhou/Frankenstein_optimizer","authors":["Chia-Wei Hsu","Nien-Ti Tsou","Yu-Cheng Chen","Yang Jeong Park","Ju Li"],"url":"https://arxiv.org/abs/2503.02147"}
{"created":"2025-05-06","title":"Artificial Intelligence in Reactor Physics: Current Status and Future Prospects","abstract":"Reactor physics is the study of neutron properties, focusing on using models to examine the interactions between neutrons and materials in nuclear reactors. Artificial intelligence (AI) has made significant contributions to reactor physics, e.g., in operational simulations, safety design, real-time monitoring, core management and maintenance. This paper presents a comprehensive review of AI approaches in reactor physics, especially considering the category of Machine Learning (ML), with the aim of describing the application scenarios, frontier topics, unsolved challenges and future research directions. From equation solving and state parameter prediction to nuclear industry applications, this paper provides a step-by-step overview of ML methods applied to steady-state, transient and combustion problems. Most literature works achieve industry-demanded models by enhancing the efficiency of deterministic methods or correcting uncertainty methods, which leads to successful applications. However, research on ML methods in reactor physics is somewhat fragmented, and the ability to generalize models needs to be strengthened. Progress is still possible, especially in addressing theoretical challenges and enhancing industrial applications such as building surrogate models and digital twins.","authors":["Ruizhi Zhang","Shengfeng Zhu","Kan Wang","Ding She","Jean-Philippe Argaud","Bertrand Bouriquet","Qing Li","Helin Gong"],"url":"https://arxiv.org/abs/2503.02440"}
{"created":"2025-05-06","title":"A Systematic Literature Review on Safety of the Intended Functionality for Automated Driving Systems","abstract":"In the automobile industry, ensuring the safety of automated vehicles equipped with the Automated Driving System (ADS) is becoming a significant focus due to the increasing development and deployment of automated driving. Automated driving depends on sensing both the external and internal environments of a vehicle, utilizing perception sensors and algorithms, and Electrical/Electronic (E/E) systems for situational awareness and response. ISO 21448 is the standard for Safety of the Intended Functionality (SOTIF) that aims to ensure that the ADS operate safely within their intended functionality. SOTIF focuses on preventing or mitigating potential hazards that may arise from the limitations or failures of the ADS, including hazards due to insufficiencies of specification, or performance insufficiencies, as well as foreseeable misuse of the intended functionality. However, the challenge lies in ensuring the safety of vehicles despite the limited availability of extensive and systematic literature on SOTIF. To address this challenge, a Systematic Literature Review (SLR) on SOTIF for the ADS is performed following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. The objective is to methodically gather and analyze the existing literature on SOTIF. The major contributions of this paper are: (i) presenting a summary of the literature by synthesizing and organizing the collective findings, methodologies, and insights into distinct thematic groups, and (ii) summarizing and categorizing the acknowledged limitations based on data extracted from an SLR of 51 research papers published between 2018 and 2023. Furthermore, research gaps are determined, and future research directions are proposed.","authors":["Milin Patel","Rolf Jung","Marzana Khatun"],"url":"https://arxiv.org/abs/2503.02498"}
{"created":"2025-05-06","title":"The Effectiveness of Large Language Models in Transforming Unstructured Text to Standardized Formats","abstract":"The exponential growth of unstructured text data presents a fundamental challenge in modern data management and information retrieval. While Large Language Models (LLMs) have shown remarkable capabilities in natural language processing, their potential to transform unstructured text into standardized, structured formats remains largely unexplored - a capability that could revolutionize data processing workflows across industries. This study breaks new ground by systematically evaluating LLMs' ability to convert unstructured recipe text into the structured Cooklang format. Through comprehensive testing of four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an innovative evaluation approach is introduced that combines traditional metrics (WER, ROUGE-L, TER) with specialized metrics for semantic element identification. Our experiments reveal that GPT-4o with few-shot prompting achieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating for the first time that LLMs can reliably transform domain-specific unstructured text into structured formats without extensive training. Although model performance generally scales with size, we uncover surprising potential in smaller models like Llama3.1:8b for optimization through targeted fine-tuning. These findings open new possibilities for automated structured data generation across various domains, from medical records to technical documentation, potentially transforming the way organizations process and utilize unstructured information.","authors":["William Brach","Kristi\\'an Ko\\v{s}\\v{t}\\'al","Michal Ries"],"url":"https://arxiv.org/abs/2503.02650"}
{"created":"2025-05-06","title":"A Theoretical Model for Grit in Pursuing Ambitious Ends","abstract":"Ambition and risk-taking have been heralded as important ways for marginalized communities to get out of cycles of poverty. As a result, educational messaging often encourages individuals to strengthen their personal resolve and develop characteristics such as discipline and grit to succeed in ambitious ends. However, recent work in philosophy and sociology highlights that this messaging often does more harm than good for students in these situations. We study similar questions using a different epistemic approach and in simple theoretical models -- we provide a quantitative model of decision-making between stable and risky choices in the improving multi-armed bandits framework. We use this model to first study how individuals' \"strategies\" are affected by their level of grittiness and how this affects their accrued rewards. Then, we study the impact of various interventions, such as increasing grit or providing a financial safety net. Our investigation of rational decision making involves two different formal models of rationality, the competitive ratio between the accrued reward and the optimal reward and Bayesian quantification of uncertainty.","authors":["Avrim Blum","Emily Diana","Kavya Ravichandran","Alexander Williams Tolbert"],"url":"https://arxiv.org/abs/2503.02952"}
{"created":"2025-05-06","title":"Conformal Transformations for Symmetric Power Transformers","abstract":"Transformers with linear attention offer significant computational advantages over softmax-based transformers but often suffer from degraded performance. The symmetric power (sympow) transformer, a particular type of linear transformer, addresses some of this performance gap by leveraging symmetric tensor embeddings, achieving comparable performance to softmax transformers. However, the finite capacity of the recurrent state in sympow transformers limits their ability to retain information, leading to performance degradation when scaling the training or evaluation context length. To address this issue, we propose the conformal-sympow transformer, which dynamically frees up capacity using data-dependent multiplicative gating and adaptively stores information using data-dependent rotary embeddings. Preliminary experiments on the LongCrawl64 dataset demonstrate that conformal-sympow overcomes the limitations of sympow transformers, achieving robust performance across scaled training and evaluation contexts.","authors":["Saurabh Kumar","Jacob Buckman","Carles Gelada","Sean Zhang"],"url":"https://arxiv.org/abs/2503.03269"}
{"created":"2025-05-06","title":"Optimal Beamforming for Multi-Target Multi-User ISAC Exploiting Prior Information: How Many Sensing Beams Are Needed?","abstract":"This paper studies a multi-target multi-user integrated sensing and communication (ISAC) system where a multi-antenna base station (BS) communicates with multiple single-antenna users in the downlink and senses the unknown and random angle information of multiple targets based on their reflected echo signals at the BS receiver as well as their prior probability information. We focus on a general beamforming structure with both communication beams and dedicated sensing beams, whose design is highly non-trivial as more sensing beams provide more flexibility in sensing, but introduce extra interference to communication. To resolve this trade-off, we first characterize the periodic posterior Cram\\'er-Rao bound (PCRB) as a lower bound of the mean-cyclic error (MCE) in multi-target sensing. Then, we optimize the beamforming to minimize the maximum periodic PCRB among all targets to ensure fairness, subject to individual communication rate constraints at multiple users. Despite the non-convexity of this problem, we propose a general construction method for the optimal solution by leveraging semi-definite relaxation (SDR), and derive a general bound on the number of sensing beams needed. Moreover, we unveil specific structures of the optimal solution in various cases, where tighter bounds on the number of sensing beams needed are derived (e.g., no or at most one sensing beam is needed under stringent rate constraints or with homogeneous targets). Next, we study the beamforming optimization to minimize the sum periodic PCRB under user rate constraints. By applying SDR, we propose a general construction method for the optimal solution and its specific structures which yield lower computational complexities. We derive a general bound and various tighter bounds on the number of sensing beams needed. Numerical results validate our analysis and effectiveness of our proposed beamforming designs.","authors":["Jiayi Yao","Shuowen Zhang"],"url":"https://arxiv.org/abs/2503.03560"}
{"created":"2025-05-06","title":"Activation Space Interventions Can Be Transferred Between Large Language Models","abstract":"The study of representation universality in AI models reveals growing convergence across domains, modalities, and architectures. However, the practical applications of representation universality remain largely unexplored. We bridge this gap by demonstrating that safety interventions can be transferred between models through learned mappings of their shared activation spaces. We demonstrate this approach on two well-established AI safety tasks: backdoor removal and refusal of harmful prompts, showing successful transfer of steering vectors that alter the models' outputs in a predictable way. Additionally, we propose a new task, \\textit{corrupted capabilities}, where models are fine-tuned to embed knowledge tied to a backdoor. This tests their ability to separate useful skills from backdoors, reflecting real-world challenges. Extensive experiments across Llama, Qwen and Gemma model families show that our method enables using smaller models to efficiently align larger ones. Furthermore, we demonstrate that autoencoder mappings between base and fine-tuned models can serve as reliable ``lightweight safety switches\", allowing dynamic toggling between model behaviors.","authors":["Narmeen Oozeer","Dhruv Nathawani","Nirmalendu Prakash","Michael Lan","Abir Harrasse","Amirali Abdullah"],"url":"https://arxiv.org/abs/2503.04429"}
{"created":"2025-05-06","title":"Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice","abstract":"The rapid proliferation of Large Language Models (LLMs) has raised significant trustworthiness and ethical concerns. Despite the widespread adoption of LLMs across domains, there is still no clear consensus on how to define and operationalise trustworthiness. This study aims to bridge the gap between theoretical discussion and practical implementation by analysing research trends, definitions of trustworthiness, and practical techniques. We conducted a bibliometric mapping analysis of 2,006 publications from Web of Science (2019-2025) using the Bibliometrix, and manually reviewed 68 papers. We found a shift from traditional AI ethics discussion to LLM trustworthiness frameworks. We identified 18 different definitions of trust/trustworthiness, with transparency, explainability and reliability emerging as the most common dimensions. We identified 20 strategies to enhance LLM trustworthiness, with fine-tuning and retrieval-augmented generation (RAG) being the most prominent. Most of the strategies are developer-driven and applied during the post-training phase. Several authors propose fragmented terminologies rather than unified frameworks, leading to the risks of \"ethics washing,\" where ethical discourse is adopted without a genuine regulatory commitment. Our findings highlight: persistent gaps between theoretical taxonomies and practical implementation, the crucial role of the developer in operationalising trust, and call for standardised frameworks and stronger regulatory measures to enable trustworthy and ethical deployment of LLMs.","authors":["Jos\\'e Siqueira de Cerqueira","Kai-Kristian Kemell","Rebekah Rousi","Nannan Xi","Juho Hamari","Pekka Abrahamsson"],"url":"https://arxiv.org/abs/2503.04785"}
{"created":"2025-05-06","title":"Generative Trajectory Stitching through Diffusion Composition","abstract":"Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods.","authors":["Yunhao Luo","Utkarsh A. Mishra","Yilun Du","Danfei Xu"],"url":"https://arxiv.org/abs/2503.05153"}
{"created":"2025-05-06","title":"Vision-based 3D Semantic Scene Completion via Capture Dynamic Representations","abstract":"The vision-based semantic scene completion task aims to predict dense geometric and semantic 3D scene representations from 2D images. However, the presence of dynamic objects in the scene seriously affects the accuracy of the model inferring 3D structures from 2D images. Existing methods simply stack multiple frames of image input to increase dense scene semantic information, but ignore the fact that dynamic objects and non-texture areas violate multi-view consistency and matching reliability. To address these issues, we propose a novel method, CDScene: Vision-based Robust Semantic Scene Completion via Capturing Dynamic Representations. First, we leverage a multimodal large-scale model to extract 2D explicit semantics and align them into 3D space. Second, we exploit the characteristics of monocular and stereo depth to decouple scene information into dynamic and static features. The dynamic features contain structural relationships around dynamic objects, and the static features contain dense contextual spatial information. Finally, we design a dynamic-static adaptive fusion module to effectively extract and aggregate complementary features, achieving robust and accurate semantic scene completion in autonomous driving scenarios. Extensive experimental results on the SemanticKITTI, SSCBench-KITTI360, and SemanticKITTI-C datasets demonstrate the superiority and robustness of CDScene over existing state-of-the-art methods.","authors":["Meng Wang","Fan Wu","Yunchuan Qin","Ruihui Li","Zhuo Tang","Kenli Li"],"url":"https://arxiv.org/abs/2503.06222"}
{"created":"2025-05-06","title":"Pretraining Generative Flow Networks with Inexpensive Rewards for Molecular Graph Generation","abstract":"Generative Flow Networks (GFlowNets) have recently emerged as a suitable framework for generating diverse and high-quality molecular structures by learning from rewards treated as unnormalized distributions. Previous works in this framework often restrict exploration by using predefined molecular fragments as building blocks, limiting the chemical space that can be accessed. In this work, we introduce Atomic GFlowNets (A-GFNs), a foundational generative model leveraging individual atoms as building blocks to explore drug-like chemical space more comprehensively. We propose an unsupervised pre-training approach using drug-like molecule datasets, which teaches A-GFNs about inexpensive yet informative molecular descriptors such as drug-likeliness, topological polar surface area, and synthetic accessibility scores. These properties serve as proxy rewards, guiding A-GFNs towards regions of chemical space that exhibit desirable pharmacological properties. We further implement a goal-conditioned finetuning process, which adapts A-GFNs to optimize for specific target properties. In this work, we pretrain A-GFN on a subset of ZINC dataset, and by employing robust evaluation metrics we show the effectiveness of our approach when compared to other relevant baseline methods for a wide range of drug design tasks.","authors":["Mohit Pandey","Gopeshh Subbaraj","Artem Cherkasov","Martin Ester","Emmanuel Bengio"],"url":"https://arxiv.org/abs/2503.06337"}
{"created":"2025-05-06","title":"Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning","abstract":"Data heterogeneity in federated learning, characterized by a significant misalignment between local and global distributions, leads to divergent local optimization directions and hinders global model training. Existing studies mainly focus on optimizing local updates or global aggregation, but these indirect approaches demonstrate instability when handling highly heterogeneous data distributions, especially in scenarios where label skew and domain skew coexist. To address this, we propose a geometry-guided data generation method that centers on simulating the global embedding distribution locally. We first introduce the concept of the geometric shape of an embedding distribution and then address the challenge of obtaining global geometric shapes under privacy constraints. Subsequently, we propose GGEUR, which leverages global geometric shapes to guide the generation of new samples, enabling a closer approximation to the ideal global distribution. In single-domain scenarios, we augment samples based on global geometric shapes to enhance model generalization; in multi-domain scenarios, we further employ class prototypes to simulate the global distribution across domains. Extensive experimental results demonstrate that our method significantly enhances the performance of existing approaches in handling highly heterogeneous data, including scenarios with label skew, domain skew, and their coexistence. Code published at: https://github.com/WeiDai-David/2025CVPR_GGEUR","authors":["Yanbiao Ma","Wei Dai","Wenke Huang","Jiayi Chen"],"url":"https://arxiv.org/abs/2503.06457"}
{"created":"2025-05-06","title":"A Survey on the Landscape of Self-adaptive Cloud Design and Operations Patterns: Goals, Strategies, Tooling, Evaluation and Dataset Perspectives","abstract":"Cloud-native applications have significantly advanced the development and scalability of online services through the use of microservices and modular architectures. However, achieving adaptability, resilience, and efficient performance management within cloud environments remains a key challenge. This survey provides an overview of self-adaptive cloud design and operations patterns published over the last seven years, focusing on a taxonomy of their objectives, scope of control, decision-making mechanisms approach, automation level and validation methodologies. Overall, 96 papers have been taken under consideration, indicating a significant increase in the years since 2023 in the produced output. The analysis highlights the prevalence of feedback loop structures, with both reactive and proactive implementations, and underscores the increasing role of machine learning techniques in predictive management, especially when it comes to resource provisioning and management of the executed applications. On the other hand, adaptive application architectures through direct application-level pattern-based management seem significantly underrepresented in the current field of research, thus serving as an uninvestigated area for future research. Furthermore, the current work highlights practical aspects such as validation datasets per category (application, resource, network, etc.), tools, technologies and frameworks usage during the experimentation, in order to guide researchers in the validation process for comparative and robust experimentation.","authors":["Apostolos Angelis","George Kousiouris"],"url":"https://arxiv.org/abs/2503.06705"}
{"created":"2025-05-06","title":"An Analysis of Safety Guarantees in Multi-Task Bayesian Optimization","abstract":"This paper addresses the integration of additional information sources into a Bayesian optimization framework while ensuring that safety constraints are satisfied. The interdependencies between these information sources are modeled using an unknown correlation matrix. We explore how uniform error bounds must be adjusted to maintain constraint satisfaction throughout the optimization process, considering both Bayesian and frequentist statistical perspectives. This is achieved by appropriately scaling the error bounds based on a confidence interval that can be estimated from the data. Furthermore, the efficacy of the proposed approach is demonstrated through experiments on two benchmark functions and a controller parameter optimization problem. Our results highlight a significant improvement in sample efficiency, demonstrating the methods suitability for optimizing expensive-to-evaluate functions.","authors":["Jannis O. Luebsen","Annika Eichler"],"url":"https://arxiv.org/abs/2503.08555"}
{"created":"2025-05-06","title":"Un-Straightening Generative AI: How Queer Artists Surface and Challenge the Normativity of Generative AI Models","abstract":"Queer people are often discussed as targets of bias, harm, or discrimination in research on generative AI. However, the specific ways that queer people engage with generative AI, and thus possible uses that support queer people, have yet to be explored. We conducted a workshop study with 13 queer artists, during which we gave participants access to GPT-4 and DALL-E 3 and facilitated group sensemaking activities. We found our participants struggled to use these models due to various normative values embedded in their designs, such as hyper-positivity and anti-sexuality. We describe various strategies our participants developed to overcome these models' limitations and how, nevertheless, our participants found value in these highly-normative technologies. Drawing on queer feminist theory, we discuss implications for the conceptualization of \"state-of-the-art\" models and consider how FAccT researchers might support queer alternatives.","authors":["Jordan Taylor","Joel Mire","Franchesca Spektor","Alicia DeVrio","Maarten Sap","Haiyi Zhu","Sarah Fox"],"url":"https://arxiv.org/abs/2503.09805"}
{"created":"2025-05-06","title":"Reinforcement Learning and Life Cycle Assessment for a Circular Economy -- Towards Progressive Computer Science","abstract":"The aim of this paper is to discuss the potential of using methods from Reinforcement Learning for Life Cycle Assessment in a circular economy, and to present some new ideas in this direction. To give some context, we explain how Reinforcement Learning was successfully applied in computer chess (and beyond). As computer chess was historically called the \"drosophila of AI\", we start by describing a method for the board representation called 'rotated bitboards' that can potentially also be applied in the context of sustainability. In the first part of this paper, the concepts of the bitboard-representation and the advantages of (rotated) bitboards in move generation are explained. In order to illustrate those ideas practice, the concrete implementation of the move-generator in FUSc# (a chess engine developed at FU Berlin in C# some years ago) is described. In addition, rotated binary neural networks are discussed briefly.","authors":["Johannes Buchner"],"url":"https://arxiv.org/abs/2503.10822"}
{"created":"2025-05-06","title":"The Junction of Immersive Analytics and Virtual Reconstructions -- A Case Study on the Mausoleum of Emperor Maxentius","abstract":"Virtual Archaeology has significantly evolved over the last decades through advancements in data acquisition and representation by, e.g., improved data recording technologies and virtual reality devices. Immersive environments provide novel ways to present historical events or objects with high visual quality for both, the general public as well as researchers. Here, we examine how the emerging field of Immersive Analytics can contribute to enhancing the understanding and exploration of archaeological data and explore the junction of Virtual Archaeology and Immersive Analytics, utilising reconstructions of the mausoleum of the late Roman emperor Maxentius in Rome for argumentation. Based on our work, we advocate the value of combining historical and computer science expertise for virtual reconstructions and immersive environments to facilitate deeper understanding and interactive exploration of archaeological data.","authors":["Wilhelm Kerle-Malcharek","Niklas Hann-von-Weyhern","Ulf Hailer","Steffen Diefenbach","Stefan P Feyer","Karsten Klein","Falk Schreiber"],"url":"https://arxiv.org/abs/2503.11653"}
{"created":"2025-05-06","title":"In vivo validation of Wireless Power Transfer System for Magnetically Controlled Robotic Capsule Endoscopy","abstract":"This paper presents the in vivo validation of an inductive wireless power transfer (WPT) system integrated for the first time into a magnetically controlled robotic capsule endoscopy platform. The proposed system enables continuous power delivery to the capsule without the need for onboard batteries, thus extending operational time and reducing size constraints. The WPT system operates through a resonant inductive coupling mechanism, based on a transmitting coil mounted on the end effector of a robotic arm that also houses an external permanent magnet and a localization coil for precise capsule manipulation. To ensure robust and stable power transmission in the presence of coil misalignment and rotation, a 3D receiving coil is integrated within the capsule. Additionally, a closed-loop adaptive control system, based on load-shift keying (LSK) modulation, dynamically adjusts the transmitted power to optimize efficiency while maintaining compliance with specific absorption rate (SAR) safety limits. The system has been extensively characterized in laboratory settings and validated through in vivo experiments using a porcine model, demonstrating reliable power transfer and effective robotic navigation in realistic gastrointestinal conditions: the average received power was 110 mW at a distance of 9 cm between the coils, with variable capsule rotation angles. The results confirm the feasibility of the proposed WPT approach for autonomous, battery-free robotic capsule endoscopy, paving the way for enhanced diagnostic in gastrointestinal medicine.","authors":["Alessandro Catania","Michele Bertozzi","Nikita J. Greenidge","Benjamin Calme","Gabriele Bandini","Christian Sbrana","Roberto Cecchi","Alice Buffi","Massimo Macucci","Sebastiano Strangio","Pietro Valdastri","Giuseppe Iannaccone"],"url":"https://arxiv.org/abs/2503.12850"}
{"created":"2025-05-06","title":"A Survey on Personalized Alignment -- The Missing Piece for Large Language Models in Real-World Applications","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities, yet their transition to real-world applications reveals a critical limitation: the inability to adapt to individual preferences while maintaining alignment with universal human values. Current alignment techniques adopt a one-size-fits-all approach that fails to accommodate users' diverse backgrounds and needs. This paper presents the first comprehensive survey of personalized alignment-a paradigm that enables LLMs to adapt their behavior within ethical boundaries based on individual preferences. We propose a unified framework comprising preference memory management, personalized generation, and feedback-based alignment, systematically analyzing implementation approaches and evaluating their effectiveness across various scenarios. By examining current techniques, potential risks, and future challenges, this survey provides a structured foundation for developing more adaptable and ethically-aligned LLMs.","authors":["Jian Guan","Junfei Wu","Jia-Nan Li","Chuanqi Cheng","Wei Wu"],"url":"https://arxiv.org/abs/2503.17003"}
{"created":"2025-05-06","title":"Research impact evaluation based on effective authorship contribution sensitivity: h-leadership index","abstract":"The evaluation of a researcher's performance has traditionally relied on various bibliometric measures, with the h-index being one of the most prominent. However, the h-index only accounts for the number of citations received in a publication and does not account for other factors such as the number of authors or their specific contributions in collaborative works. Therefore, the h-index has been placed on scrutiny as it has motivated academic integrity issues where non-contributing authors get authorship merely for raising their h-index. In this study, we comprehensively evaluate existing metrics in their ability to account for authorship contribution by their position and introduce a novel variant of the h-index, known as the h-leadership index. The h-leadership index aims to advance the fair evaluation of academic contributions in multi-authored publications by giving importance to authorship position beyond the first and last authors, motivated by Stanford's ranking of the top 2 \\% of world scientists. We assign weighted citations based on a modified complementary unit Gaussian curve, ensuring that the contributions of middle authors are appropriately recognised. We apply the h-leadership index to analyse the top 50 researchers across the Group of 8 (Go8) universities in Australia, demonstrating its potential to provide a more balanced assessment of research performance. We provide open-source software for extending the work further.","authors":["Hardik A. Jain","Rohitash Chandra"],"url":"https://arxiv.org/abs/2503.18236"}
{"created":"2025-05-06","title":"UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows","abstract":"Reduced order models (ROMs) play a critical role in fluid mechanics by providing low-cost predictions, making them an attractive tool for engineering applications. However, for ROMs to be widely applicable, they must not only generalise well across different regimes, but also provide a measure of confidence in their predictions. While recent data-driven approaches have begun to address nonlinear reduction techniques to improve predictions in transient environments, challenges remain in terms of robustness and parametrisation. In this work, we present a nonlinear reduction strategy specifically designed for transient flows that incorporates parametrisation and uncertainty quantification. Our reduction strategy features a variational auto-encoder (VAE) that uses variational inference for confidence measurement. We use a latent space transformer that incorporates recent advances in attention mechanisms to predict dynamical systems. Attention's versatility in learning sequences and capturing their dependence on external parameters enhances generalisation across a wide range of dynamics. Prediction, coupled with confidence, enables more informed decision making and addresses the need for more robust models. In addition, this confidence is used to cost-effectively sample the parameter space, improving model performance a priori across the entire parameter space without requiring evaluation data for the entire domain.","authors":["Isma\\\"el Zighed","Nicolas Thome","Patrick Gallinari","Taraneh Sayadi"],"url":"https://arxiv.org/abs/2503.23236"}
{"created":"2025-05-06","title":"Filtering with Time-frequency Analysis: An Adaptive and Lightweight Model for Sequential Recommender Systems Based on Discrete Wavelet Transform","abstract":"Sequential Recommender Systems (SRS) aim to model sequential behaviors of users to capture their interests which usually evolve over time. Transformer-based SRS have achieved distinguished successes recently. However, studies reveal self-attention mechanism in Transformer-based models is essentially a low-pass filter and ignores high frequency information potentially including meaningful user interest patterns. This motivates us to seek better filtering technologies for SRS, and finally we find Discrete Wavelet Transform (DWT), a famous time-frequency analysis technique from digital signal processing field, can effectively process both low-frequency and high-frequency information. We design an adaptive time-frequency filter with DWT technique, which decomposes user interests into multiple signals with different frequency and time, and can automatically learn weights of these signals. Furthermore, we develop DWTRec, a model for sequential recommendation all based on the adaptive time-frequency filter. Thanks to fast DWT technique, DWTRec has a lower time complexity and space complexity theoretically, and is Proficient in modeling long sequences. Experiments show that our model outperforms state-of-the-art baseline models in datasets with different domains, sparsity levels and average sequence lengths. Especially, our model shows great performance increase in contrast with previous models when the sequence grows longer, which demonstrates another advantage of our model.","authors":["Sheng Lu","Mingxi Ge","Jiuyi Zhang","Wanli Zhu","Guanjin Li","Fangming Gu"],"url":"https://arxiv.org/abs/2503.23436"}
{"created":"2025-05-06","title":"Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement","abstract":"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.","authors":["Yuqiao Tan","Shizhu He","Huanxuan Liao","Jun Zhao","Kang Liu"],"url":"https://arxiv.org/abs/2503.23895"}
{"created":"2025-05-06","title":"A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?","abstract":"As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&amp;A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/","authors":["Qiyuan Zhang","Fuyuan Lyu","Zexu Sun","Lei Wang","Weixu Zhang","Wenyue Hua","Haolun Wu","Zhihan Guo","Yufei Wang","Niklas Muennighoff","Irwin King","Xue Liu","Chen Ma"],"url":"https://arxiv.org/abs/2503.24235"}
{"created":"2025-05-06","title":"SonarSplat: Novel View Synthesis of Imaging Sonar via Gaussian Splatting","abstract":"In this paper, we present SonarSplat, a novel Gaussian splatting framework for imaging sonar that demonstrates realistic novel view synthesis and models acoustic streaking phenomena. Our method represents the scene as a set of 3D Gaussians with acoustic reflectance and saturation properties. We develop a novel method to efficiently rasterize Gaussians to produce a range/azimuth image that is faithful to the acoustic image formation model of imaging sonar. In particular, we develop a novel approach to model azimuth streaking in a Gaussian splatting framework. We evaluate SonarSplat using real-world datasets of sonar images collected from an underwater robotic platform in a controlled test tank and in a real-world river environment. Compared to the state-of-the-art, SonarSplat offers improved image synthesis capabilities (+3.2 dB PSNR) and more accurate 3D reconstruction (52% lower Chamfer Distance). We also demonstrate that SonarSplat can be leveraged for azimuth streak removal.","authors":["Advaith V. Sethuraman","Max Rucker","Onur Bagoren","Pou-Chun Kung","Nibarkavi N. B. Amutha","Katherine A. Skinner"],"url":"https://arxiv.org/abs/2504.00159"}
{"created":"2025-05-06","title":"GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments","abstract":"The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes: a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce GraphMaster, the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments.","authors":["Enjun Du","Xunkai Li","Tian Jin","Zhihan Zhang","Rong-Hua Li","Guoren Wang"],"url":"https://arxiv.org/abs/2504.00711"}
{"created":"2025-05-06","title":"Curbing the Ramifications of Authorship Abuse in Science","abstract":"Research performance is often measured using bibliometric indicators, such as publication count, total citations, and $h$-index. These metrics influence career advancements, salary adjustments, administrative opportunities, funding prospects, and professional recognition. However, the reliance on these metrics has also made them targets for manipulation, misuse, and abuse. One primary ethical concern is authorship abuse, which includes paid, ornamental, exploitative, cartel, and colonial authorship. These practices are prevalent because they artificially enhance multiple bibliometric indicators all at once. Our study confirms a significant rise in the mean and median number of authors per publication across multiple disciplines over the last 34 years. While it is important to identify the cases of authorship abuse, a thorough investigation of every paper proves impractical. In this study, we propose a credit allocation scheme based on the reciprocals of the Fibonacci numbers, designed to adjust credit for individual contributions while systematically reducing credit for potential authorship abuse. The proposed scheme aligns with rigorous authorship guidelines from scientific associations, which mandate significant contributions across most phases of a study, while accommodating more lenient guidelines from scientific publishers, which recognize authorship for minimal contributions. We recalibrate the traditional bibliometric indicators to emphasize author contribution rather than participation in publications. Additionally, we propose a new indicator, $T^{\\prime}$-index, to assess researchers' leading and contributing roles in their publications. Our proposed credit allocation scheme mitigates the effects of authorship abuse and promotes a more ethical scientific ecosystem.","authors":["Md Somir Khan","Mehmet Engin Tozal"],"url":"https://arxiv.org/abs/2504.02769"}
{"created":"2025-05-06","title":"Noise Augmented Fine Tuning for Mitigating Hallucinations in Large Language Models","abstract":"Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&amp;B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility.","authors":["Afshin Khadangi","Amir Sartipi","Igor Tchappi","Ramin Bahmani"],"url":"https://arxiv.org/abs/2504.03302"}
{"created":"2025-05-06","title":"Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis","abstract":"Traditional diffusion models typically employ a U-Net architecture. Previous studies have unveiled the roles of attention blocks in the U-Net. However, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. In this study, we first theoretically proved that, re-weighting the outputs of the Transformer blocks within the U-Net is a \"free lunch\" for improving the signal-to-noise ratio during the sampling process. Next, we proposed Importance Probe to uncover and quantify the dynamic shifts in importance of the Transformer blocks throughout the denoising process. Finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. Experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. Our method can be seamlessly integrated into any U-Net-based architecture. Code: https://github.com/Hytidel/UNetReweighting","authors":["Xi Wang","Ziqi He","Yang Zhou"],"url":"https://arxiv.org/abs/2504.03471"}
{"created":"2025-05-06","title":"APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay","abstract":"Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source 5K synthetic data trajectories and the trained xLAM-2-fc-r models to advance research in AI agents.","authors":["Akshara Prabhakar","Zuxin Liu","Ming Zhu","Jianguo Zhang","Tulika Awalgaonkar","Shiyu Wang","Zhiwei Liu","Haolin Chen","Thai Hoang","Juan Carlos Niebles","Shelby Heinecke","Weiran Yao","Huan Wang","Silvio Savarese","Caiming Xiong"],"url":"https://arxiv.org/abs/2504.03601"}
{"created":"2025-05-06","title":"Short Video Segment-level User Dynamic Interests Modeling in Personalized Recommendation","abstract":"The rapid growth of short videos has necessitated effective recommender systems to match users with content tailored to their evolving preferences. Current video recommendation models primarily treat each video as a whole, overlooking the dynamic nature of user preferences with specific video segments. In contrast, our research focuses on segment-level user interest modeling, which is crucial for understanding how users' preferences evolve during video browsing. To capture users' dynamic segment interests, we propose an innovative model that integrates a hybrid representation module, a multi-modal user-video encoder, and a segment interest decoder. Our model addresses the challenges of capturing dynamic interest patterns, missing segment-level labels, and fusing different modalities, achieving precise segment-level interest prediction. We present two downstream tasks to evaluate the effectiveness of our segment interest modeling approach: video-skip prediction and short video recommendation. Our experiments on real-world short video datasets with diverse modalities show promising results on both tasks. It demonstrates that segment-level interest modeling brings a deep understanding of user engagement and enhances video recommendations. We also release a unique dataset that includes segment-level video data and diverse user behaviors, enabling further research in segment-level interest modeling. This work pioneers a novel perspective on understanding user segment-level preference, offering the potential for more personalized and engaging short video experiences.","authors":["Zhiyu He","Zhixin Ling","Jiayu Li","Zhiqiang Guo","Weizhi Ma","Xinchen Luo","Min Zhang","Guorui Zhou"],"url":"https://arxiv.org/abs/2504.04237"}
{"created":"2025-05-06","title":"Data-Driven Reachability Analysis for Piecewise Affine Systems","abstract":"Hybrid systems play a crucial role in modeling real-world applications where discrete and continuous dynamics interact, including autonomous vehicles, power systems, and traffic networks. Safety verification for these systems requires determining whether system states can enter unsafe regions under given initial conditions and uncertainties, a question directly addressed by reachability analysis. However, hybrid systems present unique difficulties because their state space is divided into multiple regions with distinct dynamic models, causing traditional data-driven methods to produce inadequate over-approximations of reachable sets at region boundaries where dynamics change abruptly. This paper introduces a novel approach using hybrid zonotopes for data-driven reachability analysis of piecewise affine systems. Our method addresses the boundary transition problem by developing computational algorithms that calculate the family of set models guaranteed to contain the true system trajectories. Additionally, we extend and evaluate three methods for set-based estimation that account for input-output data with measurement noise.","authors":["Peng Xie","Johannes Betz","Davide M. Raimondo","Amr Alanwar"],"url":"https://arxiv.org/abs/2504.04362"}
{"created":"2025-05-06","title":"SAM2MOT: A Novel Paradigm of Multi-Object Tracking by Segmentation","abstract":"Segment Anything 2 (SAM2) enables robust single-object tracking using segmentation. To extend this to multi-object tracking (MOT), we propose SAM2MOT, introducing a novel Tracking by Segmentation paradigm. Unlike Tracking by Detection or Tracking by Query, SAM2MOT directly generates tracking boxes from segmentation masks, reducing reliance on detection accuracy. SAM2MOT has two key advantages: zero-shot generalization, allowing it to work across datasets without fine-tuning, and strong object association, inherited from SAM2. To further improve performance, we integrate a trajectory manager system for precise object addition and removal, and a cross-object interaction module to handle occlusions. Experiments on DanceTrack, UAVDT, and BDD100K show state-of-the-art results. Notably, SAM2MOT outperforms existing methods on DanceTrack by +2.1 HOTA and +4.5 IDF1, highlighting its effectiveness in MOT. Code is available at https://github.com/TripleJoy/SAM2MOT.","authors":["Junjie Jiang","Zelin Wang","Manqi Zhao","Yin Li","DongSheng Jiang"],"url":"https://arxiv.org/abs/2504.04519"}
{"created":"2025-05-06","title":"\"Trust me on this\" Explaining Agent Behavior to a Human Terminator","abstract":"Consider a setting where a pre-trained agent is operating in an environment and a human operator can decide to temporarily terminate its operation and take-over for some duration of time. These kind of scenarios are common in human-machine interactions, for example in autonomous driving, factory automation and healthcare. In these settings, we typically observe a trade-off between two extreme cases -- if no take-overs are allowed, then the agent might employ a sub-optimal, possibly dangerous policy. Alternatively, if there are too many take-overs, then the human has no confidence in the agent, greatly limiting its usefulness. In this paper, we formalize this setup and propose an explainability scheme to help optimize the number of human interventions.","authors":["Uri Menkes","Assaf Hallak","Ofra Amir"],"url":"https://arxiv.org/abs/2504.04592"}
{"created":"2025-05-06","title":"A Fast Multiplication Algorithm and RLWE-PLWE Equivalence for the Maximal Real Subfield of the $2^r p^s$-th Cyclotomic Field","abstract":"This paper proves the RLWE-PLWE equivalence for the maximal real subfields of the cyclotomic fields with conductor $n = 2^r p^s$, where $p$ is an odd prime, and $r \\geq 0$ and $s \\geq 1$ are integers. In particular, we show that the canonical embedding as a linear transform has a condition number bounded above by a polynomial in $n$. In addition, we describe a fast multiplication algorithm in the ring of integers of these real subfields. The multiplication algorithm uses the fast Discrete Cosine Transform (DCT) and has computational complexity $\\mathcal{O}(n \\log n)$. Both the proof of the RLWE-PLWE equivalence and the fast multiplication algorithm are generalizations of previous results by Ahola et al., where the same claims are proved for a single prime $p = 3$.","authors":["Wilmar Bola\\~nos","Antti Haavikko","Rodrigo Mart\\'in S\\'anchez-Ledesma"],"url":"https://arxiv.org/abs/2504.05159"}
{"created":"2025-05-06","title":"MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation","abstract":"The accurate segmentation of coronary Digital Subtraction Angiography (DSA) images is essential for diagnosing and treating coronary artery diseases. Despite advances in deep learning-based segmentation, challenges such as low contrast, noise, overlapping structures, high intra-class variance, and class imbalance limit precise vessel delineation. To overcome these limitations, we propose the MSA-UNet3+: a Multi-Scale Attention enhanced UNet3+ architecture for coronary DSA image segmentation. The framework combined Multi-Scale Dilated Bottleneck (MSD-Bottleneck) with Contextual Attention Fusion Module (CAFM), which not only enhances multi-scale feature extraction but also preserve fine-grained details, and improve contextual understanding. Furthermore, we propose a new Supervised Prototypical Contrastive Loss (SPCL), which combines supervised and prototypical contrastive learning to minimize class imbalance and high intra-class variance by focusing on hard-to-classified background samples. Experiments carried out on a private coronary DSA dataset demonstrate that MSA-UNet3+ outperforms state-of-the-art methods, achieving a Dice coefficient of 87.73%, an F1-score of 87.78%, and significantly reduced Average Surface Distance (ASD) and Average Contour Distance (ACD). The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at the following GitHub profile link https://github.com/rayanmerghani/MSA-UNet3plus.","authors":["Rayan Merghani Ahmed","Adnan Iltaf","Bin Li","Shoujun Zhou"],"url":"https://arxiv.org/abs/2504.05184"}
{"created":"2025-05-06","title":"P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation","abstract":"Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.","authors":["Yong Ren","Jiangyan Yi","Tao Wang","Jianhua Tao","Zheng Lian","Zhengqi Wen","Chenxing Li","Ruibo Fu","Ye Bai","Xiaohui Zhang"],"url":"https://arxiv.org/abs/2504.05197"}
{"created":"2025-05-06","title":"RobustDexGrasp: Robust Dexterous Grasping of General Objects","abstract":"The ability to robustly grasp a variety of objects is essential for dexterous robots. In this paper, we present a framework for zero-shot dynamic dexterous grasping using single-view visual inputs, designed to be resilient to various disturbances. Our approach utilizes a hand-centric object shape representation based on dynamic distance vectors between finger joints and object surfaces. This representation captures the local shape around potential contact regions rather than focusing on detailed global object geometry, thereby enhancing generalization to shape variations and uncertainties. To address perception limitations, we integrate a privileged teacher policy with a mixed curriculum learning approach, allowing the student policy to effectively distill grasping capabilities and explore for adaptation to disturbances. Trained in simulation, our method achieves success rates of 97.0% across 247,786 simulated objects and 94.6% across 512 real objects, demonstrating remarkable generalization. Quantitative and qualitative results validate the robustness of our policy against various disturbances.","authors":["Hui Zhang","Zijian Wu","Linyi Huang","Sammy Christen","Jie Song"],"url":"https://arxiv.org/abs/2504.05287"}
{"created":"2025-05-06","title":"TabKAN: Advancing Tabular Data Analysis using Kolmogorov-Arnold Network","abstract":"Tabular data analysis presents unique challenges due to its heterogeneous feature types, missing values, and complex interactions. While traditional machine learning methods, such as gradient boosting, often outperform deep learning approaches, recent advancements in neural architectures offer promising alternatives. This paper introduces TabKAN, a novel framework that advances tabular data modeling using Kolmogorov-Arnold Networks (KANs). Unlike conventional deep learning models, KANs leverge learnable activation functions on edges, which improve both interpretability and training efficiency. Our contributions include: (1) the introduction of modular KAN-based architectures for tabular data analysis, (2) the development of a transfer learning framework for KAN models that supports knowledge transfer between domains, (3) the development of model-specific interpretability for tabular data learning, which reduces dependence on post hoc and model-agnostic analysis, and (4) comprehensive evaluation of vanilla supervised learning across binary and multi-class classification tasks. Through extensive benchmarking on diverse public datasets, TabKAN demonstrates superior performance in supervised learning while significantly outperforming classical and Transformer-based models in transfer learning scenarios. Our findings highlight the advantage of KAN-based architectures in transferring knowledge across domains and narrowing the gap between traditional machine learning and deep learning for structured data.","authors":["Ali Eslamian","Alireza Afzal Aghaei","Qiang Cheng"],"url":"https://arxiv.org/abs/2504.06559"}
{"created":"2025-05-06","title":"Analysis of the Unscented Transform for Cooperative Localization with Ranging-Only Information","abstract":"Cooperative localization in multi-agent robotic systems is challenging, especially when agents rely on limited information, such as only peer-to-peer range measurements. Two key challenges arise: utilizing this limited information to improve position estimation; handling uncertainties from sensor noise, nonlinearity, and unknown correlations between agents measurements; and avoiding information reuse. This paper examines the use of the Unscented Transform (UT) for state estimation for a case in which range measurement between agents and covariance intersection (CI) is used to handle unknown correlations. Unlike Kalman Filter approaches, CI methods fuse complete state and covariance estimates. This makes formulating a CI approach with ranging-only measurements a challenge. To overcome this, UT is used to handle uncertainties and formulate a cooperative state update using range measurements and current cooperative state estimates. This introduces information reuse in the measurement update. Therefore, this work aims to evaluate the limitations and utility of this formulation when faced with various levels of state measurement uncertainty and errors.","authors":["Uthman Olawoye","Cagri Kilic","Jason N Gross"],"url":"https://arxiv.org/abs/2504.07242"}
{"created":"2025-05-06","title":"ID-Booth: Identity-consistent Face Generation with Diffusion Models","abstract":"Recent advances in generative modeling have enabled the generation of high-quality synthetic data that is applicable in a variety of domains, including face recognition. Here, state-of-the-art generative models typically rely on conditioning and fine-tuning of powerful pretrained diffusion models to facilitate the synthesis of realistic images of a desired identity. Yet, these models often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present in this paper a novel generative diffusion-based framework, called ID-Booth. ID-Booth consists of a denoising network responsible for data generation, a variational auto-encoder for mapping images to and from a lower-dimensional latent space and a text encoder that allows for prompt-based control over the generation procedure. The framework utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained diffusion models. Experiments with a state-of-the-art latent diffusion model and diverse prompts reveal that our method facilitates better intra-identity consistency and inter-identity separability than competing methods, while achieving higher image diversity. In turn, the produced data allows for effective augmentation of small-scale datasets and training of better-performing recognition models in a privacy-preserving manner. The source code for the ID-Booth framework is publicly available at https://github.com/dariant/ID-Booth.","authors":["Darian Toma\\v{s}evi\\'c","Fadi Boutros","Chenhao Lin","Naser Damer","Vitomir \\v{S}truc","Peter Peer"],"url":"https://arxiv.org/abs/2504.07392"}
{"created":"2025-05-06","title":"Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies","abstract":"The Model Context Protocol (MCP), introduced by Anthropic, provides a standardized framework for artificial intelligence (AI) systems to interact with external data sources and tools in real-time. While MCP offers significant advantages for AI integration and capability extension, it introduces novel security challenges that demand rigorous analysis and mitigation. This paper builds upon foundational research into MCP architecture and preliminary security assessments to deliver enterprise-grade mitigation frameworks and detailed technical implementation strategies. Through systematic threat modeling and analysis of MCP implementations and analysis of potential attack vectors, including sophisticated threats like tool poisoning, we present actionable security patterns tailored for MCP implementers and adopters. The primary contribution of this research lies in translating theoretical security concerns into a practical, implementable framework with actionable controls, thereby providing essential guidance for the secure enterprise adoption and governance of integrated AI systems.","authors":["Vineeth Sai Narajala","Idan Habler"],"url":"https://arxiv.org/abs/2504.08623"}
{"created":"2025-05-06","title":"Safe Flow Matching: Robot Motion Planning with Control Barrier Functions","abstract":"Recent advances in generative modeling have led to promising results in robot motion planning, particularly through diffusion and flow matching (FM)-based models that capture complex, multimodal trajectory distributions. However, these methods are typically trained offline and remain limited when faced with new environments with constraints, often lacking explicit mechanisms to ensure safety during deployment. In this work, we propose Safe Flow Matching (SafeFlow), a motion planning framework, for trajectory generation that integrates flow matching with safety guarantees. SafeFlow leverages our proposed flow matching barrier functions (FMBF) to ensure the planned trajectories remain within safe regions across the entire planning horizon. Crucially, our approach enables training-free, real-time safety enforcement at test time, eliminating the need for retraining. We evaluate SafeFlow on a diverse set of tasks, including planar robot navigation and 7-DoF manipulation, demonstrating superior safety and planning performance compared to state-of-the-art generative planners. Comprehensive resources are available on the project website: https://safeflowmatching.github.io","authors":["Xiaobing Dai","Zewen Yang","Dian Yu","Shanshan Zhang","Hamid Sadeghian","Sami Haddadin","Sandra Hirche"],"url":"https://arxiv.org/abs/2504.08661"}
{"created":"2025-05-06","title":"Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model","abstract":"This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/","authors":["Team Seawead","Ceyuan Yang","Zhijie Lin","Yang Zhao","Shanchuan Lin","Zhibei Ma","Haoyuan Guo","Hao Chen","Lu Qi","Sen Wang","Feng Cheng","Feilong Zuo","Xuejiao Zeng","Ziyan Yang","Fangyuan Kong","Meng Wei","Zhiwu Qing","Fei Xiao","Tuyen Hoang","Siyu Zhang","Peihao Zhu","Qi Zhao","Jiangqiao Yan","Liangke Gui","Sheng Bi","Jiashi Li","Yuxi Ren","Rui Wang","Huixia Li","Xuefeng Xiao","Shu Liu","Feng Ling","Heng Zhang","Houmin Wei","Huafeng Kuang","Jerry Duncan","Junda Zhang","Junru Zheng","Li Sun","Manlin Zhang","Renfei Sun","Xiaobin Zhuang","Xiaojie Li","Xin Xia","Xuyan Chi","Yanghua Peng","Yuping Wang","Yuxuan Wang","Zhongkai Zhao","Zhuo Chen","Zuquan Song","Zhenheng Yang","Jiashi Feng","Jianchao Yang","Lu Jiang"],"url":"https://arxiv.org/abs/2504.08685"}
{"created":"2025-05-06","title":"VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning","abstract":"Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. Our empirical results show the effectiveness of our approaches.","authors":["Haozhe Wang","Chao Qu","Zuming Huang","Wei Chu","Fangzhen Lin","Wenhu Chen"],"url":"https://arxiv.org/abs/2504.08837"}
{"created":"2025-05-06","title":"Adaptive Additive Parameter Updates of Vision Transformers for Few-Shot Continual Learning","abstract":"Integrating new class information without losing previously acquired knowledge remains a central challenge in artificial intelligence, often referred to as catastrophic forgetting. Few-shot class incremental learning (FSCIL) addresses this by first training a model on a robust dataset of base classes and then incrementally adapting it in successive sessions using only a few labeled examples per novel class. However, this approach is prone to overfitting on the limited new data, which can compromise overall performance and exacerbate forgetting. In this work, we propose a simple yet effective novel FSCIL framework that leverages a frozen Vision Transformer (ViT) backbone augmented with parameter-efficient additive updates. Our approach freezes the pre-trained ViT parameters and selectively injects trainable weights into the self-attention modules via an additive update mechanism. This design updates only a small subset of parameters to accommodate new classes without sacrificing the representations learned during the base session. By fine-tuning a limited number of parameters, our method preserves the generalizable features in the frozen ViT while reducing the risk of overfitting. Furthermore, as most parameters remain fixed, the model avoids overwriting previously learned knowledge when small novel data batches are introduced. Extensive experiments on benchmark datasets demonstrate that our approach yields state-of-the-art performance compared to baseline FSCIL methods.","authors":["Kyle Stein","Andrew Arash Mahyari","Guillermo Francia III","Eman El-Sheikh"],"url":"https://arxiv.org/abs/2504.08982"}
{"created":"2025-05-06","title":"A Hierarchical Decomposition of Kullback-Leibler Divergence: Disentangling Marginal Mismatches from Statistical Dependencies","abstract":"The Kullback-Leibler (KL) divergence is a foundational measure for comparing probability distributions. Yet in multivariate settings, its single value often obscures the underlying reasons for divergence, conflating mismatches in individual variable distributions (marginals) with effects arising from statistical dependencies. We derive an algebraically exact, additive, and hierarchical decomposition of the KL divergence between a joint distribution P(X1,...,Xn) and a standard product reference distribution Q(X1,...,Xn) = product_i q(Xi), where variables are assumed independent and identically distributed according to a common reference q. The total divergence precisely splits into two primary components: (1) the summed divergence of each marginal distribution Pi(Xi) from the common reference q(Xi), quantifying marginal deviations; and (2) the total correlation (or multi-information), capturing the total statistical dependency among variables. Leveraging Mobius inversion on the subset lattice, we further decompose this total correlation term into a hierarchy of signed contributions from distinct pairwise, triplet, and higher-order statistical interactions, expressed using standard Shannon information quantities. This decomposition provides an algebraically complete and interpretable breakdown of KL divergence using established information measures, requiring no approximations or model assumptions. Numerical validation using hypergeometric sampling confirms the decomposition's exactness to machine precision across diverse system configurations.","authors":["William Cook"],"url":"https://arxiv.org/abs/2504.09029"}
{"created":"2025-05-06","title":"Better Estimation of the KL Divergence Between Language Models","abstract":"Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient.","authors":["Afra Amini","Tim Vieira","Ryan Cotterell"],"url":"https://arxiv.org/abs/2504.10637"}
{"created":"2025-05-06","title":"Beyond Coordinates: Meta-Equivariance in Statistical Inference","abstract":"Optimal statistical decisions should transcend the language used to describe them. Yet, how do we guarantee that the choice of coordinates - the parameterisation of an optimisation problem - does not subtly dictate the solution? This paper reveals a fundamental geometric invariance principle. We first analyse the optimal combination of two asymptotically normal estimators under a strictly convex trace-AMSE risk. While methods for finding optimal weights are known, we prove that the resulting optimal estimator is invariant under direct affine reparameterisations of the weighting scheme. This exemplifies a broader principle we term meta-equivariance: the unique minimiser of any strictly convex, differentiable scalar objective over a matrix space transforms covariantly under any invertible affine reparameterisation of that space. Distinct from classical statistical equivariance tied to data symmetries, meta-equivariance arises from the immutable geometry of convex optimisation itself. It guarantees that optimality, in these settings, is not an artefact of representation but an intrinsic, coordinate-free truth.","authors":["William Cook"],"url":"https://arxiv.org/abs/2504.10667"}
{"created":"2025-05-06","title":"Mind2Matter: Creating 3D Models from EEG Signals","abstract":"The reconstruction of 3D objects from brain signals has gained significant attention in brain-computer interface (BCI) research. Current research predominantly utilizes functional magnetic resonance imaging (fMRI) for 3D reconstruction tasks due to its excellent spatial resolution. Nevertheless, the clinical utility of fMRI is limited by its prohibitive costs and inability to support real-time operations. In comparison, electroencephalography (EEG) presents distinct advantages as an affordable, non-invasive, and mobile solution for real-time brain-computer interaction systems. While recent advances in deep learning have enabled remarkable progress in image generation from neural data, decoding EEG signals into structured 3D representations remains largely unexplored. In this paper, we propose a novel framework that translates EEG recordings into 3D object reconstructions by leveraging neural decoding techniques and generative models. Our approach involves training an EEG encoder to extract spatiotemporal visual features, fine-tuning a large language model to interpret these features into descriptive multimodal outputs, and leveraging generative 3D Gaussians with layout-guided control to synthesize the final 3D structures. Experiments demonstrate that our model captures salient geometric and semantic features, paving the way for applications in brain-computer interfaces (BCIs), virtual reality, and neuroprosthetics. Our code is available in https://github.com/sddwwww/Mind2Matter.","authors":["Xia Deng","Shen Chen","Jiale Zhou","Lei Li"],"url":"https://arxiv.org/abs/2504.11936"}
{"created":"2025-05-06","title":"Cobra: Efficient Line Art COlorization with BRoAder References","abstract":"The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.","authors":["Junhao Zhuang","Lingen Li","Xuan Ju","Zhaoyang Zhang","Chun Yuan","Ying Shan"],"url":"https://arxiv.org/abs/2504.12240"}
{"created":"2025-05-06","title":"Dysarthria Normalization via Local Lie Group Transformations for Robust ASR","abstract":"We present a geometry-driven method for normalizing dysarthric speech by modeling time, frequency, and amplitude distortions as smooth, local Lie group transformations of spectrograms. Scalar fields generate these deformations via exponential maps, and a neural network is trained - using only synthetically warped healthy speech - to infer the fields and apply an approximate inverse at test time. We introduce a spontaneous-symmetry-breaking (SSB) potential that encourages the model to discover non-trivial field configurations. On real pathological speech, the system delivers consistent gains: up to 17 percentage-point WER reduction on challenging TORGO utterances and a 16 percent drop in WER variance, with no degradation on clean CommonVoice data. Character and phoneme error rates improve in parallel, confirming linguistic relevance. Our results demonstrate that geometrically structured warping provides consistent, zero-shot robustness gains for dysarthric ASR.","authors":["Mikhail Osipov"],"url":"https://arxiv.org/abs/2504.12279"}
{"created":"2025-05-06","title":"Inference-friendly Graph Compression for Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have demonstrated promising performance in graph analysis. Nevertheless, the inference process of GNNs remains costly, hindering their applications for large graphs. This paper proposes inference-friendly graph compression (IFGC), a graph compression scheme to accelerate GNNs inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed graph $G_c$, to best preserve the inference results of $M$ over $G$, such that the result can be directly inferred by accessing $G_c$ with no or little decompression cost. (1) We characterize IFGC with a class of inference equivalence relation. The relation captures the node pairs in $G$ that are not distinguishable for GNN inference. (2) We introduce three practical specifications of IFGC for representative GNNs: structural preserving compression (SPGC), which computes $G_c$ that can be directly processed by GNN inference without decompression; ($\\alpha$, $r$)-compression, that allows for a configurable trade-off between compression ratio and inference quality, and anchored compression that preserves inference results for specific nodes of interest. For each scheme, we introduce compression and inference algorithms with guarantees of efficiency and quality of the inferred results. We conduct extensive experiments on diverse sets of large-scale graphs, which verifies the effectiveness and efficiency of our graph compression approaches.","authors":["Yangxin Fan","Haolai Che","Yinghui Wu"],"url":"https://arxiv.org/abs/2504.13034"}
{"created":"2025-05-06","title":"Mixed Fractional Information: Consistency of Dissipation Measures for Stable Laws","abstract":"Symmetric alpha-stable (S alpha S) distributions with alpha<2 lack finite classical Fisher information. Building on Johnson's framework, we define Mixed Fractional Information (MFI) via the initial rate of relative entropy dissipation during interpolation between S alpha S laws with differing scales, v and s. We demonstrate two equivalent formulations for MFI in this specific S alpha S-to-S alpha S setting. The first involves the derivative D'(v) of the relative entropy between the two S alpha S densities. The second uses an integral expectation E_gv[u(x,0) (pF_v(x) - pF_s(x))] involving the difference between Fisher scores (pF_v, pF_s) and a specific MMSE-related score function u(x,0) derived from the interpolation dynamics. Our central contribution is a rigorous proof of the consistency identity: D'(v) = (1/(alpha v)) E_gv[X (pF_v(X) - pF_s(X))]. This identity mathematically validates the equivalence of the two MFI formulations for S alpha S inputs, establishing MFI's internal coherence and directly linking entropy dissipation rates to score function differences. We further establish MFI's non-negativity (zero if and only if v=s), derive its closed-form expression for the Cauchy case (alpha=1), and numerically validate the consistency identity. MFI provides a finite, coherent, and computable information-theoretic measure for comparing S alpha S distributions where classical Fisher information fails, connecting entropy dynamics to score functions and estimation concepts. This work lays a foundation for exploring potential fractional I-MMSE relations and new functional inequalities tailored to heavy-tailed systems.","authors":["William Cook"],"url":"https://arxiv.org/abs/2504.13423"}
{"created":"2025-05-06","title":"Entropic Time Schedulers for Generative Diffusion Models","abstract":"The practical performance of generative diffusion models depends on the appropriate choice of the noise scheduling function, which can also be equivalently expressed as a time reparameterization. In this paper, we present a time scheduler that selects sampling points based on entropy rather than uniform time spacing, ensuring that each point contributes an equal amount of information to the final generation. We prove that this time reparameterization does not depend on the initial choice of time. Furthermore, we provide a tractable exact formula to estimate this \\emph{entropic time} for a trained model using the training loss without substantial overhead. Alongside the entropic time, inspired by the optimality results, we introduce a rescaled entropic time. In our experiments with mixtures of Gaussian distributions and ImageNet, we show that using the (rescaled) entropic times greatly improves the inference performance of trained models. In particular, we found that the image quality in pretrained EDM2 models, as evaluated by FID and FD-DINO scores, can be substantially increased by the rescaled entropic time reparameterization without increasing the number of function evaluations, with greater improvements in the few NFEs regime.","authors":["Dejan Stancevic","Luca Ambrogioni"],"url":"https://arxiv.org/abs/2504.13612"}
{"created":"2025-05-06","title":"DiffOG: Differentiable Policy Trajectory Optimization with Generalizability","abstract":"Imitation learning-based visuomotor policies excel at manipulation tasks but often produce suboptimal action trajectories compared to model-based methods. Directly mapping camera data to actions via neural networks can result in jerky motions and difficulties in meeting critical constraints, compromising safety and robustness in real-world deployment. For tasks that require high robustness or strict adherence to constraints, ensuring trajectory quality is crucial. However, the lack of interpretability in neural networks makes it challenging to generate constraint-compliant actions in a controlled manner. This paper introduces differentiable policy trajectory optimization with generalizability (DiffOG), a learning-based trajectory optimization framework designed to enhance visuomotor policies. By leveraging the proposed differentiable formulation of trajectory optimization with transformer, DiffOG seamlessly integrates policies with a generalizable optimization layer. Visuomotor policies enhanced by DiffOG generate smoother, constraint-compliant action trajectories in a more interpretable way. DiffOG exhibits strong generalization capabilities and high flexibility. We evaluated DiffOG across 11 simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG significantly enhances the trajectory quality of visuomotor policies while having minimal impact on policy performance, outperforming trajectory processing baselines such as greedy constraint clipping and penalty-based trajectory optimization. Furthermore, DiffOG achieves superior performance compared to existing constrained visuomotor policy.","authors":["Zhengtong Xu","Zichen Miao","Qiang Qiu","Zhe Zhang","Yu She"],"url":"https://arxiv.org/abs/2504.13807"}
{"created":"2025-05-06","title":"Analysis of a finite element method for PDEs in evolving domains with topological changes","abstract":"The paper presents the first rigorous error analysis of an unfitted finite element method for a linear parabolic problem posed on an evolving domain $\\Omega(t)$ that may undergo a topological change, such as, for example, a domain splitting. The domain evolution is assumed to be $C^2$-smooth away from a critical time $t_c$, at which the topology may change instantaneously. To accommodate such topological transitions in the error analysis, we introduce several structural assumptions on the evolution of $\\Omega(t)$ in the vicinity of the critical time. These assumptions allow a specific stability estimate even across singularities. Based on this stability result we derive optimal-order discretization error bounds, provided the continuous solution is sufficiently smooth. We demonstrate the applicability of our assumptions with examples of level-set domains undergoing topological transitions and discuss cases where the analysis fails. The theoretical error estimate is confirmed by the results of a numerical experiment.","authors":["Maxim A. Olshanskii","Arnold Reusken"],"url":"https://arxiv.org/abs/2504.14116"}
{"created":"2025-05-06","title":"Program Synthesis From Partial Traces","abstract":"We present the first technique to synthesize programs that compose side-effecting functions, pure functions, and control flow, from partial traces containing records of only the side-effecting functions. This technique can be applied to synthesize API composing scripts from logs of calls made to those APIs, or a script from traces of system calls made by a workload, for example. All of the provided traces are positive examples, meaning that they describe desired behavior. Our approach does not require negative examples. Instead, it generalizes over the examples and uses cost metrics to prevent over-generalization. Because the problem is too complex for traditional monolithic program synthesis techniques, we propose a new combination of optimizing rewrites and syntax-guided program synthesis. The resulting program is correct by construction, so its output will always be able to reproduce the input traces. We evaluate the quality of the programs synthesized when considering various optimization metrics and the synthesizer's efficiency on real-world benchmarks. The results show that our approach can generate useful real-world programs.","authors":["Margarida Ferreira","Victor Nicolet","Joey Dodds","Daniel Kroening"],"url":"https://arxiv.org/abs/2504.14480"}
{"created":"2025-05-06","title":"Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark","abstract":"Recent advancements in language multimodal models (LMMs) for video have demonstrated their potential for understanding video content, yet the task of comprehending multi-discipline lectures remains largely unexplored. We introduce Video-MMLU, a massive benchmark designed to evaluate the capabilities of LMMs in understanding Multi-Discipline Lectures. We evaluate over 90 open-source and proprietary models, ranging from 0.5B to 40B parameters. Our results highlight the limitations of current models in addressing the cognitive challenges presented by these lectures, especially in tasks requiring both perception and reasoning. Additionally, we explore how the number of visual tokens and the large language models influence performance, offering insights into the interplay between multimodal perception and reasoning in lecture comprehension.","authors":["Enxin Song","Wenhao Chai","Weili Xu","Jianwen Xie","Yuxuan Liu","Gaoang Wang"],"url":"https://arxiv.org/abs/2504.14693"}
{"created":"2025-05-06","title":"Integrating Symbolic Execution into the Fine-Tuning of Code-Generating LLMs","abstract":"Code-generating Large Language Models (LLMs) have become essential tools in modern software development, enhancing productivity and accelerating development. This paper aims to investigate the fine-tuning of code-generating LLMs using Reinforcement Learning and Direct Preference Optimization, further improving their performance. To achieve this, we enhance the training data for the reward model with the help of symbolic execution techniques, ensuring more comprehensive and objective data. With symbolic execution, we create a custom dataset that better captures the nuances in code evaluation. Our reward models, fine-tuned on this dataset, demonstrate significant improvements over the baseline, CodeRL, in estimating the quality of generated code. Our code-generating LLMs, trained with the help of reward model feedback, achieve similar results compared to the CodeRL benchmark.","authors":["Marina Sakharova","Abhinav Anand","Mira Mezini"],"url":"https://arxiv.org/abs/2504.15210"}
{"created":"2025-05-06","title":"Observability conditions for neural state-space models with eigenvalues and their roots of unity","abstract":"We operate through the lens of ordinary differential equations and control theory to study the concept of observability in the context of neural state-space models and the Mamba architecture. We develop strategies to enforce observability, which are tailored to a learning context, specifically where the hidden states are learnable at initial time, in conjunction to over its continuum, and high-dimensional. We also highlight our methods emphasize eigenvalues, roots of unity, or both. Our methods effectuate computational efficiency when enforcing observability, sometimes at great scale. We formulate observability conditions in machine learning based on classical control theory and discuss their computational complexity. Our nontrivial results are fivefold. We discuss observability through the use of permutations in neural applications with learnable matrices without high precision. We present two results built upon the Fourier transform that effect observability with high probability up to the randomness in the learning. These results are worked with the interplay of representations in Fourier space and their eigenstructure, nonlinear mappings, and the observability matrix. We present a result for Mamba that is similar to a Hautus-type condition, but instead employs an argument using a Vandermonde matrix instead of eigenvectors. Our final result is a shared-parameter construction of the Mamba system, which is computationally efficient in high exponentiation. We develop a training algorithm with this coupling, showing it satisfies a Robbins-Monro condition under certain orthogonality, while a more classical training procedure fails to satisfy a contraction with high Lipschitz constant.","authors":["Andrew Gracyk"],"url":"https://arxiv.org/abs/2504.15758"}
{"created":"2025-05-06","title":"FairTranslate: An English-French Dataset for Gender Bias Evaluation in Machine Translation by Overcoming Gender Binarity","abstract":"Large Language Models (LLMs) are increasingly leveraged for translation tasks but often fall short when translating inclusive language -- such as texts containing the singular 'they' pronoun or otherwise reflecting fair linguistic protocols. Because these challenges span both computational and societal domains, it is imperative to critically evaluate how well LLMs handle inclusive translation with a well-founded framework.","authors":["Fanny Jourdan","Yannick Chevalier","C\\'ecile Favre"],"url":"https://arxiv.org/abs/2504.15941"}
{"created":"2025-05-06","title":"Neuroadaptive Haptics: Comparing Reinforcement Learning from Explicit Ratings and Neural Signals for Adaptive XR Systems","abstract":"Neuroadaptive haptics offers a path to more immersive extended reality (XR) experiences by dynamically tuning multisensory feedback to user preferences. We present a neuroadaptive haptics system that adapts XR feedback through reinforcement learning (RL) from explicit user ratings and brain-decoded neural signals. In a user study, participants interacted with virtual objects in VR while Electroencephalography (EEG) data were recorded. An RL agent adjusted haptic feedback based either on explicit ratings or on outputs from a neural decoder. Results show that the RL agent's performance was comparable across feedback sources, suggesting that implicit neural feedback can effectively guide personalization without requiring active user input. The EEG-based neural decoder achieved a mean F1 score of 0.8, supporting reliable classification of user experience. These findings demonstrate the feasibility of combining brain-computer interfaces (BCI) and RL to autonomously adapt XR interactions, reducing cognitive load and enhancing immersion.","authors":["Lukas Gehrke","Aleksandrs Koselevs","Marius Klug","Klaus Gramann"],"url":"https://arxiv.org/abs/2504.15984"}
{"created":"2025-05-06","title":"ForesightNav: Learning Scene Imagination for Efficient Exploration","abstract":"Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration.","authors":["Hardik Shah","Jiaxu Xing","Nico Messikommer","Boyang Sun","Marc Pollefeys","Davide Scaramuzza"],"url":"https://arxiv.org/abs/2504.16062"}
{"created":"2025-05-06","title":"PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation","abstract":"While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.","authors":["Wenxuan Li","Hang Zhao","Zhiyuan Yu","Yu Du","Qin Zou","Ruizhen Hu","Kai Xu"],"url":"https://arxiv.org/abs/2504.16693"}
{"created":"2025-05-06","title":"Building A Secure Agentic AI Application Leveraging A2A Protocol","abstract":"As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies.","authors":["Idan Habler","Ken Huang","Vineeth Sai Narajala","Prashant Kulkarni"],"url":"https://arxiv.org/abs/2504.16902"}
{"created":"2025-05-06","title":"On Runge-Kutta methods of order 10","abstract":"A family of explicit 15-stage Runge-Kutta methods of order 10 is derived.","authors":["Misha Stepanov"],"url":"https://arxiv.org/abs/2504.17329"}
{"created":"2025-05-06","title":"PHast -- Perfect Hashing with fast evaluation","abstract":"Perfect hash functions give unique \"names\" to arbitrary keys requiring only a few bits per key. This is an essential building block in applications like static hash tables, databases, or bioinformatics. This paper introduces the PHast approach that has the currently fastest query time with competitive construction time and space consumption. PHast improves bucket-placement which first hashes each key k to a bucket, and then looks for the bucket seed s such that a secondary hash function maps pairs (s,k) in a collision-free way. PHast can use small-range primary hash functions with linear mapping, fixed-width encoding of seeds, and parallel construction. This is achieved using small overlapping slices of allowed values and bumping to handle unsuccessful seed assignment.","authors":["Piotr Beling","Peter Sanders"],"url":"https://arxiv.org/abs/2504.17918"}
{"created":"2025-05-06","title":"Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study","abstract":"This paper investigates the limitations of neural operators in learning solutions for a Hughes model, a first-order hyperbolic conservation law system for crowd dynamics. The model couples a Fokker-Planck equation representing pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes model belongs to the class of nonlinear hyperbolic systems that often exhibit complex solution structures, including shocks and discontinuities. In this study, we assess the performance of three state-of-the-art neural operators (Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural Operator) in various challenging scenarios. Specifically, we consider (1) discontinuous and Gaussian initial conditions and (2) diverse boundary conditions, while also examining the impact of different numerical schemes.","authors":["Prajwal Chauhan","Salah Eddine Choutri","Mohamed Ghattassi","Nader Masmoudi","Saif Eddin Jabari"],"url":"https://arxiv.org/abs/2504.18267"}
{"created":"2025-05-06","title":"Pseudo-Boolean Proof Logging for Optimal Classical Planning","abstract":"We introduce lower-bound certificates for classical planning tasks, which can be used to prove the unsolvability of a task or the optimality of a plan in a way that can be verified by an independent third party. We describe a general framework for generating lower-bound certificates based on pseudo-Boolean constraints, which is agnostic to the planning algorithm used.","authors":["Simon Dold","Malte Helmert","Jakob Nordstr\\\"om","Gabriele R\\\"oger","Tanja Schindler"],"url":"https://arxiv.org/abs/2504.18443"}
{"created":"2025-05-06","title":"RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects","abstract":"We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.","authors":["Georgios Kouros","Minye Wu","Tinne Tuytelaars"],"url":"https://arxiv.org/abs/2504.18468"}
{"created":"2025-05-06","title":"Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots","abstract":"Hierarchical reinforcement learning (HRL) is hypothesized to be able to take advantage of the inherent hierarchy in robot learning tasks with sparse reward schemes, in contrast to more traditional reinforcement learning algorithms. In this research, hierarchical reinforcement learning is evaluated and contrasted with standard reinforcement learning in complex navigation tasks. We evaluate unique characteristics of HRL, including their ability to create sub-goals and the termination function. We constructed experiments to test the differences between PPO and HRL, different ways of creating sub-goals, manual vs automatic sub-goal creation, and the effects of the frequency of termination on performance. These experiments highlight the advantages of HRL and how it achieves these advantages.","authors":["Brendon Johnson","Alfredo Weitzenfeld"],"url":"https://arxiv.org/abs/2504.18794"}
{"created":"2025-05-06","title":"Cubing for Tuning","abstract":"We are exploring the problem of building an automated reasoning procedure that adaptively tunes the high-level solving strategy for a given problem. There are two main distinctive characteristics of our approach: tuning is performed solely online, unlike the common use of tuning as an offline process; and tuning data comes exclusively from the given instance, so we do not rely on the availability of similar benchmarks and can work with unique challenging instances. Our approach builds on top of the divide-and-conquer paradigm that naturally serves partitioned sub-problems for an automated tuning algorithm to obtain a good solving strategy. We demonstrate performance improvement on two classes of important problems--SAT-solving and neural network verification--and show that our method can learn unconventional solving strategies in some cases.","authors":["Haoze Wu","Clark Barrett","Nina Narodytska"],"url":"https://arxiv.org/abs/2504.19039"}
{"created":"2025-05-06","title":"SA-MIMO: Scalable Quantum-Based Wireless Communications","abstract":"Rydberg atomic receivers offer a quantum-native alternative to conventional RF front-ends by directly detecting electromagnetic fields via highly excited atomic states. While their quantum-limited sensitivity and hardware simplicity make them promising for future wireless systems, extending their use to scalable multi-antenna and multi-carrier configurations, termed Scalable Atomic-MIMO (SA-MIMO), remains largely unexplored. This paper introduces a novel RF transmitter-atomic receiver architecture that addresses this gap. The core idea lies in a novel modulation technique called Phase-Rotated Symbol Spreading (PRSS), which transforms the nonlinear phase retrieval problem inherent to atomic detection into a tractable linear demultiplexing task. PRSS enables efficient signal processing and supports scalable MUX/DeMUX operations in both atomic MIMO and atomic OFDM systems. Simulation results show that the proposed system achieves up to 2.5 dB gain under optimal maximum-likelihood detection and over 10 dB under suboptimal detection in MIMO settings. These results establish PRSS assisted SA-MIMO as a promising architecture for realizing high-sensitivity, interference-resilient atomic wireless communication.","authors":["Jiuyu Liu","Yi Ma","Rahim Tafazolli"],"url":"https://arxiv.org/abs/2504.19170"}
{"created":"2025-05-06","title":"CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation","abstract":"We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified Continuous Latent Representation. Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation.","authors":["Xueqi Ma","Yilin Liu","Tianlong Gao","Qirui Huang","Hui Huang"],"url":"https://arxiv.org/abs/2504.19174"}
{"created":"2025-05-06","title":"VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?","abstract":"Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.","authors":["Mohamed Gado","Towhid Taliee","Muhammad Memon","Dmitry Ignatov","Radu Timofte"],"url":"https://arxiv.org/abs/2504.19267"}
{"created":"2025-05-06","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler","abstract":"In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","authors":["Size Zheng","Wenlei Bao","Qi Hou","Xuegui Zheng","Jin Fang","Chenhui Huang","Tianqi Li","Haojie Duanmu","Renze Chen","Ruifan Xu","Yifan Guo","Ningxin Zheng","Ziheng Jiang","Xinyi Di","Dongyang Wang","Jianxi Ye","Haibin Lin","Li-Wen Chang","Liqiang Lu","Yun Liang","Jidong Zhai","Xin Liu"],"url":"https://arxiv.org/abs/2504.19442"}
{"created":"2025-05-06","title":"From Freshness to Effectiveness: Goal-Oriented Sampling for Remote Decision Making","abstract":"Data freshness, measured by Age of Information (AoI), is highly relevant in networked applications such as Vehicle to Everything (V2X), smart health systems, and Industrial Internet of Things (IIoT). Yet, freshness alone does not equate to informativeness. In decision-critical settings, some stale data may prove more valuable than fresh updates. To explore this nuance, we move beyond AoI-centric policies and investigate how data staleness impacts decision-making under data-staleness-induced uncertainty. We pose a central question: What is the value of information, when freshness fades, and only its power to shape remote decisions remains? To capture this endured value, we propose AR-MDP, an Age-aware Remote Markov Decision Process framework, which co-designs optimal sampling and remote decision-making under a sampling frequency constraint and random delay. To efficiently solve this problem, we design a new two-stage hierarchical algorithm namely Quick Bellman-Linear-Program (QuickBLP), where the first stage involves solving the Dinkelbach root of a Bellman variant and the second stage involves solving a streamlined linear program (LP). For the tricky first stage, we propose a new One-layer Primal-Dinkelbach Synchronous Iteration (OnePDSI) method, which overcomes the re-convergence and non-expansive divergence present in existing per-sample multi-layer algorithms. Through rigorous convergence analysis of our proposed algorithms, we establish that the worst-case optimality gap in OnePDSI exhibits exponential decay with respect to iteration $K$ at a rate of $\\mathcal{O}(\\frac{1}{R^K})$. Through sensitivity analysis, we derive a threshold for the sampling frequency, beyond which additional sampling does not yield further gains in decision-making. Simulation results validate our analyses.","authors":["Aimin Li","Shaohua Wu","Gary C. F. Lee","Sumei Sun"],"url":"https://arxiv.org/abs/2504.19507"}
{"created":"2025-05-06","title":"ARTEMIS: Autoregressive End-to-End Trajectory Planning with Mixture of Experts for Autonomous Driving","abstract":"This paper presents ARTEMIS, an end-to-end autonomous driving framework that combines autoregressive trajectory planning with Mixture-of-Experts (MoE). Traditional modular methods suffer from error propagation, while existing end-to-end models typically employ static one-shot inference paradigms that inadequately capture the dynamic changes of the environment. ARTEMIS takes a different method by generating trajectory waypoints sequentially, preserves critical temporal dependencies while dynamically routing scene-specific queries to specialized expert networks. It effectively relieves trajectory quality degradation issues encountered when guidance information is ambiguous, and overcomes the inherent representational limitations of singular network architectures when processing diverse driving scenarios. Additionally, we use a lightweight batch reallocation strategy that significantly improves the training speed of the Mixture-of-Experts model. Through experiments on the NAVSIM dataset, ARTEMIS exhibits superior competitive performance, achieving 87.0 PDMS and 83.1 EPDMS with ResNet-34 backbone, demonstrates state-of-the-art performance on multiple metrics.","authors":["Renju Feng","Ning Xi","Duanfeng Chu","Rukang Wang","Zejian Deng","Anzheng Wang","Liping Lu","Jinxiang Wang","Yanjun Huang"],"url":"https://arxiv.org/abs/2504.19580"}
{"created":"2025-05-06","title":"Mapping the Italian Telegram Ecosystem: Communities, Toxicity, and Hate Speech","abstract":"Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram.","authors":["Lorenzo Alvisi","Serena Tardelli","Maurizio Tesconi"],"url":"https://arxiv.org/abs/2504.19594"}
{"created":"2025-05-06","title":"Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation","abstract":"Face registration deforms a template mesh to closely fit a 3D face scan, the quality of which commonly degrades in non-skin regions (e.g., hair, beard, accessories), because the optimized template-to-scan distance pulls the template mesh towards the noisy scan surface. Improving registration quality requires a clean separation of skin and non-skin regions on the scan mesh. Existing image-based (2D) or scan-based (3D) segmentation methods however perform poorly. Image-based segmentation outputs multi-view inconsistent masks, and they cannot account for scan inaccuracies or scan-image misalignment, while scan-based methods suffer from lower spatial resolution compared to images. In this work, we introduce a novel method that accurately separates skin from non-skin geometry on 3D human head scans. For this, our method extracts features from multi-view images using a frozen image foundation model and aggregates these features in 3D. These lifted 2D features are then fused with 3D geometric features extracted from the scan mesh, to then predict a segmentation mask directly on the scan mesh. We show that our segmentations improve the registration accuracy over pure 2D or 3D segmentation methods by 8.89% and 14.3%, respectively. Although trained only on synthetic data, our model generalizes well to real data.","authors":["Victoria Yue Chen","Daoye Wang","Stephan Garbin","Jan Bednarik","Sebastian Winberg","Timo Bolkart","Thabo Beeler"],"url":"https://arxiv.org/abs/2504.19718"}
{"created":"2025-05-06","title":"Hierarchical Uncertainty-Aware Graph Neural Network","abstract":"Recent research on graph neural networks (GNNs) has explored mechanisms for capturing local uncertainty and exploiting graph hierarchies to mitigate data sparsity and leverage structural properties. However, the synergistic integration of these two approaches remains underexplored. This work introduces a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN), which unifies multi-scale representation learning, principled uncertainty estimation, and self-supervised embedding diversity within a single end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and estimates uncertainty at multiple structural scales from individual nodes to higher levels. These uncertainty estimates guide a robust message-passing mechanism and attention weighting, effectively mitigating noise and adversarial perturbations while preserving predictive accuracy on semi-supervised classification tasks. We also offer key theoretical contributions, including a probabilistic formulation, rigorous uncertainty-calibration guarantees, and formal robustness bounds. Extensive experiments on standard benchmarks demonstrate that our model achieves state-of-the-art robustness and interpretability.","authors":["Yoonhyuk Choi","Jiho Choi","Taewook Ko","Chong-Kwon Kim"],"url":"https://arxiv.org/abs/2504.19820"}
{"created":"2025-05-06","title":"Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents","abstract":"As generative AI (GenAI) agents become more common in enterprise settings, they introduce security challenges that differ significantly from those posed by traditional systems. These agents are not just LLMs; they reason, remember, and act, often with minimal human oversight. This paper introduces a comprehensive threat model tailored specifically for GenAI agents, focusing on how their autonomy, persistent memory access, complex reasoning, and tool integration create novel risks. This research work identifies 9 primary threats and organizes them across five key domains: cognitive architecture vulnerabilities, temporal persistence threats, operational execution vulnerabilities, trust boundary violations, and governance circumvention. These threats are not just theoretical they bring practical challenges such as delayed exploitability, cross-system propagation, cross system lateral movement, and subtle goal misalignments that are hard to detect with existing frameworks and standard approaches. To help address this, the research work present two complementary frameworks: ATFAA - Advanced Threat Framework for Autonomous AI Agents, which organizes agent-specific risks, and SHIELD, a framework proposing practical mitigation strategies designed to reduce enterprise exposure. While this work builds on existing work in LLM and AI security, the focus is squarely on what makes agents different and why those differences matter. Ultimately, this research argues that GenAI agents require a new lens for security. If we fail to adapt our threat models and defenses to account for their unique architecture and behavior, we risk turning a powerful new tool into a serious enterprise liability.","authors":["Vineeth Sai Narajala","Om Narayan"],"url":"https://arxiv.org/abs/2504.19956"}
{"created":"2025-05-06","title":"A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts","abstract":"We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works.","authors":["Robert Becker","Aron Culotta"],"url":"https://arxiv.org/abs/2504.20065"}
{"created":"2025-05-06","title":"Billions at Stake: How Self-Citation Adjusted Metrics Can Transform Equitable Research Funding","abstract":"Citation metrics serve as the cornerstone of scholarly impact evaluation despite their well-documented vulnerability to inflation through self-citation practices. This paper introduces the Self-Citation Adjusted Index (SCAI), a sophisticated metric designed to recalibrate citation counts by accounting for discipline-specific self-citation patterns. Through comprehensive analysis of 5,000 researcher profiles across diverse disciplines, we demonstrate that excessive self-citation inflates traditional metrics by 10-20%, potentially misdirecting billions in research funding. Recent studies confirm that self-citation patterns exhibit significant gender disparities, with men self-citing up to 70% more frequently than women, exacerbating existing inequalities in academic recognition. Our open-source implementation provides comprehensive tools for calculating SCAI and related metrics, offering a more equitable assessment of research impact that reduces the gender citation gap by approximately 8.5%. This work contributes to the paradigm shift toward transparent, nuanced, and equitable research evaluation methodologies in academia, with direct implications for funding allocation decisions that collectively amount to over $100 billion annually in the United States alone.","authors":["Rahul Vishwakarma","Sinchan Banerjee"],"url":"https://arxiv.org/abs/2504.20081"}
{"created":"2025-05-06","title":"Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling","abstract":"AI-driven surrogate modeling has become an increasingly effective alternative to physics-based simulations for 3D design, analysis, and manufacturing. These models leverage data-driven methods to predict physical quantities traditionally requiring computationally expensive simulations. However, the scarcity of labeled CAD-to-simulation datasets has driven recent advancements in self-supervised and foundation models, where geometric representation learning is performed offline and later fine-tuned for specific downstream tasks. While these approaches have shown promise, their effectiveness is limited in applications requiring fine-scale geometric detail preservation. This work introduces a self-supervised geometric representation learning method designed to capture fine-scale geometric features from non-parametric 3D models. Unlike traditional end-to-end surrogate models, this approach decouples geometric feature extraction from downstream physics tasks, learning a latent space embedding guided by geometric reconstruction losses. Key elements include the essential use of near-zero level sampling and the innovative batch-adaptive attention-weighted loss function, which enhance the encoding of intricate design features. The proposed method is validated through case studies in structural mechanics, demonstrating strong performance in capturing design features and enabling accurate few-shot physics predictions. Comparisons with traditional parametric surrogate modeling highlight its potential to bridge the gap between geometric and physics-based representations, providing an effective solution for surrogate modeling in data-scarce scenarios.","authors":["Yu-hsuan Chen","Jing Bi","Cyril Ngo Ngoc","Victor Oancea","Jonathan Cagan","Levent Burak Kara"],"url":"https://arxiv.org/abs/2504.20110"}
{"created":"2025-05-06","title":"ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies","abstract":"In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research.","authors":["Shubham Gandhi","Dhruv Shah","Manasi Patwardhan","Lovekesh Vig","Gautam Shroff"],"url":"https://arxiv.org/abs/2504.20117"}
{"created":"2025-05-06","title":"UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions","abstract":"CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research.","authors":["Xiulin Yang","Zhuoxuan Ju","Lanni Bu","Zoey Liu","Nathan Schneider"],"url":"https://arxiv.org/abs/2504.20304"}
{"created":"2025-05-06","title":"VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with Delayed Hits","abstract":"Caches are fundamental to latency-sensitive systems like Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit phenomenon where multiple requests for an object occur during its fetch from the remote server after a miss significantly inflates user-perceived latency. While recent algorithms acknowledge delayed hits by estimating the resulting aggregate delay, they predominantly focus on its mean value. We identify and demonstrate that such approaches are insufficient, as the real aggregate delay frequently exhibits substantial variance in the true production system, leading to suboptimal latency performance when ignored. Thus, we propose VA-CDH, a variance-aware method to optimize latency for caching with delayed hits. It employs a novel ranking function that explicitly incorporates both the empirically estimated mean and standard deviation of aggregate delay, allowing caching decisions to account for its variation. We derive the analytical distribution of aggregate delay under Poisson arrivals as a theoretical contribution, offering more statistical insight beyond the mean value. Through the simulations conducted on synthetic and real-world datasets, we show that VA-CDH reduces the total latency by 1%-6% approximately compared to state-of-the-art algorithms.","authors":["Bowen Jiang","Chaofan Ma","Duo Wang"],"url":"https://arxiv.org/abs/2504.20335"}
{"created":"2025-05-06","title":"CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices","abstract":"Large Language Models (LLMs) enable real-time function calling in edge AI systems but introduce significant computational overhead, leading to high power consumption and carbon emissions. Existing methods optimize for performance while neglecting sustainability, making them inefficient for energy-constrained environments. We introduce CarbonCall, a sustainability-aware function-calling framework that integrates dynamic tool selection, carbon-aware execution, and quantized LLM adaptation. CarbonCall adjusts power thresholds based on real-time carbon intensity forecasts and switches between model variants to sustain high tokens-per-second throughput under power constraints. Experiments on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by up to 52%, power consumption by 30%, and execution time by 30%, while maintaining high efficiency.","authors":["Varatheepan Paramanayakam","Andreas Karatzas","Iraklis Anagnostopoulos","Dimitrios Stamoulis"],"url":"https://arxiv.org/abs/2504.20348"}
{"created":"2025-05-06","title":"Approximately Dominating Sets in Elections","abstract":"Condorcet's paradox is a fundamental result in social choice theory which states that there exist elections in which, no matter which candidate wins, a majority of voters prefer a different candidate. In fact, even if we can select any $k$ winners, there still may exist another candidate that would beat each of the winners in a majority vote. That is, elections may require arbitrarily large dominating sets.","authors":["Moses Charikar","Prasanna Ramakrishnan","Kangning Wang"],"url":"https://arxiv.org/abs/2504.20372"}
{"created":"2025-05-06","title":"LMME3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs","abstract":"The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication.","authors":["Woo Yi Yang","Jiarui Wang","Sijing Wu","Huiyu Duan","Yuxin Zhu","Liu Yang","Kang Fu","Guangtao Zhai","Xiongkuo Min"],"url":"https://arxiv.org/abs/2504.20466"}
{"created":"2025-05-06","title":"OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation","abstract":"Table structure recognition is a key task in document analysis. However, the geometric deformation in deformed tables causes a weak correlation between content information and structure, resulting in downstream tasks not being able to obtain accurate content information. To obtain fine-grained spatial coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge response by Gradient Orientation-aware Extractor, combines a Heterogeneous Kernel Cross Fusion module and a scale-aware loss function to adapt to multi-scale objective features, and introduces mask-driven non-maximal suppression in the post-processing, which replaces the traditional bounding box suppression mechanism. Furthermore, we also propose a data generator, filling the gap in the dataset for fine-grained deformation table cell spatial coordinate localization, and derive a large-scale dataset named Deformation Wired Table (DWTAL). Experiments show that our proposed model demonstrates excellent segmentation accuracy on all mainstream instance segmentation models. The dataset and the source code are open source: https://github.com/justliulong/OGHFYOLO.","authors":["Long Liu","Cihui Yang"],"url":"https://arxiv.org/abs/2504.20682"}
{"created":"2025-05-06","title":"Token-Efficient RL for LLM Reasoning","abstract":"We propose reinforcement learning (RL) strategies tailored for reasoning in large language models (LLMs) under strict memory and compute limits, with a particular focus on compatibility with LoRA fine-tuning. Rather than relying on full-sequence updates or separate critic networks, we design critic-free methods that operate on a small, informative subset of output tokens to reduce memory usage and stabilize training. We introduce S-GRPO, a stochastic variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching approach for fine-grained credit assignment. Applied to Qwen2-1.5B, our methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show strong performance on multi-digit multiplication. Surprisingly, full-token GRPO under LoRA fails to improve over the base model, suggesting that selective token-level optimization may act as an implicit regularizer in low-parameter training regimes.","authors":["Alan Lee","Harry Tong"],"url":"https://arxiv.org/abs/2504.20834"}
{"created":"2025-05-06","title":"CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models","abstract":"Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights.","authors":["Hasan Md Tusfiqur Alam","Devansh Srivastav","Abdulrahman Mohamed Selim","Md Abdul Kadir","Md Moktadirul Hoque Shuvo","Daniel Sonntag"],"url":"https://arxiv.org/abs/2504.20898"}
{"created":"2025-05-06","title":"Modeling AI-Human Collaboration as a Multi-Agent Adaptation","abstract":"We develop an agent-based simulation to formalize AI-human collaboration as a function of task structure, advancing a generalizable framework for strategic decision-making in organizations. Distinguishing between heuristic-based human adaptation and rule-based AI search, we model interactions across modular (parallel) and sequenced (interdependent) tasks using an NK model. Our results reveal that in modular tasks, AI often substitutes for humans - delivering higher payoffs unless human expertise is very high, and the AI search space is either narrowly focused or extremely broad. In sequenced tasks, interesting complementarities emerge. When an expert human initiates the search and AI subsequently refines it, aggregate performance is maximized. Conversely, when AI leads, excessive heuristic refinement by the human can reduce payoffs. We also show that even \"hallucinatory\" AI - lacking memory or structure - can improve outcomes when augmenting low-capability humans by helping escape local optima. These results yield a robust implication: the effectiveness of AI-human collaboration depends less on context or industry, and more on the underlying task structure. By elevating task decomposition as the central unit of analysis, our model provides a transferable lens for strategic decision-making involving humans and an agentic AI across diverse organizational settings.","authors":["Prothit Sen","Sai Mihir Jakkaraju"],"url":"https://arxiv.org/abs/2504.20903"}
{"created":"2025-05-06","title":"Pretraining Large Brain Language Model for Active BCI: Silent Speech","abstract":"This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\\% accuracy on semantic-level classification and 39.6\\% in word-level classification, outperforming baseline methods by 5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.","authors":["Jinzhao Zhou","Zehong Cao","Yiqun Duan","Connor Barkley","Daniel Leong","Xiaowei Jiang","Quoc-Toan Nguyen","Ziyi Zhao","Thomas Do","Yu-Cheng Chang","Sheng-Fu Liang","Chin-teng Lin"],"url":"https://arxiv.org/abs/2504.21214"}
{"created":"2025-05-06","title":"Identifying Critical Dependencies in Large-Scale Continuous Software Engineering","abstract":"Continuous Software Engineering (CSE) is widely adopted in the industry, integrating practices such as Continuous Integration and Continuous Deployment (CI/CD). Beyond technical aspects, CSE also encompasses business activities like continuous planning, budgeting, and operational processes. Coordinating these activities in large-scale product development involves multiple stakeholders, increasing complexity. This study aims to address this complexity by identifying and analyzing critical dependencies in large-scale CSE. Based on 17 semi-structured interviews conducted at two Nordic fintech companies, our preliminary findings indicate that dependencies between software teams and support functions, as well as between software teams and external entities, are the primary sources of delays and bottlenecks. As a next step, we plan to further refine our understanding of critical dependencies in large-scale CSE and explore coordination mechanisms that can better support software development teams in managing these challenges.","authors":["Anastasiia Tkalich","Eriks Klotins","Nils Brede Moe"],"url":"https://arxiv.org/abs/2504.21437"}
{"created":"2025-05-06","title":"Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline","abstract":"Short video platforms like YouTube Shorts and TikTok face significant copyright compliance challenges, as infringers frequently embed arbitrary background music (BGM) to obscure original soundtracks (OST) and evade content originality detection. To tackle this issue, we propose a novel pipeline that integrates Music Source Separation (MSS) and cross-modal video-music retrieval (CMVMR). Our approach effectively separates arbitrary BGM from the original OST, enabling the restoration of authentic video audio tracks. To support this work, we introduce two domain-specific datasets: OASD-20K for audio separation and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset comprising 1,121 video and mixed-audio pairs, specifically designed for short video restoration tasks. Experimental results demonstrate that our pipeline not only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring content integrity. This approach provides an ethical and scalable solution to copyright challenges in user-generated content on short video platforms.","authors":["Minwoo Oh","Minsu Park","Eunil Park"],"url":"https://arxiv.org/abs/2504.21772"}
{"created":"2025-05-06","title":"Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning","abstract":"Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.","authors":["Shaun Baek","Shaun Esua-Mensah","Cyrus Tsui","Sejan Vigneswaralingam","Abdullah Alali","Michael Lu","Vasu Sharma","Sean O'Brien","Kevin Zhu"],"url":"https://arxiv.org/abs/2505.00001"}
{"created":"2025-05-06","title":"Improving Phishing Email Detection Performance of Small Large Language Models","abstract":"Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving both accuracy and F1 score on the SpamAssassin and CEAS\\_08 datasets. Furthermore, the fine-tuned models demonstrated strong transferability, achieving robust performance across multiple unseen phishing datasets, outperforming traditional baselines and approaching standard-sized LLMs.","authors":["Zijie Lin","Zikang Liu","Hanbo Fan"],"url":"https://arxiv.org/abs/2505.00034"}
{"created":"2025-05-06","title":"MCMComm: Hardware-Software Co-Optimization for End-to-End Communication in Multi-Chip-Modules","abstract":"Increasing AI computing demands and slowing transistor scaling have led to the advent of Multi-Chip-Module (MCMs) based accelerators. MCMs enable cost-effective scalability, higher yield, and modular reuse by partitioning large chips into smaller chiplets. However, MCMs come at an increased communication cost, which requires critical analysis and optimization. This paper makes three main contributions: (i) an end-to-end, off-chip congestion-aware and packaging-adaptive analytical framework for detailed analysis, (ii) hardware software co-optimization incorporating diagonal links, on-chip redistribution, and non-uniform workload partitioning to optimize the framework, and (iii) using metaheuristics (genetic algorithms, GA) and mixed integer quadratic programming (MIQP) to solve the optimized framework. Experimental results demonstrate significant performance improvements for CNNs and Vision Transformers, showcasing up to 1.58x and 2.7x EdP (Energy delay Product) improvement using GA and MIQP, respectively.","authors":["Ritik Raj","Shengjie Lin","William Won","Tushar Krishna"],"url":"https://arxiv.org/abs/2505.00041"}
{"created":"2025-05-06","title":"CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios","abstract":"With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing approaches, this paper proposes coordination field agentic system for coordinating heterogeneous UAV swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.","authors":["Tengchao Zhang","Yonglin Tian","Fei Lin","Jun Huang","Patrik P. S\\\"uli","Rui Qin","Fei-Yue Wang"],"url":"https://arxiv.org/abs/2505.00091"}
{"created":"2025-05-06","title":"Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture","abstract":"The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate \\textit{in vitro} vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.","authors":["Tien Comlekoglu","J. Quetzalc\\'oatl Toledo-Mar\\'in","Tina Comlekoglu","Douglas W. DeSimone","Shayn M. Peirce","Geoffrey Fox","James A. Glazier"],"url":"https://arxiv.org/abs/2505.00316"}
{"created":"2025-05-06","title":"Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution","abstract":"Image Super-Resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. The ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. While deep learning has significantly advanced SR, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. Recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. In this work, we introduce ResQu a novel SR framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. Unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. Furthermore, we also leverage the generative priors of foundation models such as Stable Diffusion. Extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding SR results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. The code will be available after the revision process.","authors":["Luigi Sigillo","Christian Bianchi","Aurelio Uncini","Danilo Comminiello"],"url":"https://arxiv.org/abs/2505.00334"}
{"created":"2025-05-06","title":"Affine matrix scrambling achieves smoothness-dependent convergence rates","abstract":"We study the convergence rate of the median estimator for affine matrix scrambled digital nets applied to integrands over the unit hypercube $[0, 1]^s$. By taking the median of $(2r-1)$ independent randomized quasi-Monte Carlo (RQMC) samples, we demonstrate that the desired convergence rates can be achieved without increasing the number of randomizations $r$ as the quadrature size $N$ grows for both bounded and unbounded integrands. For unbounded integrands, our analysis assumes a boundary growth condition on the weak derivatives and also considers singularities such as kinks and jump discontinuities. Notably, when $r = 1$, the median estimator reduces to the standard RQMC estimator. By applying analytical techniques developed for median estimators, we prove that the affine matrix scrambled estimator achieves a convergence rate depending on the integrand's smoothness, and is therefore not limited by the canonical rate $\\mathcal{O}(N^{-3/2})$. However, this smoothness-dependent theoretical rate is not observed empirically in numerical experiments when the affine matrix scrambling yields a heavy-tailed sampling distribution. In contrast, the median estimator consistently reveals the theoretical rates and yields smaller integration errors than mean estimators, further highlighting its advantages.","authors":["Yang Liu"],"url":"https://arxiv.org/abs/2505.00411"}
{"created":"2025-05-06","title":"An approach for modularly verifying the core of Rust's atomic reference counting algorithm against the (X)C20 memory consistency model","abstract":"We propose an approach for modular verification of programs that use relaxed-consistency atomic memory access primitives and fences, sufficient for verifying the core of Rust's Atomic Reference Counting (ARC) algorithm, and we argue its soundness, when combined with a simple static analysis and admitting an open sub-problem, with respect to the C20 memory consistency model, as well as, even in the absence of any static analysis and without any assumptions, with respect to XC20, a recently proposed minor strengthening of C20 that rules out out-of-thin-air behaviors but allows load buffering. In contrast to existing work on verifying ARC, we do not assume acyclicity of the union of the program-order and reads-from relations. We define an interleaving operational semantics, prove its soundness with respect to (X)C20's axiomatic semantics, and then apply any existing program logic for fine-grained interleaving concurrency, such as Iris.","authors":["Bart Jacobs","Justus Fasse"],"url":"https://arxiv.org/abs/2505.00449"}
{"created":"2025-05-06","title":"Evaluation of Thermal Control Based on Spatial Thermal Comfort with Reconstructed Environmental Data","abstract":"Achieving thermal comfort while maintaining energy efficiency is a critical objective in building system control. Conventional thermal comfort models, such as the Predicted Mean Vote (PMV), rely on both environmental and personal variables. However, the use of fixed-location sensors limits the ability to capture spatial variability, which reduces the accuracy of occupant-specific comfort estimation. To address this limitation, this study proposes a new PMV estimation method that incorporates spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. In addition, a group PMV-based control framework is developed to account for the thermal comfort of multiple occupants. The Gappy POD method enables fast and accurate reconstruction of indoor temperature fields from sparse sensor measurements. Using these reconstructed fields and occupant location data, spatially resolved PMV values are calculated. Group-level thermal conditions are then derived through statistical aggregation methods and used to control indoor temperature in a multi-occupant living lab environment. Experimental results show that the Gappy POD algorithm achieves an average relative error below 3\\% in temperature reconstruction. PMV distributions varied by up to 1.26 scale units depending on occupant location. Moreover, thermal satisfaction outcomes varied depending on the group PMV method employed. These findings underscore the importance for adaptive thermal control strategies that incorporate both spatial and individual variability, offering valuable insights for future occupant-centric building operations.","authors":["Youngkyu Kim","Byounghyun Yoo","Ji Young Yun","Hyeokmin Lee","Sehyeon Park","Jin Woo Moon","Eun Ji Choi"],"url":"https://arxiv.org/abs/2505.00468"}
{"created":"2025-05-06","title":"Variational OOD State Correction for Offline Reinforcement Learning","abstract":"The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites.","authors":["Ke Jiang","Wen Jiang","Masahiro Fujisawa","Xiaoyang Tan"],"url":"https://arxiv.org/abs/2505.00503"}
{"created":"2025-05-06","title":"The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)","abstract":"Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \\emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \\emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \\emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \\emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.","authors":["Zihao Wang","Yibo Jiang","Jiahao Yu","Heqing Huang"],"url":"https://arxiv.org/abs/2505.00626"}
{"created":"2025-05-06","title":"Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook","abstract":"Deep learning has profoundly transformed remote sensing, yet prevailing architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) remain constrained by critical trade-offs: CNNs suffer from limited receptive fields, while ViTs grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. State Space Models (SSMs), particularly the recently proposed Mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. This survey presents a comprehensive review of Mamba-based methodologies in remote sensing, systematically analyzing about 120 Mamba-based remote sensing studies to construct a holistic taxonomy of innovations and applications. Our contributions are structured across five dimensions: (i) foundational principles of vision Mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid SSM formulations, (iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. By bridging the gap between SSM theory and remote sensing practice, this survey establishes Mamba as a transformative framework for remote sensing analysis. To our knowledge, this paper is the first systematic review of Mamba architectures in remote sensing. Our work provides a structured foundation for advancing research in remote sensing systems through SSM-based methods. We curate an open-source repository (https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster community-driven advancements.","authors":["Muyi Bao","Shuchang Lyu","Zhaoyang Xu","Huiyu Zhou","Jinchang Ren","Shiming Xiang","Xiangtai Li","Guangliang Cheng"],"url":"https://arxiv.org/abs/2505.00630"}
{"created":"2025-05-06","title":"Large Language Models Understanding: an Inherent Ambiguity Barrier","abstract":"A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.","authors":["Daniel N. Nissani (Nissensohn)"],"url":"https://arxiv.org/abs/2505.00654"}
{"created":"2025-05-06","title":"Localizing Before Answering: A Hallucination Evaluation Benchmark for Grounded Medical Multimodal LLMs","abstract":"Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to inadequate localization reasoning. This work reveals a critical limitation in current medical LMMs: instead of analyzing relevant pathological regions, they often rely on linguistic patterns or attend to irrelevant image areas when responding to disease-related queries. To address this, we introduce HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive benchmark designed to evaluate LMMs' localization abilities and hallucination robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA pairs, with doctor-annotated anatomical segmentation masks for pathological regions. To improve visual reasoning, we propose the Localize-before-Answer (LobA) framework, which trains LMMs to localize target regions of interest and self-prompt to emphasize segmented pathological areas, generating grounded and reliable answers. Experimental results demonstrate that our approach significantly outperforms state-of-the-art biomedical LMMs on the challenging HEAL-MedVQA benchmark, advancing robustness in medical VQA.","authors":["Dung Nguyen","Minh Khoi Ho","Huy Ta","Thanh Tam Nguyen","Qi Chen","Kumar Rav","Quy Duong Dang","Satwik Ramchandre","Son Lam Phung","Zhibin Liao","Minh-Son To","Johan Verjans","Phi Le Nguyen","Vu Minh Hieu Phan"],"url":"https://arxiv.org/abs/2505.00744"}
{"created":"2025-05-06","title":"TSTMotion: Training-free Scene-aware Text-to-motion Generation","abstract":"Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \\textbf{T}raining-free \\textbf{S}cene-aware \\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \\href{https://tstmotion.github.io/}{Project Page}.","authors":["Ziyan Guo","Haoxuan Qu","Hossein Rahmani","Dewen Soh","Ping Hu","Qiuhong Ke","Jun Liu"],"url":"https://arxiv.org/abs/2505.01182"}
{"created":"2025-05-06","title":"Helping Large Language Models Protect Themselves: An Enhanced Filtering and Summarization System","abstract":"The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.","authors":["Sheikh Samit Muhaimin","Spyridon Mastorakis"],"url":"https://arxiv.org/abs/2505.01315"}
{"created":"2025-05-06","title":"Terminal Coalgebras and Non-wellfounded Sets in Homotopy Type Theory","abstract":"Non-wellfounded material sets have been modeled in Martin-L\\\"of type theory by Lindstr\\\"om using setoids. In this paper we construct models of non-wellfounded material sets in Homotopy Type Theory (HoTT) where equality is interpreted as the identity type. The first model satisfies Scott's Anti-Foundation Axiom (SAFA) and dualises the construction of iterative sets. The second model satisfies Aczel's Anti-Foundation Axiom (AFA), and is constructed by adaption of Aczel--Mendler's terminal coalgebra theorem to type theory, which requires propositional resizing.","authors":["H{\\aa}kon Robbestad Gylterud","Elisabeth Stenholm","Niccol\\`o Veltri"],"url":"https://arxiv.org/abs/2001.06696"}
{"created":"2025-05-06","title":"Graph and wreath products of cellular automata","abstract":"We prove that the set of subgroups of the automorphism group of a two-sided full shift is closed under countable graph products. We introduce the notion of a group action without $A$-cancellation (for an abelian group $A$), and show that when $A$ is a finite abelian group and $G$ is a group of cellular automata whose action does not have $A$-cancellation, the wreath product $A \\wr G$ embeds in the automorphism group of a full shift. We show that all free abelian groups and free groups admit such cellular automata actions. In the one-sided case, we prove variants of these results with reasonable alphabet blow-ups.","authors":["Ville Salo"],"url":"https://arxiv.org/abs/2012.10186"}
{"created":"2025-05-06","title":"Lifting couplings in Wasserstein spaces","abstract":"This paper makes mathematically precise the idea that conditional probabilities are analogous to path liftings in geometry.","authors":["Paolo Perrone"],"url":"https://arxiv.org/abs/2110.06591"}
{"created":"2025-05-06","title":"The Loop Game: Quality Assessment and Optimization for Low-Light Image Enhancement","abstract":"There is an increasing consensus that the design and optimization of low light image enhancement methods need to be fully driven by perceptual quality. With numerous approaches proposed to enhance low-light images, much less work has been dedicated to quality assessment and quality optimization of low-light enhancement. In this paper, to close the gap between enhancement and assessment, we propose a loop enhancement framework that produces a clear picture of how the enhancement of low-light images could be optimized towards better visual quality. In particular, we create a large-scale database for QUality assessment Of The Enhanced LOw-Light Image (QUOTE-LOL), which serves as the foundation in studying and developing objective quality assessment measures. The objective quality assessment measure plays a critical bridging role between visual quality and enhancement and is further incorporated in the optimization in learning the enhancement model towards perceptual optimally. Finally, we iteratively perform the enhancement and optimization tasks, enhancing the low-light images continuously. The superiority of the proposed scheme is validated based on various low-light scenes.","authors":["Danni Huang","Lingyu Zhu","Zihao Lin","Hanwei Zhu","Shiqi Wang","Baoliang Chen"],"url":"https://arxiv.org/abs/2202.09738"}
{"created":"2025-05-06","title":"Bayesian Deep Learning with Multilevel Trace-class Neural Networks","abstract":"In this article we consider Bayesian inference associated to deep neural networks (DNNs) and in particular, trace-class neural network (TNN) priors which can be preferable to traditional DNNs as (a) they are identifiable and (b) they possess desirable convergence properties. TNN priors are defined on functions with infinitely many hidden units, and have strongly convergent Karhunen-Loeve-type approximations with finitely many hidden units. A practical hurdle is that the Bayesian solution is computationally demanding, requiring simulation methods, so approaches to drive down the complexity are needed. In this paper, we leverage the strong convergence of TNN in order to apply Multilevel Monte Carlo (MLMC) to these models. In particular, an MLMC method that was introduced is used to approximate posterior expectations of Bayesian TNN models with optimal computational complexity, and this is mathematically proved. The results are verified with several numerical experiments on model problems arising in machine learning, including regression, classification, and reinforcement learning.","authors":["Neil K. Chada","Ajay Jasra","Kody J. H. Law","Sumeetpal S. Singh"],"url":"https://arxiv.org/abs/2203.12961"}
{"created":"2025-05-06","title":"Overparametrized linear dimensionality reductions: From projection pursuit to two-layer neural networks","abstract":"Given a cloud of $n$ data points in $\\mathbb{R}^d$, consider all projections onto $m$-dimensional subspaces of $\\mathbb{R}^d$ and, for each such projection, the empirical distribution of the projected points. What does this collection of probability distributions look like when $n,d$ grow large?","authors":["Andrea Montanari","Kangjie Zhou"],"url":"https://arxiv.org/abs/2206.06526"}
{"created":"2025-05-06","title":"Wasserstein Distributionally Robust Estimation in High Dimensions: Performance Analysis and Optimal Hyperparameter Tuning","abstract":"Distributionally robust optimization (DRO) has become a powerful framework for estimation under uncertainty, offering strong out-of-sample performance and principled regularization. In this paper, we propose a DRO-based method for linear regression and address a central question: how to optimally choose the robustness radius, which controls the trade-off between robustness and accuracy. Focusing on high-dimensional settings where the dimension and the number of samples are both large and comparable in size, we employ tools from high-dimensional asymptotic statistics to precisely characterize the estimation error of the resulting estimator. Remarkably, this error can be recovered by solving a simple convex-concave optimization problem involving only four scalar variables. This characterization enables efficient selection of the radius that minimizes the estimation error. In doing so, it achieves the same effect as cross-validation, but at a fraction of the computational cost. Numerical experiments confirm that our theoretical predictions closely match empirical performance and that the optimal radius selected through our method aligns with that chosen by cross-validation, highlighting both the accuracy and the practical benefits of our approach.","authors":["Liviu Aolaritei","Soroosh Shafiee","Florian D\\\"orfler"],"url":"https://arxiv.org/abs/2206.13269"}
{"created":"2025-05-06","title":"Gauss-Southwell type descent methods for low-rank matrix optimization","abstract":"We consider gradient-related methods for low-rank matrix optimization with a smooth cost function. The methods operate on single factors of the low-rank factorization and share aspects of both alternating and Riemannian optimization. Two possible choices for the search directions based on Gauss-Southwell type selection rules are compared: one using the gradient of a factorized non-convex formulation, the other using the Riemannian gradient. While both methods provide gradient convergence guarantees that are similar to the unconstrained case, numerical experiments on a quadratic cost function indicate that the version based on the Riemannian gradient is significantly more robust with respect to small singular values and the condition number of the cost function. As a side result of our approach, we also obtain new convergence results for the alternating least squares method.","authors":["Guillaume Olikier","Andr\\'e Uschmajew","Bart Vandereycken"],"url":"https://arxiv.org/abs/2306.00897"}
{"created":"2025-05-06","title":"Large Language Models at Work in China's Labor Market","abstract":"This paper explores the potential impacts of large language models (LLMs) on the Chinese labor market. We analyze occupational exposure to LLM capabilities by incorporating human expertise and LLM classifications, following the methodology of Eloundou et al. (2023). The results indicate a positive correlation between occupational exposure and both wage levels and experience premiums at the occupation level. This suggests that higher-paying and experience-intensive jobs may face greater exposure risks from LLM-powered software. We then aggregate occupational exposure at the industry level to obtain industrial exposure scores. Both occupational and industrial exposure scores align with expert assessments. Our empirical analysis also demonstrates a distinct impact of LLMs, which deviates from the routinization hypothesis. We present a stylized theoretical framework to better understand this deviation from previous digital technologies. By incorporating entropy-based information theory into the task-based framework, we propose an AI learning theory that reveals a different pattern of LLM impacts compared to the routinization hypothesis.","authors":["Qin Chen","Jinfeng Ge","Huaqing Xie","Xingcheng Xu","Yanqing Yang"],"url":"https://arxiv.org/abs/2308.08776"}
{"created":"2025-05-06","title":"A note on the power sums of the number of Fibonacci partitions","abstract":"For every nonnegative integer $n$, let $r_F(n)$ be the number of ways to write $n$ as a sum of Fibonacci numbers, where the order of the summands does not matter. Moreover, for all positive integers $p$ and $N$, let \\begin{equation*} S_{F}^{(p)}(N) := \\sum_{n = 0}^{N - 1} \\big(r_F(n)\\big)^p . \\end{equation*} Chow, Jones, and Slattery determined the order of growth of $S_{F}^{(p)}(N)$ for $p \\in \\{1,2\\}$. We prove that, for all positive integers $p$, there exists a real number $\\lambda_p > 1$ such that \\begin{equation*} S^{(p)}_F(N) \\asymp_p N^{(\\log \\lambda_p) /\\!\\log \\varphi} \\end{equation*} as $N \\to +\\infty$, where $\\varphi := (1 + \\sqrt{5})/2$ is the golden ratio. Furthermore, we show that \\begin{equation*} \\lim_{p \\to +\\infty} \\lambda_p^{1/p} = \\varphi^{1/2} . \\end{equation*} Our proofs employ automata theory and a result on the generalized spectral radius due to Blondel and Nesterov.","authors":["Carlo Sanna"],"url":"https://arxiv.org/abs/2309.12724"}
{"created":"2025-05-06","title":"Learning Quantum Processes with Quantum Statistical Queries","abstract":"In this work, we initiate the study of learning quantum processes from quantum statistical queries. We focus on two fundamental learning tasks in this new access model: shadow tomography of quantum processes and process tomography with respect to diamond distance. For the former, we present an efficient average-case algorithm along with a nearly matching lower bound with respect to the number of observables to be predicted. For the latter, we present average-case query complexity lower bounds for learning classes of unitaries. We obtain an exponential lower bound for learning unitary 2-designs and a doubly exponential lower bound for Haar-random unitaries. Finally, we demonstrate the practical relevance of our access model by applying our learning algorithm to attack an authentication protocol using Classical-Readout Quantum Physically Unclonable Functions, partially addressing an important open question in quantum hardware security.","authors":["Chirag Wadhwa","Mina Doosti"],"url":"https://arxiv.org/abs/2310.02075"}
{"created":"2025-05-06","title":"Robust Transfer Learning with Unreliable Source Data","abstract":"This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distribution. We introduce a novel quantity called the ''ambiguity level'' that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements. Our proposed ''Transfer Around Boundary'' (TAB) model, with a threshold balancing the performance of target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer. Moreover, we demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors. Simulation studies lend further support to the effectiveness of TAB. We also provide simple approaches to bound the excess misclassification error without the need for specialized knowledge in transfer learning.","authors":["Jianqing Fan","Cheng Gao","Jason M. Klusowski"],"url":"https://arxiv.org/abs/2310.04606"}
{"created":"2025-05-06","title":"Dihedral Quantum Codes","abstract":"We establish dihedral quantum codes of short block length, a class of CSS codes obtained by the lifted product construction. We present the code construction and give a formula for the code dimension, depending on the two classical codes that the CSS code is based on. We also give a lower bound on the code distance and construct an example of short dihedral quantum codes.","authors":["Nadja Willenborg","Martino Borello","Anna-Lena Horlemann","Habibul Islam"],"url":"https://arxiv.org/abs/2310.15092"}
{"created":"2025-05-06","title":"Joint Problems in Learning Multiple Dynamical Systems","abstract":"Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results. The key highlight of this method is that it does not require a predefined hidden state dimension but instead provides an upper bound. Additionally, it offers guidance for determining regularization in the system identification.","authors":["Mengjia Niu","Xiaoyu He","Petr Ry\\v{s}av\\'y","Quan Zhou","Jakub Marecek"],"url":"https://arxiv.org/abs/2311.02181"}
{"created":"2025-05-06","title":"Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation","abstract":"Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.","authors":["Jikai Jin","Vasilis Syrgkanis"],"url":"https://arxiv.org/abs/2402.14264"}
{"created":"2025-05-06","title":"Exploring Challenges in Deep Learning of Single-Station Ground Motion Records","abstract":"Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models truly extract meaningful patterns from these complex time-series signals remains underexplored. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. Our experimental results reveal a strong dependence on the highly correlated Primary (P) and Secondary (S) phase arrival times. These findings expose a critical gap in the current research landscape, highlighting the lack of robust methodologies for deep learning from single-station ground motion recordings that do not rely on auxiliary inputs.","authors":["\\\"Umit Mert \\c{C}a\\u{g}lar","Baris Yilmaz","Melek T\\\"urkmen","Erdem Akag\\\"und\\\"uz","Salih Tileylioglu"],"url":"https://arxiv.org/abs/2403.07569"}
{"created":"2025-05-06","title":"Backpropagation through space, time, and the brain","abstract":"How physical networks of neurons, bound by spatio-temporal locality constraints, can perform efficient credit assignment, remains, to a large extent, an open question. In machine learning, the answer is almost universally given by the error backpropagation algorithm, through both space and time. However, this algorithm is well-known to rely on biologically implausible assumptions, in particular with respect to spatio-temporal (non-)locality. Alternative forward-propagation models such as real-time recurrent learning only partially solve the locality problem, but only at the cost of scaling, due to prohibitive storage requirements. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by defining an energy based on neuron-local mismatches, from which we derive both neuronal dynamics via stationarity and parameter dynamics via gradient descent. The resulting dynamics can be interpreted as a real-time, biologically plausible approximation of backpropagation through space and time in deep cortical networks with continuous-time neuronal dynamics and continuously active, local synaptic plasticity. In particular, GLE exploits the morphology of dendritic trees to enable more complex information storage and processing in single neurons, as well as the ability of biological neurons to phase-shift their output rate with respect to their membrane potential, which is essential in both directions of information propagation. For the forward computation, it enables the mapping of time-continuous inputs to neuronal space, effectively performing a spatio-temporal convolution. For the backward computation, it permits the temporal inversion of feedback signals, which consequently approximate the adjoint variables necessary for useful parameter updates.","authors":["Benjamin Ellenberger","Paul Haider","Jakob Jordan","Kevin Max","Ismael Jaras","Laura Kriener","Federico Benitez","Mihai A. Petrovici"],"url":"https://arxiv.org/abs/2403.16933"}
{"created":"2025-05-06","title":"RiskLabs: Predicting Financial Risk Using Large Language Model based on Multimodal and Multi-Sources Data","abstract":"The integration of Artificial Intelligence (AI) techniques, particularly large language models (LLMs), in finance has garnered increasing academic attention. Despite progress, existing studies predominantly focus on tasks like financial text summarization, question-answering, and stock movement prediction (binary classification), the application of LLMs to financial risk prediction remains underexplored. Addressing this gap, in this paper, we introduce RiskLabs, a novel framework that leverages LLMs to analyze and predict financial risks. RiskLabs uniquely integrates multimodal financial data, including textual and vocal information from Earnings Conference Calls (ECCs), market-related time series data, and contextual news data to improve financial risk prediction. Empirical results demonstrate RiskLabs' effectiveness in forecasting both market volatility and variance. Through comparative experiments, we examine the contributions of different data sources to financial risk assessment and highlight the crucial role of LLMs in this process. We also discuss the challenges associated with using LLMs for financial risk prediction and explore the potential of combining them with multimodal data for this purpose.","authors":["Yupeng Cao","Zhi Chen","Prashant Kumar","Qingyun Pei","Yangyang Yu","Haohang Li","Fabrizio Dimino","Lorenzo Ausiello","K. P. Subbalakshmi","Papa Momar Ndiaye"],"url":"https://arxiv.org/abs/2404.07452"}
{"created":"2025-05-06","title":"Dynamic Local Average Treatment Effects","abstract":"We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may not comply with encouragements based on unobserved confounders. For settings with binary treatments and encouragements, we provide nonparametric identification, estimation, and inference for Dynamic Local Average Treatment Effects (LATEs), which are expected values of multiple time period treatment effect contrasts for the respective complier subpopulations. Under One Sided Noncompliance and sequential extensions of the assumptions in Imbens and Angrist (1994), we show that one can identify Dynamic LATEs that correspond to treating at single time steps. In Staggered Adoption settings, we show that the assumptions are sufficient to identify Dynamic LATEs for treating in multiple time periods. Moreover, this result extends to any setting where the effect of a treatment in one period is uncorrelated with the compliance event in a subsequent period.","authors":["Ravi B. Sojitra","Vasilis Syrgkanis"],"url":"https://arxiv.org/abs/2405.01463"}
{"created":"2025-05-06","title":"Which exceptional low-dimensional projections of a Gaussian point cloud can be found in polynomial time?","abstract":"Given $d$-dimensional standard Gaussian vectors $\\boldsymbol{x}_1,\\dots, \\boldsymbol{x}_n$, we consider the set of all empirical distributions of its $m$-dimensional projections, for $m$ a fixed constant. Diaconis and Freedman (1984) proved that, if $n/d\\to \\infty$, all such distributions converge to the standard Gaussian distribution. In contrast, we study the proportional asymptotics, whereby $n,d\\to \\infty$ with $n/d\\to \\alpha \\in (0, \\infty)$. In this case, the projection of the data points along a typical random subspace is again Gaussian, but the set $\\mathscr{F}_{m,\\alpha}$ of all probability distributions that are asymptotically feasible as $m$-dimensional projections contains non-Gaussian distributions corresponding to exceptional subspaces.","authors":["Andrea Montanari","Kangjie Zhou"],"url":"https://arxiv.org/abs/2406.02970"}
{"created":"2025-05-06","title":"Cycles in graphs and in hypergraphs: towards homology theory","abstract":"In this expository paper we present some ideas of algebraic topology (more precisely, of homology theory) in a language accessible to non-specialists in the area. A $1$-cycle in a graph is a set $C$ of edges such that every vertex is contained in an even number of edges from $C$. It is easy to check that the sum (modulo $2$) of $1$-cycles is a $1$-cycle. We start from the following problems: to find","authors":["A. Miroshnikov","O. Nikitenko","A. Skopenkov"],"url":"https://arxiv.org/abs/2406.16705"}
{"created":"2025-05-06","title":"SoftCVI: Contrastive variational inference with self-generated soft labels","abstract":"Estimating a distribution given access to its unnormalized density is pivotal in Bayesian inference, where the posterior is generally known only up to an unknown normalizing constant. Variational inference and Markov chain Monte Carlo methods are the predominant tools for this task; however, both are often challenging to apply reliably, particularly when the posterior has complex geometry. Here, we introduce Soft Contrastive Variational Inference (SoftCVI), which allows a family of variational objectives to be derived through a contrastive estimation framework. The approach parameterizes a classifier in terms of a variational distribution, reframing the inference task as a contrastive estimation problem aiming to identify a single true posterior sample among a set of samples. Despite this framing, we do not require positive or negative samples, but rather learn by sampling the variational distribution and computing ground truth soft classification labels from the unnormalized posterior itself. The objectives have zero variance gradient when the variational approximation is exact, without the need for specialized gradient estimators. We empirically investigate the performance on a variety of Bayesian inference tasks, using both simple (e.g. normal) and expressive (normalizing flow) variational distributions. We find that SoftCVI can be used to form objectives which are stable to train and mass-covering, frequently outperforming inference with other variational approaches.","authors":["Daniel Ward","Mark Beaumont","Matteo Fasiolo"],"url":"https://arxiv.org/abs/2407.15687"}
{"created":"2025-05-06","title":"Physics-Informed Weakly Supervised Learning for Interatomic Potentials","abstract":"Machine learning plays an increasingly important role in computational chemistry and materials science, complementing computationally intensive ab initio and first-principles methods. Despite their utility, machine-learning models often lack generalization capability and robustness during atomistic simulations, yielding unphysical energy and force predictions that hinder their real-world applications. We address this challenge by introducing a physics-informed, weakly supervised approach for training machine-learned interatomic potentials (MLIPs). We introduce two novel loss functions, extrapolating the potential energy via a Taylor expansion and using the concept of conservative forces. Our approach improves the accuracy of MLIPs applied to training tasks with sparse training data sets and reduces the need for pre-training computationally demanding models with large data sets. Particularly, we perform extensive experiments demonstrating reduced energy and force errors -- often lower by a factor of two -- for various baseline models and benchmark data sets. Moreover, we demonstrate improved robustness during MD simulations of the MLIP models trained with the proposed weakly supervised loss. Finally, our approach improves the fine-tuning of foundation models on sparse, highly accurate ab initio data. An implementation of our method and scripts for executing experiments are available at https://github.com/nec-research/PICPS-ML4Sci.","authors":["Makoto Takamoto","Viktor Zaverkin","Mathias Niepert"],"url":"https://arxiv.org/abs/2408.05215"}
{"created":"2025-05-06","title":"Fragment-Masked Molecular Optimization","abstract":"Molecular optimization is a crucial aspect of drug discovery, aimed at refining molecular structures to enhance drug efficacy and minimize side effects, ultimately accelerating the overall drug development process. Many target-based molecular optimization methods have been proposed, significantly advancing drug discovery. These methods primarily on understanding the specific drug target structures or their hypothesized roles in combating diseases. However, challenges such as a limited number of available targets and a difficulty capturing clear structures hinder innovative drug development. In contrast, phenotypic drug discovery (PDD) does not depend on clear target structures and can identify hits with novel and unbiased polypharmacology signatures. As a result, PDD-based molecular optimization can reduce potential safety risks while optimizing phenotypic activity, thereby increasing the likelihood of clinical success. Therefore, we propose a fragment-masked molecular optimization method based on PDD (FMOP). FMOP employs a regression-free diffusion model to conditionally optimize the molecular masked regions without training, effectively generating new molecules with similar scaffolds. On the large-scale drug response dataset GDSCv2, we optimize the potential molecules across all 945 cell lines. The overall experiments demonstrate that the in-silico optimization success rate reaches 94.4%, with an average efficacy increase of 5.3%. Additionally, we conduct extensive ablation and visualization experiments, confirming that FMOP is an effective and robust molecular optimization method. The code is available at:https://anonymous.4open.science/r/FMOP-98C2.","authors":["Kun Li","Xiantao Cai","Jia Wu","Bo Du","Wenbin Hu"],"url":"https://arxiv.org/abs/2408.09106"}
{"created":"2025-05-06","title":"Enhancing End Stage Renal Disease Outcome Prediction: A Multi-Sourced Data-Driven Approach","abstract":"Objective: To improve prediction of Chronic Kidney Disease (CKD) progression to End Stage Renal Disease (ESRD) using machine learning (ML) and deep learning (DL) models applied to an integrated clinical and claims dataset of varying observation windows, supported by explainable AI (XAI) to enhance interpretability and reduce bias.","authors":["Yubo Li","Rema Padman"],"url":"https://arxiv.org/abs/2410.01859"}
{"created":"2025-05-06","title":"A meta-analysis of impact factors of astrophysics journals","abstract":"We calculate the 2024 impact factors for the 38 most widely used journals in Astrophysics, using the citations collated by NASA/ADS (Astrophysics Data System) and compare them to the official impact factors. This includes journals which publish papers outside of astrophysics such as PRD, EPJC, Nature, etc. We also propose a new metric to gauge the impact factor based on the median number of citations in a journal and calculate the same for all the journals. We find that the ADS-based impact factors are mostly in agreement, albeit higher than the official impact factors for most journals. The journals with the maximum fractional difference in median-based and old impact factors are JHEAP and PTEP. We find the maximum difference between the ADS and official impact factor for Nature.","authors":["Rayani Venkat Sai Rithvik","Shantanu Desai"],"url":"https://arxiv.org/abs/2410.03342"}
{"created":"2025-05-06","title":"A distance function for stochastic matrices","abstract":"Motivated by information geometry, a distance function on the space of stochastic matrices is advocated. Starting with sequences of Markov chains the Bhattacharyya angle is advocated as the natural tool for comparing both short and long term Markov chain runs. Bounds on the convergence of the distance and mixing times are derived. Guided by the desire to compare different Markov chain models, especially in the setting of healthcare processes, a new distance function on the space of stochastic matrices is presented. It is a true distance measure which has a closed form and is efficient to implement for numerical evaluation. In the case of ergodic Markov chains, it is shown that considering either the Bhattacharyya angle on Markov sequences or the new stochastic matrix distance leads to the same distance between models.","authors":["Antony R. Lee","Peter Tino","Iain Bruce Styles"],"url":"https://arxiv.org/abs/2410.12689"}
{"created":"2025-05-06","title":"From Gradient Clipping to Normalization for Heavy Tailed SGD","abstract":"Recent empirical evidence indicates that many machine learning applications involve heavy-tailed gradient noise, which challenges the standard assumptions of bounded variance in stochastic optimization. Gradient clipping has emerged as a popular tool to handle this heavy-tailed noise, as it achieves good performance in this setting both theoretically and practically. However, our current theoretical understanding of non-convex gradient clipping has three main shortcomings. First, the theory hinges on large, increasing clipping thresholds, which are in stark contrast to the small constant clipping thresholds employed in practice. Second, clipping thresholds require knowledge of problem-dependent parameters to guarantee convergence. Lastly, even with this knowledge, current sampling complexity upper bounds for the method are sub-optimal in nearly all parameters. To address these issues, we study convergence of Normalized SGD (NSGD). First, we establish a parameter-free sample complexity for NSGD of $\\mathcal{O}\\left(\\varepsilon^{-\\frac{2p}{p-1}}\\right)$ to find an $\\varepsilon$-stationary point. Furthermore, we prove tightness of this result, by providing a matching algorithm-specific lower bound. In the setting where all problem parameters are known, we show this complexity is improved to $\\mathcal{O}\\left(\\varepsilon^{-\\frac{3p-2}{p-1}}\\right)$, matching the previously known lower bound for all first-order methods in all problem dependent parameters. Finally, we establish high-probability convergence of NSGD with a mild logarithmic dependence on the failure probability. Our work complements the studies of gradient clipping under heavy tailed noise improving the sample complexities of existing algorithms and offering an alternative mechanism to achieve high probability convergence.","authors":["Florian H\\\"ubler","Ilyas Fatkhullin","Niao He"],"url":"https://arxiv.org/abs/2410.13849"}
{"created":"2025-05-06","title":"Sliding DFT-based Signal Recovery for Modulo ADC with 1-bit Folding Information","abstract":"The modulo analog-to-digital converter (ADC) is a promising solution to resolve the limited dynamic range (DR) issue of conventional ADCs and achieve an enhanced digital resolution given a fixed quantization bit budget. However, a modulo ADC requires an unfolding scheme to correct the nonlinear distortion introduced by the modulo operation. This paper presents a sliding discrete Fourier Transform (DFT)-based method for fast signal reconstruction given the modulo ADC output sequence and a 1-bit folding information sequence. In contrast to existing DFT-based signal recovery techniques for modulo ADCs, our proposed sliding DFT method reduces the required observation time and minimizes the spectral leakage effects via proper choice of window function parameters. A mean squared error (MSE) performance guarantee is established for the proposed signal recovery algorithm. More precisely, we derive sufficient conditions for the oversampling factor ($\\mathrm{OF}$) and the number of quantization bits ($b$) to obtain a specific MSE performance. Our numerical results demonstrate that modulo ADCs equipped with our proposed recovery method can outperform conventional ADCs without modulo for $\\mathrm{OF} \\geq 4$ and $b \\geq 4$. The impact of spectral leakage on the MSE performance of the proposed sliding DFT recovery method is also quantified.","authors":["Neil Irwin Bernardo"],"url":"https://arxiv.org/abs/2410.18757"}
{"created":"2025-05-06","title":"A Spectral-based Physics-informed Finite Operator Learning for Prediction of Mechanical Behavior of Microstructures","abstract":"A novel physics-informed operator learning technique based on spectral methods is introduced to model the complex behavior of heterogeneous materials. The Lippmann-Schwinger operator in Fourier space is employed to construct physical constraints with minimal computational overhead, effectively eliminating the need for automatic differentiation. The introduced methodology accelerates the training process by enabling gradient construction on a fixed, finite discretization in Fourier space. Later, the spectral physics-informed finite operator learning (SPiFOL) framework is built based on this discretization and trained to map the arbitrary shape of microstructures to their mechanical responses (strain fields) without relying on labeled data. The training is done by minimizing equilibrium in Fourier space concerning the macroscopic loading condition, which also guarantees the periodicity. SPiFOL, as a physics-informed operator learning method, enables rapid predictions through forward inference after training. To ensure accuracy, we incorporate physical constraints and diversify the training data. However, performance may still degrade for out-of-distribution microstructures. SPiFOL is further enhanced by integrating a Fourier Neural Operator (FNO). Compared to the standard data-driven FNO, SPiFOL shows higher accuracy in predicting stress fields and provides nearly resolution-independent results. Additionally, its zero-shot super-resolution capabilities are explored in heterogeneous domains. Finally, SPiFOL is extended to handle 3D problems and further adapted to finite elasticity, demonstrating the robustness of the framework in handling nonlinear mechanical behavior. The framework shows great potential for efficient and scalable prediction of mechanical responses in complex material systems while also reducing the training time required for training physics-informed neural operators.","authors":["Ali Harandi","Hooman Danesh","Kevin Linka","Stefanie Reese","Shahed Rezaei"],"url":"https://arxiv.org/abs/2410.19027"}
{"created":"2025-05-06","title":"Neural and Time-Series Approaches for Pricing Weather Derivatives: Performance and Regime Adaptation Using Satellite Data","abstract":"This paper studies pricing of weather-derivative (WD) contracts on temperature and precipitation. For temperature-linked strangles in Toronto and Chicago, we benchmark a harmonic-regression/ARMA model against a feed-forward neural network (NN), finding that the NN reduces out-of-sample mean-squared error (MSE) and materially shifts December fair values relative to both the time-series model and the industry-standard Historic Burn Approach (HBA).","authors":["Marco Hening Tallarico","Pablo Olivares"],"url":"https://arxiv.org/abs/2411.12013"}
{"created":"2025-05-06","title":"GRAPHITE: Graph-Based Interpretable Tissue Examination for Enhanced Explainability in Breast Cancer Histopathology","abstract":"Explainable AI (XAI) in medical histopathology is essential for enhancing the interpretability and clinical trustworthiness of deep learning models in cancer diagnosis. However, the black-box nature of these models often limits their clinical adoption. We introduce GRAPHITE (Graph-based Interpretable Tissue Examination), a post-hoc explainable framework designed for breast cancer tissue microarray (TMA) analysis. GRAPHITE employs a multiscale approach, extracting patches at various magnification levels, constructing an hierarchical graph, and utilising graph attention networks (GAT) with scalewise attention (SAN) to capture scale-dependent features. We trained the model on 140 tumour TMA cores and four benign whole slide images from which 140 benign samples were created, and tested it on 53 pathologist-annotated TMA samples. GRAPHITE outperformed traditional XAI methods, achieving a mean average precision (mAP) of 0.56, an area under the receiver operating characteristic curve (AUROC) of 0.94, and a threshold robustness (ThR) of 0.70, indicating that the model maintains high performance across a wide range of thresholds. In clinical utility, GRAPHITE achieved the highest area under the decision curve (AUDC) of 4.17e+5, indicating reliable decision support across thresholds. These results highlight GRAPHITE's potential as a clinically valuable tool in computational pathology, providing interpretable visualisations that align with the pathologists' diagnostic reasoning and support precision medicine.","authors":["Raktim Kumar Mondol","Ewan K. A. Millar","Peter H. Graham","Lois Browne","Arcot Sowmya","Erik Meijering"],"url":"https://arxiv.org/abs/2501.04206"}
{"created":"2025-05-06","title":"Noise disturbance and lack of privacy: Modeling acoustic dissatisfaction in open-plan offices","abstract":"Open-plan offices are well-known to be adversely affected by acoustic issues. This study aims to model acoustic dissatisfaction using measurements of room acoustics, sound environment during occupancy, and occupant surveys (n = 349) in 28 offices representing a diverse range of workplace parameters. As latent factors, the contribution of $\\textit{lack of privacy}$ (LackPriv) was 25% higher than $\\textit{noise disturbance}$ (NseDstrb) in predicting $\\textit{acoustic dissatisfaction}$ (AcDsat). Room acoustic metrics based on sound pressure level (SPL) decay of speech ($L_{\\text{p,A,s,4m}}$ and $r_{\\text{C}}$) were better in predicting these factors than distraction distance ($r_{\\text{D}}$) based on speech transmission index. This contradicts previous findings, and the trends for SPL-based metrics in predicting AcDsat and LackPriv go against expectations based on ISO 3382-3. For sound during occupation, $L_{\\text{A,90}}$ and psychoacoustic loudness ($N_{\\text{90}}$) predicted AcDsat, and a SPL fluctuation metric ($M_{\\text{A,eq}}$) predicted LackPriv. However, these metrics were weaker predictors than ISO 3382-3 metrics. Medium-sized offices exhibited higher dissatisfaction than larger ($\\geq$50 occupants) offices. Dissatisfaction varied substantially across parameters including ceiling heights, number of workstations, and years of work, but not between offices with fixed seating compared to more flexible and activity-based working configurations. Overall, these findings highlight the complexities in characterizing occupants' perceptions using instrumental acoustic measurements.","authors":["Manuj Yadav","Jungsoo Kim","Valtteri Hongisto","Densil Cabrera","Richard de Dear"],"url":"https://arxiv.org/abs/2501.15744"}
{"created":"2025-05-06","title":"Deep learning model for ECG reconstruction reveals the information content of ECG leads","abstract":"This study introduces a deep learning model based on the U-net architecture to reconstruct missing leads in electrocardiograms (ECGs). The model was trained to reconstruct 12-lead ECG data from reduced lead configurations using publicly available datasets. The results highlight the ability of the model to quantify the information content of each ECG lead and its inter-lead correlations. This has significant implications for optimizing lead selection in diagnostic scenarios, particularly in settings where complete 12-lead ECGs are impractical. In addition, the study provides insights into the physiological underpinnings of ECG signals and their propagation. The findings pave the way for advances in telemedicine, portable ECG devices, and personalized cardiac diagnostics by reducing redundancy and improving signal interpretation.","authors":["Tomasz Gradowski","Teodor Buchner"],"url":"https://arxiv.org/abs/2502.00559"}
{"created":"2025-05-06","title":"The Towers of Fibonacci, Lucas, Pell, and Jacobsthal","abstract":"We present in this paper four new variants of the Tower of Hanoi problem, the optimal solution of each of these variants is related to one of the four known numbers Fibonacci, Lucas, Pell, and Jacobsthal. We give an optimal solution to each of these variants, and we present their associated graphs.","authors":["El-Mehdi Mehiri","Saad Mneimneh","Hac\\`ene Belbachir"],"url":"https://arxiv.org/abs/2502.11045"}
{"created":"2025-05-06","title":"Deep Neural OFDM Receivers: Two Novel Architectures for BER-BLER Optimization and Comparison with State-of-the-Art Architectures","abstract":"Neural receivers have recently become a popular topic, where the received signals can be directly decoded by data driven mechanisms such as machine learning and deep learning. In this paper, we propose two novel neural network based orthogonal frequency division multiplexing (OFDM) receivers performing channel estimation and equalization tasks and directly predicting log likelihood ratios (LLRs) from the received in phase and quadrature phase (IQ) signals. The first network, the Dual Attention Transformer (DAT), employs a state of the art (SOTA) transformer architecture with an attention mechanism. The second network, the Residual Dual Non Local Attention Network (RDNLA), utilizes a parallel residual architecture with a non local attention block. The bit error rate (BER) and block error rate (BLER) performance of various SOTA neural receiver architectures is compared with our proposed methods across different signal to noise ratio (SNR) levels. The simulation results show that DAT and RDNLA outperform both traditional communication systems and existing neural receiver models. The computational efficiency of the proposed neural receivers supports their feasibility for next generation communication systems.","authors":["Erhan Karakoca","H\\\"useyin \\c{C}evik","\\.Ibrahim H\\\"okelek","Ali G\\\"or\\c{c}in"],"url":"https://arxiv.org/abs/2503.20500"}
{"created":"2025-05-06","title":"Descriptive Complexity of Sensitivity of Cellular Automata","abstract":"We study the computational complexity of determining whether a cellular automaton is sensitive to initial conditions. We show that this problem is $\\Pi^0_2$-complete in dimension 1 and $\\Sigma^0_3$-complete in dimension 2 and higher. This solves a question posed by Sablik and Theyssier.","authors":["Tom Favereau","Ville Salo"],"url":"https://arxiv.org/abs/2504.05012"}
{"created":"2025-05-06","title":"Further Comments on Yablo's Construction","abstract":"We continue our analysis of Yablo's coding of the liar paradox by infinite acyclic graphs. The present notes are based on and continue the author's previous results on the problem. In particular, our approach is often more systematic than before.","authors":["Karl Schlechta"],"url":"https://arxiv.org/abs/2504.10370"}
{"created":"2025-05-06","title":"Fast Computation of the Discrete Fourier Transform Rectangular Index Coefficients","abstract":"In~\\cite{sic-magazine-2025}, the authors show that the square index coefficients (SICs) of the $N$-point discrete Fourier transform (DFT) -- that is, the coefficients $X_{k\\sqrt{N}}$ for $k = 0, 1, \\ldots, \\sqrt{N} - 1$ -- can be losslessly compressed from $N$ to $\\sqrt{N}$ points, thereby accelerating the computation of these specific DFT coefficients accordingly. Following up on that, in this article we generalize SICs into what we refer to as rectangular index coefficients (RICs) of the DFT, formalized as $X_{kL}, k=0,1,\\cdots,C-1$, in which the integers $C$ and $L$ are generic roots of $N$ such that $N=LC$. We present an algorithm to compress the $N$-point input signal $\\mathbf{x}$ into a $C$-point signal $\\mathbf{\\hat{x}}$ at the expense of $\\mathcal{O}(N)$ complex sums and no complex multiplication. We show that a DFT on $\\mathbf{\\hat{x}}$ is equivalent to a DFT on the RICs of $\\mathbf{x}$. In cases where specific frequencies of $\\mathbf{x}$ are of interest -- as in harmonic analysis -- one can conveniently adjust the signal parameters (e.g., frequency resolution) to align the RICs with those frequencies, and use the proposed algorithm to compute them significantly faster. If $N$ is a power of two -- as required by the fast Fourier transform (FFT) algorithm -- then $C$ can be any power of two in the range $[2, N/2]$ and one can use our algorithm along with FFT to compute all RICs in $\\mathcal{O}(C\\log C)$ time complexity.","authors":["Saulo Queiroz","Jo\\~ao P. Vilela","Benjamin Koon Kei Ng","Chan-Tong Lam","Edmundo Monteiro"],"url":"https://arxiv.org/abs/2504.12551"}
{"created":"2025-05-06","title":"A Graph Theoretic Approach for Exploring the Relationship between EV Adoption and Charging Infrastructure Growth","abstract":"The increasing global demand for conventional energy has led to significant challenges, particularly due to rising CO2 emissions and the depletion of natural resources. In the U.S., light-duty vehicles contribute significantly to transportation sector emissions, prompting a global shift toward electrified vehicles (EVs). Among the challenges that thwart the widespread adoption of EVs is the insufficient charging infrastructure (CI). This study focuses on exploring the complex relationship between EV adoption and CI growth. Employing a graph theoretic approach, we propose a graph model to analyze correlations between EV adoption and CI growth across 137 counties in six states. We examine how different time granularities impact these correlations in two distinct scenarios: Early Adoption and Late Adoption. Further, we conduct causality tests to assess the directional relationship between EV adoption and CI growth in both scenarios. Our main findings reveal that analysis using lower levels of time granularity result in more homogeneous clusters, with notable differences between clusters in EV adoption and those in CI growth. Additionally, we identify causal relationships between EV adoption and CI growth in 137 counties, and show that causality is observed more frequently in Early Adoption scenarios than in Late Adoption ones. However, the causal effects in Early Adoption are slower than those in Late Adoption.","authors":["Fahad S. Alrasheedi","Hesham H. Ali"],"url":"https://arxiv.org/abs/2504.13902"}
{"created":"2025-05-06","title":"Efficient Decomposition of Forman-Ricci Curvature on Vietoris-Rips Complexes and Data Applications","abstract":"Discrete Forman-Ricci curvature (FRC) is an efficient tool that characterizes essential geometrical features and associated transitions of real-world networks, extending seamlessly to higher-dimensional computations in simplicial complexes. In this article, we provide two major advancements: First, we give a decomposition for FRC, enabling local computations of FRC. Second, we construct a set-theoretical proof enabling an efficient algorithm for the local computation of FRC in Vietoris-Rips (VR) complexes.Strikingly, this approach reveals critical information and geometric insights often overlooked by conventional classification techniques. Our findings open new avenues for geometric computations in VR complexes and highlight an essential yet under-explored aspect of data classification: the geometry underpinning statistical patterns.","authors":["Danillo Barros de Souza","Jonatas Teodomiro","Fernando A. N. Santos","Mengjun Ding","Weiqiang Sun","Mathieu Desroches","J\\\"urgen Jost","Serafim Rodrigues"],"url":"https://arxiv.org/abs/2504.21601"}
{"created":"2025-05-06","title":"A note on the quantum Wielandt inequality","abstract":"In this note, we prove that the index of primitivity of any primitive unital Schwarz map is at most $2(D-1)^2$, where $D$ is the dimension of the underlying matrix algebra. This inequality was first proved by Rahaman for Schwarz maps which were both unital and trace preserving. As we show, the assumption of unitality is basically innocuous, but in general not all primitive unital Schwarz maps are trace preserving. Therefore, the precise purpose of this note is to showcase how to apply the method of Rahaman to unital primitive Schwarz maps that don't preserve trace. As a corollary of this theorem, we show that the index of primitivity of any primitive 2-positive map is at most $2(D-1)^2$, so in particular this bound holds for arbitrary primitive completely positive maps. We briefly discuss of how this relates to a conjecture of Perez-Garcia, Verstraete, Wolf and Cirac.","authors":["Owen Ekblad"],"url":"https://arxiv.org/abs/2504.21638"}
{"created":"2025-05-06","title":"Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model","abstract":"The Hawkes process (HP) is commonly used to model event sequences with self-reinforcing dynamics, including electronic health records (EHRs). Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which is often critical in healthcare. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs yet more interpretable than other neural network approaches, and allows us to explicitly trade flexibility for interpretability by adding transformer encoder layers to further contextualize the event embeddings. Results show that our method accurately recovers impact functions in simulations, achieves competitive performance on MIMIC-IV procedure dataset, and gains clinically meaningful interpretation on XX-EHR with children diagnosis dataset even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics in EHRs and other data effectively, implying that interpretability can be maintained without loss of performance.","authors":["Yuankang Zhao","Matthew Engelhard"],"url":"https://arxiv.org/abs/2504.21795"}
