{"created":"2025-04-24","title":"Blockchain-Driven Solutions for Carbon Credit Trading: A Decentralized Platform for SMEs","abstract":"The increasing demand for sustainability and compliance with global carbon regulations has posed significant challenges for small and medium-sized enterprises (SMEs). This paper proposes a blockchain-based decentralized carbon credit trading platform tailored for SMEs in Taiwan, aiming to simplify the complex carbon trading process and lower market entry barriers. Drawing upon the Diffusion of Innovations theory and transaction cost economics, we illustrate how blockchain technology can reduce informational asymmetry and intermediary costs in carbon markets. By integrating Ethereum-based smart contracts, the platform automates transactions, enhances transparency, and reduces administrative burdens - addressing key obstacles such as technical complexity and market risks. A controlled experimental design was conducted to compare the proposed system with a conventional centralized carbon trading platform. Statistical analysis confirms its effectiveness in minimizing time and expenses while ensuring compliance with the Carbon Border Adjustment Mechanism (CBAM) and the Clean Competition Act (CCA). User satisfaction was measured using the Kano model, with the results identifying essential features and prioritizing future enhancements. This study contributes a more comprehensive solution for SMEs seeking to achieve carbon neutrality, underscoring the transformative potential of blockchain technology in global carbon markets.","authors":["Yun-Cheng Tsai"],"url":"https://arxiv.org/abs/2504.16085"}
{"created":"2025-04-24","title":"Digital Kitchen Remodeling: Editing and Relighting Intricate Indoor Scenes from a Single Panorama","abstract":"We present a novel virtual staging application for kitchen remodeling from a single panorama. To ensure the realism of the virtual rendered scene, we capture real-world High Dynamic Range (HDR) panoramas and recover the absolute scene radiance for high-quality scene relighting. Our application pipeline consists of three key components: (1) HDR photography for capturing paired indoor and outdoor panoramas, (2) automatic kitchen layout generation with new kitchen components, and (3) an editable rendering pipeline that flexibly edits scene materials and relights the new virtual scene with global illumination. Additionally, we contribute a novel Pano-Pano HDR dataset with 141 paired indoor and outdoor panoramas and present a low-cost photometric calibration method for panoramic HDR photography.","authors":["Guanzhou Ji","Azadeh O. Sawyer","Srinivasa G. Narasimhan"],"url":"https://arxiv.org/abs/2504.16086"}
{"created":"2025-04-24","title":"Surveillance Disguised as Protection: A Comparative Analysis of Sideloaded and In-Store Parental Control Apps","abstract":"Parental control applications, software tools designed to manage and monitor children's online activities, serve as essential safeguards for parents in the digital age. However, their usage has sparked concerns about security and privacy violations inherent in various child monitoring products. Sideloaded software (i. e. apps installed outside official app stores) poses an increased risk, as it is not bound by the regulations of trusted platforms. Despite this, the market of sideloaded parental control software has remained widely unexplored by the research community. This paper examines 20 sideloaded parental control apps and compares them to 20 apps available on the Google Play Store. We base our analysis on privacy policies, Android package kit (APK) files, application behaviour, network traffic and application functionalities. Our findings reveal that sideloaded parental control apps fall short compared to their in-store counterparts, lacking specialised parental control features and safeguards against misuse while concealing themselves on the user's device. Alarmingly, three apps transmitted sensitive data unencrypted, half lacked a privacy policy and 8 out of 20 were flagged for potential stalkerware indicators of compromise (IOC).","authors":["Eva-Maria Maier","Leonie Maria Tanczer","Lukas Daniel Klausner"],"url":"https://arxiv.org/abs/2504.16087"}
{"created":"2025-04-24","title":"Paths Not Taken: A Secure Computing Tutorial","abstract":"This paper is a tutorial on the proven but currently under-appreciated security mechanisms associated with \"tagged\" or \"descriptor\" architectures. The tutorial shows how the principles behind such architectures can be applied to mitigate or eliminate vulnerabilities. The tutorial incorporates systems engineering practices by presenting the mechanisms in an informal model of an integrated artifact in its operational environment. The artifact is a special-purpose hardware/software system called a \"Guard\" which robustly hosts defensive software. It is hoped that this tutorial may encourage teachers to include significant past work in their curricula and students who are self-teaching to add that work to their exploration of secure computing.","authors":["William Earl Boebert"],"url":"https://arxiv.org/abs/2504.16088"}
{"created":"2025-04-24","title":"Carbyne: An Ultra-Lightweight DoS-Resilient Mempool for Bitcoin","abstract":"The increasing adoption of cryptocurrencies has significantly amplified the resource requirements for operating full nodes, creating substantial barriers to entry. Unlike miners, who are financially incentivized through block rewards and transaction fees, full nodes lack direct economic compensation for their critical role in maintaining the network. A key resource burden is the transaction pool, which is particularly memory-intensive as it temporarily stores unconfirmed transactions awaiting verification and propagation across the network. We present Neonpool, a novel optimization for transaction pool leveraging bloom filter variants to drastically reduce memory consumption by up to 200 (e.g., 400 MB to 2 MB) while maintaining over 99.99% transaction processing accuracy. Implemented in C++ and evaluated on unique Bitcoin and Ethereum datasets, Neonpool enables efficient operation on lightweight clients, such as smartphones, IoT devices, and systems-on-a-chip, without requiring a hard fork. By lowering the cost of node participation, Neonpool enhances decentralization and strengthens the overall security and robustness of cryptocurrency networks.","authors":["Hina Binte Haq","Syed Taha Ali","Asad Salman","Patrick McCorry","Siamak F. Shahandashti"],"url":"https://arxiv.org/abs/2504.16089"}
{"created":"2025-04-24","title":"Launching Insights: A Pilot Study on Leveraging Real-World Observational Data from the Mayo Clinic Platform to Advance Clinical Research","abstract":"Backgrounds: Artificial intelligence (AI) is transforming healthcare, yet translating AI models from theoretical frameworks to real-world clinical applications remains challenging. The Mayo Clinic Platform (MCP) was established to address these challenges by providing a scalable ecosystem that integrates real-world multiple modalities data from multiple institutions, advanced analytical tools, and secure computing environments to support clinical research and AI development. Methods: In this study, we conducted four research projects leveraging MCP's data infrastructure and analytical capabilities to demonstrate its potential in facilitating real-world evidence generation and AI-driven clinical insights. Utilizing MCP's tools and environment, we facilitated efficient cohort identification, data extraction, and subsequent statistical or AI-powered analyses. Results: The results underscore MCP's role in accelerating translational research by offering de-identified, standardized real-world data and facilitating AI model validation across diverse healthcare settings. Compared to Mayo's internal Electronic Health Record (EHR) data, MCP provides broader accessibility, enhanced data standardization, and multi-institutional integration, making it a valuable resource for both internal and external researchers. Conclusion: Looking ahead, MCP is well-positioned to transform clinical research through its scalable ecosystem, effectively bridging the divide between AI innovation and clinical deployment. Future investigations will build upon this foundation, further exploring MCP's capacity to advance precision medicine and enhance patient outcomes.","authors":["Yue Yu","Xinyue Hu","Sivaraman Rajaganapathy","Jingna Feng","Ahmed Abdelhameed","Xiaodi Li","Jianfu Li","Ken Liu","Liu Yang","Nilufer Taner","Phil Fiero","Soulmaz Boroumand","Richard Larsen","Maneesh Goyal","Clark Otley","Nansu Zong","John Halamka","Cui Tao"],"url":"https://arxiv.org/abs/2504.16090"}
{"created":"2025-04-24","title":"Post-Quantum Homomorphic Encryption: A Case for Code-Based Alternatives","abstract":"Homomorphic Encryption (HE) allows secure and privacy-protected computation on encrypted data without the need to decrypt it. Since Shor's algorithm rendered prime factorisation and discrete logarithm-based ciphers insecure with quantum computations, researchers have been working on building post-quantum homomorphic encryption (PQHE) algorithms. Most of the current PQHE algorithms are secured by Lattice-based problems and there have been limited attempts to build ciphers based on error-correcting code-based problems. This review presents an overview of the current approaches to building PQHE schemes and justifies code-based encryption as a novel way to diversify post-quantum algorithms. We present the mathematical underpinnings of existing code-based cryptographic frameworks and their security and efficiency guarantees. We compare lattice-based and code-based homomorphic encryption solutions identifying challenges that have inhibited the progress of code-based schemes. We finally propose five new research directions to advance post-quantum code-based homomorphic encryption.","authors":["Siddhartha Siddhiprada Bhoi","Arathi Arakala","Amy Beth Corman","Asha Rao"],"url":"https://arxiv.org/abs/2504.16091"}
{"created":"2025-04-24","title":"Cooperative Speech, Semantic Competence, and AI","abstract":"Cooperative speech is purposive. From the speaker's perspective, one crucial purpose is the transmission of knowledge. Cooperative speakers care about getting things right for their conversational partners. This attitude is a kind of respect. Cooperative speech is an ideal form of communication because participants have respect for each other. And having respect within a cooperative enterprise is sufficient for a particular kind of moral standing: we ought to respect those who have respect for us. Respect demands reciprocity. I maintain that large language models aren't owed the kind of respect that partly constitutes a cooperative conversation. This implies that they aren't cooperative interlocutors, otherwise we would be obliged to reciprocate the attitude. Leveraging this conclusion, I argue that present-day LLMs are incapable of assertion and that this raises an overlooked doubt about their semantic competence. One upshot of this argument is that knowledge of meaning isn't just a subject for the cognitive psychologist. It's also a subject for the moral psychologist.","authors":["Mahrad Almotahari"],"url":"https://arxiv.org/abs/2504.16092"}
{"created":"2025-04-24","title":"NeRF-APT: A New NeRF Framework for Wireless Channel Prediction","abstract":"Neural radiance fields (NeRFs) have recently attracted significant attention in the field of wireless channel prediction, primarily due to their capability for high-fidelity reconstruction of complex wireless measurement environments. However, the ray-tracing component of NeRF-based methods faces challenges in realistically representing wireless scenarios, mainly due to the limited expressive power of multilayer perceptrons (MLPs). To overcome this issue, in this paper, we propose NeRF-APT, an encoder-decoder architecture integrated within a NeRF-based wireless channel prediction framework. Our architecture leverages the strengths of NeRF-like models in learning environmental features and exploits encoder-decoder modules' capabilities for critical information extraction. Additionally, we incorporate an attention mechanism within the skip connections between encoder and decoder layers, significantly enhancing contextual understanding across layers. Extensive experimental evaluations conducted on several realistic and synthetic datasets demonstrate that our proposed method outperforms existing state-of-the-art approaches in wireless channel prediction.","authors":["Jingzhou Shen","Tianya Zhao","Yanzhao Wu","Xuyu Wang"],"url":"https://arxiv.org/abs/2504.16094"}
{"created":"2025-04-24","title":"Audio and Multiscale Visual Cues Driven Cross-modal Transformer for Idling Vehicle Detection","abstract":"Idling vehicle detection (IVD) supports real-time systems that reduce pollution and emissions by dynamically messaging drivers to curb excess idling behavior. In computer vision, IVD has become an emerging task that leverages video from surveillance cameras and audio from remote microphones to localize and classify vehicles in each frame as moving, idling, or engine-off. As with other cross-modal tasks, the key challenge lies in modeling the correspondence between audio and visual modalities, which differ in representation but provide complementary cues -- video offers spatial and motion context, while audio conveys engine activity beyond the visual field. The previous end-to-end model, which uses a basic attention mechanism, struggles to align these modalities effectively, often missing vehicle detections. To address this issue, we propose AVIVDNetv2, a transformer-based end-to-end detection network. It incorporates a cross-modal transformer with global patch-level learning, a multiscale visual feature fusion module, and decoupled detection heads. Extensive experiments show that AVIVDNetv2 improves mAP by 7.66 over the disjoint baseline and 9.42 over the E2E baseline, with consistent AP gains across all vehicle categories. Furthermore, AVIVDNetv2 outperforms the state-of-the-art method for sounding object localization, establishing a new performance benchmark on the AVIVD dataset.","authors":["Xiwen Li","Ross Whitaker","Tolga Tasdizen"],"url":"https://arxiv.org/abs/2504.16102"}
{"created":"2025-04-24","title":"Shape Your Ground: Refining Road Surfaces Beyond Planar Representations","abstract":"Road surface reconstruction from aerial images is fundamental for autonomous driving, urban planning, and virtual simulation, where smoothness, compactness, and accuracy are critical quality factors. Existing reconstruction methods often produce artifacts and inconsistencies that limit usability, while downstream tasks have a tendency to represent roads as planes for simplicity but at the cost of accuracy. We introduce FlexRoad, the first framework to directly address road surface smoothing by fitting Non-Uniform Rational B-Splines (NURBS) surfaces to 3D road points obtained from photogrammetric reconstructions or geodata providers. Our method at its core utilizes the Elevation-Constrained Spatial Road Clustering (ECSRC) algorithm for robust anomaly correction, significantly reducing surface roughness and fitting errors. To facilitate quantitative comparison between road surface reconstruction methods, we present GeoRoad Dataset (GeRoD), a diverse collection of road surface and terrain profiles derived from openly accessible geodata. Experiments on GeRoD and the photogrammetry-based DeepScenario Open 3D Dataset (DSC3D) demonstrate that FlexRoad considerably surpasses commonly used road surface representations across various metrics while being insensitive to various input sources, terrains, and noise types. By performing ablation studies, we identify the key role of each component towards high-quality reconstruction performance, making FlexRoad a generic method for realistic road surface modeling.","authors":["Oussema Dhaouadi","Johannes Meier","Jacques Kaiser","Daniel Cremers"],"url":"https://arxiv.org/abs/2504.16103"}
{"created":"2025-04-24","title":"Uncovering an Attractiveness Bias in Multimodal Large Language Models: A Case Study with LLaVA","abstract":"Physical attractiveness matters. It has been shown to influence human perception and decision-making, often leading to biased judgments that favor those deemed attractive in what is referred to as \"the attractiveness halo effect\". While extensively studied in human judgments in a broad set of domains, including hiring, judicial sentencing or credit granting, the role that attractiveness plays in the assessments and decisions made by multimodal large language models (MLLMs) is unknown. To address this gap, we conduct an empirical study using 91 socially relevant scenarios and a diverse dataset of 924 face images, corresponding to 462 individuals both with and without beauty filters applied to them, evaluated on LLaVA, a state-of-the-art, open source MLLM. Our analysis reveals that attractiveness impacts the decisions made by the MLLM in over 80% of the scenarios, demonstrating substantial bias in model behavior in what we refer to as an attractiveness bias. Similarly to humans, we find empirical evidence of the existence of the attractiveness halo effect, such that more attractive individuals are more likely to be attributed positive traits, such as intelligence or confidence, by the MLLM. Furthermore, we uncover a gender, age and race bias in 83%, 73% and 57% of the scenarios, respectively, which is impacted by attractiveness, particularly in the case of gender, highlighting the intersectional nature of the attractiveness bias. Our findings suggest that societal stereotypes and cultural norms intersect with perceptions of attractiveness, amplifying or mitigating this bias in multimodal generative AI models in a complex way. Our work emphasizes the need to account for intersectionality in algorithmic bias detection and mitigation efforts and underscores the challenges of addressing bias in modern multimodal large language models.","authors":["Aditya Gulati","Moreno D'Inc\\`a","Nicu Sebe","Bruno Lepri","Nuria Oliver"],"url":"https://arxiv.org/abs/2504.16104"}
{"created":"2025-04-24","title":"Updating Lower and Upper Bounds for the Job-Shop Scheduling Problem Test Instances","abstract":"The Job-Shop Scheduling Problem (JSSP) and its variant, the Flexible Job-Shop Scheduling Problem (FJSSP), are combinatorial optimization problems studied thoroughly in the literature. Generally, the aim is to reduce the makespan of a scheduling solution corresponding to a problem instance. Thus, finding upper and lower bounds for an optimal makespan enables the assessment of performances for multiple approaches addressed so far. We use OR-Tools, a solver portfolio, to compute new bounds for some open benchmark instances, in order to reduce the gap between upper and lower bounds. We find new numerical lower bounds for multiple benchmark instances, up to closing the Taillard's ta33 instance. We also improve upper bounds for four instances, namely Taillard's ta26 & ta45 and Dauzere's 05a & 06a. Additionally we share an optimal solution for Taillard's ta45 as well as Hurink-edata's car5.","authors":["Marc-Emmanuel Coupvent des Graviers (Safran Electronics and Defense","France)","Lotfi Kobrosly (Safran Electronics and Defense","France","LAMSADE","Universit\\'e Paris Dauphine-PSL","Place du Mar\\'echal de Lattre de Tassigny","Paris","France)","Christophe Guettier (Safran Electronics and Defense","France)","Tristan Cazenave (LAMSADE","Universit\\'e Paris Dauphine-PSL","Place du Mar\\'echal de Lattre de Tassigny","Paris","France)"],"url":"https://arxiv.org/abs/2504.16106"}
{"created":"2025-04-24","title":"Trusted Identities for AI Agents: Leveraging Telco-Hosted eSIM Infrastructure","abstract":"The rise of autonomous AI agents in enterprise and industrial environments introduces a critical challenge: how to securely assign, verify, and manage their identities across distributed systems. Existing identity frameworks based on API keys, certificates, or application-layer credentials lack the infrastructure-grade trust, lifecycle control, and interoperability needed to manage agents operating independently in sensitive contexts.","authors":["Sebastian Barros"],"url":"https://arxiv.org/abs/2504.16108"}
{"created":"2025-04-24","title":"Representation Learning for Tabular Data: A Comprehensive Survey","abstract":"Tabular data, structured as rows and columns, is among the most prevalent data types in machine learning classification and regression applications. Models for learning from tabular data have continuously evolved, with Deep Neural Networks (DNNs) recently demonstrating promising results through their capability of representation learning. In this survey, we systematically introduce the field of tabular representation learning, covering the background, challenges, and benchmarks, along with the pros and cons of using DNNs. We organize existing methods into three main categories according to their generalization capabilities: specialized, transferable, and general models. Specialized models focus on tasks where training and evaluation occur within the same data distribution. We introduce a hierarchical taxonomy for specialized models based on the key aspects of tabular data -- features, samples, and objectives -- and delve into detailed strategies for obtaining high-quality feature- and sample-level representations. Transferable models are pre-trained on one or more datasets and subsequently fine-tuned on downstream tasks, leveraging knowledge acquired from homogeneous or heterogeneous sources, or even cross-modalities such as vision and language. General models, also known as tabular foundation models, extend this concept further, allowing direct application to downstream tasks without fine-tuning. We group these general models based on the strategies used to adapt across heterogeneous datasets. Additionally, we explore ensemble methods, which integrate the strengths of multiple tabular models. Finally, we discuss representative extensions of tabular learning, including open-environment tabular machine learning, multimodal learning with tabular data, and tabular understanding. More information can be found in the following repository: https://github.com/LAMDA-Tabular/Tabular-Survey.","authors":["Jun-Peng Jiang","Si-Yang Liu","Hao-Run Cai","Qile Zhou","Han-Jia Ye"],"url":"https://arxiv.org/abs/2504.16109"}
{"created":"2025-04-24","title":"Security-First AI: Foundations for Robust and Trustworthy Systems","abstract":"The conversation around artificial intelligence (AI) often focuses on safety, transparency, accountability, alignment, and responsibility. However, AI security (i.e., the safeguarding of data, models, and pipelines from adversarial manipulation) underpins all of these efforts. This manuscript posits that AI security must be prioritized as a foundational layer. We present a hierarchical view of AI challenges, distinguishing security from safety, and argue for a security-first approach to enable trustworthy and resilient AI systems. We discuss core threat models, key attack vectors, and emerging defense mechanisms, concluding that a metric-driven approach to AI security is essential for robust AI safety, transparency, and accountability.","authors":["Krti Tallam"],"url":"https://arxiv.org/abs/2504.16110"}
{"created":"2025-04-24","title":"HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing","abstract":"The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.","authors":["Myunghyun Rhee","Joonseop Sim","Taeyoung Ahn","Seungyong Lee","Daegun Yoon","Euiseok Kim","Kyoung Park","Youngpyo Joo","Hosik Kim"],"url":"https://arxiv.org/abs/2504.16112"}
{"created":"2025-04-24","title":"AI-Based Vulnerability Analysis of NFT Smart Contracts","abstract":"In the research experiment of this article, our research work is divided into several stages. Firstly, we collected a large number of smart contract codes and classified them, identifying several common defects, including Risky Mutably Porxy, ERC-721 Recentrancy, Unlimited Mining, Missing Requirements, and Public Burns. Secondly, we used Python to process the smart contracts. On the one hand, we modified the file names, and on the other hand, we batched the process of the content for analysis and application. Next, we built a model of the decision tree. Firstly, we carried out the feature extraction. We selected the algorithm and divided the data. After comparing and processing, we chose the CART classification tree to process. By gene coefficient, we analyzed and sorted the data, and got the initial model of the decision tree. Then, we introduced the random forest model on the basis of the decision tree. From abstracting the same amount of samples to selecting features randomly.From adjusting and optimizing parameters to completing the construction of the forest model. Finally, we compared and analyzed the decision tree, random forest, and self-built model in the paper and drew general conclusions.","authors":["Xin Wang","Xiaoqi Li"],"url":"https://arxiv.org/abs/2504.16113"}
{"created":"2025-04-24","title":"Persistence-based Hough Transform for Line Detection","abstract":"The Hough transform is a popular and classical technique in computer vision for the detection of lines (or more general objects). It maps a pixel into a dual space -- the Hough space: each pixel is mapped to the set of lines through this pixel, which forms a curve in Hough space. The detection of lines then becomes a voting process to find those lines that received many votes by pixels. However, this voting is done by thresholding, which is susceptible to noise and other artifacts.","authors":["Johannes Ferner","Stefan Huber","Saverio Messineo","Angel Pop","Martin Uray"],"url":"https://arxiv.org/abs/2504.16114"}
{"created":"2025-04-24","title":"A Framework for Objective-Driven Dynamical Stochastic Fields","abstract":"Fields offer a versatile approach for describing complex systems composed of interacting and dynamic components. In particular, some of these dynamical and stochastic systems may exhibit goal-directed behaviors aimed at achieving specific objectives, which we refer to as $\\textit{intelligent fields}$. However, due to their inherent complexity, it remains challenging to develop a formal theoretical description of such systems and to effectively translate these descriptions into practical applications. In this paper, we propose three fundamental principles -- complete configuration, locality, and purposefulness -- to establish a theoretical framework for understanding intelligent fields. Moreover, we explore methodologies for designing such fields from the perspective of artificial intelligence applications. This initial investigation aims to lay the groundwork for future theoretical developments and practical advances in understanding and harnessing the potential of such objective-driven dynamical stochastic fields.","authors":["Yibo Jacky Zhang","Sanmi Koyejo"],"url":"https://arxiv.org/abs/2504.16115"}
{"created":"2025-04-24","title":"DMind Benchmark: The First Comprehensive Benchmark for LLM Evaluation in the Web3 Domain","abstract":"Recent advances in Large Language Models (LLMs) have led to significant progress on a wide range of natural language processing tasks. However, their effectiveness in specialized and rapidly evolving domains such as Web3 remains underexplored. In this paper, we introduce DMind Benchmark, a novel framework that systematically tests LLMs across nine key categories encompassing blockchain fundamentals, infrastructure, smart contract analysis, decentralized finance (DeFi), decentralized autonomous organizations (DAOs), non-fungible tokens (NFTs), token economics, meme concepts, and security vulnerabilities.","authors":["Miracle Master","Rainy Sun","Anya Reese","Joey Ouyang","Alex Chen","Winter Dong","Frank Li","James Yi","Garry Zhao","Tony Ling","Hobert Wong","Lowes Yang"],"url":"https://arxiv.org/abs/2504.16116"}
{"created":"2025-04-24","title":"Context-Awareness and Interpretability of Rare Occurrences for Discovery and Formalization of Critical Failure Modes","abstract":"Vision systems are increasingly deployed in critical domains such as surveillance, law enforcement, and transportation. However, their vulnerabilities to rare or unforeseen scenarios pose significant safety risks. To address these challenges, we introduce Context-Awareness and Interpretability of Rare Occurrences (CAIRO), an ontology-based human-assistive discovery framework for failure cases (or CP - Critical Phenomena) detection and formalization. CAIRO by design incentivizes human-in-the-loop for testing and evaluation of criticality that arises from misdetections, adversarial attacks, and hallucinations in AI black-box models. Our robust analysis of object detection model(s) failures in automated driving systems (ADS) showcases scalable and interpretable ways of formalizing the observed gaps between camera perception and real-world contexts, resulting in test cases stored as explicit knowledge graphs (in OWL/XML format) amenable for sharing, downstream analysis, logical reasoning, and accountability.","authors":["Sridevi Polavaram","Xin Zhou","Meenu Ravi","Mohammad Zarei","Anmol Srivastava"],"url":"https://arxiv.org/abs/2504.16117"}
{"created":"2025-04-24","title":"Towards Explainable and Lightweight AI for Real-Time Cyber Threat Hunting in Edge Networks","abstract":"As cyber threats continue to evolve, securing edge networks has become increasingly challenging due to their distributed nature and resource limitations. Many AI-driven threat detection systems rely on complex deep learning models, which, despite their high accuracy, suffer from two major drawbacks: lack of interpretability and high computational cost. Black-box AI models make it difficult for security analysts to understand the reasoning behind their predictions, limiting their practical deployment. Moreover, conventional deep learning techniques demand significant computational resources, rendering them unsuitable for edge devices with limited processing power. To address these issues, this study introduces an Explainable and Lightweight AI (ELAI) framework designed for real-time cyber threat detection in edge networks. Our approach integrates interpretable machine learning algorithms with optimized lightweight deep learning techniques, ensuring both transparency and computational efficiency. The proposed system leverages decision trees, attention-based deep learning, and federated learning to enhance detection accuracy while maintaining explainability. We evaluate ELAI using benchmark cybersecurity datasets, such as CICIDS and UNSW-NB15, assessing its performance across diverse cyberattack scenarios. Experimental results demonstrate that the proposed framework achieves high detection rates with minimal false positives, all while significantly reducing computational demands compared to traditional deep learning methods. The key contributions of this work include: (1) a novel interpretable AI-based cybersecurity model tailored for edge computing environments, (2) an optimized lightweight deep learning approach for real-time cyber threat detection, and (3) a comprehensive analysis of explainability techniques in AI-driven cybersecurity applications.","authors":["Milad Rahmati"],"url":"https://arxiv.org/abs/2504.16118"}
{"created":"2025-04-24","title":"A Data-Centric Approach for Safe and Secure Large Language Models against Threatening and Toxic Content","abstract":"Large Language Models (LLM) have made remarkable progress, but concerns about potential biases and harmful content persist. To address these apprehensions, we introduce a practical solution for ensuring LLM's safe and ethical use. Our novel approach focuses on a post-generation correction mechanism, the BART-Corrective Model, which adjusts generated content to ensure safety and security. Unlike relying solely on model fine-tuning or prompt engineering, our method provides a robust data-centric alternative for mitigating harmful content. We demonstrate the effectiveness of our approach through experiments on multiple toxic datasets, which show a significant reduction in mean toxicity and jail-breaking scores after integration. Specifically, our results show a reduction of 15% and 21% in mean toxicity and jail-breaking scores with GPT-4, a substantial reduction of 28% and 5% with PaLM2, a reduction of approximately 26% and 23% with Mistral-7B, and a reduction of 11.1% and 19% with Gemma-2b-it. These results demonstrate the potential of our approach to improve the safety and security of LLM, making them more suitable for real-world applications.","authors":["Chaima Njeh","Ha\\\"ifa Nakouri","Fehmi Jaafar"],"url":"https://arxiv.org/abs/2504.16120"}
{"created":"2025-04-24","title":"LegalRAG: A Hybrid RAG System for Multilingual Legal Information Retrieval","abstract":"Natural Language Processing (NLP) and computational linguistic techniques are increasingly being applied across various domains, yet their use in legal and regulatory tasks remains limited. To address this gap, we develop an efficient bilingual question-answering framework for regulatory documents, specifically the Bangladesh Police Gazettes, which contain both English and Bangla text. Our approach employs modern Retrieval Augmented Generation (RAG) pipelines to enhance information retrieval and response generation. In addition to conventional RAG pipelines, we propose an advanced RAG-based approach that improves retrieval performance, leading to more precise answers. This system enables efficient searching for specific government legal notices, making legal information more accessible. We evaluate both our proposed and conventional RAG systems on a diverse test set on Bangladesh Police Gazettes, demonstrating that our approach consistently outperforms existing methods across all evaluation metrics.","authors":["Muhammad Rafsan Kabir","Rafeed Mohammad Sultan","Fuad Rahman","Mohammad Ruhul Amin","Sifat Momen","Nabeel Mohammed","Shafin Rahman"],"url":"https://arxiv.org/abs/2504.16121"}
{"created":"2025-04-24","title":"SOTOPIA-S4: a user-friendly system for flexible, customizable, and large-scale social simulation","abstract":"Social simulation through large language model (LLM) agents is a promising approach to explore and validate hypotheses related to social science questions and LLM agents behavior. We present SOTOPIA-S4, a fast, flexible, and scalable social simulation system that addresses the technical barriers of current frameworks while enabling practitioners to generate multi-turn and multi-party LLM-based interactions with customizable evaluation metrics for hypothesis testing. SOTOPIA-S4 comes as a pip package that contains a simulation engine, an API server with flexible RESTful APIs for simulation management, and a web interface that enables both technical and non-technical users to design, run, and analyze simulations without programming. We demonstrate the usefulness of SOTOPIA-S4 with two use cases involving dyadic hiring negotiation and multi-party planning scenarios.","authors":["Xuhui Zhou","Zhe Su","Sophie Feng","Jiaxu Zhou","Jen-tse Huang","Hsien-Te Kao","Spencer Lynch","Svitlana Volkova","Tongshuang Sherry Wu","Anita Woolley","Hao Zhu","Maarten Sap"],"url":"https://arxiv.org/abs/2504.16122"}
{"created":"2025-04-24","title":"Breaking the Prompt Wall (I): A Real-World Case Study of Attacking ChatGPT via Lightweight Prompt Injection","abstract":"This report presents a real-world case study demonstrating how prompt injection can attack large language model platforms such as ChatGPT according to a proposed injection framework. By providing three real-world examples, we show how adversarial prompts can be injected via user inputs, web-based retrieval, and system-level agent instructions. These attacks, though lightweight and low-cost, can cause persistent and misleading behaviors in LLM outputs. Our case study reveals that even commercial-grade LLMs remain vulnerable to subtle manipulations that bypass safety filters and influence user decisions. \\textbf{More importantly, we stress that this report is not intended as an attack guide, but as a technical alert. As ethical researchers, we aim to raise awareness and call upon developers, especially those at OpenAI, to treat prompt-level security as a critical design priority.","authors":["Xiangyu Chang","Guang Dai","Hao Di","Haishan Ye"],"url":"https://arxiv.org/abs/2504.16125"}
{"created":"2025-04-24","title":"MonoTher-Depth: Enhancing Thermal Depth Estimation via Confidence-Aware Distillation","abstract":"Monocular depth estimation (MDE) from thermal images is a crucial technology for robotic systems operating in challenging conditions such as fog, smoke, and low light. The limited availability of labeled thermal data constrains the generalization capabilities of thermal MDE models compared to foundational RGB MDE models, which benefit from datasets of millions of images across diverse scenarios. To address this challenge, we introduce a novel pipeline that enhances thermal MDE through knowledge distillation from a versatile RGB MDE model. Our approach features a confidence-aware distillation method that utilizes the predicted confidence of the RGB MDE to selectively strengthen the thermal MDE model, capitalizing on the strengths of the RGB model while mitigating its weaknesses. Our method significantly improves the accuracy of the thermal MDE, independent of the availability of labeled depth supervision, and greatly expands its applicability to new scenarios. In our experiments on new scenarios without labeled depth, the proposed confidence-aware distillation method reduces the absolute relative error of thermal MDE by 22.88\\% compared to the baseline without distillation.","authors":["Xingxing Zuo","Nikhil Ranganathan","Connor Lee","Georgia Gkioxari","Soon-Jo Chung"],"url":"https://arxiv.org/abs/2504.16127"}
{"created":"2025-04-24","title":"Hybrid Knowledge Transfer through Attention and Logit Distillation for On-Device Vision Systems in Agricultural IoT","abstract":"Integrating deep learning applications into agricultural IoT systems faces a serious challenge of balancing the high accuracy of Vision Transformers (ViTs) with the efficiency demands of resource-constrained edge devices. Large transformer models like the Swin Transformers excel in plant disease classification by capturing global-local dependencies. However, their computational complexity (34.1 GFLOPs) limits applications and renders them impractical for real-time on-device inference. Lightweight models such as MobileNetV3 and TinyML would be suitable for on-device inference but lack the required spatial reasoning for fine-grained disease detection. To bridge this gap, we propose a hybrid knowledge distillation framework that synergistically transfers logit and attention knowledge from a Swin Transformer teacher to a MobileNetV3 student model. Our method includes the introduction of adaptive attention alignment to resolve cross-architecture mismatch (resolution, channels) and a dual-loss function optimizing both class probabilities and spatial focus. On the lantVillage-Tomato dataset (18,160 images), the distilled MobileNetV3 attains 92.4% accuracy relative to 95.9% for Swin-L but at an 95% reduction on PC and < 82% in inference latency on IoT devices. (23ms on PC CPU and 86ms/image on smartphone CPUs). Key innovations include IoT-centric validation metrics (13 MB memory, 0.22 GFLOPs) and dynamic resolution-matching attention maps. Comparative experiments show significant improvements over standalone CNNs and prior distillation methods, with a 3.5% accuracy gain over MobileNetV3 baselines. Significantly, this work advances real-time, energy-efficient crop monitoring in precision agriculture and demonstrates how we can attain ViT-level diagnostic precision on edge devices. Code and models will be made available for replication after acceptance.","authors":["Stanley Mugisha","Rashid Kisitu","Florence Tushabe"],"url":"https://arxiv.org/abs/2504.16128"}
{"created":"2025-04-24","title":"MARFT: Multi-Agent Reinforcement Fine-Tuning","abstract":"LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks requiring multifaceted reasoning and collaboration, from generating high-quality presentation slides to conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methodologies to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a universal algorithmic framework tailored for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We begin by reviewing the evolution from RL to Reinforcement Fine-Tuning, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a novel, LaMAS-oriented formulation of RFT. Central to this work is the presentation of a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work aims to serve as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.","authors":["Junwei Liao","Muning Wen","Jun Wang","Weinan Zhang"],"url":"https://arxiv.org/abs/2504.16129"}
{"created":"2025-04-24","title":"Efficacy of a Computer Tutor that Models Expert Human Tutors","abstract":"Tutoring is highly effective for promoting learning. However, the contribution of expertise to tutoring effectiveness is unclear and continues to be debated. We conducted a 9-week learning efficacy study of an intelligent tutoring system (ITS) for biology modeled on expert human tutors with two control conditions: human tutors who were experts in the domain but not in tutoring and a no-tutoring condition. All conditions were supplemental to classroom instruction, and students took learning tests immediately before and after tutoring sessions as well as delayed tests 1-2 weeks later. Analysis using logistic mixed-effects modeling indicates significant positive effects on the immediate post-test for the ITS (d =.71) and human tutors (d =.66) which are in the 99th percentile of meta-analytic effects, as well as significant positive effects on the delayed post-test for the ITS (d =.36) and human tutors (d =.39). We discuss implications for the role of expertise in tutoring and the design of future studies.","authors":["Andrew M. Olney","Sidney K. D'Mello","Natalie Person","Whitney Cade","Patrick Hays","Claire W. Dempsey","Blair Lehman","Betsy Williams","Art Graesser"],"url":"https://arxiv.org/abs/2504.16132"}
{"created":"2025-04-24","title":"A Conceptual Framework for AI-based Decision Systems in Critical Infrastructures","abstract":"The interaction between humans and AI in safety-critical systems presents a unique set of challenges that remain partially addressed by existing frameworks. These challenges stem from the complex interplay of requirements for transparency, trust, and explainability, coupled with the necessity for robust and safe decision-making. A framework that holistically integrates human and AI capabilities while addressing these concerns is notably required, bridging the critical gaps in designing, deploying, and maintaining safe and effective systems. This paper proposes a holistic conceptual framework for critical infrastructures by adopting an interdisciplinary approach. It integrates traditionally distinct fields such as mathematics, decision theory, computer science, philosophy, psychology, and cognitive engineering and draws on specialized engineering domains, particularly energy, mobility, and aeronautics. The flexibility in its adoption is also demonstrated through its instantiation on an already existing framework.","authors":["Milad Leyli-abadi","Ricardo J. Bessa","Jan Viebahn","Daniel Boos","Clark Borst","Alberto Castagna","Ricardo Chavarriaga","Mohamed Hassouna","Bruno Lemetayer","Giulia Leto","Antoine Marot","Maroua Meddeb","Manuel Meyer","Viola Schiaffonati","Manuel Schneider","Toni Waefler"],"url":"https://arxiv.org/abs/2504.16133"}
{"created":"2025-04-24","title":"Multimodal Large Language Models for Enhanced Traffic Safety: A Comprehensive Review and Future Trends","abstract":"Traffic safety remains a critical global challenge, with traditional Advanced Driver-Assistance Systems (ADAS) often struggling in dynamic real-world scenarios due to fragmented sensor processing and susceptibility to adversarial conditions. This paper reviews the transformative potential of Multimodal Large Language Models (MLLMs) in addressing these limitations by integrating cross-modal data such as visual, spatial, and environmental inputs to enable holistic scene understanding. Through a comprehensive analysis of MLLM-based approaches, we highlight their capabilities in enhancing perception, decision-making, and adversarial robustness, while also examining the role of key datasets (e.g., KITTI, DRAMA, ML4RoadSafety) in advancing research. Furthermore, we outline future directions, including real-time edge deployment, causality-driven reasoning, and human-AI collaboration. By positioning MLLMs as a cornerstone for next-generation traffic safety systems, this review underscores their potential to revolutionize the field, offering scalable, context-aware solutions that proactively mitigate risks and improve overall road safety.","authors":["Mohammad Abu Tami","Mohammed Elhenawy","Huthaifa I. Ashqar"],"url":"https://arxiv.org/abs/2504.16134"}
{"created":"2025-04-24","title":"Optimizing Post-Cancer Treatment Prognosis: A Study of Machine Learning and Ensemble Techniques","abstract":"The aim is to create a method for accurately estimating the duration of post-cancer treatment, particularly focused on chemotherapy, to optimize patient care and recovery. This initiative seeks to improve the effectiveness of cancer treatment, emphasizing the significance of each patient's journey and well-being. Our focus is to provide patients with valuable insight into their treatment timeline because we deeply believe that every life matters. We combined medical expertise with smart technology to create a model that accurately predicted each patient's treatment timeline. By using machine learning, we personalized predictions based on individual patient details which were collected from a regional government hospital named Sylhet M.A.G. Osmani Medical College & Hospital, Sylhet, Bangladesh, improving cancer care effectively. We tackled the challenge by employing around 13 machine learning algorithms and analyzing 15 distinct features, including LR, SVM, DT, RF, etc. we obtained a refined precision in predicting cancer patient's treatment durations. Furthermore, we utilized ensemble techniques to reinforce the accuracy of our methods. Notably, our study revealed that our majority voting ensemble classifier displayed exceptional performance, achieving 77% accuracy, with LightGBM and Random Forest closely following at approximately 76% accuracy. Our research unveiled the inherent complexities of cancer datasets, as seen in the Decision Tree's 59% accuracy. This emphasizes the need for improved algorithms to better predict outcomes and enhance patient care. Our comparison with other methods confirmed our promising accuracy rates, showing the potential impact of our approach in improving cancer treatment strategies. This study marks a significant step forward in optimizing post-cancer treatment prognosis using machine learning and ensemble techniques.","authors":["Joyee Chakraborty","Mazahrul Islam Tohin","Danbir Rashid","Tanjil Ahmed Tanmoy","Md. Jehadul Islam Mony"],"url":"https://arxiv.org/abs/2504.16135"}
{"created":"2025-04-24","title":"Active Learning Methods for Efficient Data Utilization and Model Performance Enhancement","abstract":"In the era of data-driven intelligence, the paradox of data abundance and annotation scarcity has emerged as a critical bottleneck in the advancement of machine learning. This paper gives a detailed overview of Active Learning (AL), which is a strategy in machine learning that helps models achieve better performance using fewer labeled examples. It introduces the basic concepts of AL and discusses how it is used in various fields such as computer vision, natural language processing, transfer learning, and real-world applications. The paper focuses on important research topics such as uncertainty estimation, handling of class imbalance, domain adaptation, fairness, and the creation of strong evaluation metrics and benchmarks. It also shows that learning methods inspired by humans and guided by questions can improve data efficiency and help models learn more effectively. In addition, this paper talks about current challenges in the field, including the need to rebuild trust, ensure reproducibility, and deal with inconsistent methodologies. It points out that AL often gives better results than passive learning, especially when good evaluation measures are used. This work aims to be useful for both researchers and practitioners by providing key insights and proposing directions for future progress in active learning.","authors":["Chiung-Yi Tseng","Junhao Song","Ziqian Bi","Tianyang Wang","Chia Xin Liang","Ming Liu"],"url":"https://arxiv.org/abs/2504.16136"}
{"created":"2025-04-24","title":"Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark","abstract":"We present the Virology Capabilities Test (VCT), a large language model (LLM) benchmark that measures the capability to troubleshoot complex virology laboratory protocols. Constructed from the inputs of dozens of PhD-level expert virologists, VCT consists of $322$ multimodal questions covering fundamental, tacit, and visual knowledge that is essential for practical work in virology laboratories. VCT is difficult: expert virologists with access to the internet score an average of $22.1\\%$ on questions specifically in their sub-areas of expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$ accuracy, outperforming $94\\%$ of expert virologists even within their sub-areas of specialization. The ability to provide expert-level virology troubleshooting is inherently dual-use: it is useful for beneficial research, but it can also be misused. Therefore, the fact that publicly available models outperform virologists on VCT raises pressing governance considerations. We propose that the capability of LLMs to provide expert-level troubleshooting of dual-use virology work should be integrated into existing frameworks for handling dual-use technologies in the life sciences.","authors":["Jasper G\\\"otting","Pedro Medeiros","Jon G Sanders","Nathaniel Li","Long Phan","Karam Elabd","Lennart Justen","Dan Hendrycks","Seth Donoughe"],"url":"https://arxiv.org/abs/2504.16137"}
{"created":"2025-04-24","title":"Trends in Frontier AI Model Count: A Forecast to 2028","abstract":"Governments are starting to impose requirements on AI models based on how much compute was used to train them. For example, the EU AI Act imposes requirements on providers of general-purpose AI with systemic risk, which includes systems trained using greater than $10^{25}$ floating point operations (FLOP). In the United States' AI Diffusion Framework, a training compute threshold of $10^{26}$ FLOP is used to identify \"controlled models\" which face a number of requirements. We explore how many models such training compute thresholds will capture over time. We estimate that by the end of 2028, there will be between 103-306 foundation models exceeding the $10^{25}$ FLOP threshold put forward in the EU AI Act (90% CI), and 45-148 models exceeding the $10^{26}$ FLOP threshold that defines controlled models in the AI Diffusion Framework (90% CI). We also find that the number of models exceeding these absolute compute thresholds each year will increase superlinearly -- that is, each successive year will see more new models captured within the threshold than the year before. Thresholds that are defined with respect to the largest training run to date (for example, such that all models within one order of magnitude of the largest training run to date are captured by the threshold) see a more stable trend, with a median forecast of 14-16 models being captured by this definition annually from 2025-2028.","authors":["Iyngkarran Kumar","Sam Manning"],"url":"https://arxiv.org/abs/2504.16138"}
{"created":"2025-04-24","title":"Enhancing Trust Through Standards: A Comparative Risk-Impact Framework for Aligning ISO AI Standards with Global Ethical and Regulatory Contexts","abstract":"As artificial intelligence (AI) reshapes industries and societies, ensuring its trustworthiness-through mitigating ethical risks like bias, opacity, and accountability deficits-remains a global challenge. International Organization for Standardization (ISO) AI standards, such as ISO/IEC 24027 and 24368, aim to foster responsible development by embedding fairness, transparency, and risk management into AI systems. However, their effectiveness varies across diverse regulatory landscapes, from the EU's risk-based AI Act to China's stability-focused measures and the U.S.'s fragmented state-led initiatives. This paper introduces a novel Comparative Risk-Impact Assessment Framework to evaluate how well ISO standards address ethical risks within these contexts, proposing enhancements to strengthen their global applicability. By mapping ISO standards to the EU AI Act and surveying regulatory frameworks in ten regions-including the UK, Canada, India, Japan, Singapore, South Korea, and Brazil-we establish a baseline for ethical alignment. The framework, applied to case studies in the EU, US-Colorado, and China, reveals gaps: voluntary ISO standards falter in enforcement (e.g., Colorado) and undervalue region-specific risks like privacy (China). We recommend mandatory risk audits, region-specific annexes, and a privacy-focused module to enhance ISO's adaptability. This approach not only synthesizes global trends but also offers a replicable tool for aligning standardization with ethical imperatives, fostering interoperability and trust in AI worldwide. Policymakers and standards bodies can leverage these insights to evolve AI governance, ensuring it meets diverse societal needs as the technology advances.","authors":["Sridharan Sankaran"],"url":"https://arxiv.org/abs/2504.16139"}
{"created":"2025-04-24","title":"SparseJEPA: Sparse Representation Learning of Joint Embedding Predictive Architectures","abstract":"Joint Embedding Predictive Architectures (JEPA) have emerged as a powerful framework for learning general-purpose representations. However, these models often lack interpretability and suffer from inefficiencies due to dense embedding representations. We propose SparseJEPA, an extension that integrates sparse representation learning into the JEPA framework to enhance the quality of learned representations. SparseJEPA employs a penalty method that encourages latent space variables to be shared among data features with strong semantic relationships, while maintaining predictive performance. We demonstrate the effectiveness of SparseJEPA by training on the CIFAR-100 dataset and pre-training a lightweight Vision Transformer. The improved embeddings are utilized in linear-probe transfer learning for both image classification and low-level tasks, showcasing the architecture's versatility across different transfer tasks. Furthermore, we provide a theoretical proof that demonstrates that the grouping mechanism enhances representation quality. This was done by displaying that grouping reduces Multiinformation among latent-variables, including proofing the Data Processing Inequality for Multiinformation. Our results indicate that incorporating sparsity not only refines the latent space but also facilitates the learning of more meaningful and interpretable representations. In further work, hope to further extend this method by finding new ways to leverage the grouping mechanism through object-centric representation learning.","authors":["Max Hartman","Lav Varshney"],"url":"https://arxiv.org/abs/2504.16140"}
{"created":"2025-04-24","title":"Deep Learning Meets Process-Based Models: A Hybrid Approach to Agricultural Challenges","abstract":"Process-based models (PBMs) and deep learning (DL) are two key approaches in agricultural modelling, each offering distinct advantages and limitations. PBMs provide mechanistic insights based on physical and biological principles, ensuring interpretability and scientific rigour. However, they often struggle with scalability, parameterisation, and adaptation to heterogeneous environments. In contrast, DL models excel at capturing complex, nonlinear patterns from large datasets but may suffer from limited interpretability, high computational demands, and overfitting in data-scarce scenarios.","authors":["Yue Shi","Liangxiu Han","Xin Zhang","Tam Sobeih","Thomas Gaiser","Nguyen Huu Thuy","Dominik Behrend","Amit Kumar Srivastava","Krishnagopal Halder","Frank Ewert"],"url":"https://arxiv.org/abs/2504.16141"}
{"created":"2025-04-24","title":"Detecting Actionable Requests and Offers on Social Media During Crises Using LLMs","abstract":"Natural disasters often result in a surge of social media activity, including requests for assistance, offers of help, sentiments, and general updates. To enable humanitarian organizations to respond more efficiently, we propose a fine-grained hierarchical taxonomy to systematically organize crisis-related information about requests and offers into three critical dimensions: supplies, emergency personnel, and actions. Leveraging the capabilities of Large Language Models (LLMs), we introduce Query-Specific Few-shot Learning (QSF Learning) that retrieves class-specific labeled examples from an embedding database to enhance the model's performance in detecting and classifying posts. Beyond classification, we assess the actionability of messages to prioritize posts requiring immediate attention. Extensive experiments demonstrate that our approach outperforms baseline prompting strategies, effectively identifying and prioritizing actionable requests and offers.","authors":["Ahmed El Fekih Zguir","Ferda Ofli","Muhammad Imran"],"url":"https://arxiv.org/abs/2504.16144"}
{"created":"2025-04-24","title":"Progressive Language-guided Visual Learning for Multi-Task Visual Grounding","abstract":"Multi-task visual grounding (MTVG) includes two sub-tasks, i.e., Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES). The existing representative approaches generally follow the research pipeline which mainly consists of three core procedures, including independent feature extraction for visual and linguistic modalities, respectively, cross-modal interaction module, and independent prediction heads for different sub-tasks. Albeit achieving remarkable performance, this research line has two limitations: 1) The linguistic content has not been fully injected into the entire visual backbone for boosting more effective visual feature extraction and it needs an extra cross-modal interaction module; 2) The relationship between REC and RES tasks is not effectively exploited to help the collaborative prediction for more accurate output. To deal with these problems, in this paper, we propose a Progressive Language-guided Visual Learning framework for multi-task visual grounding, called PLVL, which not only finely mine the inherent feature expression of the visual modality itself but also progressively inject the language information to help learn linguistic-related visual features. In this manner, our PLVL does not need additional cross-modal fusion module while fully introducing the language guidance. Furthermore, we analyze that the localization center for REC would help identify the to-be-segmented object region for RES to some extent. Inspired by this investigation, we design a multi-task head to accomplish collaborative predictions for these two sub-tasks. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that our PLVL obviously outperforms the representative methods in both REC and RES tasks. https://github.com/jcwang0602/PLVL","authors":["Jingchao Wang","Hong Wang","Wenlong Zhang","Kunhua Ji","Dingjiang Huang","Yefeng Zheng"],"url":"https://arxiv.org/abs/2504.16145"}
{"created":"2025-04-24","title":"Towards responsible AI for education: Hybrid human-AI to confront the Elephant in the room","abstract":"Despite significant advancements in AI-driven educational systems and ongoing calls for responsible AI for education, several critical issues remain unresolved -- acting as the elephant in the room within AI in education, learning analytics, educational data mining, learning sciences, and educational psychology communities. This critical analysis identifies and examines nine persistent challenges that continue to undermine the fairness, transparency, and effectiveness of current AI methods and applications in education. These include: (1) the lack of clarity around what AI for education truly means -- often ignoring the distinct purposes, strengths, and limitations of different AI families -- and the trend of equating it with domain-agnostic, company-driven large language models; (2) the widespread neglect of essential learning processes such as motivation, emotion, and (meta)cognition in AI-driven learner modelling and their contextual nature; (3) limited integration of domain knowledge and lack of stakeholder involvement in AI design and development; (4) continued use of non-sequential machine learning models on temporal educational data; (5) misuse of non-sequential metrics to evaluate sequential models; (6) use of unreliable explainable AI methods to provide explanations for black-box models; (7) ignoring ethical guidelines in addressing data inconsistencies during model training; (8) use of mainstream AI methods for pattern discovery and learning analytics without systematic benchmarking; and (9) overemphasis on global prescriptions while overlooking localised, student-specific recommendations. Supported by theoretical and empirical research, we demonstrate how hybrid AI methods -- specifically neural-symbolic AI -- can address the elephant in the room and serve as the foundation for responsible, trustworthy AI systems in education.","authors":["Danial Hooshyar","Gustav \\v{S}\\'ir","Yeongwook Yang","Eve Kikas","Raija H\\\"am\\\"al\\\"ainen","Tommi K\\\"arkk\\\"ainen","Dragan Ga\\v{s}evi\\'c","Roger Azevedo"],"url":"https://arxiv.org/abs/2504.16148"}
{"created":"2025-04-24","title":"Classification of Firn Data via Topological Features","abstract":"In this paper we evaluate the performance of topological features for generalizable and robust classification of firn image data, with the broader goal of understanding the advantages, pitfalls, and trade-offs in topological featurization. Firn refers to layers of granular snow within glaciers that haven't been compressed into ice. This compactification process imposes distinct topological and geometric structure on firn that varies with depth within the firn column, making topological data analysis (TDA) a natural choice for understanding the connection between depth and structure. We use two classes of topological features, sublevel set features and distance transform features, together with persistence curves, to predict sample depth from microCT images. A range of challenging training-test scenarios reveals that no one choice of method dominates in all categories, and uncoveres a web of trade-offs between accuracy, interpretability, and generalizability.","authors":["Sarah Day","Jesse Dimino","Matt Jester","Kaitlin Keegan","Thomas Weighill"],"url":"https://arxiv.org/abs/2504.16150"}
{"created":"2025-04-24","title":"Leveraging Social Media Analytics for Sustainability Trend Detection in Saudi Arabias Evolving Market","abstract":"Saudi Arabias rapid economic growth and social evolution under Vision 2030 present a unique opportunity to track emerging trends in real time. Uncovering trends in real time can open up new avenues for business and investment opportunities. This paper explores how AI and social media analytics can uncover and monitor these trends across sectors like sustainability, construction, food beverages industry, tourism, technology, and entertainment. This paper focus on use of AI-driven methodology to identify sustainability trends across Saudi Arabia. We processed millions of social media posts, news, blogs in order to understand sustainability trends in the region. The paper presents an AI approach that can help economists, businesses, government to understand sustainability trends and make better decisions around them. This approach offers both sector-specific and cross-sector insights, giving decision-makers a reliable, up to date snapshot of Saudi Arabias market shifts. Beyond Saudi Arabia, this framework also shows potential for adapting to other regions. Overall, our findings highlight how by using AI-methodologies, give decision makers a reliable method to understand how initiatives are perceived and adopted by the public and understand growth of trends.","authors":["Kanwal Aalijah"],"url":"https://arxiv.org/abs/2504.16153"}
{"created":"2025-04-24","title":"DATETIME: A new benchmark to measure LLM translation and reasoning capabilities","abstract":"This paper introduces DATETIME, a new high-quality benchmark designed to evaluate the translation and reasoning abilities of a Large Language Model (LLM) on datetimes. A datetime is simply a date and a time, for example '11th.february.2023 ,1:12:31'. Datetimes are an interesting domain because they are intuitive and straightforward for humans to process but present significant challenges for LLMs. At the time of writing, no publicly available benchmark exists for systematically evaluating LLMs on datetime processing. Our experiments show that state-of-the-art models exhibit significant difficulty with tasks involving reasoning on datetimes, and that General Artificial Intelligence is still a distant aspiration. We hypothesize that working with datetimes necessitates translation and/or computation capabilities, and the tasks of the benchmark are organized accordingly. Significant dispersion in performance across models is observed with surprisingly poor performance even on apparently trivial tasks. Whilst frontier models such as ChatGPT, Claude and Llama3.1 have evidently been built and trained with datetime reasoning abilities, significant improvement is required for the open-source models.","authors":["Edward Gaere","Florian Wangenheim"],"url":"https://arxiv.org/abs/2504.16155"}
{"created":"2025-04-24","title":"A detection-task-specific deep-learning method to improve the quality of sparse-view myocardial perfusion SPECT images","abstract":"Myocardial perfusion imaging (MPI) with single-photon emission computed tomography (SPECT) is a widely used and cost-effective diagnostic tool for coronary artery disease. However, the lengthy scanning time in this imaging procedure can cause patient discomfort, motion artifacts, and potentially inaccurate diagnoses due to misalignment between the SPECT scans and the CT-scans which are acquired for attenuation compensation. Reducing projection angles is a potential way to shorten scanning time, but this can adversely impact the quality of the reconstructed images. To address this issue, we propose a detection-task-specific deep-learning method for sparse-view MPI SPECT images. This method integrates an observer loss term that penalizes the loss of anthropomorphic channel features with the goal of improving performance in perfusion defect-detection task. We observed that, on the task of detecting myocardial perfusion defects, the proposed method yielded an area under the receiver operating characteristic (ROC) curve (AUC) significantly larger than the sparse-view protocol. Further, the proposed method was observed to be able to restore the structure of the left ventricle wall, demonstrating ability to overcome sparse-sampling artifacts. Our preliminary results motivate further evaluations of the method.","authors":["Zezhang Yang","Zitong Yu","Nuri Choi","Abhinav K. Jha"],"url":"https://arxiv.org/abs/2504.16171"}
{"created":"2025-04-24","title":"Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning","abstract":"High-dimensional partial differential equations (PDEs) pose significant computational challenges across fields ranging from quantum chemistry to economics and finance. Although scientific machine learning (SciML) techniques offer approximate solutions, they often suffer from bias and neglect crucial physical insights. Inspired by inference-time scaling strategies in language models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML), a physics-informed framework that dynamically refines and debiases the SCiML predictions during inference by enforcing the physical laws. SCaSML leverages derived new physical laws that quantifies systematic errors and employs Monte Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to dynamically correct the prediction. Both numerical and theoretical analysis confirms enhanced convergence rates via compute-optimal inference methods. Our numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared to the base surrogate model, establishing it as the first algorithm to refine approximated solutions to high-dimensional PDE during inference. Code of SCaSML is available at https://github.com/Francis-Fan-create/SCaSML.","authors":["Zexi Fan","Yan Sun","Shihao Yang","Yiping Lu"],"url":"https://arxiv.org/abs/2504.16172"}
{"created":"2025-04-24","title":"FPGA-Based Neural Network Accelerators for Space Applications: A Survey","abstract":"Space missions are becoming increasingly ambitious, necessitating high-performance onboard spacecraft computing systems. In response, field-programmable gate arrays (FPGAs) have garnered significant interest due to their flexibility, cost-effectiveness, and radiation tolerance potential. Concurrently, neural networks (NNs) are being recognized for their capability to execute space mission tasks such as autonomous operations, sensor data analysis, and data compression. This survey serves as a valuable resource for researchers aiming to implement FPGA-based NN accelerators in space applications. By analyzing existing literature, identifying trends and gaps, and proposing future research directions, this work highlights the potential of these accelerators to enhance onboard computing systems.","authors":["Pedro Antunes","Artur Podobas"],"url":"https://arxiv.org/abs/2504.16173"}
{"created":"2025-04-24","title":"A Systematic Literature Review of Software Engineering Research on Jupyter Notebook","abstract":"Context: Jupyter Notebook has emerged as a versatile tool that transforms how researchers, developers, and data scientists conduct and communicate their work. As the adoption of Jupyter notebooks continues to rise, so does the interest from the software engineering research community in improving the software engineering practices for Jupyter notebooks.","authors":["Md Saeed Siddik","Hao Li","Cor-Paul Bezemer"],"url":"https://arxiv.org/abs/2504.16180"}
{"created":"2025-04-24","title":"CLIP-IT: CLIP-based Pairing for Histology Images Classification","abstract":"Multimodal learning has shown significant promise for improving medical image analysis by integrating information from complementary data sources. This is widely employed for training vision-language models (VLMs) for cancer detection based on histology images and text reports. However, one of the main limitations in training these VLMs is the requirement for large paired datasets, raising concerns over privacy, and data collection, annotation, and maintenance costs. To address this challenge, we introduce CLIP-IT method to train a vision backbone model to classify histology images by pairing them with privileged textual information from an external source. At first, the modality pairing step relies on a CLIP-based model to match histology images with semantically relevant textual report data from external sources, creating an augmented multimodal dataset without the need for manually paired samples. Then, we propose a multimodal training procedure that distills the knowledge from the paired text modality to the unimodal image classifier for enhanced performance without the need for the textual data during inference. A parameter-efficient fine-tuning method is used to efficiently address the misalignment between the main (image) and paired (text) modalities. During inference, the improved unimodal histology classifier is used, with only minimal additional computational complexity. Our experiments on challenging PCAM, CRC, and BACH histology image datasets show that CLIP-IT can provide a cost-effective approach to leverage privileged textual information and outperform unimodal classifiers for histology.","authors":["Banafsheh Karimian (LIVIA ILLS Dept. of Systems Engineering ETS Montreal Canada)","Giulia Avanzato (Dept. of Computer Engineering University of Cagliari Italy)","Soufian Belharbi (LIVIA ILLS Dept. of Systems Engineering ETS Montreal Canada)","Luke McCaffrey (Goodman Cancer Research Centre Dept. of Oncology McGill University Canada)","Mohammadhadi Shateri (LIVIA ILLS Dept. of Systems Engineering ETS Montreal Canada)","Eric Granger (LIVIA ILLS Dept. of Systems Engineering ETS Montreal Canada)"],"url":"https://arxiv.org/abs/2504.16181"}
{"created":"2025-04-24","title":"Measuring Uncertainty in Shape Completion to Improve Grasp Quality","abstract":"Shape completion networks have been used recently in real-world robotic experiments to complete the missing/hidden information in environments where objects are only observed in one or few instances where self-occlusions are bound to occur. Nowadays, most approaches rely on deep neural networks that handle rich 3D point cloud data that lead to more precise and realistic object geometries. However, these models still suffer from inaccuracies due to its nondeterministic/stochastic inferences which could lead to poor performance in grasping scenarios where these errors compound to unsuccessful grasps. We present an approach to calculate the uncertainty of a 3D shape completion model during inference of single view point clouds of an object on a table top. In addition, we propose an update to grasp pose algorithms quality score by introducing the uncertainty of the completed point cloud present in the grasp candidates. To test our full pipeline we perform real world grasping with a 7dof robotic arm with a 2 finger gripper on a large set of household objects and compare against previous approaches that do not measure uncertainty. Our approach ranks the grasp quality better, leading to higher grasp success rate for the rank 5 grasp candidates compared to state of the art.","authors":["Nuno Ferreira Duarte","Seyed S. Mohammadi","Plinio Moreno","Alessio Del Bue","Jose Santos-Victor"],"url":"https://arxiv.org/abs/2504.16183"}
{"created":"2025-04-24","title":"FinNLI: Novel Dataset for Multi-Genre Financial Natural Language Inference Benchmarking","abstract":"We introduce FinNLI, a benchmark dataset for Financial Natural Language Inference (FinNLI) across diverse financial texts like SEC Filings, Annual Reports, and Earnings Call transcripts. Our dataset framework ensures diverse premise-hypothesis pairs while minimizing spurious correlations. FinNLI comprises 21,304 pairs, including a high-quality test set of 3,304 instances annotated by finance experts. Evaluations show that domain shift significantly degrades general-domain NLI performance. The highest Macro F1 scores for pre-trained (PLMs) and large language models (LLMs) baselines are 74.57% and 78.62%, respectively, highlighting the dataset's difficulty. Surprisingly, instruction-tuned financial LLMs perform poorly, suggesting limited generalizability. FinNLI exposes weaknesses in current LLMs for financial reasoning, indicating room for improvement.","authors":["Jabez Magomere","Elena Kochkina","Samuel Mensah","Simerjot Kaur","Charese H. Smiley"],"url":"https://arxiv.org/abs/2504.16188"}
{"created":"2025-04-24","title":"Quality of explanation of xAI from the prespective of Italian end-users: Italian version of System Causability Scale (SCS)","abstract":"Background and aim: Considering the scope of the application of artificial intelligence beyond the field of computer science, one of the concerns of researchers is to provide quality explanations about the functioning of algorithms based on artificial intelligence and the data extracted from it. The purpose of the present study is to validate the Italian version of system causability scale (I-SCS) to measure the quality of explanations provided in a xAI.","authors":["Carmine Attanasio","Alireza Mortezapour"],"url":"https://arxiv.org/abs/2504.16193"}
{"created":"2025-04-24","title":"Adaptive continuity-preserving simplification of street networks","abstract":"Street network data is widely used to study human-based activities and urban structure. Often, these data are geared towards transportation applications, which require highly granular, directed graphs that capture the complex relationships of potential traffic patterns. While this level of network detail is critical for certain fine-grained mobility models, it represents a hindrance for studies concerned with the morphology of the street network. For the latter case, street network simplification - the process of converting a highly granular input network into its most simple morphological form - is a necessary, but highly tedious preprocessing step, especially when conducted manually. In this manuscript, we develop and present a novel adaptive algorithm for simplifying street networks that is both fully automated and able to mimic results obtained through a manual simplification routine. The algorithm - available in the neatnet Python package - outperforms current state-of-the-art procedures when comparing those methods to manually, human-simplified data, while preserving network continuity.","authors":["Martin Fleischmann","Anastassia Vybornova","James D. Gaboardi","Anna Br\\'azdov\\'a","Daniela Dan\\v{c}ejov\\'a"],"url":"https://arxiv.org/abs/2504.16198"}
{"created":"2025-04-24","title":"Reflexive Prompt Engineering: A Framework for Responsible Prompt Engineering and Interaction Design","abstract":"Responsible prompt engineering has emerged as a critical framework for ensuring that generative artificial intelligence (AI) systems serve society's needs while minimizing potential harms. As generative AI applications become increasingly powerful and ubiquitous, the way we instruct and interact with them through prompts has profound implications for fairness, accountability, and transparency. This article examines how strategic prompt engineering can embed ethical and legal considerations and societal values directly into AI interactions, moving beyond mere technical optimization for functionality. This article proposes a comprehensive framework for responsible prompt engineering that encompasses five interconnected components: prompt design, system selection, system configuration, performance evaluation, and prompt management. Drawing from empirical evidence, the paper demonstrates how each component can be leveraged to promote improved societal outcomes while mitigating potential risks. The analysis reveals that effective prompt engineering requires a delicate balance between technical precision and ethical consciousness, combining the systematic rigor and focus on functionality with the nuanced understanding of social impact. Through examination of real-world and emerging practices, the article illustrates how responsible prompt engineering serves as a crucial bridge between AI development and deployment, enabling organizations to fine-tune AI outputs without modifying underlying model architectures. This approach aligns with broader \"Responsibility by Design\" principles, embedding ethical considerations directly into the implementation process rather than treating them as post-hoc additions. The article concludes by identifying key research directions and practical guidelines for advancing the field of responsible prompt engineering.","authors":["Christian Djeffal"],"url":"https://arxiv.org/abs/2504.16204"}
{"created":"2025-04-24","title":"A Theory of Spectral CSP Sparsification","abstract":"We initiate the study of spectral sparsification for instances of Constraint Satisfaction Problems (CSPs). In particular, we introduce a notion of the \\emph{spectral energy} of a fractional assignment for a Boolean CSP instance, and define a \\emph{spectral sparsifier} as a weighted subset of constraints that approximately preserves this energy for all fractional assignments. Our definition not only strengthens the combinatorial notion of a CSP sparsifier but also extends well-studied concepts such as spectral sparsifiers for graphs and hypergraphs.","authors":["Sanjeev Khanna","Aaron Putterman","Madhu Sudan"],"url":"https://arxiv.org/abs/2504.16206"}
{"created":"2025-04-24","title":"HTN Plan Repair Algorithms Compared: Strengths and Weaknesses of Different Methods","abstract":"This paper provides theoretical and empirical comparisons of three recent hierarchical plan repair algorithms: SHOPFixer, IPyHOPPER, and Rewrite. Our theoretical results show that the three algorithms correspond to three different definitions of the plan repair problem, leading to differences in the algorithms' search spaces, the repair problems they can solve, and the kinds of repairs they can make. Understanding these distinctions is important when choosing a repair method for any given application.","authors":["Paul Zaidins","Robert P. Goldman","Ugur Kuter","Dana Nau","Mark Roberts"],"url":"https://arxiv.org/abs/2504.16209"}
{"created":"2025-04-24","title":"One-Point Sampling for Distributed Bandit Convex Optimization with Time-Varying Constraints","abstract":"This paper considers the distributed bandit convex optimization problem with time-varying constraints. In this problem, the global loss function is the average of all the local convex loss functions, which are unknown beforehand. Each agent iteratively makes its own decision subject to time-varying inequality constraints which can be violated but are fulfilled in the long run. For a uniformly jointly strongly connected time-varying directed graph, a distributed bandit online primal-dual projection algorithm with one-point sampling is proposed. We show that sublinear dynamic network regret and network cumulative constraint violation are achieved if the path-length of the benchmark also increases in a sublinear manner. In addition, an $\\mathcal{O}({T^{3/4 + g}})$ static network regret bound and an $\\mathcal{O}( {{T^{1 - {g}/2}}} )$ network cumulative constraint violation bound are established, where $T$ is the total number of iterations and $g \\in ( {0,1/4} )$ is a trade-off parameter. Moreover, a reduced static network regret bound $\\mathcal{O}( {{T^{2/3 + 4g /3}}} )$ is established for strongly convex local loss functions. Finally, a numerical example is presented to validate the theoretical results.","authors":["Kunpeng Zhang","Lei Xu","Xinlei Yi","Guanghui Wen","Lihua Xie","Tianyou Chai","Tao Yang"],"url":"https://arxiv.org/abs/2504.16211"}
{"created":"2025-04-24","title":"TinyML for Speech Recognition","abstract":"We train and deploy a quantized 1D convolutional neural network model to conduct speech recognition on a highly resource-constrained IoT edge device. This can be useful in various Internet of Things (IoT) applications, such as smart homes and ambient assisted living for the elderly and people with disabilities, just to name a few examples. In this paper, we first create a new dataset with over one hour of audio data that enables our research and will be useful to future studies in this field. Second, we utilize the technologies provided by Edge Impulse to enhance our model's performance and achieve a high Accuracy of up to 97% on our dataset. For the validation, we implement our prototype using the Arduino Nano 33 BLE Sense microcontroller board. This microcontroller board is specifically designed for IoT and AI applications, making it an ideal choice for our target use case scenarios. While most existing research focuses on a limited set of keywords, our model can process 23 different keywords, enabling complex commands.","authors":["Andrew Barovic","Armin Moin"],"url":"https://arxiv.org/abs/2504.16213"}
{"created":"2025-04-24","title":"Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis","abstract":"Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL quantization techniques demand a new matrix multiplication operator with mixed input data types, further complicating GPU optimization. Prior high-level compilers like Triton lack the expressiveness to implement key optimizations like fine-grained data pipelines and hardware-friendly memory layouts for these operators, while low-level programming models, such as Hidet, Graphene, and CUTLASS, require significant programming efforts. To balance expressiveness with engineering effort, we propose Hexcute, a tile-based programming language that exposes shared memory and register abstractions to enable fine-grained optimization for these operators. Additionally, Hexcute leverages task mapping to schedule the GPU program, and to reduce programming efforts, it automates layout and task mapping synthesis with a novel type-inference-based algorithm. Our evaluation shows that Hexcute generalizes to a wide range of DL operators, achieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type operators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation.","authors":["Xiao Zhang","Yaoyao Ding","Yang Hu","Gennady Pekhimenko"],"url":"https://arxiv.org/abs/2504.16214"}
{"created":"2025-04-24","title":"Identifying Process Improvement Opportunities through Process Execution Benchmarking","abstract":"Benchmarking functionalities in current commercial process mining tools allow organizations to contextualize their process performance through high-level performance indicators, such as completion rate or throughput time. However, they do not suggest any measures to close potential performance gaps. To address this limitation, we propose a prescriptive technique for process execution benchmarking that recommends targeted process changes to improve process performance. The technique compares an event log from an ``own'' process to one from a selected benchmark process to identify potential activity replacements, based on behavioral similarity. It then evaluates each proposed change in terms of its feasibility and its estimated performance impact. The result is a list of potential process modifications that can serve as evidence-based decision support for process improvement initiatives.","authors":["Luka Abb","Majid Rafiei","Timotheus Kampik","Jana-Rebecca Rehse"],"url":"https://arxiv.org/abs/2504.16215"}
{"created":"2025-04-24","title":"ReGraph: A Tool for Binary Similarity Identification","abstract":"Binary Code Similarity Detection (BCSD) is not only essential for security tasks such as vulnerability identification but also for code copying detection, yet it remains challenging due to binary stripping and diverse compilation environments. Existing methods tend to adopt increasingly complex neural networks for better accuracy performance. The computation time increases with the complexity. Even with powerful GPUs, the treatment of large-scale software becomes time-consuming. To address these issues, we present a framework called ReGraph to efficiently compare binary code functions across architectures and optimization levels. Our evaluation with public datasets highlights that ReGraph exhibits a significant speed advantage, performing 700 times faster than Natural Language Processing (NLP)-based methods while maintaining comparable accuracy results with respect to the state-of-the-art models.","authors":["Li Zhou","Marc Dacier","Charalambos Konstantinou"],"url":"https://arxiv.org/abs/2504.16219"}
{"created":"2025-04-24","title":"Nash Equilibrium Learning In Large Populations With First Order Payoff Modifications","abstract":"We establish Nash equilibrium learning -- convergence of the population state to a suitably defined Nash equilibria set -- for a class of payoff dynamical mechanism with a first order modification. The first order payoff modification can model aspects of the agents' bounded rationality, anticipatory or averaging terms in the payoff mechanism, or first order Pad\\'e approximations of delays. To obtain our main results, we apply a combination of two nonstandard system-theoretic passivity notions.","authors":["Matthew S. Hankins","Jair Cert\\'orio","Tzuyu Jeng","Nuno C. Martins"],"url":"https://arxiv.org/abs/2504.16222"}
{"created":"2025-04-24","title":"Mass-Adaptive Admittance Control for Robotic Manipulators","abstract":"Handling objects with unknown or changing masses is a common challenge in robotics, often leading to errors or instability if the control system cannot adapt in real-time. In this paper, we present a novel approach that enables a six-degrees-of-freedom robotic manipulator to reliably follow waypoints while automatically estimating and compensating for unknown payload weight. Our method integrates an admittance control framework with a mass estimator, allowing the robot to dynamically update an excitation force to compensate for the payload mass. This strategy mitigates end-effector sagging and preserves stability when handling objects of unknown weights. We experimentally validated our approach in a challenging pick-and-place task on a shelf with a crossbar, improved accuracy in reaching waypoints and compliant motion compared to a baseline admittance-control scheme. By safely accommodating unknown payloads, our work enhances flexibility in robotic automation and represents a significant step forward in adaptive control for uncertain environments.","authors":["Hossein Gholampour","Jonathon E. Slightam","Logan E. Beaver"],"url":"https://arxiv.org/abs/2504.16224"}
{"created":"2025-04-24","title":"Blockchain Meets Adaptive Honeypots: A Trust-Aware Approach to Next-Gen IoT Security","abstract":"Edge computing-based Next-Generation Wireless Networks (NGWN)-IoT offer enhanced bandwidth capacity for large-scale service provisioning but remain vulnerable to evolving cyber threats. Existing intrusion detection and prevention methods provide limited security as adversaries continually adapt their attack strategies. We propose a dynamic attack detection and prevention approach to address this challenge. First, blockchain-based authentication uses the Deoxys Authentication Algorithm (DAA) to verify IoT device legitimacy before data transmission. Next, a bi-stage intrusion detection system is introduced: the first stage uses signature-based detection via an Improved Random Forest (IRF) algorithm. In contrast, the second stage applies feature-based anomaly detection using a Diffusion Convolution Recurrent Neural Network (DCRNN). To ensure Quality of Service (QoS) and maintain Service Level Agreements (SLA), trust-aware service migration is performed using Heap-Based Optimization (HBO). Additionally, on-demand virtual High-Interaction honeypots deceive attackers and extract attack patterns, which are securely stored using the Bimodal Lattice Signature Scheme (BLISS) to enhance signature-based Intrusion Detection Systems (IDS). The proposed framework is implemented in the NS3 simulation environment and evaluated against existing methods across multiple performance metrics, including accuracy, attack detection rate, false negative rate, precision, recall, ROC curve, memory usage, CPU usage, and execution time. Experimental results demonstrate that the framework significantly outperforms existing approaches, reinforcing the security of NGWN-enabled IoT ecosystems","authors":["Yazan Otoum","Arghavan Asad","Amiya Nayak"],"url":"https://arxiv.org/abs/2504.16226"}
{"created":"2025-04-24","title":"Towards a Distributed Federated Learning Aggregation Placement using Particle Swarm Intelligence","abstract":"Federated learning has become a promising distributed learning concept with extra insurance on data privacy. Extensive studies on various models of Federated learning have been done since the coinage of its term. One of the important derivatives of federated learning is hierarchical semi-decentralized federated learning, which distributes the load of the aggregation task over multiple nodes and parallelizes the aggregation workload at the breadth of each level of the hierarchy. Various methods have also been proposed to perform inter-cluster and intra-cluster aggregation optimally. Most of the solutions, nonetheless, require monitoring the nodes' performance and resource consumption at each round, which necessitates frequently exchanging systematic data. To optimally perform distributed aggregation in SDFL with minimal reliance on systematic data, we propose Flag-Swap, a Particle Swarm Optimization (PSO) method that optimizes the aggregation placement according only to the processing delay. Our simulation results show that PSO-based placement can find the optimal placement relatively fast, even in scenarios with many clients as candidates for aggregation. Our real-world docker-based implementation of Flag-Swap over the recently emerged FL framework shows superior performance compared to black-box-based deterministic placement strategies, with about 43% minutes faster than random placement, and 32% minutes faster than uniform placement, in terms of total processing time.","authors":["Amir Ali-Pour","Sadra Bekrani","Laya Samizadeh","Julien Gascon-Samson"],"url":"https://arxiv.org/abs/2504.16227"}
{"created":"2025-04-24","title":"Fast, Space-Optimal Streaming Algorithms for Clustering and Subspace Embeddings","abstract":"We show that both clustering and subspace embeddings can be performed in the streaming model with the same asymptotic efficiency as in the central/offline setting.","authors":["Vincent Cohen-Addad","Liudeng Wang","David P. Woodruff","Samson Zhou"],"url":"https://arxiv.org/abs/2504.16229"}
{"created":"2025-04-24","title":"Quasitubal Tensor Algebra Over Separable Hilbert Spaces","abstract":"Value Decomposition and Eckart-Young-like optimality results. Underlying the tubal tensor framework is a view of a tensor as a matrix of finite sized tubes. In this work, we lay the mathematical and computational foundations for working with tensors with infinite size tubes: matrices whose elements are elements from a separable Hilbert space. A key challenge is that existence of important desired matrix-mimetic features of tubal tensors rely on the existence of a unit element in the ring of tubes. Such unit element cannot exist for tubes which are elements of an infinite-dimensional Hilbert space. We sidestep this issue by embedding the tubal space in a commutative unital C*-algebra of bounded operators. The resulting quasitubal algebra recovers the structural properties needed for decomposition and low-rank approximation. In addition to laying the theoretical groundwork for working with tubal tensors with infinite dimensional tubes, we discuss computational aspects of our construction, and provide a numerical illustration where we compute a finite dimensional approximation to a infinitely-sized synthetic tensor using our theory. We believe our theory opens new exciting avenues for applying matrix mimetic tensor framework in the context of inherently infinite dimensional problems.","authors":["Uria Mor","Haim Avron"],"url":"https://arxiv.org/abs/2504.16231"}
{"created":"2025-04-24","title":"Using Phonemes in cascaded S2S translation pipeline","abstract":"This paper explores the idea of using phonemes as a textual representation within a conventional multilingual simultaneous speech-to-speech translation pipeline, as opposed to the traditional reliance on text-based language representations. To investigate this, we trained an open-source sequence-to-sequence model on the WMT17 dataset in two formats: one using standard textual representation and the other employing phonemic representation. The performance of both approaches was assessed using the BLEU metric. Our findings shows that the phonemic approach provides comparable quality but offers several advantages, including lower resource requirements or better suitability for low-resource languages.","authors":["Rene Pilz","Johannes Schneider"],"url":"https://arxiv.org/abs/2504.16234"}
{"created":"2025-04-24","title":"General Post-Processing Framework for Fairness Adjustment of Machine Learning Models","abstract":"As machine learning increasingly influences critical domains such as credit underwriting, public policy, and talent acquisition, ensuring compliance with fairness constraints is both a legal and ethical imperative. This paper introduces a novel framework for fairness adjustments that applies to diverse machine learning tasks, including regression and classification, and accommodates a wide range of fairness metrics. Unlike traditional approaches categorized as pre-processing, in-processing, or post-processing, our method adapts in-processing techniques for use as a post-processing step. By decoupling fairness adjustments from the model training process, our framework preserves model performance on average while enabling greater flexibility in model development. Key advantages include eliminating the need for custom loss functions, enabling fairness tuning using different datasets, accommodating proprietary models as black-box systems, and providing interpretable insights into the fairness adjustments. We demonstrate the effectiveness of this approach by comparing it to Adversarial Debiasing, showing that our framework achieves a comparable fairness/accuracy tradeoff on real-world datasets.","authors":["L\\'eandre Eberhard","Nirek Sharma","Filipp Shelobolin","Aalok Ganesh Shanbhag"],"url":"https://arxiv.org/abs/2504.16238"}
{"created":"2025-04-24","title":"DeepCS-TRD, a Deep Learning-based Cross-Section Tree Ring Detector","abstract":"Here, we propose Deep CS-TRD, a new automatic algorithm for detecting tree rings in whole cross-sections. It substitutes the edge detection step of CS-TRD by a deep-learning-based approach (U-Net), which allows the application of the method to different image domains: microscopy, scanner or smartphone acquired, and species (Pinus taeda, Gleditsia triachantos and Salix glauca). Additionally, we introduce two publicly available datasets of annotated images to the community. The proposed method outperforms state-of-the-art approaches in macro images (Pinus taeda and Gleditsia triacanthos) while showing slightly lower performance in microscopy images of Salix glauca. To our knowledge, this is the first paper that studies automatic tree ring detection for such different species and acquisition conditions. The dataset and source code are available in https://github.com/hmarichal93/deepcstrd","authors":["Henry Marichal","Ver\\'onica Casaravilla","Candice Power","Karolain Mello","Joaqu\\'in Mazarino","Christine Lucas","Ludmila Profumo","Diego Passarella","Gregory Randall"],"url":"https://arxiv.org/abs/2504.16242"}
{"created":"2025-04-24","title":"Numerical Derivatives, Projection Coefficients, and Truncation Errors in Analytic Hilbert Space With Gaussian Measure","abstract":"Let $f(z)$ be a holomorphic function, and let $\\langle,\\; rangle $ denote the inner product defined over an analytic Hilbert space with Gaussian measure. In this work, we demonstrate that the numerical values of the derivatives $f^{(n)}(z)$ at a point $z_{0}$ can be computed by evaluating an inner product of the form $\\langle z^{n},f(z)\\rangle$, divided by a constant. Specifically, if the inner product is taken over the Bargmann space (the analytic Hilbert space with Gaussian weight and orthogonal monomials), the constant is $\\pi$. This result assumes that $f(z)$ is a holomorphic function of a single complex variable. If the function $f(z)$ is square-integrable, then the accuracy of the computed derivative values depends on the precision and reliability of the numerical routine used to evaluate the inner products. We introduce the projection coefficients algorithm , which determines the leading terms of the Taylor series expansion for a given holomorphic function from a graph perspective, and analyze the associated truncation errors. Furthermore, the projection coefficients provide clear insights into certain properties of functions, such as whether they are odd or even, and whether the $n$-th derivatives exist. This study lays the groundwork for further applications in numerical analysis and approximation theory within Hilbert spaces equipped with Gaussian measures. Additionally, it might contribute to advancements in reproducing kernel Hilbert space (RKHS) methods, which are widely used in support vector machines (SVM) and other areas of machine learning. Also, it might have impact in probabilistic numerics.","authors":["M. W. AlMasri"],"url":"https://arxiv.org/abs/2504.16246"}
{"created":"2025-04-24","title":"Adaptive and Efficient Dynamic Memory Management for Hardware Enclaves","abstract":"The second version of Intel Software Guard Extensions (Intel SGX), or SGX2, adds dynamic management of enclave memory and threads. The first version required the address space and thread counts to be fixed before execution. The Enclave Dynamic Memory Management (EDMM) feature of SGX2 has the potential to lower launch times and overall execution time. Despite reducing the enclave loading time by 28--93%, straightforward EDMM adoption strategies actually slow execution time down by as much as 58%. Using the Gramine library OS as a representative enclave runtime environment, this paper shows how to recover EDMM performance. The paper explains how implementing mutual distrust between the OS and enclave increases the cost of modifying page mappings. The paper then describes and evaluates a series of optimizations on application benchmarks, showing that these optimizations effectively eliminate the overheads of EDMM while retaining EDMM's performance and flexibility gains.","authors":["Vijay Dhanraj","Harpreet Singh Chwarla","Tao Zhang","Daniel Manila","Eric Thomas Schneider","Erica Fu","Mona Vij","Chia-Che Tsai","Donald E. Porter"],"url":"https://arxiv.org/abs/2504.16251"}
{"created":"2025-04-24","title":"FairPlay: A Collaborative Approach to Mitigate Bias in Datasets for Improved AI Fairness","abstract":"The issue of fairness in decision-making is a critical one, especially given the variety of stakeholder demands for differing and mutually incompatible versions of fairness. Adopting a strategic interaction of perspectives provides an alternative to enforcing a singular standard of fairness. We present a web-based software application, FairPlay, that enables multiple stakeholders to debias datasets collaboratively. With FairPlay, users can negotiate and arrive at a mutually acceptable outcome without a universally agreed-upon theory of fairness. In the absence of such a tool, reaching a consensus would be highly challenging due to the lack of a systematic negotiation process and the inability to modify and observe changes. We have conducted user studies that demonstrate the success of FairPlay, as users could reach a consensus within about five rounds of gameplay, illustrating the application's potential for enhancing fairness in AI systems.","authors":["Tina Behzad","Mithilesh Kumar Singh","Anthony J. Ripa","Klaus Mueller"],"url":"https://arxiv.org/abs/2504.16255"}
{"created":"2025-04-24","title":"Accurate and generalizable protein-ligand binding affinity prediction with geometric deep learning","abstract":"Protein-ligand binding complexes are ubiquitous and essential to life. Protein-ligand binding affinity prediction (PLA) quantifies the binding strength between ligands and proteins, providing crucial insights for discovering and designing potential candidate ligands. While recent advances have been made in predicting protein-ligand complex structures, existing algorithms for interaction and affinity prediction suffer from a sharp decline in performance when handling ligands bound with novel unseen proteins. We propose IPBind, a geometric deep learning-based computational method, enabling robust predictions by leveraging interatomic potential between complex's bound and unbound status. Experimental results on widely used binding affinity prediction benchmarks demonstrate the effectiveness and universality of IPBind. Meanwhile, it provides atom-level insights into prediction. This work highlights the advantage of leveraging machine learning interatomic potential for predicting protein-ligand binding affinity.","authors":["Krinos Li","Xianglu Xiao","Zijun Zhong","Guang Yang"],"url":"https://arxiv.org/abs/2504.16261"}
{"created":"2025-04-24","title":"Learning Energy-Based Generative Models via Potential Flow: A Variational Principle Approach to Probability Density Homotopy Matching","abstract":"Energy-based models (EBMs) are a powerful class of probabilistic generative models due to their flexibility and interpretability. However, relationships between potential flows and explicit EBMs remain underexplored, while contrastive divergence training via implicit Markov chain Monte Carlo (MCMC) sampling is often unstable and expensive in high-dimensional settings. In this paper, we propose Variational Potential Flow Bayes (VPFB), a new energy-based generative framework that eliminates the need for implicit MCMC sampling and does not rely on auxiliary networks or cooperative training. VPFB learns an energy-parameterized potential flow by constructing a flow-driven density homotopy that is matched to the data distribution through a variational loss minimizing the Kullback-Leibler divergence between the flow-driven and marginal homotopies. This principled formulation enables robust and efficient generative modeling while preserving the interpretability of EBMs. Experimental results on image generation, interpolation, out-of-distribution detection, and compositional generation confirm the effectiveness of VPFB, showing that our method performs competitively with existing approaches in terms of sample quality and versatility across diverse generative modeling tasks.","authors":["Junn Yong Loo","Michelle Adeline","Julia Kaiwen Lau","Fang Yu Leong","Hwa Hui Tew","Arghya Pal","Vishnu Monn Baskaran","Chee-Ming Ting","Rapha\\\"el C. -W. Phan"],"url":"https://arxiv.org/abs/2504.16262"}
{"created":"2025-04-24","title":"Gradient-Optimized Fuzzy Classifier: A Benchmark Study Against State-of-the-Art Models","abstract":"This paper presents a performance benchmarking study of a Gradient-Optimized Fuzzy Inference System (GF) classifier against several state-of-the-art machine learning models, including Random Forest, XGBoost, Logistic Regression, Support Vector Machines, and Neural Networks. The evaluation was conducted across five datasets from the UCI Machine Learning Repository, each chosen for their diversity in input types, class distributions, and classification complexity. Unlike traditional Fuzzy Inference Systems that rely on derivative-free optimization methods, the GF leverages gradient descent to significantly improving training efficiency and predictive performance. Results demonstrate that the GF model achieved competitive, and in several cases superior, classification accuracy while maintaining high precision and exceptionally low training times. In particular, the GF exhibited strong consistency across folds and datasets, underscoring its robustness in handling noisy data and variable feature sets. These findings support the potential of gradient optimized fuzzy systems as interpretable, efficient, and adaptable alternatives to more complex deep learning models in supervised learning tasks.","authors":["Magnus Sieverding","Nathan Steffen","Kelly Cohen"],"url":"https://arxiv.org/abs/2504.16263"}
{"created":"2025-04-24","title":"CLIRudit: Cross-Lingual Information Retrieval of Scientific Documents","abstract":"Cross-lingual information retrieval (CLIR) consists in finding relevant documents in a language that differs from the language of the queries. This paper presents CLIRudit, a new dataset created to evaluate cross-lingual academic search, focusing on English queries and French documents. The dataset is built using bilingual article metadata from \\'Erudit, a Canadian publishing platform, and is designed to represent scenarios in which researchers search for scholarly content in languages other than English. We perform a comprehensive benchmarking of different zero-shot first-stage retrieval methods on the dataset, including dense and sparse retrievers, query and document machine translation, and state-of-the-art multilingual retrievers. Our results show that large dense retrievers, not necessarily trained for the cross-lingual retrieval task, can achieve zero-shot performance comparable to using ground truth human translations, without the need for machine translation. Sparse retrievers, such as BM25 or SPLADE, combined with document translation, show competitive results, providing an efficient alternative to large dense models. This research advances the understanding of cross-lingual academic information retrieval and provides a framework that others can use to build comparable datasets across different languages and disciplines. By making the dataset and code publicly available, we aim to facilitate further research that will help make scientific knowledge more accessible across language barriers.","authors":["Francisco Valentini","Diego Kozlowski","Vincent Larivi\\`ere"],"url":"https://arxiv.org/abs/2504.16264"}
{"created":"2025-04-24","title":"TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs","abstract":"Deploying large language models (LLMs) on edge platforms is challenged by their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as 1.58 bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected latency of the prefill phase. We present TeLLMe, the first ternary LLM accelerator for low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. Our contributions include: (1) a table-lookup matrix engine for ternary matmul that merges grouped activations with online precomputation to minimize resource use; (2) a fused, bandwidth-efficient attention module featuring a reversed reordering scheme to accelerate prefill; and (3) a tightly integrated normalization and quantization--dequantization unit optimized for ultra-low-bit inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128 token prompts, marking a significant energy-efficiency advance and establishing a new edge FPGA benchmark for generative AI.","authors":["Ye Qiao","Zhiheng Cheng","Yifan Zhang","Yian Wang","Sitao Huang"],"url":"https://arxiv.org/abs/2504.16266"}
{"created":"2025-04-24","title":"Two-Fold Byzantine Fault Tolerance Algorithm: Byzantine Consensus in Blockchain","abstract":"Blockchain technology offers a decentralized and secure method for storing and authenticating data, rendering it well-suited for various applications such as digital currencies, supply chain management, and voting systems. However, the decentralized nature of blockchain also exposes it to vulnerabilities, particularly Byzantine faults, which arise when nodes in the network behave maliciously or encounter unexpected failures. Such incidents can result in inconsistencies within the blockchain and, in extreme scenarios, lead to a breakdown in consensus. Byzantine fault-tolerant consensus algorithms are crafted to tackle this challenge by ensuring that network nodes can agree on the blockchain's state even in the presence of faulty or malicious nodes. To bolster the system's resilience against these faults, it is imperative to detect them within the system. However, our examination of existing literature reveals a prevalent assumption: solutions typically operate under constraints regarding the number of faulty nodes. Such constraints confine the proposed solutions to ideal environments, limiting their practical applicability. In response, we propose a novel approach inspired by social paradigms, employing a trusted and fully monitored communication sub-process to detect Byzantine nodes. Upon detection, these nodes can be either disregarded in the consensus-building process, subjected to penalties, or undergo modifications as per the system's policy. Finally, we statistically demonstrate that our approach achieves a detection probability that exceeds 95\\% for Byzantine nodes. In essence, our methodology ensures that if Byzantine nodes exhibit malicious behavior, healthy nodes can identify them with a confidence level of 95\\%.","authors":["Mohammad R. Shakournia","Pooya Jamshidi","Hamid Reza Faragardi","Nasser Yazdani"],"url":"https://arxiv.org/abs/2504.16267"}
{"created":"2025-04-24","title":"Boosting Classifier Performance with Opposition-Based Data Transformation","abstract":"In this paper, we introduce a novel data transformation framework based on Opposition-Based Learning (OBL) to boost the performance of traditional classification algorithms. Originally developed to accelerate convergence in optimization tasks, OBL is leveraged here to generate synthetic opposite samples that replace the acutely training data and improve decision boundary formation. We explore three OBL variants; Global OBL, Class-Wise OBL, and Localized Class-Wise OBL; and integrate them with several widely used classifiers, including K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Logistic Regression (LR), and Decision Tree (DT). Extensive experiments conducted on 26 heterogeneous and high-dimensional datasets demonstrate that OBL-enhanced classifiers consistently outperform their standard counterparts in terms of accuracy and F1-score, frequently achieving near-perfect or perfect classification. Furthermore, OBL contributes to improved computational efficiency, particularly in SVM and LR. These findings underscore the potential of OBL as a lightweight yet powerful data transformation strategy for enhancing classification performance, especially in complex or sparse learning environments.","authors":["Abdesslem Layeb"],"url":"https://arxiv.org/abs/2504.16268"}
{"created":"2025-04-24","title":"COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference","abstract":"Transformer-based models have demonstrated superior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communication limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers perform inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accelerator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and +1 values, surpassing ternary methods. With further hardware-friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation.","authors":["Ye Qiao","Zhiheng Cheng","Yian Wang","Yifan Zhang","Yunzhe Deng","Sitao Huang"],"url":"https://arxiv.org/abs/2504.16269"}
{"created":"2025-04-24","title":"The Language of Attachment: Modeling Attachment Dynamics in Psychotherapy","abstract":"The delivery of mental healthcare through psychotherapy stands to benefit immensely from developments within Natural Language Processing (NLP), in particular through the automatic identification of patient specific qualities, such as attachment style. Currently, the assessment of attachment style is performed manually using the Patient Attachment Coding System (PACS; Talia et al., 2017), which is complex, resource-consuming and requires extensive training. To enable wide and scalable adoption of attachment informed treatment and research, we propose the first exploratory analysis into automatically assessing patient attachment style from psychotherapy transcripts using NLP classification models. We further analyze the results and discuss the implications of using automated tools for this purpose -- e.g., confusing `preoccupied' patients with `avoidant' likely has a more negative impact on therapy outcomes with respect to other mislabeling. Our work opens an avenue of research enabling more personalized psychotherapy and more targeted research into the mechanisms of psychotherapy through advancements in NLP.","authors":["Frederik Bredgaard","Martin Lund Trinhammer","Elisa Bassignana"],"url":"https://arxiv.org/abs/2504.16271"}
{"created":"2025-04-24","title":"Learning Explainable Dense Reward Shapes via Bayesian Optimization","abstract":"Current reinforcement learning from human feedback (RLHF) pipelines for large language model (LLM) alignment typically assign scalar rewards to sequences, using the final token as a surrogate indicator for the quality of the entire sequence. However, this leads to sparse feedback and suboptimal token-level credit assignment. In this work, we frame reward shaping as an optimization problem focused on token-level credit assignment. We propose a reward-shaping function leveraging explainability methods such as SHAP and LIME to estimate per-token rewards from the reward model. To learn parameters of this shaping function, we employ a bilevel optimization framework that integrates Bayesian Optimization and policy training to handle noise from the token reward estimates. Our experiments show that achieving a better balance of token-level reward attribution leads to performance improvements over baselines on downstream tasks and finds an optimal policy faster during training. Furthermore, we show theoretically that explainability methods that are feature additive attribution functions maintain the optimal policy as the original reward.","authors":["Ryan Koo","Ian Yang","Vipul Raheja","Mingyi Hong","Kwang-Sung Jun","Dongyeop Kang"],"url":"https://arxiv.org/abs/2504.16272"}
{"created":"2025-04-24","title":"Investigating LLMs in Clinical Triage: Promising Capabilities, Persistent Intersectional Biases","abstract":"Large Language Models (LLMs) have shown promise in clinical decision support, yet their application to triage remains underexplored. We systematically investigate the capabilities of LLMs in emergency department triage through two key dimensions: (1) robustness to distribution shifts and missing data, and (2) counterfactual analysis of intersectional biases across sex and race. We assess multiple LLM-based approaches, ranging from continued pre-training to in-context learning, as well as machine learning approaches. Our results indicate that LLMs exhibit superior robustness, and we investigate the key factors contributing to the promising LLM-based approaches. Furthermore, in this setting, we identify gaps in LLM preferences that emerge in particular intersections of sex and race. LLMs generally exhibit sex-based differences, but they are most pronounced in certain racial groups. These findings suggest that LLMs encode demographic preferences that may emerge in specific clinical contexts or particular combinations of characteristics.","authors":["Joseph Lee","Tianqi Shang","Jae Young Baik","Duy Duong-Tran","Shu Yang","Lingyao Li","Li Shen"],"url":"https://arxiv.org/abs/2504.16273"}
{"created":"2025-04-24","title":"Quantum Doubly Stochastic Transformers","abstract":"At the core of the Transformer, the Softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often destabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the Softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard Vision Transformer and other doubly stochastic Transformers. Beyond the established Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. The QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data.","authors":["Jannis Born","Filip Skogh","Kahn Rhrissorrakrai","Filippo Utro","Nico Wagner","Aleksandros Sobczyk"],"url":"https://arxiv.org/abs/2504.16275"}
{"created":"2025-04-24","title":"An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon","abstract":"This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. We evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with Xeno-Canto recordings and a real-world test on the critically endangered tooth-billed pigeon (Didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. The final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. This open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction.","authors":["Abhishek Jana","Moeumu Uili","James Atherton","Mark O'Brien","Joe Wood","Leandra Brickson"],"url":"https://arxiv.org/abs/2504.16276"}
{"created":"2025-04-24","title":"DataS^3: Dataset Subset Selection for Specialization","abstract":"In many real-world machine learning (ML) applications (e.g. detecting broken bones in x-ray images, detecting species in camera traps), in practice models need to perform well on specific deployments (e.g. a specific hospital, a specific national park) rather than the domain broadly. However, deployments often have imbalanced, unique data distributions. Discrepancy between the training distribution and the deployment distribution can lead to suboptimal performance, highlighting the need to select deployment-specialized subsets from the available training data. We formalize dataset subset selection for specialization (DS3): given a training set drawn from a general distribution and a (potentially unlabeled) query set drawn from the desired deployment-specific distribution, the goal is to select a subset of the training data that optimizes deployment performance.","authors":["Neha Hulkund","Alaa Maalouf","Levi Cai","Daniel Yang","Tsun-Hsuan Wang","Abigail O'Neil","Timm Haucke","Sandeep Mukherjee","Vikram Ramaswamy","Judy Hansen Shen","Gabriel Tseng","Mike Walmsley","Daniela Rus","Ken Goldberg","Hannah Kerner","Irene Chen","Yogesh Girdhar","Sara Beery"],"url":"https://arxiv.org/abs/2504.16277"}
{"created":"2025-04-24","title":"Affect Models Have Weak Generalizability to Atypical Speech","abstract":"Speech and voice conditions can alter the acoustic properties of speech, which could impact the performance of paralinguistic models for affect for people with atypical speech. We evaluate publicly available models for recognizing categorical and dimensional affect from speech on a dataset of atypical speech, comparing results to datasets of typical speech. We investigate three dimensions of speech atypicality: intelligibility, which is related to pronounciation; monopitch, which is related to prosody, and harshness, which is related to voice quality. We look at (1) distributional trends of categorical affect predictions within the dataset, (2) distributional comparisons of categorical affect predictions to similar datasets of typical speech, and (3) correlation strengths between text and speech predictions for spontaneous speech for valence and arousal. We find that the output of affect models is significantly impacted by the presence and degree of speech atypicalities. For instance, the percentage of speech predicted as sad is significantly higher for all types and grades of atypical speech when compared to similar typical speech datasets. In a preliminary investigation on improving robustness for atypical speech, we find that fine-tuning models on pseudo-labeled atypical speech data improves performance on atypical speech without impacting performance on typical speech. Our results emphasize the need for broader training and evaluation datasets for speech emotion models, and for modeling approaches that are robust to voice and speech differences.","authors":["Jaya Narain","Amrit Romana","Vikramjit Mitra","Colin Lea","Shirley Ren"],"url":"https://arxiv.org/abs/2504.16283"}
{"created":"2025-04-24","title":"The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation","abstract":"The rapid advancement of large language models (LLMs) has reshaped the landscape of machine translation, yet challenges persist in preserving poetic intent, cultural heritage, and handling specialized terminology in Chinese-English translation. This study constructs a diverse corpus encompassing Chinese scientific terminology, historical translation paradoxes, and literary metaphors. Utilizing a back-translation and Friedman test-based evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three traditional translation tools. Key findings include: (1) Scientific abstracts often benefit from back-translation, while traditional tools outperform LLMs in linguistically distinct texts; (2) LLMs struggle with cultural and literary retention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit \"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel BLEU variant using Jieba segmentation and n-gram weighting is proposed. The study contributes to the empirical evaluation of Chinese NLP performance and advances understanding of cultural fidelity in AI-mediated translation.","authors":["Li Weigang","Pedro Carvalho Brom"],"url":"https://arxiv.org/abs/2504.16286"}
{"created":"2025-04-24","title":"Linear Time Subsequence and Supersequence Regex Matching","abstract":"It is well-known that checking whether a given string $w$ matches a given regular expression $r$ can be done in quadratic time $O(|w|\\cdot |r|)$ and that this cannot be improved to a truly subquadratic running time of $O((|w|\\cdot |r|)^{1-\\epsilon})$ assuming the strong exponential time hypothesis (SETH). We study a different matching paradigm where we ask instead whether $w$ has a subsequence that matches $r$, and show that regex matching in this sense can be solved in linear time $O(|w| + |r|)$. Further, the same holds if we ask for a supersequence. We show that the quantitative variants where we want to compute a longest or shortest subsequence or supersequence of $w$ that matches $r$ can be solved in $O(|w| \\cdot |r|)$, i. e., asymptotically no worse than classical regex matching; and we show that $O(|w| + |r|)$ is conditionally not possible for these problems. We also investigate these questions with respect to other natural string relations like the infix, prefix, left-extension or extension relation instead of the subsequence and supersequence relation. We further study the complexity of the universal problem where we ask if all subsequences (or supersequences, infixes, prefixes, left-extensions or extensions) of an input string satisfy a given regular expression.","authors":["Antoine Amarilli","Florin Manea","Tina Ringleb","Markus L. Schmid"],"url":"https://arxiv.org/abs/2504.16288"}
{"created":"2025-04-24","title":"Naturally Computed Scale Invariance in the Residual Stream of ResNet18","abstract":"An important capacity in visual object recognition is invariance to image-altering variables which leave the identity of objects unchanged, such as lighting, rotation, and scale. How do neural networks achieve this? Prior mechanistic interpretability research has illuminated some invariance-building circuitry in InceptionV1, but the results are limited and networks with different architectures have remained largely unexplored. This work investigates ResNet18 with a particular focus on its residual stream, an architectural component which InceptionV1 lacks. We observe that many convolutional channels in intermediate blocks exhibit scale invariant properties, computed by the element-wise residual summation of scale equivariant representations: the block input's smaller-scale copy with the block pre-sum output's larger-scale copy. Through subsequent ablation experiments, we attempt to causally link these neural properties with scale-robust object recognition behavior. Our tentative findings suggest how the residual stream computes scale invariance and its possible role in behavior. Code is available at: https://github.com/cest-andre/residual-stream-interp","authors":["Andr\\'e Longon"],"url":"https://arxiv.org/abs/2504.16290"}
{"created":"2025-04-24","title":"Data assimilation with model errors","abstract":"Nudging is a data assimilation method amenable to both analysis and implementation. It also has the (reported) advantage of being insensitive to model errors compared to other assimilation methods. However, nudging behavior in the presence of model errors is little analyzed. This report gives an analysis of nudging to correct model errors. The analysis indicates that the error contribution due to the model error decays as the nudging parameter $\\chi \\to \\infty$ like $\\mathcal{O}(\\chi^{-\\frac{1}{2}})$, Theorem 3.2. Numerical tests verify the predicted convergence rates and validate the nudging correction to model errors.","authors":["Aytekin \\c{C}ibik","Rui Fang","William Layton","Farjana Siddiqua"],"url":"https://arxiv.org/abs/2504.16291"}
{"created":"2025-04-24","title":"GENCNIPPET: Automated Generation of Code Snippets for Supporting Programming Questions","abstract":"Context: Software developers often ask questions on Technical Q&amp;A forums like Stack Overflow (SO) to seek solutions to their programming-related problems (e.g., errors and unexpected behavior of code). Problem: Many questions miss required code snippets due to the lack of readily available code, time constraints, employer restrictions, confidentiality concerns, or uncertainty about what code to share. Unfortunately, missing but required code snippets prevent questions from getting prompt and appropriate solutions. Objective: We plan to introduce GENCNIPPET, a tool designed to integrate with SO's question submission system. GENCNIPPET will generate relevant code examples (when required) to support questions for their timely solutions. Methodology: We first downloaded the SO April 2024 data dump, which contains 1.94 million questions related to Python that have code snippets and 1.43 million questions related to Java. Then, we filter these questions to identify those that genuinely require code snippets using a state-of-the-art machine learning model. Next, we select questions with positive scores to ensure high-quality data. Our plan is to fine-tune Llama-3 models (e.g., Llama-3-8B), using 80% of the selected questions for training and 10% for validation. The primary reasons for choosing Llama models are their open-source accessibility and robust fine-tuning capabilities, which are essential for deploying a freely accessible tool. GENCNIPPET will be integrated with the SO question submission system as a browser plugin. It will communicate with the fine-tuned model to generate code snippets tailored to the target questions. The effectiveness of the generated code examples will be assessed using automatic evaluation against ground truth, user perspectives, and live (wild) testing in real-world scenarios.","authors":["Saikat Mondal","Chanchal K. Roy"],"url":"https://arxiv.org/abs/2504.16292"}
{"created":"2025-04-24","title":"Subthreshold Jitter in VR Can Induce Visual Discomfort","abstract":"Visual-vestibular conflicts (VVCs) are a primary contributor to visually induced motion sickness (VIMS) in head-mounted displays (HMDs). However, virtual reality (VR) comfort studies often rely on exposing seated or standing users to experiences with high intensity visual motion (such as roller coasters). These drastic VVCs tend to induce pronounced VIMS symptoms that can be reliably detected across individuals using common survey measures. The conclusions from studies using these extreme motion-based conflicts may not accurately generalize to naturalistic use cases in VR where efforts are made to minimize, rather than maximize, VIMS symptoms. In this work, we show that a subthreshold visual-vestibular conflict can induce measurable discomfort during naturalistic, long duration use. We first present a psychophysical study, conducted outside of an HMD, to rigorously identify the perceptual thresholds for sinusoidal noise in render pose (i.e., jitter) resulting in erroneous 3D motion of rendered content. We next introduce subthreshold levels of jitter to a Meta Quest 3 VR HMD and demonstrate that this can induce visual discomfort in participants playing the commercially-available game Cubism across a three-session, repeated-measures study. Importantly, we did not identify statistically significant comfort differences between control and jitter conditions with traditional pre- and post-test comparison of Simulator Sickness Questionnaire (SSQ) scores. Significant differences were only identified using the Motion Illness Symptoms Classification (MISC) survey administered every 10 minutes across each 90 minute session. This highlights the benefits of incorporating time-resolved data points and suggests that lightweight, more frequent surveys may be important tools for measuring visual discomfort in more ecologically-valid scenarios.","authors":["Samuel J. Levulis","Kevin W. Rio","James Wilmott","Charlie S. Burlingham","Phillip Guan"],"url":"https://arxiv.org/abs/2504.16295"}
{"created":"2025-04-24","title":"Towards Quantum Universal Hypothesis Testing","abstract":"Hoeffding's formulation and solution to the universal hypothesis testing (UHT) problem had a profound impact on many subsequent works dealing with asymmetric hypotheses. In this work, we introduce a quantum universal hypothesis testing framework that serves as a quantum analog to Hoeffding's UHT. Motivated by Hoeffding's approach, which estimates the empirical distribution and uses it to construct the test statistic, we employ quantum state tomography to reconstruct the unknown state prior to forming the test statistic. Leveraging the concentration properties of quantum state tomography, we establish the exponential consistency of the proposed test: the type II error probability decays exponentially quickly, with the exponent determined by the trace distance between the true state and the nominal state.","authors":["Arick Grootveld","Haodong Yang","Biao Chen","Venkata Gandikota","Jason Pollack"],"url":"https://arxiv.org/abs/2504.16299"}
{"created":"2025-04-24","title":"MetaHarm: Harmful YouTube Video Dataset Annotated by Domain Experts, GPT-4-Turbo, and Crowdworkers","abstract":"Short video platforms, such as YouTube, Instagram, or TikTok, are used by billions of users. These platforms expose users to harmful content, ranging from clickbait or physical harms to hate or misinformation. Yet, we lack a comprehensive understanding and measurement of online harm on short video platforms. Toward this end, we present two large-scale datasets of multi-modal and multi-categorical online harm: (1) 60,906 systematically selected potentially harmful YouTube videos and (2) 19,422 videos annotated by three labeling actors: trained domain experts, GPT-4-Turbo (using 14 image frames, 1 thumbnail, and text metadata), and crowdworkers (Amazon Mechanical Turk master workers). The annotated dataset includes both (a) binary classification (harmful vs. harmless) and (b) multi-label categorizations of six harm categories: Information, Hate and harassment, Addictive, Clickbait, Sexual, and Physical harms. Furthermore, the annotated dataset provides (1) ground truth data with videos annotated consistently across (a) all three actors and (b) the majority of the labeling actors, and (2) three data subsets labeled by individual actors. These datasets are expected to facilitate future work on online harm, aid in (multi-modal) classification efforts, and advance the identification and potential mitigation of harmful content on video platforms.","authors":["Wonjeong Jo","Magdalena Wojcieszak"],"url":"https://arxiv.org/abs/2504.16304"}
{"created":"2025-04-24","title":"Regularizing Differentiable Architecture Search with Smooth Activation","abstract":"Differentiable Architecture Search (DARTS) is an efficient Neural Architecture Search (NAS) method but suffers from robustness, generalization, and discrepancy issues. Many efforts have been made towards the performance collapse issue caused by skip dominance with various regularization techniques towards operation weights, path weights, noise injection, and super-network redesign. It had become questionable at a certain point if there could exist a better and more elegant way to retract the search to its intended goal -- NAS is a selection problem. In this paper, we undertake a simple but effective approach, named Smooth Activation DARTS (SA-DARTS), to overcome skip dominance and discretization discrepancy challenges. By leveraging a smooth activation function on architecture weights as an auxiliary loss, our SA-DARTS mitigates the unfair advantage of weight-free operations, converging to fanned-out architecture weight values, and can recover the search process from skip-dominance initialization. Through theoretical and empirical analysis, we demonstrate that the SA-DARTS can yield new state-of-the-art (SOTA) results on NAS-Bench-201, classification, and super-resolution. Further, we show that SA-DARTS can help improve the performance of SOTA models with fewer parameters, such as Information Multi-distillation Network on the super-resolution task.","authors":["Yanlin Zhou","Mostafa El-Khamy","Kee-Bong Song"],"url":"https://arxiv.org/abs/2504.16306"}
{"created":"2025-04-24","title":"Schelling segregation dynamics in densely-connected social network graphs","abstract":"Schelling segregation is a well-established model used to investigate the dynamics of segregation in agent-based models. Since we consider segregation to be key for the development of political polarisation, we are interested in what insights it could give for this problem. We tested basic questions of segregation on an agent-based social network model where agents' connections were not restricted by their spatial position, and made the network graph much denser than previous tests of Schelling segregation in social networks.","authors":["Sage Anastasi","Giulio Dalla Riva"],"url":"https://arxiv.org/abs/2504.16307"}
{"created":"2025-04-24","title":"Improving Automated Secure Code Reviews: A Synthetic Dataset for Code Vulnerability Flaws","abstract":"Automation of code reviews using AI models has garnered substantial attention in the software engineering community as a strategy to reduce the cost and effort associated with traditional peer review processes. These models are typically trained on extensive datasets of real-world code reviews that address diverse software development concerns, including testing, refactoring, bug fixes, performance optimization, and maintainability improvements. However, a notable limitation of these datasets is the under representation of code vulnerabilities, critical flaws that pose significant security risks, with security-focused reviews comprising a small fraction of the data. This scarcity of vulnerability-specific data restricts the effectiveness of AI models in identifying and commenting on security-critical code. To address this issue, we propose the creation of a synthetic dataset consisting of vulnerability-focused reviews that specifically comment on security flaws. Our approach leverages Large Language Models (LLMs) to generate human-like code review comments for vulnerabilities, using insights derived from code differences and commit messages. To evaluate the usefulness of the generated synthetic dataset, we plan to use it to fine-tune three existing code review models. We anticipate that the synthetic dataset will improve the performance of the original code review models.","authors":["Leonardo Centellas-Claros","Juan J. Alonso-Lecaros","Juan Pablo Sandoval Alcocer","Andres Neyem"],"url":"https://arxiv.org/abs/2504.16310"}
{"created":"2025-04-24","title":"Key-agreement exists if and only if the \"interactive vs non interactive Kolmogorov problem\" is not in ioBPP: a short proof","abstract":"Ball, Liu, Mazor and Pass proved that the existence of key-agreement protocols is equivalent to the hardness of a certain problem about interactive Kolmogorov complexity. We generalize the statement and give a short proof of the difficult implication.","authors":["Bruno Bauwens","Bruno Loff"],"url":"https://arxiv.org/abs/2504.16311"}
{"created":"2025-04-24","title":"Capturing Symmetry and Antisymmetry in Language Models through Symmetry-Aware Training Objectives","abstract":"Capturing symmetric (e.g., country borders another country) and antisymmetric (e.g., parent_of) relations is crucial for a variety of applications. This paper tackles this challenge by introducing a novel Wikidata-derived natural language inference dataset designed to evaluate large language models (LLMs). Our findings reveal that LLMs perform comparably to random chance on this benchmark, highlighting a gap in relational understanding. To address this, we explore encoder retraining via contrastive learning with k-nearest neighbors. The retrained encoder matches the performance of fine-tuned classification heads while offering additional benefits, including greater efficiency in few-shot learning and improved mitigation of catastrophic forgetting.","authors":["Zhangdie Yuan","Andreas Vlachos"],"url":"https://arxiv.org/abs/2504.16312"}
{"created":"2025-04-24","title":"SignX: The Foundation Model for Sign Recognition","abstract":"The complexity of sign language data processing brings many challenges. The current approach to recognition of ASL signs aims to translate RGB sign language videos through pose information into English-based ID glosses, which serve to uniquely identify ASL signs. Note that there is no shared convention for assigning such glosses to ASL signs, so it is essential that the same glossing conventions are used for all of the data in the datasets that are employed. This paper proposes SignX, a foundation model framework for sign recognition. It is a concise yet powerful framework applicable to multiple human activity recognition scenarios. First, we developed a Pose2Gloss component based on an inverse diffusion model, which contains a multi-track pose fusion layer that unifies five of the most powerful pose information sources--SMPLer-X, DWPose, Mediapipe, PrimeDepth, and Sapiens Segmentation--into a single latent pose representation. Second, we trained a Video2Pose module based on ViT that can directly convert raw video into signer pose representation. Through this 2-stage training framework, we enable sign language recognition models to be compatible with existing pose formats, laying the foundation for the common pose estimation necessary for sign recognition. Experimental results show that SignX can recognize signs from sign language video, producing predicted gloss representations with greater accuracy than has been reported in prior work.","authors":["Sen Fang","Chunyu Sui","Hongwei Yi","Carol Neidle","Dimitris N. Metaxas"],"url":"https://arxiv.org/abs/2504.16315"}
{"created":"2025-04-24","title":"On the Consistency of GNN Explanations for Malware Detection","abstract":"Control Flow Graphs (CFGs) are critical for analyzing program execution and characterizing malware behavior. With the growing adoption of Graph Neural Networks (GNNs), CFG-based representations have proven highly effective for malware detection. This study proposes a novel framework that dynamically constructs CFGs and embeds node features using a hybrid approach combining rule-based encoding and autoencoder-based embedding. A GNN-based classifier is then constructed to detect malicious behavior from the resulting graph representations. To improve model interpretability, we apply state-of-the-art explainability techniques, including GNNExplainer, PGExplainer, and CaptumExplainer, the latter is utilized three attribution methods: Integrated Gradients, Guided Backpropagation, and Saliency. In addition, we introduce a novel aggregation method, called RankFusion, that integrates the outputs of the top-performing explainers to enhance the explanation quality. We also evaluate explanations using two subgraph extraction strategies, including the proposed Greedy Edge-wise Composition (GEC) method for improved structural coherence. A comprehensive evaluation using accuracy, fidelity, and consistency metrics demonstrates the effectiveness of the proposed framework in terms of accurate identification of malware samples and generating reliable and interpretable explanations.","authors":["Hossein Shokouhinejad","Griffin Higgins","Roozbeh Razavi-Far","Hesamodin Mohammadian","Ali A. Ghorbani"],"url":"https://arxiv.org/abs/2504.16316"}
{"created":"2025-04-24","title":"Semantics at an Angle: When Cosine Similarity Works Until It Doesn't","abstract":"Cosine similarity has become a standard metric for comparing embeddings in modern machine learning. Its scale-invariance and alignment with model training objectives have contributed to its widespread adoption. However, recent studies have revealed important limitations, particularly when embedding norms carry meaningful semantic information. This informal article offers a reflective and selective examination of the evolution, strengths, and limitations of cosine similarity. We highlight why it performs well in many settings, where it tends to break down, and how emerging alternatives are beginning to address its blind spots. We hope to offer a mix of conceptual clarity and practical perspective, especially for quantitative scientists who think about embeddings not just as vectors, but as geometric and philosophical objects.","authors":["Kisung You"],"url":"https://arxiv.org/abs/2504.16318"}
{"created":"2025-04-24","title":"Vision Controlled Orthotic Hand Exoskeleton","abstract":"This paper presents the design and implementation of an AI vision-controlled orthotic hand exoskeleton to enhance rehabilitation and assistive functionality for individuals with hand mobility impairments. The system leverages a Google Coral Dev Board Micro with an Edge TPU to enable real-time object detection using a customized MobileNet\\_V2 model trained on a six-class dataset. The exoskeleton autonomously detects objects, estimates proximity, and triggers pneumatic actuation for grasp-and-release tasks, eliminating the need for user-specific calibration needed in traditional EMG-based systems. The design prioritizes compactness, featuring an internal battery. It achieves an 8-hour runtime with a 1300 mAh battery. Experimental results demonstrate a 51ms inference speed, a significant improvement over prior iterations, though challenges persist in model robustness under varying lighting conditions and object orientations. While the most recent YOLO model (YOLOv11) showed potential with 15.4 FPS performance, quantization issues hindered deployment. The prototype underscores the viability of vision-controlled exoskeletons for real-world assistive applications, balancing portability, efficiency, and real-time responsiveness, while highlighting future directions for model optimization and hardware miniaturization.","authors":["Connor Blais","Md Abdul Baset Sarker","Masudul H. Imtiaz"],"url":"https://arxiv.org/abs/2504.16319"}
{"created":"2025-04-24","title":"PCF-Grasp: Converting Point Completion to Geometry Feature to Enhance 6-DoF Grasp","abstract":"The 6-Degree of Freedom (DoF) grasp method based on point clouds has shown significant potential in enabling robots to grasp target objects. However, most existing methods are based on the point clouds (2.5D points) generated from single-view depth images. These point clouds only have one surface side of the object providing incomplete geometry information, which mislead the grasping algorithm to judge the shape of the target object, resulting in low grasping accuracy. Humans can accurately grasp objects from a single view by leveraging their geometry experience to estimate object shapes. Inspired by humans, we propose a novel 6-DoF grasping framework that converts the point completion results as object shape features to train the 6-DoF grasp network. Here, point completion can generate approximate complete points from the 2.5D points similar to the human geometry experience, and converting it as shape features is the way to utilize it to improve grasp efficiency. Furthermore, due to the gap between the network generation and actual execution, we integrate a score filter into our framework to select more executable grasp proposals for the real robot. This enables our method to maintain a high grasp quality in any camera viewpoint. Extensive experiments demonstrate that utilizing complete point features enables the generation of significantly more accurate grasp proposals and the inclusion of a score filter greatly enhances the credibility of real-world robot grasping. Our method achieves a 17.8\\% success rate higher than the state-of-the-art method in real-world experiments.","authors":["Yaofeng Cheng","Fusheng Zha","Wei Guo","Pengfei Wang","Chao Zeng","Lining Sun","Chenguang Yang"],"url":"https://arxiv.org/abs/2504.16320"}
{"created":"2025-04-24","title":"Near-optimal Hypergraph Sparsification in Insertion-only and Bounded-deletion Streams","abstract":"We study the problem of constructing hypergraph cut sparsifiers in the streaming model where a hypergraph on $n$ vertices is revealed either via an arbitrary sequence of hyperedge insertions alone ({\\em insertion-only} streaming model) or via an arbitrary sequence of hyperedge insertions and deletions ({\\em dynamic} streaming model). For any $\\epsilon \\in (0,1)$, a $(1 \\pm \\epsilon)$ hypergraph cut-sparsifier of a hypergraph $H$ is a reweighted subgraph $H'$ whose cut values approximate those of $H$ to within a $(1 \\pm \\epsilon)$ factor. Prior work shows that in the static setting, one can construct a $(1 \\pm \\epsilon)$ hypergraph cut-sparsifier using $\\tilde{O}(nr/\\epsilon^2)$ bits of space [Chen-Khanna-Nagda FOCS 2020], and in the setting of dynamic streams using $\\tilde{O}(nr\\log m/\\epsilon^2)$ bits of space [Khanna-Putterman-Sudan FOCS 2024]; here the $\\tilde{O}$ notation hides terms that are polylogarithmic in $n$, and we use $m$ to denote the total number of hyperedges in the hypergraph. Up until now, the best known space complexity for insertion-only streams has been the same as that for the dynamic streams. This naturally poses the question of understanding the complexity of hypergraph sparsification in insertion-only streams.","authors":["Sanjeev Khanna","Aaron Putterman","Madhu Sudan"],"url":"https://arxiv.org/abs/2504.16321"}
{"created":"2025-04-24","title":"BAROC: Concealing Packet Losses in LSNs with Bimodal Behavior Awareness for Livecast Ingestion","abstract":"The advent of Low-Earth Orbit satellite networks (LSNs), exemplified by initiatives like \\emph{Starlink}, \\emph{OneWeb} and \\emph{Kuiper}, has ushered in a new era of ``Internet from Space\" global connectivity. Recent studies have shown that LSNs are capable of providing unprecedented download capacity and low latency to support Livecast viewing. However, Livecast ingestion still faces significant challenges, such as limited uplink capacity, bandwidth degradation, and the burst of packet loss due to frequent satellite reallocations, which cause previous recovery and adaptive solutions to be inferior under this new scenario. In this paper, we conduct an in-depth measurement study dedicated to understanding the implications of satellite reallocations, which reveals that the network status during reallocations with network anomalies exhibits a different distribution, leading to bimodal behaviors on the overall network performance. Motivated by this finding, we propose BAROC, a framework that can effectively conceal burst packet losses by combining a novel proposed MTP-Informer with bimodal behavior awareness during satellite reallocation. BAROC enhances video QoE on the server side by addressing the above challenges and jointly determining the optimal video encoding and recovery parameters. Our extensive evaluation shows that BAROC outperforms other video delivery recovery approaches, achieving an average PSNR improvement of $1.95$ dB and a maximum of $3.44$ dB, along with enhancements in frame rate and parity packet utilization. Additionally, a comprehensive ablation study is conducted to assess the effectiveness of MTP-Informer and components in BAROC.","authors":["Haoyuan Zhao","Jianxin Shi","Guanzhen Wu","Hao Fang","Yi Ching Chou","Long Chen","Feng Wang","Jiangchuan Liu"],"url":"https://arxiv.org/abs/2504.16322"}
{"created":"2025-04-24","title":"Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs","abstract":"As digital media use continues to evolve and influence various aspects of life, developing flexible and scalable tools to study complex media experiences is essential. This study introduces the Media Content Atlas (MCA), a novel pipeline designed to help researchers investigate large-scale screen data beyond traditional screen-use metrics. Leveraging multimodal large language models (MLLMs), MCA enables moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Evaluated on 1.12 million smartphone screenshots continuously captured during screen use from 112 adults over an entire month, MCA facilitates open-ended exploration and hypothesis generation as well as hypothesis-driven investigations at an unprecedented scale. Expert evaluators underscored its usability and potential for research and intervention design, with clustering results rated 96% relevant and descriptions 83% accurate. By bridging methodological possibilities with domain-specific needs, MCA accelerates both inductive and deductive inquiry, presenting new opportunities for media and HCI research.","authors":["Merve Cerit","Eric Zelikman","Mu-Jung Cho","Thomas N. Robinson","Byron Reeves","Nilam Ram","Nick Haber"],"url":"https://arxiv.org/abs/2504.16323"}
{"created":"2025-04-24","title":"The Dawn of Disaggregation and the Coherence Conundrum: A Call for Federated Coherence","abstract":"Disaggregated memory is an upcoming data center technology that will allow nodes (servers) to share data efficiently. Sharing data creates a debate on the level of cache coherence the system should provide. While current proposals aim to provide coherence for all or parts of the disaggregated memory, we argue that this approach is problematic, because of scalability limitations and hardware complexity. Instead, we propose and formally define federated coherence, a model that provides coherence only within nodes, not across nodes. Federated coherence can use current intra-node coherence provided by processors without requiring expensive mechanisms for inter-node coherence. Developers can use federated coherence with a few simple programming paradigms and a synchronization library. We sketch some potential applications.","authors":["Jaewan Hong","Marcos K. Aguilera","Emmanuel Amaro","Vincent Liu","Aurojit Panda","Ion Stoica"],"url":"https://arxiv.org/abs/2504.16324"}
{"created":"2025-04-24","title":"Universal Online Contention Resolution with Preselected Order","abstract":"Online contention resolution scheme (OCRS) is a powerful technique for online decision making, which--in the case of matroids--given a matroid and a prior distribution of active elements, selects a subset of active elements that satisfies the matroid constraint in an online fashion. OCRS has been studied mostly for product distributions in the literature. Recently, universal OCRS, that works even for correlated distributions, has gained interest, because it naturally generalizes the classic notion, and its existence in the random-order arrival model turns out to be equivalent to the matroid secretary conjecture. However, currently very little is known about how to design universal OCRSs for any arrival model. In this work, we consider a natural and relatively flexible arrival model, where the OCRS is allowed to preselect (i.e., non-adaptively select) the arrival order of the elements, and within this model, we design simple and optimal universal OCRSs that are computationally efficient. In the course of deriving our OCRSs, we also discover an efficient reduction from universal online contention resolution to the matroid secretary problem for any arrival model, answering a question from Dughmi (2020).","authors":["Junyao Zhao"],"url":"https://arxiv.org/abs/2504.16327"}
{"created":"2025-04-24","title":"ClarifyCoder: Clarification-Aware Fine-Tuning for Programmatic Problem Solving","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, a significant gap remains between their current performance and that of expert software engineers. A key differentiator is that human engineers actively seek clarification when faced with ambiguous requirements, while LLMs typically generate code regardless of uncertainties in the problem description. We present ClarifyCoder, a novel framework with synthetic data generation and instruction-tuning that enables LLMs to identify ambiguities and request clarification before proceeding with code generation. While recent work has focused on LLM-based agents for iterative code generation, we argue that the fundamental ability to recognize and query ambiguous requirements should be intrinsic to the models themselves. Our approach consists of two main components: (1) a data synthesis technique that augments existing programming datasets with scenarios requiring clarification to generate clarification-aware training data, and (2) a fine-tuning strategy that teaches models to prioritize seeking clarification over immediate code generation when faced with incomplete or ambiguous requirements. We further provide an empirical analysis of integrating ClarifyCoder with standard fine-tuning for a joint optimization of both clarify-awareness and coding ability. Experimental results demonstrate that ClarifyCoder significantly improves the communication capabilities of Code LLMs through meaningful clarification dialogues while maintaining code generation capabilities.","authors":["Jie JW Wu","Manav Chaudhary","Davit Abrahamyan","Arhaan Khaku","Anjiang Wei","Fatemeh H. Fard"],"url":"https://arxiv.org/abs/2504.16331"}
{"created":"2025-04-24","title":"MPAD: A New Dimension-Reduction Method for Preserving Nearest Neighbors in High-Dimensional Vector Search","abstract":"High-dimensional vector embeddings are widely used in retrieval systems, yet dimensionality reduction (DR) is seldom applied due to its tendency to distort nearest-neighbor (NN) structure critical for search. Existing DR techniques such as PCA and UMAP optimize global or manifold-preserving criteria, rather than retrieval-specific objectives. We present MPAD: Maximum Pairwise Absolute Difference, an unsupervised DR method that explicitly preserves approximate NN relations by maximizing the margin between k-NNs and non-k-NNs under a soft orthogonality constraint. This design enables MPAD to retain ANN-relevant geometry without supervision or changes to the original embedding model. Experiments across multiple domains show that MPAD consistently outperforms standard DR methods in preserving neighborhood structure, enabling more accurate search in reduced dimensions.","authors":["Jiuzhou Fu","Dongfang Zhao"],"url":"https://arxiv.org/abs/2504.16335"}
{"created":"2025-04-24","title":"Transitive Array: An Efficient GEMM Accelerator with Result Reuse","abstract":"Deep Neural Networks (DNNs) and Large Language Models (LLMs) have revolutionized artificial intelligence, yet their deployment faces significant memory and computational challenges, especially in resource-constrained environments. Quantization techniques have mitigated some of these issues by reducing data precision, primarily focusing on General Matrix Multiplication (GEMM). This study introduces a novel sparsity paradigm, transitive sparsity, which leverages the reuse of previously computed results to substantially minimize computational overhead in GEMM operations. By representing transitive relations using a directed acyclic graph, we develop an efficient strategy for determining optimal execution orders, thereby overcoming inherent challenges related to execution dependencies and parallelism. Building on this foundation, we present the Transitive Array, a multiplication-free accelerator designed to exploit transitive sparsity in GEMM. Our architecture effectively balances computational workloads across multiple parallel lanes, ensuring high efficiency and optimal resource utilization. Comprehensive evaluations demonstrate that the Transitive Array achieves approximately 7.46$\\times$ and 3.97$\\times$ speedup and 2.31$\\times$ and 1.65$\\times$ energy reduction compared to state-of-the-art accelerators such as Olive and BitVert while maintaining comparable model accuracy on LLaMA models.","authors":["Cong Guo","Chiyue Wei","Jiaming Tang","Bowen Duan","Song Han","Hai Li","Yiran Chen"],"url":"https://arxiv.org/abs/2504.16339"}
{"created":"2025-04-24","title":"Mining Software Repositories for Expert Recommendation","abstract":"We propose an automated approach to bug assignment to developers in large open-source software projects. This way, we assist human bug triagers who are in charge of finding the best developer with the right level of expertise in a particular area to be assigned to a newly reported issue. Our approach is based on the history of software development as documented in the issue tracking systems. We deploy BERTopic and techniques from TopicMiner. Our approach works based on the bug reports' features, such as the corresponding products and components, as well as their priority and severity levels. We sort developers based on their experience with specific combinations of new reports. The evaluation is performed using Top-k accuracy, and the results are compared with the reported results in prior work, namely TopicMiner MTM, BUGZIE, Bug triaging via deep Reinforcement Learning BT-RL, and LDA-SVM. The evaluation data come from various Eclipse and Mozilla projects, such as JDT, Firefox, and Thunderbird.","authors":["Chad Marshall","Andrew Barovic","Armin Moin"],"url":"https://arxiv.org/abs/2504.16343"}
{"created":"2025-04-24","title":"Real-time Bayesian inference at extreme scale: A digital twin for tsunami early warning applied to the Cascadia subduction zone","abstract":"We present a Bayesian inversion-based digital twin that employs acoustic pressure data from seafloor sensors, along with 3D coupled acoustic-gravity wave equations, to infer earthquake-induced spatiotemporal seafloor motion in real time and forecast tsunami propagation toward coastlines for early warning with quantified uncertainties. Our target is the Cascadia subduction zone, with one billion parameters. Computing the posterior mean alone would require 50 years on a 512 GPU machine. Instead, exploiting the shift invariance of the parameter-to-observable map and devising novel parallel algorithms, we induce a fast offline-online decomposition. The offline component requires just one adjoint wave propagation per sensor; using MFEM, we scale this part of the computation to the full El Capitan system (43,520 GPUs) with 92% weak parallel efficiency. Moreover, given real-time data, the online component exactly solves the Bayesian inverse and forecasting problems in 0.2 seconds on a modest GPU system, a ten-billion-fold speedup.","authors":["Stefan Henneking","Sreeram Venkat","Veselin Dobrev","John Camier","Tzanio Kolev","Milinda Fernando","Alice-Agnes Gabriel","Omar Ghattas"],"url":"https://arxiv.org/abs/2504.16344"}
{"created":"2025-04-24","title":"Road Similarity-Based BEV-Satellite Image Matching for UGV Localization","abstract":"To address the challenge of autonomous UGV localization in GNSS-denied off-road environments,this study proposes a matching-based localization method that leverages BEV perception image and satellite map within a road similarity space to achieve high-precision positioning.We first implement a robust LiDAR-inertial odometry system, followed by the fusion of LiDAR and image data to generate a local BEV perception image of the UGV. This approach mitigates the significant viewpoint discrepancy between ground-view images and satellite map. The BEV image and satellite map are then projected into the road similarity space, where normalized cross correlation (NCC) is computed to assess the matching score.Finally, a particle filter is employed to estimate the probability distribution of the vehicle's pose.By comparing with GNSS ground truth, our localization system demonstrated stability without divergence over a long-distance test of 10 km, achieving an average lateral error of only 0.89 meters and an average planar Euclidean error of 3.41 meters. Furthermore, it maintained accurate and stable global localization even under nighttime conditions, further validating its robustness and adaptability.","authors":["Zhenping Sun","Chuang Yang","Yafeng Bu","Bokai Liu","Jun Zeng","Xiaohui Li"],"url":"https://arxiv.org/abs/2504.16346"}
{"created":"2025-04-24","title":"Disentangling and Generating Modalities for Recommendation in Missing Modality Scenarios","abstract":"Multi-modal recommender systems (MRSs) have achieved notable success in improving personalization by leveraging diverse modalities such as images, text, and audio. However, two key challenges remain insufficiently addressed: (1) Insufficient consideration of missing modality scenarios and (2) the overlooking of unique characteristics of modality features. These challenges result in significant performance degradation in realistic situations where modalities are missing. To address these issues, we propose Disentangling and Generating Modality Recommender (DGMRec), a novel framework tailored for missing modality scenarios. DGMRec disentangles modality features into general and specific modality features from an information-based perspective, enabling richer representations for recommendation. Building on this, it generates missing modality features by integrating aligned features from other modalities and leveraging user modality preferences. Extensive experiments show that DGMRec consistently outperforms state-of-the-art MRSs in challenging scenarios, including missing modalities and new item settings as well as diverse missing ratios and varying levels of missing modalities. Moreover, DGMRec's generation-based approach enables cross-modal retrieval, a task inapplicable for existing MRSs, highlighting its adaptability and potential for real-world applications. Our code is available at https://github.com/ptkjw1997/DGMRec.","authors":["Jiwan Kim","Hongseok Kang","Sein Kim","Kibum Kim","Chanyoung Park"],"url":"https://arxiv.org/abs/2504.16352"}
{"created":"2025-04-24","title":"Transformer-Based Extraction of Statutory Definitions from the U.S. Code","abstract":"Automatic extraction of definitions from legal texts is critical for enhancing the comprehension and clarity of complex legal corpora such as the United States Code (U.S.C.). We present an advanced NLP system leveraging transformer-based architectures to automatically extract defined terms, their definitions, and their scope from the U.S.C. We address the challenges of automatically identifying legal definitions, extracting defined terms, and determining their scope within this complex corpus of over 200,000 pages of federal statutory law. Building upon previous feature-based machine learning methods, our updated model employs domain-specific transformers (Legal-BERT) fine-tuned specifically for statutory texts, significantly improving extraction accuracy. Our work implements a multi-stage pipeline that combines document structure analysis with state-of-the-art language models to process legal text from the XML version of the U.S. Code. Each paragraph is first classified using a fine-tuned legal domain BERT model to determine if it contains a definition. Our system then aggregates related paragraphs into coherent definitional units and applies a combination of attention mechanisms and rule-based patterns to extract defined terms and their jurisdictional scope. The definition extraction system is evaluated on multiple titles of the U.S. Code containing thousands of definitions, demonstrating significant improvements over previous approaches. Our best model achieves 96.8% precision and 98.9% recall (98.2% F1-score), substantially outperforming traditional machine learning classifiers. This work contributes to improving accessibility and understanding of legal information while establishing a foundation for downstream legal reasoning tasks.","authors":["Arpana Hosabettu (Google)","Harsh Shah (Cornell University)"],"url":"https://arxiv.org/abs/2504.16353"}
{"created":"2025-04-24","title":"VeriFix: Verifying Your Fix Towards An Atomicity Violation","abstract":"Atomicity violation is one of the most serious types of bugs in concurrent programs. Synchronizations are commonly used to enforce atomicity. However, it is very challenging to place synchronizations correctly and sufficiently","authors":["Zhuang Li","Qiuping Yi","Jeff Huang"],"url":"https://arxiv.org/abs/2504.16354"}
{"created":"2025-04-24","title":"Property-Preserving Hashing for $\\ell_1$-Distance Predicates: Applications to Countering Adversarial Input Attacks","abstract":"Perceptual hashing is used to detect whether an input image is similar to a reference image with a variety of security applications. Recently, they have been shown to succumb to adversarial input attacks which make small imperceptible changes to the input image yet the hashing algorithm does not detect its similarity to the original image. Property-preserving hashing (PPH) is a recent construct in cryptography, which preserves some property (predicate) of its inputs in the hash domain. Researchers have so far shown constructions of PPH for Hamming distance predicates, which, for instance, outputs 1 if two inputs are within Hamming distance $t$. A key feature of PPH is its strong correctness guarantee, i.e., the probability that the predicate will not be correctly evaluated in the hash domain is negligible. Motivated by the use case of detecting similar images under adversarial setting, we propose the first PPH construction for an $\\ell_1$-distance predicate. Roughly, this predicate checks if the two one-sided $\\ell_1$-distances between two images are within a threshold $t$. Since many adversarial attacks use $\\ell_2$-distance (related to $\\ell_1$-distance) as the objective function to perturb the input image, by appropriately choosing the threshold $t$, we can force the attacker to add considerable noise to evade detection, and hence significantly deteriorate the image quality. Our proposed scheme is highly efficient, and runs in time $O(t^2)$. For grayscale images of size $28 \\times 28$, we can evaluate the predicate in $0.0784$ seconds when pixel values are perturbed by up to $1 \\%$. For larger RGB images of size $224 \\times 224$, by dividing the image into 1,000 blocks, we achieve times of $0.0128$ seconds per block for $1 \\%$ change, and up to $0.2641$ seconds per block for $14\\%$ change.","authors":["Hassan Asghar","Chenhan Zhang","Dali Kaafar"],"url":"https://arxiv.org/abs/2504.16355"}
{"created":"2025-04-24","title":"DP2FL: Dual Prompt Personalized Federated Learning in Foundation Models","abstract":"Personalized federated learning (PFL) has garnered significant attention for its ability to address heterogeneous client data distributions while preserving data privacy. However, when local client data is limited, deep learning models often suffer from insufficient training, leading to suboptimal performance. Foundation models, such as CLIP (Contrastive Language-Image Pretraining), exhibit strong feature extraction capabilities and can alleviate this issue by fine-tuning on limited local data. Despite their potential, foundation models are rarely utilized in federated learning scenarios, and challenges related to integrating new clients remain largely unresolved. To address these challenges, we propose the Dual Prompt Personalized Federated Learning (DP2FL) framework, which introduces dual prompts and an adaptive aggregation strategy. DP2FL combines global task awareness with local data-driven insights, enabling local models to achieve effective generalization while remaining adaptable to specific data distributions. Moreover, DP2FL introduces a global model that enables prediction on new data sources and seamlessly integrates newly added clients without requiring retraining. Experimental results in highly heterogeneous environments validate the effectiveness of DP2FL's prompt design and aggregation strategy, underscoring the advantages of prediction on novel data sources and demonstrating the seamless integration of new clients into the federated learning framework.","authors":["Ying Chang","Xiaohu Shi","Xiaohui Zhao","Zhaohuang Chen","Deyin Ma"],"url":"https://arxiv.org/abs/2504.16357"}
{"created":"2025-04-24","title":"Text-to-TrajVis: Enabling Trajectory Data Visualizations from Natural Language Questions","abstract":"This paper introduces the Text-to-TrajVis task, which aims to transform natural language questions into trajectory data visualizations, facilitating the development of natural language interfaces for trajectory visualization systems. As this is a novel task, there is currently no relevant dataset available in the community. To address this gap, we first devised a new visualization language called Trajectory Visualization Language (TVL) to facilitate querying trajectory data and generating visualizations. Building on this foundation, we further proposed a dataset construction method that integrates Large Language Models (LLMs) with human efforts to create high-quality data. Specifically, we first generate TVLs using a comprehensive and systematic process, and then label each TVL with corresponding natural language questions using LLMs. This process results in the creation of the first large-scale Text-to-TrajVis dataset, named TrajVL, which contains 18,140 (question, TVL) pairs. Based on this dataset, we systematically evaluated the performance of multiple LLMs (GPT, Qwen, Llama, etc.) on this task. The experimental results demonstrate that this task is both feasible and highly challenging and merits further exploration within the research community.","authors":["Tian Bai","Huiyan Ying","Kailong Suo","Junqiu Wei","Tao Fan","Yuanfeng Song"],"url":"https://arxiv.org/abs/2504.16358"}
{"created":"2025-04-24","title":"VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models","abstract":"This work presents VideoMark, a training-free robust watermarking framework for video diffusion models. As diffusion models advance in generating highly realistic videos, the need for reliable content attribution mechanisms has become critical. While watermarking techniques for image diffusion models have made progress, directly extending these methods to videos presents unique challenges due to variable video lengths and vulnerability to temporal attacks. VideoMark addresses these limitations through a frame-wise watermarking strategy using pseudorandom error correction (PRC) codes to embed watermark information during the generation process. Our method generates an extended watermark message sequence and randomly selects starting positions for each video, ensuring uniform noise distribution in the latent space and maintaining generation quality. For watermark extraction, we introduce a Temporal Matching Module (TMM) that uses edit distance to align decoded messages with the original watermark sequence, providing robustness against temporal attacks such as frame deletion. Experimental results demonstrate that VideoMark achieves higher decoding accuracy than existing methods while maintaining video quality on par with watermark-free generation. Importantly, our watermark remains undetectable to attackers without the secret key, ensuring strong imperceptibility compared to other watermarking frameworks. VideoMark provides a practical solution for content attribution in diffusion-based video generation without requiring additional training or compromising video quality. Our code and data are available at \\href{https://github.com/KYRIE-LI11/VideoMark}{https://github.com/KYRIE-LI11/VideoMark}.","authors":["Xuming Hu","Hanqian Li","Jungang Li","Aiwei Liu"],"url":"https://arxiv.org/abs/2504.16359"}
{"created":"2025-04-24","title":"Disentangled Graph Representation Based on Substructure-Aware Graph Optimal Matching Kernel Convolutional Networks","abstract":"Graphs effectively characterize relational data, driving graph representation learning methods that uncover underlying predictive information. As state-of-the-art approaches, Graph Neural Networks (GNNs) enable end-to-end learning for diverse tasks. Recent disentangled graph representation learning enhances interpretability by decoupling independent factors in graph data. However, existing methods often implicitly and coarsely characterize graph structures, limiting structural pattern analysis within the graph. This paper proposes the Graph Optimal Matching Kernel Convolutional Network (GOMKCN) to address this limitation. We view graphs as node-centric subgraphs, where each subgraph acts as a structural factor encoding position-specific information. This transforms graph prediction into structural pattern recognition. Inspired by CNNs, GOMKCN introduces the Graph Optimal Matching Kernel (GOMK) as a convolutional operator, computing similarities between subgraphs and learnable graph filters. Mathematically, GOMK maps subgraphs and filters into a Hilbert space, representing graphs as point sets. Disentangled representations emerge from projecting subgraphs onto task-optimized filters, which adaptively capture relevant structural patterns via gradient descent. Crucially, GOMK incorporates local correspondences in similarity measurement, resolving the trade-off between differentiability and accuracy in graph kernels. Experiments validate that GOMKCN achieves superior accuracy and interpretability in graph pattern mining and prediction. The framework advances the theoretical foundation for disentangled graph representation learning.","authors":["Mao Wang","Tao Wu","Xingping Xian","Shaojie Qiao","Weina Niu","Canyixing Cui"],"url":"https://arxiv.org/abs/2504.16360"}
{"created":"2025-04-24","title":"Comparing Different Transformer Model Structures for Stock Prediction","abstract":"This paper compares different Transformer model architectures for stock index prediction. While many studies have shown that Transformers perform well in stock price forecasting, few have explored how different structural designs impact performance. Most existing works treat the Transformer as a black box, overlooking how specific architectural choices may affect predictive accuracy. However, understanding these differences is critical for developing more effective forecasting models. This study aims to identify which Transformer variant is most suitable for stock forecasting. This study evaluates five Transformer structures: (1) encoder-only Transformer, (2) decoder-only Transformer, (3) Vanilla Transformer (encoder + decoder), (4) Vanilla Transformer without embedding layers, and (5) Vanilla Transformer with ProbSparse attention. Results show that Transformer-based models generally outperform traditional approaches. Transformer with decoder only structure outperforms all other models in all scenarios. Transformer with ProbSparse attention has the worst performance in almost all cases.","authors":["Qizhao Chen"],"url":"https://arxiv.org/abs/2504.16361"}
{"created":"2025-04-24","title":"Almost Right: Making First-layer Kernels Nearly Orthogonal Improves Model Generalization","abstract":"An ongoing research challenge within several domains in computer vision is how to increase model generalization capabilities. Several attempts to improve model generalization performance are heavily inspired by human perceptual intelligence, which is remarkable in both its performance and efficiency to generalize to unknown samples. Many of these methods attempt to force portions of the network to be orthogonal, following some observation within neuroscience related to early vision processes. In this paper, we propose a loss component that regularizes the filtering kernels in the first convolutional layer of a network to make them nearly orthogonal. Deviating from previous works, we give the network flexibility in which pairs of kernels it makes orthogonal, allowing the network to navigate to a better solution space, imposing harsh penalties. Without architectural modifications, we report substantial gains in generalization performance using the proposed loss against previous works (including orthogonalization- and saliency-based regularization methods) across three different architectures (ResNet-50, DenseNet-121, ViT-b-16) and two difficult open-set recognition tasks: presentation attack detection in iris biometrics, and anomaly detection in chest X-ray images.","authors":["Colton R. Crum","Adam Czajka"],"url":"https://arxiv.org/abs/2504.16362"}
{"created":"2025-04-24","title":"CLPSTNet: A Progressive Multi-Scale Convolutional Steganography Model Integrating Curriculum Learning","abstract":"In recent years, a large number of works have introduced Convolutional Neural Networks (CNNs) into image steganography, which transform traditional steganography methods such as hand-crafted features and prior knowledge design into steganography methods that neural networks autonomically learn information embedding. However, due to the inherent complexity of digital images, issues of invisibility and security persist when using CNN models for information embedding. In this paper, we propose Curriculum Learning Progressive Steganophy Network (CLPSTNet). The network consists of multiple progressive multi-scale convolutional modules that integrate Inception structures and dilated convolutions. The module contains multiple branching pathways, starting from a smaller convolutional kernel and dilatation rate, extracting the basic, local feature information from the feature map, and gradually expanding to the convolution with a larger convolutional kernel and dilatation rate for perceiving the feature information of a larger receptive field, so as to realize the multi-scale feature extraction from shallow to deep, and from fine to coarse, allowing the shallow secret information features to be refined in different fusion stages. The experimental results show that the proposed CLPSTNet not only has high PSNR , SSIM metrics and decoding accuracy on three large public datasets, ALASKA2, VOC2012 and ImageNet, but also the steganographic images generated by CLPSTNet have low steganalysis scores.You can find our code at \\href{https://github.com/chaos-boops/CLPSTNet}{https://github.com/chaos-boops/CLPSTNet}.","authors":["Fengchun Liu","Tong Zhang","Chunying Zhang"],"url":"https://arxiv.org/abs/2504.16364"}
{"created":"2025-04-24","title":"Lightweight Social Computing Tools for Undergraduate Research Community Building","abstract":"Many barriers exist when new members join a research community, including impostor syndrome. These barriers can be especially challenging for undergraduate students who are new to research. In our work, we explore how the use of social computing tools in the form of spontaneous online social networks (SOSNs) can be used in small research communities to improve sense of belonging, peripheral awareness, and feelings of togetherness within an existing CS research community. Inspired by SOSNs such as BeReal, we integrated a Wizard-of-Oz photo sharing bot into a computing research lab to foster community building among members. Through a small sample of lab members (N = 17) over the course of 2 weeks, we observed an increase in participants' sense of togetherness based on pre- and post-study surveys. Our surveys and semi-structured interviews revealed that this approach has the potential to increase awareness of peers' personal lives, increase feelings of community, and reduce feelings of disconnectedness.","authors":["Noel Chacko","Hannah Vy Nguyen","Sophie Chen","Stephen MacNeil"],"url":"https://arxiv.org/abs/2504.16366"}
{"created":"2025-04-24","title":"Revisiting Radar Camera Alignment by Contrastive Learning for 3D Object Detection","abstract":"Recently, 3D object detection algorithms based on radar and camera fusion have shown excellent performance, setting the stage for their application in autonomous driving perception tasks. Existing methods have focused on dealing with feature misalignment caused by the domain gap between radar and camera. However, existing methods either neglect inter-modal features interaction during alignment or fail to effectively align features at the same spatial location across modalities. To alleviate the above problems, we propose a new alignment model called Radar Camera Alignment (RCAlign). Specifically, we design a Dual-Route Alignment (DRA) module based on contrastive learning to align and fuse the features between radar and camera. Moreover, considering the sparsity of radar BEV features, a Radar Feature Enhancement (RFE) module is proposed to improve the densification of radar BEV features with the knowledge distillation loss. Experiments show RCAlign achieves a new state-of-the-art on the public nuScenes benchmark in radar camera fusion for 3D Object Detection. Furthermore, the RCAlign achieves a significant performance gain (4.3\\% NDS and 8.4\\% mAP) in real-time 3D detection compared to the latest state-of-the-art method (RCBEVDet).","authors":["Linhua Kong","Dongxia Chang","Lian Liu","Zisen Kong","Pengyuan Li","Yao Zhao"],"url":"https://arxiv.org/abs/2504.16368"}
{"created":"2025-04-24","title":"Fast Online Adaptive Neural MPC via Meta-Learning","abstract":"Data-driven model predictive control (MPC) has demonstrated significant potential for improving robot control performance in the presence of model uncertainties. However, existing approaches often require extensive offline data collection and computationally intensive training, limiting their ability to adapt online. To address these challenges, this paper presents a fast online adaptive MPC framework that leverages neural networks integrated with Model-Agnostic Meta-Learning (MAML). Our approach focuses on few-shot adaptation of residual dynamics - capturing the discrepancy between nominal and true system behavior - using minimal online data and gradient steps. By embedding these meta-learned residual models into a computationally efficient L4CasADi-based MPC pipeline, the proposed method enables rapid model correction, enhances predictive accuracy, and improves real-time control performance. We validate the framework through simulation studies on a Van der Pol oscillator, a Cart-Pole system, and a 2D quadrotor. Results show significant gains in adaptation speed and prediction accuracy over both nominal MPC and nominal MPC augmented with a freshly initialized neural network, underscoring the effectiveness of our approach for real-time adaptive robot control.","authors":["Yu Mei","Xinyu Zhou","Shuyang Yu","Vaibhav Srivastava","Xiaobo Tan"],"url":"https://arxiv.org/abs/2504.16369"}
{"created":"2025-04-24","title":"What Sensors See, What People Feel: Exploring Subjective Collaboration Perception in Mixed Reality","abstract":"Mixed Reality (MR) enables rich, embodied collaboration, yet it's uncertain if sensor and system-logged behavioral signals capture how users experience that collaboration. This disconnect stems from a fundamental gap: behavioral signals are observable and continuous, while collaboration is interpreted subjectively, shaped by internal states like presence, cognitive availability, and social awareness. Our core insight is that sensor signals serve as observable manifestations of subjective experiences in MR collaboration, and they can be captured through sensor data such as shared gaze, speech, spatial movement, and other system-logged performance metrics. We propose the Sensor-to-Subjective (S2S) Mapping Framework, a conceptual model that links observable interaction patterns to users' subjective perceptions of collaboration and internal cognitive states through sensor-based indicators and task performance metrics. To validate this model, we conducted a study with 48 participants across 12 MR groups engaged in a collaborative image-sorting task. Our findings show a correlation between sensed behavior and perceived collaboration, particularly through shared attention and proximity.","authors":["Yasra Chandio","Diana Romero","Salma Elmalaki","Fatima Anwar"],"url":"https://arxiv.org/abs/2504.16373"}
{"created":"2025-04-24","title":"DPGP: A Hybrid 2D-3D Dual Path Potential Ghost Probe Zone Prediction Framework for Safe Autonomous Driving","abstract":"Modern robots must coexist with humans in dense urban environments. A key challenge is the ghost probe problem, where pedestrians or objects unexpectedly rush into traffic paths. This issue affects both autonomous vehicles and human drivers. Existing works propose vehicle-to-everything (V2X) strategies and non-line-of-sight (NLOS) imaging for ghost probe zone detection. However, most require high computational power or specialized hardware, limiting real-world feasibility. Additionally, many methods do not explicitly address this issue. To tackle this, we propose DPGP, a hybrid 2D-3D fusion framework for ghost probe zone prediction using only a monocular camera during training and inference. With unsupervised depth prediction, we observe ghost probe zones align with depth discontinuities, but different depth representations offer varying robustness. To exploit this, we fuse multiple feature embeddings to improve prediction. To validate our approach, we created a 12K-image dataset annotated with ghost probe zones, carefully sourced and cross-checked for accuracy. Experimental results show our framework outperforms existing methods while remaining cost-effective. To our knowledge, this is the first work extending ghost probe zone prediction beyond vehicles, addressing diverse non-vehicle objects. We will open-source our code and dataset for community benefit.","authors":["Weiming Qu","Jiawei Du","Shenghai Yuan","Jia Wang","Yang Sun","Shengyi Liu","Yuanhao Zhu","Jianfeng Yu","Song Cao","Rui Xia","Xiaoyu Tang","Xihong Wu","Dingsheng Luo"],"url":"https://arxiv.org/abs/2504.16374"}
{"created":"2025-04-24","title":"SILM: A Subjective Intent Based Low-Latency Framework for Multiple Traffic Participants Joint Trajectory Prediction","abstract":"Trajectory prediction is a fundamental technology for advanced autonomous driving systems and represents one of the most challenging problems in the field of cognitive intelligence. Accurately predicting the future trajectories of each traffic participant is a prerequisite for building high safety and high reliability decision-making, planning, and control capabilities in autonomous driving. However, existing methods often focus solely on the motion of other traffic participants without considering the underlying intent behind that motion, which increases the uncertainty in trajectory prediction. Autonomous vehicles operate in real-time environments, meaning that trajectory prediction algorithms must be able to process data and generate predictions in real-time. While many existing methods achieve high accuracy, they often struggle to effectively handle heterogeneous traffic scenarios. In this paper, we propose a Subjective Intent-based Low-latency framework for Multiple traffic participants joint trajectory prediction. Our method explicitly incorporates the subjective intent of traffic participants based on their key points, and predicts the future trajectories jointly without map, which ensures promising performance while significantly reducing the prediction latency. Additionally, we introduce a novel dataset designed specifically for trajectory prediction. Related code and dataset will be available soon.","authors":["Qu Weiming","Wang Jia","Du Jiawei","Zhu Yuanhao","Yu Jianfeng","Xia Rui","Cao Song","Wu Xihong","Luo Dingsheng"],"url":"https://arxiv.org/abs/2504.16377"}
{"created":"2025-04-24","title":"Cyberoception: Finding a Painlessly-Measurable New Sense in the Cyberworld Towards Emotion-Awareness in Computing","abstract":"In Affective computing, recognizing users' emotions accurately is the basis of affective human-computer interaction. Understanding users' interoception contributes to a better understanding of individually different emotional abilities, which is essential for achieving inter-individually accurate emotion estimation. However, existing interoception measurement methods, such as the heart rate discrimination task, have several limitations, including their dependence on a well-controlled laboratory environment and precision apparatus, making monitoring users' interoception challenging. This study aims to determine other forms of data that can explain users' interoceptive or similar states in their real-world lives and propose a novel hypothetical concept \"cyberoception,\" a new sense (1) which has properties similar to interoception in terms of the correlation with other emotion-related abilities, and (2) which can be measured only by the sensors embedded inside commodity smartphone devices in users' daily lives. Results from a 10-day-long in-lab/in-the-wild hybrid experiment reveal a specific cyberoception type \"Turn On\" (users' subjective sensory perception about the frequency of turning-on behavior on their smartphones), significantly related to participants' emotional valence. We anticipate that cyberoception to serve as a fundamental building block for developing more \"emotion-aware\", user-friendly applications and services.","authors":["Tadashi Okoshi","Zexiong Gao","Tan Yi Zhen","Takumi Karasawa","Takeshi Miki","Wataru Sasaki","Rajesh K. Balan"],"url":"https://arxiv.org/abs/2504.16378"}
{"created":"2025-04-24","title":"SplitReason: Learning To Offload Reasoning","abstract":"Reasoning in large language models (LLMs) tends to produce substantially longer token generation sequences than simpler language modeling tasks. This extended generation length reflects the multi-step, compositional nature of reasoning and is often correlated with higher solution accuracy. From an efficiency perspective, longer token generation exacerbates the inherently sequential and memory-bound decoding phase of LLMs. However, not all parts of this expensive reasoning process are equally difficult to generate. We leverage this observation by offloading only the most challenging parts of the reasoning process to a larger, more capable model, while performing most of the generation with a smaller, more efficient model; furthermore, we teach the smaller model to identify these difficult segments and independently trigger offloading when needed. To enable this behavior, we annotate difficult segments across 18k reasoning traces from the OpenR1-Math-220k chain-of-thought (CoT) dataset. We then apply supervised fine-tuning (SFT) and reinforcement learning fine-tuning (RLFT) to a 1.5B-parameter reasoning model, training it to learn to offload the most challenging parts of its own reasoning process to a larger model. This approach improves AIME24 reasoning accuracy by 24% and 28.3% while offloading 1.35% and 5% of the generated tokens respectively. We open-source our SplitReason model, data, code and logs.","authors":["Yash Akhauri","Anthony Fei","Chi-Chih Chang","Ahmed F. AbouElhamayed","Yueying Li","Mohamed S. Abdelfattah"],"url":"https://arxiv.org/abs/2504.16379"}
{"created":"2025-04-24","title":"Tight Exponential Strong Converses for Lossy Source Coding with Side-Information and Distributed Function Computation","abstract":"The exponential strong converse for a coding problem states that, if a coding rate is beyond the theoretical limit, the correct probability converges to zero exponentially. For the lossy source coding with side-information, also known as the Wyner-Ziv (WZ) problem, a lower bound on the strong converse exponent was derived by Oohama. In this paper, we derive the tight strong converse exponent for the WZ problem; as a special case, we also derive the tight strong converse exponent for the distributed function computation problem. For the converse part, we use the change-of-measure argument developed in the literature and the soft Markov constraint introduced by Oohama; the matching achievability is proved via the Poisson matching approach recently introduced by Li and Anantharam. Our result is build upon the recently derived tight strong converse exponent for the Wyner-Ahlswede-Korner (WAK) problem; however, compared to the WAK problem, more sophisticated argument is needed. As an illustration of the necessity of the soft Markov constraint, we present an example such that the soft Markov constraint is strictly positive.","authors":["Shun Watanabe"],"url":"https://arxiv.org/abs/2504.16380"}
{"created":"2025-04-24","title":"Fully Scalable MPC Algorithms for Euclidean k-Center","abstract":"The $k$-center problem is a fundamental optimization problem with numerous applications in machine learning, data analysis, data mining, and communication networks. The $k$-center problem has been extensively studied in the classical sequential setting for several decades, and more recently there have been some efforts in understanding the problem in parallel computing, on the Massively Parallel Computation (MPC) model. For now, we have a good understanding of $k$-center in the case where each local MPC machine has sufficient local memory to store some representatives from each cluster, that is, when one has $\\Omega(k)$ local memory per machine. While this setting covers the case of small values of $k$, for a large number of clusters these algorithms require undesirably large local memory, making them poorly scalable. The case of large $k$ has been considered only recently for the fully scalable low-local-memory MPC model for the Euclidean instances of the $k$-center problem. However, the earlier works have been considering only the constant dimensional Euclidean space, required a super-constant number of rounds, and produced only $k(1+o(1))$ centers whose cost is a super-constant approximation of $k$-center.","authors":["Artur Czumaj","Guichen Gao","Mohsen Ghaffari","Shaofeng H. -C. Jiang"],"url":"https://arxiv.org/abs/2504.16382"}
{"created":"2025-04-24","title":"Fast and Modular Whole-Body Lagrangian Dynamics of Legged Robots with Changing Morphology","abstract":"Fast and modular modeling of multi-legged robots (MLRs) is essential for resilient control, particularly under significant morphological changes caused by mechanical damage. Conventional fixed-structure models, often developed with simplifying assumptions for nominal gaits, lack the flexibility to adapt to such scenarios. To address this, we propose a fast modular whole-body modeling framework using Boltzmann-Hamel equations and screw theory, in which each leg's dynamics is modeled independently and assembled based on the current robot morphology. This singularity-free, closed-form formulation enables efficient design of model-based controllers and damage identification algorithms. Its modularity allows autonomous adaptation to various damage configurations without manual re-derivation or retraining of neural networks. We validate the proposed framework using a custom simulation engine that integrates contact dynamics, a gait generator, and local leg control. Comparative simulations against hardware tests on a hexapod robot with multiple leg damage confirm the model's accuracy and adaptability. Additionally, runtime analyses reveal that the proposed model is approximately three times faster than real-time, making it suitable for real-time applications in damage identification and recovery.","authors":["Sahand Farghdani","Omar Abdelrahman","Robin Chhabra"],"url":"https://arxiv.org/abs/2504.16383"}
{"created":"2025-04-24","title":"Distributed Space Resource Logistics Architecture Optimization under Economies of Scale","abstract":"This paper proposes an optimization framework for distributed resource logistics system design to support future multimission space exploration. The performance and impact of distributed In-Situ Resource Utilization (ISRU) systems in facilitating space transportation are analyzed. The proposed framework considers technology trade studies, deployment strategy, facility location evaluation, and resource logistics after production for distributed ISRU systems. We develop piecewise linear sizing and cost estimation models based on economies of scale that can be easily integrated into network-based mission planning formulations. A case study on a multi-mission cislunar logistics campaign is conducted to demonstrate the value of the proposed method and evaluate key tradeoffs to compare the performance of distributed ISRU systems with traditional concentrated ISRU. Finally, a comprehensive sensitivity analysis is performed to assess the proposed system under varying conditions, comparing concentrated and distributed ISRU systems.","authors":["Evangelia Gkaravela","Hang Woon Lee","Hao Chen"],"url":"https://arxiv.org/abs/2504.16385"}
{"created":"2025-04-24","title":"SaENeRF: Suppressing Artifacts in Event-based Neural Radiance Fields","abstract":"Event cameras are neuromorphic vision sensors that asynchronously capture changes in logarithmic brightness changes, offering significant advantages such as low latency, low power consumption, low bandwidth, and high dynamic range. While these characteristics make them ideal for high-speed scenarios, reconstructing geometrically consistent and photometrically accurate 3D representations from event data remains fundamentally challenging. Current event-based Neural Radiance Fields (NeRF) methods partially address these challenges but suffer from persistent artifacts caused by aggressive network learning in early stages and the inherent noise of event cameras. To overcome these limitations, we present SaENeRF, a novel self-supervised framework that effectively suppresses artifacts and enables 3D-consistent, dense, and photorealistic NeRF reconstruction of static scenes solely from event streams. Our approach normalizes predicted radiance variations based on accumulated event polarities, facilitating progressive and rapid learning for scene representation construction. Additionally, we introduce regularization losses specifically designed to suppress artifacts in regions where photometric changes fall below the event threshold and simultaneously enhance the light intensity difference of non-zero events, thereby improving the visual fidelity of the reconstructed scene. Extensive qualitative and quantitative experiments demonstrate that our method significantly reduces artifacts and achieves superior reconstruction quality compared to existing methods. The code is available at https://github.com/Mr-firework/SaENeRF.","authors":["Yuanjian Wang","Yufei Deng","Rong Xiao","Jiahao Fan","Chenwei Tang","Deng Xiong","Jiancheng Lv"],"url":"https://arxiv.org/abs/2504.16389"}
{"created":"2025-04-24","title":"An Explicit and Efficient $O(n^2)$-Time Algorithm for Sorting Sumsets","abstract":"We present the first explicit comparison-based algorithm that sorts the sumset $X + Y = \\{x_i + y_j,\\ \\forall 0 \\le i, j < n\\}$, where $X$ and $Y$ are sorted arrays of real numbers, in optimal $O(n^2)$ time and comparisons. While Fredman (1976) proved the theoretical existence of such an algorithm, a concrete construction has remained open for nearly five decades. Our algorithm exploits the structured monotonicity of the sumset matrix to perform amortized constant-comparisons and insertions, eliminating the $\\log(n)$ overhead typical of comparison-based sorting. We prove correctness and optimality in the standard comparison model, extend the method to $k$-fold sumsets with $O(n^k)$ performance, and outline potential support for dynamic updates. Experimental benchmarks show significant speedups over classical algorithms such as MergeSort and QuickSort when applied to sumsets. These results resolve a longstanding open problem in sorting theory and contribute novel techniques for exploiting input structure in algorithm design.","authors":["S. Mundhra (Ohio Wesleyan University)"],"url":"https://arxiv.org/abs/2504.16393"}
{"created":"2025-04-24","title":"ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs","abstract":"Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.","authors":["Fahmida Liza Piya","Rahmatollah Beheshti"],"url":"https://arxiv.org/abs/2504.16394"}
{"created":"2025-04-24","title":"Circinus: Efficient Query Planner for Compound ML Serving","abstract":"The rise of compound AI serving -- integrating multiple operators in a pipeline that may span edge and cloud tiers -- enables end-user applications such as autonomous driving, generative AI-powered meeting companions, and immersive gaming. Achieving high service goodput -- i.e., meeting service level objectives (SLOs) for pipeline latency, accuracy, and costs -- requires effective planning of operator placement, configuration, and resource allocation across infrastructure tiers. However, the diverse SLO requirements, varying edge capabilities, and high query volumes create an enormous planning search space, rendering current solutions fundamentally limited for real-time serving and cost-efficient deployments.","authors":["Banruo Liu","Wei-Yu Lin","Minghao Fang","Yihan Jiang","Fan Lai"],"url":"https://arxiv.org/abs/2504.16397"}
{"created":"2025-04-24","title":"Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection","abstract":"Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline.","authors":["Md Fahimuzzman Sohan"],"url":"https://arxiv.org/abs/2504.16404"}
{"created":"2025-04-24","title":"EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment","abstract":"The furnishing of multi-modal large language models (MLLMs) has led to the emergence of numerous benchmark studies, particularly those evaluating their perception and understanding capabilities.","authors":["Lancheng Gao","Ziheng Jia","Yunhao Zeng","Wei Sun","Yiming Zhang","Wei Zhou","Guangtao Zhai","Xiongkuo Min"],"url":"https://arxiv.org/abs/2504.16405"}
{"created":"2025-04-24","title":"Long Exposure Localization in Darkness Using Consumer Cameras","abstract":"In this paper we evaluate performance of the SeqSLAM algorithm for passive vision-based localization in very dark environments with low-cost cameras that result in massively blurred images. We evaluate the effect of motion blur from exposure times up to 10,000 ms from a moving car, and the performance of localization in day time from routes learned at night in two different environments. Finally we perform a statistical analysis that compares the baseline performance of matching unprocessed grayscale images to using patch normalization and local neighborhood normalization - the two key SeqSLAM components. Our results and analysis show for the first time why the SeqSLAM algorithm is effective, and demonstrate the potential for cheap camera-based localization systems that function despite extreme appearance change.","authors":["Michael Milford","Ian Turner","Peter Corke"],"url":"https://arxiv.org/abs/2504.16406"}
{"created":"2025-04-24","title":"Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation","abstract":"The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.","authors":["Jiahao Yuan","Xingzhe Sun","Xing Yu","Jingwen Wang","Dehui Du","Zhiqing Cui","Zixiang Di"],"url":"https://arxiv.org/abs/2504.16408"}
{"created":"2025-04-24","title":"Out-of-the-Box Conditional Text Embeddings from Large Language Models","abstract":"Conditional text embedding is a proposed representation that captures the shift in perspective on texts when conditioned on a specific aspect. Previous methods have relied on extensive training data for fine-tuning models, leading to challenges in terms of labor and resource costs. We propose PonTE, a novel unsupervised conditional text embedding method that leverages a causal large language model and a conditional prompt. Through experiments on conditional semantic text similarity and text clustering, we demonstrate that PonTE can generate useful conditional text embeddings and achieve performance comparable to supervised methods without fine-tuning. We also show the interpretability of text embeddings with PonTE by analyzing word generation following prompts and embedding visualization.","authors":["Kosuke Yamada","Peinan Zhang"],"url":"https://arxiv.org/abs/2504.16411"}
{"created":"2025-04-24","title":"Hierarchical Distributed Architecture for the Least Allan Variance Atomic Timing","abstract":"In this paper, we propose a hierarchical distributed timing architecture based on an ensemble of miniature atomic clocks. The goal is to ensure synchronized and accurate timing in a normal operating mode where Global Navigation Satellite System (GNSS) signals are available, as well as in an emergency operating mode during GNSS failures. At the lower level, the miniature atomic clocks employ a distributed control strategy that uses only local information to ensure synchronization in both modes. The resulting synchronized time or generated time scale has the best frequency stability, as measured by the Allan variance, over the short control period. In the upper layer, a supervisor controls the long-term behavior of the generated time scale. In the normal operating mode, the supervisor periodically anchors the generated time scale to the standard time based on GNSS signals, while in the emergency operating mode, it applies optimal floating control to reduce the divergence rate of the generated time scale, which is not observable from the measurable time difference between the miniature atomic clocks. This floating control aims to explicitly control the generated time scale to have the least Allan variance over the long control period. Finally, numerical examples are provided to demonstrate the effectiveness and feasibility of the architecture in high-precision, GNSS-resilient atomic timing.","authors":["Jiayu Chen","Takahiro Kawaguchi","Yuichiro Yano","Yuko Hanado","Takayuki Ishizaki"],"url":"https://arxiv.org/abs/2504.16413"}
{"created":"2025-04-24","title":"Evaluating Multi-Hop Reasoning in Large Language Models: A Chemistry-Centric Case Study","abstract":"In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.","authors":["Mohammad Khodadad","Ali Shiraee Kasmaee","Mahdi Astaraki","Nicholas Sherck","Hamidreza Mahyar","Soheila Samiee"],"url":"https://arxiv.org/abs/2504.16414"}
{"created":"2025-04-24","title":"Natural Policy Gradient for Average Reward Non-Stationary RL","abstract":"We consider the problem of non-stationary reinforcement learning (RL) in the infinite-horizon average-reward setting. We model it by a Markov Decision Process with time-varying rewards and transition probabilities, with a variation budget of $\\Delta_T$. Existing non-stationary RL algorithms focus on model-based and model-free value-based methods. Policy-based methods despite their flexibility in practice are not theoretically well understood in non-stationary RL. We propose and analyze the first model-free policy-based algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method with a restart based exploration for change and a novel interpretation of learning rates as adapting factors. Further, we present a bandit-over-RL based parameter-free algorithm BORL-NS-NAC that does not require prior knowledge of the variation budget $\\Delta_T$. We present a dynamic regret of $\\tilde{\\mathscr O}(|S|^{1/2}|A|^{1/2}\\Delta_T^{1/6}T^{5/6})$ for both algorithms, where $T$ is the time horizon, and $|S|$, $|A|$ are the sizes of the state and action spaces. The regret analysis leverages a novel adaptation of the Lyapunov function analysis of NAC to dynamic environments and characterizes the effects of simultaneous updates in policy, value function estimate and changes in the environment.","authors":["Neharika Jali","Eshika Pathak","Pranay Sharma","Guannan Qu","Gauri Joshi"],"url":"https://arxiv.org/abs/2504.16415"}
{"created":"2025-04-24","title":"FeedQUAC: Quick Unobtrusive AI-Generated Commentary","abstract":"Design thrives on feedback. However, gathering constant feedback throughout the design process can be labor-intensive and disruptive. We explore how AI can bridge this gap by providing effortless, ambient feedback. We introduce FeedQUAC, a design companion that delivers real-time AI-generated commentary from a variety of perspectives through different personas. A design probe study with eight participants highlights how designers can leverage quick yet ambient AI feedback to enhance their creative workflows. Participants highlight benefits such as convenience, playfulness, confidence boost, and inspiration from this lightweight feedback agent, while suggesting additional features, like chat interaction and context curation. We discuss the role of AI feedback, its strengths and limitations, and how to integrate it into existing design workflows while balancing user involvement. Our findings also suggest that ambient interaction is a valuable consideration for both the design and evaluation of future creativity support systems.","authors":["Tao Long","Kendra Wannamaker","Jo Vermeulen","George Fitzmaurice","Justin Matejka"],"url":"https://arxiv.org/abs/2504.16416"}
{"created":"2025-04-24","title":"Anytime Safe Reinforcement Learning","abstract":"This paper considers the problem of solving constrained","authors":["Pol Mestres","Arnau Marzabal","Jorge Cort\\'es"],"url":"https://arxiv.org/abs/2504.16417"}
{"created":"2025-04-24","title":"PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels","abstract":"Graphical User Interface (GUI) datasets are crucial for various downstream tasks. However, GUI datasets often generate annotation information through automatic labeling, which commonly results in inaccurate GUI element BBox annotations, including missing, duplicate, or meaningless BBoxes. These issues can degrade the performance of models trained on these datasets, limiting their effectiveness in real-world applications. Additionally, existing GUI datasets only provide BBox annotations visually, which restricts the development of visually related GUI downstream tasks. To address these issues, we introduce PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web pages. PixelWeb is constructed using a novel automatic annotation approach that integrates visual feature extraction and Document Object Model (DOM) structure analysis through two core modules: channel derivation and layer analysis. Channel derivation ensures accurate localization of GUI elements in cases of occlusion and overlapping elements by extracting BGRA four-channel bitmap annotations. Layer analysis uses the DOM to determine the visibility and stacking order of elements, providing precise BBox annotations. Additionally, PixelWeb includes comprehensive metadata such as element images, contours, and mask annotations. Manual verification by three independent annotators confirms the high quality and accuracy of PixelWeb annotations. Experimental results on GUI element detection tasks show that PixelWeb achieves performance on the mAP95 metric that is 3-7 times better than existing datasets. We believe that PixelWeb has great potential for performance improvement in downstream tasks such as GUI generation and automated user interaction.","authors":["Qi Yang","Weichen Bi","Haiyang Shen","Yaoqi Guo","Yun Ma"],"url":"https://arxiv.org/abs/2504.16419"}
{"created":"2025-04-24","title":"A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms","abstract":"Recommender systems (RS) have become essential in filtering information and personalizing content for users. RS techniques have traditionally relied on modeling interactions between users and items as well as the features of content using models specific to each task. The emergence of foundation models (FMs), large scale models trained on vast amounts of data such as GPT, LLaMA and CLIP, is reshaping the recommendation paradigm. This survey provides a comprehensive overview of the Foundation Models for Recommender Systems (FM4RecSys), covering their integration in three paradigms: (1) Feature-Based augmentation of representations, (2) Generative recommendation approaches, and (3) Agentic interactive systems. We first review the data foundations of RS, from traditional explicit or implicit feedback to multimodal content sources. We then introduce FMs and their capabilities for representation learning, natural language understanding, and multi-modal reasoning in RS contexts. The core of the survey discusses how FMs enhance RS under different paradigms. Afterward, we examine FM applications in various recommendation tasks. Through an analysis of recent research, we highlight key opportunities that have been realized as well as challenges encountered. Finally, we outline open research directions and technical challenges for next-generation FM4RecSys. This survey not only reviews the state-of-the-art methods but also provides a critical analysis of the trade-offs among the feature-based, the generative, and the agentic paradigms, outlining key open issues and future research directions.","authors":["Chengkai Huang","Hongtao Huang","Tong Yu","Kaige Xie","Junda Wu","Shuai Zhang","Julian Mcauley","Dietmar Jannach","Lina Yao"],"url":"https://arxiv.org/abs/2504.16420"}
{"created":"2025-04-24","title":"Advancing Radar Hand Gesture Recognition: A Hybrid Spectrum Synthetic Framework Merging Simulation with Neural Networks","abstract":"Millimeter wave (mmWave) radar sensors play a vital role in hand gesture recognition (HGR) by detecting subtle motions while preserving user privacy. However, the limited scale of radar datasets hinders the performance. Existing synthetic data generation methods fall short in two key areas. On the one hand, modeling-based approaches fail to accurately simulate the wave propagation and reflection at the hand-gesture level, facing unique complexities such as diffraction and occlusion. On the other hand, generative model-based methods are hard to converge while radar data is limited, lacking interpretability, and sometimes fail to produce kinematically plausible results. To overcome these limitations, we propose a novel hybrid spectrum synthetic framework leveraging visual hand gesture data. It combines a cylinder mesh-based hand reflection model with a small-scale neural network called RadarWeightNet, which focuses on assigning weights to simulated signals. Our framework addresses two key challenges: achieving accurate simulation of complex hand geometry and bridging the simulation-to-real gap in a data-driven manner while preserving interpretability, which balances physical accuracy with machine learning adaptability. We tested our framework under extreme scenarios where radar data is scarce. The results demonstrate the effectiveness of our hybrid framework, achieving up to 63% SSIM in synthetic performance and up to 30% improvement in classification performance in few-shot learning.","authors":["Jiaqi Tang","Xinbo Xu","Yinsong Xu","Qingchao Chen"],"url":"https://arxiv.org/abs/2504.16423"}
{"created":"2025-04-24","title":"Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark","abstract":"Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.","authors":["Hanlei Zhang","Zhuohang Li","Yeshuang Zhu","Hua Xu","Peiwu Wang","Jinchao Zhang","Jie Zhou","Haige Zhu"],"url":"https://arxiv.org/abs/2504.16427"}
{"created":"2025-04-24","title":"Give LLMs a Security Course: Securing Retrieval-Augmented Code Generation via Knowledge Injection","abstract":"Retrieval-Augmented Code Generation (RACG) leverages external knowledge to enhance Large Language Models (LLMs) in code synthesis, improving the functional correctness of the generated code. However, existing RACG systems largely overlook security, leading to substantial risks. Especially, the poisoning of malicious code into knowledge bases can mislead LLMs, resulting in the generation of insecure outputs, which poses a critical threat in modern software development. To address this, we propose a security-hardening framework for RACG systems, CodeGuarder, that shifts the paradigm from retrieving only functional code examples to incorporating both functional code and security knowledge. Our framework constructs a security knowledge base from real-world vulnerability databases, including secure code samples and root cause annotations. For each code generation query, a retriever decomposes the query into fine-grained sub-tasks and fetches relevant security knowledge. To prioritize critical security guidance, we introduce a re-ranking and filtering mechanism by leveraging the LLMs' susceptibility to different vulnerability types. This filtered security knowledge is seamlessly integrated into the generation prompt. Our evaluation shows CodeGuarder significantly improves code security rates across various LLMs, achieving average improvements of 20.12\\% in standard RACG, and 31.53\\% and 21.91\\% under two distinct poisoning scenarios without compromising functional correctness. Furthermore, CodeGuarder demonstrates strong generalization, enhancing security even when the targeted language's security knowledge is lacking. This work presents CodeGuarder as a pivotal advancement towards building secure and trustworthy RACG systems.","authors":["Bo Lin","Shangwen Wang","Yihao Qin","Liqian Chen","Xiaoguang Mao"],"url":"https://arxiv.org/abs/2504.16429"}
{"created":"2025-04-24","title":"MAGIC: Near-Optimal Data Attribution for Deep Learning","abstract":"The goal of predictive data attribution is to estimate how adding or removing a given set of training datapoints will affect model predictions. In convex settings, this goal is straightforward (i.e., via the infinitesimal jackknife). In large-scale (non-convex) settings, however, existing methods are far less successful -- current methods' estimates often only weakly correlate with ground truth. In this work, we present a new data attribution method (MAGIC) that combines classical methods and recent advances in metadifferentiation to (nearly) optimally estimate the effect of adding or removing training data on model predictions.","authors":["Andrew Ilyas","Logan Engstrom"],"url":"https://arxiv.org/abs/2504.16430"}
{"created":"2025-04-24","title":"Target Concrete Score Matching: A Holistic Framework for Discrete Diffusion","abstract":"Discrete diffusion is a promising framework for modeling and generating discrete data. In this work, we present Target Concrete Score Matching (TCSM), a novel and versatile objective for training and fine-tuning discrete diffusion models. TCSM provides a general framework with broad applicability. It supports pre-training discrete diffusion models directly from data samples, and many existing discrete diffusion approaches naturally emerge as special cases of our more general TCSM framework. Furthermore, the same TCSM objective extends to post-training of discrete diffusion models, including fine-tuning using reward functions or preference data, and distillation of knowledge from pre-trained autoregressive models. These new capabilities stem from the core idea of TCSM, estimating the concrete score of the target distribution, which resides in the original (clean) data space. This allows seamless integration with reward functions and pre-trained models, which inherently only operate in the clean data space rather than the noisy intermediate spaces of diffusion processes. Our experiments on language modeling tasks demonstrate that TCSM matches or surpasses current methods. Additionally, TCSM is versatile, applicable to both pre-training and post-training scenarios, offering greater flexibility and sample efficiency.","authors":["Ruixiang Zhang","Shuangfei Zhai","Yizhe Zhang","James Thornton","Zijing Ou","Joshua Susskind","Navdeep Jaitly"],"url":"https://arxiv.org/abs/2504.16431"}
{"created":"2025-04-24","title":"iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold Network","abstract":"As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities.","authors":["Ziran Liang","Rui An","Wenqi Fan","Yanghui Rao","Yuxuan Liang"],"url":"https://arxiv.org/abs/2504.16432"}
{"created":"2025-04-24","title":"FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing","abstract":"In recent years, large-scale vision-language models (VLMs) like CLIP have gained attention for their zero-shot inference using instructional text prompts. While these models excel in general computer vision, their potential for domain generalization in remote sensing (RS) remains underexplored. Existing approaches enhance prompt learning by generating visual prompt tokens but rely on full-image features, introducing noise and background artifacts that vary within a class, causing misclassification. To address this, we propose FrogDogNet, a novel prompt learning framework integrating Fourier frequency filtering and self-attention to improve RS scene classification and domain generalization. FrogDogNet selectively retains invariant low-frequency components while eliminating noise and irrelevant backgrounds, ensuring robust feature representation across domains. The model first extracts significant features via projection and self-attention, then applies frequency-based filtering to preserve essential structural information for prompt learning. Extensive experiments on four RS datasets and three domain generalization tasks show that FrogDogNet consistently outperforms state-of-the-art prompt learning methods, demonstrating superior adaptability across domain shifts. Our findings highlight the effectiveness of frequency-based invariant feature retention in generalization, paving the way for broader applications. Our code is available at https://github.com/HariseetharamG/FrogDogNet","authors":["Hariseetharam Gunduboina (Indian Institute of Technology Bombay","India)","Muhammad Haris Khan (Mohamed Bin Zayed University of Artificial Intelligence","UAE)","Biplab Banerjee (Indian Institute of Technology Bombay","India)"],"url":"https://arxiv.org/abs/2504.16433"}
{"created":"2025-04-24","title":"Hardness of Median and Center in the Ulam Metric","abstract":"The classical rank aggregation problem seeks to combine a set X of n permutations into a single representative \"consensus\" permutation. In this paper, we investigate two fundamental rank aggregation tasks under the well-studied Ulam metric: computing a median permutation (which minimizes the sum of Ulam distances to X) and computing a center permutation (which minimizes the maximum Ulam distance to X) in two settings.","authors":["Nick Fischer","Elazar Goldenberg","Mursalin Habib","C. S. Karthik"],"url":"https://arxiv.org/abs/2504.16437"}
{"created":"2025-04-24","title":"Private Federated Learning using Preference-Optimized Synthetic Data","abstract":"In practical settings, differentially private Federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri substantially improves the utility of DP synthetic data relative to prior work on LargeFedBench datasets and an existing benchmark from Xie et al. (2024). POPri closes the gap between next-token prediction accuracy in the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at https://github.com/meiyuw/POPri.","authors":["Charlie Hou","Mei-Yu Wang","Yige Zhu","Daniel Lazar","Giulia Fanti"],"url":"https://arxiv.org/abs/2504.16438"}
{"created":"2025-04-24","title":"Marginalized Generalized IoU (MGIoU): A Unified Objective Function for Optimizing Any Convex Parametric Shapes","abstract":"Optimizing the similarity between parametric shapes is crucial for numerous computer vision tasks, where Intersection over Union (IoU) stands as the canonical measure. However, existing optimization methods exhibit significant shortcomings: regression-based losses like L1/L2 lack correlation with IoU, IoU-based losses are unstable and limited to simple shapes, and task-specific methods are computationally intensive and not generalizable accross domains. As a result, the current landscape of parametric shape objective functions has become scattered, with each domain proposing distinct IoU approximations. To address this, we unify the parametric shape optimization objective functions by introducing Marginalized Generalized IoU (MGIoU), a novel loss function that overcomes these challenges by projecting structured convex shapes onto their unique shape Normals to compute one-dimensional normalized GIoU. MGIoU offers a simple, efficient, fully differentiable approximation strongly correlated with IoU. We then extend MGIoU to MGIoU+ that supports optimizing unstructured convex shapes. Together, MGIoU and MGIoU+ unify parametric shape optimization across diverse applications. Experiments on standard benchmarks demonstrate that MGIoU and MGIoU+ consistently outperform existing losses while reducing loss computation latency by 10-40x. Additionally, MGIoU and MGIoU+ satisfy metric properties and scale-invariance, ensuring robustness as an objective function. We further propose MGIoU- for minimizing overlaps in tasks like collision-free trajectory prediction. Code is available at https://ldtho.github.io/MGIoU","authors":["Duy-Tho Le","Trung Pham","Jianfei Cai","Hamid Rezatofighi"],"url":"https://arxiv.org/abs/2504.16443"}
{"created":"2025-04-24","title":"Power-based control of output oscillations with online estimation of biased harmonics","abstract":"The recently introduced discrete power-based control (Ruderman (2024b)) reduces largely the communication efforts in the control loop when compensating for the marginally damped or even slowly diverging output oscillations. The control commutates twice per oscillations period (at the amplitude peaks) and uses the measured harmonic output only. The power-based control scheme requires the knowledge of the instantaneous frequency, amplitude, and bias parameters of the harmonic signal. This paper extends the power-based control by the finite-time estimation of the biased harmonics (Ahmed et al. (2022)). Also an improved analytic calculation of the impulse weighting factor is provided. The power-based oscillations control with online estimation of the harmonic parameters is evaluated experimentally on the fifth-order actuator system with a free hanging load under gravity and measurement noise.","authors":["Michael Ruderman","Denis Efimov"],"url":"https://arxiv.org/abs/2504.16445"}
{"created":"2025-04-24","title":"Node Assigned physics-informed neural networks for thermal-hydraulic system simulation: CVH/FL module","abstract":"Severe accidents (SAs) in nuclear power plants have been analyzed using thermal-hydraulic (TH) system codes such as MELCOR and MAAP. These codes efficiently simulate the progression of SAs, while they still have inherent limitations due to their inconsistent finite difference schemes. The use of empirical schemes incorporating both implicit and explicit formulations inherently induces unidirectional coupling in multi-physics analyses. The objective of this study is to develop a novel numerical method for TH system codes using physics-informed neural network (PINN). They have shown strength in solving multi-physics due to the innate feature of neural networks-automatic differentiation. We propose a node-assigned PINN (NA-PINN) that is suitable for the control volume approach-based system codes. NA-PINN addresses the issue of spatial governing equation variation by assigning an individual network to each nodalization of the system code, such that spatial information is excluded from both the input and output domains, and each subnetwork learns to approximate a purely temporal solution. In this phase, we evaluated the accuracy of the PINN methods for the hydrodynamic module. In the 6 water tank simulation, PINN and NA-PINN showed maximum absolute errors of 1.678 and 0.007, respectively. It should be noted that only NA-PINN demonstrated acceptable accuracy. To the best of the authors' knowledge, this is the first study to successfully implement a system code using PINN. Our future work involves extending NA-PINN to a multi-physics solver and developing it in a surrogate manner.","authors":["Jeesuk Shin","Cheolwoong Kim","Sunwoong Yang","Minseo Lee","Sung Joong Kim","Joongoo Jeon"],"url":"https://arxiv.org/abs/2504.16447"}
{"created":"2025-04-24","title":"EMRModel: A Large Language Model for Extracting Medical Consultation Dialogues into Structured Medical Records","abstract":"Medical consultation dialogues contain critical clinical information, yet their unstructured nature hinders effective utilization in diagnosis and treatment. Traditional methods, relying on rule-based or shallow machine learning techniques, struggle to capture deep and implicit semantics. Recently, large pre-trained language models and Low-Rank Adaptation (LoRA), a lightweight fine-tuning method, have shown promise for structured information extraction. We propose EMRModel, a novel approach that integrates LoRA-based fine-tuning with code-style prompt design, aiming to efficiently convert medical consultation dialogues into structured electronic medical records (EMRs). Additionally, we construct a high-quality, realistically grounded dataset of medical consultation dialogues with detailed annotations. Furthermore, we introduce a fine-grained evaluation benchmark for medical consultation information extraction and provide a systematic evaluation methodology, advancing the optimization of medical natural language processing (NLP) models. Experimental results show EMRModel achieves an F1 score of 88.1%, improving by49.5% over standard pre-trained models. Compared to traditional LoRA fine-tuning methods, our model shows superior performance, highlighting its effectiveness in structured medical record extraction tasks.","authors":["Shuguang Zhao","Qiangzhong Feng","Zhiyang He","Peipei Sun","Yingying Wang","Xiaodong Tao","Xiaoliang Lu","Mei Cheng","Xinyue Wu","Yanyan Wang","Wei Liang"],"url":"https://arxiv.org/abs/2504.16448"}
{"created":"2025-04-24","title":"From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories","abstract":"Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into divulging private data or distributing harmful payloads to infiltrate host systems. Gaining timely insights into the current state of this ongoing battle holds significant importance. However, existing reviews exhibit 4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures understanding of how detection approaches exploit specific modal information channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate benchmarking; 4) Insufficient dataset coverage.This paper presents a comprehensive review of malicious URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced deep learning approaches (e.g. Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel modality-based taxonomy that categorizes existing works according to their primary data modalities (URL, HTML, Visual, etc.). This hierarchical classification enables both rigorous technical analysis and clear understanding of multimodal information utilization. Furthermore, to establish a profile of accessible datasets and address the lack of standardized benchmarking (where current studies often lack proper baseline comparisons), we curate and analyze: 1) publicly available datasets (2016-2024), and 2) open-source implementations from published works(2013-2025). Then, we outline essential design principles and architectural frameworks for product-level implementations. The review concludes by examining emerging challenges and proposing actionable directions for future research. We maintain a GitHub repository for ongoing curating datasets and open-source implementations: https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.","authors":["Ye Tian","Yanqiu Yu","Jianguo Sun","Yanbin Wang"],"url":"https://arxiv.org/abs/2504.16449"}
{"created":"2025-04-24","title":"An Effective Gram Matrix Characterizes Generalization in Deep Networks","abstract":"We derive a differential equation that governs the evolution of the generalization gap when a deep network is trained by gradient descent. This differential equation is controlled by two quantities, a contraction factor that brings together trajectories corresponding to slightly different datasets, and a perturbation factor that accounts for them training on different datasets. We analyze this differential equation to compute an ``effective Gram matrix'' that characterizes the generalization gap after training in terms of the alignment between this Gram matrix and a certain initial ``residual''. Empirical evaluations on image classification datasets indicate that this analysis can predict the test loss accurately. Further, at any point during training, the residual predominantly lies in the subspace of the effective Gram matrix with the smallest eigenvalues. This indicates that the training process is benign, i.e., it does not lead to significant deterioration of the generalization gap (which is zero at initialization). The alignment between the effective Gram matrix and the residual is different for different datasets and architectures. The match/mismatch of the data and the architecture is primarily responsible for good/bad generalization.","authors":["Rubing Yang","Pratik Chaudhari"],"url":"https://arxiv.org/abs/2504.16450"}
{"created":"2025-04-24","title":"Efficient Design of Compliant Mechanisms Using Multi-Objective Optimization","abstract":"Compliant mechanisms achieve motion through elastic deformation. In this work, we address the synthesis of a compliant cross-hinge mechanism capable of large angular strokes while approximating the behavior of an ideal revolute joint. To capture the competing demands of kinematic fidelity, rotational stiffness, and resistance to parasitic motion, we formulate a multi-objective optimization problem based on kinetostatic performance measures. A hybrid design strategy is employed: an efficient beam-based structural model enables extensive exploration of a high-dimensional design space using evolutionary algorithms, followed by fine-tuning with high-fidelity three-dimensional finite element analysis. The resulting Pareto-optimal designs reveal diverse geometric configurations and performance trade-offs.","authors":["Alexander Humer","Sebastian Platzer"],"url":"https://arxiv.org/abs/2504.16451"}
{"created":"2025-04-24","title":"Killing Two Birds with One Stone: Unifying Retrieval and Ranking with a Single Generative Recommendation Model","abstract":"In recommendation systems, the traditional multi-stage paradigm, which includes retrieval and ranking, often suffers from information loss between stages and diminishes performance. Recent advances in generative models, inspired by natural language processing, suggest the potential for unifying these stages to mitigate such loss. This paper presents the Unified Generative Recommendation Framework (UniGRF), a novel approach that integrates retrieval and ranking into a single generative model. By treating both stages as sequence generation tasks, UniGRF enables sufficient information sharing without additional computational costs, while remaining model-agnostic. To enhance inter-stage collaboration, UniGRF introduces a ranking-driven enhancer module that leverages the precision of the ranking stage to refine retrieval processes, creating an enhancement loop. Besides, a gradient-guided adaptive weighter is incorporated to dynamically balance the optimization of retrieval and ranking, ensuring synchronized performance improvements. Extensive experiments demonstrate that UniGRF significantly outperforms existing models on benchmark datasets, confirming its effectiveness in facilitating information transfer. Ablation studies and further experiments reveal that UniGRF not only promotes efficient collaboration between stages but also achieves synchronized optimization. UniGRF provides an effective, scalable, and compatible framework for generative recommendation systems.","authors":["Luankang Zhang","Kenan Song","Yi Quan Lee","Wei Guo","Hao Wang","Yawen Li","Huifeng Guo","Yong Liu","Defu Lian","Enhong Chen"],"url":"https://arxiv.org/abs/2504.16454"}
{"created":"2025-04-24","title":"Cross Paradigm Representation and Alignment Transformer for Image Deraining","abstract":"Transformer-based networks have achieved strong performance in low-level vision tasks like image deraining by utilizing spatial or channel-wise self-attention. However, irregular rain patterns and complex geometric overlaps challenge single-paradigm architectures, necessitating a unified framework to integrate complementary global-local and spatial-channel representations. To address this, we propose a novel Cross Paradigm Representation and Alignment Transformer (CPRAformer). Its core idea is the hierarchical representation and alignment, leveraging the strengths of both paradigms (spatial-channel and global-local) to aid image reconstruction. It bridges the gap within and between paradigms, aligning and coordinating them to enable deep interaction and fusion of features. Specifically, we use two types of self-attention in the Transformer blocks: sparse prompt channel self-attention (SPC-SA) and spatial pixel refinement self-attention (SPR-SA). SPC-SA enhances global channel dependencies through dynamic sparsity, while SPR-SA focuses on spatial rain distribution and fine-grained texture recovery. To address the feature misalignment and knowledge differences between them, we introduce the Adaptive Alignment Frequency Module (AAFM), which aligns and interacts with features in a two-stage progressive manner, enabling adaptive guidance and complementarity. This reduces the information gap within and between paradigms. Through this unified cross-paradigm dynamic interaction framework, we achieve the extraction of the most valuable interactive fusion information from the two paradigms. Extensive experiments demonstrate that our model achieves state-of-the-art performance on eight benchmark datasets and further validates CPRAformer's robustness in other image restoration tasks and downstream applications.","authors":["Shun Zou","Yi Zou","Juncheng Li","Guangwei Gao","Guojun Qi"],"url":"https://arxiv.org/abs/2504.16455"}
{"created":"2025-04-24","title":"Insect-Computer Hybrid Speaker: Speaker using Chirp of the Cicada Controlled by Electrical Muscle Stimulation","abstract":"We propose \"Insect-Computer Hybrid Speaker\", which enables us to make musics made from combinations of computer and insects. Lots of studies have proposed methods and interfaces for controlling insects and obtaining feedback. However, there have been less research on the use of insects for interaction with third parties. In this paper, we propose a method in which cicadas are used as speakers triggered by using Electrical Muscle Stimulation (EMS). We explored and investigated the suitable waveform of chirp to be controlled, the appropriate voltage range, and the maximum pitch at which cicadas can chirp.","authors":["Yuga Tsukuda","Naoto Nishida","Jun Lu","Yoichi Ochiai"],"url":"https://arxiv.org/abs/2504.16459"}
{"created":"2025-04-24","title":"T-VEC: A Telecom-Specific Vectorization Model with Enhanced Semantic Understanding via Deep Triplet Loss Fine-Tuning","abstract":"The specialized vocabulary and complex concepts of the telecommunications industry present significant challenges for standard Natural Language Processing models. Generic text embeddings often fail to capture telecom-specific semantics, hindering downstream task performance. We introduce T-VEC (Telecom Vectorization Model), a novel embedding model tailored for the telecom domain through deep fine-tuning. Developed by NetoAI, T-VEC is created by adapting the state-of-the-art gte-Qwen2-1.5B-instruct model using a triplet loss objective on a meticulously curated, large-scale dataset of telecom-specific data. Crucially, this process involved substantial modification of weights across 338 layers of the base model, ensuring deep integration of domain knowledge, far exceeding superficial adaptation techniques. We quantify this deep change via weight difference analysis. A key contribution is the development and open-sourcing (MIT License) of the first dedicated telecom-specific tokenizer, enhancing the handling of industry jargon. T-VEC achieves a leading average MTEB score (0.825) compared to established models and demonstrates vastly superior performance (0.9380 vs. less than 0.07) on our internal telecom-specific triplet evaluation benchmark, indicating an exceptional grasp of domain-specific nuances, visually confirmed by improved embedding separation. This work positions NetoAI at the forefront of telecom AI innovation, providing the community with a powerful, deeply adapted, open-source tool.","authors":["Vignesh Ethiraj","Sidhanth Menon","Divya Vijay"],"url":"https://arxiv.org/abs/2504.16460"}
{"created":"2025-04-24","title":"Multiplicative Spanners in Minor-Free Graphs","abstract":"In FOCS 2017, Borradaille, Le, and Wulff-Nilsen addressed a long-standing open problem by proving that minor-free graphs have light spanners. Specifically, they proved that every $K_h$-minor-free graph has a $(1+\\epsilon)$-spanner of lightness $O_{\\epsilon}(h \\sqrt{\\log h})$, hence constant when $h$ and $\\epsilon$ are regarded as constants.","authors":["Greg Bodwin","Gary Hoppenworth","Zihan Tan"],"url":"https://arxiv.org/abs/2504.16463"}
{"created":"2025-04-24","title":"ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance","abstract":"While recent advancements in robotic manipulation video synthesis have shown promise, significant challenges persist in ensuring effective instruction-following and achieving high visual quality. Recent methods, like RoboDreamer, utilize linguistic decomposition to divide instructions into separate lower-level primitives, conditioning the world model on these primitives to achieve compositional instruction-following. However, these separate primitives do not consider the relationships that exist between them. Furthermore, recent methods neglect valuable visual guidance, including depth and semantic guidance, both crucial for enhancing visual quality. This paper introduces ManipDreamer, an advanced world model based on the action tree and visual guidance. To better learn the relationships between instruction primitives, we represent the instruction as the action tree and assign embeddings to tree nodes, each instruction can acquire its embeddings by navigating through the action tree. The instruction embeddings can be used to guide the world model. To enhance visual quality, we combine depth and semantic guidance by introducing a visual guidance adapter compatible with the world model. This visual adapter enhances both the temporal and physical consistency of video generation. Based on the action tree and visual guidance, ManipDreamer significantly boosts the instruction-following ability and visual quality. Comprehensive evaluations on robotic manipulation benchmarks reveal that ManipDreamer achieves large improvements in video quality metrics in both seen and unseen tasks, with PSNR improved from 19.55 to 21.05, SSIM improved from 0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks, compared to the recent RoboDreamer model. Additionally, our method increases the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on average.","authors":["Ying Li","Xiaobao Wei","Xiaowei Chi","Yuming Li","Zhongyu Zhao","Hao Wang","Ningning Ma","Ming Lu","Shanghang Zhang"],"url":"https://arxiv.org/abs/2504.16464"}
{"created":"2025-04-24","title":"MTSGL: Multi-Task Structure Guided Learning for Robust and Interpretable SAR Aircraft Recognition","abstract":"Aircraft recognition in synthetic aperture radar (SAR) imagery is a fundamental mission in both military and civilian applications. Recently deep learning (DL) has emerged a dominant paradigm for its explosive performance on extracting discriminative features. However, current classification algorithms focus primarily on learning decision hyperplane without enough comprehension on aircraft structural knowledge. Inspired by the fined aircraft annotation methods for optical remote sensing images (RSI), we first introduce a structure-based SAR aircraft annotations approach to provide structural and compositional supplement information. On this basis, we propose a multi-task structure guided learning (MTSGL) network for robust and interpretable SAR aircraft recognition. Besides the classification task, MTSGL includes a structural semantic awareness (SSA) module and a structural consistency regularization (SCR) module. The SSA is designed to capture structure semantic information, which is conducive to gain human-like comprehension of aircraft knowledge. The SCR helps maintain the geometric consistency between the aircraft structure in SAR imagery and the proposed annotation. In this process, the structural attribute can be disentangled in a geometrically meaningful manner. In conclusion, the MTSGL is presented with the expert-level aircraft prior knowledge and structure guided learning paradigm, aiming to comprehend the aircraft concept in a way analogous to the human cognitive process. Extensive experiments are conducted on a self-constructed multi-task SAR aircraft recognition dataset (MT-SARD) and the effective results illustrate the superiority of robustness and interpretation ability of the proposed MTSGL.","authors":["Qishan He","Lingjun Zhao","Ru Luo","Siqian Zhang","Lin Lei","Kefeng Ji","Gangyao Kuang"],"url":"https://arxiv.org/abs/2504.16467"}
{"created":"2025-04-24","title":"Closed-form analysis of Multi-RIS Reflected Signals in RIS-Aided Networks Using Stochastic Geometry","abstract":"Reconfigurable intelligent surfaces (RISs) enhance wireless communication by creating engineered signal reflection paths in addition to direct links. This work presents a stochastic geometry framework using point processes (PPs) to model multiple randomly deployed RISs conditioned on their associated base station (BS) locations. By characterizing aggregated reflections from multiple RISs using the Laplace transform, we analytically assess the performance impact of RIS-reflected signals by integrating this characterization into well-established stochastic geometry frameworks. Specifically, we derive closed-form expressions for the Laplace transform of the reflected signal power in several deployment scenarios. These analytical results facilitate performance evaluation of RIS-enabled enhancements. Numerical simulations validate that optimal RIS placement favors proximity to BSs or user equipment (UEs), and further quantify the impact of reflected interference, various fading assumptions, and diverse spatial deployment strategies. Importantly, our analytical approach shows superior computational efficiency compared to Monte Carlo simulations.","authors":["Guodong Sun","Francois Baccelli"],"url":"https://arxiv.org/abs/2504.16469"}
{"created":"2025-04-24","title":"Improved Streaming Edge Coloring","abstract":"Given a graph, an edge coloring assigns colors to edges so that no pairs of adjacent edges share the same color. We are interested in edge coloring algorithms under the W-streaming model. In this model, the algorithm does not have enough memory to hold the entire graph, so the edges of the input graph are read from a data stream one by one in an unknown order, and the algorithm needs to print a valid edge coloring in an output stream. The performance of the algorithm is measured by the amount of space and the number of different colors it uses.","authors":["Shiri Chechik","Hongyi Chen","Tianyi Zhang"],"url":"https://arxiv.org/abs/2504.16470"}
{"created":"2025-04-24","title":"RGB-D Video Object Segmentation via Enhanced Multi-store Feature Memory","abstract":"The RGB-Depth (RGB-D) Video Object Segmentation (VOS) aims to integrate the fine-grained texture information of RGB with the spatial geometric clues of depth modality, boosting the performance of segmentation. However, off-the-shelf RGB-D segmentation methods fail to fully explore cross-modal information and suffer from object drift during long-term prediction. In this paper, we propose a novel RGB-D VOS method via multi-store feature memory for robust segmentation. Specifically, we design the hierarchical modality selection and fusion, which adaptively combines features from both modalities. Additionally, we develop a segmentation refinement module that effectively utilizes the Segmentation Anything Model (SAM) to refine the segmentation mask, ensuring more reliable results as memory to guide subsequent segmentation tasks. By leveraging spatio-temporal embedding and modality embedding, mixed prompts and fused images are fed into SAM to unleash its potential in RGB-D VOS. Experimental results show that the proposed method achieves state-of-the-art performance on the latest RGB-D VOS benchmark.","authors":["Boyue Xu","Ruichao Hou","Tongwei Ren","Gangshan Wu"],"url":"https://arxiv.org/abs/2504.16471"}
{"created":"2025-04-24","title":"Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges","abstract":"Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated `just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper\\footnote{Author order is alphabetical. The corresponding author is Mark Harman.} was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025.","authors":["Mark Harman","Peter O'Hearn","Shubho Sengupta"],"url":"https://arxiv.org/abs/2504.16472"}
{"created":"2025-04-24","title":"ERASER: Efficient RTL FAult Simulation Framework with Trimmed Execution Redundancy","abstract":"As intelligent computing devices increasingly integrate into human life, ensuring the functional safety of the corresponding electronic chips becomes more critical. A key metric for functional safety is achieving a sufficient fault coverage. To meet this requirement, extensive time-consuming fault simulation of the RTL code is necessary during the chip design phase.The main overhead in RTL fault simulation comes from simulating behavioral nodes (always blocks). Due to the limited fault propagation capacity, fault simulation results often match the good simulation results for many behavioral nodes. A key strategy for accelerating RTL fault simulation is the identification and elimination of redundant simulations. Existing methods detect redundant executions by examining whether the fault inputs to each RTL node are consistent with the good inputs. However, we observe that this input comparison mechanism overlooks a significant amount of implicit redundant execution: although the fault inputs differ from the good inputs, the node's execution results remain unchanged. Our experiments reveal that this overlooked redundant execution constitutes nearly half of the total execution overhead of behavioral nodes, becoming a significant bottleneck in current RTL fault simulation. The underlying reason for this overlooked redundancy is that, in these cases, the true execution paths within the behavioral nodes are not affected by the changes in input values. In this work, we propose a behavior-level redundancy detection algorithm that focuses on the true execution paths. Building on the elimination of redundant executions, we further developed an efficient RTL fault simulation framework, Eraser.Experimental results show that compared to commercial tools, under the same fault coverage, our framework achieves a 3.9 $\\times$ improvement in simulation performance on average.","authors":["Jiaping Tang","Jianan Mu","Silin Liu","Zizhen Liu","Feng Gu","Xinyu Zhang","Leyan Wang","Shenwen Liang","Jing Ye","Huawei Li","Xiaowei Li"],"url":"https://arxiv.org/abs/2504.16473"}
{"created":"2025-04-24","title":"Seeking Flat Minima over Diverse Surrogates for Improved Adversarial Transferability: A Theoretical Framework and Algorithmic Instantiation","abstract":"The transfer-based black-box adversarial attack setting poses the challenge of crafting an adversarial example (AE) on known surrogate models that remain effective against unseen target models. Due to the practical importance of this task, numerous methods have been proposed to address this challenge. However, most previous methods are heuristically designed and intuitively justified, lacking a theoretical foundation. To bridge this gap, we derive a novel transferability bound that offers provable guarantees for adversarial transferability. Our theoretical analysis has the advantages of \\textit{(i)} deepening our understanding of previous methods by building a general attack framework and \\textit{(ii)} providing guidance for designing an effective attack algorithm. Our theoretical results demonstrate that optimizing AEs toward flat minima over the surrogate model set, while controlling the surrogate-target model shift measured by the adversarial model discrepancy, yields a comprehensive guarantee for AE transferability. The results further lead to a general transfer-based attack framework, within which we observe that previous methods consider only partial factors contributing to the transferability. Algorithmically, inspired by our theoretical results, we first elaborately construct the surrogate model set in which models exhibit diverse adversarial vulnerabilities with respect to AEs to narrow an instantiated adversarial model discrepancy. Then, a \\textit{model-Diversity-compatible Reverse Adversarial Perturbation} (DRAP) is generated to effectively promote the flatness of AEs over diverse surrogate models to improve transferability. Extensive experiments on NIPS2017 and CIFAR-10 datasets against various target models demonstrate the effectiveness of our proposed attack.","authors":["Meixi Zheng","Kehan Wu","Yanbo Fan","Rui Huang","Baoyuan Wu"],"url":"https://arxiv.org/abs/2504.16474"}
{"created":"2025-04-24","title":"The Dodecacopter: a Versatile Multirotor System of Dodecahedron-Shaped Modules","abstract":"With the promise of greater safety and adaptability, modular reconfigurable uncrewed air vehicles have been proposed as unique, versatile platforms holding the potential to replace multiple types of monolithic vehicles at once. State-of-the-art rigidly assembled modular vehicles are generally two-dimensional configurations in which the rotors are coplanar and assume the shape of a \"flight array\". We introduce the Dodecacopter, a new type of modular rotorcraft where all modules take the shape of a regular dodecahedron, allowing the creation of richer sets of configurations beyond flight arrays. In particular, we show how the chosen module design can be used to create three-dimensional and fully actuated configurations. We justify the relevance of these types of configurations in terms of their structural and actuation properties with various performance indicators. Given the broad range of configurations and capabilities that can be achieved with our proposed design, we formulate tractable optimization programs to find optimal configurations given structural and actuation constraints. Finally, a prototype of such a vehicle is presented along with results of performed flights in multiple configurations.","authors":["K\\'evin Garanger","Thanakorn Khamvilai","Jeremy Epps","Eric Feron"],"url":"https://arxiv.org/abs/2504.16475"}
{"created":"2025-04-24","title":"Distributed Optimization with Efficient Communication, Event-Triggered Solution Enhancement, and Operation Stopping","abstract":"In modern large-scale systems with sensor networks and IoT devices it is essential to collaboratively solve complex problems while utilizing network resources efficiently. In our paper we present three distributed optimization algorithms that exhibit efficient communication among nodes. Our first algorithm presents a simple quantized averaged gradient procedure for distributed optimization, which is shown to converge to a neighborhood of the optimal solution. Our second algorithm incorporates a novel event-triggered refinement mechanism, which refines the utilized quantization level to enhance the precision of the estimated optimal solution. It enables nodes to terminate their operation according to predefined performance guarantees. Our third algorithm is tailored to operate in environments where each message consists of only a few bits. It incorporates a novel event-triggered mechanism for adjusting the quantizer basis and quantization level, allowing nodes to collaboratively decide operation termination based on predefined performance criteria. We analyze the three algorithms and establish their linear convergence. Finally, an application on distributed sensor fusion for target localization is used to demonstrate their favorable performance compared to existing algorithms in the literature.","authors":["Apostolos I. Rikos","Wei Jiang","Themistoklis Charalambous","Karl H. Johansson"],"url":"https://arxiv.org/abs/2504.16477"}
{"created":"2025-04-24","title":"Balancing Costs and Utilities in Future Networks via Market Equilibrium with Externalities","abstract":"We study the problem of market equilibrium (ME) in future wireless networks, with multiple actors competing and negotiating for a pool of heterogeneous resources (communication and computing) while meeting constraints in terms of global cost. The latter is defined in a general way but is associated with energy and/or carbon emissions. In this direction, service providers competing for network resources do not acquire the latter, but rather the right to consume, given externally defined policies and regulations. We propose to apply the Fisher market model, and prove its convergence towards an equilibrium between utilities, regulatory constraints, and individual budgets. The model is then applied to an exemplary use case of access network, edge computing, and cloud resources, and numerical results assess the theoretical findings of convergence, under different assumptions on the utility function and more or less stringent constraints.","authors":["Mandar Datar","Mattia Merluzzi"],"url":"https://arxiv.org/abs/2504.16480"}
{"created":"2025-04-24","title":"Estimating Random-Walk Probabilities in Directed Graphs","abstract":"We study discounted random walks in a directed graph. In each vertex, the walk will either terminate with some probability $\\alpha$, or continue to a random out-neighbor. We are interested in the probability $\\pi(s,t)$ that such a random walk starting in $s$ ends in $t$. We wish to, with constant probability, estimate $\\pi(s, t)$ within a constant relative error, unless $\\pi(s, t) < \\delta$ for some given threshold $\\delta$.","authors":["Christian Bertram","Mads Vestergaard Jensen","Mikkel Thorup","Hanzhi Wang","Shuyi Yan"],"url":"https://arxiv.org/abs/2504.16481"}
{"created":"2025-04-24","title":"Exploring turnover, retention and growth in an OSS Ecosystem","abstract":"The Gentoo ecosystem has evolved significantly over 23 years, highlighting the critical impact of developer sentiment on workforce dynamics such as turnover, retention, and growth. While prior research has explored sentiment at the project level, sentiment-driven dynamics at the component level remain underexplored, particularly in their implications for software stability.","authors":["Tien Rahayu Tulili","Ayushi Rastogi","Andrea Capiluppi"],"url":"https://arxiv.org/abs/2504.16483"}
{"created":"2025-04-24","title":"On Developers' Self-Declaration of AI-Generated Code: An Analysis of Practices","abstract":"AI code generation tools have gained significant popularity among developers, who use them to assist in software development due to their capability to generate code. Existing studies mainly explored the quality, e.g., correctness and security, of AI-generated code, while in real-world software development, the prerequisite is to distinguish AI-generated code from human-written code, which emphasizes the need to explicitly declare AI-generated code by developers. To this end, this study intends to understand the ways developers use to self-declare AI-generated code and explore the reasons why developers choose to self-declare or not. We conducted a mixed-methods study consisting of two phases. In the first phase, we mined GitHub repositories and collected 613 instances of AI-generated code snippets. In the second phase, we conducted a follow-up industrial survey, which received 111 valid responses. Our research revealed the practices followed by developers to self-declare AI-generated code. Most practitioners (76.6%) always or sometimes self-declare AI-generated code. In contrast, other practitioners (23.4%) noted that they never self-declare AI-generated code. The reasons for self-declaring AI-generated code include the need to track and monitor the code for future review and debugging, and ethical considerations. The reasons for not self-declaring AI-generated code include extensive modifications to AI-generated code and the developers' perception that self-declaration is an unnecessary activity. We finally provided guidelines for practitioners to self-declare AI-generated code, addressing ethical and code quality concerns.","authors":["Syed Mohammad Kashif","Peng Liang","Amjed Tahir"],"url":"https://arxiv.org/abs/2504.16485"}
{"created":"2025-04-24","title":"Rethinking Generalizable Infrared Small Target Detection: A Real-scene Benchmark and Cross-view Representation Learning","abstract":"Infrared small target detection (ISTD) is highly sensitive to sensor type, observation conditions, and the intrinsic properties of the target. These factors can introduce substantial variations in the distribution of acquired infrared image data, a phenomenon known as domain shift. Such distribution discrepancies significantly hinder the generalization capability of ISTD models across diverse scenarios. To tackle this challenge, this paper introduces an ISTD framework enhanced by domain adaptation. To alleviate distribution shift between datasets and achieve cross-sample alignment, we introduce Cross-view Channel Alignment (CCA). Additionally, we propose the Cross-view Top-K Fusion strategy, which integrates target information with diverse background features, enhancing the model' s ability to extract critical data characteristics. To further mitigate the impact of noise on ISTD, we develop a Noise-guided Representation learning strategy. This approach enables the model to learn more noise-resistant feature representations, to improve its generalization capability across diverse noisy domains. Finally, we develop a dedicated infrared small target dataset, RealScene-ISTD. Compared to state-of-the-art methods, our approach demonstrates superior performance in terms of detection probability (Pd), false alarm rate (Fa), and intersection over union (IoU). The code is available at: https://github.com/luy0222/RealScene-ISTD.","authors":["Yahao Lu","Yuehui Li","Xingyuan Guo","Shuai Yuan","Yukai Shi","Liang Lin"],"url":"https://arxiv.org/abs/2504.16487"}
{"created":"2025-04-24","title":"Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based Multi-Agent Debate","abstract":"Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment.","authors":["Senmao Qi","Yifei Zou","Peng Li","Ziyi Lin","Xiuzhen Cheng","Dongxiao Yu"],"url":"https://arxiv.org/abs/2504.16489"}
{"created":"2025-04-24","title":"LiDAL-Assisted RLNC-NOMA in OWC Systems","abstract":"Optical wireless communication (OWC) is envisioned as a key enabler for immersive indoor data transmission in future wireless communication networks. However, multi-user interference management arises as a challenge in dense indoor OWC systems composed of multiple optical access points (APs) serving multiple users. In this paper, we propose a novel dual-function OWC system for communication and localization. Non-orthogonal multiple access (NOMA) with random linear network coding (RLNC) is designed for data transmission, where NOMA allows the serving of multiple users simultaneously through controlling the power domain, and RLNC helps minimize errors that might occur during signal processing phase. This setup is assisted with a light detection and localization system (LiDAL) that can passively obtain spatio-temporal indoor information of user presence and location for dynamic-user grouping. The designed LiDAL system helps to improve the estimation of channel state information (CSI) in realistic indoor network scenarios, where the CSI of indoor users might be noisy and/or highly correlated. We evaluate the performance of NOMA combined with RLNC by analyzing the probability of successful decoding compared to conventional NOMA and orthogonal schemes. In addition, we derive the Cramer-Rao Lower Bound (CRLB) to evaluate the accuracy of location estimation. The results show that the proposed RLNC-NOMA improves the probability of successful decoding and the overall system performance. The results also show the high accuracy of the unbiased location estimator and its assistant in reducing the imperfection of CSI, leading to high overall system performance.","authors":["Ahmed A. Hassan","Ahmad Adnan Qidan","Taisir Elgorashi","Jaafar Elmirghani"],"url":"https://arxiv.org/abs/2504.16498"}
{"created":"2025-04-24","title":"PRaDA: Projective Radial Distortion Averaging","abstract":"We tackle the problem of automatic calibration of radially distorted cameras in challenging conditions. Accurately determining distortion parameters typically requires either 1) solving the full Structure from Motion (SfM) problem involving camera poses, 3D points, and the distortion parameters, which is only possible if many images with sufficient overlap are provided, or 2) relying heavily on learning-based methods that are comparatively less accurate. In this work, we demonstrate that distortion calibration can be decoupled from 3D reconstruction, maintaining the accuracy of SfM-based methods while avoiding many of the associated complexities. This is achieved by working in Projective Space, where the geometry is unique up to a homography, which encapsulates all camera parameters except for distortion. Our proposed method, Projective Radial Distortion Averaging, averages multiple distortion estimates in a fully projective framework without creating 3d points and full bundle adjustment. By relying on pairwise projective relations, our methods support any feature-matching approaches without constructing point tracks across multiple images.","authors":["Daniil Sinitsyn","Linus H\\\"arenstam-Nielsen","Daniel Cremers"],"url":"https://arxiv.org/abs/2504.16499"}
{"created":"2025-04-24","title":"Dynamic Time-aware Continual User Representation Learning","abstract":"Traditional user modeling (UM) approaches have primarily focused on designing models for a single specific task, but they face limitations in generalization and adaptability across various tasks. Recognizing these challenges, recent studies have shifted towards continual learning (CL)-based universal user representation learning aiming to develop a single model capable of handling multiple tasks. Despite advancements, existing methods are in fact evaluated under an unrealistic scenario that does not consider the passage of time as tasks progress, which overlooks newly emerged items that may change the item distribution of previous tasks. In this paper, we introduce a practical evaluation scenario on which CL-based universal user representation learning approaches should be evaluated, which takes into account the passage of time as tasks progress. Then, we propose a novel framework Dynamic Time-aware continual user representation learner, named DITTO, designed to alleviate catastrophic forgetting despite continuous shifts in item distribution, while also allowing the knowledge acquired from previous tasks to adapt to the current shifted item distribution. Through our extensive experiments, we demonstrate the superiority of DITTO over state-of-the-art methods under a practical evaluation scenario. Our source code is available at https://github.com/seungyoon-Choi/DITTO_official.","authors":["Seungyoon Choi","Sein Kim","Hongseok Kang","Wonjoong Kim","Chanyoung Park"],"url":"https://arxiv.org/abs/2504.16501"}
{"created":"2025-04-24","title":"Helping Blind People Grasp: Enhancing a Tactile Bracelet with an Automated Hand Navigation System","abstract":"Grasping constitutes a critical challenge for visually impaired people. To address this problem, we developed a tactile bracelet that assists in grasping by guiding the user's hand to a target object using vibration commands. Here we demonstrate the fully automated system around the bracelet, which can confidently detect and track target and distractor objects and reliably guide the user's hand. We validate our approach in three tasks that resemble complex, everyday use cases. In a grasping task, the participants grasp varying target objects on a table, guided via the automated hand navigation system. In the multiple objects task, participants grasp objects from the same class, demonstrating our system's ability to track one specific object without targeting surrounding distractor objects. Finally, the participants grasp one specific target object by avoiding an obstacle along the way in the depth navigation task, showcasing the potential to utilize our system's depth estimations to navigate even complex scenarios. Additionally, we demonstrate that the system can aid users in the real world by testing it in a less structured environment with a blind participant. Overall, our results demonstrate that the system, by translating the AI-processed visual inputs into a reduced data rate of actionable signals, enables autonomous behavior in everyday environments, thus potentially increasing the quality of life of visually impaired people.","authors":["Marcin Furtak","Florian P\\\"atzold","Tim Kietzmann","Silke M. K\\\"archer","Peter K\\\"onig"],"url":"https://arxiv.org/abs/2504.16502"}
{"created":"2025-04-24","title":"Neuro-Evolutionary Approach to Physics-Aware Symbolic Regression","abstract":"Symbolic regression is a technique that can automatically derive analytic models from data. Traditionally, symbolic regression has been implemented primarily through genetic programming that evolves populations of candidate solutions sampled by genetic operators, crossover and mutation. More recently, neural networks have been employed to learn the entire analytical model, i.e., its structure and coefficients, using regularized gradient-based optimization. Although this approach tunes the model's coefficients better, it is prone to premature convergence to suboptimal model structures. Here, we propose a neuro-evolutionary symbolic regression method that combines the strengths of evolutionary-based search for optimal neural network (NN) topologies with gradient-based tuning of the network's parameters. Due to the inherent high computational demand of evolutionary algorithms, it is not feasible to learn the parameters of every candidate NN topology to full convergence. Thus, our method employs a memory-based strategy and population perturbations to enhance exploitation and reduce the risk of being trapped in suboptimal NNs. In this way, each NN topology can be trained using only a short sequence of backpropagation iterations. The proposed method was experimentally evaluated on three real-world test problems and has been shown to outperform other NN-based approaches regarding the quality of the models obtained.","authors":["Ji\\v{r}\\'i Kubal\\'ik","Robert Babu\\v{s}ka"],"url":"https://arxiv.org/abs/2504.16503"}
{"created":"2025-04-24","title":"TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance","abstract":"Tourism and travel planning increasingly rely on digital assistance, yet existing multimodal AI systems often lack specialized knowledge and contextual understanding of urban environments. We present TraveLLaMA, a specialized multimodal language model designed for urban scene understanding and travel assistance. Our work addresses the fundamental challenge of developing practical AI travel assistants through a novel large-scale dataset of 220k question-answer pairs. This comprehensive dataset uniquely combines 130k text QA pairs meticulously curated from authentic travel forums with GPT-enhanced responses, alongside 90k vision-language QA pairs specifically focused on map understanding and scene comprehension. Through extensive fine-tuning experiments on state-of-the-art vision-language models (LLaVA, Qwen-VL, Shikra), we demonstrate significant performance improvements ranging from 6.5\\%-9.4\\% in both pure text travel understanding and visual question answering tasks. Our model exhibits exceptional capabilities in providing contextual travel recommendations, interpreting map locations, and understanding place-specific imagery while offering practical information such as operating hours and visitor reviews. Comparative evaluations show TraveLLaMA significantly outperforms general-purpose models in travel-specific tasks, establishing a new benchmark for multi-modal travel assistance systems.","authors":["Meng Chu","Yukang Chen","Haokun Gui","Shaozuo Yu","Yi Wang","Jiaya Jia"],"url":"https://arxiv.org/abs/2504.16505"}
{"created":"2025-04-24","title":"A Comprehensive Survey of Synthetic Tabular Data Generation","abstract":"Tabular data remains one of the most prevalent and critical data formats across diverse real-world applications. However, its effective use in machine learning (ML) is often constrained by challenges such as data scarcity, privacy concerns, and class imbalance. Synthetic data generation has emerged as a promising solution, leveraging generative models to learn the distribution of real datasets and produce high-fidelity, privacy-preserving samples. Various generative paradigms have been explored, including energy-based models (EBMs), variational autoencoders (VAEs), generative adversarial networks (GANs), large language models (LLMs), and diffusion models. While several surveys have investigated synthetic tabular data generation, most focus on narrow subdomains or specific generative methods, such as GANs, diffusion models, or privacy-preserving techniques. This limited scope often results in fragmented insights, lacking a comprehensive synthesis that bridges diverse approaches. In particular, recent advances driven by LLMs and diffusion-based models remain underexplored. This gap hinders a holistic understanding of the field`s evolution, methodological interplay, and open challenges. To address this, our survey provides a unified and systematic review of synthetic tabular data generation. Our contributions are threefold: (1) we propose a comprehensive taxonomy that organizes existing methods into traditional approaches, diffusion-based methods, and LLM-based models, and provide an in-depth comparative analysis; (2) we detail the complete pipeline for synthetic tabular data generation, including data synthesis, post-processing, and evaluation; (3) we identify major challenges, explore real-world applications, and outline open research questions and future directions to guide future work in this rapidly evolving area.","authors":["Ruxue Shi","Yili Wang","Mengnan Du","Xu Shen","Xin Wang"],"url":"https://arxiv.org/abs/2504.16506"}
{"created":"2025-04-24","title":"Streaming algorithms for products of probabilities","abstract":"We consider streaming algorithms for approximating a product of input probabilities up to multiplicative error of $1-\\epsilon$. It is shown that every randomized streaming algorithm for this problem needs space $\\Omega(\\log n + \\log b - \\log \\epsilon) - \\mathcal{O}(1)$, where $n$ is length of the input stream and $b$ is the bit length of the input numbers. This matches an upper bound from Alur et al.~up to a constant multiplicative factor. Moreover, we consider the threshold problem, where it is asked whether the product of the input probabilities is below a given threshold. It is shown that every randomized streaming algorithm for this problem needs space $\\Omega(n \\cdot b)$.","authors":["Markus Lohrey","Leon Rische","Louisa Seelbach Benkner","Julio Xochitemol"],"url":"https://arxiv.org/abs/2504.16507"}
{"created":"2025-04-24","title":"QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining","abstract":"Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.","authors":["Fengze Liu","Weidong Zhou","Binbin Liu","Zhimiao Yu","Yifan Zhang","Haobin Lin","Yifeng Yu","Xiaohuan Zhou","Taifeng Wang","Yong Cao"],"url":"https://arxiv.org/abs/2504.16511"}
{"created":"2025-04-24","title":"Federated Learning of Low-Rank One-Shot Image Detection Models in Edge Devices with Scalable Accuracy and Compute Complexity","abstract":"This paper introduces a novel federated learning framework termed LoRa-FL designed for training low-rank one-shot image detection models deployed on edge devices. By incorporating low-rank adaptation techniques into one-shot detection architectures, our method significantly reduces both computational and communication overhead while maintaining scalable accuracy. The proposed framework leverages federated learning to collaboratively train lightweight image recognition models, enabling rapid adaptation and efficient deployment across heterogeneous, resource-constrained devices. Experimental evaluations on the MNIST and CIFAR10 benchmark datasets, both in an independent-and-identically-distributed (IID) and non-IID setting, demonstrate that our approach achieves competitive detection performance while significantly reducing communication bandwidth and compute complexity. This makes it a promising solution for adaptively reducing the communication and compute power overheads, while not sacrificing model accuracy.","authors":["Abdul Hannaan","Zubair Shah","Aiman Erbad","Amr Mohamed","Ali Safa"],"url":"https://arxiv.org/abs/2504.16515"}
{"created":"2025-04-24","title":"Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.","authors":["Junrong Yue","Yifan Zhang","Chuan Qin","Bo Li","Xiaomin Lie","Xinlei Yu","Wenxin Zhang","Zhendong Zhao"],"url":"https://arxiv.org/abs/2504.16516"}
{"created":"2025-04-24","title":"Preconditioning Natural and Second Order Gradient Descent in Quantum Optimization: A Performance Benchmark","abstract":"The optimization of parametric quantum circuits is technically hindered by three major obstacles: the non-convex nature of the objective function, noisy gradient evaluations, and the presence of barren plateaus. As a result, the selection of classical optimizer becomes a critical factor in assessing and exploiting quantum-classical applications. One promising approach to tackle these challenges involves incorporating curvature information into the parameter update. The most prominent methods in this field are quasi-Newton and quantum natural gradient methods, which can facilitate faster convergence compared to first-order approaches. Second order methods however exhibit a significant trade-off between computational cost and accuracy, as well as heightened sensitivity to noise. This study evaluates the performance of three families of optimizers on synthetically generated MaxCut problems on a shallow QAOA algorithm. To address noise sensitivity and iteration cost, we demonstrate that incorporating secant-penalization in the BFGS update rule (SP-BFGS) yields improved outcomes for QAOA optimization problems, introducing a novel approach to stabilizing BFGS updates against gradient noise.","authors":["Th\\'eo Lisart-Liebermann","Arcesio Casta\\~neda Medina"],"url":"https://arxiv.org/abs/2504.16518"}
{"created":"2025-04-24","title":"A Few-Shot Metric Learning Method with Dual-Channel Attention for Cross-Modal Same-Neuron Identification","abstract":"In neuroscience research, achieving single-neuron matching across different imaging modalities is critical for understanding the relationship between neuronal structure and function. However, modality gaps and limited annotations present significant challenges. We propose a few-shot metric learning method with a dual-channel attention mechanism and a pretrained vision transformer to enable robust cross-modal neuron identification. The local and global channels extract soma morphology and fiber context, respectively, and a gating mechanism fuses their outputs. To enhance the model's fine-grained discrimination capability, we introduce a hard sample mining strategy based on the MultiSimilarityMiner algorithm, along with the Circle Loss function. Experiments on two-photon and fMOST datasets demonstrate superior Top-K accuracy and recall compared to existing methods. Ablation studies and t-SNE visualizations validate the effectiveness of each module. The method also achieves a favorable trade-off between accuracy and training efficiency under different fine-tuning strategies. These results suggest that the proposed approach offers a promising technical solution for accurate single-cell level matching and multimodal neuroimaging integration.","authors":["Wenwei Li","Liyi Cai","Wu Chen","Anan Li"],"url":"https://arxiv.org/abs/2504.16520"}
{"created":"2025-04-24","title":"Alternately-optimized SNN method for acoustic scattering problem in unbounded domain","abstract":"In this paper, we propose a novel machine learning-based method to solve the acoustic scattering problem in unbounded domain. We first employ the Dirichlet-to-Neumann (DtN) operator to truncate the physically unbounded domain into a computable bounded domain. This transformation reduces the original scattering problem in the unbounded domain to a boundary value problem within the bounded domain. To solve this boundary value problem, we design a neural network with a subspace layer, where each neuron in this layer represents a basis function. Consequently, the approximate solution can be expressed by a linear combination of these basis functions. Furthermore, we introduce an innovative alternating optimization technique which alternately updates the basis functions and their linear combination coefficients respectively by training and least squares methods. In our method, we set the coefficients of basis functions to 1 and use a new loss function each time train the subspace. These innovations ensure that the subspace formed by these basis functions is truly optimized. We refer to this method as the alternately-optimized subspace method based on neural networks (AO-SNN). Extensive numerical experiments demonstrate that our new method can significantly reduce the relative $l^2$ error to $10^{-7}$ or lower, outperforming existing machine learning-based methods to the best of our knowledge.","authors":["Haoming Song","Zhiqiang Sheng","Dong Wang","Junliang Lv"],"url":"https://arxiv.org/abs/2504.16523"}
{"created":"2025-04-24","title":"Modality Reliability Guided Multimodal Recommendation","abstract":"Multimodal recommendation faces an issue of the performance degradation that the uni-modal recommendation sometimes achieves the better performance. A possible reason is that the unreliable item modality data hurts the fusion result. Several existing studies have introduced weights for different modalities to reduce the contribution of the unreliable modality data in predicting the final user rating. However, they fail to provide appropriate supervisions for learning the modality weights, making the learned weights imprecise. Therefore, we propose a modality reliability guided multimodal recommendation framework that uniquely learns the modality weights supervised by the modality reliability. Considering that there is no explicit label provided for modality reliability, we resort to automatically identify it through the BPR recommendation objective. In particular, we define a modality reliability vector as the supervision label by the difference between modality-specific user ratings to positive and negative items, where a larger difference indicates a higher reliability of the modality as the BPR objective is better satisfied. Furthermore, to enhance the effectiveness of the supervision, we calculate the confidence level for the modality reliability vector, which dynamically adjusts the supervision strength and eliminates the harmful supervision. Extensive experiments on three real-world datasets show the effectiveness of the proposed method.","authors":["Xue Dong","Xuemeng Song","Na Zheng","Sicheng Zhao","Guiguang Ding"],"url":"https://arxiv.org/abs/2504.16524"}
{"created":"2025-04-24","title":"Using Causal Inference to Test Systems with Hidden and Interacting Variables: An Evaluative Case Study","abstract":"Software systems with large parameter spaces, nondeterminism and high computational cost are challenging to test. Recently, software testing techniques based on causal inference have been successfully applied to systems that exhibit such characteristics, including scientific models and autonomous driving systems. One significant limitation is that these are restricted to test properties where all of the variables involved can be observed and where there are no interactions between variables. In practice, this is rarely guaranteed; the logging infrastructure may not be available to record all of the necessary runtime variable values, and it can often be the case that an output of the system can be affected by complex interactions between variables. To address this, we leverage two additional concepts from causal inference, namely effect modification and instrumental variable methods. We build these concepts into an existing causal testing tool and conduct an evaluative case study which uses the concepts to test three system-level requirements of CARLA, a high-fidelity driving simulator widely used in autonomous vehicle development and testing. The results show that we can obtain reliable test outcomes without requiring large amounts of highly controlled test data or instrumentation of the code, even when variables interact with each other and are not recorded in the test data.","authors":["Michael Foster","Robert M. Hierons","Donghwan Shin","Neil Walkinshaw","Christopher Wild"],"url":"https://arxiv.org/abs/2504.16526"}
{"created":"2025-04-24","title":"Quantitative Strategy Templates","abstract":"This paper presents (permissive) \\emph{Quantitative Strategy Templates} (QaSTels) to succinctly represent infinitely many winning strategies in two-player energy and mean-payoff games. This transfers the recently introduced concept of \\emph{Permissive (qualitative) Strategy Templates} (PeSTels) for $\\omega$-regular games to games with quantitative objectives. We provide the theoretical and algorithmic foundations of (i) QaSTel synthesis, and (ii) their (incremental) combination with PeSTels for games with mixed quantitative and qualitative objectives. Using a prototype implementation of our synthesis algorithms, we demonstrate empirically that QaSTels extend the advantageous properties of strategy templates over single winning strategies -- known from PeSTels -- to games with (additional) quantitative objectives. This includes (i) the enhanced robustness of strategies due to their runtime-adaptability, and (ii) the compositionality of templates w.r.t. incrementally arriving objectives. We use control-inspired examples to illustrate these superior properties of QaSTels for CPS design.","authors":["Ashwani Anand","Satya Prakash Nayak","Ritam Raha","Irmak Sa\\u{g}lam","Anne-Kathrin Schmuck"],"url":"https://arxiv.org/abs/2504.16528"}
{"created":"2025-04-24","title":"6G EdgeAI: Performance Evaluation and Analysis","abstract":"Generative AI (GenAI) services powered by large language models (LLMs) increasingly deliver real-time interactions, yet existing 5G multi-access edge computing (MEC) architectures often treat communication and computing as separate domains, limiting their ability to meet stringent latency requirements. To address this challenge, we introduce an Integrated Communication and Computing (ICC) framework where computing capabilities are enabled to reside directly in radio access network (RAN) nodes and jointly manage bandwidth and computing resources. Our queueing-theoretic analysis shows that ICC outperforms 5G MEC, achieving higher service capacity (defined as the maximum arrival rate that maintains a specified fraction of jobs completed within a given delay budget) by 98%. We corroborate these gains through system-level simulations that account for transformer-based LLM workloads, realistic GPU specifications, and a priority-based scheduling scheme. The simulations show that ICC improves service capacity by 60%, demonstrating its potential to enable efficient, cost-effective real-time GenAI services in 6G.","authors":["Chien-Sheng Yang","Yu-Jen Ku","Yuan-Yao Lou","Nathan Tenny","Alex C. -C. Hsu"],"url":"https://arxiv.org/abs/2504.16529"}
{"created":"2025-04-24","title":"SafeSpect: Safety-First Augmented Reality Heads-up Display for Drone Inspections","abstract":"Current tablet-based interfaces for drone operations often impose a heavy cognitive load on pilots and reduce situational awareness by dividing attention between the video feed and the real world. To address these challenges, we designed a heads-up augmented reality (AR) interface that overlays in-situ information to support drone pilots in safety-critical tasks. Through participatory design workshops with professional pilots, we identified key features and developed an adaptive AR interface that dynamically switches between task and safety views to prevent information overload. We evaluated our prototype by creating a realistic building inspection task and comparing three interfaces: a 2D tablet, a static AR, and our adaptive AR design. A user study with 15 participants showed that the AR interface improved access to safety information, while the adaptive AR interface reduced cognitive load and enhanced situational awareness without compromising task performance. We offer design insights for developing safety-first heads-up AR interfaces.","authors":["Peisen Xu","J\\'er\\'emie Garcia","Wei Tsang Ooi","Christophe Jouffrais"],"url":"https://arxiv.org/abs/2504.16533"}
{"created":"2025-04-24","title":"Synthesiz3 This: an SMT-Based Approach for Synthesis with Uncomputable Symbols","abstract":"Program synthesis is the task of automatically constructing a program conforming to a given specification. In this paper we focus on synthesis of single-invocation recursion-free functions conforming to a specification given as a logical formula in the presence of uncomputable symbols (i.e., symbols used in the specification but not allowed in the resulting function). We approach the problem via SMT-solving methods: we present a quantifier elimination algorithm using model-based projections for both total and partial function synthesis, working with theories of uninterpreted functions and linear arithmetic and their combination. For this purpose we also extend model-based projection to produce witnesses for these theories. Further, we present procedures tailored for the case of uniquely determined solutions. We implemented a prototype of the algorithms using the SMT-solver Z3, demonstrating their practicality.","authors":["Petra Hozzov\\'a","Nikolaj Bj{\\o}rner"],"url":"https://arxiv.org/abs/2504.16536"}
{"created":"2025-04-24","title":"Transformers for Complex Query Answering over Knowledge Hypergraphs","abstract":"Complex Query Answering (CQA) has been extensively studied in recent years. In order to model data that is closer to real-world distribution, knowledge graphs with different modalities have been introduced. Triple KGs, as the classic KGs composed of entities and relations of arity 2, have limited representation of real-world facts. Real-world data is more sophisticated. While hyper-relational graphs have been introduced, there are limitations in representing relationships of varying arity that contain entities with equal contributions. To address this gap, we sampled new CQA datasets: JF17k-HCQA and M-FB15k-HCQA. Each dataset contains various query types that include logical operations such as projection, negation, conjunction, and disjunction. In order to answer knowledge hypergraph (KHG) existential first-order queries, we propose a two-stage transformer model, the Logical Knowledge Hypergraph Transformer (LKHGT), which consists of a Projection Encoder for atomic projection and a Logical Encoder for complex logical operations. Both encoders are equipped with Type Aware Bias (TAB) for capturing token interactions. Experimental results on CQA datasets show that LKHGT is a state-of-the-art CQA method over KHG and is able to generalize to out-of-distribution query types.","authors":["Hong Ting Tsang","Zihao Wang","Yangqiu Song"],"url":"https://arxiv.org/abs/2504.16537"}
{"created":"2025-04-24","title":"Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes","abstract":"Streetscapes are an essential component of urban space. Their assessment is presently either limited to morphometric properties of their mass skeleton or requires labor-intensive qualitative evaluations of visually perceived qualities. This paper introduces SAGAI: Streetscape Analysis with Generative Artificial Intelligence, a modular workflow for scoring street-level urban scenes using open-access data and vision-language models. SAGAI integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight version of the LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. The pipeline includes an automated mapping module that aggregates visual scores at both the point and street levels, enabling direct cartographic interpretation. It operates without task-specific training or proprietary software dependencies, supporting scalable and interpretable analysis of urban environments. Two exploratory case studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial outputs from vision-language inference. The initial results show strong performance for binary urban-rural scene classification, moderate precision in commercial feature detection, and lower estimates, but still informative, of sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a wide range of urban research themes, such as walkability, safety, or urban design, through prompt modification alone.","authors":["Joan Perez (Urban Geo Analytics","France)","Giovanni Fusco (Universite Cote-Azur-CNRS-AMU-Avignon Universite","ESPACE","France)"],"url":"https://arxiv.org/abs/2504.16538"}
{"created":"2025-04-24","title":"ToF-Splatting: Dense SLAM using Sparse Time-of-Flight Depth and Multi-Frame Integration","abstract":"Time-of-Flight (ToF) sensors provide efficient active depth sensing at relatively low power budgets; among such designs, only very sparse measurements from low-resolution sensors are considered to meet the increasingly limited power constraints of mobile and AR/VR devices. However, such extreme sparsity levels limit the seamless usage of ToF depth in SLAM. In this work, we propose ToF-Splatting, the first 3D Gaussian Splatting-based SLAM pipeline tailored for using effectively very sparse ToF input data. Our approach improves upon the state of the art by introducing a multi-frame integration module, which produces dense depth maps by merging cues from extremely sparse ToF depth, monocular color, and multi-view geometry. Extensive experiments on both synthetic and real sparse ToF datasets demonstrate the viability of our approach, as it achieves state-of-the-art tracking and mapping performances on reference datasets.","authors":["Andrea Conti","Matteo Poggi","Valerio Cambareri","Martin R. Oswald","Stefano Mattoccia"],"url":"https://arxiv.org/abs/2504.16545"}
{"created":"2025-04-24","title":"Tinkering Against Scaling","abstract":"The ascent of scaling in artificial intelligence research has revolutionized the field over the past decade, yet it presents significant challenges for academic researchers, particularly in computational social science and critical algorithm studies. The dominance of large language models, characterized by their extensive parameters and costly training processes, creates a disparity where only industry-affiliated researchers can access these resources. This imbalance restricts academic researchers from fully understanding their tools, leading to issues like reproducibility in computational social science and a reliance on black-box metaphors in critical studies.","authors":["Bolun Zhang","Yang Shen","Linzhuo Li","Yu Ji","Di Wu","Tongyu Wu","Lianghao Dai"],"url":"https://arxiv.org/abs/2504.16546"}
{"created":"2025-04-24","title":"Exploring human-SAV interaction using large language models: The impact of psychological ownership and anthropomorphism on user experience","abstract":"There has been extensive prior work exploring how psychological factors such as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs). However, limited research has been conducted on how prompt strategies in large language model (LLM)-powered SAV User Interfaces (UIs) affect users' perceptions, experiences, and intentions to adopt such technology. In this work, we investigate how conversational UIs powered by LLMs drive these psychological factors and psychological ownership, the sense of possession a user may come to feel towards an entity or object they may not legally own. We designed four SAV UIs with varying levels of anthropomorphic characteristics and psychological ownership triggers. Quantitative measures of psychological ownership, anthropomorphism, quality of service, disclosure tendency, sentiment of SAV responses, and overall acceptance were collected after participants interacted with each SAV. Qualitative feedback was also gathered regarding the experience of psychological ownership during the interactions. The results indicate that an SAV conversational UI designed to be more anthropomorphic and to induce psychological ownership improved users' perceptions of the SAV's human-like qualities and improved the sentiment of responses compared to a control condition. These findings provide practical guidance for designing LLM-based conversational UIs that enhance user experience and adoption of SAVs.","authors":["Lirui Guo","Michael G. Burke","Wynita M. Griggs"],"url":"https://arxiv.org/abs/2504.16548"}
{"created":"2025-04-24","title":"A Collaborative Intrusion Detection System Using Snort IDS Nodes","abstract":"Intrusion Detection Systems (IDSs) are integral to safeguarding networks by detecting and responding to threats from malicious traffic or compromised devices. However, standalone IDS deployments often fall short when addressing the increasing complexity and scale of modern cyberattacks. This paper proposes a Collaborative Intrusion Detection System (CIDS) that leverages Snort, an open-source network intrusion detection system, to enhance detection accuracy and reduce false positives. The proposed architecture connects multiple Snort IDS nodes to a centralised node and integrates with a Security Information and Event Management (SIEM) platform to facilitate real-time data sharing, correlation, and analysis. The CIDS design includes a scalable configuration of Snort sensors, a centralised database for log storage, and LogScale SIEM for advanced analytics and visualisation. By aggregating and analysing intrusion data from multiple nodes, the system enables improved detection of distributed and sophisticated attack patterns that standalone IDSs may miss. Performance evaluation against simulated attacks, including Nmap port scans and ICMP flood attacks, demonstrates our CIDS's ability to efficiently process large-scale network traffic, detect threats with higher accuracy, and reduce alert fatigue. This paper highlights the potential of CIDS in modern network environments and explores future enhancements, such as integrating machine learning for advanced threat detection and creating public datasets to support collaborative research. The proposed CIDS framework provides a promising foundation for building more resilient and adaptive network security systems.","authors":["Tom Davies","Max Hashem Eiza","Nathan Shone","Rob Lyon"],"url":"https://arxiv.org/abs/2504.16550"}
{"created":"2025-04-24","title":"DTVM: Revolutionizing Smart Contract Execution with Determinism and Compatibility","abstract":"We introduce the DeTerministic Virtual Machine (DTVM) Stack, a next-generation smart contract execution framework designed to address critical performance, determinism, and ecosystem compatibility challenges in blockchain networks. Building upon WebAssembly (Wasm) while maintaining full Ethereum Virtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle Intermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to balance compilation speed and execution efficiency. DTVM further accommodates diverse instruction set architectures (e.g., EVM, RISC-V) through modular adaptation layers. This enables seamless integration with DTVM's hybrid lazy-JIT compilation engine, which dynamically optimizes performance while preserving deterministic execution guarantees across heterogeneous environments. The key contributions including: 1). The framework achieves up to 2$\\times$ acceleration over evmone in dominant Ethereum contract (e.g. ERC20/721/1155) execution and reduces fibonacci computation latency by 11.8$\\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch mechanism enables sub-millisecond (0.95ms) post-deployment invocation times, outperforming up to about 23$\\times$ in compilation and invocation efficiency. 3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and AssemblyScript) through unified bytecode conversion while maintaining EVM ABI compatibility for seamless invocation. It reduces machine code object sizes by 30.0$\\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers SmartCogent, an AI-driven full-stack development experience, leveraging fine-tuned LLMs and retrieval-augmented generation to automate tasks across the smart contract lifecycle: development, debugging, security auditing, and deployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack).","authors":["Wei Zhou","Changzheng Wei","Ying Yan","Wei Tang","Zhihao Chen","Xiong Xu","Xuebing Huang","Wengang Chen","Jie Zhang","Yang Chen","Xiaofu Zheng","Hanghang Wu","Shenglong Chen","Ermei Wang","Xiangfei Chen","Yang Yu","Meng Wu","Tao Zhu","Liwei Yuan","Feng Yu","Alex Zhang","Wei Wang","Ji Luo","Zhengyu He","Wenbiao Zhao"],"url":"https://arxiv.org/abs/2504.16552"}
{"created":"2025-04-24","title":"Least-Squares-Embedded Optimization for Accelerated Convergence of PINNs in Acoustic Wavefield Simulations","abstract":"Physics-Informed Neural Networks (PINNs) have shown promise in solving partial differential equations (PDEs), including the frequency-domain Helmholtz equation. However, standard training of PINNs using gradient descent (GD) suffers from slow convergence and instability, particularly for high-frequency wavefields. For scattered acoustic wavefield simulation based on Helmholtz equation, we derive a hybrid optimization framework that accelerates training convergence by embedding a least-squares (LS) solver directly into the GD loss function. This formulation enables optimal updates for the linear output layer. Our method is applicable with or without perfectly matched layers (PML), and we provide practical tensor-based implementations for both scenarios. Numerical experiments on benchmark velocity models demonstrate that our approach achieves faster convergence, higher accuracy, and improved stability compared to conventional PINN training. In particular, our results show that the LS-enhanced method converges rapidly even in cases where standard GD-based training fails. The LS solver operates on a small normal matrix, ensuring minimal computational overhead and making the method scalable for large-scale wavefield simulations.","authors":["Mohammad Mahdi Abedi","David Pardo","Tariq Alkhalifah"],"url":"https://arxiv.org/abs/2504.16553"}
{"created":"2025-04-24","title":"How Irrationality Shapes Nash Equilibria: A Prospect-Theoretic Perspective","abstract":"Noncooperative games with uncertain payoffs have been classically studied under the expected-utility theory framework, which relies on the strong assumption that agents behave rationally. However, simple experiments on human decision makers found them to be not fully rational, due to their subjective risk perception. Prospect theory was proposed as an empirically-grounded model to incorporate irrational behaviours into game-theoretic models. But, how prospect theory shapes the set of Nash equilibria when considering irrational agents, is still poorly understood. To this end, we study how prospect theoretic transformations may generate new equilibria while eliminating existing ones. Focusing on aggregative games, we show that capturing users' irrationality can preserve symmetric equilibria while causing the vanishing of asymmetric equilibria. Further, there exist value functions which map uncountable sets of equilibria in the expected-utility maximization framework to finite sets. This last result may shape some equilibrium selection theories for human-in-the-loop systems where computing a single equilibrium is insufficient and comparison of equilibria is needed.","authors":["Ashok Krishnan K. S. (ARGO)","H\\'el\\`ene Le Cadre (INOCS)","Ana Bu\\v{s}i\\'c (ARGO","LINCS)"],"url":"https://arxiv.org/abs/2504.16556"}
{"created":"2025-04-24","title":"Beyond Anonymization: Object Scrubbing for Privacy-Preserving 2D and 3D Vision Tasks","abstract":"We introduce ROAR (Robust Object Removal and Re-annotation), a scalable framework for privacy-preserving dataset obfuscation that eliminates sensitive objects instead of modifying them. Our method integrates instance segmentation with generative inpainting to remove identifiable entities while preserving scene integrity. Extensive evaluations on 2D COCO-based object detection show that ROAR achieves 87.5% of the baseline detection average precision (AP), whereas image dropping achieves only 74.2% of the baseline AP, highlighting the advantage of scrubbing in preserving dataset utility. The degradation is even more severe for small objects due to occlusion and loss of fine-grained details. Furthermore, in NeRF-based 3D reconstruction, our method incurs a PSNR loss of at most 1.66 dB while maintaining SSIM and improving LPIPS, demonstrating superior perceptual quality. Our findings establish object removal as an effective privacy framework, achieving strong privacy guarantees with minimal performance trade-offs. The results highlight key challenges in generative inpainting, occlusion-robust segmentation, and task-specific scrubbing, setting the foundation for future advancements in privacy-preserving vision systems.","authors":["Murat Bilgehan Ertan","Ronak Sahu","Phuong Ha Nguyen","Kaleel Mahmood","Marten van Dijk"],"url":"https://arxiv.org/abs/2504.16557"}
{"created":"2025-04-24","title":"Unified Molecule Generation and Property Prediction","abstract":"Modeling the joint distribution of the data samples and their properties allows to construct a single model for both data generation and property prediction, with synergistic capabilities reaching beyond purely generative or predictive models. However, training joint models presents daunting architectural and optimization challenges. Here, we propose Hyformer, a transformer-based joint model that successfully blends the generative and predictive functionalities, using an alternating attention mask together with a unified pre-training scheme. We show that Hyformer rivals other joint models, as well as state-of-the-art molecule generation and property prediction models. Additionally, we show the benefits of joint modeling in downstream tasks of molecular representation learning, hit identification and antimicrobial peptide design.","authors":["Adam Izdebski","Jan Olszewski","Pankhil Gawade","Krzysztof Koras","Serra Korkmaz","Valentin Rauscher","Jakub M. Tomczak","Ewa Szczurek"],"url":"https://arxiv.org/abs/2504.16559"}
{"created":"2025-04-24","title":"A Vision for AI-Driven Adaptation of Dynamic AR Content to Users and Environments","abstract":"Augmented Reality (AR) is transforming the way we interact with virtual information in the physical world. By overlaying digital content in real-world environments, AR enables new forms of immersive and engaging experiences. However, existing AR systems often struggle to effectively manage the many interactive possibilities that AR presents. This vision paper speculates on AI-driven approaches for adaptive AR content placement, dynamically adjusting to user movement and environmental changes. By leveraging machine learning methods, such a system would intelligently manage content distribution between AR projections integrated into the external environment and fixed static content, enabling seamless UI layout and potentially reducing users' cognitive load. By exploring the possibilities of AI-driven dynamic AR content placement, we aim to envision new opportunities for innovation and improvement in various industries, from urban navigation and workplace productivity to immersive learning and beyond. This paper outlines a vision for the development of more intuitive, engaging, and effective AI-powered AR experiences.","authors":["Julian Rasch","Florian M\\\"uller","Francesco Chiossi"],"url":"https://arxiv.org/abs/2504.16562"}
{"created":"2025-04-24","title":"Enhancing LLM-Based Agents via Global Planning and Hierarchical Execution","abstract":"Intelligent agent systems based on Large Language Models (LLMs) have shown great potential in real-world applications. However, existing agent frameworks still face critical limitations in task planning and execution, restricting their effectiveness and generalizability. Specifically, current planning methods often lack clear global goals, leading agents to get stuck in local branches, or produce non-executable plans. Meanwhile, existing execution mechanisms struggle to balance complexity and stability, and their limited action space restricts their ability to handle diverse real-world tasks. To address these limitations, we propose GoalAct, a novel agent framework that introduces a continuously updated global planning mechanism and integrates a hierarchical execution strategy. GoalAct decomposes task execution into high-level skills, including searching, coding, writing and more, thereby reducing planning complexity while enhancing the agents' adaptability across diverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark with multiple types of legal tasks that require the use of multiple types of tools. Experimental results demonstrate that GoalAct achieves state-of-the-art (SOTA) performance, with an average improvement of 12.22% in success rate. These findings highlight GoalAct's potential to drive the development of more advanced intelligent agent systems, making them more effective across complex real-world applications. Our code can be found at https://github.com/cjj826/GoalAct.","authors":["Junjie Chen","Haitao Li","Jingli Yang","Yiqun Liu","Qingyao Ai"],"url":"https://arxiv.org/abs/2504.16563"}
{"created":"2025-04-24","title":"SAIP-Net: Enhancing Remote Sensing Image Segmentation via Spectral Adaptive Information Propagation","abstract":"Semantic segmentation of remote sensing imagery demands precise spatial boundaries and robust intra-class consistency, challenging conventional hierarchical models. To address limitations arising from spatial domain feature fusion and insufficient receptive fields, this paper introduces SAIP-Net, a novel frequency-aware segmentation framework that leverages Spectral Adaptive Information Propagation. SAIP-Net employs adaptive frequency filtering and multi-scale receptive field enhancement to effectively suppress intra-class feature inconsistencies and sharpen boundary lines. Comprehensive experiments demonstrate significant performance improvements over state-of-the-art methods, highlighting the effectiveness of spectral-adaptive strategies combined with expanded receptive fields for remote sensing image segmentation.","authors":["Zhongtao Wang","Xizhe Cao","Yisong Chen","Guoping Wang"],"url":"https://arxiv.org/abs/2504.16564"}
{"created":"2025-04-24","title":"Adaptive Query Algorithms for Relational Structures Based on Homomorphism Counts","abstract":"A query algorithm based on homomorphism counts is a procedure to decide membership for a class of finite relational structures using only homomorphism count queries. A left query algorithm can ask the number of homomorphisms from any structure to the input structure and a right query algorithm can ask the number of homomorphisms from the input structure to any other structure. We systematically compare the expressive power of different types of left or right query algorithms, including non-adaptive query algorithms, adaptive query algorithms that can ask a bounded number of queries, and adaptive query algorithms that can ask an unbounded number of queries. We also consider query algorithms where the homomorphism counting is done over the Boolean semiring $\\mathbb{B}$, meaning that only the existence of a homomorphism is recorded, not the precise number of them.","authors":["Balder ten Cate","Phokion G. Kolaitis","Arnar \\'A. Kristj\\'ansson"],"url":"https://arxiv.org/abs/2504.16567"}
{"created":"2025-04-24","title":"CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones","abstract":"Class-agnostic counting (CAC) aims to estimate the number of objects in images without being restricted to predefined categories. However, while current exemplar-based CAC methods offer flexibility at inference time, they still rely heavily on labeled data for training, which limits scalability and generalization to many downstream use cases. In this paper, we introduce CountingDINO, the first training-free exemplar-based CAC framework that exploits a fully unsupervised feature extractor. Specifically, our approach employs self-supervised vision-only backbones to extract object-aware features, and it eliminates the need for annotated data throughout the entire proposed pipeline. At inference time, we extract latent object prototypes via ROI-Align from DINO features and use them as convolutional kernels to generate similarity maps. These are then transformed into density maps through a simple yet effective normalization scheme. We evaluate our approach on the FSC-147 benchmark, where we outperform a baseline under the same label-free setting. Our method also achieves competitive -- and in some cases superior -- results compared to training-free approaches relying on supervised backbones, as well as several fully supervised state-of-the-art methods. This demonstrates that training-free CAC can be both scalable and competitive. Website: https://lorebianchi98.github.io/CountingDINO/","authors":["Giacomo Pacini","Lorenzo Bianchi","Luca Ciampi","Nicola Messina","Giuseppe Amato","Fabrizio Falchi"],"url":"https://arxiv.org/abs/2504.16570"}
{"created":"2025-04-24","title":"LaSDVS : A Post-Quantum Secure Compact Strong-Designated Verifier Signature","abstract":"Digital signatures are fundamental cryptographic primitives that ensure the authenticity and integrity of digital communication. However, in scenarios involving sensitive interactions -- such as e-voting or e-cash -- there is a growing need for more controlled signing mechanisms. Strong-Designated Verifier Signature (SDVS) offers such control by allowing the signer to specify and restrict the verifier of a signature. The existing state-of-the-art SDVS are mostly based on number-theoretic hardness assumptions. Thus, they are not secure against quantum attacks. Moreover, Post-Quantum Cryptography (PQC)-based SDVS are inefficient and have large key and signature sizes. In this work, we address these challenges and propose an efficient post-quantum SDVS (namely, LaSDVS) based on ideal lattices under the hardness assumptions of the Ring-SIS and Ring-LWE problems. LaSDVS achieves advanced security properties including strong unforgeability under chosen-message attacks, non-transferability, non-delegatability, and signer anonymity. By employing the algebraic structure of rings and the gadget trapdoor mechanism of Micciancio et al., we design LaSDVS to minimize computational overhead and significantly reduce key and signature sizes. Notably, our scheme achieves a compact signature size of $\\mathcal{O}(n\\log q)$, compared to $\\mathcal{O}(n^2)$ size, where $n$ is the security parameter, in the existing state-of-the-art PQC designs. To the best of our knowledge, LaSDVS offers the \\textit{smallest private key and signature size} among the existing PQC-based SDVS schemes.","authors":["Shanu Poddar","Sweta Mishra","Tapaswini Mohanty","Vikas Srivastava","Sugata Gangopadhyay"],"url":"https://arxiv.org/abs/2504.16571"}
{"created":"2025-04-24","title":"Bridging Data Gaps and Building Knowledge Networks in Indian Football Analytics","abstract":"The global rise of football analytics has rapidly transformed how clubs make strategic decisions. However, in India, the adoption of analytics remains constrained by institutional resistance, infrastructural limitations, and cultural barriers -- challenges that grassroots innovation and low-cost data solutions have the potential to overcome. Despite the growing popularity of the Indian Super League, resource scarcity and fragmented governance continue to hinder the widespread adoption and impact of analytics. This mixed-methods study explores how informal, decentralised analytics communities -- comprising amateur analysts and Twitter-based \"data sleuths\" -- navigate these constraints through peer mentorship and grassroots innovation. Drawing on extensive digital ethnography, participant observation, and interviews, the study illustrates how these informal networks mitigate data scarcity, limited digital infrastructure, and institutional indifference while fostering skill development and professional growth. Building on these insights, the paper proposes HCI interventions such as decentralised knowledge platforms to facilitate structured, cross-border peer mentorship and low-cost data solutions -- including AI-assisted player tracking and mobile analytics dashboards -- rooted in principles of frugal innovation. These interventions aim to bridge the data divide, support inclusive technical engagement in sport, and enhance analytics-driven decision-making in resource-constrained environments. This paper contributes to HCIxB's focus on cross-border collaboration by highlighting how community-driven technological adaptation in the Global South can foster meaningful participation, skill-building, and long-term sustainability through informal learning networks and scalable, context-sensitive tools.","authors":["Sneha Nanavati","Nimmi Rangaswamy"],"url":"https://arxiv.org/abs/2504.16572"}
{"created":"2025-04-24","title":"PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling Assistant System","abstract":"Psychological counseling is a highly personalized and dynamic process that requires therapists to continuously monitor emotional changes, document session insights, and maintain therapeutic continuity. In this paper, we introduce PsyCounAssist, a comprehensive AI-powered counseling assistant system specifically designed to augment psychological counseling practices. PsyCounAssist integrates multimodal emotion recognition combining speech and photoplethysmography (PPG) signals for accurate real-time affective analysis, automated structured session reporting using large language models (LLMs), and personalized AI-generated follow-up support. Deployed on Android-based tablet devices, the system demonstrates practical applicability and flexibility in real-world counseling scenarios. Experimental evaluation confirms the reliability of PPG-based emotional classification and highlights the system's potential for non-intrusive, privacy-aware emotional support. PsyCounAssist represents a novel approach to ethically and effectively integrating AI into psychological counseling workflows.","authors":["Xianghe Liu","Jiaqi Xu","Tao Sun"],"url":"https://arxiv.org/abs/2504.16573"}
{"created":"2025-04-24","title":"PIS: Linking Importance Sampling and Attention Mechanisms for Efficient Prompt Compression","abstract":"Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.","authors":["Lizhe Chen","Binjia Zhou","Yuyao Ge","Jiayi Chen","Shiguang NI"],"url":"https://arxiv.org/abs/2504.16574"}
{"created":"2025-04-24","title":"MMHCL: Multi-Modal Hypergraph Contrastive Learning for Recommendation","abstract":"The burgeoning presence of multimodal content-sharing platforms propels the development of personalized recommender systems. Previous works usually suffer from data sparsity and cold-start problems, and may fail to adequately explore semantic user-product associations from multimodal data. To address these issues, we propose a novel Multi-Modal Hypergraph Contrastive Learning (MMHCL) framework for user recommendation. For a comprehensive information exploration from user-product relations, we construct two hypergraphs, i.e. a user-to-user (u2u) hypergraph and an item-to-item (i2i) hypergraph, to mine shared preferences among users and intricate multimodal semantic resemblance among items, respectively. This process yields denser second-order semantics that are fused with first-order user-item interaction as complementary to alleviate the data sparsity issue. Then, we design a contrastive feature enhancement paradigm by applying synergistic contrastive learning. By maximizing/minimizing the mutual information between second-order (e.g. shared preference pattern for users) and first-order (information of selected items for users) embeddings of the same/different users and items, the feature distinguishability can be effectively enhanced. Compared with using sparse primary user-item interaction only, our MMHCL obtains denser second-order hypergraphs and excavates more abundant shared attributes to explore the user-product associations, which to a certain extent alleviates the problems of data sparsity and cold-start. Extensive experiments have comprehensively demonstrated the effectiveness of our method. Our code is publicly available at: https://github.com/Xu107/MMHCL.","authors":["Xu Guo","Tong Zhang","Fuyun Wang","Xudong Wang","Xiaoya Zhang","Xin Liu","Zhen Cui"],"url":"https://arxiv.org/abs/2504.16576"}
{"created":"2025-04-24","title":"Hyper-Transforming Latent Diffusion Models","abstract":"We introduce a novel generative framework for functions by integrating Implicit Neural Representations (INRs) and Transformer-based hypernetworks into latent variable models. Unlike prior approaches that rely on MLP-based hypernetworks with scalability limitations, our method employs a Transformer-based decoder to generate INR parameters from latent variables, addressing both representation capacity and computational efficiency. Our framework extends latent diffusion models (LDMs) to INR generation by replacing standard decoders with a Transformer-based hypernetwork, which can be trained either from scratch or via hyper-transforming-a strategy that fine-tunes only the decoder while freezing the pre-trained latent space. This enables efficient adaptation of existing generative models to INR-based representations without requiring full retraining.","authors":["Ignacio Peis","Batuhan Koyuncu","Isabel Valera","Jes Frellsen"],"url":"https://arxiv.org/abs/2504.16580"}
{"created":"2025-04-24","title":"Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code","abstract":"Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows.","authors":["Md. Azizul Hakim Bappy (Institute of Information","Communication Technology","Bangladesh University of Engineering Technology","Dhaka","Bangladesh)","Hossen A Mustafa (Institute of Information","Communication Technology","Bangladesh University of Engineering Technology","Dhaka","Bangladesh)","Prottoy Saha (Institute of Information","Communication Technology","Bangladesh University of Engineering Technology","Dhaka","Bangladesh)","Rajinus Salehat (Hajee Mohammad Danesh Science","Technology University","Dinajpur","Bangladesh)"],"url":"https://arxiv.org/abs/2504.16584"}
{"created":"2025-04-24","title":"Enhancing Variable Selection in Large-scale Logistic Regression: Leveraging Manual Labeling with Beneficial Noise","abstract":"In large-scale supervised learning, penalized logistic regression (PLR) effectively addresses the overfitting problem by introducing regularization terms yet its performance still depends on efficient variable selection strategies. This paper theoretically demonstrates that label noise stemming from manual labeling, which is solely related to classification difficulty, represents a type of beneficial noise for variable selection in PLR. This benefit is reflected in a more accurate estimation of the selected non-zero coefficients when compared with the case where only truth labels are used. Under large-scale settings, the sample size for PLR can become very large, making it infeasible to store on a single machine. In such cases, distributed computing methods are required to handle PLR model with manual labeling. This paper presents a partition-insensitive parallel algorithm founded on the ADMM (alternating direction method of multipliers) algorithm to address PLR by incorporating manual labeling. The partition insensitivity of the proposed algorithm refers to the fact that the solutions obtained by the algorithm will not change with the distributed storage of data. In addition, the algorithm has global convergence and a sublinear convergence rate. Experimental results indicate that, as compared with traditional variable selection classification techniques, the PLR with manually-labeled noisy data achieves higher estimation and classification accuracy across multiple large-scale datasets.","authors":["Xiaofei Wu","Rongmei Liang"],"url":"https://arxiv.org/abs/2504.16585"}
{"created":"2025-04-24","title":"Learning Switchable Priors for Neural Image Compression","abstract":"Neural image compression (NIC) usually adopts a predefined family of probabilistic distributions as the prior of the latent variables, and meanwhile relies on entropy models to estimate the parameters for the probabilistic family. More complex probabilistic distributions may fit the latent variables more accurately, but also incur higher complexity of the entropy models, limiting their practical value. To address this dilemma, we propose a solution to decouple the entropy model complexity from the prior distributions. We use a finite set of trainable priors that correspond to samples of the parametric probabilistic distributions. We train the entropy model to predict the index of the appropriate prior within the set, rather than the specific parameters. Switching between the trained priors further enables us to embrace a skip mode into the prior set, which simply omits a latent variable during the entropy coding. To demonstrate the practical value of our solution, we present a lightweight NIC model, namely FastNIC, together with the learning of switchable priors. FastNIC obtains a better trade-off between compression efficiency and computational complexity for neural image compression. We also implanted the switchable priors into state-of-the-art NIC models and observed improved compression efficiency with a significant reduction of entropy coding complexity.","authors":["Haotian Zhang","Yuqi Li","Li Li","Dong Liu"],"url":"https://arxiv.org/abs/2504.16586"}
{"created":"2025-04-24","title":"Data-Assimilated Model-Based Reinforcement Learning for Partially Observed Chaotic Flows","abstract":"The goal of many applications in energy and transport sectors is to control turbulent flows. However, because of chaotic dynamics and high dimensionality, the control of turbulent flows is exceedingly difficult. Model-free reinforcement learning (RL) methods can discover optimal control policies by interacting with the environment, but they require full state information, which is often unavailable in experimental settings. We propose a data-assimilated model-based RL (DA-MBRL) framework for systems with partial observability and noisy measurements. Our framework employs a control-aware Echo State Network for data-driven prediction of the dynamics, and integrates data assimilation with an Ensemble Kalman Filter for real-time state estimation. An off-policy actor-critic algorithm is employed to learn optimal control strategies from state estimates. The framework is tested on the Kuramoto-Sivashinsky equation, demonstrating its effectiveness in stabilizing a spatiotemporally chaotic flow from noisy and partial measurements.","authors":["Defne E. Ozan","Andrea N\\'ovoa","Luca Magri"],"url":"https://arxiv.org/abs/2504.16588"}
{"created":"2025-04-24","title":"JEPA for RL: Investigating Joint-Embedding Predictive Architectures for Reinforcement Learning","abstract":"Joint-Embedding Predictive Architectures (JEPA) have recently become popular as promising architectures for self-supervised learning. Vision transformers have been trained using JEPA to produce embeddings from images and videos, which have been shown to be highly suitable for downstream tasks like classification and segmentation. In this paper, we show how to adapt the JEPA architecture to reinforcement learning from images. We discuss model collapse, show how to prevent it, and provide exemplary data on the classical Cart Pole task.","authors":["Tristan Kenneweg","Philip Kenneweg","Barbara Hammer"],"url":"https://arxiv.org/abs/2504.16591"}
{"created":"2025-04-24","title":"Algorithmic Pricing and Algorithmic Collusion","abstract":"The rise of algorithmic pricing in online retail platforms has attracted significant interest in how autonomous software agents interact under competition. This article explores the potential emergence of algorithmic collusion - supra-competitive pricing outcomes that arise without explicit agreements - as a consequence of repeated interactions between learning agents. Most of the literature focuses on oligopoly pricing environments modeled as repeated Bertrand competitions, where firms use online learning algorithms to adapt prices over time. While experimental research has demonstrated that specific reinforcement learning algorithms can learn to maintain prices above competitive equilibrium levels in simulated environments, theoretical understanding of when and why such outcomes occur remains limited. This work highlights the interdisciplinary nature of this challenge, which connects computer science concepts of online learning with game-theoretical literature on equilibrium learning. We examine implications for the Business & Information Systems Engineering (BISE) community and identify specific research opportunities to address challenges of algorithmic competition in digital marketplaces.","authors":["Martin Bichler","Julius Durmann","Matthias Oberlechner"],"url":"https://arxiv.org/abs/2504.16592"}
{"created":"2025-04-24","title":"HERB: Human-augmented Efficient Reinforcement learning for Bin-packing","abstract":"Packing objects efficiently is a fundamental problem in logistics, warehouse automation, and robotics. While traditional packing solutions focus on geometric optimization, packing irregular, 3D objects presents significant challenges due to variations in shape and stability. Reinforcement Learning~(RL) has gained popularity in robotic packing tasks, but training purely from simulation can be inefficient and computationally expensive. In this work, we propose HERB, a human-augmented RL framework for packing irregular objects. We first leverage human demonstrations to learn the best sequence of objects to pack, incorporating latent factors such as space optimization, stability, and object relationships that are difficult to model explicitly. Next, we train a placement algorithm that uses visual information to determine the optimal object positioning inside a packing container. Our approach is validated through extensive performance evaluations, analyzing both packing efficiency and latency. Finally, we demonstrate the real-world feasibility of our method on a robotic system. Experimental results show that our method outperforms geometric and purely RL-based approaches by leveraging human intuition, improving both packing robustness and adaptability. This work highlights the potential of combining human expertise-driven RL to tackle complex real-world packing challenges in robotic systems.","authors":["Gojko Perovic","Nuno Ferreira Duarte","Atabak Dehban","Gon\\c{c}alo Teixeira","Egidio Falotico","Jos\\'e Santos-Victor"],"url":"https://arxiv.org/abs/2504.16595"}
{"created":"2025-04-24","title":"Learning Weighted Automata over Number Rings, Concretely and Categorically","abstract":"We develop a generic reduction procedure for active learning problems. Our approach is inspired by a recent polynomial-time reduction of the exact learning problem for weighted automata over integers to that for weighted automata over rationals (Buna-Marginean et al. 2024). Our procedure improves the efficiency of a category-theoretic automata learning algorithm, and poses new questions about the complexity of its implementation when instantiated to concrete categories. As our second main contribution, we address these complexity aspects in the concrete setting of learning weighted automata over number rings, that is, rings of integers in an algebraic number field. Assuming a full representation of a number ring OK, we obtain an exact learning algorithm of OK-weighted automata that runs in polynomial time in the size of the target automaton, the logarithm of the length of the longest counterexample, the degree of the number field, and the logarithm of its discriminant. Our algorithm produces an automaton that has at most one more state than the minimal one, and we prove that doing better requires solving the principal ideal problem, for which the best currently known algorithm is in quantum polynomial time.","authors":["Quentin Aristote (UPCit\\'e","IRIF)","Sam van Gool (UPCit\\'e","IRIF)","Daniela Petri\\c{s}an (UPCit\\'e","IRIF)","Mahsa Shirmohammadi (UPCit\\'e","IRIF)"],"url":"https://arxiv.org/abs/2504.16596"}
{"created":"2025-04-24","title":"3D-1D modelling of cranial plate heating induced by low or medium frequency magnetic fields","abstract":"Safety assessment of patients with one-dimensionally structured passive implants, like cranial plates or stents, exposed to low or medium frequency magnetic fields, like those generated in magnetic resonance imaging or magnetic hyperthermia, can be challenging, because of the different length scales of the implant and the human body. Most of the methods used to estimate the heating induced near such implants neglect the presence of the metallic materials within the body, modeling the metal as thermal seeds. To overcome this limitation, a novel numerical approach that solves three-dimensional and one-dimensional coupled problems is proposed. This method leads to improved results by modelling the thermal diffusion through the highly conductive metallic implants. A comparison of the proposed method predictions with measurements performed on a cranial plate exposed to the magnetic field generated by a gradient coil system for magnetic resonance imaging is presented, showing an improved accuracy up to 25 % with respect to the method based on thermal seeds. The proposed method is finally applied to a magnetic hyperthermia case study in which a patient with a cranial plate is exposed to the magnetic field generated by a collar-type magnetic hyperthermia applicator for neck tumour treatment, predicting a temperature increase in proximity of the implant that is 10 % lower than the one overestimated by relying on thermal seeds.","authors":["Alessandro Arduino","Oriano Bottauscio","Denise Grappein","Stefano Scial\\'o","Fabio Vicini","Umberto Zanovello","Luca Zilberti"],"url":"https://arxiv.org/abs/2504.16600"}
{"created":"2025-04-24","title":"Comparing Large Language Models and Traditional Machine Translation Tools for Translating Medical Consultation Summaries: A Pilot Study","abstract":"This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.","authors":["Andy Li","Wei Zhou","Rashina Hoda","Chris Bain","Peter Poon"],"url":"https://arxiv.org/abs/2504.16601"}
{"created":"2025-04-24","title":"Debunking with Dialogue? Exploring AI-Generated Counterspeech to Challenge Conspiracy Theories","abstract":"Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.","authors":["Mareike Lisker","Christina Gottschalk","Helena Mihaljevi\\'c"],"url":"https://arxiv.org/abs/2504.16604"}
{"created":"2025-04-24","title":"HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction","abstract":"As urban 3D scenes become increasingly complex and the demand for high-quality rendering grows, efficient scene reconstruction and rendering techniques become crucial. We present HUG, a novel approach to address inefficiencies in handling large-scale urban environments and intricate details based on 3D Gaussian splatting. Our method optimizes data partitioning and the reconstruction pipeline by incorporating a hierarchical neural Gaussian representation. We employ an enhanced block-based reconstruction pipeline focusing on improving reconstruction quality within each block and reducing the need for redundant training regions around block boundaries. By integrating neural Gaussian representation with a hierarchical architecture, we achieve high-quality scene rendering at a low computational cost. This is demonstrated by our state-of-the-art results on public benchmarks, which prove the effectiveness and advantages in large-scale urban scene representation.","authors":["Zhongtao Wang","Mai Su","Huishan Au","Yilong Li","Xizhe Cao","Chengwei Pan","Yisong Chen","Guoping Wang"],"url":"https://arxiv.org/abs/2504.16606"}
{"created":"2025-04-24","title":"A hybrid high-order method for the biharmonic problem","abstract":"This paper proposes a new hybrid high-order discretization for the biharmonic problem and the corresponding eigenvalue problem. The discrete ansatz space includes degrees of freedom in $n-2$ dimensional submanifolds (e.g., nodal values in 2D and edge values in 3D), in addition to the typical degrees of freedom in the mesh and on the hyperfaces in the HHO literature. This approach enables the characteristic commuting property of the hybrid high-order methodology in any space dimension and allows for lower eigenvalue bounds of higher order for the eigenvalue problem. The main results are quasi-best approximation estimates as well as reliable and efficient error control. The latter motivates an adaptive mesh-refining algorithm that empirically recovers optimal convergence rates for singular solutions.","authors":["Yizhou Liang","Ngoc Tien Tran"],"url":"https://arxiv.org/abs/2504.16608"}
{"created":"2025-04-24","title":"Information Leakage of Sentence Embeddings via Generative Embedding Inversion Attacks","abstract":"Text data are often encoded as dense vectors, known as embeddings, which capture semantic, syntactic, contextual, and domain-specific information. These embeddings, widely adopted in various applications, inherently contain rich information that may be susceptible to leakage under certain attacks. The GEIA framework highlights vulnerabilities in sentence embeddings, demonstrating that they can reveal the original sentences they represent. In this study, we reproduce GEIA's findings across various neural sentence embedding models. Additionally, we contribute new analysis to examine whether these models leak sensitive information from their training datasets. We propose a simple yet effective method without any modification to the attacker's architecture proposed in GEIA. The key idea is to examine differences between log-likelihood for masked and original variants of data that sentence embedding models have been pre-trained on, calculated on the embedding space of the attacker. Our findings indicate that following our approach, an adversary party can recover meaningful sensitive information related to the pre-training knowledge of the popular models used for creating sentence embeddings, seriously undermining their security. Our code is available on: https://github.com/taslanidis/GEIA","authors":["Antonios Tragoudaras","Theofanis Aslanidis","Emmanouil Georgios Lionis","Marina Orozco Gonz\\'alez","Panagiotis Eustratiadis"],"url":"https://arxiv.org/abs/2504.16609"}
{"created":"2025-04-24","title":"Federated EndoViT: Pretraining Vision Transformers via Federated Learning on Endoscopic Image Collections","abstract":"Purpose: In this study, we investigate the training of foundation models using federated learning to address data-sharing limitations and enable collaborative model training without data transfer for minimally invasive surgery. Methods: Inspired by the EndoViT study, we adapt the Masked Autoencoder for federated learning, enhancing it with adaptive Sharpness-Aware Minimization (FedSAM) and Stochastic Weight Averaging (SWA). Our model is pretrained on the Endo700k dataset collection and later fine-tuned and evaluated for tasks such as Semantic Segmentation, Action Triplet Recognition, and Surgical Phase Recognition. Results: Our findings demonstrate that integrating adaptive FedSAM into the federated MAE approach improves pretraining, leading to a reduction in reconstruction loss per patch. The application of FL-EndoViT in surgical downstream tasks results in performance comparable to CEN-EndoViT. Furthermore, FL-EndoViT exhibits advantages over CEN-EndoViT in surgical scene segmentation when data is limited and in action triplet recognition when large datasets are used. Conclusion: These findings highlight the potential of federated learning for privacy-preserving training of surgical foundation models, offering a robust and generalizable solution for surgical data science. Effective collaboration requires adapting federated learning methods, such as the integration of FedSAM, which can accommodate the inherent data heterogeneity across institutions. In future, exploring FL in video-based models may enhance these capabilities by incorporating spatiotemporal dynamics crucial for real-world surgical environments.","authors":["Max Kirchner","Alexander C. Jenke","Sebastian Bodenstedt","Fiona R. Kolbinger","Oliver Saldanha","Jakob N. Kather","Martin Wagner","Stefanie Speidel"],"url":"https://arxiv.org/abs/2504.16612"}
{"created":"2025-04-24","title":"Algorithmic Mirror: Designing an Interactive Tool to Promote Self-Reflection for YouTube Recommendations","abstract":"Big Data analytics and Artificial Intelligence systems derive non-intuitive and often unverifiable inferences about individuals' behaviors, preferences, and private lives. Drawing on diverse, feature-rich datasets of unpredictable value, these systems erode the intuitive connection between our actions and how we are perceived, diminishing control over our digital identities. While Explainable Artificial Intelligence scholars have attempted to explain the inner workings of algorithms, their visualizations frequently overwhelm end-users with complexity. This research introduces 'hypothetical inference', a novel approach that uses language models to simulate how algorithms might interpret users' digital footprints and infer personal characteristics without requiring access to proprietary platform algorithms. Through empirical studies with fourteen adult participants, we identified three key design opportunities to foster critical algorithmic literacy: (1) reassembling scattered digital footprints into a unified map, (2) simulating algorithmic inference through LLM-generated interpretations, and (3) incorporating temporal dimensions to visualize evolving patterns. This research lays the groundwork for tools that can help users recognize the influence of data on platforms and develop greater autonomy in increasingly algorithm-mediated digital environments.","authors":["Yui Kondo","Kevin Dunnell","Qing Xiao","Jun Zhao","Luc Rocher"],"url":"https://arxiv.org/abs/2504.16615"}
{"created":"2025-04-24","title":"EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception","abstract":"Event cameras, with microsecond temporal resolution and high dynamic range (HDR) characteristics, emit high-speed event stream for perception tasks. Despite the recent advancement in GNN-based perception methods, they are prone to use straightforward pairwise connectivity mechanisms in the pure Euclidean space where they struggle to capture long-range dependencies and fail to effectively characterize the inherent hierarchical structures of non-uniformly distributed event stream. To this end, in this paper we propose a novel approach named EHGCN, which is a pioneer to perceive event stream in both Euclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an adaptive sampling strategy to dynamically regulate sampling rates, retaining discriminative events while attenuating chaotic noise. Then we present a Markov Vector Field (MVF)-driven motion-aware hyperedge generation method based on motion state transition probabilities, thereby eliminating cross-target spurious associations and providing critically topological priors while capturing long-range dependencies between events. Finally, we propose a Euclidean-Hyperbolic GCN to fuse the information locally aggregated and globally hierarchically modeled in Euclidean and hyperbolic spaces, respectively, to achieve hybrid event perception. Experimental results on event perception tasks such as object detection and recognition validate the effectiveness of our approach.","authors":["Haosheng Chen","Lian Luo","Mengjingcheng Mo","Zhanjie Wu","Guobao Xiao","Ji Gan","Jiaxu Leng","Xinbo Gao"],"url":"https://arxiv.org/abs/2504.16616"}
{"created":"2025-04-24","title":"Security Science (SecSci), Basic Concepts and Mathematical Foundations","abstract":"This textbook compiles the lecture notes from security courses taught at Oxford in the 2000s, at Royal Holloway in the 2010s, and currently in Hawaii. The early chapters are suitable for a first course in security. The middle chapters have been used in advanced courses. Towards the end there are also some research problems.","authors":["Dusko Pavlovic","Peter-Michael Seidel"],"url":"https://arxiv.org/abs/2504.16617"}
{"created":"2025-04-24","title":"Cognitive Silicon: An Architectural Blueprint for Post-Industrial Computing Systems","abstract":"Autonomous AI systems reveal foundational limitations in deterministic, human-authored computing architectures. This paper presents Cognitive Silicon: a hypothetical full-stack architectural framework projected toward 2035, exploring a possible trajectory for cognitive computing system design. The proposed architecture would integrate symbolic scaffolding, governed memory, runtime moral coherence, and alignment-aware execution across silicon-to-semantics layers. Our design grammar has emerged from dialectical co-design with LLMs under asymmetric epistemic conditions--creating structured friction to expose blind spots and trade-offs. The envisioned framework would establish mortality as a natural consequence of physical constraints, non-copyable tacit knowledge, and non-cloneable identity keys as cognitive-embodiment primitives. Core tensions (trust/agency, scaffolding/emergence, execution/governance) would function as central architectural pressures rather than edge cases. The architecture theoretically converges with the Free Energy Principle, potentially offering a formal account of how cognitive systems could maintain identity through prediction error minimization across physical and computational boundaries. The resulting framework aims to deliver a morally tractable cognitive infrastructure that could maintain human-alignment through irreversible hardware constraints and identity-bound epistemic mechanisms resistant to replication or subversion.","authors":["Christoforus Yoga Haryanto","Emily Lomempow"],"url":"https://arxiv.org/abs/2504.16622"}
{"created":"2025-04-24","title":"Compositional Active Learning of Synchronous Systems through Automated Alphabet Refinement","abstract":"Active automata learning infers automaton models of systems from behavioral observations, a technique successfully applied to a wide range of domains. Compositional approaches for concurrent systems have recently emerged. We take a significant step beyond available results, including those by the authors, and develop a general technique for compositional learning of a synchronizing parallel system with an unknown decomposition. Our approach automatically refines the global alphabet into component alphabets while learning the component models. We develop a theoretical treatment of distributions of alphabets, i.e., sets of possibly overlapping component alphabets. We characterize counter-examples that reveal inconsistencies with global observations, and show how to systematically update the distribution to restore consistency. We present a compositional learning algorithm implementing these ideas, where learning counterexamples precisely correspond to distribution counterexamples under well-defined conditions. We provide an implementation, called CoalA, using the state-of-the-art active learning library LearnLib. Our experiments show that in more than 630 subject systems, CoalA delivers orders of magnitude improvements (up to five orders) in membership queries and in systems with significant concurrency, it also achieves better scalability in the number of equivalence queries.","authors":["Leo Henry","Thomas Neele","Mohammad Mousavi","Matteo Sammartino"],"url":"https://arxiv.org/abs/2504.16624"}
{"created":"2025-04-24","title":"TIFIN India at SemEval-2025: Harnessing Translation to Overcome Multilingual IR Challenges in Fact-Checked Claim Retrieval","abstract":"We address the challenge of retrieving previously fact-checked claims in monolingual and crosslingual settings - a critical task given the global prevalence of disinformation. Our approach follows a two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker. Our key contribution is demonstrating how LLM-based translation can overcome the hurdles of multilingual information retrieval. Additionally, we focus on ensuring that the bulk of the pipeline can be replicated on a consumer GPU. Our final integrated system achieved a success@10 score of 0.938 and 0.81025 on the monolingual and crosslingual test sets, respectively.","authors":["Prasanna Devadiga","Arya Suneesh","Pawan Kumar Rajpoot","Bharatdeep Hazarika","Aditya U Baliga"],"url":"https://arxiv.org/abs/2504.16627"}
{"created":"2025-04-24","title":"ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data","abstract":"Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.","authors":["Haoran Gu","Handing Wang","Yi Mei","Mengjie Zhang","Yaochu Jin"],"url":"https://arxiv.org/abs/2504.16628"}
{"created":"2025-04-24","title":"Bridging Econometrics and AI: VaR Estimation via Reinforcement Learning and GARCH Models","abstract":"In an environment of increasingly volatile financial markets, the accurate estimation of risk remains a major challenge. Traditional econometric models, such as GARCH and its variants, are based on assumptions that are often too rigid to adapt to the complexity of the current market dynamics. To overcome these limitations, we propose a hybrid framework for Value-at-Risk (VaR) estimation, combining GARCH volatility models with deep reinforcement learning. Our approach incorporates directional market forecasting using the Double Deep Q-Network (DDQN) model, treating the task as an imbalanced classification problem. This architecture enables the dynamic adjustment of risk-level forecasts according to market conditions. Empirical validation on daily Eurostoxx 50 data covering periods of crisis and high volatility shows a significant improvement in the accuracy of VaR estimates, as well as a reduction in the number of breaches and also in capital requirements, while respecting regulatory risk thresholds. The ability of the model to adjust risk levels in real time reinforces its relevance to modern and proactive risk management.","authors":["Fredy Pokou (CRIStAL","INOCS)","Jules Sadefo Kamdem (MRE)","Fran\\c{c}ois Benhmad (MRE)"],"url":"https://arxiv.org/abs/2504.16635"}
{"created":"2025-04-24","title":"Dual-Camera All-in-Focus Neural Radiance Fields","abstract":"We present the first framework capable of synthesizing the all-in-focus neural radiance field (NeRF) from inputs without manual refocusing. Without refocusing, the camera will automatically focus on the fixed object for all views, and current NeRF methods typically using one camera fail due to the consistent defocus blur and a lack of sharp reference. To restore the all-in-focus NeRF, we introduce the dual-camera from smartphones, where the ultra-wide camera has a wider depth-of-field (DoF) and the main camera possesses a higher resolution. The dual camera pair saves the high-fidelity details from the main camera and uses the ultra-wide camera's deep DoF as reference for all-in-focus restoration. To this end, we first implement spatial warping and color matching to align the dual camera, followed by a defocus-aware fusion module with learnable defocus parameters to predict a defocus map and fuse the aligned camera pair. We also build a multi-view dataset that includes image pairs of the main and ultra-wide cameras in a smartphone. Extensive experiments on this dataset verify that our solution, termed DC-NeRF, can produce high-quality all-in-focus novel views and compares favorably against strong baselines quantitatively and qualitatively. We further show DoF applications of DC-NeRF with adjustable blur intensity and focal plane, including refocusing and split diopter.","authors":["Xianrui Luo","Zijin Wu","Juewen Peng","Huiqiang Sun","Zhiguo Cao","Guosheng Lin"],"url":"https://arxiv.org/abs/2504.16636"}
{"created":"2025-04-24","title":"RouteWinFormer: A Route-Window Transformer for Middle-range Attention in Image Restoration","abstract":"Transformer models have recently garnered significant attention in image restoration due to their ability to capture long-range pixel dependencies. However, long-range attention often results in computational overhead without practical necessity, as degradation and context are typically localized. Normalized average attention distance across various degradation datasets shows that middle-range attention is enough for image restoration. Building on this insight, we propose RouteWinFormer, a novel window-based Transformer that models middle-range context for image restoration. RouteWinFormer incorporates Route-Windows Attnetion Module, which dynamically selects relevant nearby windows based on regional similarity for attention aggregation, extending the receptive field to a mid-range size efficiently. In addition, we introduce Multi-Scale Structure Regularization during training, enabling the sub-scale of the U-shaped network to focus on structural information, while the original-scale learns degradation patterns based on generalized image structure priors. Extensive experiments demonstrate that RouteWinFormer outperforms state-of-the-art methods across 9 datasets in various image restoration tasks.","authors":["Qifan Li","Tianyi Liang","Xingtao Wang","Xiaopeng Fan"],"url":"https://arxiv.org/abs/2504.16637"}
{"created":"2025-04-24","title":"DAPLSR: Data Augmentation Partial Least Squares Regression Model via Manifold Optimization","abstract":"Traditional Partial Least Squares Regression (PLSR) models frequently underperform when handling data characterized by uneven categories. To address the issue, this paper proposes a Data Augmentation Partial Least Squares Regression (DAPLSR) model via manifold optimization. The DAPLSR model introduces the Synthetic Minority Over-sampling Technique (SMOTE) to increase the number of samples and utilizes the Value Difference Metric (VDM) to select the nearest neighbor samples that closely resemble the original samples for generating synthetic samples. In solving the model, in order to obtain a more accurate numerical solution for PLSR, this paper proposes a manifold optimization method that uses the geometric properties of the constraint space to improve model degradation and optimization. Comprehensive experiments show that the proposed DAPLSR model achieves superior classification performance and outstanding evaluation metrics on various datasets, significantly outperforming existing methods.","authors":["Haoran Chen","Jiapeng Liu","Jiafan Wang","Wenjun Shi"],"url":"https://arxiv.org/abs/2504.16639"}
{"created":"2025-04-24","title":"SSLR: A Semi-Supervised Learning Method for Isolated Sign Language Recognition","abstract":"Sign language is the primary communication language for people with disabling hearing loss. Sign language recognition (SLR) systems aim to recognize sign gestures and translate them into spoken language. One of the main challenges in SLR is the scarcity of annotated datasets. To address this issue, we propose a semi-supervised learning (SSL) approach for SLR (SSLR), employing a pseudo-label method to annotate unlabeled samples. The sign gestures are represented using pose information that encodes the signer's skeletal joint points. This information is used as input for the Transformer backbone model utilized in the proposed approach. To demonstrate the learning capabilities of SSL across various labeled data sizes, several experiments were conducted using different percentages of labeled data with varying numbers of classes. The performance of the SSL approach was compared with a fully supervised learning-based model on the WLASL-100 dataset. The obtained results of the SSL model outperformed the supervised learning-based model with less labeled data in many cases.","authors":["Hasan Algafri","Hamzah Luqman","Sarah Alyami","Issam Laradji"],"url":"https://arxiv.org/abs/2504.16640"}
{"created":"2025-04-24","title":"Hitting and Covering Affine Families of Convex Polyhedra, with Applications to Robust Optimization","abstract":"Geometric hitting set problems, in which we seek a smallest set of points that collectively hit a given set of ranges, are ubiquitous in computational geometry. Most often, the set is discrete and is given explicitly. We propose new variants of these problems, dealing with continuous families of convex polyhedra, and show that they capture decision versions of the two-level finite adaptability problem in robust optimization. We show that these problems can be solved in strongly polynomial time when the size of the hitting/covering set and the dimension of the polyhedra and the parameter space are constant. We also show that the hitting set problem can be solved in strongly quadratic time for one-parameter families of convex polyhedra in constant dimension. This leads to new tractability results for finite adaptability that are the first ones with so-called left-hand-side uncertainty, where the underlying problem is non-linear.","authors":["Jean Cardinal","Xavier Goaoc","Sarah Wajsbrot"],"url":"https://arxiv.org/abs/2504.16642"}
{"created":"2025-04-24","title":"A Systematic Review of Common Beginner Programming Mistakes in Data Engineering","abstract":"The design of effective programming languages, libraries, frameworks, tools, and platforms for data engineering strongly depends on their ease and correctness of use. Anyone who ignores that it is humans who use these tools risks building tools that are useless, or worse, harmful. To ensure our data engineering tools are based on solid foundations, we performed a systematic review of common programming mistakes in data engineering. We focus on programming beginners (students) by analyzing both the limited literature specific to data engineering mistakes and general programming mistakes in languages commonly used in data engineering (Python, SQL, Java). Through analysis of 21 publications spanning from 2003 to 2024, we synthesized these complementary sources into a comprehensive classification that captures both general programming challenges and domain-specific data engineering mistakes. This classification provides an empirical foundation for future tool development and educational strategies. We believe our systematic categorization will help researchers, practitioners, and educators better understand and address the challenges faced by novice data engineers.","authors":["Max Neuwinger","Dirk Riehle"],"url":"https://arxiv.org/abs/2504.16644"}
{"created":"2025-04-24","title":"PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands","abstract":"Robots are increasingly envisioned as human companions, assisting with everyday tasks that often involve manipulating deformable objects. Although recent advances in robotic hardware and embodied AI have expanded their capabilities, current systems still struggle with handling thin, flat, and deformable objects such as paper and fabric. This limitation arises from the lack of suitable perception techniques for robust state estimation under diverse object appearances, as well as the absence of planning techniques for generating appropriate grasp motions. To bridge these gaps, this paper introduces PP-Tac, a robotic system for picking up paper-like objects. PP-Tac features a multi-fingered robotic hand with high-resolution omnidirectional tactile sensors \\sensorname. This hardware configuration enables real-time slip detection and online frictional force control that mitigates such slips. Furthermore, grasp motion generation is achieved through a trajectory synthesis pipeline, which first constructs a dataset of finger's pinching motions. Based on this dataset, a diffusion-based policy is trained to control the hand-arm robotic system. Experiments demonstrate that PP-Tac can effectively grasp paper-like objects of varying material, thickness, and stiffness, achieving an overall success rate of 87.5\\%. To our knowledge, this work is the first attempt to grasp paper-like deformable objects using a tactile dexterous hand. Our project webpage can be found at: https://peilin-666.github.io/projects/PP-Tac/","authors":["Pei Lin","Yuzhe Huang","Wanlin Li","Jianpeng Ma","Chenxi Xiao","Ziyuan Jiao"],"url":"https://arxiv.org/abs/2504.16649"}
{"created":"2025-04-24","title":"MAYA: Addressing Inconsistencies in Generative Password Guessing through a Unified Benchmark","abstract":"The rapid evolution of generative models has led to their integration across various fields, including password guessing, aiming to generate passwords that resemble human-created ones in complexity, structure, and patterns. Despite generative model's promise, inconsistencies in prior research and a lack of rigorous evaluation have hindered a comprehensive understanding of their true potential. In this paper, we introduce MAYA, a unified, customizable, plug-and-play password benchmarking framework. MAYA provides a standardized approach for evaluating generative password-guessing models through a rigorous set of advanced testing scenarios and a collection of eight real-life password datasets. Using MAYA, we comprehensively evaluate six state-of-the-art approaches, which have been re-implemented and adapted to ensure standardization, for a total of over 15,000 hours of computation. Our findings indicate that these models effectively capture different aspects of human password distribution and exhibit strong generalization capabilities. However, their effectiveness varies significantly with long and complex passwords. Through our evaluation, sequential models consistently outperform other generative architectures and traditional password-guessing tools, demonstrating unique capabilities in generating accurate and complex guesses. Moreover, models learn and generate different password distributions, enabling a multi-model attack that outperforms the best individual model. By releasing MAYA, we aim to foster further research, providing the community with a new tool to consistently and reliably benchmark password-generation techniques. Our framework is publicly available at https://github.com/williamcorrias/MAYA-Password-Benchmarking","authors":["William Corrias","Fabio De Gaspari","Dorjan Hitaj","Luigi V. Mancini"],"url":"https://arxiv.org/abs/2504.16651"}
{"created":"2025-04-24","title":"WiFi based Human Fall and Activity Recognition using Transformer based Encoder Decoder and Graph Neural Networks","abstract":"Human pose estimation and action recognition have received attention due to their critical roles in healthcare monitoring, rehabilitation, and assistive technologies. In this study, we proposed a novel architecture named Transformer based Encoder Decoder Network (TED Net) designed for estimating human skeleton poses from WiFi Channel State Information (CSI). TED Net integrates convolutional encoders with transformer based attention mechanisms to capture spatiotemporal features from CSI signals. The estimated skeleton poses were used as input to a customized Directed Graph Neural Network (DGNN) for action recognition. We validated our model on two datasets: a publicly available multi modal dataset for assessing general pose estimation, and a newly collected dataset focused on fall related scenarios involving 20 participants. Experimental results demonstrated that TED Net outperformed existing approaches in pose estimation, and that the DGNN achieves reliable action classification using CSI based skeletons, with performance comparable to RGB based systems. Notably, TED Net maintains robust performance across both fall and non fall cases. These findings highlight the potential of CSI driven human skeleton estimation for effective action recognition, particularly in home environments such as elderly fall detection. In such settings, WiFi signals are often readily available, offering a privacy preserving alternative to vision based methods, which may raise concerns about continuous camera monitoring.","authors":["Younggeol Cho","Elisa Motta","Olivia Nocentini","Marta Lagomarsino","Andrea Merello","Marco Crepaldi","Arash Ajoudani"],"url":"https://arxiv.org/abs/2504.16655"}
{"created":"2025-04-24","title":"Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning","abstract":"We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we propose the Selective Sample Buffer (SSB) mechanism, which effectively counters the ``Vanishing Advantages'' dilemma inherent in Group Relative Policy Optimization (GRPO) by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinations--a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 79.0 on AIME2024, 63.6 on LiveCodeBench, and 74.0 on MMMU. These results underscore R1V2's superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility https://huggingface.co/Skywork/Skywork-R1V2-38B.","authors":["Chris","Yichen Wei","Yi Peng","Xiaokun Wang","Weijie Qiu","Wei Shen","Tianyidan Xie","Jiangbo Pei","Jianhao Zhang","Yunzhuo Hao","Xuchen Song","Yang Liu","Yahui Zhou"],"url":"https://arxiv.org/abs/2504.16656"}
{"created":"2025-04-24","title":"A Time Series Dataset of NIR Spectra and RGB and NIR-HSI Images of the Barley Germination Process","abstract":"We provide an open-source dataset of RGB and NIR-HSI (near-infrared hyperspectral imaging) images with associated segmentation masks and NIR spectra of 2242 individual malting barley kernels. We imaged every kernel pre-exposure to moisture and every 24 hours after exposure to moisture for five consecutive days. Every barley kernel was labeled as germinated or not germinated during each image acquisition. The barley kernels were imaged with black filter paper as the background, facilitating straight-forward intensity threshold-based segmentation, e.g., by Otsu's method. This dataset facilitates time series analysis of germination time for barley kernels using either RGB image analysis, NIR spectral analysis, NIR-HSI analysis, or a combination hereof.","authors":["Ole-Christian Galbo Engstr{\\o}m","Erik Schou Dreier","Birthe M{\\o}ller Jespersen","Kim Steenstrup Pedersen"],"url":"https://arxiv.org/abs/2504.16658"}
{"created":"2025-04-24","title":"A Diff-Attention Aware State Space Fusion Model for Remote Sensing Classification","abstract":"Multispectral (MS) and panchromatic (PAN) images describe the same land surface, so these images not only have their own advantages, but also have a lot of similar information. In order to separate these similar information and their respective advantages, reduce the feature redundancy in the fusion stage. This paper introduces a diff-attention aware state space fusion model (DAS2F-Model) for multimodal remote sensing image classification. Based on the selective state space model, a cross-modal diff-attention module (CMDA-Module) is designed to extract and separate the common features and their respective dominant features of MS and PAN images. Among this, space preserving visual mamba (SPVM) retains image spatial features and captures local features by optimizing visual mamba's input reasonably. Considering that features in the fusion stage will have large semantic differences after feature separation and simple fusion operations struggle to effectively integrate these significantly different features, an attention-aware linear fusion module (AALF-Module) is proposed. It performs pixel-wise linear fusion by calculating influence coefficients. This mechanism can fuse features with large semantic differences while keeping the feature size unchanged. Empirical evaluations indicate that the presented method achieves better results than alternative approaches. The relevant code can be found at:https://github.com/AVKSKVL/DAS-F-Model","authors":["Wenping Ma","Boyou Xue","Mengru Ma","Chuang Chen","Hekai Zhang","Hao Zhu"],"url":"https://arxiv.org/abs/2504.16665"}
{"created":"2025-04-24","title":"Representation Learning via Non-Contrastive Mutual Information","abstract":"Labeling data is often very time consuming and expensive, leaving us with a majority of unlabeled data. Self-supervised representation learning methods such as SimCLR (Chen et al., 2020) or BYOL (Grill et al., 2020) have been very successful at learning meaningful latent representations from unlabeled image data, resulting in much more general and transferable representations for downstream tasks. Broadly, self-supervised methods fall into two types: 1) Contrastive methods, such as SimCLR; and 2) Non-Contrastive methods, such as BYOL. Contrastive methods are generally trying to maximize mutual information between related data points, so they need to compare every data point to every other data point, resulting in high variance, and thus requiring large batch sizes to work well. Non-contrastive methods like BYOL have much lower variance as they do not need to make pairwise comparisons, but are much trickier to implement as they have the possibility of collapsing to a constant vector. In this paper, we aim to develop a self-supervised objective that combines the strength of both types. We start with a particular contrastive method called the Spectral Contrastive Loss (HaoChen et al., 2021; Lu et al., 2024), and we convert it into a more general non-contrastive form; this removes the pairwise comparisons resulting in lower variance, but keeps the mutual information formulation of the contrastive method preventing collapse. We call our new objective the Mutual Information Non-Contrastive (MINC) loss. We test MINC by learning image representations on ImageNet (similar to SimCLR and BYOL) and show that it consistently improves upon the Spectral Contrastive loss baseline.","authors":["Zhaohan Daniel Guo","Bernardo Avila Pires","Khimya Khetarpal","Dale Schuurmans","Bo Dai"],"url":"https://arxiv.org/abs/2504.16667"}
{"created":"2025-04-24","title":"Efficient Data Valuation Approximation in Federated Learning: A Sampling-based Approach","abstract":"Federated learning paradigm to utilize datasets across multiple data providers. In FL, cross-silo data providers often hesitate to share their high-quality dataset unless their data value can be fairly assessed. Shapley value (SV) has been advocated as the standard metric for data valuation in FL due to its desirable properties. However, the computational overhead of SV is prohibitive in practice, as it inherently requires training and evaluating an FL model across an exponential number of dataset combinations. Furthermore, existing solutions fail to achieve high accuracy and efficiency, making practical use of SV still out of reach, because they ignore choosing suitable computation scheme for approximation framework and overlook the property of utility function in FL. We first propose a unified stratified-sampling framework for two widely-used schemes. Then, we analyze and choose the more promising scheme under the FL linear regression assumption. After that, we identify a phenomenon termed key combinations, where only limited dataset combinations have a high-impact on final data value. Building on these insights, we propose a practical approximation algorithm, IPSS, which strategically selects high-impact dataset combinations rather than evaluating all possible combinations, thus substantially reducing time cost with minor approximation error. Furthermore, we conduct extensive evaluations on the FL benchmark datasets to demonstrate that our proposed algorithm outperforms a series of representative baselines in terms of efficiency and effectiveness.","authors":["Shuyue Wei","Yongxin Tong","Zimu Zhou","Tianran He","Yi Xu"],"url":"https://arxiv.org/abs/2504.16668"}
{"created":"2025-04-24","title":"Open Source Software Lifecycle Classification: Developing Wrangling Techniques for Complex Sociotechnical Systems","abstract":"Open source software is a rapidly evolving center for distributed work, and understanding the characteristics of this work across its different contexts is vital for informing policy, economics, and the design of enabling software. The steep increase in open source projects and corporate participation have transformed a peripheral, cottage industry component of the global technology ecosystem into a large, infinitely complex \"technology parts supplier\" wired into every corner of contemporary life. The lack of theory and tools for breaking this complexity down into identifiable project types or strategies for understanding them more systematically is incommensurate with current industry, society, and developer needs. This paper reviews previous attempts to classify open source software and other organizational ecosystems, using open source scientific software ecosystems in contrast with those found in corporatized open source software. It then examines the divergent and sometimes conflicting purposes that may exist for classifying open source projects and how these competing interests impede our progress in developing a comprehensive understanding of how open source software projects and companies operate. Finally, we will present an empirical, mixed-methods study demonstrating how to classify open-source projects by their lifecycle position. This is the first step forward, advancing our scientific and practical knowledge of open source software through the lens of dynamic and evolving open source genres. It concludes with examples and a proposed path forward.","authors":["Wenyi Lu","Enock Kasaadah","S M Rakib Ul Karim","Matt Germonprez","Sean Goggins"],"url":"https://arxiv.org/abs/2504.16670"}
{"created":"2025-04-24","title":"LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative Analysis","abstract":"The use of large language models (LLMs) in qualitative analysis offers enhanced efficiency but raises questions about their alignment with the contextual nature of research for design (RfD). This research examines the trustworthiness of LLM-driven design insights, using qualitative coding as a case study to explore the interpretive processes central to RfD. We introduce LLMCode, an open-source tool integrating two metrics, namely Intersection over Union (IoU) and Modified Hausdorff Distance, to assess the alignment between human and LLM-generated insights. Across two studies involving 26 designers, we find that while the model performs well with deductive coding, its ability to emulate a designer's deeper interpretive lens over the data is limited, emphasising the importance of human-AI collaboration. Our results highlight a reciprocal dynamic where users refine LLM outputs and adapt their own perspectives based on the model's suggestions. These findings underscore the importance of fostering appropriate reliance on LLMs by designing tools that preserve interpretive depth while facilitating intuitive collaboration between designers and AI.","authors":["Joel Oksanen","Andr\\'es Lucero","Perttu H\\\"am\\\"al\\\"ainen"],"url":"https://arxiv.org/abs/2504.16671"}
{"created":"2025-04-24","title":"A Post-trainer's Guide to Multilingual Training Data: Uncovering Cross-lingual Transfer Dynamics","abstract":"In order for large language models to be useful across the globe, they are fine-tuned to follow instructions on multilingual data. Despite the ubiquity of such post-training, a clear understanding of the dynamics that enable cross-lingual transfer remains elusive. This study examines cross-lingual transfer (CLT) dynamics in realistic post-training settings. We study two model families of up to 35B parameters in size trained on carefully controlled mixtures of multilingual data on three generative tasks with varying levels of complexity (summarization, instruction following, and mathematical reasoning) in both single-task and multi-task instruction tuning settings. Overall, we find that the dynamics of cross-lingual transfer and multilingual performance cannot be explained by isolated variables, varying depending on the combination of post-training settings. Finally, we identify the conditions that lead to effective cross-lingual transfer in practice.","authors":["Luisa Shimabucoro","Ahmet Ustun","Marzieh Fadaee","Sebastian Ruder"],"url":"https://arxiv.org/abs/2504.16677"}
{"created":"2025-04-24","title":"Offline Robotic World Model: Learning Robotic Policies without a Physics Simulator","abstract":"Reinforcement Learning (RL) has demonstrated impressive capabilities in robotic control but remains challenging due to high sample complexity, safety concerns, and the sim-to-real gap. While offline RL eliminates the need for risky real-world exploration by learning from pre-collected data, it suffers from distributional shift, limiting policy generalization. Model-Based RL (MBRL) addresses this by leveraging predictive models for synthetic rollouts, yet existing approaches often lack robust uncertainty estimation, leading to compounding errors in offline settings. We introduce Offline Robotic World Model (RWM-O), a model-based approach that explicitly estimates epistemic uncertainty to improve policy learning without reliance on a physics simulator. By integrating these uncertainty estimates into policy optimization, our approach penalizes unreliable transitions, reducing overfitting to model errors and enhancing stability. Experimental results show that RWM-O improves generalization and safety, enabling policy learning purely from real-world data and advancing scalable, data-efficient RL for robotics.","authors":["Chenhao Li","Andreas Krause","Marco Hutter"],"url":"https://arxiv.org/abs/2504.16680"}
{"created":"2025-04-24","title":"Provable wavelet-based neural approximation","abstract":"In this paper, we develop a wavelet-based theoretical framework for analyzing the universal approximation capabilities of neural networks over a wide range of activation functions. Leveraging wavelet frame theory on the spaces of homogeneous type, we derive sufficient conditions on activation functions to ensure that the associated neural network approximates any functions in the given space, along with an error estimate. These sufficient conditions accommodate a variety of smooth activation functions, including those that exhibit oscillatory behavior. Furthermore, by considering the $L^2$-distance between smooth and non-smooth activation functions, we establish a generalized approximation result that is applicable to non-smooth activations, with the error explicitly controlled by this distance. This provides increased flexibility in the design of network architectures.","authors":["Youngmi Hur","Hyojae Lim","Mikyoung Lim"],"url":"https://arxiv.org/abs/2504.16682"}
{"created":"2025-04-24","title":"MCMC for Bayesian estimation of Differential Privacy from Membership Inference Attacks","abstract":"We propose a new framework for Bayesian estimation of differential privacy, incorporating evidence from multiple membership inference attacks (MIA). Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC) algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior distribution of the privacy parameter (e.g., instead of just credible intervals). Critically, the proposed method does not assume that privacy auditing is performed with the most powerful attack on the worst-case (dataset, challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est jointly estimates the strengths of MIAs used and the privacy of the training algorithm, yielding a more cautious privacy analysis. We also present an economical way to generate measurements for the performance of an MIA that is to be used by the MCMC method to estimate privacy. We present the use of the methods with numerical examples with both artificial and real data.","authors":["Ceren Yildirim","Kamer Kaya","Sinan Yildirim","Erkay Savas"],"url":"https://arxiv.org/abs/2504.16683"}
{"created":"2025-04-24","title":"SemanticSugarBeets: A Multi-Task Framework and Dataset for Inspecting Harvest and Storage Characteristics of Sugar Beets","abstract":"While sugar beets are stored prior to processing, they lose sugar due to factors such as microorganisms present in adherent soil and excess vegetation. Their automated visual inspection promises to aide in quality assurance and thereby increase efficiency throughout the processing chain of sugar production. In this work, we present a novel high-quality annotated dataset and two-stage method for the detection, semantic segmentation and mass estimation of post-harvest and post-storage sugar beets in monocular RGB images. We conduct extensive ablation experiments for the detection of sugar beets and their fine-grained semantic segmentation regarding damages, rot, soil adhesion and excess vegetation. For these tasks, we evaluate multiple image sizes, model architectures and encoders, as well as the influence of environmental conditions. Our experiments show an mAP50-95 of 98.8 for sugar-beet detection and an mIoU of 64.0 for the best-performing segmentation model.","authors":["Gerardus Croonen","Andreas Trondl","Julia Simon","Daniel Steininger"],"url":"https://arxiv.org/abs/2504.16684"}
{"created":"2025-04-24","title":"A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation for 6G: MLR, ANOVA, and Residual Distribution Analysis","abstract":"Modeling path loss in indoor LoRaWAN technology deployments is inherently challenging due to structural obstructions, occupant density and activities, and fluctuating environmental conditions. This study proposes a two-stage approach to capture and analyze these complexities using an extensive dataset of 1,328,334 field measurements collected over six months in a single-floor office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First, we implement a multiple linear regression model that includes traditional propagation metrics (distance, structural walls) and an extension with proposed environmental variables (relative humidity, temperature, carbon dioxide, particulate matter, and barometric pressure). Using analysis of variance, we demonstrate that adding these environmental factors can reduce unexplained variance by 42.32 percent. Secondly, we examine residual distributions by fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy, Student's t, and Gaussian Mixture Models with one to five components. Our results show that a four-component Gaussian Mixture Model captures the residual heterogeneity of indoor signal propagation most accurately, significantly outperforming single-distribution approaches. Given the push toward ultra-reliable, context-aware communications in 6G networks, our analysis shows that environment-aware modeling can substantially improve LoRaWAN network design in dynamic indoor IoT deployments.","authors":["Nahshon Mokua","Obiri","Kristof","Van Laerhoven"],"url":"https://arxiv.org/abs/2504.16688"}
{"created":"2025-04-24","title":"Rethinking Vision Transformer for Large-Scale Fine-Grained Image Retrieval","abstract":"Large-scale fine-grained image retrieval (FGIR) aims to retrieve images belonging to the same subcategory as a given query by capturing subtle differences in a large-scale setting. Recently, Vision Transformers (ViT) have been employed in FGIR due to their powerful self-attention mechanism for modeling long-range dependencies. However, most Transformer-based methods focus primarily on leveraging self-attention to distinguish fine-grained details, while overlooking the high computational complexity and redundant dependencies inherent to these models, limiting their scalability and effectiveness in large-scale FGIR. In this paper, we propose an Efficient and Effective ViT-based framework, termed \\textbf{EET}, which integrates token pruning module with a discriminative transfer strategy to address these limitations. Specifically, we introduce a content-based token pruning scheme to enhance the efficiency of the vanilla ViT, progressively removing background or low-discriminative tokens at different stages by exploiting feature responses and self-attention mechanism. To ensure the resulting efficient ViT retains strong discriminative power, we further present a discriminative transfer strategy comprising both \\textit{discriminative knowledge transfer} and \\textit{discriminative region guidance}. Using a distillation paradigm, these components transfer knowledge from a larger ``teacher'' ViT to a more efficient ``student'' model, guiding the latter to focus on subtle yet crucial regions in a cost-free manner. Extensive experiments on two widely-used fine-grained datasets and four large-scale fine-grained datasets demonstrate the effectiveness of our method. Specifically, EET reduces the inference latency of ViT-Small by 42.7\\% and boosts the retrieval performance of 16-bit hash codes by 5.15\\% on the challenging NABirds dataset.","authors":["Xin Jiang","Hao Tang","Yonghua Pan","Zechao Li"],"url":"https://arxiv.org/abs/2504.16691"}
{"created":"2025-04-24","title":"Energy-Based Pseudo-Label Refining for Source-free Domain Adaptation","abstract":"Source-free domain adaptation (SFDA), which involves adapting models without access to source data, is both demanding and challenging. Existing SFDA techniques typically rely on pseudo-labels generated from confidence levels, leading to negative transfer due to significant noise. To tackle this problem, Energy-Based Pseudo-Label Refining (EBPR) is proposed for SFDA. Pseudo-labels are created for all sample clusters according to their energy scores. Global and class energy thresholds are computed to selectively filter pseudo-labels. Furthermore, a contrastive learning strategy is introduced to filter difficult samples, aligning them with their augmented versions to learn more discriminative features. Our method is validated on the Office-31, Office-Home, and VisDA-C datasets, consistently finding that our model outperformed state-of-the-art methods.","authors":["Xinru Meng","Han Sun","Jiamei Liu","Ningzhong Liu","Huiyu Zhou"],"url":"https://arxiv.org/abs/2504.16692"}
{"created":"2025-04-24","title":"PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation","abstract":"While non-prehensile manipulation (e.g., controlled pushing/poking) constitutes a foundational robotic skill, its learning remains challenging due to the high sensitivity to complex physical interactions involving friction and restitution. To achieve robust policy learning and generalization, we opt to learn a world model of the 3D rigid body dynamics involved in non-prehensile manipulations and use it for model-based reinforcement learning. We propose PIN-WM, a Physics-INformed World Model that enables efficient end-to-end identification of a 3D rigid body dynamical system from visual observations. Adopting differentiable physics simulation, PIN-WM can be learned with only few-shot and task-agnostic physical interaction trajectories. Further, PIN-WM is learned with observational loss induced by Gaussian Splatting without needing state estimation. To bridge Sim2Real gaps, we turn the learned PIN-WM into a group of Digital Cousins via physics-aware randomizations which perturb physics and rendering parameters to generate diverse and meaningful variations of the PIN-WM. Extensive evaluations on both simulation and real-world tests demonstrate that PIN-WM, enhanced with physics-aware digital cousins, facilitates learning robust non-prehensile manipulation skills with Sim2Real transfer, surpassing the Real2Sim2Real state-of-the-arts.","authors":["Wenxuan Li","Hang Zhao","Zhiyuan Yu","Yu Du","Qin Zou","Ruizhen Hu","Kai Xu"],"url":"https://arxiv.org/abs/2504.16693"}
{"created":"2025-04-24","title":"CAIBA: Multicast Source Authentication for CAN Through Reactive Bit Flipping","abstract":"Controller Area Networks (CANs) are the backbone for reliable intra-vehicular communication. Recent cyberattacks have, however, exposed the weaknesses of CAN, which was designed without any security considerations in the 1980s. Current efforts to retrofit security via intrusion detection or message authentication codes are insufficient to fully secure CAN as they cannot adequately protect against masquerading attacks, where a compromised communication device, a so-called electronic control units, imitates another device. To remedy this situation, multicast source authentication is required to reliably identify the senders of messages. In this paper, we present CAIBA, a novel multicast source authentication scheme specifically designed for communication buses like CAN. CAIBA relies on an authenticator overwriting authentication tags on-the-fly, such that a receiver only reads a valid tag if not only the integrity of a message but also its source can be verified. To integrate CAIBA into CAN, we devise a special message authentication scheme and a reactive bit overwriting mechanism. We achieve interoperability with legacy CAN devices, while protecting receivers implementing the AUTOSAR SecOC standard against masquerading attacks without communication overhead or verification delays.","authors":["Eric Wagner","Frederik Basels","Jan Bauer","Till Zimmermann","Klaus Wehrle","Martin Henze"],"url":"https://arxiv.org/abs/2504.16695"}
{"created":"2025-04-24","title":"Decidability Problems for Micro-Stipula","abstract":"Micro-Stipula is a stateful calculus in which clauses can be activated either through interactions with the external environment or by the evaluation of time expressions. Despite the apparent simplicity of its syntax and operational model, the combination of state evolution, time reasoning, and nondeterminism gives rise to significant analytical challenges. In particular, we show that determining whether a clause is never executed is undecidable. We formally prove that this undecidability result holds even for syntactically restricted fragments: namely, the time-ahead fragment, where all time expressions are strictly positive, and the instantaneous fragment, where all time expressions evaluate to zero. On the other hand, we identify a decidable subfragment: within the instantaneous fragment, reachability becomes decidable when the initial states of functions and events are disjoint.","authors":["Giorgio Delzanno","Cosimo Laneve","Arnaud Sangnier","Gianluigi Zavattaro"],"url":"https://arxiv.org/abs/2504.16703"}
{"created":"2025-04-24","title":"Sorting as Gradient Flow on the Permutohedron","abstract":"We investigate how sorting algorithms efficiently overcome the exponential size of the permutation space. Our main contribution is a new continuous-time formulation of sorting as a gradient flow on the permutohedron, yielding an independent proof of the classical $\\Omega(n \\log n)$ lower bound for comparison-based sorting. This formulation reveals how exponential contraction of disorder occurs under simple geometric dynamics. In support of this analysis, we present algebraic, combinatorial, and geometric perspectives, including decision-tree arguments and linear constraints on the permutohedron. The idea that efficient sorting arises from structure-guided logarithmic reduction offers a unifying lens for how comparisons tame exponential spaces. These observations connect to broader questions in theoretical computer science, such as whether the existence of structure can explain why certain computational problems permit efficient solutions.","authors":["Jonathan Landers"],"url":"https://arxiv.org/abs/2504.16706"}
{"created":"2025-04-24","title":"Density of rational languages under shift invariant measures","abstract":"We study density of rational languages under shift invariant probability measures on spaces of two-sided infinite words, which generalizes the classical notion of density studied in formal languages and automata theory. The density for a language is defined as the limit in average (if it exists) of the probability that a word of a given length belongs to the language. We establish the existence of densities for all rational languages under all shift invariant measures. We also give explicit formulas under certain conditions, in particular when the language is aperiodic. Our approach combines tools and ideas from semigroup theory and ergodic theory.","authors":["Val\\'erie Berth\\'e","Herman Goulet-Ouellet","Dominique Perrin"],"url":"https://arxiv.org/abs/2504.16708"}
{"created":"2025-04-24","title":"A Unified Retrieval Framework with Document Ranking and EDU Filtering for Multi-document Summarization","abstract":"In the field of multi-document summarization (MDS), transformer-based models have demonstrated remarkable success, yet they suffer an input length limitation. Current methods apply truncation after the retrieval process to fit the context length; however, they heavily depend on manually well-crafted queries, which are impractical to create for each document set for MDS. Additionally, these methods retrieve information at a coarse granularity, leading to the inclusion of irrelevant content. To address these issues, we propose a novel retrieval-based framework that integrates query selection and document ranking and shortening into a unified process. Our approach identifies the most salient elementary discourse units (EDUs) from input documents and utilizes them as latent queries. These queries guide the document ranking by calculating relevance scores. Instead of traditional truncation, our approach filters out irrelevant EDUs to fit the context length, ensuring that only critical information is preserved for summarization. We evaluate our framework on multiple MDS datasets, demonstrating consistent improvements in ROUGE metrics while confirming its scalability and flexibility across diverse model architectures. Additionally, we validate its effectiveness through an in-depth analysis, emphasizing its ability to dynamically select appropriate queries and accurately rank documents based on their relevance scores. These results demonstrate that our framework effectively addresses context-length constraints, establishing it as a robust and reliable solution for MDS.","authors":["Shiyin Tan","Jaeeon Park","Dongyuan Li","Renhe Jiang","Manabu Okumura"],"url":"https://arxiv.org/abs/2504.16711"}
{"created":"2025-04-24","title":"Mixing Data-Driven and Physics-Based Constitutive Models using Uncertainty-Driven Phase Fields","abstract":"There is a high interest in accelerating multiscale models using data-driven surrogate modeling techniques. Creating a large training dataset encompassing all relevant load scenarios is essential for a good surrogate, yet the computational cost of producing this data quickly becomes a limiting factor. Commonly, a pre-trained surrogate is used throughout the computational domain. Here, we introduce an alternative adaptive mixture approach that uses a fast probabilistic surrogate model as constitutive model when possible, but resorts back to the true high-fidelity model when necessary. The surrogate is thus not required to be accurate for every possible load condition, enabling a significant reduction in the data collection time. We achieve this by creating phases in the computational domain corresponding to the different models. These phases evolve using a phase-field model driven by the surrogate uncertainty. When the surrogate uncertainty becomes large, the phase-field model causes a local transition from the surrogate to the high-fidelity model, maintaining a highly accurate simulation. We discuss the requirements of this approach to achieve accurate and numerically stable results and compare the phase-field model to a purely local approach that does not enforce spatial smoothness for the phase mixing. Using a Gaussian Process surrogate for an elasto-plastic material, we demonstrate the potential of this mixture of models to accelerate multiscale simulations.","authors":["J. Storm","W. Sun","I. B. C. M. Rocha","F. P. van der Meer"],"url":"https://arxiv.org/abs/2504.16713"}
{"created":"2025-04-24","title":"From Theory to Practice: Engineering Approximation Algorithms for Dynamic Orientation","abstract":"Dynamic graph algorithms have seen significant theoretical advancements, but practical evaluations often lag behind. This work bridges the gap between theory and practice by engineering and empirically evaluating recently developed approximation algorithms for dynamically maintaining graph orientations. We comprehensively describe the underlying data structures, including efficient bucketing techniques and round-robin updates. Our implementation has a natural parameter $\\lambda$, which allows for a trade-off between algorithmic efficiency and the quality of the solution. In the extensive experimental evaluation, we demonstrate that our implementation offers a considerable speedup. Using different quality metrics, we show that our implementations are very competitive and can outperform previous methods. Overall, our approach solves more instances than other methods while being up to 112 times faster on instances that are solvable by all methods compared.","authors":["Ernestine Gro{\\ss}mann","Ivor van der Hoog","Henrik Reinst\\\"adtler","Eva Rotenberg","Christian Schulz","Juliette Vlieghe"],"url":"https://arxiv.org/abs/2504.16720"}
{"created":"2025-04-24","title":"PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning","abstract":"In computer animation, game design, and human-computer interaction, synthesizing human motion that aligns with user intent remains a significant challenge. Existing methods have notable limitations: textual approaches offer high-level semantic guidance but struggle to describe complex actions accurately; trajectory-based techniques provide intuitive global motion direction yet often fall short in generating precise or customized character movements; and anchor poses-guided methods are typically confined to synthesize only simple motion patterns. To generate more controllable and precise human motions, we propose \\textbf{ProMoGen (Progressive Motion Generation)}, a novel framework that integrates trajectory guidance with sparse anchor motion control. Global trajectories ensure consistency in spatial direction and displacement, while sparse anchor motions only deliver precise action guidance without displacement. This decoupling enables independent refinement of both aspects, resulting in a more controllable, high-fidelity, and sophisticated motion synthesis. ProMoGen supports both dual and single control paradigms within a unified training process. Moreover, we recognize that direct learning from sparse motions is inherently unstable, we introduce \\textbf{SAP-CL (Sparse Anchor Posture Curriculum Learning)}, a curriculum learning strategy that progressively adjusts the number of anchors used for guidance, thereby enabling more precise and stable convergence. Extensive experiments demonstrate that ProMoGen excels in synthesizing vivid and diverse motions guided by predefined trajectory and arbitrary anchor frames. Our approach seamlessly integrates personalized motion with structured guidance, significantly outperforming state-of-the-art methods across multiple control scenarios.","authors":["Yingjie Xi","Jian Jun Zhang","Xiaosong Yang"],"url":"https://arxiv.org/abs/2504.16722"}
{"created":"2025-04-24","title":"Detecting and Understanding Hateful Contents in Memes Through Captioning and Visual Question-Answering","abstract":"Memes are widely used for humor and cultural commentary, but they are increasingly exploited to spread hateful content. Due to their multimodal nature, hateful memes often evade traditional text-only or image-only detection systems, particularly when they employ subtle or coded references. To address these challenges, we propose a multimodal hate detection framework that integrates key components: OCR to extract embedded text, captioning to describe visual content neutrally, sub-label classification for granular categorization of hateful content, RAG for contextually relevant retrieval, and VQA for iterative analysis of symbolic and contextual cues. This enables the framework to uncover latent signals that simpler pipelines fail to detect. Experimental results on the Facebook Hateful Memes dataset reveal that the proposed framework exceeds the performance of unimodal and conventional multimodal models in both accuracy and AUC-ROC.","authors":["Ali Anaissi","Junaid Akram","Kunal Chaturvedi","Ali Braytee"],"url":"https://arxiv.org/abs/2504.16723"}
{"created":"2025-04-24","title":"Partial orders and contraction for BISO channels","abstract":"A fundamental question in information theory is to quantify the loss of information under a noisy channel. Partial orders and contraction coefficients are typical tools to that end, however, they are often also challenging to evaluate. For the special class of binary input symmetric output (BISO) channels, Geng et al. showed that among channels with the same capacity, the binary symmetric channel (BSC) and binary erasure channel (BEC) are extremal with respect to the more capable order. Here, we show two main results. First, for channels with the same KL contraction coefficient, the same holds with respect to the less noisy order. Second, for channels with the same Dobrushin coefficient, or equiv. maximum leakage or Doeblin coefficient, the same holds with respect to the degradability order. In the process, we provide a closed-form expression for the contraction coefficients of BISO channels. We also discuss the comparability of BISO channels and extensions to binary channels in general.","authors":["Christoph Hirche","Oxana Shaya"],"url":"https://arxiv.org/abs/2504.16726"}
{"created":"2025-04-24","title":"V$^2$R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations","abstract":"Large Vision Language Models (LVLMs) excel in various vision-language tasks. Yet, their robustness to visual variations in position, scale, orientation, and context that objects in natural scenes inevitably exhibit due to changes in viewpoint and environment remains largely underexplored. To bridge this gap, we introduce V$^2$R-Bench, a comprehensive benchmark framework for evaluating Visual Variation Robustness of LVLMs, which encompasses automated evaluation dataset generation and principled metrics for thorough robustness assessment. Through extensive evaluation on 21 LVLMs, we reveal a surprising vulnerability to visual variations, in which even advanced models that excel at complex vision-language tasks significantly underperform on simple tasks such as object recognition. Interestingly, these models exhibit a distinct visual position bias that contradicts theories of effective receptive fields, and demonstrate a human-like visual acuity threshold. To identify the source of these vulnerabilities, we present a systematic framework for component-level analysis, featuring a novel visualization approach for aligned visual features. Results show that these vulnerabilities stem from error accumulation in the pipeline architecture and inadequate multimodal alignment. Complementary experiments with synthetic data further demonstrate that these limitations are fundamentally architectural deficiencies, scoring the need for architectural innovations in future LVLM designs.","authors":["Zhiyuan Fan (May)","Yumeng Wang (May)","Sandeep Polisetty (May)","Yi R. (May)","Fung"],"url":"https://arxiv.org/abs/2504.16727"}
{"created":"2025-04-24","title":"IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery","abstract":"The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System","authors":["Aniketh Garikaparthi","Manasi Patwardhan","Lovekesh Vig","Arman Cohan"],"url":"https://arxiv.org/abs/2504.16728"}
{"created":"2025-04-24","title":"MEC Task Offloading in AIoT: A User-Centric DRL Model Splitting Inference Scheme","abstract":"With the rapid development of the Artificial Intelligence of Things (AIoT), mobile edge computing (MEC) becomes an essential technology underpinning AIoT applications. However, multi-angle resource constraints, multi-user task competition, and the complexity of task offloading decisions in dynamic MEC environments present new technical challenges. Therefore, a user-centric deep reinforcement learning (DRL) model splitting inference scheme is proposed to address the problem. This scheme combines model splitting inference technology and designs a UCMS_MADDPG-based offloading algorithm to realize efficient model splitting inference responses in the dynamic MEC environment with multi-angle resource constraints. Specifically, we formulate a joint optimization problem that integrates resource allocation, server selection, and task offloading, aiming to minimize the weighted sum of task execution delay and energy consumption. We also introduce a user-server co-selection algorithm to address the selection issue between users and servers. Furthermore, we design an algorithm centered on user pre-decision to coordinate the outputs of continuous and discrete hybrid decisions, and introduce a priority sampling mechanism based on reward-error trade-off to optimize the experience replay mechanism of the network. Simulation results show that the proposed UCMS_MADDPG-based offloading algorithm demonstrates superior overall performance compared with other benchmark algorithms in dynamic environments.","authors":["Weixi Li","Rongzuo Guo","Yuning Wang","Fangying Chen"],"url":"https://arxiv.org/abs/2504.16729"}
{"created":"2025-04-24","title":"Simplified Swarm Learning Framework for Robust and Scalable Diagnostic Services in Cancer Histopathology","abstract":"The complexities of healthcare data, including privacy concerns, imbalanced datasets, and interoperability issues, necessitate innovative machine learning solutions. Swarm Learning (SL), a decentralized alternative to Federated Learning, offers privacy-preserving distributed training, but its reliance on blockchain technology hinders accessibility and scalability. This paper introduces a \\textit{Simplified Peer-to-Peer Swarm Learning (P2P-SL) Framework} tailored for resource-constrained environments. By eliminating blockchain dependencies and adopting lightweight peer-to-peer communication, the proposed framework ensures robust model synchronization while maintaining data privacy. Applied to cancer histopathology, the framework integrates optimized pre-trained models, such as TorchXRayVision, enhanced with DenseNet decoders, to improve diagnostic accuracy. Extensive experiments demonstrate the framework's efficacy in handling imbalanced and biased datasets, achieving comparable performance to centralized models while preserving privacy. This study paves the way for democratizing advanced machine learning in healthcare, offering a scalable, accessible, and efficient solution for privacy-sensitive diagnostic applications.","authors":["Yanjie Wu","Yuhao Ji","Saiho Lee","Juniad Akram","Ali Braytee","Ali Anaissi"],"url":"https://arxiv.org/abs/2504.16732"}
{"created":"2025-04-24","title":"DYNUS: Uncertainty-aware Trajectory Planner in Dynamic Unknown Environments","abstract":"This paper introduces DYNUS, an uncertainty-aware trajectory planner designed for dynamic unknown environments. Operating in such settings presents many challenges -- most notably, because the agent cannot predict the ground-truth future paths of obstacles, a previously planned trajectory can become unsafe at any moment, requiring rapid replanning to avoid collisions.","authors":["Kota Kondo","Mason Peterson","Nicholas Rober","Juan Rached Viso","Lucas Jia","Jialin Chen","Harvey Merton","Jonathan P. How"],"url":"https://arxiv.org/abs/2504.16734"}
{"created":"2025-04-24","title":"An Expressive Coalgebraic Modal Logic for Cellular Automata","abstract":"Cellular automata provide models of parallel computation based on cells, whose connectivity is given by an action of a monoid on the cells. At each step in the computation, every cell is decorated with a state that evolves in discrete steps according to a local update rule, which determines the next state of a cell based on its neighbour's states. In this paper, we consider a coalgebraic view on cellular automata, which does not require typical restrictions, such as uniform neighbourhood connectivity and uniform local rules. Using the coalgebraic view, we devise a behavioural equivalence for cellular automata and a modal logic to reason about their behaviour. We then prove a Hennessy-Milner style theorem, which states that pairs of cells satisfy the same modal formulas exactly if they are identified under cellular behavioural equivalence.","authors":["Henning Basold","Chase Ford","Lulof Pir\\'ee"],"url":"https://arxiv.org/abs/2504.16735"}
{"created":"2025-04-24","title":"A Survey of AI Agent Protocols","abstract":"The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide a systematic overview of existing communication protocols for LLM agents. We classify them into four main categories and make an analysis to help users and developers select the most suitable protocols for specific applications. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore future challenges, such as how protocols can adapt and survive in fast-evolving environments, and what qualities future protocols might need to support the next generation of LLM agent ecosystems. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.","authors":["Yingxuan Yang","Huacan Chai","Yuanyi Song","Siyuan Qi","Muning Wen","Ning Li","Junwei Liao","Haoyi Hu","Jianghao Lin","Gaowei Chang","Weiwen Liu","Ying Wen","Yong Yu","Weinan Zhang"],"url":"https://arxiv.org/abs/2504.16736"}
{"created":"2025-04-24","title":"MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning","abstract":"Planning long-horizon motions using a set of predefined skills is a key challenge in robotics and AI. Addressing this challenge requires methods that systematically explore skill combinations to uncover task-solving sequences, harness generic, easy-to-learn skills (e.g., pushing, grasping) to generalize across unseen tasks, and bypass reliance on symbolic world representations that demand extensive domain and task-specific knowledge. Despite significant progress, these elements remain largely disjoint in existing approaches, leaving a critical gap in achieving robust, scalable solutions for complex, long-horizon problems. In this work, we present MOSAIC, a skill-centric framework that unifies these elements by using the skills themselves to guide the planning process. MOSAIC uses two families of skills: Generators compute executable trajectories and world configurations, and Connectors link these independently generated skill trajectories by solving boundary value problems, enabling progress toward completing the overall task. By breaking away from the conventional paradigm of incrementally discovering skills from predefined start or goal states--a limitation that significantly restricts exploration--MOSAIC focuses planning efforts on regions where skills are inherently effective. We demonstrate the efficacy of MOSAIC in both simulated and real-world robotic manipulation tasks, showcasing its ability to solve complex long-horizon planning problems using a diverse set of skills incorporating generative diffusion models, motion planning algorithms, and manipulation-specific models. Visit https://skill-mosaic.github.io for demonstrations and examples.","authors":["Itamar Mishani","Yorai Shaoul","Maxim Likhachev"],"url":"https://arxiv.org/abs/2504.16738"}
{"created":"2025-04-24","title":"Prompt-Tuning SAM: From Generalist to Specialist with only 2048 Parameters and 16 Training Images","abstract":"The Segment Anything Model (SAM) is widely used for segmenting a diverse range of objects in natural images from simple user prompts like points or bounding boxes. However, SAM's performance decreases substantially when applied to non-natural domains like microscopic imaging. Furthermore, due to SAM's interactive design, it requires a precise prompt for each image and object, which is unfeasible in many automated biomedical applications. Previous solutions adapt SAM by training millions of parameters via fine-tuning large parts of the model or of adapter layers. In contrast, we show that as little as 2,048 additional parameters are sufficient for turning SAM into a use-case specialist for a certain downstream task. Our novel PTSAM (prompt-tuned SAM) method uses prompt-tuning, a parameter-efficient fine-tuning technique, to adapt SAM for a specific task. We validate the performance of our approach on multiple microscopic and one medical dataset. Our results show that prompt-tuning only SAM's mask decoder already leads to a performance on-par with state-of-the-art techniques while requiring roughly 2,000x less trainable parameters. For addressing domain gaps, we find that additionally prompt-tuning SAM's image encoder is beneficial, further improving segmentation accuracy by up to 18% over state-of-the-art results. Since PTSAM can be reliably trained with as little as 16 annotated images, we find it particularly helpful for applications with limited training data and domain shifts.","authors":["Tristan Piater","Bj\\\"orn Barz","Alexander Freytag"],"url":"https://arxiv.org/abs/2504.16739"}
{"created":"2025-04-24","title":"Gaussian Splatting is an Effective Data Generator for 3D Object Detection","abstract":"We investigate data augmentation for 3D object detection in autonomous driving. We utilize recent advancements in 3D reconstruction based on Gaussian Splatting for 3D object placement in driving scenes. Unlike existing diffusion-based methods that synthesize images conditioned on BEV layouts, our approach places 3D objects directly in the reconstructed 3D space with explicitly imposed geometric transformations. This ensures both the physical plausibility of object placement and highly accurate 3D pose and position annotations.","authors":["Farhad G. Zanjani","Davide Abati","Auke Wiggers","Dimitris Kalatzis","Jens Petersen","Hong Cai","Amirhossein Habibian"],"url":"https://arxiv.org/abs/2504.16740"}
{"created":"2025-04-24","title":"Search Timelines: Visualizing Search History to Enable Cross-Session Exploratory Search","abstract":"Purpose: The timespan over which exploratory searching can occur, as well as the scope and volume of the search activities undertaken, can make it difficult for searchers to remember key details about their search activities. These difficulties are present both in the midst of searching as well as when resuming a search that spans multiple sessions. In this paper, we present a search interface designed to support cross-session exploratory search in a public digital library context. Methods: Search Timelines provides a visualization of current and past search activities via a dynamic timeline of the search activity (queries and saved resources). This timeline is presented at two levels of detail. An overview timeline is provided alongside the search results in a typical search engine results page design. A detailed timeline is provided in the workspace, where searchers can review the history of their search activities and their saved resources. A controlled laboratory study was conducted to compare this approach to a baseline interface modelled after a typical public digital library search/workspace interface. Results: Participants who used Search Timelines reported higher levels of user engagement, usability, and perceived knowledge gain, during an initial search session and when resuming the search after a 7-8 day interval. This came at the expense of the searchers taking more time to complete the search task, which we view as positive evidence of engagement in cross-session exploratory search processes. Conclusion: Search Timelines serves as an example of how lightweight visualization approaches can be used to enhance typical search interface designs to support exploratory search. The results highlight the value of providing persistent representations of past search activities within the search interface.","authors":["Orland Hoeber","Md Nazmul Islam","Miriam Boon","Dale Storie","Veronica Ramshaw"],"url":"https://arxiv.org/abs/2504.16741"}
{"created":"2025-04-24","title":"Can Automated Feedback Turn Students into Happy Prologians?","abstract":"Giving personalized feedback to students is very important to the learning process. However, doing so in a timely manner can be difficult to accomplish in very large courses. Recent work has explored different types of automated feedback adapted to different languages and programming paradigms, particularly logic programming. In ProHelp, we implemented several of these types of feedback so that they could be used by students enrolled in a logic programming class. Then, we surveyed those students to find if the feedback was useful and which types of feedback they preferred. Results show that students found all types of feedback helpful, with automatic testing, in particular, being the most helpful type. We also explore student preferences for which types of feedback they would most like to see implemented in the future.","authors":["Ricardo Brancas","Pedro Orvalho","Carolina Carreira","Vasco Manquinho","Ruben Martins"],"url":"https://arxiv.org/abs/2504.16742"}
{"created":"2025-04-24","title":"Implementing AI Bill of Materials (AI BOM) with SPDX 3.0: A Comprehensive Guide to Creating AI and Dataset Bill of Materials","abstract":"A Software Bill of Materials (SBOM) is becoming an increasingly important tool in regulatory and technical spaces to introduce more transparency and security into a project's software supply chain.","authors":["Karen Bennet","Gopi Krishnan Rajbahadur","Arthit Suriyawongkul","Kate Stewart"],"url":"https://arxiv.org/abs/2504.16743"}
{"created":"2025-04-24","title":"Traffic-Oblivious Multi-Commodity Flow Network Design","abstract":"We consider the Minimum Multi-Commodity Flow Subgraph (MMCFS) problem: given a directed graph $G$ with edge capacities $\\mathit{cap}$ and a retention ratio $\\alpha\\in(0,1)$, find an edge-wise minimum subgraph $G' \\subseteq G$ such that for all traffic matrices $T$ routable in $G$ using a multi-commodity flow, $\\alpha\\cdot T$ is routable in $G'$. This natural yet novel problem is motivated by recent research that investigates how the power consumption in backbone computer networks can be reduced by turning off connections during times of low demand without compromising the quality of service. Since the actual traffic demands are generally not known beforehand, our approach must be traffic-oblivious, i.e., work for all possible sets of simultaneously routable traffic demands in the original network.","authors":["Markus Chimani","Max Ilsen"],"url":"https://arxiv.org/abs/2504.16744"}
{"created":"2025-04-24","title":"Simple Graph Contrastive Learning via Fractional-order Neural Diffusion Networks","abstract":"Graph Contrastive Learning (GCL) has recently made progress as an unsupervised graph representation learning paradigm. GCL approaches can be categorized into augmentation-based and augmentation-free methods. The former relies on complex data augmentations, while the latter depends on encoders that can generate distinct views of the same input. Both approaches may require negative samples for training. In this paper, we introduce a novel augmentation-free GCL framework based on graph neural diffusion models. Specifically, we utilize learnable encoders governed by Fractional Differential Equations (FDE). Each FDE is characterized by an order parameter of the differential operator. We demonstrate that varying these parameters allows us to produce learnable encoders that generate diverse views, capturing either local or global information, for contrastive learning. Our model does not require negative samples for training and is applicable to both homophilic and heterophilic datasets. We demonstrate its effectiveness across various datasets, achieving state-of-the-art performance.","authors":["Yanan Zhao","Feng Ji","Kai Zhao","Xuhao Li","Qiyu Kang","Wenfei Liang","Yahya Alkhatib","Xingchao Jian","Wee Peng Tay"],"url":"https://arxiv.org/abs/2504.16748"}
{"created":"2025-04-24","title":"Feature Mixing Approach for Detecting Intraoperative Adverse Events in Laparoscopic Roux-en-Y Gastric Bypass Surgery","abstract":"Intraoperative adverse events (IAEs), such as bleeding or thermal injury, can lead to severe postoperative complications if undetected. However, their rarity results in highly imbalanced datasets, posing challenges for AI-based detection and severity quantification. We propose BetaMixer, a novel deep learning model that addresses these challenges through a Beta distribution-based mixing approach, converting discrete IAE severity scores into continuous values for precise severity regression (0-5 scale). BetaMixer employs Beta distribution-based sampling to enhance underrepresented classes and regularizes intermediate embeddings to maintain a structured feature space. A generative approach aligns the feature space with sampled IAE severity, enabling robust classification and severity regression via a transformer. Evaluated on the MultiBypass140 dataset, which we extended with IAE labels, BetaMixer achieves a weighted F1 score of 0.76, recall of 0.81, PPV of 0.73, and NPV of 0.84, demonstrating strong performance on imbalanced data. By integrating Beta distribution-based sampling, feature mixing, and generative modeling, BetaMixer offers a robust solution for IAE detection and quantification in clinical settings.","authors":["Rupak Bose","Chinedu Innocent Nwoye","Jorge Lazo","Jo\\\"el Lukas Lavanchy","Nicolas Padoy"],"url":"https://arxiv.org/abs/2504.16749"}
{"created":"2025-04-24","title":"Adversarial Knapsack for Sequential Competitive Resource Allocation","abstract":"This work addresses competitive resource allocation in a sequential setting, where two players allocate resources across objects or locations of shared interest. Departing from the simultaneous Colonel Blotto game, our framework introduces a sequential decision-making dynamic, where players act with partial or complete knowledge of previous moves. Unlike traditional approaches that rely on complex mixed strategies, we focus on deterministic pure strategies, streamlining computation while preserving strategic depth. Additionally, we extend the payoff structure to accommodate fractional allocations and payoffs, moving beyond the binary, all-or-nothing paradigm to allow more granular outcomes. We model this problem as an adversarial knapsack game, formulating it as a bilevel optimization problem that integrates the leader's objective with the follower's best-response. This knapsack-based approach is novel in the context of competitive resource allocation, with prior work only partially leveraging it for follower analysis. Our contributions include: (1) proposing an adversarial knapsack formulation for the sequential resource allocation problem, (2) developing efficient heuristics for fractional allocation scenarios, and (3) analyzing the 0-1 knapsack case, providing a computational hardness result alongside a heuristic solution.","authors":["Omkar Thakoor","Rajgopal Kannan","Victor Prasanna"],"url":"https://arxiv.org/abs/2504.16752"}
{"created":"2025-04-24","title":"ViMoTest: A Tool to Specify ViewModel-Based GUI Test Scenarios using Projectional Editing","abstract":"Automated GUI testing is crucial in ensuring that presentation logic behaves as expected. However, existing tools often apply end-to-end approaches and face challenges such as high specification efforts, maintenance difficulties, and flaky tests while coupling to GUI framework specifics. To address these challenges, we introduce the ViMoTest tool, which leverages Behavior-driven Development, the ViewModel architectural pattern, and projectional Domain-specific Languages (DSLs) to isolate and test presentation logic independently of GUI frameworks. We demonstrate the tool with a small JavaFX-based task manager example and generate executable code.","authors":["Mario Fuksa","Sandro Speth","Steffen Becker"],"url":"https://arxiv.org/abs/2504.16753"}
{"created":"2025-04-24","title":"HEMA : A Hippocampus-Inspired Extended Memory Architecture for Long-Context AI Conversations","abstract":"Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.","authors":["Kwangseob Ahn"],"url":"https://arxiv.org/abs/2504.16754"}
{"created":"2025-04-24","title":"QAOA-PCA: Enhancing Efficiency in the Quantum Approximate Optimization Algorithm via Principal Component Analysis","abstract":"The Quantum Approximate Optimization Algorithm (QAOA) is a promising variational algorithm for solving combinatorial optimization problems on near-term devices. However, as the number of layers in a QAOA circuit increases, which is correlated with the quality of the solution, the number of parameters to optimize grows linearly. This results in more iterations required by the classical optimizer, which results in an increasing computational burden as more circuit executions are needed. To mitigate this issue, we introduce QAOA-PCA, a novel reparameterization technique that employs Principal Component Analysis (PCA) to reduce the dimensionality of the QAOA parameter space. By extracting principal components from optimized parameters of smaller problem instances, QAOA-PCA facilitates efficient optimization with fewer parameters on larger instances. Our empirical evaluation on the prominent MaxCut problem demonstrates that QAOA-PCA consistently requires fewer iterations than standard QAOA, achieving substantial efficiency gains. While this comes at the cost of a slight reduction in approximation ratio compared to QAOA with the same number of layers, QAOA-PCA almost always outperforms standard QAOA when matched by parameter count. QAOA-PCA strikes a favorable balance between efficiency and performance, reducing optimization overhead without significantly compromising solution quality.","authors":["Owain Parry","Phil McMinn"],"url":"https://arxiv.org/abs/2504.16755"}
{"created":"2025-04-24","title":"The root-exponential convergence of lightning plus polynomial approximation on corner domains (II)","abstract":"This paper builds rigorous analysis on the root-exponential convergence for the lightning schemes via rational functions in approximating corner singularity problems with uniform exponentially clustered poles proposed by Gopal and Trefethen. The start point is to set up the representations of $z^\\alpha$ and $z^\\alpha\\log z$ in the slit disk and develop results akin to Paley-Wiener theorem, from which, together with the Poisson summation formula, the root-exponential convergence of the lightning plus polynomial scheme with an exact order for each clustered parameter is established in approximation of prototype functions $g(z)z^\\alpha$ or $g(z)z^\\alpha\\log z$ on a sector-shaped domain, which includes $[0,1]$ as a special case. In addition, the fastest convergence rate is confirmed based upon the best choice of the clustered parameter. Furthermore, the optimal choice of the clustered parameter and the convergence rate for corner singularity problems in solving Laplace equations are attested based on Lehman and Wasow's study of corner singularities and along with the decomposition of Gopal and Trefethen. The thorough analysis provides a solid foundation for lightning schemes and rational approximation. Ample numerical evidences demonstrate the optimality and sharpness of the estimates.","authors":["Shuhuang Xiang","Shunfeng Yang","Yanghao Wu"],"url":"https://arxiv.org/abs/2504.16756"}
{"created":"2025-04-24","title":"Lightweight Latent Verifiers for Efficient Meta-Generation Strategies","abstract":"Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications.","authors":["Bartosz Piotrowski","Witold Drzewakowski","Konrad Staniszewski","Piotr Mi{\\l}o\\'s"],"url":"https://arxiv.org/abs/2504.16760"}
{"created":"2025-04-24","title":"Tri-FusionNet: Enhancing Image Description Generation with Transformer-based Fusion Network and Dual Attention Mechanism","abstract":"Image description generation is essential for accessibility and AI understanding of visual content. Recent advancements in deep learning have significantly improved natural language processing and computer vision. In this work, we propose Tri-FusionNet, a novel image description generation model that integrates transformer modules: a Vision Transformer (ViT) encoder module with dual-attention mechanism, a Robustly Optimized BERT Approach (RoBERTa) decoder module, and a Contrastive Language-Image Pre-Training (CLIP) integrating module. The ViT encoder, enhanced with dual attention, focuses on relevant spatial regions and linguistic context, improving image feature extraction. The RoBERTa decoder is employed to generate precise textual descriptions. CLIP's integrating module aligns visual and textual data through contrastive learning, ensuring effective combination of both modalities. This fusion of ViT, RoBERTa, and CLIP, along with dual attention, enables the model to produce more accurate, contextually rich, and flexible descriptions. The proposed framework demonstrated competitive performance on the Flickr30k and Flickr8k datasets, with BLEU scores ranging from 0.767 to 0.456 and 0.784 to 0.479, CIDEr scores of 1.679 and 1.483, METEOR scores of 0.478 and 0.358, and ROUGE-L scores of 0.567 and 0.789, respectively. On MS-COCO, the framework obtained BLEU scores of 0.893 (B-1), 0.821 (B-2), 0.794 (B-3), and 0.725 (B-4). The results demonstrate the effectiveness of Tri-FusionNet in generating high-quality image descriptions.","authors":["Lakshita Agarwal","Bindu Verma"],"url":"https://arxiv.org/abs/2504.16761"}
{"created":"2025-04-24","title":"Drainability and Fillability of Polyominoes in Diverse Models of Global Control","abstract":"Tilt models offer intuitive and clean definitions of complex systems in which particles are influenced by global control commands. Despite a wide range of applications, there has been almost no theoretical investigation into the associated issues of filling and draining geometric environments. This is partly because a globally controlled system (i.e., passive matter) exhibits highly complex behavior that cannot be locally restricted. Thus, there is a strong need for theoretical studies that investigate these models both (1) in terms of relative power to each other, and (2) from a complexity theory perspective. In this work, we provide (1) general tools for comparing and contrasting different models of global control, and (2) both complexity and algorithmic results on filling and draining.","authors":["S\\'andor P. Fekete","Peter Kramer","Jan-Marc Reinhardt","Christian Rieck","Christian Scheffer"],"url":"https://arxiv.org/abs/2504.16762"}
{"created":"2025-04-24","title":"Noise-Tolerant Coreset-Based Class Incremental Continual Learning","abstract":"Many applications of computer vision require the ability to adapt to novel data distributions after deployment. Adaptation requires algorithms capable of continual learning (CL). Continual learners must be plastic to adapt to novel tasks while minimizing forgetting of previous tasks.However, CL opens up avenues for noise to enter the training pipeline and disrupt the CL. This work focuses on label noise and instance noise in the context of class-incremental learning (CIL), where new classes are added to a classifier over time, and there is no access to external data from past classes. We aim to understand the sensitivity of CL methods that work by replaying items from a memory constructed using the idea of Coresets. We derive a new bound for the robustness of such a method to uncorrelated instance noise under a general additive noise threat model, revealing several insights. Putting the theory into practice, we create two continual learning algorithms to construct noise-tolerant replay buffers. We empirically compare the effectiveness of prior memory-based continual learners and the proposed algorithms under label and uncorrelated instance noise on five diverse datasets. We show that existing memory-based CL are not robust whereas the proposed methods exhibit significant improvements in maximizing classification accuracy and minimizing forgetting in the noisy CIL setting.","authors":["Edison Mucllari","Aswin Raghavan","Zachary Alan Daniels"],"url":"https://arxiv.org/abs/2504.16763"}
{"created":"2025-04-24","title":"Online model learning with data-assimilated reservoir computers","abstract":"We propose an online learning framework for forecasting nonlinear spatio-temporal signals (fields). The method integrates (i) dimensionality reduction, here, a simple proper orthogonal decomposition (POD) projection; (ii) a generalized autoregressive model to forecast reduced dynamics, here, a reservoir computer; (iii) online adaptation to update the reservoir computer (the model), here, ensemble sequential data assimilation.We demonstrate the framework on a wake past a cylinder governed by the Navier-Stokes equations, exploring the assimilation of full flow fields (projected onto POD modes) and sparse sensors. Three scenarios are examined: a na\\\"ive physical state estimation; a two-fold estimation of physical and reservoir states; and a three-fold estimation that also adjusts the model parameters. The two-fold strategy significantly improves ensemble convergence and reduces reconstruction error compared to the na\\\"ive approach. The three-fold approach enables robust online training of partially-trained reservoir computers, overcoming limitations of a priori training. By unifying data-driven reduced order modelling with Bayesian data assimilation, this work opens new opportunities for scalable online model learning for nonlinear time series forecasting.","authors":["Andrea N\\'ovoa","Luca Magri"],"url":"https://arxiv.org/abs/2504.16767"}
{"created":"2025-04-24","title":"How Effective are Generative Large Language Models in Performing Requirements Classification?","abstract":"In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.","authors":["Waad Alhoshan","Alessio Ferrari","Liping Zhao"],"url":"https://arxiv.org/abs/2504.16768"}
{"created":"2025-04-24","title":"DeBiasMe: De-biasing Human-AI Interactions with Metacognitive AIED (AI in Education) Interventions","abstract":"While generative artificial intelligence (Gen AI) increasingly transforms academic environments, a critical gap exists in understanding and mitigating human biases in AI interactions, such as anchoring and confirmation bias. This position paper advocates for metacognitive AI literacy interventions to help university students critically engage with AI and address biases across the Human-AI interaction workflows. The paper presents the importance of considering (1) metacognitive support with deliberate friction focusing on human bias; (2) bi-directional Human-AI interaction intervention addressing both input formulation and output interpretation; and (3) adaptive scaffolding that responds to diverse user engagement patterns. These frameworks are illustrated through ongoing work on \"DeBiasMe,\" AIED (AI in Education) interventions designed to enhance awareness of cognitive biases while empowering user agency in AI interactions. The paper invites multiple stakeholders to engage in discussions on design and evaluation methods for scaffolding mechanisms, bias visualization, and analysis frameworks. This position contributes to the emerging field of AI-augmented learning by emphasizing the critical role of metacognition in helping students navigate the complex interaction between human, statistical, and systemic biases in AI use while highlighting how cognitive adaptation to AI systems must be explicitly integrated into comprehensive AI literacy frameworks.","authors":["Chaeyeon Lim"],"url":"https://arxiv.org/abs/2504.16770"}
{"created":"2025-04-24","title":"IsaBIL: A Framework for Verifying (In)correctness of Binaries in Isabelle/HOL (Extended Version)","abstract":"This paper presents IsaBIL, a binary analysis framework in Isabelle/HOL that is based on the widely used Binary Analysis Platform (BAP). Specifically, in IsaBIL, we formalise BAP's intermediate language, called BIL and integrate it with Hoare logic (to enable proofs of correctness) as well as incorrectness logic (to enable proofs of incorrectness). IsaBIL inherits the full flexibility of BAP, allowing us to verify binaries for a wide range of languages (C, C++, Rust), toolchains (LLVM, Ghidra) and target architectures (x86, RISC-V), and can also be used when the source code for a binary is unavailable.","authors":["Matt Griffin","Brijesh Dongol","Azalea Raad"],"url":"https://arxiv.org/abs/2504.16775"}
{"created":"2025-04-24","title":"Systemic Flakiness: An Empirical Analysis of Co-Occurring Flaky Test Failures","abstract":"Flaky tests produce inconsistent outcomes without code changes, creating major challenges for software developers. An industrial case study reported that developers spend 1.28% of their time repairing flaky tests at a monthly cost of $2,250. We discovered that flaky tests often exist in clusters, with co-occurring failures that share the same root causes, which we call systemic flakiness. This suggests that developers can reduce repair costs by addressing shared root causes, enabling them to fix multiple flaky tests at once rather than tackling them individually. This study represents an inflection point by challenging the deep-seated assumption that flaky test failures are isolated occurrences. We used an established dataset of 10,000 test suite runs from 24 Java projects on GitHub, spanning domains from data orchestration to job scheduling. It contains 810 flaky tests, which we levered to perform a mixed-method empirical analysis of co-occurring flaky test failures. Systemic flakiness is significant and widespread. We performed agglomerative clustering of flaky tests based on their failure co-occurrence, finding that 75% of flaky tests across all projects belong to a cluster, with a mean cluster size of 13.5 flaky tests. Instead of requiring 10,000 test suite runs to identify systemic flakiness, we demonstrated a lightweight alternative by training machine learning models based on static test case distance measures. Through manual inspection of stack traces, conducted independently by four authors and resolved through negotiated agreement, we identified intermittent networking issues and instabilities in external dependencies as the predominant causes of systemic flakiness.","authors":["Owain Parry","Gregory Kapfhammer","Michael Hilton","Phil McMinn"],"url":"https://arxiv.org/abs/2504.16777"}
{"created":"2025-04-24","title":"Evaluation Framework for AI Systems in \"the Wild\"","abstract":"Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful.","authors":["Sarah Jabbour","Trenton Chang","Anindya Das Antar","Joseph Peper","Insu Jang","Jiachen Liu","Jae-Won Chung","Shiqi He","Michael Wellman","Bryan Goodman","Elizabeth Bondi-Kelly","Kevin Samy","Rada Mihalcea","Mosharaf Chowhury","David Jurgens","Lu Wang"],"url":"https://arxiv.org/abs/2504.16778"}
{"created":"2025-04-24","title":"Evaluating the Impact of a Yoga-Based Intervention on Software Engineers' Well-Being","abstract":"Software engineering tasks are high-stress and cognitively demanding. Additionally, there is a latent risk of software engineers presenting burnout, depression and anxiety. Established interventions in other fields centred around attention awareness have shown positive results in mental well-being.","authors":["Cristina Martinez Montes","Birgit Penzenstadler"],"url":"https://arxiv.org/abs/2504.16779"}
{"created":"2025-04-24","title":"Graph2Nav: 3D Object-Relation Graph Generation to Robot Navigation","abstract":"We propose Graph2Nav, a real-time 3D object-relation graph generation framework, for autonomous navigation in the real world. Our framework fully generates and exploits both 3D objects and a rich set of semantic relationships among objects in a 3D layered scene graph, which is applicable to both indoor and outdoor scenes. It learns to generate 3D semantic relations among objects, by leveraging and advancing state-of-the-art 2D panoptic scene graph works into the 3D world via 3D semantic mapping techniques. This approach avoids previous training data constraints in learning 3D scene graphs directly from 3D data. We conduct experiments to validate the accuracy in locating 3D objects and labeling object-relations in our 3D scene graphs. We also evaluate the impact of Graph2Nav via integration with SayNav, a state-of-the-art planner based on large language models, on an unmanned ground robot to object search tasks in real environments. Our results demonstrate that modeling object relations in our scene graphs improves search efficiency in these navigation tasks.","authors":["Tixiao Shan","Abhinav Rajvanshi","Niluthpol Mithun","Han-Pang Chiu"],"url":"https://arxiv.org/abs/2504.16782"}
{"created":"2025-04-24","title":"MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores","abstract":"Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.","authors":["Fengwei Zhou","Jiafei Song","Wenjin Jason Li","Gengjian Xue","Zhikang Zhao","Yichao Lu","Bailin Na"],"url":"https://arxiv.org/abs/2504.16786"}
{"created":"2025-04-24","title":"Credible plan-driven RAG method for Multi-hop Question Answering","abstract":"Multi-hop question answering (QA) presents a considerable challenge for Retrieval-Augmented Generation (RAG), requiring the structured decomposition of complex queries into logical reasoning paths and the generation of dependable intermediate results. However, deviations in reasoning paths or errors in intermediate results, which are common in current RAG methods, may propagate and accumulate throughout the reasoning process, diminishing the accuracy of the answer to complex queries. To address this challenge, we propose the Plan-then-Act-and-Review (PAR RAG) framework, which is organized into three key stages: planning, act, and review, and aims to offer an interpretable and incremental reasoning paradigm for accurate and reliable multi-hop question answering by mitigating error propagation.PAR RAG initially applies a top-down problem decomposition strategy, formulating a comprehensive plan that integrates multiple executable steps from a holistic viewpoint. This approach avoids the pitfalls of local optima common in traditional RAG methods, ensuring the accuracy of the entire reasoning path. Subsequently, PAR RAG incorporates a plan execution mechanism based on multi-granularity verification. By utilizing both coarse-grained similarity information and fine-grained relevant data, the framework thoroughly checks and adjusts intermediate results, ensuring process accuracy while effectively managing error propagation and amplification. Experimental results on multi-hop QA datasets demonstrate that the PAR RAG framework substantially outperforms existing state-of-the-art methods in key metrics, including EM and F1 scores.","authors":["Ningning Zhang","Chi Zhang","Zhizhong Tan","Xingxing Yang","Weiping Deng","Wenyong Wang"],"url":"https://arxiv.org/abs/2504.16787"}
{"created":"2025-04-24","title":"Towards Explainable AI: Multi-Modal Transformer for Video-based Image Description Generation","abstract":"Understanding and analyzing video actions are essential for producing insightful and contextualized descriptions, especially for video-based applications like intelligent monitoring and autonomous systems. The proposed work introduces a novel framework for generating natural language descriptions from video datasets by combining textual and visual modalities. The suggested architecture makes use of ResNet50 to extract visual features from video frames that are taken from the Microsoft Research Video Description Corpus (MSVD), and Berkeley DeepDrive eXplanation (BDD-X) datasets. The extracted visual characteristics are converted into patch embeddings and then run through an encoder-decoder model based on Generative Pre-trained Transformer-2 (GPT-2). In order to align textual and visual representations and guarantee high-quality description production, the system uses multi-head self-attention and cross-attention techniques. The model's efficacy is demonstrated by performance evaluation using BLEU (1-4), CIDEr, METEOR, and ROUGE-L. The suggested framework outperforms traditional methods with BLEU-4 scores of 0.755 (BDD-X) and 0.778 (MSVD), CIDEr scores of 1.235 (BDD-X) and 1.315 (MSVD), METEOR scores of 0.312 (BDD-X) and 0.329 (MSVD), and ROUGE-L scores of 0.782 (BDD-X) and 0.795 (MSVD). By producing human-like, contextually relevant descriptions, strengthening interpretability, and improving real-world applications, this research advances explainable AI.","authors":["Lakshita Agarwal","Bindu Verma"],"url":"https://arxiv.org/abs/2504.16788"}
{"created":"2025-04-24","title":"Preemption Aware Task Scheduling for Priority and Deadline Constrained DNN Inference Task Offloading in Homogeneous Mobile-Edge Networks","abstract":"This paper addresses the computational offloading of Deep Neural Networks (DNNs) to nearby devices with similar processing capabilities, to avoid the larger communication delays incurred for cloud offloading. We present a preemption aware scheduling approach for priority and deadline constrained task offloading in homogeneous edge networks. Our scheduling approach consists of two distinct scheduling algorithms, designed to accommodate the differing requirements of high and low priority tasks. To satisfy a task's deadline, our scheduling approach considers the availability of both communication and computational resources in the network when making placements in both the current time-slot and future time-slots. The scheduler implements a deadline-aware preemption mechanism to guarantee resource access to high priority tasks. When low-priority tasks are selected for preemption, the scheduler will attempt to reallocate them if possible before their deadline. We implement this scheduling approach into a task offloading system which we evaluate empirically in the real-world on a network of edge devices composed of four Raspberry Pi 2 Model B's. We evaluate this system under against a version without a task preemption mechanism as well as workstealing approaches to compare the impact on high priority task completion and the ability to complete overall frames. These solutions are evaluated under a workload of 1296 frames. Our findings show that our scheduling approach allows for 99\\% of high-priority tasks to complete while also providing a 3 - 8\\% increase in the number of frames fully classified end-to-end over both workstealing approaches and systems without a preemption mechanism.","authors":["Jamie Cotter","Ignacio Castineiras","Donna O'Shea","Victor Cionca"],"url":"https://arxiv.org/abs/2504.16792"}
{"created":"2025-04-24","title":"Random Long-Context Access for Mamba via Hardware-aligned Hierarchical Sparse Attention","abstract":"A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \\textbf{H}ierarchical \\textbf{S}parse \\textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.","authors":["Xiang Hu","Jiaqi Leng","Jun Zhao","Kewei Tu","Wei Wu"],"url":"https://arxiv.org/abs/2504.16795"}
{"created":"2025-04-24","title":"The extended adjoint state and nonlinearity in correlation-based passive imaging","abstract":"This articles investigates physics-based passive imaging problem, wherein one infers an unknown medium using ambient noise and correlation of the noise signal. We develop a general backpropagation framework via the so-called extended adjoint state, suitable for any linear PDE; crucially, this approach reduces by half the number of required PDE solves. Applications to several different PDE models demonstrate the universality of our method. In addition, we analyze the nonlinearity of the correlated model, revealing a surprising tangential cone condition-like structure, thereby advancing the state of the art towards a convergence guarantee for regularized reconstruction in passive imaging.","authors":["Tram Thi Ngoc Nguyen"],"url":"https://arxiv.org/abs/2504.16797"}
{"created":"2025-04-24","title":"4D Multimodal Co-attention Fusion Network with Latent Contrastive Alignment for Alzheimer's Diagnosis","abstract":"Multimodal neuroimaging provides complementary structural and functional insights into both human brain organization and disease-related dynamics. Recent studies demonstrate enhanced diagnostic sensitivity for Alzheimer's disease (AD) through synergistic integration of neuroimaging data (e.g., sMRI, fMRI) with behavioral cognitive scores tabular data biomarkers. However, the intrinsic heterogeneity across modalities (e.g., 4D spatiotemporal fMRI dynamics vs. 3D anatomical sMRI structure) presents critical challenges for discriminative feature fusion. To bridge this gap, we propose M2M-AlignNet: a geometry-aware multimodal co-attention network with latent alignment for early AD diagnosis using sMRI and fMRI. At the core of our approach is a multi-patch-to-multi-patch (M2M) contrastive loss function that quantifies and reduces representational discrepancies via geometry-weighted patch correspondence, explicitly aligning fMRI components across brain regions with their sMRI structural substrates without one-to-one constraints. Additionally, we propose a latent-as-query co-attention module to autonomously discover fusion patterns, circumventing modality prioritization biases while minimizing feature redundancy. We conduct extensive experiments to confirm the effectiveness of our method and highlight the correspondance between fMRI and sMRI as AD biomarkers.","authors":["Yuxiang Wei","Yanteng Zhang","Xi Xiao","Tianyang Wang","Xiao Wang","Vince D. Calhoun"],"url":"https://arxiv.org/abs/2504.16798"}
{"created":"2025-04-24","title":"Decoupled Global-Local Alignment for Improving Compositional Understanding","abstract":"Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA","authors":["Xiaoxing Hu","Kaicheng Yang","Jun Wang","Haoran Xu","Ziyong Feng","Yupei Wang"],"url":"https://arxiv.org/abs/2504.16801"}
{"created":"2025-04-24","title":"Graph modification of bounded size to minor-closed classes as fast as vertex deletion","abstract":"A replacement action is a function $\\mathcal{L}$ that maps each graph $H$ to a collection of graphs of size at most $|V(H)|$. Given a graph class $\\mathcal{H}$, we consider a general family of graph modification problems, called $\\mathcal{L}$-Replacement to $\\mathcal{H}$, where the input is a graph $G$ and the question is whether it is possible to replace some induced subgraph $H_1$ of $G$ on at most $k$ vertices by a graph $H_2$ in $\\mathcal{L}(H_1)$ so that the resulting graph belongs to $\\mathcal{H}$. $\\mathcal{L}$-Replacement to $\\mathcal{H}$ can simulate many graph modification problems including vertex deletion, edge deletion/addition/edition/contraction, vertex identification, subgraph complementation, independent set deletion, (induced) matching deletion/contraction, etc. We present two algorithms. The first one solves $\\mathcal{L}$-Replacement to $\\mathcal{H}$ in time $2^{{\\rm poly}(k)}\\cdot |V(G)|^2$ for every minor-closed graph class $\\mathcal{H}$, where {\\rm poly} is a polynomial whose degree depends on $\\mathcal{H}$, under a mild technical condition on $\\mathcal{L}$. This generalizes the results of Morelle, Sau, Stamoulis, and Thilikos [ICALP 2020, ICALP 2023] for the particular case of Vertex Deletion to $\\mathcal{H}$ within the same running time. Our second algorithm is an improvement of the first one when $\\mathcal{H}$ is the class of graphs embeddable in a surface of Euler genus at most $g$ and runs in time $2^{\\mathcal{O}(k^{9})}\\cdot |V(G)|^2$, where the $\\mathcal{O}(\\cdot)$ notation depends on $g$. To the best of our knowledge, these are the first parameterized algorithms with a reasonable parametric dependence for such a general family of graph modification problems to minor-closed classes.","authors":["Laure Morelle","Ignasi Sau","Dimitrios M. Thilikos"],"url":"https://arxiv.org/abs/2504.16803"}
{"created":"2025-04-24","title":"LLM-assisted Graph-RAG Information Extraction from IFC Data","abstract":"IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.","authors":["Sima Iranmanesh","Hadeel Saadany","Edlira Vakaj"],"url":"https://arxiv.org/abs/2504.16813"}
{"created":"2025-04-24","title":"Distributed Unknown Input Observers for Discrete-Time Linear Time-Invariant Systems","abstract":"This paper introduces a Distributed Unknown Input Observer (D-UIO) design methodology that uses a technique called node-wise detectability decomposition to estimate the state of a discrete-time linear time-invariant (LTI) system in a distributed way, even when there are noisy measurements and unknown inputs. In the considered scenario, sensors are associated to nodes of an underlying communication graph. Each node has a limited scope as it can only access local measurements and share data with its neighbors. The problem of designing the observer gains is divided into two separate sub-problems: (i) design local output injection gains to mitigate the impact of measurement noise, and (ii) design diffusive gains to compensate for the lack of information through a consensus protocol. A direct and computationally efficient synthesis strategy is formulated by linear matrix inequalities (LMIs) and solved via semidefinite programming. Finally, two simulative scenarios are presented to illustrate the effectiveness of the distributed observer when two different node-wise decompositions are adopted.","authors":["Franco Angelo Torchiaro","Gianfranco Gagliardi","Francesco Tedesco","Alessandro Casavola"],"url":"https://arxiv.org/abs/2504.16815"}
{"created":"2025-04-24","title":"Using games and universal trees to characterise the nondeterministic index of tree languages","abstract":"The parity index problem of tree automata asks, given a regular tree language $L$ and a set of priorities $J$, is $L$ $J$-feasible, that is, recognised by a nondeterministic parity automaton with priorities $J$? This is a long-standing open problem, of which only a few sub-cases and variations are known to be decidable. In a significant but technically difficult step, Colcombet and L\\\"oding reduced the problem to the uniform universality of distance-parity automata. In this article, we revisit the index problem using tools from the parity game literature.","authors":["Karoliina Lehtinen","Olivier Idir"],"url":"https://arxiv.org/abs/2504.16819"}
{"created":"2025-04-24","title":"Symmetric Proofs in the Ideal Proof System","abstract":"We consider the Ideal Proof System (IPS) introduced by Grochow and Pitassi and pose the question of which tautologies admit symmetric proofs, and of what complexity. The symmetry requirement in proofs is inspired by recent work establishing lower bounds in other symmetric models of computation. We link the existence of symmetric IPS proofs to the expressive power of logics such as fixed-point logic with counting and Choiceless Polynomial Time, specifically regarding the graph isomorphism problem. We identify relationships and tradeoffs between the symmetry of proofs and other parameters of IPS proofs such as size, degree and linearity. We study these on a number of standard families of tautologies from proof complexity and finite model theory such as the pigeonhole principle, the subset sum problem and the Cai-F\\\"urer-Immerman graphs, exhibiting non-trivial upper bounds on the size of symmetric IPS proofs.","authors":["Anuj Dawar","Erich Gr\\\"adel","Leon Kullmann","Benedikt Pago"],"url":"https://arxiv.org/abs/2504.16820"}
{"created":"2025-04-24","title":"From Diverse Origins to a DEI Crisis: The Pushback Against Equity, Diversity, and Inclusion in Software Engineering","abstract":"Background: Diversity, equity, and inclusion are rooted in the very origins of software engineering, shaped by the contributions from many individuals from underrepresented groups to the field. Yet today, DEI efforts in the industry face growing resistance. As companies retreat from visible commitments, and pushback initiatives started only a few years ago. Aims: This study explores how the DEI backlash is unfolding in the software industry by investigating institutional changes, lived experiences, and the strategies used to sustain DEI practices. Method: We conducted an exploratory case study using 59 publicly available Reddit posts authored by self-identified software professionals. Data were analyzed using reflexive thematic analysis. Results: Our findings show that software companies are responding to the DEI backlash in varied ways, including re-structuring programs, scaling back investments, or quietly continuing efforts under new labels. Professionals reported a wide range of emotional responses, from anxiety and frustration to relief and happiness, shaped by identity, role, and organizational culture. Yet, despite the backlash, multiple forms of resistance and adaptation have emerged to protect inclusive practices in software engineering. Conclusions: The DEI backlash is reshaping DEI in software engineering. While public messaging may soften or disappear, core DEI values persist in adapted forms. This study offers a new perspective into how inclusion is evolving under pressure and highlights the resilience of DEI in software environments.","authors":["Ronnie de Souza Santos","Ann Barcom","Mairieli Wessel","Cleyton Magalhaes"],"url":"https://arxiv.org/abs/2504.16821"}
{"created":"2025-04-24","title":"Energy Variational Modeling and Numerical Simulation of Open Membranes in Stokes Flow","abstract":"Lipid bilayer membranes are fundamental biological structures that serve as cellular boundaries, mediating transport, signaling, and maintaining structural integrity. This study introduces a novel mathematical model for open membranes immersed in Stokes flows, accounting for membrane elasticity, line tension at the open edge, and fluid-membrane interactions. The model is derived from an energy functional that incorporates Helfrich bending energy and a line energy associated with the open edge. By balancing dissipation in both the bulk fluid and the membrane surface, following the maximal dissipation principle, we derive the governing equations within an energy variational framework. Assuming axisymmetry and employing a boundary integral reduction, we transform the 3D problem into an effectively 1D problem, for which we develop a finite element-based numerical method to solve the resulting moving boundary problem. Several numerical examples are provided to validate the model and compare the results with existing studies.","authors":["Han Zhou","Yuan-Nan Young","Yoichiro Mori"],"url":"https://arxiv.org/abs/2504.16823"}
{"created":"2025-04-24","title":"Nurturing Language Proficiency in Spanish.speaking children Through digital competence","abstract":"This article explores into the intricate design and meticulous construction of a digital platform aimed at revolutionizing early-age English education, particularly for Spanish-speaking children. The focus of this work used an innovative methodologies, vibrant and engaging visuals, and a comprehensive approach to phonics. The principles of usability, accessibility, and user-centered design are intricately woven into every facet of the platform's architecture.","authors":["Rhayza Jolley Rangel"],"url":"https://arxiv.org/abs/2504.16824"}
{"created":"2025-04-24","title":"Process Reward Models That Think","abstract":"Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.","authors":["Muhammad Khalifa","Rishabh Agarwal","Lajanugen Logeswaran","Jaekyeom Kim","Hao Peng","Moontae Lee","Honglak Lee","Lu Wang"],"url":"https://arxiv.org/abs/2504.16828"}
{"created":"2025-04-24","title":"Evaluating Autoencoders for Parametric and Invertible Multidimensional Projections","abstract":"Recently, neural networks have gained attention for creating parametric and invertible multidimensional data projections. Parametric projections allow for embedding previously unseen data without recomputing the projection as a whole, while invertible projections enable the generation of new data points. However, these properties have never been explored simultaneously for arbitrary projection methods. We evaluate three autoencoder (AE) architectures for creating parametric and invertible projections. Based on a given projection, we train AEs to learn a mapping into 2D space and an inverse mapping into the original space. We perform a quantitative and qualitative comparison on four datasets of varying dimensionality and pattern complexity using t-SNE. Our results indicate that AEs with a customized loss function can create smoother parametric and inverse projections than feed-forward neural networks while giving users control over the strength of the smoothing effect.","authors":["Frederik L. Dennig","Nina Geyer","Daniela Blumberg","Yannick Metz","Daniel A. Keim"],"url":"https://arxiv.org/abs/2504.16831"}
{"created":"2025-04-24","title":"GreenMind: A Next-Generation Vietnamese Large Language Model for Structured and Logical Reasoning","abstract":"Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.","authors":["Luu Quy Tung","Hoang Quoc Viet","Vo Trong Thu"],"url":"https://arxiv.org/abs/2504.16832"}
{"created":"2025-04-24","title":"LRASGen: LLM-based RESTful API Specification Generation","abstract":"REpresentation State Transfer (REST) is an architectural style for designing web applications that enable scalable, stateless communication between clients and servers via common HTTP techniques. Web APIs that employ the REST style are known as RESTful (or REST) APIs. When using or testing a RESTful API, developers may need to employ its specification, which is often defined by open-source standards such as the OpenAPI Specification (OAS). However, it can be very time-consuming and error-prone to write and update these specifications, which may negatively impact the use of RESTful APIs, especially when the software requirements change. Many tools and methods have been proposed to solve this problem, such as Respector and Swagger Core. OAS generation can be regarded as a common text-generation task that creates a formal description of API endpoints derived from the source code. A potential solution for this may involve using Large Language Models (LLMs), which have strong capabilities in both code understanding and text generation. Motivated by this, we propose a novel approach for generating the OASs of RESTful APIs using LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the best of our knowledge, this is the first use of LLMs and API source code to generate OASs for RESTful APIs. Compared with existing tools and methods, LRASGen can generate the OASs, even when the implementation is incomplete (with partial code, and/or missing annotations/comments, etc.). To evaluate the LRASGen performance, we conducted a series of empirical studies on 20 real-world RESTful APIs. The results show that two LLMs (GPT-4o mini and DeepSeek V3) can both support LARSGen to generate accurate specifications, and LRASGen-generated specifications cover an average of 48.85% more missed entities than the developer-provided specifications.","authors":["Sida Deng","Rubing Huang","Man Zhang","Chenhui Cui","Dave Towey","Rongcun Wang"],"url":"https://arxiv.org/abs/2504.16833"}
{"created":"2025-04-24","title":"Improving Significant Wave Height Prediction Using Chronos Models","abstract":"Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.","authors":["Yilin Zhai","Hongyuan Shi","Chao Zhan","Qing Wang","Zaijin You","Nan Wang"],"url":"https://arxiv.org/abs/2504.16834"}
{"created":"2025-04-24","title":"Snorkeling in dark waters: A longitudinal surface exploration of unique Tor Hidden Services (Extended Version)","abstract":"The Onion Router (Tor) is a controversial network whose utility is constantly under scrutiny. On the one hand, it allows for anonymous interaction and cooperation of users seeking untraceable navigation on the Internet. This freedom also attracts criminals who aim to thwart law enforcement investigations, e.g., trading illegal products or services such as drugs or weapons. Tor allows delivering content without revealing the actual hosting address, by means of .onion (or hidden) services. Different from regular domains, these services can not be resolved by traditional name services, are not indexed by regular search engines, and they frequently change. This generates uncertainty about the extent and size of the Tor network and the type of content offered.","authors":["Alfonso Rodriguez Barredo-Valenzuela","Sergio Pastrana Portillo","Guillermo Suarez-Tangil"],"url":"https://arxiv.org/abs/2504.16836"}
{"created":"2025-04-24","title":"Approximating Optimal Labelings for Temporal Connectivity","abstract":"In a temporal graph the edge set dynamically changes over time according to a set of time-labels associated with each edge that indicates at which time-steps the edge is available. Two vertices are connected if there is a path connecting them in which the edges are traversed in increasing order of their labels. We study the problem of scheduling the availability time of the edges of a temporal graph in such a way that all pairs of vertices are connected within a given maximum allowed time $a$ and the overall number of labels is minimized.","authors":["Daniele Carnevale (Gran Sasso Science Institute)","Gianlorenzo D'Angelo (Gran Sasso Science Institute)","Martin Olsen (Aarhus University)"],"url":"https://arxiv.org/abs/2504.16837"}
{"created":"2025-04-24","title":"SMART: Tuning a symbolic music generation system with an audio domain aesthetic reward","abstract":"Recent work has proposed training machine learning models to predict aesthetic ratings for music audio. Our work explores whether such models can be used to finetune a symbolic music generation system with reinforcement learning, and what effect this has on the system outputs. To test this, we use group relative policy optimization to finetune a piano MIDI model with Meta Audiobox Aesthetics ratings of audio-rendered outputs as the reward. We find that this optimization has effects on multiple low-level features of the generated outputs, and improves the average subjective ratings in a preliminary listening study with $14$ participants. We also find that over-optimization dramatically reduces diversity of model outputs.","authors":["Nicolas Jonason","Luca Casini","Bob L. T. Sturm"],"url":"https://arxiv.org/abs/2504.16839"}
{"created":"2025-04-24","title":"A Low-Cost Photogrammetry System for 3D Plant Modeling and Phenotyping","abstract":"We present an open-source, low-cost photogrammetry system for 3D plant modeling and phenotyping. The system uses a structure-from-motion approach to reconstruct 3D representations of the plants via point clouds. Using wheat as an example, we demonstrate how various phenotypic traits can be computed easily from the point clouds. These include standard measurements such as plant height and radius, as well as features that would be more cumbersome to measure by hand, such as leaf angles and convex hull. We further demonstrate the utility of the system through the investigation of specific metrics that may yield objective classifications of erectophile versus planophile wheat canopy architectures.","authors":["Joe Hrzich","Michael A. Beck","Christopher P. Bidinosti","Christopher J. Henry","Kalhari Manawasinghe","Karen Tanino"],"url":"https://arxiv.org/abs/2504.16840"}
{"created":"2025-04-24","title":"Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion Models","abstract":"This paper uses the capabilities of latent diffusion models (LDMs) to generate realistic RGB human-object interaction scenes to guide humanoid loco-manipulation planning. To do so, we extract from the generated images both the contact locations and robot configurations that are then used inside a whole-body trajectory optimization (TO) formulation to generate physically consistent trajectories for humanoids. We validate our full pipeline in simulation for different long-horizon loco-manipulation scenarios and perform an extensive analysis of the proposed contact and robot configuration extraction pipeline. Our results show that using the information extracted from LDMs, we can generate physically consistent trajectories that require long-horizon reasoning.","authors":["Ilyass Taouil","Haizhou Zhao","Angela Dai","Majid Khadiv"],"url":"https://arxiv.org/abs/2504.16843"}
{"created":"2025-04-24","title":"Improving QoS Prediction in Urban V2X Networks by Leveraging Data from Leading Vehicles and Historical Trends","abstract":"With the evolution of Vehicle-to-Everything (V2X) technology and increased deployment of 5G networks and edge computing, Predictive Quality of Service (PQoS) is seen as an enabler for resilient and adaptive V2X communication systems. PQoS incorporates data-driven techniques, such as Machine Learning (ML), to forecast/predict Key Performing Indicators (KPIs) such as throughput, latency, etc. In this paper, we aim to predict downlink throughput in an urban environment using the Berlin V2X cellular dataset. We select features from the ego and lead vehicles to train different ML models to help improve the predicted throughput for the ego vehicle. We identify these features based on an in-depth exploratory data analysis. Results show an improvement in model performance when adding features from the lead vehicle. Moreover, we show that the improvement in model performance is model-agnostic.","authors":["Sanket Partani","Michael Zentarra","Anthony Kiggundu","Hans D. Schotten"],"url":"https://arxiv.org/abs/2504.16848"}
{"created":"2025-04-24","title":"Hyperspectral Vision Transformers for Greenhouse Gas Estimations from Space","abstract":"Hyperspectral imaging provides detailed spectral information and holds significant potential for monitoring of greenhouse gases (GHGs). However, its application is constrained by limited spatial coverage and infrequent revisit times. In contrast, multispectral imaging offers broader spatial and temporal coverage but often lacks the spectral detail that can enhance GHG detection. To address these challenges, this study proposes a spectral transformer model that synthesizes hyperspectral data from multispectral inputs. The model is pre-trained via a band-wise masked autoencoder and subsequently fine-tuned on spatio-temporally aligned multispectral-hyperspectral image pairs. The resulting synthetic hyperspectral data retain the spatial and temporal benefits of multispectral imagery and improve GHG prediction accuracy relative to using multispectral data alone. This approach effectively bridges the trade-off between spectral resolution and coverage, highlighting its potential to advance atmospheric monitoring by combining the strengths of hyperspectral and multispectral systems with self-supervised deep learning.","authors":["Ruben Gonzalez Avil\\'es","Linus Scheibenreif","Nassim Ait Ali Braham","Benedikt Blumenstiel","Thomas Brunschwiler","Ranjini Guruprasad","Damian Borth","Conrad Albrecht","Paolo Fraccaro","Devyani Lambhate","Johannes Jakubik"],"url":"https://arxiv.org/abs/2504.16851"}
{"created":"2025-04-24","title":"Fair division of the replacement-units without an appraiser in urban renewal processes","abstract":"Rebuild and Divide is an urban renewal process that involves the demolition of old buildings and the construction of new ones. Original homeowners are compensated with upgraded apartments, while surplus units are sold for profit, so theoretically it is a win-win project for all parties involved. However, many rebuild-and-divide projects withheld or delayed due to disagreements over the assignment of new units, claiming they are not \"fair\". The goal of this research is to develop algorithms for envy-free allocation of the new units. The main challenge is that, in contrast to previous work on envy-free allocation, the envy depends also on the value of the old units, as people with more valuable old units are entitled to more valuable new units. We introduce three models that capture different notions of fairness: (1) the Difference Model, where agents evaluate their gains relative to others; (2) the Envy Sum Model, which permits some envy as long as the total envy does not exceed that of the original allocation; and (3) the Ratio Model, where fairness is assessed based on the proportional value of old apartments. For each model, we establish an envy criterion and seek a payment vector and allocation that ensure envy-freeness. These models present both theoretical challenges and intriguing insights. Additionally, within the Envy Sum Model, we present a mechanism that computes an allocation and payment scheme that minimizes total envy. We also analyze the mechanism's vulnerability to manipulation and identify conditions under which it is obviously manipulable.","authors":["Noga Klein Elmalem","Rica Gonen","Erel Segal-Halevi"],"url":"https://arxiv.org/abs/2504.16852"}
{"created":"2025-04-24","title":"Formal Verification of Blockchain Nonforking in DAG-Based BFT Consensus with Dynamic Stake","abstract":"Blockchain consensus protocols enable participants to agree on consistent views of the blockchain that may be ahead or behind relative to each other but do not fork into different chains. A number of recently popular Byzantine-fault-tolerant (BFT) protocols first construct a directed acyclic graph (DAG) that partially orders transactions, then linearize the DAG into a blockchain that totally orders transactions. The definitions and correctness proofs of these DAG-based protocols typically assume that the set of participants is fixed, which is impractical in long-lived blockchains. Additionally, only a few of those proofs have been machine-checked, uncovering errors in some published proofs. We developed a formal model of a DAG-based BFT protocol with dynamic stake, where participants can join and leave at every block, with stake used to weigh decisions in the protocol. We formally proved that blockchains never fork in the model, also clarifying how BFT bounds on faulty participants generalize to these highly dynamic sets of participants. Our model and proofs are formalized in the ACL2 theorem prover, apply to arbitrarily long executions and arbitrarily large system states, and are verified in 1 minute by ACL2.","authors":["Alessandro Coglio","Eric McCarthy"],"url":"https://arxiv.org/abs/2504.16853"}
{"created":"2025-04-24","title":"Monte Carlo Planning with Large Language Model for Text-Based Game Agents","abstract":"Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.","authors":["Zijing Shi","Meng Fang","Ling Chen"],"url":"https://arxiv.org/abs/2504.16855"}
{"created":"2025-04-24","title":"Emo Pillars: Knowledge Distillation to Support Fine-Grained Context-Aware and Context-Less Emotion Classification","abstract":"Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.","authors":["Alexander Shvets"],"url":"https://arxiv.org/abs/2504.16856"}
{"created":"2025-04-24","title":"Planning with Diffusion Models for Target-Oriented Dialogue Systems","abstract":"Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.","authors":["Hanwen Du","Bo Peng","Xia Ning"],"url":"https://arxiv.org/abs/2504.16858"}
{"created":"2025-04-24","title":"Neural Network Element Method for Partial Differential Equations","abstract":"In this paper, based on the combination of finite element mesh and neural network, a novel type of neural network element space and corresponding machine learning method are designed for solving partial differential equations. The application of finite element mesh makes the neural network element space satisfy the boundary value conditions directly on the complex geometric domains. The use of neural networks allows the accuracy of the approximate solution to reach the high level of neural network approximation even for the problems with singularities. We also provide the error analysis of the proposed method for the understanding. The proposed numerical method in this paper provides the way to enable neural network-based machine learning algorithms to solve a broader range of problems arising from engineering applications.","authors":["Yifan Wang","Zhongshuo Lin","Hehu Xie"],"url":"https://arxiv.org/abs/2504.16862"}
{"created":"2025-04-24","title":"An Adaptive ML Framework for Power Converter Monitoring via Federated Transfer Learning","abstract":"This study explores alternative framework configurations for adapting thermal machine learning (ML) models for power converters by combining transfer learning (TL) and federated learning (FL) in a piecewise manner. This approach inherently addresses challenges such as varying operating conditions, data sharing limitations, and security implications. The framework starts with a base model that is incrementally adapted by multiple clients via adapting three state-of-the-art domain adaptation techniques: Fine-tuning, Transfer Component Analysis (TCA), and Deep Domain Adaptation (DDA). The Flower framework is employed for FL, using Federated Averaging for aggregation. Validation with field data demonstrates that fine-tuning offers a straightforward TL approach with high accuracy, making it suitable for practical applications. Benchmarking results reveal a comprehensive comparison of these methods, showcasing their respective strengths and weaknesses when applied in different scenarios. Locally hosted FL enhances performance when data aggregation is not feasible, while cloud-based FL becomes more practical with a significant increase in the number of clients, addressing scalability and connectivity challenges.","authors":["Panagiotis Kakosimos","Alireza Nemat Saberi","Luca Peretti"],"url":"https://arxiv.org/abs/2504.16866"}
{"created":"2025-04-24","title":"High-Quality Cloud-Free Optical Image Synthesis Using Multi-Temporal SAR and Contaminated Optical Data","abstract":"Addressing gaps caused by cloud cover and the long revisit cycle of satellites is vital for providing essential data to support remote sensing applications. This paper tackles the challenges of missing optical data synthesis, particularly in complex scenarios with cloud cover. We propose CRSynthNet, a novel image synthesis network that incorporates innovative designed modules such as the DownUp Block and Fusion Attention to enhance accuracy. Experimental results validate the effectiveness of CRSynthNet, demonstrating substantial improvements in restoring structural details, preserving spectral consist, and achieving superior visual effects that far exceed those produced by comparison methods. It achieves quantitative improvements across multiple metrics: a peak signal-to-noise ratio (PSNR) of 26.978, a structural similarity index measure (SSIM) of 0.648, and a root mean square error (RMSE) of 0.050. Furthermore, this study creates the TCSEN12 dataset, a valuable resource specifically designed to address cloud cover challenges in missing optical data synthesis study. The dataset uniquely includes cloud-covered images and leverages earlier image to predict later image, offering a realistic representation of real-world scenarios. This study offer practical method and valuable resources for optical satellite image synthesis task.","authors":["Chenxi Duan"],"url":"https://arxiv.org/abs/2504.16870"}
{"created":"2025-04-24","title":"Exploring How LLMs Capture and Represent Domain-Specific Knowledge","abstract":"We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the model's internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks","authors":["Mirian Hipolito Garcia","Camille Couturier","Daniel Madrigal Diaz","Ankur Mallick","Anastasios Kyrillidis","Robert Sim","Victor Ruhle","Saravan Rajmohan"],"url":"https://arxiv.org/abs/2504.16871"}
{"created":"2025-04-24","title":"Reconfigurable Intelligent Surface Control for a Moving Receiver","abstract":"Reconfigurable intelligent surfaces (RISs) are emerging as key enablers of reliable industrial automation in the millimeter-wave (mmWave) band, particularly in environments with frequent line-of-sight (LoS) blockage. While prior works have largely focused on theoretical aspects, real-time validation under user mobility remains underexplored. In this work, we propose and experimentally evaluate a self-adaptive beamforming algorithm that enables RIS reconfiguration based on a low-rate feedback link from the mobile user equipment (UE) to the RIS controller. The algorithm maintains received signal power above a predefined threshold without requiring UE position knowledge. Using a hexagonal RIS with 127 elements operating at 23.8 GHz, we validate our approach in a semi-anechoic environment over a 60 cm*100 cm observation area. The results demonstrate up to 24 dB gain in received power compared to the baseline with inactive RIS elements, highlighting the practical benefits of adaptive RIS control in mobile non-line-of-sight (NLoS) scenarios.","authors":["Hamed Radpour","Markus Hofer","Thomas Zemen"],"url":"https://arxiv.org/abs/2504.16874"}
{"created":"2025-04-24","title":"Hybrid Reinforcement Learning and Model Predictive Control for Adaptive Control of Hydrogen-Diesel Dual-Fuel Combustion","abstract":"Reinforcement Learning (RL) and Machine Learning Integrated Model Predictive Control (ML-MPC) are promising approaches for optimizing hydrogen-diesel dual-fuel engine control, as they can effectively control multiple-input multiple-output systems and nonlinear processes. ML-MPC is advantageous for providing safe and optimal controls, ensuring the engine operates within predefined safety limits. In contrast, RL is distinguished by its adaptability to changing conditions through its learning-based approach. However, the practical implementation of either method alone poses challenges. RL requires high variance in control inputs during early learning phases, which can pose risks to the system by potentially executing unsafe actions, leading to mechanical damage. Conversely, ML-MPC relies on an accurate system model to generate optimal control inputs and has limited adaptability to system drifts, such as injector aging, which naturally occur in engine applications. To address these limitations, this study proposes a hybrid RL and ML-MPC approach that uses an ML-MPC framework while incorporating an RL agent to dynamically adjust the ML-MPC load tracking reference in response to changes in the environment. At the same time, the ML-MPC ensures that actions stay safe throughout the RL agent's exploration. To evaluate the effectiveness of this approach, fuel pressure is deliberately varied to introduce a model-plant mismatch between the ML-MPC and the engine test bench. The result of this mismatch is a root mean square error (RMSE) in indicated mean effective pressure of 0.57 bar when running the ML-MPC. The experimental results demonstrate that RL successfully adapts to changing boundary conditions by altering the tracking reference while ML-MPC ensures safe control inputs. The quantitative improvement in load tracking by implementing RL is an RSME of 0.44 bar.","authors":["Julian Bedei","Murray McBain","Charles Robert Koch","Jakob Andert","David Gordon"],"url":"https://arxiv.org/abs/2504.16875"}
{"created":"2025-04-24","title":"Context-Enhanced Vulnerability Detection Based on Large Language Model","abstract":"Vulnerability detection is a critical aspect of software security. Accurate detection is essential to prevent potential security breaches and protect software systems from malicious attacks. Recently, vulnerability detection methods leveraging deep learning and large language models (LLMs) have garnered increasing attention. However, existing approaches often focus on analyzing individual files or functions, which limits their ability to gather sufficient contextual information. Analyzing entire repositories to gather context introduces significant noise and computational overhead. To address these challenges, we propose a context-enhanced vulnerability detection approach that combines program analysis with LLMs. Specifically, we use program analysis to extract contextual information at various levels of abstraction, thereby filtering out irrelevant noise. The abstracted context along with source code are provided to LLM for vulnerability detection. We investigate how different levels of contextual granularity improve LLM-based vulnerability detection performance. Our goal is to strike a balance between providing sufficient detail to accurately capture vulnerabilities and minimizing unnecessary complexity that could hinder model performance. Based on an extensive study using GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key findings includes: (1) incorporating abstracted context significantly enhances vulnerability detection effectiveness; (2) different models benefit from distinct levels of abstraction depending on their code understanding capabilities; and (3) capturing program behavior through program analysis for general LLM-based code analysis tasks can be a direction that requires further attention.","authors":["Yixin Yang","Bowen Xu","Xiang Gao","Hailong Sun"],"url":"https://arxiv.org/abs/2504.16877"}
{"created":"2025-04-24","title":"Learning Verifiable Control Policies Using Relaxed Verification","abstract":"To provide safety guarantees for learning-based control systems, recent work has developed formal verification methods to apply after training ends. However, if the trained policy does not meet the specifications, or there is conservatism in the verification algorithm, establishing these guarantees may not be possible. Instead, this work proposes to perform verification throughout training to ultimately aim for policies whose properties can be evaluated throughout runtime with lightweight, relaxed verification algorithms. The approach is to use differentiable reachability analysis and incorporate new components into the loss function. Numerical experiments on a quadrotor model and unicycle model highlight the ability of this approach to lead to learned control policies that satisfy desired reach-avoid and invariance specifications.","authors":["Puja Chaudhury","Alexander Estornell","Michael Everett"],"url":"https://arxiv.org/abs/2504.16879"}
{"created":"2025-04-24","title":"Enhancing Critical Thinking with AI: A Tailored Warning System for RAG Models","abstract":"Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. However, fairness and reliability concerns persist, as hallucinations can emerge at both the retrieval and generation stages, affecting users' reasoning and decision-making. Our research explores how tailored warning messages -- whose content depends on the specific context of hallucination -- shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also introduce cognitive friction, leading to confusion and diminished trust in the system. By examining these interactions, this work contributes to the broader goal of AI-augmented reasoning: developing systems that actively support human reflection, critical thinking, and informed decision-making rather than passive information consumption.","authors":["Xuyang Zhu","Sejoon Chang","Andrew Kuik"],"url":"https://arxiv.org/abs/2504.16883"}
{"created":"2025-04-24","title":"Do Large Language Models know who did what to whom?","abstract":"Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.","authors":["Joseph M. Denning (Hannah)","Xiaohan (Hannah)","Guo","Bryor Snefjella","Idan A. Blank"],"url":"https://arxiv.org/abs/2504.16884"}
{"created":"2025-04-24","title":"AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset","abstract":"This paper presents our winning submission to the AI Mathematical Olympiad - Progress Prize 2 (AIMO-2) competition. Our recipe for building state-of-the-art mathematical reasoning models relies on three key pillars. First, we create a large-scale dataset comprising 540K unique high-quality math problems, including olympiad-level problems, and their 3.2M long-reasoning solutions. Second, we develop a novel method to integrate code execution with long reasoning models through iterative training, generation, and quality filtering, resulting in 1.7M high-quality Tool-Integrated Reasoning solutions. Third, we create a pipeline to train models to select the most promising solution from many candidates. We show that such generative solution selection (GenSelect) can significantly improve upon majority voting baseline. Combining these ideas, we train a series of models that achieve state-of-the-art results on mathematical reasoning benchmarks. To facilitate further research, we release our code, models, and the complete OpenMathReasoning dataset under a commercially permissive license.","authors":["Ivan Moshkov","Darragh Hanley","Ivan Sorokin","Shubham Toshniwal","Christof Henkel","Benedikt Schifferer","Wei Du","Igor Gitman"],"url":"https://arxiv.org/abs/2504.16891"}
{"created":"2025-04-24","title":"Memory-efficient Sketch Acceleration for Handling Large Network Flows on FPGAs","abstract":"Sketch-based algorithms for network traffic monitoring have drawn increasing interest in recent years due to their sub-linear memory efficiency and high accuracy. As the volume of network traffic grows, software-based sketch implementations cannot match the throughput of the incoming network flows. FPGA-based hardware sketch has shown better performance compared to software running on a CPU when handling these packets. Among the various sketch algorithms, Count-min sketch is one of the most popular and efficient. However, due to the limited amount of on-chip memory, the FPGA-based count-Min sketch accelerator suffers from performance drops as network traffic grows. In this work, we propose a hardware-friendly architecture with a variable width memory counter for count-min sketch. Our architecture provides a more compact design to store the sketch data structure effectively, allowing us to support larger hash tables and reduce overestimation errors. The design makes use of a P4-based programmable data plane and the AMD OpenNIC shell. The design is implemented and verified on the Open Cloud Testbed running on AMD Alveo U280s and can keep up with the 100 Gbit link speed.","authors":["Zhaoyang Han","Yicheng Qian Michael Zink","Miriam Leeser"],"url":"https://arxiv.org/abs/2504.16896"}
{"created":"2025-04-24","title":"Assessing SSL/TLS Certificate Centralization: Implications for Digital Sovereignty","abstract":"SSL/TLS is a fundamental technology in the network protocol stack that enables encrypted data transmission and authentication of web domains. However, the current model relies on a small number of Certificate Authorities (CAs) to provide and validate certificates, thus creating a highly centralized ecosystem. In this paper, we analyze the degree of centralization of certificate provisioning from CAs in two major political groups: Brazil, Russia, India, China, and South Africa (BRICS) and the European Union (EU). We have found that over 75\\% of certificates for both BRICS and EU domains originate from CAs based in the United States, indicating possible risks to their digital sovereignty due to the high level of external dependency. This indicates the need for nations within those groups to research alternatives to reduce the high level of dependency on foreign CAs and increase their digital autonomy.","authors":["Andrei Cordova Azevedo","Eder John Scheid","Muriel Figueredo Franco","Lisandro Zambenedetti Granville"],"url":"https://arxiv.org/abs/2504.16897"}
{"created":"2025-04-24","title":"Texture: Structured Exploration of Text Datasets","abstract":"Exploratory analysis of a text corpus is essential for assessing data quality and developing meaningful hypotheses. Text analysis relies on understanding documents through structured attributes spanning various granularities of the documents such as words, phrases, sentences, topics, or clusters. However, current text visualization tools typically adopt a fixed representation tailored to specific tasks or domains, requiring users to switch tools as their analytical goals change. To address this limitation, we present Texture, a general-purpose interactive text exploration tool. Texture introduces a configurable data schema for representing text documents enriched with descriptive attributes. These attributes can appear at arbitrary levels of granularity in the text and possibly have multiple values, including document-level attributes, multi-valued attributes (e.g., topics), fine-grained span-level attributes (e.g., words), and vector embeddings. The system then combines existing interactive methods for text exploration into a single interface that provides attribute overview visualizations, supports cross-filtering attribute charts to explore subsets, uses embeddings for a dataset overview and similar instance search, and contextualizes filters in the actual documents. We evaluated Texture through a two-part user study with 10 participants from varied domains who each analyzed their own dataset in a baseline session and then with Texture. Texture was able to represent all of the previously derived dataset attributes, enabled participants to more quickly iterate during their exploratory analysis, and discover new insights about their data. Our findings contribute to the design of scalable, interactive, and flexible exploration systems that improve users' ability to make sense of text data.","authors":["Will Epperson","Arpit Mathur","Adam Perer","Dominik Moritz"],"url":"https://arxiv.org/abs/2504.16898"}
{"created":"2025-04-24","title":"Building A Secure Agentic AI Application Leveraging A2A Protocol","abstract":"As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies.","authors":["Idan Habler","Ken Huang","Vineeth Sai Narajala","Prashant Kulkarni"],"url":"https://arxiv.org/abs/2504.16902"}
{"created":"2025-04-24","title":"BadVideo: Stealthy Backdoor Attack against Text-to-Video Generation","abstract":"Text-to-video (T2V) generative models have rapidly advanced and found widespread applications across fields like entertainment, education, and marketing. However, the adversarial vulnerabilities of these models remain rarely explored. We observe that in T2V generation tasks, the generated videos often contain substantial redundant information not explicitly specified in the text prompts, such as environmental elements, secondary objects, and additional details, providing opportunities for malicious attackers to embed hidden harmful content. Exploiting this inherent redundancy, we introduce BadVideo, the first backdoor attack framework tailored for T2V generation. Our attack focuses on designing target adversarial outputs through two key strategies: (1) Spatio-Temporal Composition, which combines different spatiotemporal features to encode malicious information; (2) Dynamic Element Transformation, which introduces transformations in redundant elements over time to convey malicious information. Based on these strategies, the attacker's malicious target seamlessly integrates with the user's textual instructions, providing high stealthiness. Moreover, by exploiting the temporal dimension of videos, our attack successfully evades traditional content moderation systems that primarily analyze spatial information within individual frames. Extensive experiments demonstrate that BadVideo achieves high attack success rates while preserving original semantics and maintaining excellent performance on clean inputs. Overall, our work reveals the adversarial vulnerability of T2V models, calling attention to potential risks and misuse. Our project page is at https://wrt2000.github.io/BadVideo2025/.","authors":["Ruotong Wang","Mingli Zhu","Jiarong Ou","Rui Chen","Xin Tao","Pengfei Wan","Baoyuan Wu"],"url":"https://arxiv.org/abs/2504.16907"}
{"created":"2025-04-24","title":"Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM Behind AI-Generated Text","abstract":"In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.","authors":["Shifali Agrahari","Sanasam Ranbir Singh"],"url":"https://arxiv.org/abs/2504.16913"}
{"created":"2025-04-24","title":"MorphoNavi: Aerial-Ground Robot Navigation with Object Oriented Mapping in Digital Twin","abstract":"This paper presents a novel mapping approach for a universal aerial-ground robotic system utilizing a single monocular camera. The proposed system is capable of detecting a diverse range of objects and estimating their positions without requiring fine-tuning for specific environments. The system's performance was evaluated through a simulated search-and-rescue scenario, where the MorphoGear robot successfully located a robotic dog while an operator monitored the process. This work contributes to the development of intelligent, multimodal robotic systems capable of operating in unstructured environments.","authors":["Sausar Karaf","Mikhail Martynov","Oleg Sautenkov","Zhanibek Darush","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2504.16914"}
{"created":"2025-04-24","title":"DreamO: A Unified Framework for Image Customization","abstract":"Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions.","authors":["Chong Mou","Yanze Wu","Wenxu Wu","Zinan Guo","Pengze Zhang","Yufeng Cheng","Yiming Luo","Fei Ding","Shiwen Zhang","Xinghui Li","Mengtian Li","Songtao Zhao","Jian Zhang","Qian He","Xinglong Wu"],"url":"https://arxiv.org/abs/2504.16915"}
{"created":"2025-04-24","title":"Zero-shot Sim-to-Real Transfer for Reinforcement Learning-based Visual Servoing of Soft Continuum Arms","abstract":"Soft continuum arms (SCAs) soft and deformable nature presents challenges in modeling and control due to their infinite degrees of freedom and non-linear behavior. This work introduces a reinforcement learning (RL)-based framework for visual servoing tasks on SCAs with zero-shot sim-to-real transfer capabilities, demonstrated on a single section pneumatic manipulator capable of bending and twisting. The framework decouples kinematics from mechanical properties using an RL kinematic controller for motion planning and a local controller for actuation refinement, leveraging minimal sensing with visual feedback. Trained entirely in simulation, the RL controller achieved a 99.8% success rate. When deployed on hardware, it achieved a 67% success rate in zero-shot sim-to-real transfer, demonstrating robustness and adaptability. This approach offers a scalable solution for SCAs in 3D visual servoing, with potential for further refinement and expanded applications.","authors":["Hsin-Jung Yang","Mahsa Khosravi","Benjamin Walt","Girish Krishnan","Soumik Sarkar"],"url":"https://arxiv.org/abs/2504.16916"}
{"created":"2025-04-24","title":"OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents","abstract":"Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \\textbf{OptimAI}, a framework for solving \\underline{Optim}ization problems described in natural language by leveraging LLM-powered \\underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \\emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \\emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \\emph{coder} and a \\emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\\times$ and $3.1\\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\\% accuracy on the NLP4LP dataset and 71.2\\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\\% and 50\\% respectively over prior best results.","authors":["Raghav Thind","Youran Sun","Ling Liang","Haizhao Yang"],"url":"https://arxiv.org/abs/2504.16918"}
{"created":"2025-04-24","title":"IberBench: LLM Evaluation on Iberian Languages","abstract":"Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.","authors":["Jos\\'e \\'Angel Gonz\\'alez","Ian Borrego Obrador","\\'Alvaro Romo Herrero","Areg Mikael Sarvazyan","Mara Chinea-R\\'ios","Angelo Basile","Marc Franco-Salvador"],"url":"https://arxiv.org/abs/2504.16921"}
{"created":"2025-04-24","title":"Generalized Neighborhood Attention: Multi-dimensional Sparse Attention at the Speed of Light","abstract":"Many sparse attention mechanisms such as Neighborhood Attention have typically failed to consistently deliver speedup over the self attention baseline. This is largely due to the level of complexity in attention infrastructure, and the rapid evolution of AI hardware architecture. At the same time, many state-of-the-art foundational models, particularly in computer vision, are heavily bound by attention, and need reliable sparsity to escape the O(n^2) complexity. In this paper, we study a class of promising sparse attention mechanisms that focus on locality, and aim to develop a better analytical model of their performance improvements. We first introduce Generalized Neighborhood Attention (GNA), which can describe sliding window, strided sliding window, and blocked attention. We then consider possible design choices in implementing these approaches, and create a simulator that can provide much more realistic speedup upper bounds for any given setting. Finally, we implement GNA on top of a state-of-the-art fused multi-headed attention (FMHA) kernel designed for the NVIDIA Blackwell architecture in CUTLASS. Our implementation can fully realize the maximum speedup theoretically possible in many perfectly block-sparse cases, and achieves an effective utilization of 1.3 petaFLOPs/second in FP16. In addition, we plug various GNA configurations into off-the-shelf generative models, such as Cosmos-7B, HunyuanVideo, and FLUX, and show that it can deliver 28% to 46% end-to-end speedup on B200 without any fine-tuning. We will open source our simulator and Blackwell kernels directly through the NATTEN project.","authors":["Ali Hassani","Fengzhe Zhou","Aditya Kane","Jiannan Huang","Chieh-Yun Chen","Min Shi","Steven Walton","Markus Hoehnerbach","Vijay Thakkar","Michael Isaev","Qinsheng Zhang","Bing Xu","Haicheng Wu","Wen-mei Hwu","Ming-Yu Liu","Humphrey Shi"],"url":"https://arxiv.org/abs/2504.16922"}
{"created":"2025-04-24","title":"Meta-Learning Online Dynamics Model Adaptation in Off-Road Autonomous Driving","abstract":"High-speed off-road autonomous driving presents unique challenges due to complex, evolving terrain characteristics and the difficulty of accurately modeling terrain-vehicle interactions. While dynamics models used in model-based control can be learned from real-world data, they often struggle to generalize to unseen terrain, making real-time adaptation essential. We propose a novel framework that combines a Kalman filter-based online adaptation scheme with meta-learned parameters to address these challenges. Offline meta-learning optimizes the basis functions along which adaptation occurs, as well as the adaptation parameters, while online adaptation dynamically adjusts the onboard dynamics model in real time for model-based control. We validate our approach through extensive experiments, including real-world testing on a full-scale autonomous off-road vehicle, demonstrating that our method outperforms baseline approaches in prediction accuracy, performance, and safety metrics, particularly in safety-critical scenarios. Our results underscore the effectiveness of meta-learned dynamics model adaptation, advancing the development of reliable autonomous systems capable of navigating diverse and unseen environments. Video is available at: https://youtu.be/cCKHHrDRQEA","authors":["Jacob Levy","Jason Gibson","Bogdan Vlahov","Erica Tevere","Evangelos Theodorou","David Fridovich-Keil","Patrick Spieler"],"url":"https://arxiv.org/abs/2504.16923"}
{"created":"2025-04-24","title":"Latent Diffusion Planning for Imitation Learning","abstract":"Recent progress in imitation learning has been enabled by policy architectures that scale to complex visuomotor tasks, multimodal distributions, and large datasets. However, these methods often rely on learning from large amount of expert demonstrations. To address these shortcomings, we propose Latent Diffusion Planning (LDP), a modular approach consisting of a planner which can leverage action-free demonstrations, and an inverse dynamics model which can leverage suboptimal data, that both operate over a learned latent space. First, we learn a compact latent space through a variational autoencoder, enabling effective forecasting of future states in image-based domains. Then, we train a planner and an inverse dynamics model with diffusion objectives. By separating planning from action prediction, LDP can benefit from the denser supervision signals of suboptimal and action-free data. On simulated visual robotic manipulation tasks, LDP outperforms state-of-the-art imitation learning approaches, as they cannot leverage such additional data.","authors":["Amber Xie","Oleh Rybkin","Dorsa Sadigh","Chelsea Finn"],"url":"https://arxiv.org/abs/2504.16925"}
{"created":"2025-04-24","title":"I-Con: A Unifying Framework for Representation Learning","abstract":"As the field of representation learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We introduce a single information-theoretic equation that generalizes a large collection of modern loss functions in machine learning. In particular, we introduce a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two conditional distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. This framework enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 23 different approaches, but we also leverage these theoretical results to create state-of-the-art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K. We also demonstrate that I-Con can be used to derive principled debiasing methods which improve contrastive representation learners.","authors":["Shaden Alshammari","John Hershey","Axel Feldmann","William T. Freeman","Mark Hamilton"],"url":"https://arxiv.org/abs/2504.16929"}
{"created":"2025-04-24","title":"Procedural Dataset Generation for Zero-Shot Stereo Matching","abstract":"Synthetic datasets are a crucial ingredient for training stereo matching networks, but the question of what makes a stereo dataset effective remains largely unexplored. We investigate the design space of synthetic datasets by varying the parameters of a procedural dataset generator, and report the effects on zero-shot stereo matching performance using standard benchmarks. We collect the best settings to produce Infinigen-Stereo, a procedural generator specifically optimized for zero-shot stereo datasets. Models trained only on data from our system outperform robust baselines trained on a combination of existing synthetic datasets and have stronger zero-shot stereo matching performance than public checkpoints from prior works. We open source our system at https://github.com/princeton-vl/InfinigenStereo to enable further research on procedural stereo datasets.","authors":["David Yan","Alexander Raistrick","Jia Deng"],"url":"https://arxiv.org/abs/2504.16930"}
{"created":"2025-04-24","title":"Efficient Portfolio Selection through Preference Aggregation with Quicksort and the Bradley--Terry Model","abstract":"How to allocate limited resources to projects that will yield the greatest long-term benefits is a problem that often arises in decision-making under uncertainty. For example, organizations may need to evaluate and select innovation projects with risky returns. Similarly, when allocating resources to research projects, funding agencies are tasked with identifying the most promising proposals based on idiosyncratic criteria. Finally, in participatory budgeting, a local community may need to select a subset of public projects to fund. Regardless of context, agents must estimate the uncertain values of a potentially large number of projects. Developing parsimonious methods to compare these projects, and aggregating agent evaluations so that the overall benefit is maximized, are critical in assembling the best project portfolio. Unlike in standard sorting algorithms, evaluating projects on the basis of uncertain long-term benefits introduces additional complexities. We propose comparison rules based on Quicksort and the Bradley--Terry model, which connects rankings to pairwise \"win\" probabilities. In our model, each agent determines win probabilities of a pair of projects based on his or her specific evaluation of the projects' long-term benefit. The win probabilities are then appropriately aggregated and used to rank projects. Several of the methods we propose perform better than the two most effective aggregation methods currently available. Additionally, our methods can be combined with sampling techniques to significantly reduce the number of pairwise comparisons. We also discuss how the Bradley--Terry portfolio selection approach can be implemented in practice.","authors":["Yurun Ge","Lucas B\\\"ottcher","Tom Chou","Maria R. D'Orsogna"],"url":"https://arxiv.org/abs/2504.16093"}
{"created":"2025-04-24","title":"BrainPrompt: Multi-Level Brain Prompt Enhancement for Neurological Condition Identification","abstract":"Neurological conditions, such as Alzheimer's Disease, are challenging to diagnose, particularly in the early stages where symptoms closely resemble healthy controls. Existing brain network analysis methods primarily focus on graph-based models that rely solely on imaging data, which may overlook important non-imaging factors and limit the model's predictive power and interpretability. In this paper, we present BrainPrompt, an innovative framework that enhances Graph Neural Networks (GNNs) by integrating Large Language Models (LLMs) with knowledge-driven prompts, enabling more effective capture of complex, non-imaging information and external knowledge for neurological disease identification. BrainPrompt integrates three types of knowledge-driven prompts: (1) ROI-level prompts to encode the identity and function of each brain region, (2) subject-level prompts that incorporate demographic information, and (3) disease-level prompts to capture the temporal progression of disease. By leveraging these multi-level prompts, BrainPrompt effectively harnesses knowledge-enhanced multi-modal information from LLMs, enhancing the model's capability to predict neurological disease stages and meanwhile offers more interpretable results. We evaluate BrainPrompt on two resting-state functional Magnetic Resonance Imaging (fMRI) datasets from neurological disorders, showing its superiority over state-of-the-art methods. Additionally, a biomarker study demonstrates the framework's ability to extract valuable and interpretable information aligned with domain knowledge in neuroscience.","authors":["Jiaxing Xu","Kai He","Yue Tang","Wei Li","Mengcheng Lan","Xia Dong","Yiping Ke","Mengling Feng"],"url":"https://arxiv.org/abs/2504.16096"}
{"created":"2025-04-24","title":"A CNN-based Local-Global Self-Attention via Averaged Window Embeddings for Hierarchical ECG Analysis","abstract":"Cardiovascular diseases remain the leading cause of global mortality, emphasizing the critical need for efficient diagnostic tools such as electrocardiograms (ECGs). Recent advancements in deep learning, particularly transformers, have revolutionized ECG analysis by capturing detailed waveform features as well as global rhythm patterns. However, traditional transformers struggle to effectively capture local morphological features that are critical for accurate ECG interpretation. We propose a novel Local-Global Attention ECG model (LGA-ECG) to address this limitation, integrating convolutional inductive biases with global self-attention mechanisms. Our approach extracts queries by averaging embeddings obtained from overlapping convolutional windows, enabling fine-grained morphological analysis, while simultaneously modeling global context through attention to keys and values derived from the entire sequence. Experiments conducted on the CODE-15 dataset demonstrate that LGA-ECG outperforms state-of-the-art models and ablation studies validate the effectiveness of the local-global attention strategy. By capturing the hierarchical temporal dependencies and morphological patterns in ECG signals, this new design showcases its potential for clinical deployment with robust automated ECG classification.","authors":["Arthur Buzelin","Pedro Robles Dutenhefner","Turi Rezende","Luisa G. Porfirio","Pedro Bento","Yan Aquino","Jose Fernandes","Caio Santana","Gabriela Miana","Gisele L. Pappa","Antonio Ribeiro","Wagner Meira Jr"],"url":"https://arxiv.org/abs/2504.16097"}
{"created":"2025-04-24","title":"SeizureFormer: A Transformer Model for IEA-Based Seizure Risk Forecasting","abstract":"We present SeizureFormer, a Transformer-based model for long-term seizure risk forecasting using interictal epileptiform activity (IEA) surrogate biomarkers and long episode (LE) biomarkers from responsive neurostimulation (RNS) systems. Unlike raw scalp EEG-based models, SeizureFormer leverages structured, clinically relevant features and integrates CNN-based patch embedding, multi-head self-attention, and squeeze-and-excitation blocks to model both short-term dynamics and long-term seizure cycles. Tested across five patients and multiple prediction windows (1 to 14 days), SeizureFormer achieved state-of-the-art performance with mean ROC AUC of 79.44 percent and mean PR AUC of 76.29 percent. Compared to statistical, machine learning, and deep learning baselines, it demonstrates enhanced generalizability and seizure risk forecasting performance under class imbalance. This work supports future clinical integration of interpretable and robust seizure forecasting tools for personalized epilepsy management.","authors":["Tianning Feng (Department of Computer Science","Emory University","Atlanta","GA","USA)","Junting Ni (Department of Computer Science","Emory University","Atlanta","GA","USA)","Ezequiel Gleichgerrcht (Department of Neurology","Emory University","Atlanta","GA","USA)","Wei Jin (Department of Computer Science","Emory University","Atlanta","GA","USA)"],"url":"https://arxiv.org/abs/2504.16098"}
{"created":"2025-04-24","title":"Two-Timescale Joint Transmit and Pinching Beamforming for Pinching-Antenna Systems","abstract":"Pinching antenna systems (PASS) have been proposed as a revolutionary flexible antenna technology which facilitates line-of-sight links via numerous low-cost pinching antennas with adjustable activation positions over waveguides. This letter proposes a two-timescale joint transmit and pinching beamforming design for the maximization of sum rate of a PASS-based downlink multi-user multiple input single output system. A primal dual decomposition method is developed to decouple the two-timescale problem into two sub-problems: 1) A Karush-Kuhn-Tucker-guided dual learning-based approach is proposed to solve the short-term transmit beamforming design sub-problem; 2) The long-term pinching beamforming design sub-problem is tackled by adopting a stochastic successive convex approximation method. Simulation results demonstrate that the proposed two-timescale algorithm achieves a significant performance gain compared to other baselines.","authors":["Luyuan Zhang","Xidong Mu","An Liu","Yuanwei Liu"],"url":"https://arxiv.org/abs/2504.16099"}
{"created":"2025-04-24","title":"Towards Accurate Forecasting of Renewable Energy : Building Datasets and Benchmarking Machine Learning Models for Solar and Wind Power in France","abstract":"Accurate prediction of non-dispatchable renewable energy sources is essential for grid stability and price prediction. Regional power supply forecasts are usually indirect through a bottom-up approach of plant-level forecasts, incorporate lagged power values, and do not use the potential of spatially resolved data. This study presents a comprehensive methodology for predicting solar and wind power production at country scale in France using machine learning models trained with spatially explicit weather data combined with spatial information about production sites capacity. A dataset is built spanning from 2012 to 2023, using daily power production data from RTE (the national grid operator) as the target variable, with daily weather data from ERA5, production sites capacity and location, and electricity prices as input features. Three modeling approaches are explored to handle spatially resolved weather data: spatial averaging over the country, dimension reduction through principal component analysis, and a computer vision architecture to exploit complex spatial relationships. The study benchmarks state-of-the-art machine learning models as well as hyperparameter tuning approaches based on cross-validation methods on daily power production data. Results indicate that cross-validation tailored to time series is best suited to reach low error. We found that neural networks tend to outperform traditional tree-based models, which face challenges in extrapolation due to the increasing renewable capacity over time. Model performance ranges from 4% to 10% in nRMSE for midterm horizon, achieving similar error metrics to local models established at a single-plant level, highlighting the potential of these methods for regional power supply forecasting.","authors":["Eloi Lindas","Yannig Goude","Philippe Ciais"],"url":"https://arxiv.org/abs/2504.16100"}
{"created":"2025-04-24","title":"xLSTM-ECG: Multi-label ECG Classification via Feature Fusion with xLSTM","abstract":"Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, highlighting the critical need for efficient and accurate diagnostic tools. Electrocardiograms (ECGs) are indispensable in diagnosing various heart conditions; however, their manual interpretation is time-consuming and error-prone. In this paper, we propose xLSTM-ECG, a novel approach that leverages an extended Long Short-Term Memory (xLSTM) network for multi-label classification of ECG signals, using the PTB-XL dataset. To the best of our knowledge, this work represents the first design and application of xLSTM modules specifically adapted for multi-label ECG classification. Our method employs a Short-Time Fourier Transform (STFT) to convert time-series ECG waveforms into the frequency domain, thereby enhancing feature extraction. The xLSTM architecture is specifically tailored to address the complexities of 12-lead ECG recordings by capturing both local and global signal features. Comprehensive experiments on the PTB-XL dataset reveal that our model achieves strong multi-label classification performance, while additional tests on the Georgia 12-Lead dataset underscore its robustness and efficiency. This approach significantly improves ECG classification accuracy, thereby advancing clinical diagnostics and patient care. The code will be publicly available upon acceptance.","authors":["Lei Kang","Xuanshuo Fu","Javier Vazquez-Corral","Ernest Valveny","Dimosthenis Karatzas"],"url":"https://arxiv.org/abs/2504.16101"}
{"created":"2025-04-24","title":"Phased Array Calibration based on Rotating-Element Harmonic Electric-Field Vector with Time Modulation","abstract":"Calibration is crucial for ensuring the performance of phased array since amplitude-phase imbalance between elements results in significant performance degradation. While amplitude-only calibration methods offer advantages when phase measurements are impractical, conventional approaches face two key challenges: they typically require high-resolution phase shifters and remain susceptible to phase errors inherent in these components. To overcome these limitations, we propose a Rotating element Harmonic Electric-field Vector (RHEV) strategy, which enables precise calibration through time modulation principles. The proposed technique functions as follows. Two 1-bit phase shifters are periodically phase-switched at the same frequency, each generating corresponding harmonics. By adjusting the relative delay between their modulation timings, the phase difference between the $+1$st harmonics produced by the two elements can be precisely controlled, utilizing the time-shift property of the Fourier transform. Furthermore, the +1st harmonic generated by sequential modulation of individual elements exhibits a linear relationship with the amplitude of the modulated element, enabling amplitude ambiguity resolution. The proposed RHEV-based calibration method generates phase shifts through relative timing delays rather than physical phase shifter adjustments, rendering it less susceptible to phase shift errors. Additionally, since the calibration process exclusively utilizes the $+1$st harmonic, which is produced solely by the modulated unit, the method demonstrates consistent performance regardless of array size. Extensive numerical simulations, practical in-channel and over-the-air (OTA) calibration experiments demonstrate the effectiveness and distinct advantages of the proposed method.","authors":["Shiyuan Li","Yuyue Zhou","Chi Zhang","Liang Kong","Kebin Liu","Yihan Xie","Chong He"],"url":"https://arxiv.org/abs/2504.16107"}
{"created":"2025-04-24","title":"A Self-supervised Learning Method for Raman Spectroscopy based on Masked Autoencoders","abstract":"Raman spectroscopy serves as a powerful and reliable tool for analyzing the chemical information of substances. The integration of Raman spectroscopy with deep learning methods enables rapid qualitative and quantitative analysis of materials. Most existing approaches adopt supervised learning methods. Although supervised learning has achieved satisfactory accuracy in spectral analysis, it is still constrained by costly and limited well-annotated spectral datasets for training. When spectral annotation is challenging or the amount of annotated data is insufficient, the performance of supervised learning in spectral material identification declines. In order to address the challenge of feature extraction from unannotated spectra, we propose a self-supervised learning paradigm for Raman Spectroscopy based on a Masked AutoEncoder, termed SMAE. SMAE does not require any spectral annotations during pre-training. By randomly masking and then reconstructing the spectral information, the model learns essential spectral features. The reconstructed spectra exhibit certain denoising properties, improving the signal-to-noise ratio (SNR) by more than twofold. Utilizing the network weights obtained from masked pre-training, SMAE achieves clustering accuracy of over 80% for 30 classes of isolated bacteria in a pathogenic bacterial dataset, demonstrating significant improvements compared to classical unsupervised methods and other state-of-the-art deep clustering methods. After fine-tuning the network with a limited amount of annotated data, SMAE achieves an identification accuracy of 83.90% on the test set, presenting competitive performance against the supervised ResNet (83.40%).","authors":["Pengju Ren","Ri-gui Zhou","Yaochong Li"],"url":"https://arxiv.org/abs/2504.16130"}
{"created":"2025-04-24","title":"Introduction to Quantum Machine Learning and Quantum Architecture Search","abstract":"Recent advancements in quantum computing (QC) and machine learning (ML) have fueled significant research efforts aimed at integrating these two transformative technologies. Quantum machine learning (QML), an emerging interdisciplinary field, leverages quantum principles to enhance the performance of ML algorithms. Concurrently, the exploration of systematic and automated approaches for designing high-performance quantum circuit architectures for QML tasks has gained prominence, as these methods empower researchers outside the quantum computing domain to effectively utilize quantum-enhanced tools. This tutorial will provide an in-depth overview of recent breakthroughs in both areas, highlighting their potential to expand the application landscape of QML across diverse fields.","authors":["Samuel Yen-Chi Chen","Zhiding Liang"],"url":"https://arxiv.org/abs/2504.16131"}
{"created":"2025-04-24","title":"A Non-Invasive Load Monitoring Method for Edge Computing Based on MobileNetV3 and Dynamic Time Regulation","abstract":"In recent years, non-intrusive load monitoring (NILM) technology has attracted much attention in the related research field by virtue of its unique advantage of utilizing single meter data to achieve accurate decomposition of device-level energy consumption. Cutting-edge methods based on machine learning and deep learning have achieved remarkable results in load decomposition accuracy by fusing time-frequency domain features. However, these methods generally suffer from high computational costs and huge memory requirements, which become the main obstacles for their deployment on resource-constrained microcontroller units (MCUs). To address these challenges, this study proposes an innovative Dynamic Time Warping (DTW) algorithm in the time-frequency domain and systematically compares and analyzes the performance of six machine learning techniques in home electricity scenarios. Through complete experimental validation on edge MCUs, this scheme successfully achieves a recognition accuracy of 95%. Meanwhile, this study deeply optimizes the frequency domain feature extraction process, which effectively reduces the running time by 55.55% and the storage overhead by about 34.6%. The algorithm performance will be further optimized in future research work. Considering that the elimination of voltage transformer design can significantly reduce the cost, the subsequent research will focus on this direction, and is committed to providing more cost-effective solutions for the practical application of NILM, and providing a solid theoretical foundation and feasible technical paths for the design of efficient NILM systems in edge computing environments.","authors":["Hangxu Liu","Yaojie Sun","Yu Wang"],"url":"https://arxiv.org/abs/2504.16142"}
{"created":"2025-04-24","title":"A Statistical Approach for Synthetic EEG Data Generation","abstract":"Electroencephalogram (EEG) data is crucial for diagnosing mental health conditions but is costly and time-consuming to collect at scale. Synthetic data generation offers a promising solution to augment datasets for machine learning applications. However, generating high-quality synthetic EEG that preserves emotional and mental health signals remains challenging. This study proposes a method combining correlation analysis and random sampling to generate realistic synthetic EEG data.","authors":["Gideon Vos","Maryam Ebrahimpour","Liza van Eijk","Zoltan Sarnyai","Mostafa Rahimi Azghadi"],"url":"https://arxiv.org/abs/2504.16143"}
{"created":"2025-04-24","title":"Aerial Active STAR-RIS-assisted Satellite-Terrestrial Covert Communications","abstract":"An integration of satellites and terrestrial networks is crucial for enhancing performance of next generation communication systems. However, the networks are hindered by the long-distance path loss and security risks in dense urban environments. In this work, we propose a satellite-terrestrial covert communication system assisted by the aerial active simultaneous transmitting and reflecting reconfigurable intelligent surface (AASTAR-RIS) to improve the channel capacity while ensuring the transmission covertness. Specifically, we first derive the minimal detection error probability (DEP) under the worst condition that the Warden has perfect channel state information (CSI). Then, we formulate an AASTAR-RIS-assisted satellite-terrestrial covert communication optimization problem (ASCCOP) to maximize the sum of the fair channel capacity for all ground users while meeting the strict covert constraint, by jointly optimizing the trajectory and active beamforming of the AASTAR-RIS. Due to the challenges posed by the complex and high-dimensional state-action spaces as well as the need for efficient exploration in dynamic environments, we propose a generative deterministic policy gradient (GDPG) algorithm, which is a generative deep reinforcement learning (DRL) method to solve the ASCCOP. Concretely, the generative diffusion model (GDM) is utilized as the policy representation of the algorithm to enhance the exploration process by generating diverse and high-quality samples through a series of denoising steps. Moreover, we incorporate an action gradient mechanism to accomplish the policy improvement of the algorithm, which refines the better state-action pairs through the gradient ascent. Simulation results demonstrate that the proposed approach significantly outperforms important benchmarks.","authors":["Chuang Zhang","Geng Sun","Jiahui Li","Jiacheng Wang","Ruichen Zhang","Dusit Niyato","Shiwen Mao","Tony Q. S. Quek"],"url":"https://arxiv.org/abs/2504.16146"}
{"created":"2025-04-24","title":"Heterogeneous networks in drug-target interaction prediction","abstract":"Drug discovery requires a tremendous amount of time and cost. Computational drug-target interaction prediction, a significant part of this process, can reduce these requirements by narrowing the search space for wet lab experiments. In this survey, we provide comprehensive details of graph machine learning-based methods in predicting drug-target interaction, as they have shown promising results in this field. These details include the overall framework, main contribution, datasets, and their source codes. The selected papers were mainly published from 2020 to 2024. Prior to discussing papers, we briefly introduce the datasets commonly used with these methods and measurements to assess their performance. Finally, future challenges and some crucial areas that need to be explored are discussed.","authors":["Mohammad Molaee","Nasrollah Moghadam Charkari"],"url":"https://arxiv.org/abs/2504.16152"}
{"created":"2025-04-24","title":"Behavior of prediction performance metrics with rare events","abstract":"Area under the receiving operator characteristic curve (AUC) is commonly reported alongside binary prediction models. However, there are concerns that AUC might be a misleading measure of prediction performance in the rare event setting. This setting is common since many events of clinical importance are rare events. We conducted a simulation study to determine when or whether AUC is unstable in the rare event setting. Specifically, we aimed to determine whether the bias and variance of AUC are driven by the number of events or the event rate. We also investigated the behavior of other commonly used measures of prediction performance, including positive predictive value, accuracy, sensitivity, and specificity. Our results indicate that poor AUC behavior -- as measured by empirical bias, variability of cross-validated AUC estimates, and empirical coverage of confidence intervals -- is driven by the minimum class size, not event rate. Performance of sensitivity is driven by the number of events, while that of specificity is driven by the number of non-events. Other measures, including positive predictive value and accuracy, depend on the event rate even in large samples. AUC is reliable in the rare event setting provided that the total number of events is moderately large.","authors":["Emily Minus","R. Yates Coley","Susan M. Shortreed","Brian D. Williamson"],"url":"https://arxiv.org/abs/2504.16185"}
{"created":"2025-04-24","title":"Probabilistic Emulation of the Community Radiative Transfer Model Using Machine Learning","abstract":"The continuous improvement in weather forecast skill over the past several decades is largely due to the increasing quantity of available satellite observations and their assimilation into operational forecast systems. Assimilating these observations requires observation operators in the form of radiative transfer models. Significant efforts have been dedicated to enhancing the computational efficiency of these models. Computational cost remains a bottleneck, and a large fraction of available data goes unused for assimilation. To address this, we used machine learning to build an efficient neural network based probabilistic emulator of the Community Radiative Transfer Model (CRTM), applied to the GOES Advanced Baseline Imager. The trained NN emulator predicts brightness temperatures output by CRTM and the corresponding error with respect to CRTM. RMSE of the predicted brightness temperature is 0.3 K averaged across all channels. For clear sky conditions, the RMSE is less than 0.1 K for 9 out of 10 infrared channels. The error predictions are generally reliable across a wide range of conditions. Explainable AI methods demonstrate that the trained emulator reproduces the relevant physics, increasing confidence that the model will perform well when presented with new data.","authors":["Lucas Howard","Aneesh C. Subramanian","Gregory Thompson","Benjamin Johnson","Thomas Auligne"],"url":"https://arxiv.org/abs/2504.16192"}
{"created":"2025-04-24","title":"Towards a Generalized Theory of Observers","abstract":"We propose a formal framework for understanding and unifying the concept of observers across physics, computer science, philosophy, and related fields. Building on cybernetic feedback models, we introduce an operational definition of minimal observers, explore their role in shaping foundational concepts, and identify what remains unspecified in their absence. Drawing upon insights from quantum gravity, digital physics, second-order cybernetics, and recent ruliological and pregeometric approaches, we argue that observers serve as indispensable reference points for measurement, reference frames, and the emergence of meaning. We show how this formalism sheds new light on debates related to consciousness, quantum measurement, and computational boundaries; by way of theorems on observer equivalences and complexity measures. This perspective opens new avenues for investigating how complexity and structure arise in both natural and artificial systems.","authors":["Hatem Elshatlawy","Dean Rickles","Xerxes D. Arsiwalla","Alexander Blum"],"url":"https://arxiv.org/abs/2504.16225"}
{"created":"2025-04-24","title":"Comprehensive Evaluation of Quantitative Measurements from Automated Deep Segmentations of PSMA PET/CT Images","abstract":"This study performs a comprehensive evaluation of quantitative measurements as extracted from automated deep-learning-based segmentation methods, beyond traditional Dice Similarity Coefficient assessments, focusing on six quantitative metrics, namely SUVmax, SUVmean, total lesion activity (TLA), tumor volume (TMTV), lesion count, and lesion spread. We analyzed 380 prostate-specific membrane antigen (PSMA) targeted [18F]DCFPyL PET/CT scans of patients with biochemical recurrence of prostate cancer, training deep neural networks, U-Net, Attention U-Net and SegResNet with four loss functions: Dice Loss, Dice Cross Entropy, Dice Focal Loss, and our proposed L1 weighted Dice Focal Loss (L1DFL). Evaluations indicated that Attention U-Net paired with L1DFL achieved the strongest correlation with the ground truth (concordance correlation = 0.90-0.99 for SUVmax and TLA), whereas models employing the Dice Loss and the other two compound losses, particularly with SegResNet, underperformed. Equivalence testing (TOST, alpha = 0.05, Delta = 20%) confirmed high performance for SUV metrics, lesion count and TLA, with L1DFL yielding the best performance. By contrast, tumor volume and lesion spread exhibited greater variability. Bland-Altman, Coverage Probability, and Total Deviation Index analyses further highlighted that our proposed L1DFL minimizes variability in quantification of the ground truth clinical measures. The code is publicly available at: https://github.com/ObedDzik/pca\\_segment.git.","authors":["Obed Korshie Dzikunu","Amirhossein Toosi","Shadab Ahamed","Sara Harsini","Francois Benard","Xiaoxiao Li","Arman Rahmim"],"url":"https://arxiv.org/abs/2504.16237"}
{"created":"2025-04-24","title":"A Geometric Approach to Problems in Optimization and Data Science","abstract":"We give new results for problems in computational and statistical machine learning using tools from high-dimensional geometry and probability.","authors":["Naren Sarayu Manoj"],"url":"https://arxiv.org/abs/2504.16270"}
{"created":"2025-04-24","title":"Detecting Correlation between Multiple Unlabeled Gaussian Networks","abstract":"This paper studies the hypothesis testing problem to determine whether m > 2 unlabeled graphs with Gaussian edge weights are correlated under a latent permutation. Previously, a sharp detection threshold for the correlation parameter \\rho was established by Wu, Xu and Yu for this problem when m = 2. Presently, their result is leveraged to derive necessary and sufficient conditions for general m. In doing so, an interval for \\rho is uncovered for which detection is impossible using 2 graphs alone but becomes possible with m > 2 graphs.","authors":["Taha Ameen","Bruce Hajek"],"url":"https://arxiv.org/abs/2504.16279"}
{"created":"2025-04-24","title":"Deep, data-driven modeling of room acoustics: literature review and research perspectives","abstract":"Our everyday auditory experience is shaped by the acoustics of the indoor environments in which we live. Room acoustics modeling is aimed at establishing mathematical representations of acoustic wave propagation in such environments. These representations are relevant to a variety of problems ranging from echo-aided auditory indoor navigation to restoring speech understanding in cocktail party scenarios. Many disciplines in science and engineering have recently witnessed a paradigm shift powered by deep learning (DL), and room acoustics research is no exception. The majority of deep, data-driven room acoustics models are inspired by DL-based speech and image processing, and hence lack the intrinsic space-time structure of acoustic wave propagation. More recently, DL-based models for room acoustics that include either geometric or wave-based information have delivered promising results, primarily for the problem of sound field reconstruction. In this review paper, we will provide an extensive and structured literature review on deep, data-driven modeling in room acoustics. Moreover, we position these models in a framework that allows for a conceptual comparison with traditional physical and data-driven models. Finally, we identify strengths and shortcomings of deep, data-driven room acoustics models and outline the main challenges for further research.","authors":["Toon van Waterschoot"],"url":"https://arxiv.org/abs/2504.16289"}
{"created":"2025-04-24","title":"Eigendecomposition Parameterization of Penalty Matrices for Enhanced Control Design: Aerospace Applications","abstract":"Modern control algorithms require tuning of square weight/penalty matrices appearing in quadratic functions/costs to improve performance and/or stability output. Due to simplicity in gain-tuning and enforcing positive-definiteness, diagonal penalty matrices are used extensively in control methods such as linear quadratic regulator (LQR), model predictive control, and Lyapunov-based control. In this paper, we propose an eigendecomposition approach to parameterize penalty matrices, allowing positive-definiteness with non-zero off-diagonal entries to be implicitly satisfied, which not only offers notable computational and implementation advantages, but broadens the class of achievable controls. We solve three control problems: 1) a variation of Zermelo's navigation problem, 2) minimum-energy spacecraft attitude control using both LQR and Lyapunov-based methods, and 3) minimum-fuel and minimum-time Lyapunov-based low-thrust trajectory design. Particle swarm optimization is used to optimize the decision variables, which will parameterize the penalty matrices. The results demonstrate improvements of up to 65% in the performance objective in the example problems utilizing the proposed method.","authors":["Nicholas P. Nurre","Ehsan Taheri"],"url":"https://arxiv.org/abs/2504.16328"}
{"created":"2025-04-24","title":"Deep Neural Network Emulation of the Quantum-Classical Transition via Learned Wigner Function Dynamics","abstract":"The emergence of classical behavior from quantum mechanics as Planck's constant $\\hbar$ approaches zero remains a fundamental challenge in physics [1-3]. This paper introduces a novel approach employing deep neural networks to directly learn the dynamical mapping from initial quantum state parameters (for Gaussian wave packets of the one-dimensional harmonic oscillator) and $\\hbar$ to the parameters of the time-evolved Wigner function in phase space [4-6]. A comprehensive dataset of analytically derived time-evolved Wigner functions was generated, and a deep feedforward neural network with an enhanced architecture was successfully trained for this prediction task, achieving a final training loss of ~ 0.0390. The network demonstrates a significant and previously unrealized ability to accurately capture the underlying mapping of the Wigner function dynamics. This allows for a direct emulation of the quantum-classical transition by predicting the evolution of phase-space distributions as $\\hbar$ is systematically varied. The implications of these findings for providing a new computational lens on the emergence of classicality are discussed, highlighting the potential of this direct phase-space learning approach for studying fundamental aspects of quantum mechanics. This work presents a significant advancement beyond previous efforts that focused on learning observable mappings [7], offering a direct route via the phase-space representation.","authors":["Kamran Majid"],"url":"https://arxiv.org/abs/2504.16334"}
{"created":"2025-04-24","title":"QAOA-GPT: Efficient Generation of Adaptive and Regular Quantum Approximate Optimization Algorithm Circuits","abstract":"Quantum computing has the potential to improve our ability to solve certain optimization problems that are computationally difficult for classical computers, by offering new algorithmic approaches that may provide speedups under specific conditions. In this work, we introduce QAOA-GPT, a generative framework that leverages Generative Pretrained Transformers (GPT) to directly synthesize quantum circuits for solving quadratic unconstrained binary optimization problems, and demonstrate it on the MaxCut problem on graphs. To diversify the training circuits and ensure their quality, we have generated a synthetic dataset using the adaptive QAOA approach, a method that incrementally builds and optimizes problem-specific circuits. The experiments conducted on a curated set of graph instances demonstrate that QAOA-GPT, generates high quality quantum circuits for new problem instances unseen in the training as well as successfully parametrizes QAOA. Our results show that using QAOA-GPT to generate quantum circuits will significantly decrease both the computational overhead of classical QAOA and adaptive approaches that often use gradient evaluation to generate the circuit and the classical optimization of the circuit parameters. Our work shows that generative AI could be a promising avenue to generate compact quantum circuits in a scalable way.","authors":["Ilya Tyagin","Marwa H. Farag","Kyle Sherbert","Karunya Shirali","Yuri Alexeev","Ilya Safro"],"url":"https://arxiv.org/abs/2504.16350"}
{"created":"2025-04-24","title":"Covariate-dependent Graphical Model Estimation via Neural Networks with Statistical Guarantees","abstract":"Graphical models are widely used in diverse application domains to model the conditional dependencies amongst a collection of random variables. In this paper, we consider settings where the graph structure is covariate-dependent, and investigate a deep neural network-based approach to estimate it. The method allows for flexible functional dependency on the covariate, and fits the data reasonably well in the absence of a Gaussianity assumption. Theoretical results with PAC guarantees are established for the method, under assumptions commonly used in an Empirical Risk Minimization framework. The performance of the proposed method is evaluated on several synthetic data settings and benchmarked against existing approaches. The method is further illustrated on real datasets involving data from neuroscience and finance, respectively, and produces interpretable results.","authors":["Jiahe Lin","Yikai Zhang","George Michailidis"],"url":"https://arxiv.org/abs/2504.16356"}
{"created":"2025-04-24","title":"The Safety-Privacy Tradeoff in Linear Bandits","abstract":"We consider a collection of linear stochastic bandit problems, each modeling the random response of different agents to proposed interventions, coupled together by a global safety constraint. We assume a central coordinator must choose actions to play on each bandit with the objective of regret minimization, while also ensuring that the expected response of all agents satisfies the global safety constraints at each round, in spite of uncertainty about the bandits' parameters. The agents consider their observed responses to be private and in order to protect their sensitive information, the data sharing with the central coordinator is performed under local differential privacy (LDP). However, providing higher level of privacy to different agents would have consequences in terms of safety and regret. We formalize these tradeoffs by building on the notion of the sharpness of the safety set - a measure of how the geometric properties of the safe set affects the growth of regret - and propose a unilaterally unimprovable vector of privacy levels for different agents given a maximum regret budget.","authors":["Arghavan Zibaie","Spencer Hutchinson","Ramtin Pedarsani","Mahnoosh Alizadeh"],"url":"https://arxiv.org/abs/2504.16371"}
{"created":"2025-04-24","title":"PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems","abstract":"Characterizing conformational transitions in physical systems remains a fundamental challenge in the computational sciences. Traditional sampling methods like molecular dynamics (MD) or MCMC often struggle with the high-dimensional nature of molecular systems and the high energy barriers of transitions between stable states. While these transitions are rare events in simulation timescales, they often represent the most biologically significant processes - for example, the conformational change of an ion channel protein from its closed to open state, which controls cellular ion flow and is crucial for neural signaling. Such transitions in real systems may take milliseconds to seconds but could require months or years of continuous simulation to observe even once. We present a method that reformulates transition path generation as a continuous optimization problem solved through physics-informed neural networks (PINNs) inspired by string methods for minimum-energy path (MEP) generation. By representing transition paths as implicit neural functions and leveraging automatic differentiation with differentiable molecular dynamics force fields, our method enables the efficient discovery of physically realistic transition pathways without requiring expensive path sampling. We demonstrate our method's effectiveness on two proteins, including an explicitly hydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300 atoms.","authors":["Magnus Petersen","Roberto Covino"],"url":"https://arxiv.org/abs/2504.16381"}
{"created":"2025-04-24","title":"Public-Key Quantum Fire and Key-Fire From Classical Oracles","abstract":"Quantum fire was recently formalized by Bostanci, Nehoran and Zhandry (STOC 25). This notion considers a distribution of quantum states that can be efficiently cloned, but cannot be converted into a classical string. Previously, work of Nehoran and Zhandry (ITCS 24) showed how to construct quantum fire relative to an inefficient unitary oracle. Later, the work of Bostanci, Nehoran, Zhandry gave a candidate construction based on group action assumptions, and proved the correctness of their scheme; however, even in the classical oracle model they only conjectured the security, and no security proof was given.","authors":["Alper \\c{C}akan","Vipul Goyal","Omri Shmueli"],"url":"https://arxiv.org/abs/2504.16407"}
{"created":"2025-04-24","title":"SoCov: Semi-Orthogonal Parametric Pooling of Covariance Matrix for Speaker Recognition","abstract":"In conventional deep speaker embedding frameworks, the pooling layer aggregates all frame-level features over time and computes their mean and standard deviation statistics as inputs to subsequent segment-level layers. Such statistics pooling strategy produces fixed-length representations from variable-length speech segments. However, this method treats different frame-level features equally and discards covariance information. In this paper, we propose the Semi-orthogonal parameter pooling of Covariance matrix (SoCov) method. The SoCov pooling computes the covariance matrix from the self-attentive frame-level features and compresses it into a vector using the semi-orthogonal parametric vectorization, which is then concatenated with the weighted standard deviation vector to form inputs to the segment-level layers. Deep embedding based on SoCov is called ``sc-vector''. The proposed sc-vector is compared to several different baselines on the SRE21 development and evaluation sets. The sc-vector system significantly outperforms the conventional x-vector system, with a relative reduction in EER of 15.5% on SRE21Eval. When using self-attentive deep feature, SoCov helps to reduce EER on SRE21Eval by about 30.9% relatively to the conventional ``mean + standard deviation'' statistics.","authors":["Rongjin Li","Weibin Zhang","Dongpeng Chen","Jintao Kang","Xiaofen Xing"],"url":"https://arxiv.org/abs/2504.16441"}
{"created":"2025-04-24","title":"HAQA: A Hardware-Guided and Fidelity-Aware Strategy for Efficient Qubit Mapping Optimization","abstract":"Quantum algorithms rely on quantum computers for implementation, but the physical connectivity constraints of modern quantum processors impede the efficient realization of quantum algorithms. Qubit mapping, a critical technology for practical quantum computing applications, directly determines the execution efficiency and feasibility of algorithms on superconducting quantum processors. Existing mapping methods overlook intractable quantum hardware fidelity characteristics, reducing circuit execution quality. They also exhibit prolonged solving times or even failure to complete when handling large-scale quantum architectures, compromising efficiency. To address these challenges, we propose a novel qubit mapping method HAQA. HAQA first introduces a community-based iterative region identification strategy leveraging hardware connection topology, achieving effective dimensionality reduction of mapping space. This strategy avoids global search procedures, with complexity analysis demonstrating quadratic polynomial-level acceleration. Furthermore, HAQA implements a hardware-characteristic-based region evaluation mechanism, enabling quantitative selection of mapping regions based on fidelity metrics. This approach effectively integrates hardware fidelity information into the mapping process, enabling fidelity-aware qubit allocation. Experimental results demonstrate that HAQA significantly improves solving speed and fidelity while ensuring solution quality. When applied to state-of-the-art quantum mapping techniques Qsynth-v2 and TB-OLSQ2, HAQA achieves acceleration ratios of 632.76 and 286.87 respectively, while improving fidelity by up to 52.69% and 238.28%","authors":["Wenjie Sun","Xiaoyu Li","Lianhui Yu","Zhigang Wang","Geng Chen","Desheng Zheng","Guowu Yang"],"url":"https://arxiv.org/abs/2504.16468"}
{"created":"2025-04-24","title":"The Dance of Atoms-De Novo Protein Design with Diffusion Model","abstract":"The de novo design of proteins refers to creating proteins with specific structures and functions that do not naturally exist. In recent years, the accumulation of high-quality protein structure and sequence data and technological advancements have paved the way for the successful application of generative artificial intelligence (AI) models in protein design. These models have surpassed traditional approaches that rely on fragments and bioinformatics. They have significantly enhanced the success rate of de novo protein design, and reduced experimental costs, leading to breakthroughs in the field. Among various generative AI models, diffusion models have yielded the most promising results in protein design. In the past two to three years, more than ten protein design models based on diffusion models have emerged. Among them, the representative model, RFDiffusion, has demonstrated success rates in 25 protein design tasks that far exceed those of traditional methods, and other AI-based approaches like RFjoint and hallucination. This review will systematically examine the application of diffusion models in generating protein backbones and sequences. We will explore the strengths and limitations of different models, summarize successful cases of protein design using diffusion models, and discuss future development directions.","authors":["Yujie Qin","Ming He","Changyong Yu","Ming Ni","Xian Liu","Xiaochen Bo"],"url":"https://arxiv.org/abs/2504.16479"}
{"created":"2025-04-24","title":"Breaking scaling relations with inverse catalysts: a machine learning exploration of trends in $\\mathrm{CO_2}$ hydrogenation energy barriers","abstract":"The conversion of $\\mathrm{CO_2}$ into useful products such as methanol is a key strategy for abating climate change and our dependence on fossil fuels. Developing new catalysts for this process is costly and time-consuming and can thus benefit from computational exploration of possible active sites. However, this is complicated by the complexity of the materials and reaction networks. Here, we present a workflow for exploring transition states of elementary reaction steps at inverse catalysts, which is based on the training of a neural network-based machine learning interatomic potential. We focus on the crucial formate intermediate and its formation over nanoclusters of indium oxide supported on Cu(111). The speedup compared to an approach purely based on density functional theory allows us to probe a wide variety of active sites found at nanoclusters of different sizes and stoichiometries. Analysis of the obtained set of transition state geometries reveals different structure--activity trends at the edge or interior of the nanoclusters. Furthermore, the identified geometries allow for the breaking of linear scaling relations, which could be a key underlying reason for the excellent catalytic performance of inverse catalysts observed in experiments.","authors":["Luuk H. E. Kempen","Marius Juul Nielsen","Mie Andersen"],"url":"https://arxiv.org/abs/2504.16493"}
{"created":"2025-04-24","title":"Intelligent Depression Prevention via LLM-Based Dialogue Analysis: Overcoming the Limitations of Scale-Dependent Diagnosis through Precise Emotional Pattern Recognition","abstract":"Existing depression screening predominantly relies on standardized questionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates (18-34% in clinical studies) due to their static, symptom-counting nature and susceptibility to patient recall bias. This paper presents an AI-powered depression prevention system that leverages large language models (LLMs) to analyze real-time conversational cues--including subtle emotional expressions (e.g., micro-sentiment shifts, self-referential language patterns)--for more accurate and dynamic mental state assessment. Our system achieves three key innovations: (1) Continuous monitoring through natural dialogue, detecting depression-indicative linguistic features (anhedonia markers, hopelessness semantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk stratification that updates severity levels based on conversational context, reducing false positives by 41% compared to scale-based thresholds; and (3) Personalized intervention strategies tailored to users' emotional granularity, demonstrating 2.3x higher adherence rates than generic advice. Clinical validation with 450 participants shows the system identifies 92% of at-risk cases missed by traditional scales, while its explainable AI interface bridges the gap between automated analysis and clinician judgment. This work establishes conversational AI as a paradigm shift from episodic scale-dependent diagnosis to continuous, emotionally intelligent mental health monitoring.","authors":["Zhenguang Zhong","Zhixuan Wang"],"url":"https://arxiv.org/abs/2504.16504"}
{"created":"2025-04-24","title":"Confidence Sequences for Generalized Linear Models via Regret Analysis","abstract":"We develop a methodology for constructing confidence sets for parameters of statistical models via a reduction to sequential prediction. Our key observation is that for any generalized linear model (GLM), one can construct an associated game of sequential probability assignment such that achieving low regret in the game implies a high-probability upper bound on the excess likelihood of the true parameter of the GLM. This allows us to develop a scheme that we call online-to-confidence-set conversions, which effectively reduces the problem of proving the desired statistical claim to an algorithmic question. We study two varieties of this conversion scheme: 1) analytical conversions that only require proving the existence of algorithms with low regret and provide confidence sets centered at the maximum-likelihood estimator 2) algorithmic conversions that actively leverage the output of the online algorithm to construct confidence sets (and may be centered at other, adaptively constructed point estimators). The resulting methodology recovers all state-of-the-art confidence set constructions within a single framework, and also provides several new types of confidence sets that were previously unknown in the literature.","authors":["Eugenio Clerico","Hamish Flynn","Wojciech Kot{\\l}owski","Gergely Neu"],"url":"https://arxiv.org/abs/2504.16555"}
{"created":"2025-04-24","title":"Uplink Sum Rate Maximization for Pinching Antenna-Assisted Multiuser MISO","abstract":"This article investigates the application of pinching-antenna systems (PASS) in multiuser multiple-input single-output (MISO) communications. Two sum-rate maximization problems are formulated under minimum mean square error (MMSE) decoding, with and without successive interference cancellation (SIC). To address the joint optimization of pinching antenna locations and user transmit powers, a fractional programming-based approach is proposed. Numerical results validate the effectiveness of the proposed method and show that PASS can significantly enhance uplink sum-rate performance compared to conventional fixed-antenna designs.","authors":["Jiarui Zhang","Hao Xu","Chongjun Ouyang","Qiuyun Zou","Hongwen Yang"],"url":"https://arxiv.org/abs/2504.16577"}
{"created":"2025-04-24","title":"Optimization Framework for Reducing Mid-circuit Measurements and Resets","abstract":"The paper addresses the optimization of dynamic circuits in quantum computing, with a focus on reducing the cost of mid-circuit measurements and resets. We extend the probabilistic circuit model (PCM) and implement an optimization framework that targets both mid-circuit measurements and resets. To overcome the limitation of the prior PCM-based pass, where optimizations are only possible on pure single-qubit states, we incorporate circuit synthesis to enable optimizations on multi-qubit states. With a parameter $n_{pcm}$, our framework balances optimization level against resource usage.We evaluate our framework using a large dataset of randomly generated dynamic circuits. Experimental results demonstrate that our method is highly effective in reducing mid-circuit measurements and resets. In our demonstrative example, when applying our optimization framework to the Bernstein-Vazirani algorithm after employing qubit reuse, we significantly reduce its runtime overhead by removing all of the resets.","authors":["Yanbin Chen","Innocenzo Fulginiti","Christian B. Mendl"],"url":"https://arxiv.org/abs/2504.16579"}
{"created":"2025-04-24","title":"Revisiting Regret Benchmarks in Online Non-Stochastic Control","abstract":"In the online non-stochastic control problem, an agent sequentially selects control inputs for a linear dynamical system when facing unknown and adversarially selected convex costs and disturbances. A common metric for evaluating control policies in this setting is policy regret, defined relative to the best-in-hindsight linear feedback controller. However, for general convex costs, this benchmark may be less meaningful since linear controllers can be highly suboptimal. To address this, we introduce an alternative, more suitable benchmark--the performance of the best fixed input. We show that this benchmark can be viewed as a natural extension of the standard benchmark used in online convex optimization and propose a novel online control algorithm that achieves sublinear regret with respect to this new benchmark. We also discuss the connections between our method and the original one proposed by Agarwal et al. in their seminal work introducing the online non-stochastic control problem, and compare the performance of both approaches through numerical simulations.","authors":["Vijeth Hebbar","C\\'edric Langbort"],"url":"https://arxiv.org/abs/2504.16581"}
{"created":"2025-04-24","title":"Efficient Algorithms for Minimal Matroid Extensions and Irreducible Decompositions of Circuit Varieties","abstract":"We introduce an efficient method for decomposing the circuit variety of a given matroid $M$, based on an algorithm that identifies its minimal extensions. These extensions correspond to the smallest elements above $M$ in the poset defined by the dependency order. We apply our algorithm to several classical configurations: the V\\'amos matroid, the unique Steiner quadruple system $S(3,4,8)$, the projective and affine planes, the dual of the Fano matroid, and the dual of the graphic matroid of $K_{3,3}$. In each case, we compute the minimal irreducible decomposition of their circuit varieties.","authors":["Emiliano Liwski","Fatemeh Mohammadi","R\\'emi Pr\\'ebet"],"url":"https://arxiv.org/abs/2504.16632"}
{"created":"2025-04-24","title":"Online and feasible presentability: from trees to modal algebras","abstract":"We investigate whether every computable member of a given class of structures admits a fully primitive recursive (also known as punctual) or fully P-TIME copy. A class with this property is referred to as punctually robust or P-TIME robust, respectively. We present both positive and negative results for structures corresponding to well-known representations of trees, such as binary trees, ordered trees, sequential (or prefix) trees, and partially ordered (poset) trees. A corollary of one of our results on trees is that semilattices and lattices are not punctually robust. In the main result of the paper, we demonstrate that, unlike Boolean algebras, modal algebras - that is, Boolean algebras with modality - are not punctually robust. The question of whether distributive lattices are punctually robust remains open. The paper contributes to a decades-old program on effective and feasible algebra, which has recently gained momentum due to rapid developments in punctual structure theory and its connections to online presentations of structures.","authors":["Nikolay Bazhenov","Dariusz Kaloci\\'nski","Micha{\\l} Wroc{\\l}awski"],"url":"https://arxiv.org/abs/2504.16663"}
{"created":"2025-04-24","title":"Logic and Concepts in the 2-category of Topoi","abstract":"We use Kan injectivity to axiomatise concepts in the 2-category of topoi. We showcase the expressivity of this language through many examples, and we establish some aspects of the formal theory of Kan extension in this 2-category (pointwise Kan extensions, fully faithful morphisms, etc.). We use this technology to introduce fragments of geometric logic, and we accommodate essentially algebraic, disjunctive, regular, and coherent logic in our framework, together with some more exotic examples. We show that each fragment $\\mathcal{H}$ in our sense identifies a lax-idempotent (relative) pseudomonad $\\mathsf{T}^{\\mathcal{H}}$ on $\\mathsf{lex}$, the $2$-category of finitely complete categories. We show that the algebras for $\\mathsf{T}^{\\mathcal{H}}$ admit a notion of classifying topos, for which we deliver several Diaconescu-type results. The construction of classifying topoi allows us to define conceptually complete fragments of geometric logic.","authors":["Ivan Di Liberti","Lingyuan Ye"],"url":"https://arxiv.org/abs/2504.16690"}
{"created":"2025-04-24","title":"On deciding transcendence of power series","abstract":"It is well known that algebraic power series are differentially finite (D-finite): they satisfy linear differential equations with polynomial coefficients. The converse problem, whether a given D-finite power series is algebraic or transcendental, is notoriously difficult. We prove that this problem is decidable: we give two theoretical algorithms and a transcendence test that is efficient in practice.","authors":["Alin Bostan","Bruno Salvy","Michael F. Singer"],"url":"https://arxiv.org/abs/2504.16697"}
{"created":"2025-04-24","title":"Resource Reduction in Multiparty Quantum Secret Sharing of both Classical and Quantum Information under Noisy Scenario","abstract":"Quantum secret sharing (QSS) enables secure distribution of information among multiple parties but remains vulnerable to noise. We analyze the effects of bit-flip, phase-flip, and amplitude damping noise on the multiparty QSS for classical message (QSSCM) and secret sharing of quantum information (SSQI) protocols proposed by Zhang et al. (Phys. Rev. A, 71:044301, 2005). To scale down these effects, we introduce an efficient quantum error correction (QEC) scheme based on a simplified version of Shor's code. Leveraging the specific structure of the QSS protocols, we reduce the qubit overhead from the standard 9 of Shor's code to as few as 3 while still achieving lower average error rates than existing QEC methods. Thus, our approach can also be adopted for other single-qubit-based quantum protocols. Simulations demonstrate that our approach significantly enhances the protocols' resilience, improving their practicality for real-world deployment.","authors":["Nirupam Basak","Goutam Paul"],"url":"https://arxiv.org/abs/2504.16709"}
{"created":"2025-04-24","title":"Frequency-Compensated Network for Daily Arctic Sea Ice Concentration Prediction","abstract":"Accurately forecasting sea ice concentration (SIC) in the Arctic is critical to global ecosystem health and navigation safety. However, current methods still is confronted with two challenges: 1) these methods rarely explore the long-term feature dependencies in the frequency domain. 2) they can hardly preserve the high-frequency details, and the changes in the marginal area of the sea ice cannot be accurately captured. To this end, we present a Frequency-Compensated Network (FCNet) for Arctic SIC prediction on a daily basis. In particular, we design a dual-branch network, including branches for frequency feature extraction and convolutional feature extraction. For frequency feature extraction, we design an adaptive frequency filter block, which integrates trainable layers with Fourier-based filters. By adding frequency features, the FCNet can achieve refined prediction of edges and details. For convolutional feature extraction, we propose a high-frequency enhancement block to separate high and low-frequency information. Moreover, high-frequency features are enhanced via channel-wise attention, and temporal attention unit is employed for low-frequency feature extraction to capture long-range sea ice changes. Extensive experiments are conducted on a satellite-derived daily SIC dataset, and the results verify the effectiveness of the proposed FCNet. Our codes and data will be made public available at: https://github.com/oucailab/FCNet .","authors":["Jialiang Zhang","Feng Gao","Yanhai Gan","Junyu Dong","Qian Du"],"url":"https://arxiv.org/abs/2504.16745"}
{"created":"2025-04-24","title":"Projective Variety Recovery from Unknown Linear Projections","abstract":"We study how a smooth irreducible algebraic variety $X$ of dimension $n$ embedded in $\\mathbb{C} \\mathbb{P}^{m}$ (with $m \\geq n+2$), which degree is $d$, can be recovered using two projections from unknown points onto unknown hyperplanes. The centers and the hyperplanes of projection are unknown: the only input is the defining equations of each projected varieties. We show how both the projection operators and the variety in $\\mathbb{C} \\mathbb{P}^{m}$ can be recovered modulo some action of the group of projective transformations of $\\mathbb{C} \\mathbb{P}^{m}$. This configuration generalizes results obtained in the context of curves embedded in $\\mathbb{C} \\mathbb{P}^3$ and results concerning surfaces embedded in $\\mathbb{C} \\mathbb{P}^4$.","authors":["Yirmeyahy Kaminski"],"url":"https://arxiv.org/abs/2504.16771"}
{"created":"2025-04-24","title":"Advanced Chest X-Ray Analysis via Transformer-Based Image Descriptors and Cross-Model Attention Mechanism","abstract":"The examination of chest X-ray images is a crucial component in detecting various thoracic illnesses. This study introduces a new image description generation model that integrates a Vision Transformer (ViT) encoder with cross-modal attention and a GPT-4-based transformer decoder. The ViT captures high-quality visual features from chest X-rays, which are fused with text data through cross-modal attention to improve the accuracy, context, and richness of image descriptions. The GPT-4 decoder transforms these fused features into accurate and relevant captions. The model was tested on the National Institutes of Health (NIH) and Indiana University (IU) Chest X-ray datasets. On the IU dataset, it achieved scores of 0.854 (B-1), 0.883 (CIDEr), 0.759 (METEOR), and 0.712 (ROUGE-L). On the NIH dataset, it achieved the best performance on all metrics: BLEU 1--4 (0.825, 0.788, 0.765, 0.752), CIDEr (0.857), METEOR (0.726), and ROUGE-L (0.705). This framework has the potential to enhance chest X-ray evaluation, assisting radiologists in more precise and efficient diagnosis.","authors":["Lakshita Agarwal","Bindu Verma"],"url":"https://arxiv.org/abs/2504.16774"}
{"created":"2025-04-24","title":"Radiometer Calibration using Machine Learning","abstract":"Radiometers are crucial instruments in radio astronomy, forming the primary component of nearly all radio telescopes. They measure the intensity of electromagnetic radiation, converting this radiation into electrical signals. A radiometer's primary components are an antenna and a Low Noise Amplifier (LNA), which is the core of the ``receiver'' chain. Instrumental effects introduced by the receiver are typically corrected or removed during calibration. However, impedance mismatches between the antenna and receiver can introduce unwanted signal reflections and distortions. Traditional calibration methods, such as Dicke switching, alternate the receiver input between the antenna and a well-characterised reference source to mitigate errors by comparison. Recent advances in Machine Learning (ML) offer promising alternatives. Neural networks, which are trained using known signal sources, provide a powerful means to model and calibrate complex systems where traditional analytical approaches struggle. These methods are especially relevant for detecting the faint sky-averaged 21-cm signal from atomic hydrogen at high redshifts. This is one of the main challenges in observational Cosmology today. Here, for the first time, we introduce and test a machine learning-based calibration framework capable of achieving the precision required for radiometric experiments aiming to detect the 21-cm line.","authors":["S. A. K. Leeney","H. T. J. Bevins","E. de Lera Acedo","W. J. Handley","C. Kirkham","R. S. Patel","J. Zhu","D. Molnar","J. Cumner","D. Anstey","K. Artuc","G. Bernardi","M. Bucher","S. Carey","J. Cavillot","R. Chiello","W. Croukamp","D. I. L. de Villiers","J. A. Ely","A. Fialkov","T. Gessey-Jones","G. Kulkarni","A. Magro","P. D. Meerburg","S. Mittal","J. H. N. Pattison","S. Pegwal","C. M. Pieterse","J. R. Pritchard","E. Puchwein","N. Razavi-Ghods","I. L. V. Roque","A. Saxena","K. H. Scheutwinkel","P. Scott","E. Shen","P. H. Sims","M. Spinelli"],"url":"https://arxiv.org/abs/2504.16791"}
{"created":"2025-04-24","title":"On graphs with a simple structure of maximal cliques","abstract":"We say that a hereditary graph class $\\mathcal{G}$ is \\emph{clique-sparse} if there is a constant $k=k(\\mathcal{G})$ such that for every graph $G\\in\\mathcal{G}$, every vertex of $G$ belongs to at most $k$ maximal cliques, and any maximal clique of $G$ can be intersected in at most $k$ different ways by other maximal cliques.","authors":["J. Pascal Gollin","Meike Hatzel","Sebastian Wiederrecht"],"url":"https://arxiv.org/abs/2504.16863"}
{"created":"2025-04-24","title":"Common Functional Decompositions Can Mis-attribute Differences in Outcomes Between Populations","abstract":"In science and social science, we often wish to explain why an outcome is different in two populations. For instance, if a jobs program benefits members of one city more than another, is that due to differences in program participants (particular covariates) or the local labor markets (outcomes given covariates)? The Kitagawa-Oaxaca-Blinder (KOB) decomposition is a standard tool in econometrics that explains the difference in the mean outcome across two populations. However, the KOB decomposition assumes a linear relationship between covariates and outcomes, while the true relationship may be meaningfully nonlinear. Modern machine learning boasts a variety of nonlinear functional decompositions for the relationship between outcomes and covariates in one population. It seems natural to extend the KOB decomposition using these functional decompositions. We observe that a successful extension should not attribute the differences to covariates -- or, respectively, to outcomes given covariates -- if those are the same in the two populations. Unfortunately, we demonstrate that, even in simple examples, two common decompositions -- functional ANOVA and Accumulated Local Effects -- can attribute differences to outcomes given covariates, even when they are identical in two populations. We provide a characterization of when functional ANOVA misattributes, as well as a general property that any discrete decomposition must satisfy to avoid misattribution. We show that if the decomposition is independent of its input distribution, it does not misattribute. We further conjecture that misattribution arises in any reasonable additive decomposition that depends on the distribution of the covariates.","authors":["Manuel Quintero","William T. Stephenson","Advik Shreekumar","Tamara Broderick"],"url":"https://arxiv.org/abs/2504.16864"}
{"created":"2025-04-24","title":"Exploring zero-shot structure-based protein fitness prediction","abstract":"The ability to make zero-shot predictions about the fitness consequences of protein sequence changes with pre-trained machine learning models enables many practical applications. Such models can be applied for downstream tasks like genetic variant interpretation and protein engineering without additional labeled data. The advent of capable protein structure prediction tools has led to the availability of orders of magnitude more precomputed predicted structures, giving rise to powerful structure-based fitness prediction models. Through our experiments, we assess several modeling choices for structure-based models and their effects on downstream fitness prediction. Zero-shot fitness prediction models can struggle to assess the fitness landscape within disordered regions of proteins, those that lack a fixed 3D structure. We confirm the importance of matching protein structures to fitness assays and find that predicted structures for disordered regions can be misleading and affect predictive performance. Lastly, we evaluate an additional structure-based model on the ProteinGym substitution benchmark and show that simple multi-modal ensembles are strong baselines.","authors":["Arnav Sharma","Anthony Gitter"],"url":"https://arxiv.org/abs/2504.16886"}
{"created":"2025-04-24","title":"The Sponge is Quantum Indifferentiable","abstract":"The sponge is a cryptographic construction that turns a public permutation into a hash function. When instantiated with the Keccak permutation, the sponge forms the NIST SHA-3 standard. SHA-3 is a core component of most post-quantum public-key cryptography schemes slated for worldwide adoption.","authors":["Gorjan Alagic","Joseph Carolan","Christian Majenz","Saliha Tokat"],"url":"https://arxiv.org/abs/2504.16887"}
{"created":"2025-04-24","title":"Linear convergence of a one-cut conditional gradient method for total variation regularization","abstract":"We introduce a fully-corrective generalized conditional gradient method for convex minimization problems involving total variation regularization on multidimensional domains. It relies on alternating between updating an active set of subsets of the spatial domain as well as of an iterate given by a conic combination of the associated characteristic functions. Different to previous approaches in the same spirit, the computation of a new candidate set only requires the solution of one prescribed mean curvature problem instead of the resolution of a fractional minimization task analogous to finding a generalized Cheeger set. After discretization, the former can be realized by a single run of a graph cut algorithm leading to significant speedup in practice. We prove the global sublinear convergence of the resulting method, under mild assumptions, and its asymptotic linear convergence in a more restrictive two-dimensional setting which uses results of stability of surfaces of prescribed curvature under perturbations of the curvature. Finally, we numerically demonstrate this convergence behavior in some model PDE-constrained minimization problems.","authors":["Giacomo Cristinelli","Jos\\'e A. Iglesias","Daniel Walter"],"url":"https://arxiv.org/abs/2504.16899"}
{"created":"2025-04-24","title":"Application of an attention-based CNN-BiLSTM framework for in vivo two-photon calcium imaging of neuronal ensembles: decoding complex bilateral forelimb movements from unilateral M1","abstract":"Decoding behavior, such as movement, from multiscale brain networks remains a central objective in neuroscience. Over the past decades, artificial intelligence and machine learning have played an increasingly significant role in elucidating the neural mechanisms underlying motor function. The advancement of brain-monitoring technologies, capable of capturing complex neuronal signals with high spatial and temporal resolution, necessitates the development and application of more sophisticated machine learning models for behavioral decoding. In this study, we employ a hybrid deep learning framework, an attention-based CNN-BiLSTM model, to decode skilled and complex forelimb movements using signals obtained from in vivo two-photon calcium imaging. Our findings demonstrate that the intricate movements of both ipsilateral and contralateral forelimbs can be accurately decoded from unilateral M1 neuronal ensembles. These results highlight the efficacy of advanced hybrid deep learning models in capturing the spatiotemporal dependencies of neuronal networks activity linked to complex movement execution.","authors":["Ghazal Mirzaee","Jonathan Chang","Shahrzad Latifi"],"url":"https://arxiv.org/abs/2504.16917"}
{"created":"2025-04-24","title":"Multiplicative weights, equalizers, and P=PPAD","abstract":"We show that, by using multiplicative weights in a game-theoretic thought experiment (and an important convexity result on the composition of multiplicative weights with the relative entropy function), a symmetric bimatrix game (that is, a bimatrix matrix wherein the payoff matrix of each player is the transpose of the payoff matrix of the other) either has an interior symmetric equilibrium or there is a pure strategy that is weakly dominated by some mixed strategy. Weakly dominated pure strategies can be detected and eliminated in polynomial time by solving a linear program. Furthermore, interior symmetric equilibria are a special case of a more general notion, namely, that of an \"equalizer,\" which can also be computed efficiently in polynomial time by solving a linear program. An elegant \"symmetrization method\" of bimatrix games [Jurg et al., 1992] and the well-known PPAD-completeness results on equilibrium computation in bimatrix games [Daskalakis et al., 2009, Chen et al., 2009] imply then the compelling P = PPAD.","authors":["Ioannis Avramopoulos"],"url":"https://arxiv.org/abs/1609.08934"}
{"created":"2025-04-24","title":"A robust graph-based approach to observational equivalence","abstract":"We propose a new step-wise approach to proving observational equivalence, and in particular reasoning about fragility of observational equivalence. Our approach is based on what we call local reasoning. The local reasoning exploits the graphical concept of neighbourhood, and it extracts a new, formal, concept of robustness as a key sufficient condition of observational equivalence. Moreover, our proof methodology is capable of proving a generalised notion of observational equivalence. The generalised notion can be quantified over syntactically restricted contexts instead of all contexts, and also quantitatively constrained in terms of the number of reduction steps. The operational machinery we use is given by a hypergraph-rewriting abstract machine inspired by Girard's Geometry of Interaction. The behaviour of language features, including function abstraction and application, is provided by hypergraph-rewriting rules. We demonstrate our proof methodology using the call-by-value lambda-calculus equipped with (higher-order) state.","authors":["Dan R. Ghica","Koko Muroya","Todd Waugh Ambridge"],"url":"https://arxiv.org/abs/1907.01257"}
{"created":"2025-04-24","title":"Equivalences between Non-trivial Variants of 3LDT and Conv3LDT","abstract":"The popular 3SUM conjecture states that there is no strongly subquadratic time algorithm for checking if a given set of integers contains three distinct elements $x_1, x_2, x_3$ such that $x_1+x_2=x_3$. A closely related problem is to check if a given set of integers contains distinct elements satisfying $x_1+x_2=2x_3$. This can be reduced to 3SUM in almost-linear time, but surprisingly a reverse reduction establishing 3SUM hardness was not known.","authors":["Bart{\\l}omiej Dudek","Pawe{\\l} Gawrychowski","Tatiana Starikovskaya"],"url":"https://arxiv.org/abs/2001.01289"}
{"created":"2025-04-24","title":"GenEO spectral coarse spaces in SPD domain decomposition","abstract":"Two-level domain decomposition methods are preconditioned Krylov solvers. What separates one- and two-level domain decomposition methods is the presence of a coarse space in the latter. The abstract Schwarz framework is a formalism that allows to define and study a large variety of two-level methods. The objective of this article is to define, in the abstract Schwarz framework, a family of coarse spaces called the GenEO coarse spaces (for Generalized Eigenvalues in the Overlaps). In detail, this work is a generalization of several methods, each of which exists for a particular choice of domain decomposition method. The article both unifies the GenEO theory and extends it to new settings. The proofs are based on an abstract Schwarz theory which now applies to coarse space corrections by projection, and has been extended to consider singular local solves. Bounds for the condition numbers of the preconditioned operators are proved that are independent of the parameters in the problem (e.g., any coefficients in an underlying PDE or the number of subdomains). The coarse spaces are computed by finding low- or high-frequency spaces of some well-chosen generalized eigenvalue problems in each subdomain. The abstract framework is illustrated by defining two-level Additive Schwarz, Neumann-Neumann and Inexact Schwarz preconditioners for a two-dimensional linear elasticity problem. Explicit theoretical bounds as well as numerical results are provided for this example.","authors":["Nicole Spillane (CMAP)"],"url":"https://arxiv.org/abs/2104.00280"}
{"created":"2025-04-24","title":"Beyond Self Attention: A Subquadratic Fourier Wavelet Transformer with Multi Modal Fusion","abstract":"We revisit the use of spectral techniques to replaces the attention mechanism in Transformers through Fourier Transform based token mixing, and present a comprehensive and novel reformulation of this technique in next generation transformer models. We provide expanded literature context, detailed mathematical formulations of Fourier mixing and causal masking, and introduce a novel MultiDomain Fourier Wavelet Attention(MDFWA) that integrates frequency and time localized transforms to capture both global and local dependencies efficiently. We derive the complexity bounds, gradient formulas, and show that MDFWA achieves sub quadratic time and memory cost while improving expressive power. We validate our design on an abstractive summarization task using PubMed dataset, by enhancing the proposed approach with learned frequency bases, adaptive scale selection, and multi-modal extensions.","authors":["Andrew Kiruluta","Andreas Lemos","Eric Lundy"],"url":"https://arxiv.org/abs/2111.15473"}
{"created":"2025-04-24","title":"Co-domain Symmetry for Complex-Valued Deep Learning","abstract":"We study complex-valued scaling as a type of symmetry natural and unique to complex-valued measurements and representations. Deep Complex Networks (DCN) extends real-valued algebra to the complex domain without addressing complex-valued scaling. SurReal takes a restrictive manifold view of complex numbers, adopting a distance metric to achieve complex-scaling invariance while losing rich complex-valued information. We analyze complex-valued scaling as a co-domain transformation and design novel equivariant and invariant neural network layer functions for this special transformation. We also propose novel complex-valued representations of RGB images, where complex-valued scaling indicates hue shift or correlated changes across color channels. Benchmarked on MSTAR, CIFAR10, CIFAR100, and SVHN, our co-domain symmetric (CDS) classifiers deliver higher accuracy, better generalization, robustness to co-domain transformations, and lower model bias and variance than DCN and SurReal with far fewer parameters.","authors":["Utkarsh Singhal","Yifei Xing","Stella X. Yu"],"url":"https://arxiv.org/abs/2112.01525"}
{"created":"2025-04-24","title":"Rethinking and Recomputing the Value of Machine Learning Models","abstract":"In this paper, we argue that the prevailing approach to training and evaluating machine learning models often fails to consider their real-world application within organizational or societal contexts, where they are intended to create beneficial value for people. We propose a shift in perspective, redefining model assessment and selection to emphasize integration into workflows that combine machine predictions with human expertise, particularly in scenarios requiring human intervention for low-confidence predictions. Traditional metrics like accuracy and f-score fail to capture the beneficial value of models in such hybrid settings. To address this, we introduce a simple yet theoretically sound \"value\" metric that incorporates task-specific costs for correct predictions, errors, and rejections, offering a practical framework for real-world evaluation. Through extensive experiments, we show that existing metrics fail to capture real-world needs, often leading to suboptimal choices in terms of value when used to rank classifiers. Furthermore, we emphasize the critical role of calibration in determining model value, showing that simple, well-calibrated models can often outperform more complex models that are challenging to calibrate.","authors":["Burcu Sayin","Jie Yang","Xinyue Chen","Andrea Passerini","Fabio Casati"],"url":"https://arxiv.org/abs/2209.15157"}
{"created":"2025-04-24","title":"Embodied Self-Supervised Learning (EMSSL) with Sampling and Training Coordination for Robot Arm Inverse Kinematics Model Learning","abstract":"Forward and inverse kinematics models are fundamental to robot arms, serving as the basis for the robot arm's operational tasks. However, in model learning of robot arms, especially in the presence of redundant degrees of freedom, inverse model learning is more challenging than forward model learning due to the non-convex problem caused by multiple solutions. In this paper, we propose a framework for autonomous learning of the robot arm inverse model based on embodied self-supervised learning (EMSSL) with sampling and training coordination. We investigate batch inference and parallel computation strategies for data sampling in order to accelerate model learning and propose two approaches for fast adaptation of the robot arm model. A series of experiments demonstrate the effectiveness of the method we proposed. The related code will be available soon.","authors":["Qu Weiming","Liu Tianlin","Wu Xihong","Luo Dingsheng"],"url":"https://arxiv.org/abs/2302.13346"}
{"created":"2025-04-24","title":"The Mutual Information In The Vicinity of Capacity-Achieving Input Distributions","abstract":"The mutual information is bounded from above by a decreasing affine function of the square of the distance between the input distribution and the set of all capacity-achieving input distributions $\\Pi_{\\mathcal{A}}$, on small enough neighborhoods of $\\Pi_{\\mathcal{A}}$, using an identity due to Tops{\\o}e and the Pinsker's inequality, assuming that the input set of the channel is finite and the constraint set $\\mathcal{A}$ is polyhedral, i.e., can be described by (possibly multiple but) finitely many linear constraints. Counterexamples demonstrating nonexistence of such a quadratic bound are provided for the case of infinitely many linear constraints and the case of infinite input sets. Using Taylor's theorem with the remainder term, rather than the Pinsker's inequality and invoking Moreau's decomposition theorem the exact characterization of the slowest decrease of the mutual information with the distance to $\\Pi_{\\mathcal{A}}$ is determined on small neighborhoods of $\\Pi_{\\mathcal{A}}$. Corresponding results for classical-quantum channels are established under separable output Hilbert space assumption for the quadratic bound and under finite-dimensional output Hilbert space assumption for the exact characterization. Implications of these observations for the channel coding problem and applications of the proof techniques to related problems are discussed.","authors":["Bar{\\i}\\c{s} Nakibo\\u{g}lu","Hao-Chung Cheng"],"url":"https://arxiv.org/abs/2304.14219"}
{"created":"2025-04-24","title":"Analyzing Maintenance Activities of Software Libraries","abstract":"Industrial applications heavily integrate open-source software libraries nowadays. Beyond the benefits that libraries bring, they can also impose a real threat in case a library is affected by a vulnerability but its community is not active in creating a fixing release. Therefore, I want to introduce an automatic monitoring approach for industrial applications to identify open-source dependencies that show negative signs regarding their current or future maintenance activities. Since most research in this field is limited due to lack of features, labels, and transitive links, and thus is not applicable in industry, my approach aims to close this gap by capturing the impact of direct and transitive dependencies in terms of their maintenance activities. Automatically monitoring the maintenance activities of dependencies reduces the manual effort of application maintainers and supports application security by continuously having well-maintained dependencies.","authors":["Alexandros Tsakpinis"],"url":"https://arxiv.org/abs/2306.06030"}
{"created":"2025-04-24","title":"CEMSSL: Conditional Embodied Self-Supervised Learning is All You Need for High-precision Multi-solution Inverse Kinematics of Robot Arms","abstract":"In the field of signal processing for robotics, the inverse kinematics of robot arms presents a significant challenge due to multiple solutions caused by redundant degrees of freedom (DOFs). Precision is also a crucial performance indicator for robot arms. Current methods typically rely on conditional deep generative models (CDGMs), which often fall short in precision. In this paper, we propose Conditional Embodied Self-Supervised Learning (CEMSSL) and introduce a unified framework based on CEMSSL for high-precision multi-solution inverse kinematics learning. This framework enhances the precision of existing CDGMs by up to 2-3 orders of magnitude while maintaining their original properties. Furthermore, our method is extendable to other fields of signal processing where obtaining multi-solution data in advance is challenging, as well as to other problems involving multi-solution inverse processes.","authors":["Qu Weiming","Liu Tianlin","Du Jiawei","Luo Dingsheng"],"url":"https://arxiv.org/abs/2306.12718"}
{"created":"2025-04-24","title":"Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge","abstract":"Background: Identification of the interactions and regulatory relations between biomolecules play pivotal roles in understanding complex biological systems and the mechanisms underlying diverse biological functions. However, the collection of such molecular interactions has heavily relied on expert curation in the past, making it labor-intensive and time-consuming. To mitigate these challenges, we propose leveraging the capabilities of large language models (LLMs) to automate genome-scale extraction of this crucial knowledge.","authors":["Gilchan Park","Byung-Jun Yoon","Xihaier Luo","Vanessa L\\'opez-Marrero","Shinjae Yoo","Shantenu Jha"],"url":"https://arxiv.org/abs/2307.08813"}
{"created":"2025-04-24","title":"The Expansion Problem for Infinite Trees","abstract":"We study Ramsey like theorems for infinite trees and similar combinatorial tools. As an application we consider the expansion problem for tree algebras.","authors":["Achim Blumensath"],"url":"https://arxiv.org/abs/2308.01174"}
{"created":"2025-04-24","title":"Finding Small Complete Subgraphs Efficiently","abstract":"(I) We revisit the algorithmic problem of finding all triangles in a graph $G=(V,E)$ with $n$ vertices and $m$ edges. According to a result of Chiba and Nishizeki (1985), this task can be achieved by a combinatorial algorithm running in $O(m \\alpha) = O(m^{3/2})$ time, where $\\alpha= \\alpha(G)$ is the graph arboricity. We provide a new very simple combinatorial algorithm for finding all triangles in a graph and show that is amenable to the same running time analysis. We derive these worst-case bounds from first principles and with very simple proofs that do not rely on classic results due to Nash-Williams from the 1960s. Our experimental results show that our simple algorithm for triangle listing is substantially faster in practice than that of Chiba and Nishizeki on all examples of real-world graphs we tried.","authors":["Ke Chen","Adrian Dumitrescu","Andrzej Lingas"],"url":"https://arxiv.org/abs/2308.11146"}
{"created":"2025-04-24","title":"Wasserstein Gradient Flow over Variational Parameter Space for Variational Inference","abstract":"Variational inference (VI) can be cast as an optimization problem in which the variational parameters are tuned to closely align a variational distribution with the true posterior. The optimization task can be approached through vanilla gradient descent in black-box VI or natural-gradient descent in natural-gradient VI. In this work, we reframe VI as the optimization of an objective that concerns probability distributions defined over a \\textit{variational parameter space}. Subsequently, we propose Wasserstein gradient descent for tackling this optimization problem. Notably, the optimization techniques, namely black-box VI and natural-gradient VI, can be reinterpreted as specific instances of the proposed Wasserstein gradient descent. To enhance the efficiency of optimization, we develop practical methods for numerically solving the discrete gradient flows. We validate the effectiveness of the proposed methods through empirical experiments on a synthetic dataset, supplemented by theoretical analyses.","authors":["Dai Hai Nguyen","Tetsuya Sakurai","Hiroshi Mamitsuka"],"url":"https://arxiv.org/abs/2310.16705"}
{"created":"2025-04-24","title":"Advances in Embodied Navigation Using Large Language Models: A Survey","abstract":"In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN.","authors":["Jinzhou Lin","Han Gao","Xuxiang Feng","Rongtao Xu","Changwei Wang","Man Zhang","Li Guo","Shibiao Xu"],"url":"https://arxiv.org/abs/2311.00530"}
{"created":"2025-04-24","title":"Reinforcement Learning With LLMs Interaction For Distributed Diffusion Model Services","abstract":"Distributed Artificial Intelligence-Generated Content (AIGC) has attracted significant attention, but two key challenges remain: maximizing subjective Quality of Experience (QoE) and improving energy efficiency, which are particularly pronounced in widely adopted Generative Diffusion Model (GDM)-based image generation services. In this paper, we propose a novel user-centric Interactive AI (IAI) approach for service management, with a distributed GDM-based AIGC framework that emphasizes efficient and cooperative deployment. The proposed method restructures the GDM inference process by allowing users with semantically similar prompts to share parts of the denoising chain. Furthermore, to maximize the users' subjective QoE, we propose an IAI approach, i.e., Reinforcement Learning With Large Language Models Interaction (RLLI), which utilizes Large Language Model (LLM)-empowered generative agents to replicate user interaction, providing real-time and subjective QoE feedback aligned with diverse user personalities. Lastly, we present the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm, adapted to the proposed RLLI framework, to allocate communication and computing resources effectively while accounting for subjective user traits and dynamic wireless conditions. Simulation results demonstrate that G-DDPG improves total QoE by 15% compared with the standard DDPG algorithm.","authors":["Hongyang Du","Ruichen Zhang","Dusit Niyato","Jiawen Kang","Zehui Xiong","Shuguang Cui","Xuemin Shen","Dong In Kim"],"url":"https://arxiv.org/abs/2311.11094"}
{"created":"2025-04-24","title":"Two-timescale joint power control and beamforming design with applications to cell-free massive MIMO","abstract":"In this study we derive novel optimal algorithms for joint power control and beamforming design in modern large-scale MIMO systems, such as those based on the cell-free massive MIMO and XL-MIMO concepts. In particular, motivated by the need for scalable system architectures, we formulate and solve nontrivial two-timescale extensions of the classical uplink power minimization and max-min fair resource allocation problems. In our formulations, we let the beamformers be functions mapping partial instantaneous channel state information (CSI) to beamforming weights, and we jointly optimize these functions and the power control coefficients based on long-term statistical CSI. This long-term approach mitigates the severe scalability issues of competing short-term iterative algorithms in the literature, where a central controller endowed with global instantaneous CSI must solve a complex optimization problem for every channel realization, hence imposing very demanding requirements in terms of computational complexity and signaling overhead. Moreover, our approach outperforms the available long-term approaches, which do not jointly optimize powers and beamformers. The obtained optimal long-term algorithms are then illustrated and compared against existing short-term and long-term algorithms via numerical simulations in a cell-free massive MIMO setup with different levels of cooperation.","authors":["Lorenzo Miretti","Renato L. G. Cavalcante","S{\\l}awomir Sta\\'nczak"],"url":"https://arxiv.org/abs/2312.02080"}
{"created":"2025-04-24","title":"Evolutionary Optimization of Physics-Informed Neural Networks: Advancing Generalizability by the Baldwin Effect","abstract":"Physics-informed neural networks (PINNs) are at the forefront of scientific machine learning, making possible the creation of machine intelligence that is cognizant of physical laws and able to accurately simulate them. However, today's PINNs are often trained for a single physics task and require computationally expensive re-training for each new task, even for tasks from similar physics domains. To address this limitation, this paper proposes a pioneering approach to advance the generalizability of PINNs through the framework of Baldwinian evolution. Drawing inspiration from the neurodevelopment of precocial species that have evolved to learn, predict and react quickly to their environment, we envision PINNs that are pre-wired with connection strengths inducing strong biases towards efficient learning of physics. A novel two-stage stochastic programming formulation coupling evolutionary selection pressure (based on proficiency over a distribution of physics tasks) with lifetime learning (to specialize on a sampled subset of those tasks) is proposed to instantiate the Baldwin effect. The evolved Baldwinian-PINNs demonstrate fast and physics-compliant prediction capabilities across a range of empirically challenging problem instances with more than an order of magnitude improvement in prediction accuracy at a fraction of the computation cost compared to state-of-the-art gradient-based meta-learning methods. For example, when solving the diffusion-reaction equation, a 70x improvement in accuracy was obtained while taking 700x less computational time. This paper thus marks a leap forward in the meta-learning of PINNs as generalizable physics solvers. Sample codes are available at https://github.com/chiuph/Baldwinian-PINN.","authors":["Jian Cheng Wong","Chin Chun Ooi","Abhishek Gupta","Pao-Hsiung Chiu","Joshua Shao Zheng Low","My Ha Dao","Yew-Soon Ong"],"url":"https://arxiv.org/abs/2312.03243"}
{"created":"2025-04-24","title":"HandDiffuse: Generative Controllers for Two-Hand Interactions via Diffusion Models","abstract":"Existing hands datasets are largely short-range and the interaction is weak due to the self-occlusion and self-similarity of hands, which can not yet fit the need for interacting hands motion generation. To rescue the data scarcity, we propose HandDiffuse12.5M, a novel dataset that consists of temporal sequences with strong two-hand interactions. HandDiffuse12.5M has the largest scale and richest interactions among the existing two-hand datasets. We further present a strong baseline method HandDiffuse for the controllable motion generation of interacting hands using various controllers. Specifically, we apply the diffusion model as the backbone and design two motion representations for different controllers. To reduce artifacts, we also propose Interaction Loss which explicitly quantifies the dynamic interaction process. Our HandDiffuse enables various applications with vivid two-hand interactions, i.e., motion in-betweening and trajectory control. Experiments show that our method outperforms the state-of-the-art techniques in motion generation and can also contribute to data augmentation for other datasets. Our dataset, corresponding codes, and pre-trained models will be disseminated to the community for future research towards two-hand interaction modeling.","authors":["Pei Lin","Sihang Xu","Hongdi Yang","Yiran Liu","Xin Chen","Jingya Wang","Jingyi Yu","Lan Xu"],"url":"https://arxiv.org/abs/2312.04867"}
{"created":"2025-04-24","title":"Coding for Gaussian Two-Way Channels: Linear and Learning-Based Approaches","abstract":"Although user cooperation cannot improve the capacity of Gaussian two-way channels (GTWCs) with independent noises, it can improve communication reliability. In this work, we aim to enhance and balance the communication reliability in GTWCs by minimizing the sum of error probabilities via joint design of encoders and decoders at the users. We first formulate general encoding/decoding functions, where the user cooperation is captured by the coupling of user encoding processes. The coupling effect renders the encoder/decoder design non-trivial, requiring effective decoding to capture this effect, as well as efficient power management at the encoders within power constraints. To address these challenges, we propose two different two-way coding strategies: linear coding and learning-based coding. For linear coding, we propose optimal linear decoding and discuss new insights on encoding regarding user cooperation to balance reliability. We then propose an efficient algorithm for joint encoder/decoder design. For learning-based coding, we introduce a novel recurrent neural network (RNN)-based coding architecture, where we propose interactive RNNs and a power control layer for encoding, and we incorporate bi-directional RNNs with an attention mechanism for decoding. Through simulations, we show that our two-way coding methodologies outperform conventional channel coding schemes (that do not utilize user cooperation) significantly in sum-error performance. We also demonstrate that our linear coding excels at high signal-to-noise ratios (SNRs), while our RNN-based coding performs best at low SNRs. We further investigate our two-way coding strategies in terms of power distribution, two-way coding benefit, different coding rates, and block-length gain.","authors":["Junghoon Kim","Taejoon Kim","Anindya Bijoy Das","Seyyedali Hosseinalipour","David J. Love","Christopher G. Brinton"],"url":"https://arxiv.org/abs/2401.00477"}
{"created":"2025-04-24","title":"Synergy: Towards On-Body AI via Tiny AI Accelerator Collaboration on Wearables","abstract":"The advent of tiny artificial intelligence (AI) accelerators enables AI to run at the extreme edge, offering reduced latency, lower power cost, and improved privacy. When integrated into wearable devices, these accelerators open exciting opportunities, allowing various AI apps to run directly on the body. We present Synergy that provides AI apps with best-effort performance via system-driven holistic collaboration over AI accelerator-equipped wearables. To achieve this, Synergy provides device-agnostic programming interfaces to AI apps, giving the system visibility and controllability over the app's resource use. Then, Synergy maximizes the inference throughput of concurrent AI models by creating various execution plans for each app considering AI accelerator availability and intelligently selecting the best set of execution plans. Synergy further improves throughput by leveraging parallelization opportunities over multiple computation units. Our evaluations with 7 baselines and 8 models demonstrate that, on average, Synergy achieves a 23.0 times improvement in throughput, while reducing latency by 73.9% and power consumption by 15.8%, compared to the baselines.","authors":["Taesik Gong","Si Young Jang","Utku G\\\"unay Acer","Fahim Kawsar","Chulhong Min"],"url":"https://arxiv.org/abs/2401.08637"}
{"created":"2025-04-24","title":"Fair and Efficient Ridesharing: A Dynamic Programming-based Relocation Approach","abstract":"Recommending routes by their probability of having a rider has long been the goal of conventional route recommendation systems. While this maximizes the platform-specific criteria of efficiency, it results in sub-optimal outcomes with the disparity among the income of drivers who work for similar time frames. Pioneer studies on fairness in ridesharing platforms have focused on algorithms that match drivers and riders. However, these studies do not consider the time schedules of different riders sharing a ride in the ridesharing mode. To overcome this shortcoming, we present the first route recommendation system for ridesharing networks that explicitly considers fairness as an evaluation criterion. In particular, we design a routing mechanism that reduces the inequality among drivers and provides them with routes that have a similar probability of finding riders over a period of time. However, while optimizing fairness the efficiency of the platform should not be affected as both of these goals are important for the long-term sustainability of the system. In order to jointly optimize fairness and efficiency we consider repositioning drivers with low income to the areas that have a higher probability of finding riders in future. While applying driver repositioning, we design a future-aware policy and allocate the areas to the drivers considering the destination of requests in the corresponding area. Extensive simulations on real-world datasets of Washington DC and New York demonstrate superior performance by our proposed system in comparison to the existing baselines.","authors":["Aqsa Ashraf Makhdomi","Iqra Altaf Gillani"],"url":"https://arxiv.org/abs/2401.15363"}
{"created":"2025-04-24","title":"A New Framework to Predict and Visualize Technology Acceptance: A Case Study of Shared Autonomous Vehicles","abstract":"Public acceptance is critical to the adoption of Shared Autonomous Vehicles (SAVs) in the transport sector. Traditional acceptance models, primarily reliant on Structural Equation Modeling, may not adequately capture the complex, non-linear relationships among factors influencing technology acceptance and often have limited predictive capabilities. This paper introduces a framework that combines Machine Learning techniques with chord diagram visualizations to analyze and predict public acceptance of technologies. Using SAV acceptance as a case study, we applied a Random Forest machine learning approach to model the non-linear relationships among psychological factors influencing acceptance. Chord diagrams were then employed to provide an intuitive visualization of the relative importance and interplay of these factors at both factor and item levels in a single plot. Our findings identified Attitude as the primary predictor of SAV usage intention, followed by Perceived Risk, Perceived Usefulness, Trust, and Perceived Ease of Use. The framework also reveals divergent perceptions between SAV adopters and non-adopters, providing insights for tailored strategies to enhance SAV acceptance. This study contributes a data-driven perspective to the technology acceptance discourse, demonstrating the efficacy of integrating predictive modeling with visual analytics to understand the relative importance of factors in predicting public acceptance of emerging technologies.","authors":["Lirui Guo","Michael G. Burke","Wynita M. Griggs"],"url":"https://arxiv.org/abs/2401.15921"}
{"created":"2025-04-24","title":"WildfireGPT: Tailored Large Language Model for Wildfire Analysis","abstract":"Recent advancement of large language models (LLMs) represents a transformational capability at the frontier of artificial intelligence. However, LLMs are generalized models, trained on extensive text corpus, and often struggle to provide context-specific information, particularly in areas requiring specialized knowledge, such as wildfire details within the broader context of climate change. For decision-makers focused on wildfire resilience and adaptation, it is crucial to obtain responses that are not only precise but also domain-specific. To that end, we developed WildfireGPT, a prototype LLM agent designed to transform user queries into actionable insights on wildfire risks. We enrich WildfireGPT by providing additional context, such as climate projections and scientific literature, to ensure its information is current, relevant, and scientifically accurate. This enables WildfireGPT to be an effective tool for delivering detailed, user-specific insights on wildfire risks to support a diverse set of end users, including but not limited to researchers and engineers, for making positive impact and decision making.","authors":["Yangxinyu Xie","Bowen Jiang","Tanwi Mallick","Joshua David Bergerson","John K. Hutchison","Duane R. Verner","Jordan Branham","M. Ross Alexander","Robert B. Ross","Yan Feng","Leslie-Anne Levy","Weijie Su","Camillo J. Taylor"],"url":"https://arxiv.org/abs/2402.07877"}
{"created":"2025-04-24","title":"Learning-augmented Online Minimization of Age of Information and Transmission Costs","abstract":"We consider a discrete-time system where a resource-constrained source (e.g., a small sensor) transmits its time-sensitive data to a destination over a time-varying wireless channel. Each transmission incurs a fixed transmission cost (e.g., energy cost), and no transmission results in a staleness cost represented by the Age-of-Information. The source must balance the tradeoff between transmission and staleness costs. To address this challenge, we develop a robust online algorithm to minimize the sum of transmission and staleness costs, ensuring a worst-case performance guarantee. While online algorithms are robust, they are usually overly conservative and may have a poor average performance in typical scenarios. In contrast, by leveraging historical data and prediction models, machine learning (ML) algorithms perform well in average cases. However, they typically lack worst-case performance guarantees. To achieve the best of both worlds, we design a learning-augmented online algorithm that exhibits two desired properties: (i) consistency: closely approximating the optimal offline algorithm when the ML prediction is accurate and trusted; (ii) robustness: ensuring worst-case performance guarantee even ML predictions are inaccurate. Finally, we perform extensive simulations to show that our online algorithm performs well empirically and that our learning-augmented algorithm achieves both consistency and robustness.","authors":["Zhongdong Liu","Keyuan Zhang","Bin Li","Yin Sun","Y. Thomas Hou","Bo Ji"],"url":"https://arxiv.org/abs/2403.02573"}
{"created":"2025-04-24","title":"A Categorical Treatment of Open Linear Systems","abstract":"An open stochastic system \\`a la Jan Willems is a system affected by two qualitatively different kinds of uncertainty: one is probabilistic fluctuation, and the other one is nondeterminism caused by a fundamental lack of information. We present a formalization of open stochastic systems in the language of category theory. Central to this is the notion of copartiality which models how the lack of information propagates through a system (corresponding to the coarseness of sigma-algebras in Willems' work). As a concrete example, we study extended Gaussian distributions, which combine Gaussian probability with nondeterminism and correspond precisely to Willems' notion of Gaussian linear systems. We describe them both as measure-theoretic and abstract categorical entities, which enables us to rigorously describe a variety of phenomena like noisy physical laws and uninformative priors in Bayesian statistics. The category of extended Gaussian maps can be seen as a mutual generalization of Gaussian probability and linear relations, which connects the literature on categorical probability with ideas from control theory like signal-flow diagrams.","authors":["Dario Stein","Richard Samuelson"],"url":"https://arxiv.org/abs/2403.03934"}
{"created":"2025-04-24","title":"Natural Language Processing in the Patent Domain: A Survey","abstract":"Patents, which encapsulate crucial technical and legal information in text form and referenced drawings, present a rich domain for natural language processing (NLP) applications. As NLP technologies evolve, large language models (LLMs) have demonstrated outstanding capabilities in general text processing and generation tasks. However, the application of LLMs in the patent domain remains under-explored and under-developed due to the complexity of patents, particularly their language and legal framework. Understanding the unique characteristics of patent documents and related research in the patent domain becomes essential for researchers to apply these tools effectively. Therefore, this paper aims to equip NLP researchers with the essential knowledge to navigate this complex domain efficiently. We introduce the relevant fundamental aspects of patents to provide solid background information. In addition, we systematically break down the structural and linguistic characteristics unique to patents and map out how NLP can be leveraged for patent analysis and generation. Moreover, we demonstrate the spectrum of text-based and multimodal patent-related tasks, including nine patent analysis and four patent generation tasks.","authors":["Lekang Jiang","Stephan Goetz"],"url":"https://arxiv.org/abs/2403.04105"}
{"created":"2025-04-24","title":"A dataset and benchmark for hospital course summarization with adapted large language models","abstract":"Brief hospital course (BHC) summaries are clinical documents that summarize a patient's hospital stay. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as synthesizing BHCs from clinical notes have not been shown. We introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating clinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC synthesis. Furthermore, we introduce a benchmark of the summarization performance of two general-purpose LLMs and three healthcare-adapted LLMs. Using clinical notes as input, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We evaluate these LLMs across multiple context-length inputs using natural language similarity metrics. We further conduct a clinical study with five clinicians, comparing clinician-written and LLM-generated BHCs across 30 samples, focusing on their potential to enhance clinical decision-making through improved summary quality. We observe that the Llama2-13B fine-tuned LLM outperforms other domain-adapted models given quantitative evaluation metrics of BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to increasing context lengths of clinical note inputs than fine-tuned Llama2-13B. Despite comparable quantitative metrics, the reader study depicts a significant preference for summaries generated by GPT-4 with in-context learning compared to both Llama2-13B fine-tuned summaries and the original summaries, highlighting the need for qualitative clinical evaluation.","authors":["Asad Aali","Dave Van Veen","Yamin Ishraq Arefeen","Jason Hom","Christian Bluethgen","Eduardo Pontes Reis","Sergios Gatidis","Namuun Clifford","Joseph Daws","Arash S. Tehrani","Jangwon Kim","Akshay S. Chaudhari"],"url":"https://arxiv.org/abs/2403.05720"}
{"created":"2025-04-24","title":"NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens","abstract":"Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.","authors":["Cunxiang Wang","Ruoxi Ning","Boqi Pan","Tonghui Wu","Qipeng Guo","Cheng Deng","Guangsheng Bao","Xiangkun Hu","Zheng Zhang","Qian Wang","Yue Zhang"],"url":"https://arxiv.org/abs/2403.12766"}
{"created":"2025-04-24","title":"Group Fairness and Multi-criteria Optimization in School Assignment","abstract":"We consider the problem of assigning students to schools, when students have different utilities for schools and schools have capacity. There are additional group fairness considerations over students that can be captured either by concave objectives, or additional constraints on the groups. We present approximation algorithms for this problem via convex program rounding that achieve various trade-offs between utility violation, capacity violation, and running time. We also show that our techniques easily extend to the setting where there are arbitrary covering constraints on the feasible assignment, capturing multi-criteria and ranking optimization.","authors":["Santhini K. A.","Kamesh Munagala","Meghana Nasre","Govind S. Sankar"],"url":"https://arxiv.org/abs/2403.15623"}
{"created":"2025-04-24","title":"ChatDBG: Augmenting Debugging with Large Language Models","abstract":"Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like \"why is x null?\". To handle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.","authors":["Kyla H. Levin","Nicolas van Kempen","Emery D. Berger","Stephen N. Freund"],"url":"https://arxiv.org/abs/2403.16354"}
{"created":"2025-04-24","title":"Effective Lymph Nodes Detection in CT Scans Using Location Debiased Query Selection and Contrastive Query Representation in Transformer","abstract":"Lymph node (LN) assessment is a critical, indispensable yet very challenging task in the routine clinical workflow of radiology and oncology. Accurate LN analysis is essential for cancer diagnosis, staging, and treatment planning. Finding scatteredly distributed, low-contrast clinically relevant LNs in 3D CT is difficult even for experienced physicians under high inter-observer variations. Previous automatic LN detection works typically yield limited recall and high false positives (FPs) due to adjacent anatomies with similar image intensities, shapes, or textures (vessels, muscles, esophagus, etc). In this work, we propose a new LN DEtection TRansformer, named LN-DETR, to achieve more accurate performance. By enhancing the 2D backbone with a multi-scale 2.5D feature fusion to incorporate 3D context explicitly, more importantly, we make two main contributions to improve the representation quality of LN queries. 1) Considering that LN boundaries are often unclear, an IoU prediction head and a location debiased query selection are proposed to select LN queries of higher localization accuracy as the decoder query's initialization. 2) To reduce FPs, query contrastive learning is employed to explicitly reinforce LN queries towards their best-matched ground-truth queries over unmatched query predictions. Trained and tested on 3D CT scans of 1067 patients (with 10,000+ labeled LNs) via combining seven LN datasets from different body parts (neck, chest, and abdomen) and pathologies/cancers, our method significantly improves the performance of previous leading methods by > 4-5% average recall at the same FP rates in both internal and external testing. We further evaluate on the universal lesion detection task using NIH DeepLesion benchmark, and our method achieves the top performance of 88.46% averaged recall across 0.5 to 4 FPs per image, compared with other leading reported results.","authors":["Yirui Wang","Qinji Yu","Ke Yan","Haoshen Li","Dazhou Guo","Li Zhang","Le Lu","Na Shen","Qifeng Wang","Xiaowei Ding","Xianghua Ye","Dakai Jin"],"url":"https://arxiv.org/abs/2404.03819"}
{"created":"2025-04-24","title":"TCAN: Text-oriented Cross Attention Network for Multimodal Sentiment Analysis","abstract":"Multimodal Sentiment Analysis (MSA) endeavors to understand human sentiment by leveraging language, visual, and acoustic modalities. Despite the remarkable performance exhibited by previous MSA approaches, the presence of inherent multimodal heterogeneities poses a challenge, with the contribution of different modalities varying considerably. Past research predominantly focused on improving representation learning techniques and feature fusion strategies. However, many of these efforts overlooked the variation in semantic richness among different modalities, treating each modality uniformly. This approach may lead to underestimating the significance of strong modalities while overemphasizing the importance of weak ones. Motivated by these insights, we introduce a Text-oriented Cross-Attention Network (TCAN), emphasizing the predominant role of the text modality in MSA. Specifically, for each multimodal sample, by taking unaligned sequences of the three modalities as inputs, we initially allocate the extracted unimodal features into a visual-text and an acoustic-text pair. Subsequently, we implement self-attention on the text modality and apply text-queried cross-attention to the visual and acoustic modalities. To mitigate the influence of noise signals and redundant features, we incorporate a gated control mechanism into the framework. Additionally, we introduce unimodal joint learning to gain a deeper understanding of homogeneous emotional tendencies across diverse modalities through backpropagation. Experimental results demonstrate that TCAN consistently outperforms state-of-the-art MSA methods on two datasets (CMU-MOSI and CMU-MOSEI).","authors":["Weize Quan","Yunfei Feng","Ming Zhou","Yunzhen Zhao","Tong Wang","Dong-Ming Yan"],"url":"https://arxiv.org/abs/2404.04545"}
{"created":"2025-04-24","title":"Sequential Outlier Hypothesis Testing under Universality Constraints","abstract":"We revisit sequential outlier hypothesis testing and derive bounds on achievable exponents when both the nominal and anomalous distributions are unknown. The task of outlier hypothesis testing is to identify the set of outliers that are generated from an anomalous distribution among all observed sequences where the rest majority are generated from a nominal distribution. In the sequential setting, one obtains a symbol from each sequence per unit time until a reliable decision could be made. For the case with exactly one outlier, our exponent bounds are tight, providing exact large deviations characterization of sequential tests and strengthening a previous result of Li, Nitinawarat and Veeravalli (2017). In particular, the average sample size of our sequential test is bounded universally under any pair of nominal and anomalous distributions and our sequential test achieves larger Bayesian exponent than the fixed-length test, which could not be guaranteed by the sequential test of Li, Nitinawarat and Veeravalli (2017). For the case with at most one outlier, we propose a threshold-based test that has bounded expected stopping time under mild conditions and we bound the exponential decay rate of error probabilities under each non-null hypothesis and the null hypothesis. Our sequential test resolves the tradeoff among the exponential decay rates of misclassification, false reject and false alarm probabilities for the fixed-length test of Zhou, Wei and Hero (TIT 2022). Finally, with a further step towards practical applications, we generalize our results to the cases of multiple outliers and show that there is a penalty in the error exponents when the number of outliers is unknown.","authors":["Jun Diao","Lin Zhou"],"url":"https://arxiv.org/abs/2404.14221"}
{"created":"2025-04-24","title":"Analyzing the Accessibility of GitHub Repositories for PyPI and NPM Libraries","abstract":"Industrial applications heavily rely on open-source software (OSS) libraries, which provide various benefits. But, they can also present a substantial risk if a vulnerability or attack arises and the community fails to promptly address the issue and release a fix due to inactivity. To be able to monitor the activities of such communities, a comprehensive list of repositories for the libraries of an ecosystem must be accessible. Based on these repositories, integrated libraries of an application can be monitored to observe whether they are adequately maintained. In this descriptive study, we analyze the accessibility of GitHub repositories for PyPI and NPM libraries. For all available libraries, we extract assigned repository URLs, direct dependencies and use the page rank algorithm to comprehensively analyze the ecosystems from a library and dependency chain perspective. For invalid repository URLs, we derive potential reasons. Both ecosystems show varying accessibility to GitHub repository URLs, depending on the page rank score of the analyzed libraries. For individual libraries, up to 73.8% of PyPI and up to 69.4% of NPM libraries have repository URLs. Within dependency chains, up to 80.1% of PyPI libraries have URLs, while up to 81.1% for NPM. That means, most libraries, especially the ones of increasing importance, can be monitored on GitHub. Among the most common reasons for invalid repository URLs is no URLs being assigned at all, which amounts up to 17.9% for PyPI and up to 39.6% for NPM. Package maintainers should address this issue and update the repository information to enable monitoring of their libraries.","authors":["Alexandros Tsakpinis","Alexander Pretschner"],"url":"https://arxiv.org/abs/2404.17403"}
{"created":"2025-04-24","title":"Sharp Bounds for Sequential Federated Learning on Heterogeneous Data","abstract":"There are two paradigms in Federated Learning (FL): parallel FL (PFL), where models are trained in a parallel manner across clients, and sequential FL (SFL), where models are trained in a sequential manner across clients. Specifically, in PFL, clients perform local updates independently and send the updated model parameters to a global server for aggregation; in SFL, one client starts its local updates only after receiving the model parameters from the previous client in the sequence. In contrast to that of PFL, the convergence theory of SFL on heterogeneous data is still lacking. To resolve the theoretical dilemma of SFL, we establish sharp convergence guarantees for SFL on heterogeneous data with both upper and lower bounds. Specifically, we derive the upper bounds for the strongly convex, general convex and non-convex objective functions, and construct the matching lower bounds for the strongly convex and general convex objective functions. Then, we compare the upper bounds of SFL with those of PFL, showing that SFL outperforms PFL on heterogeneous data (at least, when the level of heterogeneity is relatively high). Experimental results validate the counterintuitive theoretical finding.","authors":["Yipeng Li","Xinchen Lyu"],"url":"https://arxiv.org/abs/2405.01142"}
{"created":"2025-04-24","title":"MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks","abstract":"The emergence of multimodal large language models (MLLMs) has triggered extensive research in model evaluation. While existing evaluation studies primarily focus on unimodal (vision-only) comprehension and reasoning capabilities, they overlook critical assessments of complex multimodal reasoning tasks that require integrated understanding of both visual and textual contexts. Such multimodal tasks present unique challenges, demanding sophisticated reasoning across multiple modalities and deep comprehension of multimodal contexts. In this paper, we present MM-InstructEval, a comprehensive evaluation framework that incorporates diverse metrics to assess model performance across various multimodal reasoning tasks with vision-text contexts. We conduct extensive zero-shot evaluations on 45 models (including 36 MLLMs) across 16 multimodal datasets, encompassing 6 distinct tasks using 10 different instructions. Our framework introduces multiple innovative metrics, including the 'Best Performance' metric to benchmark peak model capabilities, the 'Mean Relative Gain' metric to assess overall efficacy across models and instructions, the 'Stability' metric to measure robustness, and the 'Adaptability' metric to quantify the compatibility between models and instructions. Through comprehensive evaluation and analysis, we uncover several significant insights about model architectures, instruction formats, and their interactions in multimodal reasoning tasks. Our findings establish new benchmarks for assessing the reasoning capabilities of MLLMs and provide strategic guidance for future developments. To facilitate continued research and evaluation in this field, we release our framework and resources at https://github.com/declare-lab/MM-InstructEval, with an interactive leaderboard available at MM-InstructEval Leaderboard (https://declare-lab.github.io/MM-InstructEval/).","authors":["Xiaocui Yang","Wenfang Wu","Shi Feng","Ming Wang","Daling Wang","Yang Li","Qi Sun","Yifei Zhang","Xiaoming Fu","Soujanya Poria"],"url":"https://arxiv.org/abs/2405.07229"}
{"created":"2025-04-24","title":"Coded Downlink Massive Random Access and a Finite de Finetti Theorem","abstract":"This paper considers a massive connectivity setting in which a base-station (BS) aims to communicate sources $(X_1,\\cdots,X_k)$ to a randomly activated subset of $k$ users, among a large pool of $n$ users, via a common message in the downlink. Although the identities of the $k$ active users are assumed to be known at the BS, each active user only knows whether itself is active and does not know the identities of the other active users. A naive coding strategy is to transmit the sources alongside the identities of the users for which the source information is intended. This requires $H(X_1,\\cdots,X_k) + k\\log(n)$ bits, because the cost of specifying the identity of one out of $n$ users is $\\log(n)$ bits. For large $n$, this overhead can be significant. This paper shows that it is possible to develop coding techniques that eliminate the dependency of the overhead on $n$, if the source distribution follows certain symmetry. Specifically, if the source distribution is independently and identically distributed (i.i.d.) then the overhead can be reduced to at most $O(\\log(k))$ bits, and in case of uniform i.i.d. sources, the overhead can be further reduced to $O(1)$ bits. For sources that follow a more general exchangeable distribution, the overhead is at most $O(k)$ bits, and in case of finite-alphabet exchangeable sources, the overhead can be further reduced to $O(\\log(k))$ bits. The downlink massive random access problem is closely connected to the study of finite exchangeable sequences. The proposed coding strategy allows bounds on the Kullback-Leibler (KL) divergence between finite exchangeable distributions and i.i.d. mixture distributions to be developed, and gives a new KL divergence version of the finite de Finetti theorem which is scaling optimal.","authors":["Ryan Song","Kareem M. Attiah","Wei Yu"],"url":"https://arxiv.org/abs/2405.08301"}
{"created":"2025-04-24","title":"Multi-Player Approaches for Dueling Bandits","abstract":"Various approaches have emerged for multi-armed bandits in distributed systems. The multiplayer dueling bandit problem, common in scenarios with only preference-based information like human feedback, introduces challenges related to controlling collaborative exploration of non-informative arm pairs, but has received little attention. To fill this gap, we demonstrate that the direct use of a Follow Your Leader black-box approach matches the lower bound for this setting when utilizing known dueling bandit algorithms as a foundation. Additionally, we analyze a message-passing fully distributed approach with a novel Condorcet-winner recommendation protocol, resulting in expedited exploration in many cases. Our experimental comparisons reveal that our multiplayer algorithms surpass single-player benchmark algorithms, underscoring their efficacy in addressing the nuanced challenges of the multiplayer dueling bandit setting.","authors":["Or Raveh","Junya Honda","Masashi Sugiyama"],"url":"https://arxiv.org/abs/2405.16168"}
{"created":"2025-04-24","title":"Towards Practicable Algorithms for Rewriting Graph Queries beyond DL-Lite","abstract":"Despite the many advantages that ontology-based data access (OBDA) has brought to a range of application domains, state-of-the-art OBDA systems still do not support popular graph database management systems such as Neo4j. Algorithms for query rewriting focus on languages like conjunctive queries and their unions, which are fragments of first-order logic and were developed for relational data. Such query languages are poorly suited for querying graph data. Moreover, they also limit the expressiveness of the ontology languages that admit rewritings, restricting them to those where the data complexity of reasoning is not higher than it is in first-order logic. In this paper, we propose a technique for rewriting a family of navigational queries for a suitably restricted fragment of ELHI that extends DL-Lite and that is NL-complete in data complexity. We implemented a proof-of-concept prototype that rewrites into Cypher queries, and tested it on a real-world cognitive neuroscience use case with promising results.","authors":["Bianca L\\\"ohnert","Nikolaus Augsten","Cem Okulmus","Magdalena Ortiz"],"url":"https://arxiv.org/abs/2405.18181"}
{"created":"2025-04-24","title":"Large Language Model Sentinel: LLM Agent for Adversarial Purification","abstract":"Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.","authors":["Guang Lin","Toshihisa Tanaka","Qibin Zhao"],"url":"https://arxiv.org/abs/2405.20770"}
{"created":"2025-04-24","title":"Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering","abstract":"For vision-language models (VLMs), understanding the dynamic properties of objects and their interactions in 3D scenes from videos is crucial for effective reasoning about high-level temporal and action semantics. Although humans are adept at understanding these properties by constructing 3D and temporal (4D) representations of the world, current video understanding models struggle to extract these dynamic semantics, arguably because these models use cross-frame reasoning without underlying knowledge of the 3D/4D scenes. In this work, we introduce DynSuperCLEVR, the first video question answering dataset that focuses on language understanding of the dynamic properties of 3D objects. We concentrate on three physical concepts -- velocity, acceleration, and collisions within 4D scenes. We further generate three types of questions, including factual queries, future predictions, and counterfactual reasoning that involve different aspects of reasoning about these 4D dynamic properties. To further demonstrate the importance of explicit scene representations in answering these 4D dynamics questions, we propose NS-4DPhysics, a Neural-Symbolic VideoQA model integrating Physics prior for 4D dynamic properties with explicit scene representation of videos. Instead of answering the questions directly from the video text input, our method first estimates the 4D world states with a 3D generative model powered by physical priors, and then uses neural symbolic reasoning to answer the questions based on the 4D world states. Our evaluation on all three types of questions in DynSuperCLEVR shows that previous video question answering models and large multimodal models struggle with questions about 4D dynamics, while our NS-4DPhysics significantly outperforms previous state-of-the-art models. Our code and data are released in https://xingruiwang.github.io/projects/DynSuperCLEVR/.","authors":["Xingrui Wang","Wufei Ma","Angtian Wang","Shuo Chen","Adam Kortylewski","Alan Yuille"],"url":"https://arxiv.org/abs/2406.00622"}
{"created":"2025-04-24","title":"Combinatorial Multivariant Multi-Armed Bandits with Applications to Episodic Reinforcement Learning and Beyond","abstract":"We introduce a novel framework of combinatorial multi-armed bandits (CMAB) with multivariant and probabilistically triggering arms (CMAB-MT), where the outcome of each arm is a $d$-dimensional multivariant random variable and the feedback follows a general arm triggering process. Compared with existing CMAB works, CMAB-MT not only enhances the modeling power but also allows improved results by leveraging distinct statistical properties for multivariant random variables. For CMAB-MT, we propose a general 1-norm multivariant and triggering probability-modulated smoothness condition, and an optimistic CUCB-MT algorithm built upon this condition. Our framework can include many important problems as applications, such as episodic reinforcement learning (RL) and probabilistic maximum coverage for goods distribution, all of which meet the above smoothness condition and achieve matching or improved regret bounds compared to existing works. Through our new framework, we build the first connection between the episodic RL and CMAB literature, by offering a new angle to solve the episodic RL through the lens of CMAB, which may encourage more interactions between these two important directions.","authors":["Xutong Liu","Siwei Wang","Jinhang Zuo","Han Zhong","Xuchuang Wang","Zhiyong Wang","Shuai Li","Mohammad Hajiesmaili","John C. S. Lui","Wei Chen"],"url":"https://arxiv.org/abs/2406.01386"}
{"created":"2025-04-24","title":"A Nearly Optimal Deterministic Algorithm for Online Transportation Problem","abstract":"For the online transportation problem with $m$ server sites, it has long been known that the competitive ratio of any deterministic algorithm is at least $2m-1$. Kalyanasundaram and Pruhs conjectured in 1998 that a deterministic $(2m-1)$-competitive algorithm exists for this problem, a conjecture that has remained open for over two decades.","authors":["Tsubasa Harada","Toshiya Itoh"],"url":"https://arxiv.org/abs/2406.03778"}
{"created":"2025-04-24","title":"Solving Inverse Problems in Protein Space Using Diffusion-Based Priors","abstract":"The interaction of a protein with its environment can be understood and controlled via its 3D structure. Experimental methods for protein structure determination, such as X-ray crystallography or cryogenic electron microscopy, shed light on biological processes but introduce challenging inverse problems. Learning-based approaches have emerged as accurate and efficient methods to solve these inverse problems for 3D structure determination, but are specialized for a predefined type of measurement. Here, we introduce a versatile framework to turn biophysical measurements, such as cryo-EM density maps, into 3D atomic models. Our method combines a physics-based forward model of the measurement process with a pretrained generative model providing a task-agnostic, data-driven prior. Our method outperforms posterior sampling baselines on linear and non-linear inverse problems. In particular, it is the first diffusion-based method for refining atomic models from cryo-EM maps and building atomic models from sparse distance matrices.","authors":["Axel Levy","Eric R. Chan","Sara Fridovich-Keil","Fr\\'ed\\'eric Poitevin","Ellen D. Zhong","Gordon Wetzstein"],"url":"https://arxiv.org/abs/2406.04239"}
{"created":"2025-04-24","title":"Pessimism Traps and Algorithmic Interventions","abstract":"In this paper, we relate the philosophical literature on pessimism traps to information cascades, a formal model derived from the economics and mathematics literature. A pessimism trap is a social pattern in which individuals in a community, in situations of uncertainty, begin to copy the sub-optimal actions of others, despite their individual beliefs. This maps nicely onto the concept of an information cascade, which involves a sequence of agents making a decision between two alternatives, with a private signal of the superior alternative and a public history of others' actions. Key results from the economics literature show that information cascades occur with probability one in many contexts, and depending on the strength of the signal, populations can fall into the incorrect cascade very easily and quickly. Once formed, in the absence of external perturbation, a cascade cannot be broken -- therefore, we derive an intervention that can be used to nudge a population from an incorrect to a correct cascade and, importantly, maintain the cascade once the subsidy is discontinued. We study this both theoretically and empirically.","authors":["Avrim Blum","Emily Diana","Kavya Ravichandran","Alexander Williams Tolbert"],"url":"https://arxiv.org/abs/2406.04462"}
{"created":"2025-04-24","title":"Multi-Label Requirements Classification with Large Taxonomies","abstract":"Classification aids software development activities by organizing requirements in classes for easier access and retrieval. The majority of requirements classification research has, so far, focused on binary or multi-class classification. Multi-label classification with large taxonomies could aid requirements traceability but is prohibitively costly with supervised training. Hence, we investigate zero-short learning to evaluate the feasibility of multi-label requirements classification with large taxonomies. We associated, together with domain experts from the industry, 129 requirements with 769 labels from taxonomies ranging between 250 and 1183 classes. Then, we conducted a controlled experiment to study the impact of the type of classifier, the hierarchy, and the structural characteristics of taxonomies on the classification performance. The results show that: (1) The sentence-based classifier had a significantly higher recall compared to the word-based classifier; however, the precision and F1-score did not improve significantly. (2) The hierarchical classification strategy did not always improve the performance of requirements classification. (3) The total and leaf nodes of the taxonomies have a strong negative correlation with the recall of the hierarchical sentence-based classifier. We investigate the problem of multi-label requirements classification with large taxonomies, illustrate a systematic process to create a ground truth involving industry participants, and provide an analysis of different classification pipelines using zero-shot learning.","authors":["Waleed Abdeen","Michael Unterkalmsteiner","Krzysztof Wnuk","Alexandros Chirtoglou","Christoph Schimanski","Heja Goli"],"url":"https://arxiv.org/abs/2406.04797"}
{"created":"2025-04-24","title":"Memory Complexity of Estimating Entropy and Mutual Information","abstract":"We observe an infinite sequence of independent identically distributed random variables $X_1,X_2,\\ldots$ drawn from an unknown distribution $p$ over $[n]$, and our goal is to estimate the entropy $H(p)=-\\mathbb{E}[\\log p(X)]$ within an $\\varepsilon$-additive error. To that end, at each time point we are allowed to update a finite-state machine with $S$ states, using a possibly randomized but time-invariant rule, where each state of the machine is assigned an entropy estimate. Our goal is to characterize the minimax memory complexity $S^*$ of this problem, which is the minimal number of states for which the estimation task is feasible with probability at least $1-\\delta$ asymptotically, uniformly in $p$. Specifically, we show that there exist universal constants $C_1$ and $C_2$ such that $ S^* \\leq C_1\\cdot\\frac{n (\\log n)^4}{\\varepsilon^2\\delta}$ for $\\varepsilon$ not too small, and $S^* \\geq C_2 \\cdot \\max \\{n, \\frac{\\log n}{\\varepsilon}\\}$ for $\\varepsilon$ not too large. The upper bound is proved using approximate counting to estimate the logarithm of $p$, and a finite memory bias estimation machine to estimate the expectation operation. The lower bound is proved via a reduction of entropy estimation to uniformity testing. We also apply these results to derive bounds on the memory complexity of mutual information estimation.","authors":["Tomer Berg","Or Ordentlich","Ofer Shayevitz"],"url":"https://arxiv.org/abs/2406.06312"}
{"created":"2025-04-24","title":"Pushing the Frontier on Approximate EFX Allocations","abstract":"We study the problem of allocating a set of indivisible goods to a set of agents with additive valuation functions, aiming to achieve approximate envy-freeness up to any good ($\\alpha$-EFX). The state-of-the-art results on the problem include that (exact) EFX allocations exist when (a) there are at most three agents, or (b) the agents' valuation functions can take at most two values, or (c) the agents' valuation functions can be represented via a graph. For $\\alpha$-EFX, it is known that a $0.618$-EFX allocation exists for any number of agents with additive valuation functions. In this paper, we show that $2/3$-EFX allocations exist when (a) there are at most \\emph{seven agents}, (b) the agents' valuation functions can take at most \\emph{three values}, or (c) the agents' valuation functions can be represented via a \\emph{multigraph}. Our results can be interpreted in two ways. First, by relaxing the notion of EFX to $2/3$-EFX, we obtain existence results for strict generalizations of the settings for which exact EFX allocations are known to exist. Secondly, by imposing restrictions on the setting, we manage to beat the barrier of $0.618$ and achieve an approximation guarantee of $2/3$. Therefore, our results push the \\emph{frontier} of existence and computation of approximate EFX allocations, and provide insights into the challenges of settling the existence of exact EFX allocations.","authors":["Georgios Amanatidis","Aris Filos-Ratsikas","Alkmini Sgouritsa"],"url":"https://arxiv.org/abs/2406.12413"}
{"created":"2025-04-24","title":"Synthetic Lyrics Detection Across Languages and Genres","abstract":"In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.","authors":["Yanis Labrak","Markus Frohmann","Gabriel Meseguer-Brocal","Elena V. Epure"],"url":"https://arxiv.org/abs/2406.15231"}
{"created":"2025-04-24","title":"A Language for Smart Contracts with Secure Control Flow (Technical Report)","abstract":"Smart contracts are frequently vulnerable to control-flow attacks based on confused deputies, reentrancy, and incorrect error handling. These attacks exploit the complexity of interactions among multiple possibly unknown contracts. Existing best practices to prevent vulnerabilities rely on code patterns and heuristics that produce both false positives and false negatives. Even with extensive audits and heuristic tools, new vulnerabilities continue to arise, routinely costing tens of millions of dollars.","authors":["Siqiu Yao","Haobin Ni","Stephanie Ma","Noah Schiff","Andrew C. Myers","Ethan Cecchetti"],"url":"https://arxiv.org/abs/2407.01204"}
{"created":"2025-04-24","title":"SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from Unstructured Clinical Narratives in Epilepsy","abstract":"Large Language Models (LLMs) have been shown to encode clinical knowledge. Many evaluations, however, rely on structured question-answer benchmarks, overlooking critical challenges of interpreting and reasoning about unstructured clinical narratives in real-world settings. Using free-text clinical descriptions, we present SemioLLM, an evaluation framework that benchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of 1,269 seizure descriptions, we show that most LLMs are able to accurately and confidently generate probabilistic predictions of seizure onset zones in the brain. Most models approach clinician-level performance after prompt engineering, with expert-guided chain-of-thought reasoning leading to the most consistent improvements. Performance was further strongly modulated by clinical in-context impersonation, narrative length and language context (13.7%, 32.7% and 14.2% performance variation, respectively). However, expert analysis of reasoning outputs revealed that correct prediction can be based on hallucinated knowledge and deficient source citation accuracy, underscoring the need to improve interpretability of LLMs in clinical use. Overall, SemioLLM provides a scalable, domain-adaptable framework for evaluating LLMs in clinical disciplines where unstructured verbal descriptions encode diagnostic information. By identifying both the strengths and limitations of state-of-the-art models, our work supports the development of clinically robust and globally applicable AI systems for healthcare.","authors":["Meghal Dani","Muthu Jeyanthi Prakash","Zeynep Akata","Stefanie Liebe"],"url":"https://arxiv.org/abs/2407.03004"}
{"created":"2025-04-24","title":"P2P: Part-to-Part Motion Cues Guide a Strong Tracking Framework for LiDAR Point Clouds","abstract":"3D single object tracking (SOT) methods based on appearance matching has long suffered from insufficient appearance information incurred by incomplete, textureless and semantically deficient LiDAR point clouds. While motion paradigm exploits motion cues instead of appearance matching for tracking, it incurs complex multi-stage processing and segmentation module. In this paper, we first provide in-depth explorations on motion paradigm, which proves that (\\textbf{i}) it is feasible to directly infer target relative motion from point clouds across consecutive frames; (\\textbf{ii}) fine-grained information comparison between consecutive point clouds facilitates target motion modeling. We thereby propose to perform part-to-part motion modeling for consecutive point clouds and introduce a novel tracking framework, termed \\textbf{P2P}. The novel framework fuses each corresponding part information between consecutive point clouds, effectively exploring detailed information changes and thus modeling accurate target-related motion cues. Following this framework, we present P2P-point and P2P-voxel models, incorporating implicit and explicit part-to-part motion modeling by point- and voxel-based representation, respectively. Without bells and whistles, P2P-voxel sets a new state-of-the-art performance ($\\sim$\\textbf{89\\%}, \\textbf{72\\%} and \\textbf{63\\%} precision on KITTI, NuScenes and Waymo Open Dataset, respectively). Moreover, under the same point-based representation, P2P-point outperforms the previous motion tracker M$^2$Track by \\textbf{3.3\\%} and \\textbf{6.7\\%} on the KITTI and NuScenes, while running at a considerably high speed of \\textbf{107 Fps} on a single RTX3090 GPU. The source code and pre-trained models are available at https://github.com/haooozi/P2P.","authors":["Jiahao Nie","Fei Xie","Sifan Zhou","Xueyi Zhou","Dong-Kyu Chae","Zhiwei He"],"url":"https://arxiv.org/abs/2407.05238"}
{"created":"2025-04-24","title":"Tightly-Coupled LiDAR-IMU-Wheel Odometry with an Online Neural Kinematic Model Learning via Factor Graph Optimization","abstract":"Environments lacking geometric features (e.g., tunnels and long straight corridors) are challenging for LiDAR-based odometry algorithms because LiDAR point clouds degenerate in such environments. For wheeled robots, a wheel kinematic model (i.e., wheel odometry) can improve the reliability of the odometry estimation. However, the kinematic model suffers from complex motions (e.g., wheel slippage, lateral movement) in the case of skid-steering robots particularly because this robot model rotates by skidding its wheels. Furthermore, these errors change nonlinearly when the wheel slippage is large (e.g., drifting) and are subject to terrain-dependent parameters. To simultaneously tackle point cloud degeneration and the kinematic model errors, we developed a LiDAR-IMU-wheel odometry algorithm incorporating online training of a neural network that learns the kinematic model of wheeled robots with nonlinearity. We propose to train the neural network online on a factor graph along with robot states, allowing the learning-based kinematic model to adapt to the current terrain condition. The proposed method jointly solves online training of the neural network and LiDAR-IMU-wheel odometry on a unified factor graph to retain the consistency of all those constraints. Through experiments, we first verified that the proposed network adapted to a changing environment, resulting in an accurate odometry estimation across different environments. We then confirmed that the proposed odometry estimation algorithm was robust against point cloud degeneration and nonlinearity (e.g., large wheel slippage by drifting) of the kinematic model. The summary video is available here: https://www.youtube.com/watch?v=CvRVhdda7Cw","authors":["Taku Okawara","Kenji Koide","Shuji Oishi","Masashi Yokozuka","Atsuhiko Banno","Kentaro Uno","Kazuya Yoshida"],"url":"https://arxiv.org/abs/2407.08907"}
{"created":"2025-04-24","title":"ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code Generation","abstract":"Recently, large language models (LLMs) have demonstrated excellent performance, inspiring researchers to explore their use in automating register transfer level (RTL) code generation and improving hardware design efficiency. However, the existing approaches to fine-tune LLMs for RTL generation typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data, which are costly to acquire. To mitigate these issues, we innovatively introduce an iterative training paradigm named ITERTL. During each iteration, samples are drawn from the model trained in the previous cycle. Then these new samples are employed for training in current loop. Furthermore, we introduce a plug-and-play data filtering strategy, thereby encouraging the model to generate high-quality, self-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA) open-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human benchmark. Under similar conditions of data quantity and quality, our approach significantly outperforms the baseline. Extensive experiments validate the effectiveness of the proposed method.","authors":["Peiyang Wu","Nan Guo","Xiao Xiao","Wenming Li","Xiaochun Ye","Dongrui Fan"],"url":"https://arxiv.org/abs/2407.12022"}
{"created":"2025-04-24","title":"Towards the Causal Complete Cause of Multi-Modal Representation Learning","abstract":"Multi-Modal Learning (MML) aims to learn effective representations across modalities for accurate predictions. Existing methods typically focus on modality consistency and specificity to learn effective representations. However, from a causal perspective, they may lead to representations that contain insufficient and unnecessary information. To address this, we propose that effective MML representations should be causally sufficient and necessary. Considering practical issues like spurious correlations and modality conflicts, we relax the exogeneity and monotonicity assumptions prevalent in prior works and explore the concepts specific to MML, i.e., Causal Complete Cause (\\(C^3\\)). We begin by defining \\(C^3\\), which quantifies the probability of representations being causally sufficient and necessary. We then discuss the identifiability of \\(C^3\\) and introduce an instrumental variable to support identifying \\(C^3\\) with non-exogeneity and non-monotonicity. Building on this, we conduct the $C^3$ measurement, i.e., \\(C^3\\) risk. We propose a twin network to estimate it through (i) the real-world branch: utilizing the instrumental variable for sufficiency, and (ii) the hypothetical-world branch: applying gradient-based counterfactual modeling for necessity. Theoretical analyses confirm its reliability. Based on these results, we propose $C^3$ Regularization, a plug-and-play method that enforces the causal completeness of the learned representations by minimizing \\(C^3\\) risk. Extensive experiments demonstrate its effectiveness.","authors":["Jingyao Wang","Siyu Zhao","Wenwen Qiang","Jiangmeng Li","Fuchun Sun","Hui Xiong"],"url":"https://arxiv.org/abs/2407.14058"}
{"created":"2025-04-24","title":"Error Detection and Constraint Recovery in Hierarchical Multi-Label Classification without Prior Knowledge","abstract":"Recent advances in Hierarchical Multi-label Classification (HMC), particularly neurosymbolic-based approaches, have demonstrated improved consistency and accuracy by enforcing constraints on a neural model during training. However, such work assumes the existence of such constraints a-priori. In this paper, we relax this strong assumption and present an approach based on Error Detection Rules (EDR) that allow for learning explainable rules about the failure modes of machine learning models. We show that these rules are not only effective in detecting when a machine learning classifier has made an error but also can be leveraged as constraints for HMC, thereby allowing the recovery of explainable constraints even if they are not provided. We show that our approach is effective in detecting machine learning errors and recovering constraints, is noise tolerant, and can function as a source of knowledge for neurosymbolic models on multiple datasets, including a newly introduced military vehicle recognition dataset.","authors":["Joshua Shay Kricheli","Khoa Vo","Aniruddha Datta","Spencer Ozgur","Paulo Shakarian"],"url":"https://arxiv.org/abs/2407.15192"}
{"created":"2025-04-24","title":"DiffArtist: Towards Structure and Appearance Controllable Image Stylization","abstract":"Artistic style includes both structural and appearance elements. Existing neural stylization techniques primarily focus on transferring appearance features such as color and texture, often neglecting the equally crucial aspect of structural stylization. In this paper, we present a comprehensive study on the simultaneous stylization of structure and appearance of 2D images. Specifically, we introduce DiffArtist, which, to the best of our knowledge, is the first stylization method to allow for dual controllability over structure and appearance. Our key insight is to represent structure and appearance as separate diffusion processes to achieve complete disentanglement without requiring any training, thereby endowing users with unprecedented controllability for both components. The evaluation of stylization of both appearance and structure, however, remains challenging as it necessitates semantic understanding. To this end, we further propose a Multimodal LLM-based style evaluator, which better aligns with human preferences than metrics lacking semantic understanding. With this powerful evaluator, we conduct extensive analysis, demonstrating that DiffArtist achieves superior style fidelity, editability, and structure-appearance disentanglement. These merits make DiffArtist a highly versatile solution for creative applications. Project homepage: https://github.com/songrise/Artist.","authors":["Ruixiang Jiang","Changwen Chen"],"url":"https://arxiv.org/abs/2407.15842"}
{"created":"2025-04-24","title":"Well-posedness of the Stokes problem under modified pressure Dirichlet boundary conditions","abstract":"This paper shows that the Stokes problem is well-posed when velocity and pressure simultaneously vanish on the domain boundary. This result is achieved by extending Ne\\v{c}as' inequality to square-integrable functions that vanish in a small band covering the boundary. It is found that the associated a priori pressure estimate depends inversely on the volume of the band. Numerical experiments confirm these findings. Based on these results, guidelines are provided for applying vanishing pressure boundary conditions in model coupling and domain decomposition methods.","authors":["Igor Tominec","Josefin Ahlkrona","Malte Braack"],"url":"https://arxiv.org/abs/2407.15971"}
{"created":"2025-04-24","title":"Lawma: The Power of Specialization for Legal Annotation","abstract":"Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, our understanding of how to best utilize large language models for legal annotation remains limited. To bridge this gap, we introduce CaselawQA, a benchmark comprising 260 legal annotation tasks, nearly all new to the machine learning community. We demonstrate that commercial models, such as GPT-4.5 and Claude 3.7 Sonnet, achieve non-trivial yet highly variable accuracy, generally falling short of the performance required for legal work. We then demonstrate that small, lightly fine-tuned models outperform commercial models. A few hundred to a thousand labeled examples are usually enough to achieve higher accuracy. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal annotation tasks with some available labeled data, researchers are likely better off using a fine-tuned open-source model.","authors":["Ricardo Dominguez-Olmedo","Vedant Nanda","Rediet Abebe","Stefan Bechtold","Christoph Engel","Jens Frankenreiter","Krishna Gummadi","Moritz Hardt","Michael Livermore"],"url":"https://arxiv.org/abs/2407.16615"}
{"created":"2025-04-24","title":"Modelling Multimodal Integration in Human Concept Processing with Vision-Language Models","abstract":"Text representations from language models have proven remarkably predictive of human neural activity involved in language processing, with the recent transformer-based models outperforming previous architectures in downstream tasks and prediction of brain responses. However, the word representations learnt by language-only models may be limited in that they lack sensory information from other modalities, which several cognitive and neuroscience studies showed to be reflected in human meaning representations. Here, we leverage current pre-trained vision-language models (VLMs) to investigate whether the integration of visuo-linguistic information they operate leads to representations that are more aligned with human brain activity than those obtained by models trained with language-only input. We focus on fMRI responses recorded while participants read concept words in the context of either a full sentence or a picture. Our results reveal that VLM representations correlate more strongly than those by language-only models with activations in brain areas functionally related to language processing. Additionally, we find that transformer-based vision-language encoders -- e.g., LXMERT and VisualBERT -- yield more brain-aligned representations than generative VLMs, whose autoregressive abilities do not seem to provide an advantage when modelling single words. Finally, our ablation analyses suggest that the high brain alignment achieved by some of the VLMs we evaluate results from semantic information acquired specifically during multimodal pretraining as opposed to being already encoded in their unimodal modules. Altogether, our findings indicate an advantage of multimodal models in predicting human brain activations, which reveals that modelling language and vision integration has the potential to capture the multimodal nature of human concept representations.","authors":["Anna Bavaresco","Marianne de Heer Kloots","Sandro Pezzelle","Raquel Fern\\'andez"],"url":"https://arxiv.org/abs/2407.17914"}
{"created":"2025-04-24","title":"Con4m: Context-aware Consistency Learning Framework for Segmented Time Series Classification","abstract":"Time Series Classification (TSC) encompasses two settings: classifying entire sequences or classifying segmented subsequences. The raw time series for segmented TSC usually contain Multiple classes with Varying Duration of each class (MVD). Therefore, the characteristics of MVD pose unique challenges for segmented TSC, yet have been largely overlooked by existing works. Specifically, there exists a natural temporal dependency between consecutive instances (segments) to be classified within MVD. However, mainstream TSC models rely on the assumption of independent and identically distributed (i.i.d.), focusing on independently modeling each segment. Additionally, annotators with varying expertise may provide inconsistent boundary labels, leading to unstable performance of noise-free TSC models. To address these challenges, we first formally demonstrate that valuable contextual information enhances the discriminative power of classification instances. Leveraging the contextual priors of MVD at both the data and label levels, we propose a novel consistency learning framework Con4m, which effectively utilizes contextual information more conducive to discriminating consecutive segments in segmented TSC tasks, while harmonizing inconsistent boundary labels for training. Extensive experiments across multiple datasets validate the effectiveness of Con4m in handling segmented TSC tasks on MVD. The source code is available at https://github.com/MrNobodyCali/Con4m.","authors":["Junru Chen","Tianyu Cao","Jing Xu","Jiahe Li","Zhilong Chen","Tao Xiao","Yang Yang"],"url":"https://arxiv.org/abs/2408.00041"}
{"created":"2025-04-24","title":"Adapting General Disentanglement-Based Speaker Anonymization for Enhanced Emotion Preservation","abstract":"A general disentanglement-based speaker anonymization system typically separates speech into content, speaker, and prosody features using individual encoders. This paper explores how to adapt such a system when a new speech attribute, for example, emotion, needs to be preserved to a greater extent. While existing systems are good at anonymizing speaker embeddings, they are not designed to preserve emotion. Two strategies for this are examined. First, we show that integrating emotion embeddings from a pre-trained emotion encoder can help preserve emotional cues, even though this approach slightly compromises privacy protection. Alternatively, we propose an emotion compensation strategy as a post-processing step applied to anonymized speaker embeddings. This conceals the original speaker's identity and reintroduces the emotional traits lost during speaker embedding anonymization. Specifically, we model the emotion attribute using support vector machines to learn separate boundaries for each emotion. During inference, the original speaker embedding is processed in two ways: one, by an emotion indicator to predict emotion and select the emotion-matched SVM accurately; and two, by a speaker anonymizer to conceal speaker characteristics. The anonymized speaker embedding is then modified along the corresponding SVM boundary towards an enhanced emotional direction to save the emotional cues. The proposed strategies are also expected to be useful for adapting a general disentanglement-based speaker anonymization system to preserve other target paralinguistic attributes, with potential for a range of downstream tasks.","authors":["Xiaoxiao Miao","Yuxiang Zhang","Xin Wang","Natalia Tomashenko","Donny Cheng Lock Soh","Ian Mcloughlin"],"url":"https://arxiv.org/abs/2408.05928"}
{"created":"2025-04-24","title":"The advantages of context specific language models: the case of the Erasmian Language Model","abstract":"The current trend to improve language model performance seems to be based on scaling up with the number of parameters (e.g. the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the model. However this comes at significant costs in terms of computational resources and energy costs that compromise the sustainability of AI solutions, as well as risk relating to privacy and misuse. In this paper we present the Erasmian Language Model (ELM) a small context specific, 900 million parameter model, pre-trained and fine-tuned by and for Erasmus University Rotterdam. We show how the model performs adequately in a classroom context for essay writing, and how it achieves superior performance in subjects that are part of its context. This has implications for a wide range of institutions and organizations, showing that context specific language models may be a viable alternative for resource constrained, privacy sensitive use cases.","authors":["Jo\\~ao Gon\\c{c}alves","Nick Jelicic","Michele Murgia","Evert Stamhuis"],"url":"https://arxiv.org/abs/2408.06931"}
{"created":"2025-04-24","title":"PooDLe: Pooled and dense self-supervised learning from naturalistic videos","abstract":"Self-supervised learning has driven significant progress in learning from single-subject, iconic images. However, there are still unanswered questions about the use of minimally-curated, naturalistic video data, which contain dense scenes with many independent objects, imbalanced class distributions, and varying object sizes. In this paper, we propose PooDLe, a self-supervised learning method that combines an invariance-based objective on pooled representations with a dense SSL objective that enforces equivariance to optical flow warping. Our results show that a unified objective applied at multiple feature scales is essential for learning effective image representations from naturalistic videos. We validate our method with experiments on the BDD100K driving video dataset and the Walking Tours first-person video dataset, demonstrating its ability to capture spatial understanding from a dense objective and semantic understanding via a pooled representation objective.","authors":["Alex N. Wang","Christopher Hoang","Yuwen Xiong","Yann LeCun","Mengye Ren"],"url":"https://arxiv.org/abs/2408.11208"}
{"created":"2025-04-24","title":"Practical Aspects on Solving Differential Equations Using Deep Learning: A Primer","abstract":"Deep learning has become a popular tool across many scientific fields, including the study of differential equations, particularly partial differential equations. This work introduces the basic principles of deep learning and the Deep Galerkin method, which uses deep neural networks to solve differential equations. This primer aims to provide technical and practical insights into the Deep Galerkin method and its implementation. We demonstrate how to solve the one-dimensional heat equation step-by-step. We also show how to apply the Deep Galerkin method to solve systems of ordinary differential equations and integral equations, such as the Fredholm of the second kind. Additionally, we provide code snippets within the text and the complete source code on Github. The examples are designed so that one can run them on a simple computer without needing a GPU.","authors":["Georgios Is. Detorakis"],"url":"https://arxiv.org/abs/2408.11266"}
{"created":"2025-04-24","title":"Formalizing equivalences without tears","abstract":"This expository note describes two convenient techniques in the context of homotopy type theory for proving and formalizing that a given map is an equivalence. The first technique decomposes the map as a series of basic equivalences, while the second refines this approach using the 3-for-2 property of equivalences. The techniques are illustrated by proving a basic result in synthetic homotopy theory.","authors":["Tom de Jong"],"url":"https://arxiv.org/abs/2408.11501"}
{"created":"2025-04-24","title":"Top Score on the Wrong Exam: On Benchmarking in Machine Learning for Vulnerability Detection","abstract":"According to our survey of machine learning for vulnerability detection (ML4VD), 9 in every 10 papers published in the past five years define ML4VD as a function-level binary classification problem:","authors":["Niklas Risse","Jing Liu","Marcel B\\\"ohme"],"url":"https://arxiv.org/abs/2408.12986"}
{"created":"2025-04-24","title":"Coverage and metadata completeness and accuracy of African research publications in OpenAlex: A comparative analysis","abstract":"Unlike traditional proprietary data sources such as Scopus and the Web of Science (WoS), OpenAlex emphasizes its comprehensiveness. This study analyzes OpenAlex coverage and metadata completeness and accuracy of African research publications. To achieve this, OpenAlex is compared with Scopus, WoS, and African Journals Online (AJOL). First, we examine the coverage of African research publications in OpenAlex relative to Scopus, WoS, and AJOL. Then, we assess and compare the availability and accuracy of metadata in OpenAlex, Scopus, and WoS. The findings indicate that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex provides high coverage for publication and author information, though its coverage of affiliations, references, and funder information is comparatively lower. Metadata accuracy is similarly high for publication and author fields, while affiliation, reference, and funding information show higher rates of missing or incomplete data. Notably, the results demonstrate that both metadata availability and accuracy in OpenAlex improve significantly for publications also indexed in Scopus and WoS. These findings suggest that OpenAlex has the potential to replace proprietary data sources for certain types of analyses. However, for some metadata fields, there remains a trade-off between extensiveness and accuracy.","authors":["Patricia Alonso-Alvarez","Nees Jan van Eck"],"url":"https://arxiv.org/abs/2409.01120"}
{"created":"2025-04-24","title":"Oh the Prices You'll See: Designing a Fair Exchange System to Mitigate Personalized Pricing","abstract":"Many online marketplaces personalize prices based on consumer attributes. Since these prices are private, consumers may be unaware that they have spent more on a good than the lowest possible price, and cannot easily take action to pay less. In this paper, we introduce a fairness-centered exchange system that takes advantage of personalized pricing, while still allowing consumers to individually benefit. Our system produces a matching of consumers to promote trading; the lower-paying consumer buys the good for the higher-paying consumer for some fee. We explore various modeling choices and fairness targets to determine which schema will leave consumers best off, while also earning revenue for the system itself. We show that when consumers individually negotiate the transaction price, and our fairness objective is to minimize mean net cost, we are able to achieve the most fair outcomes. Conversely, when transaction prices are centrally set, consumers are often unwilling to transact. When price dispersion (or range) is high, the system can reduce the mean net cost to each individual by $66\\%$, or the mean net cost to a group by $69\\%$. We find that a high dispersion of original prices is necessary for our system to be viable. Higher dispersion can actually lead to decreased net price paid by consumers, and act as a check against extreme personalization, increasing seller accountability. Our results provide theoretical evidence that such a system could improve fairness for consumers while sustaining itself financially.","authors":["Aditya Karan","Naina Balepur","Hari Sundaram"],"url":"https://arxiv.org/abs/2409.02777"}
{"created":"2025-04-24","title":"A Survey on Mixup Augmentations and Beyond","abstract":"As Deep Neural Networks have achieved thrilling breakthroughs in the past decade, data augmentations have garnered increasing attention as regularization techniques when massive labeled data are unavailable. Among existing augmentations, Mixup and relevant data-mixing methods that convexly combine selected samples and the corresponding labels are widely adopted because they yield high performances by generating data-dependent virtual data while easily migrating to various domains. This survey presents a comprehensive review of foundational mixup methods and their applications. We first elaborate on the training pipeline with mixup augmentations as a unified framework containing modules. A reformulated framework could contain various mixup methods and give intuitive operational procedures. Then, we systematically investigate the applications of mixup augmentations on vision downstream tasks, various data modalities, and some analysis \\& theorems of mixup. Meanwhile, we conclude the current status and limitations of mixup research and point out further work for effective and efficient mixup augmentations. This survey can provide researchers with the current state of the art in mixup methods and provide some insights and guidance roles in the mixup arena. An online project with this survey is available at https://github.com/Westlake-AI/Awesome-Mixup.","authors":["Xin Jin","Hongyu Zhu","Siyuan Li","Zedong Wang","Zicheng Liu","Juanxi Tian","Chang Yu","Huafeng Qin","Stan Z. Li"],"url":"https://arxiv.org/abs/2409.05202"}
{"created":"2025-04-24","title":"On a shrink-and-expand technique for symmetric block eigensolvers","abstract":"In symmetric block eigenvalue algorithms, such as the subspace iteration algorithm and the locally optimal block preconditioned conjugate gradient (LOBPCG) algorithm, a large block size is often employed to achieve robustness and rapid convergence. However, using a large block size also increases the computational cost. Traditionally, the block size is typically reduced after convergence of some eigenpairs, known as deflation. In this work, we propose a non-deflation-based, more aggressive technique, where the block size is adjusted dynamically during the algorithm. This technique can be applied to a wide range of block eigensolvers, reducing computational cost without compromising convergence speed. We present three adaptive strategies for adjusting the block size, and apply them to four well-known eigensolvers as examples. Detailed theoretical analysis and numerical experiments are provided to illustrate the efficiency of the proposed technique. In practice, an overall acceleration of 20% to 30% is observed.","authors":["Yuqi Liu","Yuxin Ma","Meiyue Shao"],"url":"https://arxiv.org/abs/2409.05572"}
{"created":"2025-04-24","title":"lamss: when large language models meet self-skepticism","abstract":"Hallucination is a major challenge for large language models (LLMs), prevent ing their further application in some fields. The skeptical thinking of humankind","authors":["Yetao Wu","Yihong Wang","Teng Chen","Ningyuan Xi","Qingqing Gu","Hongyang Lei","Luo Ji"],"url":"https://arxiv.org/abs/2409.06601"}
{"created":"2025-04-24","title":"Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation","abstract":"This paper proposes a lightweight systematic solution for multi-robot coordinated navigation with decentralized cooperative perception. An information flow is first created to facilitate real-time observation sharing over unreliable ad-hoc networks. Then, the environmental uncertainties of each robot are reduced by interaction fields that deliver complementary information. Finally, path optimization is achieved, enabling self-organized coordination with effective convergence, divergence, and collision avoidance. Our method is fully interpretable and ready for deployment without gaps. Comprehensive simulations and real-world experiments demonstrate reduced path redundancy, robust performance across various tasks, and minimal demands on computation and communication.","authors":["Chenxi Li","Weining Lu","Qingquan Lin","Litong Meng","Haolu Li","Bin Liang"],"url":"https://arxiv.org/abs/2409.10049"}
{"created":"2025-04-24","title":"Friedkin-Johnsen Model with Diminishing Competition","abstract":"This letter studies the Friedkin-Johnsen (FJ) model with diminishing competition, or stubbornness. The original FJ model assumes that each agent assigns a constant competition weight to its initial opinion. In contrast, we investigate the effect of diminishing competition on the convergence point and speed of the FJ dynamics. We prove that, if the competition is uniform across agents and vanishes asymptotically, the convergence point coincides with the nominal consensus reached with no competition. However, the diminishing competition slows down convergence according to its own rate of decay. We study this phenomenon analytically and provide upper and lower bounds on the convergence rate. Further, if competition is not uniform across agents, we show that the convergence point may not coincide with the nominal consensus point. Finally, we evaluate our analytical insights numerically.","authors":["Luca Ballotta","\\'Aron V\\'ek\\'assy","Stephanie Gil","Michal Yemini"],"url":"https://arxiv.org/abs/2409.12601"}
{"created":"2025-04-24","title":"Evaluating ML Robustness in GNSS Interference Classification, Characterization & Localization","abstract":"Jamming devices disrupt signals from the global navigation satellite system (GNSS) and pose a significant threat, as they compromise the robustness of accurate positioning. The detection of anomalies within frequency snapshots is crucial to counteract these interferences effectively. A critical preliminary countermeasure involves the reliable classification of interferences and the characterization and localization of jamming devices. This paper introduces an extensive dataset comprising snapshots obtained from a low-frequency antenna that capture various generated interferences within a large-scale environment, including controlled multipath effects. Our objective is to assess the resilience of machine learning (ML) models against environmental changes, such as multipath effects, variations in interference attributes, such as interference class, bandwidth, and signal power, the accuracy of jamming device localization, and the constraints imposed by snapshot input lengths. Furthermore, we evaluate the performance of a diverse set of 129 distinct vision encoder models across all tasks. By analyzing the aleatoric and epistemic uncertainties, we demonstrate the adaptability of our model in generalizing across diverse facets, thus establishing its suitability for real-world applications. Dataset: https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/controlled_low_frequency","authors":["Lucas Heublein","Tobias Feigl","Thorsten Nowak","Alexander R\\\"ugamer","Christopher Mutschler","Felix Ott"],"url":"https://arxiv.org/abs/2409.15114"}
{"created":"2025-04-24","title":"Pix2Next: Leveraging Vision Foundation Models for RGB to NIR Image Translation","abstract":"This paper proposes Pix2Next, a novel image-to-image translation framework designed to address the challenge of generating high-quality Near-Infrared (NIR) images from RGB inputs. Our approach leverages a state-of-the-art Vision Foundation Model (VFM) within an encoder-decoder architecture, incorporating cross-attention mechanisms to enhance feature integration. This design captures detailed global representations and preserves essential spectral characteristics, treating RGB-to-NIR translation as more than a simple domain transfer problem. A multi-scale PatchGAN discriminator ensures realistic image generation at various detail levels, while carefully designed loss functions couple global context understanding with local feature preservation. We performed experiments on the RANUS dataset to demonstrate Pix2Next's advantages in quantitative metrics and visual quality, improving the FID score by 34.81% compared to existing methods. Furthermore, we demonstrate the practical utility of Pix2Next by showing improved performance on a downstream object detection task using generated NIR data to augment limited real NIR datasets. The proposed approach enables the scaling up of NIR datasets without additional data acquisition or annotation efforts, potentially accelerating advancements in NIR-based computer vision applications.","authors":["Youngwan Jin","Incheol Park","Hanbin Song","Hyeongjin Ju","Yagiz Nalcakan","Shiho Kim"],"url":"https://arxiv.org/abs/2409.16706"}
{"created":"2025-04-24","title":"Building Real-time Awareness of Out-of-distribution in Trajectory Prediction for Autonomous Vehicles","abstract":"Accurate trajectory prediction is essential for the safe operation of autonomous vehicles in real-world environments. Even well-trained machine learning models may produce unreliable predictions due to discrepancies between training data and real-world conditions encountered during inference. In particular, the training dataset tends to overrepresent common scenes (e.g., straight lanes) while underrepresenting less frequent ones (e.g., traffic circles). In addition, it often overlooks unpredictable real-world events such as sudden braking or falling objects. To ensure safety, it is critical to detect in real-time when a model's predictions become unreliable. Leveraging the intuition that in-distribution (ID) scenes exhibit error patterns similar to training data, while out-of-distribution (OOD) scenes do not, we introduce a principled, real-time approach for OOD detection by framing it as a change-point detection problem. We address the challenging settings where the OOD scenes are deceptive, meaning that they are not easily detectable by human intuitions. Our lightweight solutions can handle the occurrence of OOD at any time during trajectory prediction inference. Experimental results on multiple real-world datasets using a benchmark trajectory prediction model demonstrate the effectiveness of our methods.","authors":["Tongfe Guo","Taposh Banerjee","Rui Liu","Lili Su"],"url":"https://arxiv.org/abs/2409.17277"}
{"created":"2025-04-24","title":"Pre-Chirp-Domain Index Modulation for Full-Diversity Affine Frequency Division Multiplexing towards 6G","abstract":"Affine frequency division multiplexing (AFDM), tailored as a superior multicarrier technique utilizing chirp signals for high-mobility communications, is envisioned as a promising candidate for the sixth-generation (6G) wireless network. AFDM is based on the discrete affine Fourier transform (DAFT) with two adjustable parameters of the chirp signals, termed as the pre-chirp and post-chirp parameters, respectively. We show that the pre-chirp counterpart can be flexibly manipulated for additional degree-of-freedom (DoF). Therefore, this paper proposes a novel AFDM scheme with the pre-chirp index modulation (PIM) philosophy (AFDM-PIM), which can implicitly convey extra information bits through dynamic pre-chirp parameter assignment, thus enhancing both spectral and energy efficiency. Specifically, we first demonstrate that the subcarrier orthogonality is still maintained by applying distinct pre-chirp parameters to various subcarriers in the AFDM modulation process. Inspired by this property, each AFDM subcarrier is constituted with a unique pre-chirp signal according to the incoming bits. By such arrangement, extra binary bits can be embedded into the index patterns of pre-chirp parameter assignment without additional energy consumption. For performance analysis, we derive the asymptotically tight upper bounds on the average bit error rates (BERs) of the proposed schemes with maximum-likelihood (ML) detection, and validate that the proposed AFDM-PIM can achieve the optimal diversity order under doubly dispersive channels. Based on the derivations, we further propose an optimal pre-chirp alphabet design to enhance the BER performance via intelligent optimization algorithms. Simulations demonstrate that the proposed AFDM-PIM outperforms the classical benchmarks under doubly dispersive channel.","authors":["Guangyao Liu","Tianqi Mao","Zhenyu Xiao","Miaowen Wen","Ruiqi Liu","Jingjing Zhao","Ertugrul Basar","Zhaocheng Wang","Sheng Chen"],"url":"https://arxiv.org/abs/2410.00313"}
{"created":"2025-04-24","title":"M2P2: A Multi-Modal Passive Perception Dataset for Off-Road Mobility in Extreme Low-Light Conditions","abstract":"Long-duration, off-road, autonomous missions require robots to continuously perceive their surroundings regardless of the ambient lighting conditions. Most existing autonomy systems heavily rely on active sensing, e.g., LiDAR, RADAR, and Time-of-Flight sensors, or use (stereo) visible light imaging sensors, e.g., color cameras, to perceive environment geometry and semantics. In scenarios where fully passive perception is required and lighting conditions are degraded to an extent that visible light cameras fail to perceive, most downstream mobility tasks such as obstacle avoidance become impossible. To address such a challenge, this paper presents a Multi-Modal Passive Perception dataset, M2P2, to enable off-road mobility in low-light to no-light conditions. We design a multi-modal sensor suite including thermal, event, and stereo RGB cameras, GPS, two Inertia Measurement Units (IMUs), as well as a high-resolution LiDAR for ground truth, with a novel multi-sensor calibration procedure that can efficiently transform multi-modal perceptual streams into a common coordinate system. Our 10-hour, 32 km dataset also includes mobility data such as robot odometry and actions and covers well-lit, low-light, and no-light conditions, along with paved, on-trail, and off-trail terrain. Our results demonstrate that off-road mobility is possible through only passive perception in extreme low-light conditions using end-to-end learning and classical planning. The project website can be found at https://cs.gmu.edu/~xiao/Research/M2P2/","authors":["Aniket Datar","Anuj Pokhrel","Mohammad Nazeri","Madhan B. Rao","Chenhui Pan","Yufan Zhang","Andre Harrison","Maggie Wigness","Philip R. Osteen","Jinwei Ye","Xuesu Xiao"],"url":"https://arxiv.org/abs/2410.01105"}
{"created":"2025-04-24","title":"Multimodal Situational Safety","abstract":"Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. We argue that for an MLLM to respond safely, whether through language or action, it often needs to assess the safety implications of a language query within its corresponding visual context. To evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. The dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. We also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. Our findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. Furthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response. Code and data: mssbench.github.io.","authors":["Kaiwen Zhou","Chengzhi Liu","Xuandong Zhao","Anderson Compalas","Dawn Song","Xin Eric Wang"],"url":"https://arxiv.org/abs/2410.06172"}
{"created":"2025-04-24","title":"Semantic Segmentation and Scene Reconstruction of RGB-D Image Frames: An End-to-End Modular Pipeline for Robotic Applications","abstract":"Robots operating in unstructured environments require a comprehensive understanding of their surroundings, necessitating geometric and semantic information from sensor data. Traditional RGB-D processing pipelines focus primarily on geometric reconstruction, limiting their ability to support advanced robotic perception, planning, and interaction. A key challenge is the lack of generalized methods for segmenting RGB-D data into semantically meaningful components while maintaining accurate geometric representations. We introduce a novel end-to-end modular pipeline that integrates state-of-the-art semantic segmentation, human tracking, point-cloud fusion, and scene reconstruction. Our approach improves semantic segmentation accuracy by leveraging the foundational segmentation model SAM2 with a hybrid method that combines its mask generation with a semantic classification model, resulting in sharper masks and high classification accuracy. Compared to SegFormer and OneFormer, our method achieves a similar semantic segmentation accuracy (mIoU of 47.0% vs 45.9% in the ADE20K dataset) but provides much more precise object boundaries. Additionally, our human tracking algorithm interacts with the segmentation enabling continuous tracking even when objects leave and re-enter the frame by object re-identification. Our point cloud fusion approach reduces computation time by 1.81x while maintaining a small mean reconstruction error of 25.3 mm by leveraging the semantic information. We validate our approach on benchmark datasets and real-world Kinect RGB-D data, demonstrating improved efficiency, accuracy, and usability. Our structured representation, stored in the Universal Scene Description (USD) format, supports efficient querying, visualization, and robotic simulation, making it practical for real-world deployment.","authors":["Zhiwu Zheng","Lauren Mentzer","Berk Iskender","Michael Price","Colm Prendergast","Audren Cloitre"],"url":"https://arxiv.org/abs/2410.17988"}
{"created":"2025-04-24","title":"ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems","abstract":"Retrieval-Augmented Generation (RAG) systems using large language models (LLMs) often generate inaccurate responses due to the retrieval of irrelevant or loosely related information. Existing methods, which operate at the document level, fail to effectively filter out such content. We propose LLM-driven chunk filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and filtering retrieved information at the chunk level. Our approach employs semantic chunking to divide documents into coherent sections and utilizes LLM-based relevance scoring to assess each chunk's alignment with the user's query. By filtering out less pertinent chunks before the generation phase, we significantly reduce hallucinations and improve factual accuracy. Experiments show that our method outperforms existing RAG models, achieving higher accuracy on tasks requiring precise information retrieval. This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications like fact-checking and multi-hop reasoning.","authors":["Ishneet Sukhvinder Singh","Ritvik Aggarwal","Ibrahim Allahverdiyev","Muhammad Taha","Aslihan Akalin","Kevin Zhu","Sean O'Brien"],"url":"https://arxiv.org/abs/2410.19572"}
{"created":"2025-04-24","title":"Predicting sub-population specific viral evolution","abstract":"Forecasting the change in the distribution of viral variants is crucial for therapeutic design and disease surveillance. This task poses significant modeling challenges due to the sharp differences in virus distributions across sub-populations (e.g., countries) and their dynamic interactions. Existing machine learning approaches that model the variant distribution as a whole are incapable of making location-specific predictions and ignore transmissions that shape the viral landscape. In this paper, we propose a sub-population specific protein evolution model, which predicts the time-resolved distributions of viral proteins in different locations. The algorithm explicitly models the transmission rates between sub-populations and learns their interdependence from data. The change in protein distributions across all sub-populations is defined through a linear ordinary differential equation (ODE) parametrized by transmission rates. Solving this ODE yields the likelihood of a given protein occurring in particular sub-populations. Multi-year evaluation on both SARS-CoV-2 and influenza A/H3N2 demonstrates that our model outperforms baselines in accurately predicting distributions of viral proteins across continents and countries. We also find that the transmission rates learned from data are consistent with the transmission pathways discovered by retrospective phylogenetic analysis.","authors":["Wenxian Shi","Menghua Wu","Regina Barzilay"],"url":"https://arxiv.org/abs/2410.21518"}
{"created":"2025-04-24","title":"Automated Trustworthiness Oracle Generation for Machine Learning Text Classifiers","abstract":"Machine learning (ML) for text classification has been widely used in various domains. These applications can significantly impact ethics, economics, and human behavior, raising serious concerns about trusting ML decisions. Studies indicate that conventional metrics are insufficient to build human trust in ML models. These models often learn spurious correlations and predict based on them. In the real world, their performance can deteriorate significantly. To avoid this, a common practice is to test whether predictions are reasonable based on valid patterns in the data. Along with this, a challenge known as the trustworthiness oracle problem has been introduced. Due to the lack of automated trustworthiness oracles, the assessment requires manual validation of the decision process disclosed by explanation methods. However, this is time-consuming, error-prone, and unscalable.","authors":["Lam Nguyen Tung","Steven Cho","Xiaoning Du","Neelofar Neelofar","Valerio Terragni","Stefano Ruberto","Aldeida Aleti"],"url":"https://arxiv.org/abs/2410.22663"}
{"created":"2025-04-24","title":"Attention Tracker: Detecting Prompt Injection Attacks in LLMs","abstract":"Large Language Models (LLMs) have revolutionized various domains but remain vulnerable to prompt injection attacks, where malicious inputs manipulate the model into ignoring original instructions and executing designated action. In this paper, we investigate the underlying mechanisms of these attacks by analyzing the attention patterns within LLMs. We introduce the concept of the distraction effect, where specific attention heads, termed important heads, shift focus from the original instruction to the injected instruction. Building on this discovery, we propose Attention Tracker, a training-free detection method that tracks attention patterns on instruction to detect prompt injection attacks without the need for additional LLM inference. Our method generalizes effectively across diverse models, datasets, and attack types, showing an AUROC improvement of up to 10.0% over existing methods, and performs well even on small LLMs. We demonstrate the robustness of our approach through extensive evaluations and provide insights into safeguarding LLM-integrated systems from prompt injection vulnerabilities.","authors":["Kuo-Han Hung","Ching-Yun Ko","Ambrish Rawat","I-Hsin Chung","Winston H. Hsu","Pin-Yu Chen"],"url":"https://arxiv.org/abs/2411.00348"}
{"created":"2025-04-24","title":"Combining Physics-based and Data-driven Modeling for Building Energy Systems","abstract":"Building energy modeling plays a vital role in optimizing the operation of building energy systems by providing accurate predictions of the building's real-world conditions. In this context, various techniques have been explored, ranging from traditional physics-based models to data-driven models. Recently, researchers are combining physics-based and data-driven models into hybrid approaches. This includes using the physics-based model output as additional data-driven input, learning the residual between physics-based model and real data, learning a surrogate of the physics-based model, or fine-tuning a surrogate model with real data. However, a comprehensive comparison of the inherent advantages of these hybrid approaches is still missing. The primary objective of this work is to evaluate four predominant hybrid approaches in building energy modeling through a real-world case study, with focus on indoor thermodynamics. To achieve this, we devise three scenarios reflecting common levels of building documentation and sensor availability, assess their performance, and analyze their explainability using hierarchical Shapley values. The real-world study reveals three notable findings. First, greater building documentation and sensor availability lead to higher prediction accuracy for hybrid approaches. Second, the performance of hybrid approaches depends on the type of building room, but the residual approach using a Feedforward Neural Network as data-driven sub-model performs best on average across all rooms. This hybrid approach also demonstrates a superior ability to leverage the simulation from the physics-based sub-model. Third, hierarchical Shapley values prove to be an effective tool for explaining and improving hybrid models while accounting for input correlations.","authors":["Leandro Von Krannichfeldt","Kristina Orehounig","Olga Fink"],"url":"https://arxiv.org/abs/2411.01055"}
{"created":"2025-04-24","title":"A Highly Scalable LLM Clusters with Optical Interconnect","abstract":"We propose \\emph{LumosCore} to build high-bandwidth and large-scale data center networks for LLM jobs. By replacing the core-layer electrical packet switches by optical circuit switches, \\emph{LumosCore} could achieves $2\\times$ increase in bandwidth or $8\\times$ increase in network size. We offer the detailed design of \\emph{LumosCore} at both deployment stage and running stage. At deployment stage, we propose Interleaved Wiring, which is compatible with all possible logical topologies. At running stage, we design polynomial-time algorithms for GPU placement, logical topology generating and OCS reconfiguration to minimize network contention and reduce impact to scheduled jobs. We evaluate \\emph{LumosCore} using both testbed experiments and large-scale simulation. Compared to traditional hybrid optical/electrical architectures, \\emph{LumosCore} increases the end-to-end training throughput by up to 39.5\\% on a 128-node testbed. Compared to the state-of-art Clos architectures, \\emph{LumosCore} reduces the average job completion time by up to 34.1\\% in a 16k simulation platform.","authors":["Xinchi Han","Yongxi Lv","Shizhen Zhao","Zhuotao Liu","Ximeng Liu","Xinbing Wang"],"url":"https://arxiv.org/abs/2411.01503"}
{"created":"2025-04-24","title":"MEG: Medical Knowledge-Augmented Large Language Models for Question Answering","abstract":"Question answering is a natural language understanding task that involves reasoning over both explicit context, and unstated relevant domain knowledge. Despite the high cost of training, large language models (LLMs) -- the backbone of most modern question-answering systems -- still struggle to reliably capture the nuanced relationships between concepts that are crucial for reasoning in specialized fields like medicine. In this work, we present MEG, a parameter-efficient approach for medical knowledge-augmented LLMs. MEG uses a lightweight mapping network to incorporate knowledge graph embeddings into the LLM, enabling it to leverage external knowledge in a cost-effective way. We evaluate our method on four popular medical multiple-choice datasets and show that LLMs i) can effectively interpret knowledge graph embeddings and ii) gain significant advantages from the factual grounding these embeddings provide. MEG attains an average of +6.7% and +9.9% accuracy over specialized models like BioMistral-7B and MediTron-7B, respectively. Finally, we show that MEG's performance remains robust to the choice of graph encoder.","authors":["Laura Cabello","Carmen Martin-Turrero","Uchenna Akujuobi","Anders S{\\o}gaard","Carlos Bobed"],"url":"https://arxiv.org/abs/2411.03883"}
{"created":"2025-04-24","title":"Approximate Equivariance in Reinforcement Learning","abstract":"Equivariant neural networks have shown great success in reinforcement learning, improving sample efficiency and generalization when there is symmetry in the task. However, in many problems, only approximate symmetry is present, which makes imposing exact symmetry inappropriate. Recently, approximately equivariant networks have been proposed for supervised classification and modeling physical systems. In this work, we develop approximately equivariant algorithms in reinforcement learning (RL). We define approximately equivariant MDPs and theoretically characterize the effect of approximate equivariance on the optimal $Q$ function. We propose novel RL architectures using relaxed group and steerable convolutions and experiment on several continuous control domains and stock trading with real financial data. Our results demonstrate that the approximately equivariant network performs on par with exactly equivariant networks when exact symmetries are present, and outperforms them when the domains exhibit approximate symmetry. As an added byproduct of these techniques, we observe increased robustness to noise at test time. Our code is available at https://github.com/jypark0/approx_equiv_rl.","authors":["Jung Yeon Park","Sujay Bhatt","Sihan Zeng","Lawson L. S. Wong","Alec Koppel","Sumitra Ganesh","Robin Walters"],"url":"https://arxiv.org/abs/2411.04225"}
{"created":"2025-04-24","title":"Needle Threading: Can LLMs Follow Threads through Near-Million-Scale Haystacks?","abstract":"As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.","authors":["Jonathan Roberts","Kai Han","Samuel Albanie"],"url":"https://arxiv.org/abs/2411.05000"}
{"created":"2025-04-24","title":"Sufficient Context: A New Lens on Retrieval Augmented Generation Systems","abstract":"Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a method to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that larger models with higher baseline performance (Gemini 1.5 Pro, GPT 4o, Claude 3.5) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, smaller models with lower baseline performance (Mistral 3, Gemma 2) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2--10\\% for Gemini, GPT, and Gemma. Key findings and the prompts used in our autorater analysis are available on our github.","authors":["Hailey Joren","Jianyi Zhang","Chun-Sung Ferng","Da-Cheng Juan","Ankur Taly","Cyrus Rashtchian"],"url":"https://arxiv.org/abs/2411.06037"}
{"created":"2025-04-24","title":"Advancements in Constitutive Model Calibration: Leveraging the Power of Full-Field DIC Measurements and In-Situ Load Path Selection for Reliable Parameter Inference","abstract":"Accurate material characterization and model calibration are essential for computationally-supported engineering decisions. Current characterization and calibration methods (1) use simplified test specimen geometries and global data, (2) cannot guarantee that sufficient characterization data is collected for a specific model of interest, (3) use deterministic methods that provide best-fit parameter values with no uncertainty quantification, and (4) are sequential, inflexible, and time-consuming. This work brings together several recent advancements into an improved workflow called Interlaced Characterization and Calibration that advances the state-of-the-art in constitutive model calibration. The ICC paradigm (1) efficiently uses full-field data to calibrate a high-fidelity material model, (2) aligns the data needed with the data collected with an optimal experimental design protocol, (3) quantifies parameter uncertainty through Bayesian inference, and (4) incorporates these advances into a quasi real-time feedback loop. The ICC framework is demonstrated on the calibration of a material model using simulated full-field data for an aluminum cruciform specimen being deformed bi-axially. The cruciform is actively driven through the myopically optimal load path using Bayesian optimal experimental design, which selects load steps that yield the maximum expected information gain. To aid in numerical stability and preserve computational resources, the full-field data is dimensionally reduced via principal component analysis, and fast surrogate models which approximate the input-output relationships of the expensive finite element model are used. The tools demonstrated here show that high-fidelity constitutive models can be efficiently and reliably calibrated with quantified uncertainty, thus supporting credible decision-making and potentially increasing the agility of solid mechanics modeling.","authors":["Denielle Ricciardi","D. Tom Seidl","Brian Lester","Amanda Jones","Elizabeth Jones"],"url":"https://arxiv.org/abs/2411.07310"}
{"created":"2025-04-24","title":"Constrained composite Bayesian optimization for rational synthesis of polymeric particles","abstract":"Polymeric nano- and micro-scale particles have critical roles in tackling critical healthcare and energy challenges with their miniature characteristics. However, tailoring their synthesis process to meet specific design targets has traditionally depended on domain expertise and costly trial-and-errors. Recently, modeling strategies, particularly Bayesian optimization (BO), have been proposed to aid materials discovery for maximized/minimized properties. Coming from practical demands, this study for the first time integrates constrained and composite Bayesian optimization (CCBO) to perform efficient target value optimization under black-box feasibility constraints and limited data for laboratory experimentation. Using a synthetic problem that simulates electrospraying, a model nanomanufacturing process, CCBO strategically avoided infeasible conditions and efficiently optimized particle production towards predefined size targets, surpassing standard BO pipelines and providing decisions comparable to human experts. Further laboratory experiments validated CCBO capability to guide the rational synthesis of poly(lactic-co-glycolic acid) (PLGA) particles with diameters of 300 nm and 3.0 $\\mu$m via electrospraying. With minimal initial data and unknown experiment constraints, CCBO reached the design targets within 4 iterations. Overall, the CCBO approach presents a versatile and holistic optimization paradigm for next-generation target-driven particle synthesis empowered by artificial intelligence (AI).","authors":["Fanjin Wang","Maryam Parhizkar","Anthony Harker","Mohan Edirisinghe"],"url":"https://arxiv.org/abs/2411.10471"}
{"created":"2025-04-24","title":"Better Together? The Role of Explanations in Supporting Novices in Individual and Collective Deliberations about AI","abstract":"Deploying AI systems in public institutions can have far-reaching consequences for many people, making it a matter of public interest. Providing opportunities for stakeholders to come together, understand these systems, and debate their merits and harms is thus essential. Explainable AI often focuses on individuals, but deliberation benefits from group settings, which are underexplored. To address this gap, we present findings from an interview study with 8 focus groups and 12 individuals. Our findings provide insight into how explanations support AI novices in deliberating alone and in groups. Participants used modular explanations with four information categories to solve tasks and decide about an AI system's deployment. We found that the explanations supported groups in creating shared understanding and in finding arguments for and against the system's deployment. In comparison, individual participants engaged with explanations in more depth and performed better in the study tasks, but missed an exchange with others. Based on our findings, we provide suggestions on how explanations should be designed to work in group settings and describe their potential use in real-world contexts. With this, our contributions inform XAI research that aims to enable AI novices to understand and deliberate AI systems in the public sector.","authors":["Timoth\\'ee Schmude","Laura Koesten","Torsten M\\\"oller","Sebastian Tschiatschek"],"url":"https://arxiv.org/abs/2411.11449"}
{"created":"2025-04-24","title":"Distributed Maximum Flow in Planar Graphs","abstract":"The dual of a planar graph $G$ is a planar graph $G^*$ that has a vertex for each face of $G$ and an edge for each pair of adjacent faces of $G$. The profound relationship between a planar graph and its dual has been the algorithmic basis for solving numerous (centralized) classical problems on planar graphs. In the distributed setting however, the only use of planar duality is for finding a recursive decomposition of $G$ [DISC 2017, STOC 2019].","authors":["Yaseen Abd-Elhaleem (University of Haifa)","Michal Dory (University of Haifa)","Merav Parter (Weizmann Institute of Science)","Oren Weimann (University of Haifa)"],"url":"https://arxiv.org/abs/2411.11718"}
{"created":"2025-04-24","title":"SNN-Based Online Learning of Concepts and Action Laws in an Open World","abstract":"We present the architecture of a fully autonomous, bio-inspired cognitive agent built around a spiking neural network (SNN) implementing the agent's semantic memory. This agent explores its universe and learns concepts of objects/situations and of its own actions in a one-shot manner. While object/situation concepts are unary, action concepts are triples made up of an initial situation, a motor activity, and an outcome. They embody the agent's knowledge of its universe's action laws. Both kinds of concepts have different degrees of generality. To make decisions the agent queries its semantic memory for the expected outcomes of envisaged actions and chooses the action to take on the basis of these predictions. Our experiments show that the agent handles new situations by appealing to previously learned general concepts and rapidly modifies its concepts to adapt to environment changes.","authors":["Christel Grimaud (IRIT-LILaC)","Dominique Longin (IRIT-LILaC)","Andreas Herzig (IRIT-LILaC)"],"url":"https://arxiv.org/abs/2411.12308"}
{"created":"2025-04-24","title":"GOT4Rec: Graph of Thoughts for Sequential Recommendation","abstract":"With their vast open-world knowledge and reasoning abilities, large language models (LLMs) have become a promising tool for sequential recommendation. Researchers have explored various methods to harness these capabilities, but most existing approaches rely on simple input-output prompting, failing to effectively bridge the gap between LLMs' general knowledge and the specific needs of recommendation tasks. While reasoning strategies like chain-of-thought (CoT) have been introduced to enhance performance, they often produce inaccurate recommendations due to underutilized user preference information and insufficient reasoning depth. To address these challenges, we propose GOT4Rec, a novel sequential recommendation method leveraging the graph of thoughts (GoT) reasoning strategy. Our method focuses on three key types of information in user histories: short-term interests, long-term interests and collaborative information from other users. It enables LLMs to reason independently and generate recommendations, subsequently aggregating results to derive final items. This method allows LLMs, with enhanced reasoning capabilities, to better utilize the user sequence information, producing more accurate recommendations and comprehensive explanations. Extensive experiments on real-world datasets demonstrate the effectiveness of GOT4Rec, outperforming existing state-of-the-art baselines with an average improvement of 37.11%. Our code is available at https://anonymous.4open.science/r/GOT4Rec.","authors":["Zewen Long","Liang Wang","Shu Wu","Qiang Liu","Liang Wang"],"url":"https://arxiv.org/abs/2411.14922"}
{"created":"2025-04-24","title":"OSDFace: One-Step Diffusion Model for Face Restoration","abstract":"Diffusion models have demonstrated impressive performance in face restoration. Yet, their multi-step inference process remains computationally intensive, limiting their applicability in real-world scenarios. Moreover, existing methods often struggle to generate face images that are harmonious, realistic, and consistent with the subject's identity. In this work, we propose OSDFace, a novel one-step diffusion model for face restoration. Specifically, we propose a visual representation embedder (VRE) to better capture prior information and understand the input face. In VRE, low-quality faces are processed by a visual tokenizer and subsequently embedded with a vector-quantized dictionary to generate visual prompts. Additionally, we incorporate a facial identity loss derived from face recognition to further ensure identity consistency. We further employ a generative adversarial network (GAN) as a guidance model to encourage distribution alignment between the restored face and the ground truth. Experimental results demonstrate that OSDFace surpasses current state-of-the-art (SOTA) methods in both visual quality and quantitative metrics, generating high-fidelity, natural face images with high identity consistency. The code and model will be released at https://github.com/jkwang28/OSDFace.","authors":["Jingkai Wang","Jue Gong","Lin Zhang","Zheng Chen","Xing Liu","Hong Gu","Yutong Liu","Yulun Zhang","Xiaokang Yang"],"url":"https://arxiv.org/abs/2411.17163"}
{"created":"2025-04-24","title":"Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning","abstract":"Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward~(RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence.","authors":["Di Zhang","Junxian Li","Jingdi Lei","Xunzhi Wang","Yujie Liu","Zonglin Yang","Jiatong Li","Weida Wang","Suorong Yang","Jianbo Wu","Peng Ye","Wanli Ouyang","Dongzhan Zhou"],"url":"https://arxiv.org/abs/2411.18203"}
{"created":"2025-04-24","title":"Enhancing Sentiment Analysis in Bengali Texts: A Hybrid Approach Using Lexicon-Based Algorithm and Pretrained Language Model Bangla-BERT","abstract":"Sentiment analysis (SA) is a process of identifying the emotional tone or polarity within a given text and aims to uncover the user's complex emotions and inner feelings. While sentiment analysis has been extensively studied for languages like English, research in Bengali, remains limited, particularly for fine-grained sentiment categorization. This work aims to connect this gap by developing a novel approach that integrates rule-based algorithms with pre-trained language models. We developed a dataset from scratch, comprising over 15,000 manually labeled reviews. Next, we constructed a Lexicon Data Dictionary, assigning polarity scores to the reviews. We developed a novel rule based algorithm Bangla Sentiment Polarity Score (BSPS), an approach capable of generating sentiment scores and classifying reviews into nine distinct sentiment categories. To assess the performance of this method, we evaluated the classified sentiments using BanglaBERT, a pre-trained transformer-based language model. We also performed sentiment classification directly with BanglaBERT on the original data and evaluated this model's results. Our analysis revealed that the BSPS + BanglaBERT hybrid approach outperformed the standalone BanglaBERT model, achieving higher accuracy, precision, and nuanced classification across the nine sentiment categories. The results of our study emphasize the value and effectiveness of combining rule-based and pre-trained language model approaches for enhanced sentiment analysis in Bengali and suggest pathways for future research and application in languages with similar linguistic complexities.","authors":["Hemal Mahmud","Hasan Mahmud","Mohammad Rifat Ahmmad Rashid"],"url":"https://arxiv.org/abs/2411.19584"}
{"created":"2025-04-24","title":"GFreeDet: Exploiting Gaussian Splatting and Foundation Models for Model-free Unseen Object Detection in the BOP Challenge 2024","abstract":"We present GFreeDet, an unseen object detection approach that leverages Gaussian splatting and vision Foundation models under model-free setting. Unlike existing methods that rely on predefined CAD templates, GFreeDet reconstructs objects directly from reference videos using Gaussian splatting, enabling robust detection of novel objects without prior 3D models. Evaluated on the BOP-H3 benchmark, GFreeDet achieves comparable performance to CAD-based methods, demonstrating the viability of model-free detection for mixed reality (MR) applications. Notably, GFreeDet won the best overall method and the best fast method awards in the model-free 2D detection track at BOP Challenge 2024.","authors":["Xingyu Liu","Gu Wang","Chengxi Li","Yingyue Li","Chenyangguang Zhang","Ziqin Huang","Xiangyang Ji"],"url":"https://arxiv.org/abs/2412.01552"}
{"created":"2025-04-24","title":"Fairness-Aware Dense Subgraph Discovery","abstract":"Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations - the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions.","authors":["Emmanouil Kariotakis","Nicholas D. Sidiropoulos","Aritra Konar"],"url":"https://arxiv.org/abs/2412.02604"}
{"created":"2025-04-24","title":"Dynamic EventNeRF: Reconstructing General Dynamic Scenes from Multi-view RGB and Event Streams","abstract":"Volumetric reconstruction of dynamic scenes is an important problem in computer vision. It is especially challenging in poor lighting and with fast motion. This is partly due to limitations of RGB cameras: To capture frames under low lighting, the exposure time needs to be increased, which leads to more motion blur. In contrast, event cameras, which record changes in pixel brightness asynchronously, are much less dependent on lighting, making them more suitable for recording fast motion. We hence propose the first method to spatiotemporally reconstruct a scene from sparse multi-view event streams and sparse RGB frames. We train a sequence of cross-faded time-conditioned NeRF models, one per short recording segment. The individual segments are supervised with a set of event- and RGB-based losses and sparse-view regularisation. We assemble a real-world multi-view camera rig with six static event cameras around the object and record a benchmark multi-view event stream dataset of challenging motions. Our work outperforms RGB-based baselines, producing state-of-the-art results, and opens up the topic of multi-view event-based reconstruction as a new path for fast scene capture beyond RGB cameras. The code and the data will be released soon at https://4dqv.mpi-inf.mpg.de/DynEventNeRF/","authors":["Viktor Rudnev","Gereon Fox","Mohamed Elgharib","Christian Theobalt","Vladislav Golyanik"],"url":"https://arxiv.org/abs/2412.06770"}
{"created":"2025-04-24","title":"7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement","abstract":"Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO), an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation.","authors":["Pu Zhao","Xuan Shen","Zhenglun Kong","Yixin Shen","Sung-En Chang","Timothy Rupprecht","Lei Lu","Enfu Nan","Changdi Yang","Yumei He","Weiyan Shi","Xingchen Xu","Yu Huang","Wei Jiang","Wei Wang","Yue Chen","Yong He","Yanzhi Wang"],"url":"https://arxiv.org/abs/2412.06845"}
{"created":"2025-04-24","title":"{\\alpha}-RACER: Real-Time Algorithm for Game-Theoretic Motion Planning and Control in Autonomous Racing using Near-Potential Function","abstract":"Autonomous racing extends beyond the challenge of controlling a racecar at its physical limits. Professional racers employ strategic maneuvers to outwit other competing opponents to secure victory. While modern control algorithms can achieve human-level performance by computing offline racing lines for single-car scenarios, research on real-time algorithms for multi-car autonomous racing is limited. To bridge this gap, we develop game-theoretic modeling framework that incorporates the competitive aspect of autonomous racing like overtaking and blocking through a novel policy parametrization, while operating the car at its limit. Furthermore, we propose an algorithmic approach to compute the (approximate) Nash equilibrium strategy, which represents the optimal approach in the presence of competing agents. Specifically, we introduce an algorithm inspired by recently introduced framework of dynamic near-potential function, enabling real-time computation of the Nash equilibrium. Our approach comprises two phases: offline and online. During the offline phase, we use simulated racing data to learn a near-potential function that approximates utility changes for agents. This function facilitates the online computation of approximate Nash equilibria by maximizing its value. We evaluate our method in a head-to-head 3-car racing scenario, demonstrating superior performance compared to several existing baselines.","authors":["Dvij Kalaria","Chinmay Maheshwari","Shankar Sastry"],"url":"https://arxiv.org/abs/2412.08855"}
{"created":"2025-04-24","title":"Should We Learn Contact-Rich Manipulation Policies from Sampling-Based Planners?","abstract":"The tremendous success of behavior cloning (BC) in robotic manipulation has been largely confined to tasks where demonstrations can be effectively collected through human teleoperation. However, demonstrations for contact-rich manipulation tasks that require complex coordination of multiple contacts are difficult to collect due to the limitations of current teleoperation interfaces. We investigate how to leverage model-based planning and optimization to generate training data for contact-rich dexterous manipulation tasks. Our analysis reveals that popular sampling-based planners like rapidly exploring random tree (RRT), while efficient for motion planning, produce demonstrations with unfavorably high entropy. This motivates modifications to our data generation pipeline that prioritizes demonstration consistency while maintaining solution diversity. Combined with a diffusion-based goal-conditioned BC approach, our method enables effective policy learning and zero-shot transfer to hardware for two challenging contact-rich manipulation tasks.","authors":["Huaijiang Zhu","Tong Zhao","Xinpei Ni","Jiuguang Wang","Kuan Fang","Ludovic Righetti","Tao Pang"],"url":"https://arxiv.org/abs/2412.09743"}
{"created":"2025-04-24","title":"Deep Gaussian Process Priors for Bayesian Image Reconstruction","abstract":"In image reconstruction, an accurate quantification of uncertainty is of great importance for informed decision making. Here, the Bayesian approach to inverse problems can be used: the image is represented through a random function that incorporates prior information which is then updated through Bayes' formula. However, finding a prior is difficult, as images often exhibit non-stationary effects and multi-scale behaviour. Thus, usual Gaussian process priors are not suitable. Deep Gaussian processes, on the other hand, encode non-stationary behaviour in a natural way through their hierarchical structure. To apply Bayes' formula, one commonly employs a Markov chain Monte Carlo (MCMC) method. In the case of deep Gaussian processes, sampling is especially challenging in high dimensions: the associated covariance matrices are large, dense, and changing from sample to sample. A popular strategy towards decreasing computational complexity is to view Gaussian processes as the solutions to a fractional stochastic partial differential equation (SPDE). In this work, we investigate efficient computational strategies to solve the fractional SPDEs occurring in deep Gaussian process sampling, as well as MCMC algorithms to sample from the posterior. Namely, we combine rational approximation and a determinant-free sampling approach to achieve sampling via the fractional SPDE. We test our techniques in standard Bayesian image reconstruction problems: upsampling, edge detection, and computed tomography. In these examples, we show that choosing a non-stationary prior such as the deep GP over a stationary GP can improve the reconstruction. Moreover, our approach enables us to compare results for a range of fractional and non-fractional regularity parameter values.","authors":["Jonas Latz","Aretha L. Teckentrup","Simon Urbainczyk"],"url":"https://arxiv.org/abs/2412.10248"}
{"created":"2025-04-24","title":"Physics-based battery model parametrisation from impedance data","abstract":"Non-invasive parametrisation of physics-based battery models can be performed by fitting the model to electrochemical impedance spectroscopy (EIS) data containing features related to the different physical processes. However, this requires an impedance model to be derived, which may be complex to obtain analytically. We have developed the open-source software PyBaMM-EIS that provides a fast method to compute the impedance of any PyBaMM model at any operating point using automatic differentiation. Using PyBaMM-EIS, we investigate the impedance of the single particle model, single particle model with electrolyte (SPMe), and Doyle-Fuller-Newman model, and identify the SPMe as a parsimonious option that shows the typical features of measured lithium-ion cell impedance data. We provide a grouped parameter SPMe and analyse the features in the impedance related to each parameter. Using the open-source software PyBOP, we estimate 18 grouped parameters both from simulated impedance data and from measured impedance data from a LG M50LT lithium-ion battery. The parameters that directly affect the response of the SPMe can be accurately determined and assigned to the correct electrode. Crucially, parameter fitting must be done simultaneously to data across a wide range of states-of-charge. Overall, this work presents a practical way to find the parameters of physics-based models.","authors":["No\\\"el Hallemans","Nicola E. Courtier","Colin P. Please","Brady Planden","Rishit Dhoot","Robert Timms","S. Jon chapman","David Howey","Stephen R. Duncan"],"url":"https://arxiv.org/abs/2412.10896"}
{"created":"2025-04-24","title":"Adapter-Enhanced Semantic Prompting for Continual Learning","abstract":"Continual learning (CL) enables models to adapt to evolving data streams. A major challenge of CL is catastrophic forgetting, where new knowledge will overwrite previously acquired knowledge. Traditional methods usually retain the past data for replay or add additional branches in the model to learn new knowledge, which has high memory requirements. In this paper, we propose a novel lightweight CL framework, Adapter-Enhanced Semantic Prompting (AESP), which integrates prompt tuning and adapter techniques. Specifically, we design semantic-guided prompts to enhance the generalization ability of visual features and utilize adapters to efficiently fuse the semantic information, aiming to learn more adaptive features for the continual learning task. Furthermore, to choose the right task prompt for feature adaptation, we have developed a novel matching mechanism for prompt selection. Extensive experiments on three CL datasets demonstrate that our approach achieves favorable performance across multiple metrics, showing its potential for advancing CL.","authors":["Baocai Yin","Ji Zhao","Huajie Jiang","Ningning Hou","Yongli Hu","Amin Beheshti","Ming-Hsuan Yang","Yuankai Qi"],"url":"https://arxiv.org/abs/2412.11074"}
{"created":"2025-04-24","title":"A Mapper Algorithm with implicit intervals and its optimization","abstract":"The Mapper algorithm is an essential tool for visualizing complex, high dimensional data in topology data analysis (TDA) and has been widely used in biomedical research. It outputs a combinatorial graph whose structure implies the shape of the data. However,the need for manual parameter tuning and fixed intervals, along with fixed overlapping ratios may impede the performance of the standard Mapper algorithm. Variants of the standard Mapper algorithms have been developed to address these limitations, yet most of them still require manual tuning of parameters. Additionally, many of these variants, including the standard version found in the literature, were built within a deterministic framework and overlooked the uncertainty inherent in the data. To relax these limitations, in this work, we introduce a novel framework that implicitly represents intervals through a hidden assignment matrix, enabling automatic parameter optimization via stochastic gradient descent. In this work, we develop a soft Mapper framework based on a Gaussian mixture model(GMM) for flexible and implicit interval construction. We further illustrate the robustness of the soft Mapper algorithm by introducing the Mapper graph mode as a point estimation for the output graph. Moreover, a stochastic gradient descent algorithm with a specific topological loss function is proposed for optimizing parameters in the model. Both simulation and application studies demonstrate its effectiveness in capturing the underlying topological structures. In addition, the application to an RNA expression dataset obtained from the Mount Sinai/JJ Peters VA Medical Center Brain Bank (MSBB) successfully identifies a distinct subgroup of Alzheimer's Disease.","authors":["Yuyang Tao","Shufei Ge"],"url":"https://arxiv.org/abs/2412.11631"}
{"created":"2025-04-24","title":"Discover physical concepts and equations with machine learning","abstract":"Machine learning can uncover physical concepts or physical equations when prior knowledge from the other is available. However, these two aspects are often intertwined and cannot be discovered independently. We extend SciNet, which is a neural network architecture that simulates the human physical reasoning process for physics discovery, by proposing a model that combines Variational Autoencoders (VAE) with Neural Ordinary Differential Equations (Neural ODEs). This allows us to simultaneously discover physical concepts and governing equations from simulated experimental data across various physical systems. We apply the model to several examples inspired by the history of physics, including Copernicus' heliocentrism, Newton's law of gravity, Schr\\\"odinger's wave mechanics, and Pauli's spin-magnetic formulation. The results demonstrate that the correct physical theories can emerge in the neural network.","authors":["Bao-Bing Li","Yi Gu","Shao-Feng Wu"],"url":"https://arxiv.org/abs/2412.12161"}
{"created":"2025-04-24","title":"MedMax: Mixed-Modal Instruction Tuning for Training Biomedical Assistants","abstract":"Recent advancements in mixed-modal generative have opened new avenues for developing unified biomedical assistants capable of analyzing biomedical images, answering complex questions about them, and generating multimodal patient reports. However, existing datasets face challenges such as small sizes, limited coverage of biomedical tasks and domains, and a reliance on narrow sources. To address these gaps, we present MedMax, a large-scale multimodal biomedical instruction-tuning dataset for mixed-modal foundation models. With 1.47 million instances, MedMax encompasses a diverse range of tasks, including interleaved image-text generation, biomedical image captioning and generation, visual chat, and report understanding. These tasks span knowledge across diverse biomedical domains, including radiology and histopathology, grounded in medical papers and YouTube videos. Subsequently, we fine-tune a mixed-modal foundation model on the MedMax dataset, achieving significant performance improvements: a 26% gain over the Chameleon model and an 18.3% improvement over GPT-4o across 12 downstream biomedical visual question-answering tasks. Finally, we introduce a unified evaluation suite for biomedical tasks to guide the development of mixed-modal biomedical AI assistants. The data, model, and code is available at https://mint-medmax.github.io/.","authors":["Hritik Bansal","Daniel Israel","Siyan Zhao","Shufan Li","Tung Nguyen","Aditya Grover"],"url":"https://arxiv.org/abs/2412.12661"}
{"created":"2025-04-24","title":"Geodesic Flow Kernels for Semi-Supervised Learning on Mixed-Variable Tabular Dataset","abstract":"Tabular data poses unique challenges due to its heterogeneous nature, combining both continuous and categorical variables. Existing approaches often struggle to effectively capture the underlying structure and relationships within such data. We propose GFTab (Geodesic Flow Kernels for Semi- Supervised Learning on Mixed-Variable Tabular Dataset), a semi-supervised framework specifically designed for tabular datasets. GFTab incorporates three key innovations: 1) Variable-specific corruption methods tailored to the distinct properties of continuous and categorical variables, 2) A Geodesic flow kernel based similarity measure to capture geometric changes between corrupted inputs, and 3) Tree-based embedding to leverage hierarchical relationships from available labeled data. To rigorously evaluate GFTab, we curate a comprehensive set of 21 tabular datasets spanning various domains, sizes, and variable compositions. Our experimental results show that GFTab outperforms existing ML/DL models across many of these datasets, particularly in settings with limited labeled data.","authors":["Yoontae Hwang","Yongjae Lee"],"url":"https://arxiv.org/abs/2412.12864"}
{"created":"2025-04-24","title":"Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition","abstract":"Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.","authors":["Xuemei Tang","Xufeng Duan","Zhenguang G. Cai"],"url":"https://arxiv.org/abs/2412.13612"}
{"created":"2025-04-24","title":"Consistency Matters: Defining Demonstration Data Quality Metrics in Robot Learning from Demonstration","abstract":"Learning from Demonstration (LfD) empowers robots to acquire new skills through human demonstrations, making it feasible for everyday users to teach robots. However, the success of learning and generalization heavily depends on the quality of these demonstrations. Consistency is often used to indicate quality in LfD, yet the factors that define this consistency remain underexplored. In this paper, we evaluate a comprehensive set of motion data characteristics to determine which consistency measures best predict learning performance. By ensuring demonstration consistency prior to training, we enhance models' predictive accuracy and generalization to novel scenarios. We validate our approach with two user studies involving participants with diverse levels of robotics expertise. In the first study (N = 24), users taught a PR2 robot to perform a button-pressing task in a constrained environment, while in the second study (N = 30), participants trained a UR5 robot on a pick-and-place task. Results show that demonstration consistency significantly impacts success rates in both learning and generalization, with 70% and 89% of task success rates in the two studies predicted using our consistency metrics. Moreover, our metrics estimate generalized performance success rates with 76% and 91% accuracy. These findings suggest that our proposed measures provide an intuitive, practical way to assess demonstration data quality before training, without requiring expert data or algorithm-specific modifications. Our approach offers a systematic way to evaluate demonstration quality, addressing a critical gap in LfD by formalizing consistency metrics that enhance the reliability of robot learning from human demonstrations.","authors":["Maram Sakr","H. F. Machiel Van der Loos","Dana Kulic","Elizabeth Croft"],"url":"https://arxiv.org/abs/2412.14309"}
{"created":"2025-04-24","title":"Vulnerable Connectivity Caused by Local Communities in Spatial Networks","abstract":"Local communities by concentration of nodes connected with short links are widely observed in spatial networks. However, how such structure affects robustness of connectivity against malicious attacks remains unclear. This study investigates the impact of local communities on the robustness by modeling planar infrastructure reveals that the robustness is weakened by strong local communities in spatial networks. These results highlight the potential of long-distance links in mitigating the negative effects of local community on the robustness.","authors":["Yingzhou Mou","Yukio Hayashi"],"url":"https://arxiv.org/abs/2412.14513"}
{"created":"2025-04-24","title":"Parallel Contraction Hierarchies Can Be Efficient and Scalable","abstract":"Contraction Hierarchies (CH) (Geisberger et al., 2008) is one of the most widely used algorithms for shortest-path queries on road networks. Compared to Dijkstra's algorithm, CH enables orders of magnitude faster query performance through a preprocessing phase, which iteratively categorizes vertices into hierarchies and adds shortcuts. However, constructing a CH is an expensive task. Existing solutions, including parallel ones, may suffer from long construction time. Especially, in our experiments, we observe that existing parallel solutions demonstrate unsatisfactory scalability, and have performance close to sequential algorithms.","authors":["Zijin Wan","Xiaojun Dong","Letong Wang","Enzuo Zhu","Yan Gu","Yihan Sun"],"url":"https://arxiv.org/abs/2412.18008"}
{"created":"2025-04-24","title":"Make Shuffling Great Again: A Side-Channel Resistant Fisher-Yates Algorithm for Protecting Neural Networks","abstract":"Neural network models implemented in embedded devices have been shown to be susceptible to side-channel attacks (SCAs), allowing recovery of proprietary model parameters, such as weights and biases. There are already available countermeasure methods currently used for protecting cryptographic implementations that can be tailored to protect embedded neural network models. Shuffling, a hiding-based countermeasure that randomly shuffles the order of computations, was shown to be vulnerable to SCA when the Fisher-Yates algorithm is used. In this paper, we propose a design of an SCA-secure version of the Fisher-Yates algorithm. By integrating the masking technique for modular reduction and Blakely's method for modular multiplication, we effectively remove the vulnerability in the division operation that led to side-channel leakage in the original version of the algorithm. We experimentally evaluate that the countermeasure is effective against SCA by implementing a correlation power analysis attack on an embedded neural network model implemented on ARM Cortex-M4. Compared to the original proposal, the memory overhead is $2\\times$ the biggest layer of the network, while the time overhead varies from $4\\%$ to $0.49\\%$ for a layer with $100$ and $1000$ neurons, respectively.","authors":["Leonard Pu\\v{s}k\\'a\\v{c}","Marek Benovi\\v{c}","Jakub Breier","Xiaolu Hou"],"url":"https://arxiv.org/abs/2501.00798"}
{"created":"2025-04-24","title":"Truthful mechanisms for linear bandit games with private contexts","abstract":"The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes. However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system. For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments. We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents. We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\\epsilon$-greedy algorithm incur sublinear but high regret. We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\\ln T)$ frequentist regret. Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families.","authors":["Yiting Hu","Lingjie Duan"],"url":"https://arxiv.org/abs/2501.03865"}
{"created":"2025-04-24","title":"Enhancing Low-Cost Video Editing with Lightweight Adaptors and Temporal-Aware Inversion","abstract":"Recent advancements in text-to-image (T2I) generation using diffusion models have enabled cost-effective video-editing applications by leveraging pre-trained models, eliminating the need for resource-intensive training. However, the frame-independence of T2I generation often results in poor temporal consistency. Existing methods address this issue through temporal layer fine-tuning or inference-based temporal propagation, but these approaches suffer from high training costs or limited temporal coherence. To address these challenges, we propose a General and Efficient Adapter (GE-Adapter) that integrates temporal-spatial and semantic consistency with Baliteral DDIM inversion. This framework introduces three key components: (1) Frame-based Temporal Consistency Blocks (FTC Blocks) to capture frame-specific features and enforce smooth inter-frame transitions via temporally-aware loss functions; (2) Channel-dependent Spatial Consistency Blocks (SCD Blocks) employing bilateral filters to enhance spatial coherence by reducing noise and artifacts; and (3) Token-based Semantic Consistency Module (TSC Module) to maintain semantic alignment using shared prompt tokens and frame-specific tokens. Our method significantly improves perceptual quality, text-image alignment, and temporal coherence, as demonstrated on the MSR-VTT dataset. Additionally, it achieves enhanced fidelity and frame-to-frame coherence, offering a practical solution for T2V editing.","authors":["Yangfan He","Sida Li","Jianhui Wang","Kun Li","Xinyuan Song","Xinhang Yuan","Keqin Li","Kuan Lu","Menghao Huo","Jiaqi Chen","Miao Zhang","Xueqian Wang"],"url":"https://arxiv.org/abs/2501.04606"}
{"created":"2025-04-24","title":"Dataset-Agnostic Recommender Systems","abstract":"Recommender systems have become a cornerstone of personalized user experiences, yet their development typically involves significant manual intervention, including dataset-specific feature engineering, hyperparameter tuning, and configuration. To this end, we introduce a novel paradigm: Dataset-Agnostic Recommender Systems (DAReS) that aims to enable a single codebase to autonomously adapt to various datasets without the need for fine-tuning, for a given recommender system task. Central to this approach is the Dataset Description Language (DsDL), a structured format that provides metadata about the dataset's features and labels, and allow the system to understand dataset's characteristics, allowing it to autonomously manage processes like feature selection, missing values imputation, noise removal, and hyperparameter optimization. By reducing the need for domain-specific expertise and manual adjustments, DAReS offers a more efficient and scalable solution for building recommender systems across diverse application domains. It addresses critical challenges in the field, such as reusability, reproducibility, and accessibility for non-expert users or entry-level researchers.","authors":["Tri Kurniawan Wijaya","Edoardo D'Amico","Xinyang Shao"],"url":"https://arxiv.org/abs/2501.07294"}
{"created":"2025-04-24","title":"DEFOM-Stereo: Depth Foundation Model Based Stereo Matching","abstract":"Stereo matching is a key technique for metric depth estimation in computer vision and robotics. Real-world challenges like occlusion and non-texture hinder accurate disparity estimation from binocular matching cues. Recently, monocular relative depth estimation has shown remarkable generalization using vision foundation models. Thus, to facilitate robust stereo matching with monocular depth cues, we incorporate a robust monocular relative depth model into the recurrent stereo-matching framework, building a new framework for depth foundation model-based stereo-matching, DEFOM-Stereo. In the feature extraction stage, we construct the combined context and matching feature encoder by integrating features from conventional CNNs and DEFOM. In the update stage, we use the depth predicted by DEFOM to initialize the recurrent disparity and introduce a scale update module to refine the disparity at the correct scale. DEFOM-Stereo is verified to have much stronger zero-shot generalization compared with SOTA methods. Moreover, DEFOM-Stereo achieves top performance on the KITTI 2012, KITTI 2015, Middlebury, and ETH3D benchmarks, ranking $1^{st}$ on many metrics. In the joint evaluation under the robust vision challenge, our model simultaneously outperforms previous models on the individual benchmarks, further demonstrating its outstanding capabilities.","authors":["Hualie Jiang","Zhiqiang Lou","Laiyan Ding","Rui Xu","Minglang Tan","Wenjie Jiang","Rui Huang"],"url":"https://arxiv.org/abs/2501.09466"}
{"created":"2025-04-24","title":"A High-Resolution Analysis of Receiver Quantization in Communication","abstract":"We investigate performance limits and design of communication in the presence of uniform output quantization with moderate to high resolution. Under independent and identically distributed (i.i.d.) complex Gaussian codebook and nearest neighbor decoding rule, an achievable rate is derived in an analytical form by the generalized mutual information (GMI). The gain control before quantization is shown to be increasingly important as the resolution decreases, due to the fact that the loading factor (normalized one-sided quantization range) has increasing impact on performance. The impact of imperfect gain control in the high-resolution regime is characterized by two asymptotic results: 1) the rate loss due to overload distortion decays exponentially as the loading factor increases, and 2) the rate loss due to granular distortion decays quadratically as the step size vanishes. For a $2K$-level uniform quantizer, we prove that the optimal loading factor that maximizes the achievable rate scales like $2\\sqrt{\\ln (2K)}$ as the resolution increases. An asymptotically tight estimate of the optimal loading factor is further given, which is also highly accurate for finite resolutions.","authors":["Jing Zhou","Shuqin Pang","Wenyi Zhang"],"url":"https://arxiv.org/abs/2501.09961"}
{"created":"2025-04-24","title":"Robotic World Model: A Neural Network Simulator for Robust Policy Optimization in Robotics","abstract":"Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.","authors":["Chenhao Li","Andreas Krause","Marco Hutter"],"url":"https://arxiv.org/abs/2501.10100"}
{"created":"2025-04-24","title":"The NIC should be part of the OS","abstract":"The network interface adapter (NIC) is a critical component of a cloud server occupying a unique position. Not only is network performance vital to efficient operation of the machine, but unlike compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency.","authors":["Pengcheng Xu","Timothy Roscoe"],"url":"https://arxiv.org/abs/2501.10138"}
{"created":"2025-04-24","title":"UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion","abstract":"Capturing high dynamic range (HDR) scenes is one of the most important issues in camera design. Majority of cameras use exposure fusion, which fuses images captured by different exposure levels, to increase dynamic range. However, this approach can only handle images with limited exposure difference, normally 3-4 stops. When applying to very high dynamic range scenes where a large exposure difference is required, this approach often fails due to incorrect alignment or inconsistent lighting between inputs, or tone mapping artifacts. In this work, we propose \\model, the first exposure fusion technique that can merge inputs with 9 stops differences. The key idea is that we model exposure fusion as a guided inpainting problem, where the under-exposed image is used as a guidance to fill the missing information of over-exposed highlights in the over-exposed region. Using an under-exposed image as a soft guidance, instead of a hard constraint, our model is robust to potential alignment issue or lighting variations. Moreover, by utilizing the image prior of the generative model, our model also generates natural tone mapping, even for very high-dynamic range scenes. Our approach outperforms HDR-Transformer on latest HDR benchmarks. Moreover, to test its performance in ultra high dynamic range scenes, we capture a new real-world exposure fusion benchmark, UltraFusion dataset, with exposure differences up to 9 stops, and experiments show that UltraFusion can generate beautiful and high-quality fusion results under various scenarios. Code and data will be available at https://openimaginglab.github.io/UltraFusion.","authors":["Zixuan Chen","Yujin Wang","Xin Cai","Zhiyuan You","Zheming Lu","Fan Zhang","Shi Guo","Tianfan Xue"],"url":"https://arxiv.org/abs/2501.11515"}
{"created":"2025-04-24","title":"A Contrastive Framework with User, Item and Review Alignment for Recommendation","abstract":"Learning effective latent representations for users and items is the cornerstone of recommender systems. Traditional approaches rely on user-item interaction data to map users and items into a shared latent space, but the sparsity of interactions often poses challenges. While leveraging user reviews could mitigate this sparsity, existing review-aware recommendation models often exhibit two key limitations. First, they typically rely on reviews as additional features, but reviews are not universal, with many users and items lacking them. Second, such approaches do not integrate reviews into the user-item space, leading to potential divergence or inconsistency among user, item, and review representations. To overcome these limitations, our work introduces a Review-centric Contrastive Alignment Framework for Recommendation (ReCAFR), which incorporates reviews into the core learning process, ensuring alignment among user, item, and review representations within a unified space. Specifically, we leverage two self-supervised contrastive strategies that not only exploit review-based augmentation to alleviate sparsity, but also align the tripartite representations to enhance robustness. Empirical studies on public benchmark datasets demonstrate the effectiveness and robustness of ReCAFR.","authors":["Hoang V. Dong","Yuan Fang","Hady W. Lauw"],"url":"https://arxiv.org/abs/2501.11963"}
{"created":"2025-04-24","title":"Removal of Small Weight Stopping Sets for Asynchronous Unsourced Multiple Access","abstract":"In this paper, we analyze the formation of small stopping sets in joint factor graphs describing a frame-asynchronous two-user transmission. Furthermore, we propose an algorithm to completely avoid small stopping sets in the joint factor graph over the entire range of symbol delays. The error floor caused by these stopping sets is completely mitigated. Our key observation is that, while the order of bits in the codeword is irrelevant in a single-user environment, it turns out to be crucial in an asynchronous, unsourced two-user system. Subsequently, our algorithm finds a reordering of variable nodes which avoids the smallest stopping set in the joint graph. We show that further improvements can be achieved when girth optimization of the single-user graphs by progressive edge growth (PEG) is used in combination with our proposed algorithm. Starting with a randomized code construction with optimized degree distribution, our simulation results show that PEG followed by the proposed algorithm can improve the average per user probability of error in a noiseless channel by almost two orders of magnitude for a broad range of frame delays.","authors":["Frederik Ritter","Jonathan Mandelbaum","Alexander Fengler","Holger J\\\"akel","Laurent Schmalen"],"url":"https://arxiv.org/abs/2501.12186"}
{"created":"2025-04-24","title":"D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network","abstract":"The deployment of LoRa networks necessitates joint performance optimization, including packet delivery rate, energy efficiency, and throughput. Additionally, multiple LoRa parameters for packet transmission must be dynamically configured to tailor the performance metrics prioritization across varying channel environments. Because of the coupling relationship between LoRa parameters and metrics, existing works have opted to focus on certain parameters or specific metrics to circumvent the intricate coupling relationship, leading to limited adaptability. Therefore, we propose D-LoRa, a distributed parameter adaptation scheme, based on reinforcement learning towards network performance. We decompose the joint performance optimization problem into multiple independent Multi-Armed Bandit (MAB) problems with different reward functions. We have also built a comprehensive analytical model for the LoRa network that considers path loss, quasi-orthogonality of spreading factor, and packet collision. Experimental results show that our scheme can increase packet delivery rate by up to 28.8% and demonstrates superior adaptability across different performance metrics.","authors":["Ruiqi Wang","Tongyu Song","Jing Ren","Xiong Wang","Shizhong Xu","Sheng Wang"],"url":"https://arxiv.org/abs/2501.12589"}
{"created":"2025-04-24","title":"MixRec: Individual and Collective Mixing Empowers Data Augmentation for Recommender Systems","abstract":"The core of the general recommender systems lies in learning high-quality embedding representations of users and items to investigate their positional relations in the feature space. Unfortunately, data sparsity caused by difficult-to-access interaction data severely limits the effectiveness of recommender systems. Faced with such a dilemma, various types of self-supervised learning methods have been introduced into recommender systems in an attempt to alleviate the data sparsity through distribution modeling or data augmentation. However, most data augmentation relies on elaborate manual design, which is not only not universal, but the bloated and redundant augmentation process may significantly slow down model training progress. To tackle these limitations, we propose a novel Dual Mixing-based Recommendation Framework (MixRec) to empower data augmentation as we wish. Specifically, we propose individual mixing and collective mixing, respectively. The former aims to provide a new positive sample that is unique to the target (user or item) and to make the pair-wise recommendation loss benefit from it, while the latter aims to portray a new sample that contains group properties in a batch. The two mentioned mixing mechanisms allow for data augmentation with only one parameter that does not need to be set multiple times and can be done in linear time complexity. Besides, we propose the dual-mixing contrastive learning to maximize the utilization of these new-constructed samples to enhance the consistency between pairs of positive samples. Experimental results on four real-world datasets demonstrate the advantages of MixRec in terms of effectiveness, simplicity, efficiency, and scalability.","authors":["Yi Zhang","Yiwen Zhang"],"url":"https://arxiv.org/abs/2501.13579"}
{"created":"2025-04-24","title":"Strong Converse Exponent for Remote Lossy Source Coding","abstract":"Past works on remote lossy source coding studied the rate under average distortion and the error exponent of excess distortion probability. In this work, we look into how fast the excess distortion probability converges to 1 at small rates, also known as exponential strong converse. We characterize its exponent by establishing matched upper and lower bounds. From the exponent, we also recover two previous results on lossy source coding and biometric authentication.","authors":["Han Wu","Hamdi Joudeh"],"url":"https://arxiv.org/abs/2501.14620"}
{"created":"2025-04-24","title":"Enabling Low-Cost Secure Computing on Untrusted In-Memory Architectures","abstract":"Modern computing systems are limited in performance by the memory bandwidth available to processors, a problem known as the memory wall. Processing-in-Memory (PIM) promises to substantially improve this problem by moving processing closer to the data, improving effective data bandwidth, and leading to superior performance on memory-intensive workloads. However, integrating PIM modules within a secure computing system raises an interesting challenge: unencrypted data has to move off-chip to the PIM, exposing the data to attackers and breaking assumptions on Trusted Computing Bases (TCBs). To tackle this challenge, this paper leverages multi-party computation (MPC) techniques, specifically arithmetic secret sharing and Yao's garbled circuits, to outsource bandwidth-intensive computation securely to PIM. Additionally, we leverage precomputation optimization to prevent the CPU's portion of the MPC from becoming a bottleneck. We evaluate our approach using the UPMEM PIM system over various applications such as Deep Learning Recommendation Model inference and Logistic Regression. Our evaluations demonstrate up to a $14.66\\times$ speedup compared to a secure CPU configuration while maintaining data confidentiality and integrity when outsourcing linear and/or nonlinear computation.","authors":["Sahar Ghoflsaz Ghinani","Jingyao Zhang","Elaheh Sadredini"],"url":"https://arxiv.org/abs/2501.17292"}
{"created":"2025-04-24","title":"GNN-based Anchor Embedding for Exact Subgraph Matching","abstract":"Subgraph matching query is a classic problem in graph data management and has a variety of real-world applications, such as discovering structures in biological or chemical networks, finding communities in social network analysis, explaining neural networks, and so on. To further solve the subgraph matching problem, several recent advanced works attempt to utilize deep-learning-based techniques to handle the subgraph matching query. However, most of these works only obtain approximate results for subgraph matching without theoretical guarantees of accuracy. In this paper, we propose a novel and effective graph neural network (GNN)-based anchor embedding framework (GNN-AE), which allows exact subgraph matching. Unlike GNN-based approximate subgraph matching approaches that only produce inexact results, in this paper, we pioneer a series of concepts related to anchor (including anchor, anchor graph/path, etc.) in subgraph matching and carefully devise the anchor (graph) embedding technique based on GNN models. We transform the subgraph matching problem into a search problem in the embedding space via the anchor (graph & path) embedding techniques. With the proposed anchor matching mechanism, GNN-AE can guarantee subgraph matching has no false dismissals. We design an efficient matching growth algorithm, which can retrieve the locations of all exact matches in parallel. We also propose a cost-model-based DFS query plan to enhance the parallel matching growth algorithm. Through extensive experiments on 6 real-world and 3 synthetic datasets, we confirm the effectiveness and efficiency of our GNN-AE approach for exact subgraph matching.","authors":["Bin Yang","Zhaonian Zou","Jianxiong Ye"],"url":"https://arxiv.org/abs/2502.00031"}
{"created":"2025-04-24","title":"Evaluating Text Style Transfer Evaluation: Are There Any Reliable Metrics?","abstract":"Text style transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TST outputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. Using human evaluation is ideal but costly, as is common in other natural language processing (NLP) tasks, however, automatic metrics for TST have not received as much attention as metrics for, e.g., machine translation or summarization. In this paper, we examine both set of existing and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks, sentiment transfer and detoxification, in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with human judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles. Additionally, we investigate the potential of large language models (LLMs) as tools for TST evaluation. Our findings highlight newly applied advanced NLP metrics and LLM-based evaluations provide better insights than existing TST metrics. Our oracle ensemble approaches show even more potential.","authors":["Sourabrata Mukherjee","Atul Kr. Ojha","John P. McCrae","Ondrej Dusek"],"url":"https://arxiv.org/abs/2502.04718"}
{"created":"2025-04-24","title":"Energy-Efficient Autonomous Aerial Navigation with Dynamic Vision Sensors: A Physics-Guided Neuromorphic Approach","abstract":"Vision-based object tracking is a critical component for achieving autonomous aerial navigation, particularly for obstacle avoidance. Neuromorphic Dynamic Vision Sensors (DVS) or event cameras, inspired by biological vision, offer a promising alternative to conventional frame-based cameras. These cameras can detect changes in intensity asynchronously, even in challenging lighting conditions, with a high dynamic range and resistance to motion blur. Spiking neural networks (SNNs) are increasingly used to process these event-based signals efficiently and asynchronously. Meanwhile, physics-based artificial intelligence (AI) provides a means to incorporate system-level knowledge into neural networks via physical modeling. This enhances robustness, energy efficiency, and provides symbolic explainability. In this work, we present a neuromorphic navigation framework for autonomous drone navigation. The focus is on detecting and navigating through moving gates while avoiding collisions. We use event cameras for detecting moving objects through a shallow SNN architecture in an unsupervised manner. This is combined with a lightweight energy-aware physics-guided neural network (PgNN) trained with depth inputs to predict optimal flight times, generating near-minimum energy paths. The system is implemented in the Gazebo simulator and integrates a sensor-fused vision-to-planning neuro-symbolic framework built with the Robot Operating System (ROS) middleware. This work highlights the future potential of integrating event-based vision with physics-guided planning for energy-efficient autonomous navigation, particularly for low-latency decision-making.","authors":["Sourav Sanyal","Amogh Joshi","Manish Nagaraj","Rohan Kumar Manna","Kaushik Roy"],"url":"https://arxiv.org/abs/2502.05938"}
{"created":"2025-04-24","title":"Novel computational workflows for natural and biomedical image processing based on hypercomplex algebras","abstract":"Hypercomplex image processing extends conventional techniques in a unified paradigm encompassing algebraic and geometric principles. This work leverages quaternions and the two-dimensional orthogonal planes split framework (splitting of a quaternion - representing a pixel - into pairs of orthogonal 2D planes) for natural/biomedical image analysis through the following computational workflows and outcomes: natural/biomedical image re-colorization, natural image de-colorization, natural/biomedical image contrast enhancement, computational re-staining and stain separation in histological images, and performance gains in machine/deep learning pipelines for histological images. The workflows are analyzed separately for natural and biomedical images to showcase the effectiveness of the proposed approaches. The proposed workflows can regulate color appearance (e.g. with alternative renditions and grayscale conversion) and image contrast, be part of automated image processing pipelines (e.g. isolating stain components, boosting learning models), and assist in digital pathology applications (e.g. enhancing biomarker visibility, enabling colorblind-friendly renditions). Employing only basic arithmetic and matrix operations, this work offers a computationally accessible methodology - in the hypercomplex domain - that showcases versatility and consistency across image processing tasks and a range of computer vision and biomedical applications. The proposed non-data-driven methods achieve comparable or better results (particularly in cases involving well-known methods) to those reported in the literature, showcasing the potential of robust theoretical frameworks with practical effectiveness. Results, methods, and limitations are detailed alongside discussion of promising extensions, emphasizing the potential of feature-rich mathematical/computational frameworks for natural and biomedical images.","authors":["Nektarios A. Valous","Eckhard Hitzer","Drago\\c{s} Du\\c{s}e","Rodrigo Rojas Moraleda","Ferdinand Popp","Meggy Suarez-Carmona","Anna Berthel","Ismini Papageorgiou","Carlo Fremd","Alexander R\\\"olle","Christina C. Westhoff","B\\'en\\'edicte Lenoir","Niels Halama","Inka Z\\\"ornig","Dirk J\\\"ager"],"url":"https://arxiv.org/abs/2502.07758"}
{"created":"2025-04-24","title":"X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks","abstract":"3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D generation. Training to get a 3DGS scene often takes a lot of time and resources and even valuable inspiration. The increasing amount of 3DGS digital asset have brought great challenges to the copyright protection. However, it still lacks profound exploration targeted at 3DGS. In this paper, we propose a new framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages while keeping the original 3DGS scene almost unchanged. Generally, we have a X-SG$^2$S injector for adding multi-modal messages simultaneously and an extractor for extract them. Specifically, we first split the watermarks into message patches in a fixed manner and sort the 3DGS points. A self-adaption gate is used to pick out suitable location for watermarking. Then use a XD(multi-dimension)-injection heads to add multi-modal messages into sorted 3DGS points. A learnable gate can recognize the location with extra messages and XD-extraction heads can restore hidden messages from the location recommended by the learnable gate. Extensive experiments demonstrated that the proposed X-SG$^2$S can effectively conceal multi modal messages without changing pretrained 3DGS pipeline or the original form of 3DGS parameters. Meanwhile, with simple and efficient model structure and high practicality, X-SG$^2$S still shows good performance in hiding and extracting multi-modal inner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to 3D watermarking model for 3DGS and the first framework to add multi-modal watermarks simultaneous in one 3DGS which pave the wave for later researches.","authors":["Zihang Cheng","Huiping Zhuang","Chun Li","Xin Meng","Ming Li","Fei Richard Yu","Liqiang Nie"],"url":"https://arxiv.org/abs/2502.10475"}
{"created":"2025-04-24","title":"Causal Models in Requirement Specifications for Machine Learning: A vision","abstract":"Specifying data requirements for machine learning (ML) software systems remains a challenge in requirements engineering (RE). This vision paper explores causal modelling as an RE activity that allows the systematic integration of prior domain knowledge into the design of ML software systems. We propose a workflow to elicit low-level model and data requirements from high-level prior knowledge using causal models. The approach is demonstrated on an industrial fault detection system. This paper outlines future research needed to establish causal modelling as an RE practice.","authors":["Hans-Martin Heyn","Yufei Mao","Roland Weiss","Eric Knauss"],"url":"https://arxiv.org/abs/2502.11629"}
{"created":"2025-04-24","title":"Clinical QA 2.0: Multi-Task Learning for Answer Extraction and Categorization","abstract":"Clinical Question Answering (CQA) plays a crucial role in medical decision-making, enabling physicians to extract relevant information from Electronic Medical Records (EMRs). While transformer-based models such as BERT, BioBERT, and ClinicalBERT have demonstrated state-of-the-art performance in CQA, existing models lack the ability to categorize extracted answers, which is critical for structured retrieval, content filtering, and medical decision support.","authors":["Priyaranjan Pattnayak","Hitesh Laxmichand Patel","Amit Agarwal","Bhargava Kumar","Srikant Panda","Tejaswini Kumar"],"url":"https://arxiv.org/abs/2502.13108"}
{"created":"2025-04-24","title":"Towards Routing and Edge Computing in Satellite-Terrestrial Networks: A Column Generation Approach","abstract":"Edge computing that enables satellites to process raw data locally is expected to bring further timeliness and flexibility to satellite-terrestrial networks (STNs). In this letter, we propose a three-layer edge computing protocol, where raw data collected by the satellites can be processed locally, or transmitted to other satellites or the ground station via multi-hop routing for further processing. The overall computing capacity of the proposed framework is maximized by determining the offloading strategy and routing formation, subject to channel capacity and hop constraints. Given that the problem scale grows exponentially with the number of satellites and maximum-allowed hops, the column generation approach is employed to obtain the global optimal solution by activating only a subset of variables. Numerical results reveal that the proposed three-layer computing protocol, when tolerating a 5-hop routing latency, achieves a 60% improvement in computation capacity compared to the single-layer local computing configuration.","authors":["Yuan Liao","Kan Cheng","Fan Lu","Hao Jin","Zhaohui Yang"],"url":"https://arxiv.org/abs/2502.14422"}
{"created":"2025-04-24","title":"Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning","abstract":"Given inputs of diverse soil characteristics and climate data gathered from various regions, we aimed to build a model to predict accurate land emissions. The problem is important since accurate quantification of the carbon cycle in agroecosystems is crucial for mitigating climate change and ensuring sustainable food production. Predicting accurate land emissions is challenging since calibrating the heterogeneous nature of soil properties, moisture, and environmental conditions is hard at decision-relevant scales. Traditional approaches do not adequately estimate land emissions due to location-independent parameters failing to leverage the spatial heterogeneity and also require large datasets. To overcome these limitations, we proposed Spatial Distribution-Shift Aware Knowledge-Guided Machine Learning (SDSA-KGML), which leverages location-dependent parameters that account for significant spatial heterogeneity in soil moisture from multiple sites within the same region. Experimental results demonstrate that SDSA-KGML models achieve higher local accuracy for the specified states in the Midwest Region.","authors":["Arun Sharma","Majid Farhadloo","Mingzhou Yang","Ruolei Zeng","Subhankar Ghosh","Shashi Shekhar"],"url":"https://arxiv.org/abs/2502.14840"}
{"created":"2025-04-24","title":"Towards Physics-Guided Foundation Models","abstract":"Traditional foundation models are pre-trained on broad datasets to reduce the training resources (e.g., time, energy, labeled samples) needed for fine-tuning a wide range of downstream tasks. However, traditional foundation models struggle with out-of-distribution prediction and can produce outputs that are unrealistic and physically infeasible. We propose the notation of physics-guided foundation models (PGFM), that is, foundation models integrated with broad or general domain (e.g., scientific) physical knowledge applicable to a wide range of downstream tasks.","authors":["Majid Farhadloo","Arun Sharma","Mingzhou Yang","Bharat Jayaprakash","William Northrop","Shashi Shekhar"],"url":"https://arxiv.org/abs/2502.15013"}
{"created":"2025-04-24","title":"Random Number Generation from Pulsars","abstract":"Pulsars exhibit signals with precise inter-arrival times that are on the order of milliseconds to seconds, depending on the individual pulsar. There are subtle variations in the timing of pulsar signals. We show that these variations can serve as a natural entropy source for the creation of Random Number Generators (RNGs). We also explore the effects of using randomness extractors to increase the entropy of random bits extracted from Pulsar timing data. To evaluate the quality of the Pulsar RNG, we model its entropy as a $k$-source and use well-known cryptographic results to show its closeness to a theoretically ideal uniformly random source. To remain consistent with prior work, we also show that the Pulsar RNG passes well-known statistical tests such as the NIST test suite.","authors":["Hayder Tirmazi"],"url":"https://arxiv.org/abs/2502.18430"}
{"created":"2025-04-24","title":"FLAP: Fully-controllable Audio-driven Portrait Video Generation through 3D head conditioned diffusion model","abstract":"Diffusion-based video generation techniques have significantly improved zero-shot talking-head avatar generation, enhancing the naturalness of both head motion and facial expressions. However, existing methods suffer from poor controllability, making them less applicable to real-world scenarios such as filmmaking and live streaming for e-commerce. To address this limitation, we propose FLAP, a novel approach that integrates explicit 3D intermediate parameters (head poses and facial expressions) into the diffusion model for end-to-end generation of realistic portrait videos. The proposed architecture allows the model to generate vivid portrait videos from audio while simultaneously incorporating additional control signals, such as head rotation angles and eye-blinking frequency. Furthermore, the decoupling of head pose and facial expression allows for independent control of each, offering precise manipulation of both the avatar's pose and facial expressions. We also demonstrate its flexibility in integrating with existing 3D head generation methods, bridging the gap between 3D model-based approaches and end-to-end diffusion techniques. Extensive experiments show that our method outperforms recent audio-driven portrait video models in both naturalness and controllability.","authors":["Lingzhou Mu","Baiji Liu","Ruonan Zhang","Guiming Mo","Jiawei Jin","Kai Zhang","Haozhi Huang"],"url":"https://arxiv.org/abs/2502.19455"}
{"created":"2025-04-24","title":"A Quarter of a Century of Neuromorphic Architectures on FPGAs -- an Overview","abstract":"Neuromorphic computing is a relatively new discipline of computer science, where the principles of biological brain's computation and memory are used to create a new way of processing information, based on networks of spiking neurons. Those networks can be implemented as both analog and digital implementations, where for the latter, the Field Programmable Gate Arrays (FPGAs) are a frequent choice, due to their inherent flexibility, allowing the researchers to easily design hardware neuromorphic architecture (NMAs). Moreover, digital NMAs show good promise in simulating various spiking neural networks because of their inherent accuracy and resilience to noise, as opposed to analog implementations. This paper presents an overview of digital NMAs implemented on FPGAs, with a goal of providing useful references to various architectural design choices to the researchers interested in digital neuromorphic systems. We present a taxonomy of NMAs that highlights groups of distinct architectural features, their advantages and disadvantages and identify trends and predictions for the future of those architectures.","authors":["Wiktor J. Szczerek","Artur Podobas"],"url":"https://arxiv.org/abs/2502.20415"}
{"created":"2025-04-24","title":"EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test","abstract":"The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves a 1.38x throughput improvement at a batch size of 64. The code is available at https://github.com/SafeAILab/EAGLE.","authors":["Yuhui Li","Fangyun Wei","Chao Zhang","Hongyang Zhang"],"url":"https://arxiv.org/abs/2503.01840"}
{"created":"2025-04-24","title":"Characterization of Deletion/Substitution Channel Capacity for Small Deletion and Substitution Probabilities","abstract":"We consider binary input deletion/substitution channels, which model certain channels with synchronization errors encountered in practice. Specifically, we focus on the regime of small deletion and substitution probabilities, and by extending an approach developed for the deletion-only channel, we obtain an asymptotic characterization of the channel capacity for independent and identically distributed deletion/substitution channels. We first present an upper bound on the capacity for arbitrary but fixed numbers of deletions and substitutions, and then we extend the result to the case of random deletions and substitutions. Our final result is as follows: The i.i.d. deletion/substitution channel capacity is approximately $1 - H(p_d) - H(p_s)$, for $p_d, p_s \\approx0$, where $p_d$ is the deletion probability, and $p_s$ is the substitution probability.","authors":["Mohammad Kazemi","Tolga M. Duman"],"url":"https://arxiv.org/abs/2503.02545"}
{"created":"2025-04-24","title":"Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for Contact-Rich Manipulation","abstract":"Humans can accomplish complex contact-rich tasks using vision and touch, with highly reactive capabilities such as fast response to external changes and adaptive control of contact forces; however, this remains challenging for robots. Existing visual imitation learning (IL) approaches rely on action chunking to model complex behaviors, which lacks the ability to respond instantly to real-time tactile feedback during the chunk execution. Furthermore, most teleoperation systems struggle to provide fine-grained tactile / force feedback, which limits the range of tasks that can be performed. To address these challenges, we introduce TactAR, a low-cost teleoperation system that provides real-time tactile feedback through Augmented Reality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast visual-tactile imitation learning algorithm for learning contact-rich manipulation skills. RDP employs a two-level hierarchy: (1) a slow latent diffusion policy for predicting high-level action chunks in latent space at low frequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback control at high frequency. This design enables both complex trajectory modeling and quick reactive behavior within a unified framework. Through extensive evaluation across three challenging contact-rich tasks, RDP significantly improves performance compared to state-of-the-art visual IL baselines. Furthermore, experiments show that RDP is applicable across different tactile / force sensors. Code and videos are available on https://reactive-diffusion-policy.github.io.","authors":["Han Xue","Jieji Ren","Wendi Chen","Gu Zhang","Yuan Fang","Guoying Gu","Huazhe Xu","Cewu Lu"],"url":"https://arxiv.org/abs/2503.02881"}
{"created":"2025-04-24","title":"FREAK: Frequency-modulated High-fidelity and Real-time Audio-driven Talking Portrait Synthesis","abstract":"Achieving high-fidelity lip-speech synchronization in audio-driven talking portrait synthesis remains challenging. While multi-stage pipelines or diffusion models yield high-quality results, they suffer from high computational costs. Some approaches perform well on specific individuals with low resources, yet still exhibit mismatched lip movements. The aforementioned methods are modeled in the pixel domain. We observed that there are noticeable discrepancies in the frequency domain between the synthesized talking videos and natural videos. Currently, no research on talking portrait synthesis has considered this aspect. To address this, we propose a FREquency-modulated, high-fidelity, and real-time Audio-driven talKing portrait synthesis framework, named FREAK, which models talking portraits from the frequency domain perspective, enhancing the fidelity and naturalness of the synthesized portraits. FREAK introduces two novel frequency-based modules: 1) the Visual Encoding Frequency Modulator (VEFM) to couple multi-scale visual features in the frequency domain, better preserving visual frequency information and reducing the gap in the frequency spectrum between synthesized and natural frames. and 2) the Audio Visual Frequency Modulator (AVFM) to help the model learn the talking pattern in the frequency domain and improve audio-visual synchronization. Additionally, we optimize the model in both pixel domain and frequency domain jointly. Furthermore, FREAK supports seamless switching between one-shot and video dubbing settings, offering enhanced flexibility. Due to its superior performance, it can simultaneously support high-resolution video results and real-time inference. Extensive experiments demonstrate that our method synthesizes high-fidelity talking portraits with detailed facial textures and precise lip synchronization in real-time, outperforming state-of-the-art methods.","authors":["Ziqi Ni","Ao Fu","Yi Zhou"],"url":"https://arxiv.org/abs/2503.04067"}
{"created":"2025-04-24","title":"ML-based Adaptive Prefetching and Data Placement for US HEP Systems","abstract":"Although benefits from caching in US HEP are well-known, current caching strategies are not adaptive i.e they do not adapt to changing cache access patterns. Newer developments such as the High-Luminosity - Large Hadron Collider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward streaming readout based Data Acquisition systems (DAQs) will increase the data production exponentially and hence burden the storage, compute & network infrastructures. Moreover, existing caching frameworks are optimized to reduce latency, but not optimized for storage. This, in combination with limited cache capacities relative to total data, makes it difficult to achieve data locality.","authors":["Venkat Sai Suman Lamba Karanam","Sarat Sasank Barla","Byrav Ramamurthy","Derek Weitzel"],"url":"https://arxiv.org/abs/2503.06015"}
{"created":"2025-04-24","title":"Exploring Adversarial Transferability between Kolmogorov-arnold Networks","abstract":"Kolmogorov-Arnold Networks (KANs) have emerged as a transformative model paradigm, significantly impacting various fields. However, their adversarial robustness remains less underexplored, especially across different KAN architectures. To explore this critical safety issue, we conduct an analysis and find that due to overfitting to the specific basis functions of KANs, they possess poor adversarial transferability among different KANs. To tackle this challenge, we propose AdvKAN, the first transfer attack method for KANs. AdvKAN integrates two key components: 1) a Breakthrough-Defense Surrogate Model (BDSM), which employs a breakthrough-defense training strategy to mitigate overfitting to the specific structures of KANs. 2) a Global-Local Interaction (GLI) technique, which promotes sufficient interaction between adversarial gradients of hierarchical levels, further smoothing out loss surfaces of KANs. Both of them work together to enhance the strength of transfer attack among different KANs. Extensive experimental results on various KANs and datasets demonstrate the effectiveness of AdvKAN, which possesses notably superior attack capabilities and deeply reveals the vulnerabilities of KANs. Code will be released upon acceptance.","authors":["Songping Wang","Xinquan Yue","Yueming Lyu","Caifeng Shan"],"url":"https://arxiv.org/abs/2503.06276"}
{"created":"2025-04-24","title":"AudioX: Diffusion Transformer for Anything-to-Audio Generation","abstract":"Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/","authors":["Zeyue Tian","Yizhu Jin","Zhaoyang Liu","Ruibin Yuan","Xu Tan","Qifeng Chen","Wei Xue","Yike Guo"],"url":"https://arxiv.org/abs/2503.10522"}
{"created":"2025-04-24","title":"Lorecast: Layout-Aware Performance and Power Forecasting from Natural Language","abstract":"In chip design planning, obtaining reliable performance and power forecasts for various design options is of critical importance. Traditionally, this involves using system-level models, which often lack accuracy, or trial synthesis, which is both labor-intensive and time-consuming. We introduce a new methodology, called Lorecast, which accepts English prompts as input to rapidly generate layout-aware performance and power estimates. This approach bypasses the need for HDL code development and synthesis, making it both fast and user-friendly. Experimental results demonstrate that Lorecast achieves accuracy within a few percent of error compared to post-layout analysis, while significantly reducing turnaround time.","authors":["Runzhi Wang","Prianka Sengupta","Cristhian Roman-Vicharra","Yiran Chen","Jiang Hu"],"url":"https://arxiv.org/abs/2503.11662"}
{"created":"2025-04-24","title":"ST-Think: How Multimodal Large Language Models Reason About 4D Worlds from Ego-Centric Videos","abstract":"Humans excel at spatial-temporal reasoning, effortlessly interpreting dynamic visual events from an egocentric viewpoint. However, whether multimodal large language models (MLLMs) can similarly understand the 4D world remains uncertain. This paper explores multimodal spatial-temporal reasoning from an egocentric perspective, aiming to equip MLLMs with human-like reasoning capabilities. To support this objective, we introduce \\textbf{Ego-ST Bench}, a novel benchmark containing over 5,000 question-answer pairs across four categories, systematically evaluating spatial, temporal, and integrated spatial-temporal reasoning. Additionally, we propose \\textbf{ST-R1} training paradigm, a video-based reasoning model that incorporates reverse thinking into its reinforcement learning process, significantly enhancing performance. We combine long-chain-of-thought (long-CoT) supervised fine-tuning with Group Relative Policy Optimization (GRPO) reinforcement learning, achieving notable improvements with limited high-quality data. Ego-ST Bench and ST-R1 provide valuable insights and resources for advancing video-based spatial-temporal reasoning research.","authors":["Peiran Wu","Yunze Liu","Miao Liu","Junxiao Shen"],"url":"https://arxiv.org/abs/2503.12542"}
{"created":"2025-04-24","title":"UniVG: A Generalist Diffusion Model for Unified Image Generation and Editing","abstract":"Text-to-Image (T2I) diffusion models have shown impressive results in generating visually compelling images following user prompts. Building on this, various methods further fine-tune the pre-trained T2I model for specific tasks. However, this requires separate model architectures, training designs, and multiple parameter sets to handle different tasks. In this paper, we introduce UniVG, a generalist diffusion model capable of supporting a diverse range of image generation tasks with a single set of weights. UniVG treats multi-modal inputs as unified conditions to enable various downstream applications, ranging from T2I generation, inpainting, instruction-based editing, identity-preserving generation, and layout-guided generation, to depth estimation and referring segmentation. Through comprehensive empirical studies on data mixing and multi-task training, we provide detailed insights into the training processes and decisions that inform our final designs. For example, we show that T2I generation and other tasks, such as instruction-based editing, can coexist without performance trade-offs, while auxiliary tasks like depth estimation and referring segmentation enhance image editing. Notably, our model can even outperform some task-specific models on their respective benchmarks, marking a significant step towards a unified image generation model.","authors":["Tsu-Jui Fu","Yusu Qian","Chen Chen","Wenze Hu","Zhe Gan","Yinfei Yang"],"url":"https://arxiv.org/abs/2503.12652"}
{"created":"2025-04-24","title":"The deep multi-FBSDE method: a robust deep learning method for coupled FBSDEs","abstract":"We introduce the deep multi-FBSDE method for robust approximation of coupled forward-backward stochastic differential equations (FBSDEs), focusing on cases where the deep BSDE method of Han, Jentzen, and E (2018) fails to converge. To overcome the convergence issues, we consider a family of FBSDEs that are equivalent to the original problem in the sense that they satisfy the same associated partial differential equation (PDE). Our algorithm proceeds in two phases: first, we approximate the initial condition for the FBSDE family, and second, we approximate the original FBSDE using the initial condition approximated in the first phase. Numerical experiments show that our method converges even when the standard deep BSDE method does not.","authors":["Kristoffer Andersson","Adam Andersson","Cornelis W. Oosterlee"],"url":"https://arxiv.org/abs/2503.13193"}
{"created":"2025-04-24","title":"Why Do Multi-Agent LLM Systems Fail?","abstract":"Despite growing enthusiasm for Multi-Agent LLM Systems (MAS), their performance gains on popular benchmarks often remain minimal compared with single-agent frameworks. This gap highlights the need to systematically analyze the challenges hindering MAS effectiveness.","authors":["Mert Cemri","Melissa Z. Pan","Shuyi Yang","Lakshya A. Agrawal","Bhavya Chopra","Rishabh Tiwari","Kurt Keutzer","Aditya Parameswaran","Dan Klein","Kannan Ramchandran","Matei Zaharia","Joseph E. Gonzalez","Ion Stoica"],"url":"https://arxiv.org/abs/2503.13657"}
{"created":"2025-04-24","title":"Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations","abstract":"LLMs often adopt an assertive language style also when making false claims. Such ``overconfident hallucinations'' mislead users and erode trust. Achieving the ability to express in language the actual degree of uncertainty around a claim is therefore of great importance. We find that ``verbal uncertainty'' is governed by a single linear feature in the representation space of LLMs, and show that this has only moderate correlation with the actual ``semantic uncertainty'' of the model. We apply this insight and show that (1) the mismatch between semantic and verbal uncertainty is a better predictor of hallucinations than semantic uncertainty alone and (2) we can intervene on verbal uncertainty at inference time and reduce confident hallucinations on short-form answers, achieving an average relative reduction of ~30%.","authors":["Ziwei Ji","Lei Yu","Yeskendir Koishekenov","Yejin Bang","Anthony Hartshorn","Alan Schelten","Cheng Zhang","Pascale Fung","Nicola Cancedda"],"url":"https://arxiv.org/abs/2503.14477"}
{"created":"2025-04-24","title":"Toward a Human-AI Task Tensor: A Taxonomy for Organizing Work in the Age of Generative AI","abstract":"We introduce a framework for understanding the impact of generative AI on human work, which we call the human-AI task tensor. A tensor is a structured framework that organizes tasks along multiple interdependent dimensions. Our human-AI task tensor introduces a systematic approach to studying how humans and AI interact to perform tasks, and has eight dimensions: task definition, AI contribution, interaction modality, audit requirement, output definition, decision-making authority, AI structure, and human persona. After describing the eight dimensions of the tensor, we provide illustrative frameworks (derived from projections of the tensor) and a human-AI task canvas that provide analytical tractability and practical insight for organizational decision-making. We demonstrate how the human-AI task tensor can be used to organize emerging and future research on generative AI. We propose that the human-AI task tensor offers a starting point for understanding how work will be performed with the emergence of generative AI.","authors":["Anil R. Doshi","Alastair Moore"],"url":"https://arxiv.org/abs/2503.15490"}
{"created":"2025-04-24","title":"Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the \"overthinking phenomenon\". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.","authors":["Yang Sui","Yu-Neng Chuang","Guanchu Wang","Jiamu Zhang","Tianyi Zhang","Jiayi Yuan","Hongyi Liu","Andrew Wen","Shaochen Zhong","Hanjie Chen","Xia Hu"],"url":"https://arxiv.org/abs/2503.16419"}
{"created":"2025-04-24","title":"SuperARC: An Agnostic Test for Narrow, General, and Super Intelligence Based On the Principles of Recursive Compression and Algorithmic Probability","abstract":"We introduce an open-ended test grounded in algorithmic probability that can avoid benchmark contamination in the quantitative evaluation of frontier models in the context of their Artificial General Intelligence (AGI) and Superintelligence (ASI) claims. Unlike other tests, this test does not rely on statistical compression methods (such as GZIP or LZW), which are more closely related to Shannon entropy than to Kolmogorov complexity and are not able to test beyond simple pattern matching. The test challenges aspects of AI, in particular LLMs, related to features of intelligence of fundamental nature such as synthesis and model creation in the context of inverse problems (generating new knowledge from observation). We argue that metrics based on model abstraction and abduction (optimal Bayesian `inference') for predictive `planning' can provide a robust framework for testing intelligence, including natural intelligence (human and animal), narrow AI, AGI, and ASI. We found that LLM model versions tend to be fragile and incremental as a result of memorisation only with progress likely driven by the size of training data. The results were compared with a hybrid neurosymbolic approach that theoretically guarantees universal intelligence based on the principles of algorithmic probability and Kolmogorov complexity. The method outperforms LLMs in a proof-of-concept on short binary sequences. We prove that compression is equivalent and directly proportional to a system's predictive power and vice versa. That is, if a system can better predict it can better compress, and if it can better compress, then it can better predict. Our findings strengthen the suspicion regarding the fundamental limitations of LLMs, exposing them as systems optimised for the perception of mastery over human language.","authors":["Alberto Hern\\'andez-Espinosa","Luan Ozelim","Felipe S. Abrah\\~ao","Hector Zenil"],"url":"https://arxiv.org/abs/2503.16743"}
{"created":"2025-04-24","title":"OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery","abstract":"Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks.","authors":["Vignesh Prabhakar","Md Amirul Islam","Adam Atanas","Yao-Ting Wang","Joah Han","Aastha Jhunjhunwala","Rucha Apte","Robert Clark","Kang Xu","Zihan Wang","Kai Liu"],"url":"https://arxiv.org/abs/2503.17604"}
{"created":"2025-04-24","title":"On Symmetries in Convolutional Weights","abstract":"We explore the symmetry of the mean k x k weight kernel in each layer of various convolutional neural networks. Unlike individual neurons, the mean kernels in internal layers tend to be symmetric about their centers instead of favoring specific directions. We investigate why this symmetry emerges in various datasets and models, and how it is impacted by certain architectural choices. We show how symmetry correlates with desirable properties such as shift and flip consistency, and might constitute an inherent inductive bias in convolutional neural networks.","authors":["Bilal Alsallakh","Timothy Wroge","Vivek Miglani","Narine Kokhlikyan"],"url":"https://arxiv.org/abs/2503.19215"}
{"created":"2025-04-24","title":"Multiplication of 0-1 matrices via clustering","abstract":"We study applications of clustering (in particular the $k$-center","authors":["Jesper Jansson","Miroslaw Kowaluk","Andrzej Lingas","Mia Persson"],"url":"https://arxiv.org/abs/2503.19631"}
{"created":"2025-04-24","title":"Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence","abstract":"Recent advances in reasoning models have demonstrated significant improvements in accuracy, particularly for complex tasks such as mathematical reasoning, by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning branches exist, we decode multiple tokens per step using a specialized attention mask, processing them within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves over 100% speedup in decoding time while maintaining the answer quality.","authors":["Yijiong Yu"],"url":"https://arxiv.org/abs/2503.20533"}
{"created":"2025-04-24","title":"MMMORRF: Multimodal Multilingual Modularized Reciprocal Rank Fusion","abstract":"Videos inherently contain multiple modalities, including visual events, text overlays, sounds, and speech, all of which are important for retrieval. However, state-of-the-art multimodal language models like VAST and LanguageBind are built on vision-language models (VLMs), and thus overly prioritize visual signals. Retrieval benchmarks further reinforce this bias by focusing on visual queries and neglecting other modalities. We create a search system MMMORRF that extracts text and features from both visual and audio modalities and integrates them with a novel modality-aware weighted reciprocal rank fusion. MMMORRF is both effective and efficient, demonstrating practicality in searching videos based on users' information needs instead of visual descriptive queries. We evaluate MMMORRF on MultiVENT 2.0 and TVR, two multimodal benchmarks designed for more targeted information needs, and find that it improves nDCG@20 by 81% over leading multimodal encoders and 37% over single-modality retrieval, demonstrating the value of integrating diverse modalities.","authors":["Saron Samuel","Dan DeGenaro","Jimena Guallar-Blasco","Kate Sanders","Oluwaseun Eisape","Arun Reddy","Alexander Martin","Andrew Yates","Eugene Yang","Cameron Carpenter","David Etter","Efsun Kayi","Matthew Wiesner","Kenton Murray","Reno Kriz"],"url":"https://arxiv.org/abs/2503.20698"}
{"created":"2025-04-24","title":"MAD Chairs: A new tool to evaluate AI","abstract":"This paper contributes a new way to evaluate AI. Much as one might evaluate a machine in terms of its performance at chess, this approach involves evaluating a machine in terms of its performance at a game called \"MAD Chairs\". At the time of writing, evaluation with this game exposed opportunities to improve Claude, Gemini, ChatGPT, Qwen and DeepSeek. Furthermore, this paper sets a stage for future innovation in game theory and AI safety by providing an example of success with non-standard approaches to each: studying a game beyond the scope of previous game theoretic tools and mitigating a serious AI safety risk in a way that requires neither determination of values nor their enforcement.","authors":["Chris Santos-Lang","Christopher M. Homan"],"url":"https://arxiv.org/abs/2503.20986"}
{"created":"2025-04-24","title":"Integrated utilization of equations and small dataset in the Koopman operator: applications to forward and inverse problems","abstract":"In recent years, there has been a growing interest in data-driven approaches in physics, such as extended dynamic mode decomposition (EDMD). The EDMD algorithm focuses on nonlinear time-evolution systems, and the constructed Koopman matrix yields the next-time prediction with only linear matrix-product operations. Note that data-driven approaches generally require a large dataset. However, assume that one has some prior knowledge, even if it may be ambiguous. Then, one could achieve sufficient learning from only a small dataset by taking advantage of the prior knowledge. This paper yields methods for incorporating ambiguous prior knowledge into the EDMD algorithm. The ambiguous prior knowledge in this paper corresponds to the underlying time-evolution equations with unknown parameters. First, we apply the proposed method to forward problems, i.e., prediction tasks. Second, we propose a scheme to apply the proposed method to inverse problems, i.e., parameter estimation tasks. We demonstrate the learning with only a small dataset using guiding examples, i.e., the Duffing and the van der Pol systems.","authors":["Ichiro Ohta","Shota Koyanagi","Kayo Kinjo","Jun Ohkubo"],"url":"https://arxiv.org/abs/2503.21048"}
{"created":"2025-04-24","title":"A Measure Based Generalizable Approach to Understandability","abstract":"Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).","authors":["Vikas Kushwaha","Sruti Srinivasa Ragavan","Subhajit Roy"],"url":"https://arxiv.org/abs/2503.21615"}
{"created":"2025-04-24","title":"Application of Battery Storage to Switching Predictive Control of Power Distribution Systems Including Road Heating","abstract":"In regions with heavy snowfall, the living environment is becoming a serious problem due to heavy snow accumulation. A road heating is an electrical device which promotes snow melting by burying a heating cable as a thermal source underground in such regions. When integrating the road heating into power distribution systems, we need to optimize the flow of electric power by appropriately integrating distributed power sources and conventional power distribution equipment. In this paper, we introduce a battery storage to the power distribution system including road heating, and extend the predictive switching control of the systems due to the authors' previous study to the case where battery storage is installed. As a main result, we propose a predictive switching control that utilizes photovoltaic (PV) power generation and surplus power stored in the battery storage effectively, and achieves the reduction of distribution loss, attenuation of voltage fluctuation, and efficient snow melting, simultaneously. We verify the effectiveness of the application of battery storage through numerical simulation using actual time series data of weather conditions and active power of the PV power generation and load.","authors":["Chiaki Kojima","Yuya Muto","Hikaru Akutsu","Rinnosuke Shima","Yoshihiko Susuki"],"url":"https://arxiv.org/abs/2503.24104"}
{"created":"2025-04-24","title":"Dynamic hashtag recommendation in social media with trend shift detection and adaptation","abstract":"Hashtag recommendation systems have emerged as a key tool for automatically suggesting relevant hashtags and enhancing content categorization and search. However, existing static models struggle to adapt to the highly dynamic nature of social media conversations, where new hashtags constantly emerge and existing ones undergo semantic shifts. To address these challenges, this paper introduces H-ADAPTS (Hashtag recommendAtion by Detecting and adAPting to Trend Shifts), a dynamic hashtag recommendation methodology that employs a trend-aware mechanism to detect shifts in hashtag usage-reflecting evolving trends and topics within social media conversations-and triggers efficient model adaptation based on a (small) set of recent posts. Additionally, the Apache Storm framework is leveraged to support scalable and fault-tolerant analysis of high-velocity social data, enabling the timely detection of trend shifts. Experimental results from two real-world case studies, including the COVID-19 pandemic and the 2020 US presidential election, demonstrate the effectiveness of H-ADAPTS in providing timely and relevant hashtag recommendations by adapting to emerging trends, significantly outperforming existing solutions.","authors":["Riccardo Cantini","Fabrizio Marozzo","Alessio Orsino","Domenico Talia","Paolo Trunfio"],"url":"https://arxiv.org/abs/2504.00044"}
{"created":"2025-04-24","title":"CF-CAM: Cluster Filter Class Activation Mapping for Reliable Gradient-Based Interpretability","abstract":"As deep learning continues to advance, the transparency of neural network decision-making remains a critical challenge, limiting trust and applicability in high-stakes domains. Class Activation Mapping (CAM) techniques have emerged as a key approach toward visualizing model decisions, yet existing methods face inherent trade-offs. Gradient-based CAM variants suffer from sensitivity to gradient perturbations due to gradient noise, leading to unstable and unreliable explanations. Conversely, gradient-free approaches mitigate gradient instability but incur significant computational overhead and inference latency. To address these limitations, we propose a Cluster Filter Class Activation Map (CF-CAM) technique, a novel framework that reintroduces gradient-based weighting while enhancing robustness against gradient noise. CF-CAM utilizes hierarchical importance weighting strategy to balance discriminative feature preservation and noise elimination. A density-aware channel clustering method via Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups semantically relevant feature channels and discard noise-prone activations. Additionally, cluster-conditioned gradient filtering leverages Gaussian filters to refine gradient signals, preserving edge-aware localization while suppressing noise impact. Experiment results demonstrate that CF-CAM achieves superior interpretability performance while enhancing computational efficiency, outperforming state-of-the-art CAM methods in faithfulness and robustness. By effectively mitigating gradient instability without excessive computational cost, CF-CAM provides a competitive solution for enhancing the interpretability of deep neural networks in critical applications such as autonomous driving and medical diagnosis.","authors":["Hongjie He","Xu Pan","Yudong Yao"],"url":"https://arxiv.org/abs/2504.00060"}
{"created":"2025-04-24","title":"Distributed Model Predictive Control for Dynamic Cooperation of Multi-Agent Systems","abstract":"We propose a distributed model predictive control (MPC) framework for coordinating heterogeneous, nonlinear multi-agent systems under individual and coupling constraints. The cooperative task is encoded as a shared objective function minimized collectively by the agents. Each agent optimizes an artificial reference as an intermediate step towards the cooperative objective, along with a control input to track it. We establish recursive feasibility, asymptotic stability, and transient performance bounds under suitable assumptions. The solution to the cooperative task is not predetermined but emerges from the optimized interactions of the agents. We demonstrate the framework on numerical examples inspired by satellite constellation control, collision-free narrow-passage traversal, and coordinated quadrotor flight.","authors":["Matthias K\\\"ohler","Matthias A. M\\\"uller","Frank Allg\\\"ower"],"url":"https://arxiv.org/abs/2504.00225"}
{"created":"2025-04-24","title":"Leveraging LLMs for User Stories in AI Systems: UStAI Dataset","abstract":"AI systems are gaining widespread adoption across various sectors and domains. Creating high-quality AI system requirements is crucial for aligning the AI system with business goals and consumer values and for social responsibility. However, with the uncertain nature of AI systems and the heavy reliance on sensitive data, more research is needed to address the elicitation and analysis of AI systems requirements. With the proprietary nature of many AI systems, there is a lack of open-source requirements artifacts and technical requirements documents for AI systems, limiting broader research and investigation. With Large Language Models (LLMs) emerging as a promising alternative to human-generated text, this paper investigates the potential use of LLMs to generate user stories for AI systems based on abstracts from scholarly papers. We conducted an empirical evaluation using three LLMs and generated $1260$ user stories from $42$ abstracts from $26$ domains. We assess their quality using the Quality User Story (QUS) framework. Moreover, we identify relevant non-functional requirements (NFRs) and ethical principles. Our analysis demonstrates that the investigated LLMs can generate user stories inspired by the needs of various stakeholders, offering a promising approach for generating user stories for research purposes and for aiding in the early requirements elicitation phase of AI systems. We have compiled and curated a collection of stories generated by various LLMs into a dataset (UStAI), which is now publicly available for use.","authors":["Asma Yamani","Malak Baslyman","Moataz Ahmed"],"url":"https://arxiv.org/abs/2504.00513"}
{"created":"2025-04-24","title":"On Benchmarking Code LLMs for Android Malware Analysis","abstract":"Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names. This paper presents CAMA, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. CAMA specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics (consistency, fidelity, and semantic relevance), enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use CAMA to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.","authors":["Yiling He","Hongyu She","Xingzhi Qian","Xinran Zheng","Zhuo Chen","Zhan Qin","Lorenzo Cavallaro"],"url":"https://arxiv.org/abs/2504.00694"}
{"created":"2025-04-24","title":"Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment","abstract":"Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view-adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions -- including low-light, overexposure, and varying exposure -- while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality.","authors":["Ziteng Cui","Xuangeng Chu","Tatsuya Harada"],"url":"https://arxiv.org/abs/2504.01503"}
{"created":"2025-04-24","title":"MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated Expert Parallelism","abstract":"Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity. However, its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs. We present MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models. MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules. To fully exploit disaggregation in the presence of MoE's sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference. Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization. To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization. Experimental results indicate that MegaScale-Infer achieves up to 1.90x higher per-GPU throughput than state-of-the-art solutions.","authors":["Ruidong Zhu","Ziheng Jiang","Chao Jin","Peng Wu","Cesar A. Stuardo","Dongyang Wang","Xinlei Zhang","Huaping Zhou","Haoran Wei","Yang Cheng","Jianzhe Xiao","Xinyi Zhang","Lingjun Liu","Haibin Lin","Li-Wen Chang","Jianxi Ye","Xiao Yu","Xuanzhe Liu","Xin Jin","Xin Liu"],"url":"https://arxiv.org/abs/2504.02263"}
{"created":"2025-04-24","title":"OnRL-RAG: Real-Time Personalized Mental Health Dialogue System","abstract":"Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment.","authors":["Ahsan Bilal","Beiyu Lin"],"url":"https://arxiv.org/abs/2504.02894"}
{"created":"2025-04-24","title":"Short Video Segment-level User Dynamic Interests Modeling in Personalized Recommendation","abstract":"The rapid growth of short videos has necessitated effective recommender systems to match users with content tailored to their evolving preferences. Current video recommendation models primarily treat each video as a whole, overlooking the dynamic nature of user preferences with specific video segments. In contrast, our research focuses on segment-level user interest modeling, which is crucial for understanding how users' preferences evolve during video browsing. To capture users' dynamic segment interests, we propose an innovative model that integrates a hybrid representation module, a multi-modal user-video encoder, and a segment interest decoder. Our model addresses the challenges of capturing dynamic interest patterns, missing segment-level labels, and fusing different modalities, achieving precise segment-level interest prediction. We present two downstream tasks to evaluate the effectiveness of our segment interest modeling approach: video-skip prediction and short video recommendation. Our experiments on real-world short video datasets with diverse modalities show promising results on both tasks. It demonstrates that segment-level interest modeling brings a deep understanding of user engagement and enhances video recommendations. We also release a unique dataset that includes segment-level video data and diverse user behaviors, enabling further research in segment-level interest modeling. This work pioneers a novel perspective on understanding user segment-level preference, offering the potential for more personalized and engaging short video experiences.","authors":["Zhiyu He","Zhixin Ling","Jiayu Li","Zhiqiang Guo","Weizhi Ma","Xinchen Luo","Min Zhang","Guorui Zhou"],"url":"https://arxiv.org/abs/2504.04237"}
{"created":"2025-04-24","title":"Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization","abstract":"While large language models (LLMs) have demonstrated exceptional capabilities in challenging tasks such as mathematical reasoning, existing methods to enhance reasoning ability predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data after pre-training. However, these approaches critically depend on external supervision--such as human-labelled reasoning traces, verified golden answers, or pre-trained reward models--which limits scalability and practical applicability. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. EMPO does not require any supervised information for incentivizing reasoning capabilities (i.e., neither verifiable reasoning traces, problems with golden answers, nor additional pre-trained reward models). By continuously minimizing the predictive entropy of LLMs on unlabeled user queries in a latent semantic space, EMPO enables purely self-supervised evolution of reasoning capabilities with strong flexibility and practicality. Our experiments demonstrate competitive performance of EMPO on both mathematical reasoning and free-form natural reasoning tasks. Specifically, without any supervised signals, \\ours boosts the accuracy of Qwen2.5-Math-7B Base from 30.7\\% to 48.1\\% on mathematical benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro.","authors":["Qingyang Zhang","Haitao Wu","Changqing Zhang","Peilin Zhao","Yatao Bian"],"url":"https://arxiv.org/abs/2504.05812"}
{"created":"2025-04-24","title":"A Robust Real-Time Lane Detection Method with Fog-Enhanced Feature Fusion for Foggy Conditions","abstract":"Lane detection is a critical component of Advanced Driver Assistance Systems (ADAS). Existing lane detection algorithms generally perform well under favorable weather conditions. However, their performance degrades significantly in adverse conditions, such as fog, which increases the risk of traffic accidents. This challenge is compounded by the lack of specialized datasets and methods designed for foggy environments. To address this, we introduce the FoggyLane dataset, captured in real-world foggy scenarios, and synthesize two additional datasets, FoggyCULane and FoggyTusimple, from existing popular lane detection datasets. Furthermore, we propose a robust Fog-Enhanced Network for lane detection, incorporating a Global Feature Fusion Module (GFFM) to capture global relationships in foggy images, a Kernel Feature Fusion Module (KFFM) to model the structural and positional relationships of lane instances, and a Low-level Edge Enhanced Module (LEEM) to address missing edge details in foggy conditions. Comprehensive experiments demonstrate that our method achieves state-of-the-art performance, with F1-scores of 95.04 on FoggyLane, 79.85 on FoggyCULane, and 96.95 on FoggyTusimple. Additionally, with TensorRT acceleration, the method reaches a processing speed of 38.4 FPS on the NVIDIA Jetson AGX Orin, confirming its real-time capabilities and robustness in foggy environments.","authors":["Ronghui Zhang","Yuhang Ma","Tengfei Li","Ziyu Lin","Yueying Wu","Junzhou Chen","Lin Zhang","Jia Hu","Tony Z. Qiu","Konghui Guo"],"url":"https://arxiv.org/abs/2504.06121"}
{"created":"2025-04-24","title":"Towards Distribution Matching between Collaborative and Language Spaces for Generative Recommendation","abstract":"Generative recommendation aims to learn the underlying generative process over the entire item set to produce recommendations for users. Although it leverages non-linear probabilistic models to surpass the limited modeling capacity of linear factor models, it is often constrained by a trade-off between representation ability and tractability. With the rise of a new generation of generative methods based on pre-trained language models (LMs), incorporating LMs into general recommendation with implicit feedback has gained considerable attention. However, adapting them to generative recommendation remains challenging. The core reason lies in the mismatch between the input-output formats and semantics of generative models and LMs, making it challenging to achieve optimal alignment in the feature space. This work addresses this issue by proposing a model-agnostic generative recommendation framework called DMRec, which introduces a probabilistic meta-network to bridge the outputs of LMs with user interactions, thereby enabling an equivalent probabilistic modeling process. Subsequently, we design three cross-space distribution matching processes aimed at maximizing shared information while preserving the unique semantics of each space and filtering out irrelevant information. We apply DMRec to three different types of generative recommendation methods and conduct extensive experiments on three public datasets. The experimental results demonstrate that DMRec can effectively enhance the recommendation performance of these generative models, and it shows significant advantages over mainstream LM-enhanced recommendation methods.","authors":["Yi Zhang","Yiwen Zhang","Yu Wang","Tong Chen","Hongzhi Yin"],"url":"https://arxiv.org/abs/2504.07363"}
{"created":"2025-04-24","title":"PoGO: A Scalable Proof of Useful Work via Quantized Gradient Descent and Merkle Proofs","abstract":"We present a design called Proof of Gradient Optimization (PoGO) for blockchain consensus, where miners produce verifiable evidence of training large-scale machine-learning models. Building on previous work, we incorporate quantized gradients (4-bit precision) to reduce storage and computation requirements, while still preserving the ability of verifiers to check that real progress has been made on lowering the model's loss. Additionally, we employ Merkle proofs over the full 32-bit model to handle large parameter sets and to enable random leaf checks with minimal on-chain data. We illustrate these ideas using GPT-3 (175B parameters) as a reference example and also refer to smaller but high-performance models (e.g., Gemma~3 with 27B parameters). We provide an empirical cost analysis showing that verification is significantly cheaper than training, thanks in part to quantization and sampling. We also discuss the necessity of longer block times (potentially hours) when incorporating meaningful training steps, the trade-offs when using specialized GPU hardware, and how binary diffs may incrementally optimize updates. Finally, we note that fine-tuning can be handled in a similar manner, merely changing the dataset and the manner of sampling but preserving the overall verification flow. Our protocol allows verifiers to issue either positive or negative attestations; these are aggregated at finalization to either confirm the update or slash the miner.","authors":["Jos\\'e I. Orlicki"],"url":"https://arxiv.org/abs/2504.07540"}
{"created":"2025-04-24","title":"Ozaki Scheme II: A GEMM-oriented emulation of floating-point matrix multiplication using an integer modular technique","abstract":"This paper addresses emulation algorithms for matrix multiplication. General Matrix-Matrix Multiplication (GEMM), a fundamental operation in the Basic Linear Algebra Subprograms (BLAS), is typically optimized for specific hardware architectures. The Ozaki scheme is a well-established GEMM-based emulation method for matrix multiplication, wherein input matrices are decomposed into several low-precision components to ensure that the resulting matrix product is computed exactly through numerical operations. This study proposes a novel GEMM-based emulation method for matrix multiplication that leverages the Chinese Remainder Theorem. The proposed method inherits the computational efficiency of highly optimized GEMM routines and further enables control over the number of matrix multiplications, which can enhance computational accuracy. We present numerical experiments featuring INT8 Tensor Core operations on GPUs and FP64 arithmetic on CPUs as case studies. The results demonstrate that FP64 emulation using the proposed method achieves performance levels of up to 7.4 to 9.8 TFLOPS on the NVIDIA RTX 4090 and 56.6 to 80.2 TFLOPS on the NVIDIA GH200, exceeding the measured performance of native FP64 arithmetic. Furthermore, for FP64 computations on CPUs, the proposed method achieved up to a 2.3x speedup in emulating quadruple-precision arithmetic compared to the conventional Ozaki scheme.","authors":["Katsuhisa Ozaki","Yuki Uchino","Toshiyuki Imamura"],"url":"https://arxiv.org/abs/2504.08009"}
{"created":"2025-04-24","title":"On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction","abstract":"The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. A few challenges in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best trade-off between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. In this paper, we attack this problem and present our contributions biased to the applied data science side, including:","authors":["Jinfeng Zhuang","Yinrui Li","Runze Su","Ke Xu","Zhixuan Shao","Kungang Li","Ling Leng","Han Sun","Meng Qi","Yixiong Meng","Yang Tang","Zhifang Liu","Qifei Shen","Aayush Mudgal","Caleb Lu","Jie Liu","Hongda Shen"],"url":"https://arxiv.org/abs/2504.08169"}
{"created":"2025-04-24","title":"Analyzing 16,193 LLM Papers for Fun and Profits","abstract":"Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science conferences over the past six years (2019-2024). We approach this analysis from four distinct perspectives: (1) We investigate how LLM research is driving topic shifts within major conferences. (2) We adopt a topic modeling approach to identify various areas of LLM-related topic growth and reveal the topics of concern at different conferences. (3) We explore distinct contribution patterns of academic and industrial institutions. (4) We study the influence of national origins on LLM development trajectories. Synthesizing the findings from these diverse analytical angles, we derive ten key insights that illuminate the dynamics and evolution of the LLM research ecosystem.","authors":["Zhiqiu Xia","Lang Zhu","Bingzhe Li","Feng Chen","Qiannan Li","Chunhua Liao","Feiyi Wang","Hang Liu"],"url":"https://arxiv.org/abs/2504.08619"}
{"created":"2025-04-24","title":"Hub Star Modeling 2.0 for Medallion Architecture","abstract":"Data warehousing enables performant access to high-quality data integrated from dynamic data sources. The medallion architecture, a standard for data warehousing, addresses these goals by organizing data into bronze, silver and gold layers, representing raw, integrated, and fit-to-purpose data, respectively. In terms of data modeling, bronze layer retains the structure of source data with additional metadata. The gold layer follows established modeling approaches such as star schema, snowflake, and flattened tables. The silver layer, acting as a canonical form, requires a flexible and scalable model to support continuous changes and incremental development. This paper introduces an enhanced Hub Star modeling approach tailored for the medallion architecture, simplifying silver-layer data modeling by generalizing hub and star concepts. This approach has been demonstrated using Databricks and the retail-org sample dataset, with all modeling and transformation scripts available on GitHub.","authors":["Shahram Salami"],"url":"https://arxiv.org/abs/2504.08788"}
{"created":"2025-04-24","title":"Debiasing 6-DOF IMU via Hierarchical Learning of Continuous Bias Dynamics","abstract":"This paper develops a deep learning approach to the online debiasing of IMU gyroscopes and accelerometers. Most existing methods rely on implicitly learning a bias term to compensate for raw IMU data. Explicit bias learning has recently shown its potential as a more interpretable and motion-independent alternative. However, it remains underexplored and faces challenges, particularly the need for ground truth bias data, which is rarely available. To address this, we propose a neural ordinary differential equation (NODE) framework that explicitly models continuous bias dynamics, requiring only pose ground truth, often available in datasets. This is achieved by extending the canonical NODE framework to the matrix Lie group for IMU kinematics with a hierarchical training strategy. The validation on two public datasets and one real-world experiment demonstrates significant accuracy improvements in IMU measurements, reducing errors in both pure IMU integration and visual-inertial odometry.","authors":["Ben Liu","Tzu-Yuan Lin","Wei Zhang","Maani Ghaffari"],"url":"https://arxiv.org/abs/2504.09495"}
{"created":"2025-04-24","title":"SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users","abstract":"Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.","authors":["Xinnong Zhang","Jiayu Lin","Xinyi Mou","Shiyue Yang","Xiawei Liu","Libo Sun","Hanjia Lyu","Yihang Yang","Weihong Qi","Yue Chen","Guanying Li","Ling Yan","Yao Hu","Siming Chen","Yu Wang","Xuanjing Huang","Jiebo Luo","Shiping Tang","Libo Wu","Baohua Zhou","Zhongyu Wei"],"url":"https://arxiv.org/abs/2504.10157"}
{"created":"2025-04-24","title":"Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs","abstract":"Large language models (LLMs) perform well in medical QA, but their effectiveness in Japanese contexts is limited due to privacy constraints that prevent the use of commercial models like GPT-4 in clinical settings. As a result, recent efforts focus on instruction-tuning open-source LLMs, though the potential of combining them with retrieval-augmented generation (RAG) remains underexplored. To bridge this gap, we are the first to explore a knowledge graph-based (KG) RAG framework for Japanese medical QA small-scale open-source LLMs. Experimental results show that KG-based RAG has only a limited impact on Japanese medical QA using small-scale open-source LLMs. Further case studies reveal that the effectiveness of the RAG is sensitive to the quality and relevance of the external retrieved content. These findings offer valuable insights into the challenges and potential of applying RAG in Japanese medical QA, while also serving as a reference for other low-resource languages.","authors":["Yingjian Chen","Feiyang Li","Xingyu Song","Tianxiao Li","Zixin Xu","Xiujie Chen","Issey Sukeda","Irene Li"],"url":"https://arxiv.org/abs/2504.10982"}
{"created":"2025-04-24","title":"MediSee: Reasoning-based Pixel-level Perception in Medical Images","abstract":"Despite remarkable advancements in pixel-level medical image perception, existing methods are either limited to specific tasks or heavily rely on accurate bounding boxes or text labels as input prompts. However, the medical knowledge required for input is a huge obstacle for general public, which greatly reduces the universality of these methods. Compared with these domain-specialized auxiliary information, general users tend to rely on oral queries that require logical reasoning. In this paper, we introduce a novel medical vision task: Medical Reasoning Segmentation and Detection (MedSD), which aims to comprehend implicit queries about medical images and generate the corresponding segmentation mask and bounding box for the target object. To accomplish this task, we first introduce a Multi-perspective, Logic-driven Medical Reasoning Segmentation and Detection (MLMR-SD) dataset, which encompasses a substantial collection of medical entity targets along with their corresponding reasoning. Furthermore, we propose MediSee, an effective baseline model designed for medical reasoning segmentation and detection. The experimental results indicate that the proposed method can effectively address MedSD with implicit colloquial queries and outperform traditional medical referring segmentation methods.","authors":["Qinyue Tong","Ziqian Lu","Jun Liu","Yangming Zheng","Zheming Lu"],"url":"https://arxiv.org/abs/2504.11008"}
{"created":"2025-04-24","title":"Lagrangian finite elements in Sobolev-like spaces of order $3/2$","abstract":"This paper introduces a Sobolev-like space of order $3/2$, denoted as $\\widehat{H}^{3/2}$, for Lagrangian finite elements, especially for $C^0$ elements. It is motivated by the limitations of current stability analysis of the evolving surface finite element method (ESFEM), which relies exclusively on an energy estimate framework. To establish a PDE-based analysis framework for ESFEM, we encounter a fundamental regularity mismatch: the ESFEM adopts the $C^0$ elements, while the PDE regularity theory requires $H^{3/2}$ regularity for solutions. To overcome this difficulty, we first examine the properties of the continuous $H^{3/2}$ space, then introduce a Dirichlet lift and Scott-Zhang type interpolation operators to bridge to the discrete $\\widehat{H}^{3/2}$ space. Our new $\\widehat{H}^{3/2}$ space is shown to be compatible with the elliptic PDE regularity theory, the trace inequality, and the inverse inequality. Notably, we extend the critical domain deformation estimate in ESFEM to the $\\widehat{H}^{3/2}$ setting. The $\\widehat{H}^{3/2}$ theory provides a foundation for establishing a PDE-based convergence analysis framework of ESFEM.","authors":["Yifei Li"],"url":"https://arxiv.org/abs/2504.11920"}
{"created":"2025-04-24","title":"Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis","abstract":"The rise of customized diffusion models has spurred a boom in personalized visual content creation, but also poses risks of malicious misuse, severely threatening personal privacy and copyright protection. Some studies show that the aesthetic properties of images are highly positively correlated with human perception of image quality. Inspired by this, we approach the problem from a novel and intriguing aesthetic perspective to degrade the generation quality of maliciously customized models, thereby achieving better protection of facial identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA) framework to fully explore aesthetic cues, which consists of two key branches: 1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward mechanism and a global anti-aesthetic loss, it can degrade the overall aesthetics of the generated content; 2) Local Anti-Aesthetics: A local anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to guide adversarial perturbations to disrupt local facial identity. By seamlessly integrating both branches, our HAA effectively achieves the goal of anti-aesthetics from a global to a local level during customized generation. Extensive experiments show that HAA outperforms existing SOTA methods largely in identity removal, providing a powerful tool for protecting facial privacy and copyright.","authors":["Songping Wang","Yueming Lyu","Shiqi Liu","Ning Li","Tong Tong","Hao Sun","Caifeng Shan"],"url":"https://arxiv.org/abs/2504.12129"}
{"created":"2025-04-24","title":"Decentralised collaborative action: cryptoeconomics in space","abstract":"Blockchains and peer-to-peer systems are part of a trend towards computer systems that are \"radically decentralised\", by which we mean that they 1) run across many participants, 2) without central control, and 3) are such that qualities 1 and 2 are essential to the system's intended use cases.","authors":["Murdoch J. Gabbay"],"url":"https://arxiv.org/abs/2504.12493"}
{"created":"2025-04-24","title":"Code Improvement Practices at Meta","abstract":"The focus on rapid software delivery inevitably results in the accumulation of technical debt, which, in turn, affects quality and slows future development. Yet, companies with a long history of rapid delivery exist. Our primary aim is to discover how such companies manage to keep their codebases maintainable. Method: we investigate Meta's practices by collaborating with engineers on code quality and by analyzing rich source code change history","authors":["Audris Mockus","Peter C Rigby","Rui Abreu","Anatoly Akkerman","Yogesh Bhootada","Payal Bhuptani","Gurnit Ghardhora","Lan Hoang Dao","Chris Hawley","Renzhi He","Sagar Krishnamoorthy","Sergei Krauze","Jianmin Li","Anton Lunov","Dragos Martac","Francois Morin","Neil Mitchell","Venus Montes","Maher Saba","Matt Steiner","Andrea Valori","Shanchao Wang","Nachiappan Nagappan"],"url":"https://arxiv.org/abs/2504.12517"}
{"created":"2025-04-24","title":"TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback","abstract":"In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks.","authors":["Siow Meng Low","Akshat Kumar"],"url":"https://arxiv.org/abs/2504.12557"}
{"created":"2025-04-24","title":"Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration","abstract":"Teaching robots dexterous manipulation skills often requires collecting hundreds of demonstrations using wearables or teleoperation, a process that is challenging to scale. Videos of human-object interactions are easier to collect and scale, but leveraging them directly for robot learning is difficult due to the lack of explicit action labels from videos and morphological differences between robot and human hands. We propose Human2Sim2Robot, a novel real-to-sim-to-real framework for training dexterous manipulation policies using only one RGB-D video of a human demonstrating a task. Our method utilizes reinforcement learning (RL) in simulation to cross the human-robot embodiment gap without relying on wearables, teleoperation, or large-scale data collection typically necessary for imitation learning methods. From the demonstration, we extract two task-specific components: (1) the object pose trajectory to define an object-centric, embodiment-agnostic reward function, and (2) the pre-manipulation hand pose to initialize and guide exploration during RL training. We found that these two components are highly effective for learning the desired task, eliminating the need for task-specific reward shaping and tuning. We demonstrate that Human2Sim2Robot outperforms object-aware open-loop trajectory replay by 55% and imitation learning with data augmentation by 68% across grasping, non-prehensile manipulation, and multi-step tasks. Project Site: https://human2sim2robot.github.io","authors":["Tyler Ga Wei Lum","Olivia Y. Lee","C. Karen Liu","Jeannette Bohg"],"url":"https://arxiv.org/abs/2504.12609"}
{"created":"2025-04-24","title":"Riemannian Patch Assignment Gradient Flows","abstract":"This paper introduces patch assignment flows for metric data labeling on graphs. Labelings are determined by regularizing initial local labelings through the dynamic interaction of both labels and label assignments across the graph, entirely encoded by a dictionary of competing labeled patches and mediated by patch assignment variables. Maximal consistency of patch assignments is achieved by geometric numerical integration of a Riemannian ascent flow, as critical point of a Lagrangian action functional. Experiments illustrate properties of the approach, including uncertainty quantification of label assignments.","authors":["Daniel Gonzalez-Alvarado","Fabio Schlindwein","Jonas Cassel","Laura Steingruber","Stefania Petra","Christoph Schn\\\"orr"],"url":"https://arxiv.org/abs/2504.13024"}
{"created":"2025-04-24","title":"Sustainability via LLM Right-sizing","abstract":"Large language models (LLMs) have become increasingly embedded in organizational workflows. This has raised concerns over their energy consumption, financial costs, and data sovereignty. While performance benchmarks often celebrate cutting-edge models, real-world deployment decisions require a broader perspective: when is a smaller, locally deployable model \"good enough\"? This study offers an empirical answer by evaluating eleven proprietary and open-weight LLMs across ten everyday occupational tasks, including summarizing texts, generating schedules, and drafting emails and proposals. Using a dual-LLM-based evaluation framework, we automated task execution and standardized evaluation across ten criteria related to output quality, factual accuracy, and ethical responsibility. Results show that GPT-4o delivers consistently superior performance but at a significantly higher cost and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4 achieved strong and reliable results on most tasks, suggesting their viability in contexts requiring cost-efficiency, local deployment, or privacy. A cluster analysis revealed three model groups -- premium all-rounders, competent generalists, and limited but safe performers -- highlighting trade-offs between quality, control, and sustainability. Significantly, task type influenced model effectiveness: conceptual tasks challenged most models, while aggregation and transformation tasks yielded better performances. We argue for a shift from performance-maximizing benchmarks to task- and context-aware sufficiency assessments that better reflect organizational priorities. Our approach contributes a scalable method to evaluate AI models through a sustainability lens and offers actionable guidance for responsible LLM deployment in practice.","authors":["Jennifer Haase","Finn Klessascheck","Jan Mendling","Sebastian Pokutta"],"url":"https://arxiv.org/abs/2504.13217"}
{"created":"2025-04-24","title":"Modelling Mean-Field Games with Neural Ordinary Differential Equations","abstract":"Mean-field game theory relies on approximating games that would otherwise have been intractable to model. While the games can be solved analytically via the associated system of partial derivatives, this approach is not model-free, can lead to the loss of the existence or uniqueness of solutions and may suffer from modelling bias. To reduce the dependency between the model and the game, we combine mean-field game theory with deep learning in the form of neural ordinary differential equations. The resulting model is data-driven, lightweight and can learn extensive strategic interactions that are hard to capture using mean-field theory alone. In addition, the model is based on automatic differentiation, making it more robust and objective than approaches based on finite differences. We highlight the efficiency and flexibility of our approach by solving three mean-field games that vary in their complexity, observability and the presence of noise. Using these results, we show that the model is flexible, lightweight and requires few observations to learn the distribution underlying the data.","authors":["Anna C. M. Th\\\"oni","Yoram Bachrach","Tal Kachman"],"url":"https://arxiv.org/abs/2504.13228"}
{"created":"2025-04-24","title":"Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization","abstract":"Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model's ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark.","authors":["Hongwei Ji","Wulian Yun","Mengshi Qi","Huadong Ma"],"url":"https://arxiv.org/abs/2504.13460"}
{"created":"2025-04-24","title":"Decoding Vision Transformers: the Diffusion Steering Lens","abstract":"Logit Lens is a widely adopted method for mechanistic interpretability of transformer-based language models, enabling the analysis of how internal representations evolve across layers by projecting them into the output vocabulary space. Although applying Logit Lens to Vision Transformers (ViTs) is technically straightforward, its direct use faces limitations in capturing the richness of visual representations. Building on the work of Toker et al. (2024)~\\cite{Toker2024-ve}, who introduced Diffusion Lens to visualize intermediate representations in the text encoders of text-to-image diffusion models, we demonstrate that while Diffusion Lens can effectively visualize residual stream representations in image encoders, it fails to capture the direct contributions of individual submodules. To overcome this limitation, we propose \\textbf{Diffusion Steering Lens} (DSL), a novel, training-free approach that steers submodule outputs and patches subsequent indirect contributions. We validate our method through interventional studies, showing that DSL provides an intuitive and reliable interpretation of the internal processing in ViTs.","authors":["Ryota Takatsuki","Sonia Joseph","Ippei Fujisawa","Ryota Kanai"],"url":"https://arxiv.org/abs/2504.13763"}
{"created":"2025-04-24","title":"Evaluating Menu OCR and Translation: A Benchmark for Aligning Human and Automated Evaluations in Large Vision-Language Models","abstract":"The rapid advancement of large vision-language models (LVLMs) has significantly propelled applications in document understanding, particularly in optical character recognition (OCR) and multilingual translation. However, current evaluations of LVLMs, like the widely used OCRBench, mainly focus on verifying the correctness of their short-text responses and long-text responses with simple layout, while the evaluation of their ability to understand long texts with complex layout design is highly significant but largely overlooked. In this paper, we propose Menu OCR and Translation Benchmark (MOTBench), a specialized evaluation framework emphasizing the pivotal role of menu translation in cross-cultural communication. MOTBench requires LVLMs to accurately recognize and translate each dish, along with its price and unit items on a menu, providing a comprehensive assessment of their visual understanding and language processing capabilities. Our benchmark is comprised of a collection of Chinese and English menus, characterized by intricate layouts, a variety of fonts, and culturally specific elements across different languages, along with precise human annotations. Experiments show that our automatic evaluation results are highly consistent with professional human evaluation. We evaluate a range of publicly available state-of-the-art LVLMs, and through analyzing their output to identify the strengths and weaknesses in their performance, offering valuable insights to guide future advancements in LVLM development. MOTBench is available at https://github.com/gitwzl/MOTBench.","authors":["Zhanglin Wu","Tengfei Song","Ning Xie","Mengli Zhu","Weidong Zhang","Shuang Wu","Pengfei Li","Chong Li","Junhao Zhu","Hao Yang","Shiliang Sun"],"url":"https://arxiv.org/abs/2504.13945"}
{"created":"2025-04-24","title":"Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations","abstract":"The advancement of AI systems for mental health support is hindered by limited access to therapeutic conversation data, particularly for trauma treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset of 3,000 therapy conversations based on Prolonged Exposure therapy protocols for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique cases, each explored through six conversational perspectives that mirror the progression of therapy from initial anxiety to peak distress to emotional processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3, 49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10 trauma-related behaviors using deterministic and probabilistic generation methods. Analysis reveals realistic distributions of trauma types (witnessing violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse 20.8%). Clinical experts validated the dataset's therapeutic fidelity, highlighting its emotional depth while suggesting refinements for greater authenticity. We also developed an emotional trajectory benchmark with standardized metrics for evaluating model responses. This privacy-preserving dataset addresses critical gaps in trauma-focused mental health data, offering a valuable resource for advancing both patient-facing applications and clinician training tools.","authors":["Suhas BN","Dominik Mattioli","Saeed Abdullah","Rosa I. Arriaga","Chris W. Wiese","Andrew M. Sherrill"],"url":"https://arxiv.org/abs/2504.13955"}
{"created":"2025-04-24","title":"Tinker Tales: Interactive Storytelling Framework for Early Childhood Narrative Development and AI Literacy","abstract":"This paper presents Tinker Tales, an interactive storytelling framework in the format of a board game, designed to support both narrative development and AI literacy in early childhood. The framework integrates tangible and speech-based interactions with AI through NFC chip-attached pawns and tokens, along with a speaker and microphone. Children select and define key story elements-such as characters, places, items, and emotions-using the pawns and tokens, providing further details to the AI and receiving proper assistance, similar to how adults prompt AI for specific tasks (e.g., writing). For evaluation, several game sessions were simulated with a child AI agent, and the quality and safety of the generated stories were assessed from various perspectives. This work highlights the potential of combining physical and digital elements in AI literacy, offering a safe and engaging way for children to learn how to effectively collaborate with AI.","authors":["Nayoung Choi","Peace Cyebukayire","Jinho D. Choi"],"url":"https://arxiv.org/abs/2504.13969"}
{"created":"2025-04-24","title":"CAOTE: KV Caching through Attention Output Error based Token Eviction","abstract":"While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.","authors":["Raghavv Goel","Junyoung Park","Mukul Gagrani","Dalton Jones","Matthew Morse","Harper Langston","Mingu Lee","Chris Lott"],"url":"https://arxiv.org/abs/2504.14051"}
{"created":"2025-04-24","title":"TALES: Text Adventure Learning Environment Suite","abstract":"Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tales.","authors":["Christopher Zhang Cui","Xingdi Yuan","Ziang Xiao","Prithviraj Ammanabrolu","Marc-Alexandre C\\^ot\\'e"],"url":"https://arxiv.org/abs/2504.14128"}
{"created":"2025-04-24","title":"Convergence Laws for Extensions of First-Order Logic with Averaging","abstract":"For many standard models of random structure, first-order logic sentences exhibit a convergence phenomenon on random inputs. The most well-known example is for random graphs with constant edge probability, where the probabilities of first-order sentences converge to 0 or 1. In other cases, such as certain ``sparse random graph'' models, the probabilities of sentences converge, although not necessarily to 0 or 1. In this work we deal with extensions of first-order logic with aggregate operators, variations of averaging. These logics will consist of real-valued terms, and we allow arbitrary Lipschitz functions to be used as ``connectives''. We show that some of the well-known convergence laws extend to this setting.","authors":["Sam Adam-Day","Michael Benedikt","Alberto Larrauri"],"url":"https://arxiv.org/abs/2504.14270"}
{"created":"2025-04-24","title":"SEGA: Drivable 3D Gaussian Head Avatar from a Single Image","abstract":"Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.","authors":["Chen Guo","Zhuo Su","Jian Wang","Shuang Li","Xu Chang","Zhaohu Li","Yang Zhao","Guidong Wang","Ruqi Huang"],"url":"https://arxiv.org/abs/2504.14373"}
{"created":"2025-04-24","title":"DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning","abstract":"In this paper, we introduce DreamID, a diffusion-based face swapping model that achieves high levels of ID similarity, attribute preservation, image fidelity, and fast inference speed. Unlike the typical face swapping training process, which often relies on implicit supervision and struggles to achieve satisfactory results. DreamID establishes explicit supervision for face swapping by constructing Triplet ID Group data, significantly enhancing identity similarity and attribute preservation. The iterative nature of diffusion models poses challenges for utilizing efficient image-space loss functions, as performing time-consuming multi-step sampling to obtain the generated image during training is impractical. To address this issue, we leverage the accelerated diffusion model SD Turbo, reducing the inference steps to a single iteration, enabling efficient pixel-level end-to-end training with explicit Triplet ID Group supervision. Additionally, we propose an improved diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter. This robust architecture fully unlocks the power of the Triplet ID Group explicit supervision. Finally, to further extend our method, we explicitly modify the Triplet ID Group data during training to fine-tune and preserve specific attributes, such as glasses and face shape. Extensive experiments demonstrate that DreamID outperforms state-of-the-art methods in terms of identity similarity, pose and expression preservation, and image fidelity. Overall, DreamID achieves high-quality face swapping results at 512*512 resolution in just 0.6 seconds and performs exceptionally well in challenging scenarios such as complex lighting, large angles, and occlusions.","authors":["Fulong Ye","Miao Hua","Pengze Zhang","Xinghui Li","Qichao Sun","Songtao Zhao","Qian He","Xinglong Wu"],"url":"https://arxiv.org/abs/2504.14509"}
{"created":"2025-04-24","title":"A Complete and Bounded-Suboptimal Algorithm for a Moving Target Traveling Salesman Problem with Obstacles in 3D","abstract":"The moving target traveling salesman problem with obstacles (MT-TSP-O) seeks an obstacle-free trajectory for an agent that intercepts a given set of moving targets, each within specified time windows, and returns to the agent's starting position. Each target moves with a constant velocity within its time windows, and the agent has a speed limit no smaller than any target's speed. We present FMC*-TSP, the first complete and bounded-suboptimal algorithm for the MT-TSP-O, and results for an agent whose configuration space is $\\mathbb{R}^3$. Our algorithm interleaves a high-level search and a low-level search, where the high-level search solves a generalized traveling salesman problem with time windows (GTSP-TW) to find a sequence of targets and corresponding time windows for the agent to visit. Given such a sequence, the low-level search then finds an associated agent trajectory. To solve the low-level planning problem, we develop a new algorithm called FMC*, which finds a shortest path on a graph of convex sets (GCS) via implicit graph search and pruning techniques specialized for problems with moving targets. We test FMC*-TSP on 280 problem instances with up to 40 targets and demonstrate its smaller median runtime than a baseline based on prior work.","authors":["Anoop Bhat","Geordan Gutow","Bhaskar Vundurthy","Zhongqiang Ren","Sivakumar Rathinam","Howie Choset"],"url":"https://arxiv.org/abs/2504.14680"}
{"created":"2025-04-24","title":"vApps: Verifiable Applications at Internet Scale","abstract":"Blockchain technology promises a decentralized, trustless, and interoperable infrastructure. However, widespread adoption remains hindered by issues such as limited scalability, high transaction costs, and the complexity of maintaining coherent verification logic across different blockchain layers. This paper introduces Verifiable Applications (vApps), a novel development framework designed to streamline the creation and deployment of verifiable blockchain computing applications. vApps offer a unified Rust-based Domain-Specific Language (DSL) within a comprehensive SDK, featuring modular abstractions for verification, proof generation, and inter-chain connectivity. This eases the developer's burden in securing diverse software components, allowing them to focus on application logic. The DSL also ensures that applications can automatically take advantage of specialized precompiles and hardware acceleration to achieve consistently high performance with minimal developer effort, as demonstrated by benchmark results for zero-knowledge virtual machines (zkVMs). Experiments show that native Rust execution eliminates interpretation overhead, delivering up to an 832x cycle count improvement compared to EVM-based approaches. Precompiled circuits can accelerate the proof by more than 95\\%, while GPU acceleration increases throughput by up to 30x and recursion compresses the proof size by up to 230x, enabling succinct and efficient verification. The framework also supports seamless integration with the Web2 and Web3 systems, enabling developers to focus solely on their application logic. Through modular architecture, robust security guarantees, and composability, vApps pave the way toward a trust-minimized and verifiable Internet-scale application environment.","authors":["Isaac Zhang","Kshitij Kulkarni","Tan Li","Daniel Wong","Thomas Kim","John Guibas","Uma Roy","Bryan Pellegrino","Ryan Zarick"],"url":"https://arxiv.org/abs/2504.14809"}
{"created":"2025-04-24","title":"A Short Proof of Coding Theorems for Reed-Muller Codes Under a Mild Assumption","abstract":"In this paper, by treating Reed-Muller (RM) codes as a special class of low-density parity-check (LDPC) codes and assuming that sub-blocks of the parity-check matrix are randomly interleaved to each other as Gallager's codes, we present a short proof that RM codes are entropy-achieving as source coding for Bernoulli sources and capacity-achieving as channel coding for binary memoryless symmetric (BMS) channels, also known as memoryless binary-input output-symmetric (BIOS) channels, in terms of bit error rate (BER) under maximum-likelihood (ML) decoding.","authors":["Xiao Ma"],"url":"https://arxiv.org/abs/2504.14842"}
{"created":"2025-04-24","title":"Physics-Aware Compression of Plasma Distribution Functions with GPU-Accelerated Gaussian Mixture Models","abstract":"Data compression is a critical technology for large-scale plasma simulations. Storing complete particle information requires Terabyte-scale data storage, and analysis requires ad-hoc scalable post-processing tools. We propose a physics-aware in-situ compression method using Gaussian Mixture Models (GMMs) to approximate electron and ion velocity distribution functions with a number of Gaussian components. This GMM-based method allows us to capture plasma features such as mean velocity and temperature, and it enables us to identify heating processes and generate beams. We first construct a histogram to reduce computational overhead and apply GPU-accelerated, in-situ GMM fitting within iPIC3D, a large-scale implicit Particle-in-Cell simulator, ensuring real-time compression. The compressed representation is stored using the ADIOS 2 library, thus optimizing the I/O process. The GPU and histogramming implementation provides a significant speed-up with respect to GMM on particles (both in time and required memory at run-time), enabling real-time compression. Compared to algorithms like SZ, MGARD, and BLOSC2, our GMM-based method has a physics-based approach, retaining the physical interpretation of plasma phenomena such as beam formation, acceleration, and heating mechanisms. Our GMM algorithm achieves a compression ratio of up to $10^4$, requiring a processing time comparable to, or even lower than, standard compression engines.","authors":["Andong Hu","Luca Pennati","Ivy Peng","Stefano Markidis"],"url":"https://arxiv.org/abs/2504.14897"}
{"created":"2025-04-24","title":"Fast Adversarial Training with Weak-to-Strong Spatial-Temporal Consistency in the Frequency Domain on Videos","abstract":"Adversarial Training (AT) has been shown to significantly enhance adversarial robustness via a min-max optimization approach. However, its effectiveness in video recognition tasks is hampered by two main challenges. First, fast adversarial training for video models remains largely unexplored, which severely impedes its practical applications. Specifically, most video adversarial training methods are computationally costly, with long training times and high expenses. Second, existing methods struggle with the trade-off between clean accuracy and adversarial robustness. To address these challenges, we introduce Video Fast Adversarial Training with Weak-to-Strong consistency (VFAT-WS), the first fast adversarial training method for video data. Specifically, VFAT-WS incorporates the following key designs: First, it integrates a straightforward yet effective temporal frequency augmentation (TF-AUG), and its spatial-temporal enhanced form STF-AUG, along with a single-step PGD attack to boost training efficiency and robustness. Second, it devises a weak-to-strong spatial-temporal consistency regularization, which seamlessly integrates the simpler TF-AUG and the more complex STF-AUG. Leveraging the consistency regularization, it steers the learning process from simple to complex augmentations. Both of them work together to achieve a better trade-off between clean accuracy and robustness. Extensive experiments on UCF-101 and HMDB-51 with both CNN and Transformer-based models demonstrate that VFAT-WS achieves great improvements in adversarial robustness and corruption robustness, while accelerating training by nearly 490%.","authors":["Songping Wang","Hanqing Liu","Yueming Lyu","Xiantao Hu","Ziwen He","Wei Wang","Caifeng Shan","Liang Wang"],"url":"https://arxiv.org/abs/2504.14921"}
{"created":"2025-04-24","title":"MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core","abstract":"Mixture of Experts (MoE) models enhance neural network scalability by dynamically selecting relevant experts per input token, enabling larger model sizes while maintaining manageable computation costs. However, efficient training of large-scale MoE models across thousands of GPUs presents significant challenges due to limitations in existing parallelism strategies. We introduce an end-to-end training framework for large-scale MoE models that utilizes five-dimensional hybrid parallelism: Tensor Parallelism, Expert Parallelism, Context Parallelism, Data Parallelism, and Pipeline Parallelism. Central to our approach is MoE Parallel Folding, a novel strategy that decouples the parallelization of attention and MoE layers in Transformer models, allowing each layer type to adopt optimal parallel configurations. Additionally, we develop a flexible token-level dispatcher that supports both token-dropping and token-dropless MoE training across all five dimensions of parallelism. This dispatcher accommodates dynamic tensor shapes and coordinates different parallelism schemes for Attention and MoE layers, facilitating complex parallelism implementations. Our experiments demonstrate significant improvements in training efficiency and scalability. We achieve up to 49.3% Model Flops Utilization (MFU) for the Mixtral 8x22B model and 39.0% MFU for the Qwen2-57B-A14B model on H100 GPUs, outperforming existing methods. The framework scales efficiently up to 1,024 GPUs and maintains high performance with sequence lengths up to 128K tokens, validating its effectiveness for large-scale MoE model training. The code is available in Megatron-Core.","authors":["Dennis Liu","Zijie Yan","Xin Yao","Tong Liu","Vijay Korthikanti","Evan Wu","Shiqing Fan","Gao Deng","Hongxiao Bai","Jianbin Chang","Ashwath Aithal","Michael Andersch","Mohammad Shoeybi","Jiajie Yao","Chandler Zhou","David Wu","Xipeng Li","June Yang"],"url":"https://arxiv.org/abs/2504.14960"}
{"created":"2025-04-24","title":"aiXamine: Simplified LLM Safety and Security","abstract":"Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats. To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security. aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment. The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations. We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations. Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0. Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness. Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices.","authors":["Fatih Deniz","Dorde Popovic","Yazan Boshmaf","Euisuh Jeong","Minhaj Ahmad","Sanjay Chawla","Issa Khalil"],"url":"https://arxiv.org/abs/2504.14985"}
{"created":"2025-04-24","title":"Tightening Quadratic Convex Relaxations for the AC Optimal Transmission Switching Problem","abstract":"The Alternating Current Optimal Transmission Switching (ACOTS) problem incorporates line switching decisions into the AC Optimal Power Flow (ACOPF) framework, offering well-known benefits in reducing operational costs and enhancing system reliability. ACOTS optimization models contain discrete variables and nonlinear, non-convex constraints, which make it difficult to solve. In this work, we develop strengthened quadratic convex (QC) relaxations for ACOTS, where we tighten the relaxation with several new valid inequalities, including a novel kind of on/off cycle-based polynomial constraints by taking advantage of the network structure. We linearize the sum of on/off trilinear terms in the relaxation using extreme-point representation, demonstrating theoretical tightness, and efficiently incorporate on/off cycle-based polynomial constraints through disjunctive programming-based cutting planes. Combined with an optimization-based bound tightening algorithm, this results in the tightest QC-based ACOTS relaxation to date. We additionally propose a novel maximum spanning tree-based heuristic to improve the computational performance by fixing certain lines to be switched on. Our extensive numerical experiments on medium-scale PGLib instances show significant improvements on relaxation bounds, while tests on large-scale instances with up to 2,312 buses demonstrate substantial performance gains. To our knowledge, this is the first ACOTS relaxation-based approach to demonstrate near-optimal switching solutions on realistic large-scale power grid instances.","authors":["Cheng Guo","Harsha Nagarajan","Merve Bodur"],"url":"https://arxiv.org/abs/2212.12097"}
{"created":"2025-04-24","title":"Deep Anatomical Federated Network (Dafne): An open client-server framework for the continuous, collaborative improvement of deep learning-based medical image segmentation","abstract":"Purpose: To present and evaluate Dafne (deep anatomical federated network), a freely available decentralized, collaborative deep learning system for the semantic segmentation of radiological images through federated incremental learning. Materials and Methods: Dafne is free software with a client-server architecture. The client side is an advanced user interface that applies the deep learning models stored on the server to the user's data and allows the user to check and refine the prediction. Incremental learning is then performed at the client's side and sent back to the server, where it is integrated into the root model. Dafne was evaluated locally, by assessing the performance gain across model generations on 38 MRI datasets of the lower legs, and through the analysis of real-world usage statistics (n = 639 use-cases). Results: Dafne demonstrated a statistically improvement in the accuracy of semantic segmentation over time (average increase of the Dice Similarity Coefficient by 0.007 points/generation on the local validation set, p < 0.001). Qualitatively, the models showed enhanced performance on various radiologic image types, including those not present in the initial training sets, indicating good model generalizability. Conclusion: Dafne showed improvement in segmentation quality over time, demonstrating potential for learning and generalization.","authors":["Francesco Santini","Jakob Wasserthal","Abramo Agosti","Xeni Deligianni","Kevin R. Keene","Hermien E. Kan","Stefan Sommer","Fengdan Wang","Claudia Weidensteiner","Giulia Manco","Matteo Paoletti","Valentina Mazzoli","Arjun Desai","Anna Pichiecchio"],"url":"https://arxiv.org/abs/2302.06352"}
{"created":"2025-04-24","title":"Optimal Noise Reduction in Dense Mixed-Membership Stochastic Block Models under Diverging Spiked Eigenvalues Condition","abstract":"Community detection is one of the most critical problems in modern network science. Its applications can be found in various fields, from protein modeling to social network analysis. Recently, many papers appeared studying the problem of overlapping community detection, where each node of a network may belong to several communities. In this work, we consider Mixed-Membership Stochastic Block Model (MMSB) first proposed by Airoldi et al. MMSB provides quite a general setting for modeling overlapping community structure in graphs. The central question of this paper is to reconstruct relations between communities given an observed network. We compare different approaches and establish the minimax lower bound on the estimation error. Then, we propose a new estimator that matches this lower bound. Theoretical results are proved under fairly general conditions on the considered model. Finally, we illustrate the theory in a series of experiments.","authors":["Fedor Noskov","Maxim Panov"],"url":"https://arxiv.org/abs/2307.14530"}
{"created":"2025-04-24","title":"Approximate solution of stochastic infinite horizon optimal control problems for constrained linear uncertain systems","abstract":"We propose a Model Predictive Control (MPC) with a single-step prediction horizon to approximate the solution of infinite horizon optimal control problems with the expected sum of convex stage costs for constrained linear uncertain systems. The proposed method aims to enhance a given sub-optimal controller, leveraging data to achieve a nearly optimal solution for the infinite horizon problem. The method is built on two techniques. First, we estimate the expected values of the convex costs using a computationally tractable approximation, achieved by sampling across the space of disturbances. Second, we implement a data-driven approach to approximate the optimal value function and its corresponding domain, through systematic exploration of the system's state space. These estimates are subsequently used to calculate the terminal cost and terminal set within the proposed MPC. We prove recursive feasibility, robust constraint satisfaction, and convergence in probability to the target set. Furthermore, we prove that the estimated value function converges to the optimal value function in a local region. The effectiveness of the proposed MPC is illustrated with detailed numerical simulations and comparisons with a value iteration method and a Learning MPC that minimizes a certainty equivalent cost.","authors":["Eunhyek Joa","Francesco Borrelli"],"url":"https://arxiv.org/abs/2401.12556"}
{"created":"2025-04-24","title":"Neural Risk Limiting Dispatch in Power Networks: Formulation and Generalization Guarantees","abstract":"Risk limiting dispatch (RLD) has been proposed as an approach that effectively trades off economic costs with operational risks for power dispatch under uncertainty. However, how to solve the RLD problem with provably near-optimal performance still remains an open problem. This paper presents a learning-based solution to this challenge. We first design a data-driven formulation for the RLD problem, which aims to construct a decision rule that directly maps day-ahead observable information to cost-effective dispatch decisions for the future delivery interval. Unlike most existing works that follow a predict-then-optimize paradigm, this end-to-end rule bypasses the additional suboptimality introduced by separately handling prediction and optimization. We then propose neural RLD, a novel solution method to the data-driven formulation. This method leverages an L2-regularized neural network to learn the decision rule, thereby transforming the data-driven formulation into a neural network training task that can be efficiently completed by stochastic gradient descent. A theoretical performance guarantee is further established to bound the suboptimality of our method, which implies that its suboptimality approaches zero with high probability as more samples are utilized. Simulation tests across various systems demonstrate our method's superior performance in convergence, suboptimality, and computational efficiency compared with benchmarks.","authors":["Ge Chen","Junjie Qin"],"url":"https://arxiv.org/abs/2402.00772"}
{"created":"2025-04-24","title":"Deep Learning for Low-Latency, Quantum-Ready RF Sensing","abstract":"Recent work has shown the promise of applying deep learning to enhance software processing of radio frequency (RF) signals. In parallel, hardware developments with quantum RF sensors based on Rydberg atoms are breaking longstanding barriers in frequency range, resolution, and sensitivity. In this paper, we describe our implementations of quantum-ready machine learning approaches for RF signal classification. Our primary objective is latency: while deep learning offers a more powerful computational paradigm, it also traditionally incurs latency overheads that hinder wider scale deployment. Our work spans three axes. (1) A novel continuous wavelet transform (CWT) based recurrent neural network (RNN) architecture that enables flexible online classification of RF signals on-the-fly with reduced sampling time. (2) Low-latency inference techniques for both GPU and CPU that span over 100x reductions in inference time, enabling real-time operation with sub-millisecond inference. (3) Quantum-readiness validated through application of our models to physics-based simulation of Rydberg atom QRF sensors. Altogether, our work bridges towards next-generation RF sensors that use quantum technology to surpass previous physical limits, paired with latency-optimized AI/ML software that is suitable for real-time deployment.","authors":["Pranav Gokhale","Caitlin Carnahan","William Clark","Teague Tomesh","Frederic T. Chong"],"url":"https://arxiv.org/abs/2404.17962"}
{"created":"2025-04-24","title":"Certifying solutions of degenerate semidefinite programs","abstract":"This paper deals with the algorithmic aspects of solving feasibility problems of semidefinite programming (SDP), aka linear matrix inequalities (LMI). Since in some SDP instances all feasible solutions have irrational entries, numerical solvers that work with rational numbers can only find an approximate solution. We study the following question: is it possible to certify feasibility of a given SDP using an approximate solution that is sufficiently close to some exact solution? Existing approaches make the assumption that there exist rational feasible solutions (and use techniques such as rounding and lattice reduction algorithms). We propose an alternative approach that does not need this assumption. More specifically, we show how to construct a system of polynomial equations whose set of real solutions is guaranteed to have an isolated correct solution (assuming that the target exact solution is maximum-rank). This allows, in particular, to use algorithms from real algebraic geometry for solving systems of polynomial equations, yielding a hybrid (or symbolic-numerical) method for SDPs. We experimentally compare it with a pure symbolic method; the hybrid method was able to certify feasibility of many SDP instances on which the exact method failed. Our approach may have further applications, such as refining an approximate solution using methods of numerical algebraic geometry for systems of polynomial equations.","authors":["Vladimir Kolmogorov","Simone Naldi","Jeferson Zapata"],"url":"https://arxiv.org/abs/2405.13625"}
{"created":"2025-04-24","title":"Convergence Analysis for Entropy-Regularized Control Problems: A Probabilistic Approach","abstract":"In this paper we investigate the convergence of the Policy Iteration Algorithm (PIA) for a class of general continuous-time entropy-regularized stochastic control problems. In particular, instead of employing sophisticated PDE estimates for the iterative PDEs involved in the algorithm (see, e.g., Huang-Wang-Zhou(2025)), we shall provide a simple proof from scratch for the convergence of the PIA. Our approach builds on probabilistic representation formulae for solutions of PDEs and their derivatives. Moreover, in the finite horizon model and in the infinite horizon model with large discount factor, the similar arguments lead to a super-exponential rate of convergence without tear. Finally, with some extra efforts we show that our approach can be extended to the diffusion control case in the one dimensional setting, also with a super-exponential rate of convergence.","authors":["Jin Ma","Gaozhan Wang","Jianfeng Zhang"],"url":"https://arxiv.org/abs/2406.10959"}
{"created":"2025-04-24","title":"A Deep Learning System for Rapid and Accurate Warning of Acute Aortic Syndrome on Non-contrast CT in China","abstract":"The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients presenting with acute chest pain remains a clinical challenge. Aortic CT angiography (CTA) is the imaging protocol of choice in patients with suspected AAS. However, due to economic and workflow constraints in China, the majority of suspected patients initially undergo non-contrast CT as the initial imaging testing, and CTA is reserved for those at higher risk. In this work, we present an artificial intelligence-based warning system, iAorta, using non-contrast CT for AAS identification in China, which demonstrates remarkably high accuracy and provides clinicians with interpretable warnings. iAorta was evaluated through a comprehensive step-wise study. In the multi-center retrospective study (n = 20,750), iAorta achieved a mean area under the receiver operating curve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study (n = 137,525), iAorta demonstrated consistently high performance across various non-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a specificity of 0.991-0.993. In the prospective comparative study (n = 13,846), iAorta demonstrated the capability to significantly shorten the time to correct diagnostic pathway. For the prospective pilot deployment that we conducted, iAorta correctly identified 21 out of 22 patients with AAS among 15,584 consecutive patients presenting with acute chest pain and under non-contrast CT protocol in the emergency department (ED) and enabled the average diagnostic time of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the iAorta can help avoid delayed or missed diagnosis of AAS in settings where non-contrast CT remains the unavoidable the initial or only imaging test in resource-constrained regions and in patients who cannot or did not receive intravenous contrast.","authors":["Yujian Hu","Yilang Xiang","Yan-Jie Zhou","Yangyan He","Dehai Lang","Shifeng Yang","Xiaolong Du","Chunlan Den","Youyao Xu","Gaofeng Wang","Zhengyao Ding","Jingyong Huang","Wenjun Zhao","Xuejun Wu","Donglin Li","Qianqian Zhu","Zhenjiang Li","Chenyang Qiu","Ziheng Wu","Yunjun He","Chen Tian","Yihui Qiu","Zuodong Lin","Xiaolong Zhang","Yuan He","Zhenpeng Yuan","Xiaoxiang Zhou","Rong Fan","Ruihan Chen","Wenchao Guo","Jianpeng Zhang","Tony C. W. Mok","Zi Li","Mannudeep K. Kalra","Le Lu","Wenbo Xiao","Xiaoqiang Li","Yun Bian","Chengwei Shao","Guofu Wang","Wei Lu","Zhengxing Huang","Minfeng Xu","Hongkun Zhang"],"url":"https://arxiv.org/abs/2406.15222"}
{"created":"2025-04-24","title":"Quantum computational complexity of matrix functions","abstract":"We investigate the dividing line between classical and quantum computational power in estimating properties of matrix functions. More precisely, we study the computational complexity of two primitive problems: given a function $f$ and a Hermitian matrix $A$, compute a matrix element of $f(A)$ or compute a local measurement on $f(A)|0\\rangle^{\\otimes n}$, with $|0\\rangle^{\\otimes n}$ an $n$-qubit reference state vector, in both cases up to additive approximation error. We consider four functions -- monomials, Chebyshev polynomials, the time evolution function, and the inverse function -- and probe the complexity across a broad landscape covering different problem input regimes. Namely, we consider two types of matrix inputs (sparse and Pauli access), matrix properties (norm, sparsity), the approximation error, and function-specific parameters. We identify BQP-complete forms of both problems for each function and then toggle the problem parameters to easier regimes to see where hardness remains, or where the problem becomes classically easy. As part of our results, we make concrete a hierarchy of hardness across the functions; in parameter regimes where we have classically efficient algorithms for monomials, all three other functions remain robustly BQP-hard, or hard under usual computational complexity assumptions. In identifying classically easy regimes, among others, we show that for any polynomial of degree $\\mathrm{poly}(n)$ both problems can be efficiently classically simulated when $A$ has $O(\\log n)$ non-zero coefficients in the Pauli basis. This contrasts with the fact that the problems are BQP-complete in the sparse access model even for constant row sparsity, whereas the stated Pauli access efficiently constructs sparse access with row sparsity $O(\\log n)$. Our work provides a catalog of efficient quantum and classical algorithms for fundamental linear-algebra tasks.","authors":["Santiago Cifuentes","Samson Wang","Thais L. Silva","Mario Berta","Leandro Aolita"],"url":"https://arxiv.org/abs/2410.13937"}
{"created":"2025-04-24","title":"A Passivity Analysis for Nonlinear Consensus on Balanced Digraphs","abstract":"This work deals with the output consensus problem for multiagent systems over balanced digraphs by passivity analysis. As the standard diffusive coupling structure only models the undirected interconnection, we propose a general approach capable of processing directed coupling and performing passivity analysis. To mitigate the complexity arising from the nonlinearity and directed interconnections, we reformulate the output consensus problem as a convergence analysis on a submanifold. We provide passivity analysis and establish a sufficient condition based on passivity for achieving output agreement in multi-agent systems over balanced digraphs. The results are supported by a numerical example.","authors":["Feng-Yu Yue","Daniel Zelazo"],"url":"https://arxiv.org/abs/2411.05933"}
{"created":"2025-04-24","title":"A Novel Adaptive Hybrid Focal-Entropy Loss for Enhancing Diabetic Retinopathy Detection Using Convolutional Neural Networks","abstract":"Diabetic retinopathy is a leading cause of blindness around the world and demands precise AI-based diagnostic tools. Traditional loss functions in multi-class classification, such as Categorical Cross-Entropy (CCE), are very common but break down with class imbalance, especially in cases with inherently challenging or overlapping classes, which leads to biased and less sensitive models. Since a heavy imbalance exists in the number of examples for higher severity stage 4 diabetic retinopathy, etc., classes compared to those very early stages like class 0, achieving class balance is key. For this purpose, we propose the Adaptive Hybrid Focal-Entropy Loss which combines the ideas of focal loss and entropy loss with adaptive weighting in order to focus on minority classes and highlight the challenging samples. The state-of-the art models applied for diabetic retinopathy detection with AHFE revealed good performance improvements, indicating the top performances of ResNet50 at 99.79%, DenseNet121 at 98.86%, Xception at 98.92%, MobileNetV2 at 97.84%, and InceptionV3 at 93.62% accuracy. This sheds light into how AHFE promotes enhancement in AI-driven diagnostics for complex and imbalanced medical datasets.","authors":["Santhosh Malarvannan","Pandiyaraju V","Shravan Venkatraman","Abeshek A","Priyadarshini B","Kannan A"],"url":"https://arxiv.org/abs/2411.10843"}
{"created":"2025-04-24","title":"Program Evaluation with Remotely Sensed Outcomes","abstract":"Economists often estimate treatment effects in experiments using remotely sensed variables (RSVs), e.g. satellite images or mobile phone activity, in place of directly measured economic outcomes. A common practice is to use an observational sample to train a predictor of the economic outcome from the RSV, and then to use its predictions as the outcomes in the experiment. We show that this method is biased whenever the RSV is post-outcome, i.e. if variation in the economic outcome causes variation in the RSV. In program evaluation, changes in poverty or environmental quality cause changes in satellite images, but not vice versa. As our main result, we nonparametrically identify the treatment effect by formalizing the intuition that underlies common practice: the conditional distribution of the RSV given the outcome and treatment is stable across the samples.Based on our identifying formula, we find that the efficient representation of RSVs for causal inference requires three predictions rather than one. Valid inference does not require any rate conditions on RSV predictions, justifying the use of complex deep learning algorithms with unknown statistical properties. We re-analyze the effect of an anti-poverty program in India using satellite images.","authors":["Ashesh Rambachan","Rahul Singh","Davide Viviano"],"url":"https://arxiv.org/abs/2411.10959"}
{"created":"2025-04-24","title":"Robust multi-coil MRI reconstruction via self-supervised denoising","abstract":"We study the effect of incorporating self-supervised denoising as a pre-processing step for training deep learning (DL) based reconstruction methods on data corrupted by Gaussian noise. K-space data employed for training are typically multi-coil and inherently noisy. Although DL-based reconstruction methods trained on fully sampled data can enable high reconstruction quality, obtaining large, noise-free datasets is impractical. We leverage Generalized Stein's Unbiased Risk Estimate (GSURE) for denoising. We evaluate two DL-based reconstruction methods: Diffusion Probabilistic Models (DPMs) and Model-Based Deep Learning (MoDL). We evaluate the impact of denoising on the performance of these DL-based methods in solving accelerated multi-coil magnetic resonance imaging (MRI) reconstruction. The experiments were carried out on T2-weighted brain and fat-suppressed proton-density knee scans. We observed that self-supervised denoising enhances the quality and efficiency of MRI reconstructions across various scenarios. Specifically, employing denoised images rather than noisy counterparts when training DL networks results in lower normalized root mean squared error (NRMSE), higher structural similarity index measure (SSIM) and peak signal-to-noise ratio (PSNR) across different SNR levels, including 32dB, 22dB, and 12dB for T2-weighted brain data, and 24dB, 14dB, and 4dB for fat-suppressed knee data. Overall, we showed that denoising is an essential pre-processing technique capable of improving the efficacy of DL-based MRI reconstruction methods under diverse conditions. By refining the quality of input data, denoising enables training more effective DL networks, potentially bypassing the need for noise-free reference MRI scans.","authors":["Asad Aali","Marius Arvinte","Sidharth Kumar","Yamin I. Arefeen","Jonathan I. Tamir"],"url":"https://arxiv.org/abs/2411.12919"}
{"created":"2025-04-24","title":"StarWhisper Telescope: Agent-Based Observation Assistant System to Approach AI Astrophysicist","abstract":"With the rapid advancements in Large Language Models (LLMs), LLM-based agents have introduced convenient and user-friendly methods for leveraging tools across various domains. In the field of astronomical observation, the construction of new telescopes has significantly increased astronomers' workload. Deploying LLM-powered agents can effectively alleviate this burden and reduce the costs associated with training personnel. Within the Nearby Galaxy Supernovae Survey (NGSS) project, which encompasses eight telescopes across three observation sites, aiming to find the transients from the galaxies in 50 mpc, we have developed the \\textbf{StarWhisper Telescope System} to manage the entire observation process. This system automates tasks such as generating observation lists, conducting observations, analyzing data, and providing feedback to the observer. Observation lists are customized for different sites and strategies to ensure comprehensive coverage of celestial objects. After manual verification, these lists are uploaded to the telescopes via the agents in the system, which initiates observations upon neutral language. The observed images are analyzed in real-time, and the transients are promptly communicated to the observer. The agent modifies them into a real-time follow-up observation proposal and send to the Xinglong observatory group chat, then add them to the next-day observation lists. Additionally, the integration of AI agents within the system provides online accessibility, saving astronomers' time and encouraging greater participation from amateur astronomers in the NGSS project.","authors":["Cunshi Wang","Xinjie Hu","Yu Zhang","Xunhao Chen","Pengliang Du","Yiming Mao","Rui Wang","Yuyang Li","Ying Wu","Hang Yang","Yansong Li","Beichuan Wang","Haiyang Mu","Zheng Wang","Jianfeng Tian","Liang Ge","Yongna Mao","Shengming Li","Xiaomeng Lu","Jinhang Zou","Yang Huang","Ningchen Sun","Jie Zheng","Min He","Yu Bai","Junjie Jin","Hong Wu","Jifeng Liu"],"url":"https://arxiv.org/abs/2412.06412"}
{"created":"2025-04-24","title":"A Cardinality-Constrained Approach to Combinatorial Bilevel Congestion Pricing","abstract":"Combinatorial bilevel congestion pricing (CBCP), a variant of the mixed (continuous/discrete) network design problems, seeks to minimize the total travel time experienced by all travelers in a road network, by strategically selecting toll locations and determining toll charges. Conventional wisdom suggests that these problems are intractable since they have to be formulated and solved with a significant number of integer variables. Here, we devise a scalable local algorithm for the CBCP problem that guarantees convergence to an approximate Karush-Kuhn-Tucker point. Our approach is novel in that it eliminates the use of integer variables altogether, instead introducing a cardinality constraint that limits the number of toll locations to a user-specified upper bound. The resulting bilevel program with the cardinality constraint is then transformed into a block-separable, single-level optimization problem that can be solved efficiently after penalization and decomposition. We are able to apply the algorithm to solve, in about 20 minutes, a CBCP instance with up to 3,000 links. To the best of our knowledge, no existing algorithm can solve CBCP problems at such a scale while providing any assurance of convergence.","authors":["Lei Guo","Jiayang Li","Yu Marco Nie","Jun Xie"],"url":"https://arxiv.org/abs/2412.06482"}
{"created":"2025-04-24","title":"Indirect Adaptive Control Using a Static Update Law","abstract":"The update law in the indirect adaptive control scheme can be extended to include feedthrough of an error term. This reduces undesired oscillations of the calculated weights. When the ${\\sigma}$-modification is used for achieving robustness against unstructured uncertainties, the gain of the feedthrough in the update law cannot be chosen arbitrarily. Compared to our previous result, we show stability of the closed loop for a larger parameter-range for the gain of the feedthrough in the update law. This parameter-range includes a configuration for which the influence of the integration in the update law diminishes over time, i.e. for which the adaptation for large times is governed solely by the feedthrough in the update law. By initializing at zero, this allows for removing the integration from the update law, resulting in a static update law. For the purely linear case, the adaptation acts like a disturbance observer. Frequency-domain analysis of the closed loop with a second order plant shows that removing the integration from the update law with ${\\sigma}$-modification and feedthrough affects how precisely disturbances in the low-frequency band are observed. If the damping injected into the adaptation process by the ${\\sigma}$-modification exceeds certain bounds, then the precision is increased by using the static update law.","authors":["Tom Kaufmann","Johann Reger"],"url":"https://arxiv.org/abs/2412.10102"}
{"created":"2025-04-24","title":"Dual NUP Representations and Min-Maximization in Factor Graphs","abstract":"Normals with unknown parameters (NUP) can be used to convert nontrivial model-based estimation problems into iterations of linear least-squares or Gaussian estimation problems. In this paper, we extend this approach by augmenting factor graphs with convex-dual variables and pertinent NUP representations. In particular, in a state space setting, we propose a new iterative forward-backward algorithm that is dual to a recently proposed backward-forward algorithm.","authors":["Yun-Peng Li","Hans-Andrea Loeliger"],"url":"https://arxiv.org/abs/2501.12113"}
{"created":"2025-04-24","title":"Rethinking Timing Residuals: Advancing PET Detectors with Explicit TOF Corrections","abstract":"PET is a functional imaging method that visualizes metabolic processes. TOF information can be derived from coincident detector signals and incorporated into image reconstruction to enhance the SNR. PET detectors are typically assessed by their CTR, but timing performance is degraded by various factors. Research on timing calibration seeks to mitigate these degradations and restore accurate timing information. While many calibration methods use analytical approaches, machine learning techniques have recently gained attention due to their flexibility. We developed a residual physics-based calibration approach that combines prior domain knowledge with the power of machine learning models. This approach begins with an initial analytical calibration addressing first-order skews. The remaining deviations, regarded as residual effects, are used to train machine learning models to eliminate higher-order skews. The key advantage is that the experimenter guides the learning process through the definition of timing residuals. In earlier studies, we developed models that directly predicted the expected time difference, which offered corrections only implicitly (implicit correction models). In this study, we introduce a new definition for timing residuals, enabling us to train models that directly predict correction values (explicit correction models). The explicit correction approach significantly simplifies data acquisition, improves linearity, and enhances timing performance from $371 \\pm 6$ ps to $281 \\pm 5$ ps for coincidences from 430 keV to 590 keV. Additionally, the new definition reduces model size, making it suitable for high-throughput applications like PET scanners. Experiments were conducted using two detector stacks composed of $4 \\times 4$ LYSO:Ce,Ca crystals ($3.8\\times 3.8\\times 20$ mm$^{3}$) coupled to $4 \\times 4$ Broadcom NUV-MT SiPMs and digitized with the TOFPET2 ASIC.","authors":["Stephan Naunheim","Luis Lopes de Paiva","Vanessa Nadig","Yannick Kuhl","Stefan Gundacker","Florian Mueller","Volkmar Schulz"],"url":"https://arxiv.org/abs/2502.07630"}
{"created":"2025-04-24","title":"Constructing optimal Wannier functions via potential theory: isolated single band for matrix models","abstract":"We present a rapidly convergent scheme for computing globally optimal Wannier functions of isolated single bands for matrix models in two dimensions. The scheme proceeds first by constructing provably exponentially localized Wannier functions directly from parallel transport (with simple analytically computable corrections) when topological obstructions are absent. We prove that the corresponding Wannier functions are real when the matrix model possesses time-reversal symmetry. When a band has a nonzero Berry curvature, the resulting Wannier function is not optimal, but it is transformed into the global optimum by a single gauge transformation that eliminates the divergence of the Berry connection. Complete analysis of the construction is presented, paving the way for further improvements and generalizations. The performance of the scheme is illustrated with several numerical examples.","authors":["Hanwen Zhang"],"url":"https://arxiv.org/abs/2502.08641"}
{"created":"2025-04-24","title":"Learning Smooth and Expressive Interatomic Potentials for Physical Property Prediction","abstract":"Machine learning interatomic potentials (MLIPs) have become increasingly effective at approximating quantum mechanical calculations at a fraction of the computational cost. However, lower errors on held out test sets do not always translate to improved results on downstream physical property prediction tasks. In this paper, we propose testing MLIPs on their practical ability to conserve energy during molecular dynamic simulations. If passed, improved correlations are found between test errors and their performance on physical property prediction tasks. We identify choices which may lead to models failing this test, and use these observations to improve upon highly-expressive models. The resulting model, eSEN, provides state-of-the-art results on a range of physical property prediction tasks, including materials stability prediction, thermal conductivity prediction, and phonon calculations.","authors":["Xiang Fu","Brandon M. Wood","Luis Barroso-Luque","Daniel S. Levine","Meng Gao","Misko Dzamba","C. Lawrence Zitnick"],"url":"https://arxiv.org/abs/2502.12147"}
{"created":"2025-04-24","title":"A mathematical model for a universal digital quantum computer with an application to the Grover-Rudolph algorithm","abstract":"In this work, we develop a novel mathematical framework for universal digital quantum computation using algebraic probability theory. We rigorously define quantum circuits as finite sequences of elementary quantum gates and establish their role in implementing unitary transformations. A key result demonstrates that every unitary matrix in \\(\\mathrm{U}(N)\\) can be expressed as a product of elementary quantum gates, leading to the concept of a universal dictionary for quantum computation. We apply this framework to the construction of quantum circuits that encode probability distributions, focusing on the Grover-Rudolph algorithm. By leveraging controlled quantum gates and rotation matrices, we design a quantum circuit that approximates a given probability density function. Numerical simulations, conducted using Qiskit, confirm the theoretical predictions and validate the effectiveness of our approach. These results provide a rigorous foundation for quantum circuit synthesis within an algebraic probability framework and offer new insights into the encoding of probability distributions in quantum algorithms. Potential applications include quantum machine learning, circuit optimization, and experimental implementations on real quantum hardware.","authors":["Antonio Falc\\'o","Daniela Falc\\'o--Pomares","Hermann G. Matthies"],"url":"https://arxiv.org/abs/2503.13388"}
{"created":"2025-04-24","title":"The Price of Simplicity: Analyzing Decoupled Policies for Multi-Location Inventory Control","abstract":"What is the performance cost of using simple, decoupled control policies in inherently coupled systems? Motivated by industrial refrigeration systems, where centralized compressors exhibit economies of scale yet traditional control employs decoupled room-by-room temperature regulation, we address this question through the lens of multi-location inventory control. Here, a planner manages multiple inventories to meet stochastic demand while minimizing costs that are coupled through nonlinear ordering functions reflecting economies of scale. Our main contributions are: (i) a surprising equivalence result showing that optimal stationary base-stock levels for individual locations remain unchanged despite the coupling when restricting attention to decoupled strategies; (ii) tight performance bounds for simple decoupled policies relative to optimal coupled policies, revealing that the worst-case ratio depends primarily on the degree of nonlinearity in the cost function and scales with the number of locations for systems with fixed costs; and (iii) analysis of practical online algorithms that achieve competitive performance without solving complex dynamic programs. Numerical simulations demonstrate that while decoupled policies significantly outperform their worst-case guarantees in typical scenarios, they still exhibit meaningful suboptimality compared to fully coordinated strategies. These results provide actionable guidance for system operators navigating the trade-off between control complexity and operational efficiency in coupled systems.","authors":["Yohan John","Vade Shah","James A. Preiss","Mahnoosh Alizadeh","Jason R. Marden"],"url":"https://arxiv.org/abs/2503.22639"}
{"created":"2025-04-24","title":"Plane-Wave Decomposition and Randomised Training; a Novel Path to Generalised PINNs for SHM","abstract":"In this paper, we introduce a formulation of Physics-Informed Neural Networks (PINNs), based on learning the form of the Fourier decomposition, and a training methodology based on a spread of randomly chosen boundary conditions. By training in this way we produce a PINN that generalises; after training it can be used to correctly predict the solution for an arbitrary set of boundary conditions and interpolate this solution between the samples that spanned the training domain. We demonstrate for a toy system of two coupled oscillators that this gives the PINN formulation genuine predictive capability owing to an effective reduction of the training to evaluation times ratio due to this decoupling of the solution from specific boundary conditions.","authors":["Rory Clements","James Ellis","Geoff Hassall","Simon Horsley","Gavin Tabor"],"url":"https://arxiv.org/abs/2504.00249"}
{"created":"2025-04-24","title":"Distinct hydrologic response patterns and trends worldwide revealed by physics-embedded learning","abstract":"To track rapid changes within our water sector, Global Water Models (GWMs) need to realistically represent hydrologic systems' response patterns - such as baseflow fraction - but are hindered by their limited ability to learn from data. Here we introduce a high-resolution physics-embedded big-data-trained model as a breakthrough in reliably capturing characteristic hydrologic response patterns ('signatures') and their shifts. By realistically representing the long-term water balance, the model revealed widespread shifts - up to ~20% over 20 years - in fundamental green-blue-water partitioning and baseflow ratios worldwide. Shifts in these response patterns, previously considered static, contributed to increasing flood risks in northern mid-latitudes, heightening water supply stresses in southern subtropical regions, and declining freshwater inputs to many European estuaries, all with ecological implications. With more accurate simulations at monthly and daily scales than current operational systems, this next-generation model resolves large, nonlinear seasonal runoff responses to rainfall ('elasticity') and streamflow flashiness in semi-arid and arid regions. These metrics highlight regions with management challenges due to large water supply variability and high climate sensitivity, but also provide tools to forecast seasonal water availability. This capability newly enables global-scale models to deliver reliable and locally relevant insights for water management.","authors":["Haoyu Ji","Yalan Song","Tadd Bindas","Chaopeng Shen","Yuan Yang","Ming Pan","Jiangtao Liu","Farshid Rahmani","Ather Abbas","Hylke Beck","Kathryn Lawson","Yoshihide Wada"],"url":"https://arxiv.org/abs/2504.10707"}
{"created":"2025-04-24","title":"Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification","abstract":"Background: Deep learning has significantly advanced medical image analysis, with Vision Transformers (ViTs) offering a powerful alternative to convolutional models by modeling long-range dependencies through self-attention. However, ViTs are inherently data-intensive and lack domain-specific inductive biases, limiting their applicability in medical imaging. In contrast, radiomics provides interpretable, handcrafted descriptors of tissue heterogeneity but suffers from limited scalability and integration into end-to-end learning frameworks. In this work, we propose the Radiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features with data-driven visual embeddings within a ViT backbone.","authors":["Zhenyu Yang","Haiming Zhu","Rihui Zhang","Haipeng Zhang","Jianliang Wang","Chunhao Wang","Minbin Chen","Fang-Fang Yin"],"url":"https://arxiv.org/abs/2504.10916"}
{"created":"2025-04-24","title":"Learned enclosure method for experimental EIT data","abstract":"Electrical impedance tomography (EIT) is a non-invasive imaging method with diverse applications, including medical imaging and non-destructive testing. The inverse problem of reconstructing internal electrical conductivity from boundary measurements is nonlinear and highly ill-posed, making it difficult to solve accurately. In recent years, there has been growing interest in combining analytical methods with machine learning to solve inverse problems. In this paper, we propose a method for estimating the convex hull of inclusions from boundary measurements by combining the enclosure method proposed by Ikehata with neural networks. We demonstrate its performance using experimental data. Compared to the classical enclosure method with least squares fitting, the learned convex hull achieves superior performance on both simulated and experimental data.","authors":["Sara Sippola","Siiri Rautio","Andreas Hauptmann","Takanori Ide","Samuli Siltanen"],"url":"https://arxiv.org/abs/2504.11512"}
{"created":"2025-04-24","title":"Dead Gate Elimination","abstract":"Hybrid quantum algorithms combine the strengths of quantum and classical computing. Many quantum algorithms, such as the variational quantum eigensolver (VQE), leverage this synergy. However, quantum circuits are executed in full, even when only subsets of measurement outcomes contribute to subsequent classical computations. In this manuscript, we propose a novel circuit optimization technique that identifies and removes dead gates. We prove that the removal of dead gates has no influence on the probability distribution of the measurement outcomes that contribute to the subsequent calculation result. We implemented and evaluated our optimization on a VQE instance, a quantum phase estimation (QPE) instance, and hybrid programs embedded with random circuits of varying circuit width, confirming its capability to remove a non-trivial number of dead gates in real-world algorithms. The effect of our optimization scales up as more measurement outcomes are identified as non-contributory, resulting in a proportionally greater reduction of dead gates.","authors":["Yanbin Chen","Christian B. Mendl","Helmut Seidl"],"url":"https://arxiv.org/abs/2504.12729"}
{"created":"2025-04-24","title":"Putting the Segment Anything Model to the Test with 3D Knee MRI -- A Comparison with State-of-the-Art Performance","abstract":"Menisci are cartilaginous tissue found within the knee that contribute to joint lubrication and weight dispersal. Damage to menisci can lead to onset and progression of knee osteoarthritis (OA), a condition that is a leading cause of disability, and for which there are few effective therapies. Accurate automated segmentation of menisci would allow for earlier detection and treatment of meniscal abnormalities, as well as shedding more light on the role the menisci play in OA pathogenesis. Focus in this area has mainly used variants of convolutional networks, but there has been no attempt to utilise recent large vision transformer segmentation models. The Segment Anything Model (SAM) is a so-called foundation segmentation model, which has been found useful across a range of different tasks due to the large volume of data used for training the model. In this study, SAM was adapted to perform fully-automated segmentation of menisci from 3D knee magnetic resonance images. A 3D U-Net was also trained as a baseline. It was found that, when fine-tuning only the decoder, SAM was unable to compete with 3D U-Net, achieving a Dice score of $0.81\\pm0.03$, compared to $0.87\\pm0.03$, on a held-out test set. When fine-tuning SAM end-to-end, a Dice score of $0.87\\pm0.03$ was achieved. The performance of both the end-to-end trained SAM configuration and the 3D U-Net were comparable to the winning Dice score ($0.88\\pm0.03$) in the IWOAI Knee MRI Segmentation Challenge 2019. Performance in terms of the Hausdorff Distance showed that both configurations of SAM were inferior to 3D U-Net in matching the meniscus morphology. Results demonstrated that, despite its generalisability, SAM was unable to outperform a basic 3D U-Net in meniscus segmentation, and may not be suitable for similar 3D medical image segmentation tasks also involving fine anatomical structures with low contrast and poorly-defined boundaries.","authors":["Oliver Mills","Philip Conaghan","Nishant Ravikumar","Samuel Relton"],"url":"https://arxiv.org/abs/2504.13340"}
{"created":"2025-04-24","title":"Expected Free Energy-based Planning as Variational Inference","abstract":"We address the problem of planning under uncertainty, where an agent must choose actions that not only achieve desired outcomes but also reduce uncertainty. Traditional methods often treat exploration and exploitation as separate objectives, lacking a unified inferential foundation. Active inference, grounded in the Free Energy Principle, provides such a foundation by minimizing Expected Free Energy (EFE), a cost function that combines utility with epistemic drives, such as ambiguity resolution and novelty seeking. However, the computational burden of EFE minimization had remained a significant obstacle to its scalability. In this paper, we show that EFE-based planning arises naturally from minimizing a variational free energy functional on a generative model augmented with preference and epistemic priors. This result reinforces theoretical consistency with the Free Energy Principle by casting planning under uncertainty itself as a form of variational inference. Our formulation yields policies that jointly support goal achievement and information gain, while incorporating a complexity term that accounts for bounded computational resources. This unifying framework connects and extends existing methods, enabling scalable, resource-aware implementations of active inference agents.","authors":["Bert de Vries","Wouter Nuijten","Thijs van de Laar","Wouter Kouw","Sepideh Adamiat","Tim Nisslbeck","Mykola Lukashchuk","Hoang Minh Huu Nguyen","Marco Hidalgo Araya","Raphael Tresor","Thijs Jenneskens","Ivana Nikoloska","Raaja Ganapathy Subramanian","Bart van Erp","Dmitry Bagaev","Albert Podusenko"],"url":"https://arxiv.org/abs/2504.14898"}
{"created":"2025-04-24","title":"Feedback Stackelberg-Nash equilibria in difference games with quasi-hierarchical interactions and inequality constraints","abstract":"In this paper, we study a class of two-player deterministic finite-horizon difference games with coupled inequality constraints, where each player has two types of decision variables: one involving sequential interactions and the other simultaneous interactions. We refer to this class of games as quasi-hierarchical dynamic games and define a solution concept called the feedback Stackelberg-Nash (FSN) equilibrium. Under separability assumption on cost functions, we provide a recursive formulation of the FSN solution using dynamic programming. We show that the FSN solution can be derived from the parametric feedback Stackelberg solution of an associated unconstrained game involving only sequential interactions, with a specific choice of the parameters that satisfy certain implicit complementarity conditions. For the linear-quadratic case, we show that an FSN solution is obtained by reformulating these complementarity conditions as a single large-scale linear complementarity problem. Finally, we illustrate our results using a dynamic duopoly game with production constraints.","authors":["Partha Sarathi Mohapatra","Puduru Viswanadha Reddy","Georges Zaccour"],"url":"https://arxiv.org/abs/2504.15019"}
