{"created":"2025-05-05","title":"Numerical Integration of Navier-Stokes Equations by Time Series Expansion and Stabilized FEM","abstract":"This manuscript introduces an advanced numerical approach for the integration of incompressible Navier-Stokes (NS) equations using a Time Series Expansion (TSE) method within a Finite Element Method (FEM) framework. The technique is enhanced by a novel stabilization strategy, incorporating a Divergent Series Resummation (DSR) technique, which significantly augments the computational efficiency of the algorithm. The stabilization mechanism is meticulously designed to improve the stability and validity of computed series terms, enabling the application of the Factorial Series (FS) algorithm for series resummation. This approach is pivotal in addressing the challenges associated with the accurate and stable numerical solution of NS equations, which are critical in Computational Fluid Dynamics (CFD) applications. The manuscript elaborates on the variational formulation of Stokes problem and present convergence analysis of the method using the Ladyzhenskaya-Babuska-Brezzi (LBB) condition. It is followed by the NS equations and the implementation details of the stabilization technique, underscored by numerical tests on laminar flow past a cylinder, showcasing the method's efficacy and potential for broad applicability in fluid dynamics simulations. The results of the stabilization indicate a substantial enhancement in computational stability and accuracy, offering a promising avenue for future research in the field.","authors":["Ahmad Deeb","Denys Dutykh"],"url":"https://arxiv.org/abs/2505.00705"}
{"created":"2025-05-05","title":"Relative position of a parabola or a hyperbola and an ellipse without computing intersection points","abstract":"Efficient methods to determine the relative position of two conics are of great interest for applications in robotics, computer animation, CAGD, computational physics, and other areas. We present a method to obtain the relative position of a parabola or a hyperbola, and a coplanar ellipse, directly from the coefficients of their implicit equations, even if they are not given in canonical form, and avoiding the computation of the corresponding intersection points (and their characteristics).","authors":["Jorge Caravantes","Gema M. Diaz-Toca","Mario Fioravanti","Laureano Gonzalez-Vega"],"url":"https://arxiv.org/abs/2505.00706"}
{"created":"2025-05-05","title":"A high-order combined interpolation/finite element technique for evolutionary coupled groundwater-surface water problem","abstract":"A high-order combined interpolation/finite element technique is developed for solving the coupled groundwater-surface water system that governs flows in karst aquifers. In the proposed high-order scheme we approximate the time derivative with piecewise polynomial interpolation of second-order and use the finite element discretization of piecewise polynomials of degree $d$ and $d+1$, where $d \\geq 2$ is an integer, to approximate the space derivatives. The stability together with the error estimates of the constructed technique are established in $L^{\\infty}(0,T;\\text{\\,}L^{2})$-norm. The analysis suggests that the developed computational technique is unconditionally stable, temporal second-order accurate and convergence in space of order $d+1$. Furthermore, the new approach is faster and more efficient than a broad range of numerical methods discussed in the literature for the given initial-boundary value problem. Some examples are carried out to confirm the theoretical results.","authors":["Eric Ngondiep","Areej A. Binsultant","Ibtisam M. Aldawish"],"url":"https://arxiv.org/abs/2505.00707"}
{"created":"2025-05-05","title":"Numerical Approaches for non-local Transport-Dominated PDE Models with Applications to Biology","abstract":"Transport-dominated partial differential equation models have been used extensively over the past two decades to describe various collective migration phenomena in cell biology and ecology. To understand the behaviour of these models (and the biological systems they describe) different analytical and numerical approaches have been used. While the analytical approaches have been discussed by different recent review studies, the numerical approaches are still facing different open problems, and thus are being employed on a rather ad-hoc basis for each developed non-local model. The goal of this review is to summarise the basic ideas behind these transport-dominated non-local models, to discuss the current numerical approaches used to simulate these models, and finally to discuss some open problems related to the applications of these numerical methods, in particular the finite element method. This allows us to emphasize the opportunities offered by this numerical method to advance the research in this field. In addition, we present in detail some numerical schemes that we used to discretize these non-local equations; in particular a new semi-implicit scheme we introduced to stabilize the oscillations obtained with classical schemes.","authors":["Johan Marguet (LMB)","Raluca Eftimie (LMB)","Alexei Lozinski (LMB)"],"url":"https://arxiv.org/abs/2505.00708"}
{"created":"2025-05-05","title":"MOR-T L : A Novel Model Order Reduction Method for Parametrized Problems with Application to Seismic Wave Propagation","abstract":"This paper presents an efficient strategy for constructing Reduced-Order Model (ROM) bases using Taylor polynomial expansions and Fr{\\'e}chet derivatives with respect to model parameters. The proposed approach enables the construction of ROM bases with minimal additional computational cost. By exploiting Fr{\\'e}chet derivatives -solution to the same problem with distinct right-hand sides -the method introduces a streamlined multiple-right-hand-side (RHS) strategy for ROM bases construction. This approach not only reduces overall computational expenses but also improves accuracy during model parameter updates. Numerical experiments on a two-dimensional wave problem demonstrate significant efficiency gains and enhanced performance, highlighting the potential of the proposed method to advance computational cost-effectiveness, particularly in seismic inversion applications.","authors":["Julien Besset (IRIS)","H\\'el\\`ene Barucq (IRIS)","Rabia Djellouli (IRIS)","Stefano Frambati (Total Energies One Tech)"],"url":"https://arxiv.org/abs/2505.00709"}
{"created":"2025-05-05","title":"A Goal-Oriented Adaptive Sampling Procedure for Projection-Based Reduced-Order Models with Hyperreduction","abstract":"Projection-based reduced-order models (PROMs) have demonstrated accuracy, reliability, and robustness in approximating high-dimensional, differential equation-based computational models across many applications. For this reason, it has been proposed as a tool for high-querying parametric design problems like those arising in modern aircraft design. Since aerodynamic simulations can be computationally expensive, PROMs offer the potential for more rapid estimations of high-fidelity solutions. However, the efficiency can still be tied to the dimension of the full-order model (FOM), particularly when projected quantities must be frequently recomputed due to non-linearities or parameter dependence. In the case of Petrov-Galerkin models, the projected residual and Jacobian are re-evaluated at every Newton iteration, thereby limiting the anticipated cost improvements. Hyperreduction is one of the tools available to approximate these quantities and address this issue. This work tests the energy-conserving sampling and weighting (ECSW) method as a potential approach for hyperreduction. It will be incorporated into the work in a previous article {10.1016/j.compfluid.2025.106568} which had developed an adaptive sampling procedure for building a reduced-order model (ROM) with a controlled functional error. The impacts of hyperreduction on computational cost and accuracy will be studied using the NACA0012 airfoil.","authors":["Calista Biondic","Siva Nadarajah"],"url":"https://arxiv.org/abs/2505.00712"}
{"created":"2025-05-05","title":"Partial integration based regularization in BEM for 3D elastostatic problems: The role of line integrals","abstract":"The Boundary Element Method (BEM) is a powerful numerical approach for solving 3D elastostatic problems, particularly advantageous for crack propagation in fracture mechanics and half-space problems. Despite its benefits, BEM faces significant challenges related to dense system matrices and singular integral kernels. The computational expense can be mitigated using various fast methods; this study employs the Chebyshev interpolation-based Fast Multipole Method (FMM). To handle singular kernels, several analytical and numerical integration or regularization techniques exist. One such technique combines partial integration with Stokes' theorem to transform hyper-singular and strong singular kernels into weakly singular ones. However, applying Stokes' theorem introduces line integrals in half-space problems and with FMM, where the geometry is partitioned into near-field and far-field regions and must be treated as an open surface. In this paper, the necessary line integrals for strongly singular and hyper-singular kernels are presented and their significance in the aforementioned problems is demonstrated.","authors":["Vibudha Lakshmi Keshava","Martin Schanz"],"url":"https://arxiv.org/abs/2505.00713"}
{"created":"2025-05-05","title":"Comparison of FMM and $\\mathcal{H}$-matrix based 3D-ACA for a time domain boundary element method","abstract":"The time domain Boundary Element Method (BEM) for the homogeneous wave equation with vanishing initial conditions is considered. The generalized convolution quadrature method (gCQ) developed by Lopez-Fernandez and Sauter is used for the temporal discretisation. The spatial discretisation is done classically using low order shape functions.","authors":["Martin Schanz","Vibudha Lakshmi Keshava","Herbert de Gersem"],"url":"https://arxiv.org/abs/2505.00715"}
{"created":"2025-05-05","title":"FinBERT-QA: Financial Question Answering with pre-trained BERT Language Models","abstract":"Motivated by the emerging demand in the financial industry for the automatic analysis of unstructured and structured data at scale, Question Answering (QA) systems can provide lucrative and competitive advantages to companies by facilitating the decision making of financial advisers. Consequently, we propose a novel financial QA system using the transformer-based pre-trained BERT language model to address the limitations of data scarcity and language specificity in the financial domain. Our system focuses on financial non-factoid answer selection, which retrieves a set of passage-level texts and selects the most relevant as the answer. To increase efficiency, we formulate the answer selection task as a re-ranking problem, in which our system consists of an Answer Retriever using BM25, a simple information retrieval approach, to first return a list of candidate answers, and an Answer Re-ranker built with variants of pre-trained BERT language models to re-rank and select the most relevant answers. We investigate various learning, further pre-training, and fine-tuning approaches for BERT. Our experiments suggest that FinBERT-QA, a model built from applying the Transfer and Adapt further fine-tuning and pointwise learning approach, is the most effective, improving the state-of-the-art results of task 2 of the FiQA dataset by 16% on MRR, 17% on NDCG, and 21% on Precision@1.","authors":["Bithiah Yuan"],"url":"https://arxiv.org/abs/2505.00725"}
{"created":"2025-05-05","title":"Faster All-Pairs Optimal Electric Car Routing","abstract":"We present a randomized $\\tilde{O}(n^{3.5})$-time algorithm for computing \\emph{optimal energetic paths} for an electric car between all pairs of vertices in an $n$-vertex directed graph with positive and negative \\emph{costs}. The optimal energetic paths are finite and well-defined even if the graph contains negative-cost cycles. This makes the problem much more challenging than standard shortest paths problems.","authors":["Dani Dorfman","Haim Kaplan","Robert E. Tarjan","Mikkel Thorup","Uri Zwick"],"url":"https://arxiv.org/abs/2505.00728"}
{"created":"2025-05-05","title":"Primality Testing via Circulant Matrix Eigenvalue Structure: A Novel Approach Using Cyclotomic Field Theory","abstract":"This paper presents a novel primality test based on the eigenvalue structure of circulant matrices constructed from roots of unity. We prove that an integer $n > 2$ is prime if and only if the minimal polynomial of the circulant matrix $C_n = W_n + W_n^2$ has exactly two irreducible factors over $\\mathbb{Q}$. This characterization connects cyclotomic field theory with matrix algebra, providing both theoretical insights and practical applications. We demonstrate that the eigenvalue patterns of these matrices reveal fundamental distinctions between prime and composite numbers, leading to a deterministic primality test. Our approach leverages the relationship between primitive roots of unity, Galois theory, and the factorization of cyclotomic polynomials. We provide comprehensive experimental validation across various ranges of integers, discuss practical implementation considerations, and analyze the computational complexity of our method in comparison with established primality tests. The visual interpretation of our mathematical framework provides intuitive understanding of the algebraic structures that distinguish prime numbers. Our experimental validation demonstrates that our approach offers a deterministic alternative to existing methods, with performance characteristics reflecting its algebraic foundations.","authors":["Marius-Constantin Dinu"],"url":"https://arxiv.org/abs/2505.00730"}
{"created":"2025-05-05","title":"ROSA: A Knowledge-based Solution for Robot Self-Adaptation","abstract":"Autonomous robots must operate in diverse environments and handle multiple tasks despite uncertainties. This creates challenges in designing software architectures and task decision-making algorithms, as different contexts may require distinct task logic and architectural configurations. To address this, robotic systems can be designed as self-adaptive systems capable of adapting their task execution and software architecture at runtime based on their context.This paper introduces ROSA, a novel knowledge-based framework for RObot Self-Adaptation, which enables task-and-architecture co-adaptation (TACA) in robotic systems. ROSA achieves this by providing a knowledge model that captures all application-specific knowledge required for adaptation and by reasoning over this knowledge at runtime to determine when and how adaptation should occur. In addition to a conceptual framework, this work provides an open-source ROS 2-based reference implementation of ROSA and evaluates its feasibility and performance in an underwater robotics application. Experimental results highlight ROSA's advantages in reusability and development effort for designing self-adaptive robotic systems.","authors":["Gustavo Rezende Silva","Juliane P\\\"a{\\ss}ler","S. Lizeth Tapia Tarifa","Einar Broch Johnsen","Carlos Hern\\'andez Corbato"],"url":"https://arxiv.org/abs/2505.00733"}
{"created":"2025-05-05","title":"Unconstrained Large-scale 3D Reconstruction and Rendering across Altitudes","abstract":"Production of photorealistic, navigable 3D site models requires a large volume of carefully collected images that are often unavailable to first responders for disaster relief or law enforcement. Real-world challenges include limited numbers of images, heterogeneous unposed cameras, inconsistent lighting, and extreme viewpoint differences for images collected from varying altitudes. To promote research aimed at addressing these challenges, we have developed the first public benchmark dataset for 3D reconstruction and novel view synthesis based on multiple calibrated ground-level, security-level, and airborne cameras. We present datasets that pose real-world challenges, independently evaluate calibration of unposed cameras and quality of novel rendered views, demonstrate baseline performance using recent state-of-practice methods, and identify challenges for further research.","authors":["Neil Joshi","Joshua Carney","Nathanael Kuo","Homer Li","Cheng Peng","Myron Brown"],"url":"https://arxiv.org/abs/2505.00734"}
{"created":"2025-05-05","title":"MoSAM: Motion-Guided Segment Anything Model with Spatial-Temporal Memory Selection","abstract":"The recent Segment Anything Model 2 (SAM2) has demonstrated exceptional capabilities in interactive object segmentation for both images and videos. However, as a foundational model on interactive segmentation, SAM2 performs segmentation directly based on mask memory from the past six frames, leading to two significant challenges. Firstly, during inference in videos, objects may disappear since SAM2 relies solely on memory without accounting for object motion information, which limits its long-range object tracking capabilities. Secondly, its memory is constructed from fixed past frames, making it susceptible to challenges associated with object disappearance or occlusion, due to potentially inaccurate segmentation results in memory. To address these problems, we present MoSAM, incorporating two key strategies to integrate object motion cues into the model and establish more reliable feature memory. Firstly, we propose Motion-Guided Prompting (MGP), which represents the object motion in both sparse and dense manners, then injects them into SAM2 through a set of motion-guided prompts. MGP enables the model to adjust its focus towards the direction of motion, thereby enhancing the object tracking capabilities. Furthermore, acknowledging that past segmentation results may be inaccurate, we devise a Spatial-Temporal Memory Selection (ST-MS) mechanism that dynamically identifies frames likely to contain accurate segmentation in both pixel- and frame-level. By eliminating potentially inaccurate mask predictions from memory, we can leverage more reliable memory features to exploit similar regions for improving segmentation results. Extensive experiments on various benchmarks of video object segmentation and video instance segmentation demonstrate that our MoSAM achieves state-of-the-art results compared to other competitors.","authors":["Qiushi Yang","Yuan Yao","Miaomiao Cui","Liefeng Bo"],"url":"https://arxiv.org/abs/2505.00739"}
{"created":"2025-05-05","title":"Fast2comm:Collaborative perception combined with prior knowledge","abstract":"Collaborative perception has the potential to significantly enhance perceptual accuracy through the sharing of complementary information among agents. However, real-world collaborative perception faces persistent challenges, particularly in balancing perception performance and bandwidth limitations, as well as coping with localization errors. To address these challenges, we propose Fast2comm, a prior knowledge-based collaborative perception framework. Specifically, (1)we propose a prior-supervised confidence feature generation method, that effectively distinguishes foreground from background by producing highly discriminative confidence features; (2)we propose GT Bounding Box-based spatial prior feature selection strategy to ensure that only the most informative prior-knowledge features are selected and shared, thereby minimizing background noise and optimizing bandwidth efficiency while enhancing adaptability to localization inaccuracies; (3)we decouple the feature fusion strategies between model training and testing phases, enabling dynamic bandwidth adaptation. To comprehensively validate our framework, we conduct extensive experiments on both real-world and simulated datasets. The results demonstrate the superior performance of our model and highlight the necessity of the proposed methods. Our code is available at https://github.com/Zhangzhengbin-TJ/Fast2comm.","authors":["Zhengbin Zhang","Yan Wu","Hongkun Zhang"],"url":"https://arxiv.org/abs/2505.00740"}
{"created":"2025-05-05","title":"Detection and Classification of Diseases in Multi-Crop Leaves using LSTM and CNN Models","abstract":"Plant diseases pose a serious challenge to agriculture by reducing crop yield and affecting food quality. Early detection and classification of these diseases are essential for minimising losses and improving crop management practices. This study applies Convolutional Neural Networks (CNN) and Long Short-Term Memory (LSTM) models to classify plant leaf diseases using a dataset containing 70,295 training images and 17,572 validation images across 38 disease classes. The CNN model was trained using the Adam optimiser with a learning rate of 0.0001 and categorical cross-entropy as the loss function. After 10 training epochs, the model achieved a training accuracy of 99.1% and a validation accuracy of 96.4%. The LSTM model reached a validation accuracy of 93.43%. Performance was evaluated using precision, recall, F1-score, and confusion matrix, confirming the reliability of the CNN-based approach. The results suggest that deep learning models, particularly CNN, enable an effective solution for accurate and scalable plant disease classification, supporting practical applications in agricultural monitoring.","authors":["Srinivas Kanakala","Sneha Ningappa"],"url":"https://arxiv.org/abs/2505.00741"}
{"created":"2025-05-05","title":"Zoomer: Adaptive Image Focus Optimization for Black-box MLLM","abstract":"Recent advancements in multimodal large language models (MLLMs) have broadened the scope of vision-language tasks, excelling in applications like image captioning and interactive question-answering. However, these models struggle with accurately processing visual data, particularly in tasks requiring precise object recognition and fine visual details. Stringent token limits often result in the omission of critical information, hampering performance. To address these limitations, we introduce \\SysName, a novel visual prompting mechanism designed to enhance MLLM performance while preserving essential visual details within token limits. \\SysName features three key innovations: a prompt-aware strategy that dynamically highlights relevant image regions, a spatial-preserving orchestration schema that maintains object integrity, and a budget-aware prompting method that balances global context with crucial visual details. Comprehensive evaluations across multiple datasets demonstrate that \\SysName consistently outperforms baseline methods, achieving up to a $26.9\\%$ improvement in accuracy while significantly reducing token consumption.","authors":["Jiaxu Qian","Chendong Wang","Yifan Yang","Chaoyun Zhang","Huiqiang Jiang","Xufang Luo","Yu Kang","Qingwei Lin","Anlan Zhang","Shiqi Jiang","Ting Cao","Tianjun Mao","Suman Banerjee","Guyue Liu","Saravan Rajmohan","Dongmei Zhang","Yuqing Yang","Qi Zhang","Lili Qiu"],"url":"https://arxiv.org/abs/2505.00742"}
{"created":"2025-05-05","title":"DOPE: Dual Object Perception-Enhancement Network for Vision-and-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) is a challenging task where an agent must understand language instructions and navigate unfamiliar environments using visual cues. The agent must accurately locate the target based on visual information from the environment and complete tasks through interaction with the surroundings. Despite significant advancements in this field, two major limitations persist: (1) Many existing methods input complete language instructions directly into multi-layer Transformer networks without fully exploiting the detailed information within the instructions, thereby limiting the agent's language understanding capabilities during task execution; (2) Current approaches often overlook the modeling of object relationships across different modalities, failing to effectively utilize latent clues between objects, which affects the accuracy and robustness of navigation decisions. We propose a Dual Object Perception-Enhancement Network (DOPE) to address these issues to improve navigation performance. First, we design a Text Semantic Extraction (TSE) to extract relatively essential phrases from the text and input them into the Text Object Perception-Augmentation (TOPA) to fully leverage details such as objects and actions within the instructions. Second, we introduce an Image Object Perception-Augmentation (IOPA), which performs additional modeling of object information across different modalities, enabling the model to more effectively utilize latent clues between objects in images and text, enhancing decision-making accuracy. Extensive experiments on the R2R and REVERIE datasets validate the efficacy of the proposed approach.","authors":["Yinfeng Yu","Dongsheng Yang"],"url":"https://arxiv.org/abs/2505.00743"}
{"created":"2025-05-05","title":"Localizing Before Answering: A Benchmark for Grounded Medical Visual Question Answering","abstract":"Medical Large Multi-modal Models (LMMs) have demonstrated remarkable capabilities in medical data interpretation. However, these models frequently generate hallucinations contradicting source evidence, particularly due to inadequate localization reasoning. This work reveals a critical limitation in current medical LMMs: instead of analyzing relevant pathological regions, they often rely on linguistic patterns or attend to irrelevant image areas when responding to disease-related queries. To address this, we introduce HEAL-MedVQA (Hallucination Evaluation via Localization MedVQA), a comprehensive benchmark designed to evaluate LMMs' localization abilities and hallucination robustness. HEAL-MedVQA features (i) two innovative evaluation protocols to assess visual and textual shortcut learning, and (ii) a dataset of 67K VQA pairs, with doctor-annotated anatomical segmentation masks for pathological regions. To improve visual reasoning, we propose the Localize-before-Answer (LobA) framework, which trains LMMs to localize target regions of interest and self-prompt to emphasize segmented pathological areas, generating grounded and reliable answers. Experimental results demonstrate that our approach significantly outperforms state-of-the-art biomedical LMMs on the challenging HEAL-MedVQA benchmark, advancing robustness in medical VQA.","authors":["Dung Nguyen","Minh Khoi Ho","Huy Ta","Thanh Tam Nguyen","Qi Chen","Kumar Rav","Quy Duong Dang","Satwik Ramchandre","Son Lam Phung","Zhibin Liao","Minh-Son To","Johan Verjans","Phi Le Nguyen","Vu Minh Hieu Phan"],"url":"https://arxiv.org/abs/2505.00744"}
{"created":"2025-05-05","title":"Responsive DNN Adaptation for Video Analytics against Environment Shift via Hierarchical Mobile-Cloud Collaborations","abstract":"Mobile video analysis systems often encounter various deploying environments, where environment shifts present greater demands for responsiveness in adaptations of deployed \"expert DNN models\". Existing model adaptation frameworks primarily operate in a cloud-centric way, exhibiting degraded performance during adaptation and delayed reactions to environment shifts. Instead, this paper proposes MOCHA, a novel framework optimizing the responsiveness of continuous model adaptation through hierarchical collaborations between mobile and cloud resources. Specifically, MOCHA (1) reduces adaptation response delays by performing on-device model reuse and fast fine-tuning before requesting cloud model retrieval and end-to-end retraining; (2) accelerates history expert model retrieval by organizing them into a structured taxonomy utilizing domain semantics analyzed by a cloud foundation model as indices; (3) enables efficient local model reuse by maintaining onboard expert model caches for frequent scenes, which proactively prefetch model weights from the cloud model database. Extensive evaluations with real-world videos on three DNN tasks show MOCHA improves the model accuracy during adaptation by up to 6.8% while saving the response delay and retraining time by up to 35.5x and 3.0x respectively.","authors":["Maozhe Zhao","Shengzhong Liu","Fan Wu","Guihai Chen"],"url":"https://arxiv.org/abs/2505.00745"}
{"created":"2025-05-05","title":"Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis","abstract":"Vision-language models such as OpenAI GPT-4o can transcribe mathematical documents directly from images, yet their token-level confidence signals are seldom used to pinpoint local recognition mistakes. We present an entropy-heat-mapping proof-of-concept that turns per-token Shannon entropy into a visual ''uncertainty landscape''. By scanning the entropy sequence with a fixed-length sliding window, we obtain hotspots that are likely to contain OCR errors such as missing symbols, mismatched braces, or garbled prose. Using a small, curated set of scanned research pages rendered at several resolutions, we compare the highlighted hotspots with the actual transcription errors produced by GPT-4o. Our analysis shows that the vast majority of true errors are indeed concentrated inside the high-entropy regions. This study demonstrates--in a minimally engineered setting--that sliding-window entropy can serve as a practical, lightweight aid for post-editing GPT-based OCR. All code, sample data, and annotation guidelines are released to encourage replication and further research.","authors":["Alexei Kaltchenko"],"url":"https://arxiv.org/abs/2505.00746"}
{"created":"2025-05-05","title":"Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey","abstract":"Cooperative perception extends the perception capabilities of autonomous vehicles by enabling multi-agent information sharing via Vehicle-to-Everything (V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic \"information sensor\" characterized by limited communication, heterogeneity, mobility, and scalability. This survey provides a comprehensive review of recent advancements from the perspective of information-centric cooperative perception, focusing on three key dimensions: information representation, information fusion, and large-scale deployment. We categorize information representation into data-level, feature-level, and object-level schemes, and highlight emerging methods for reducing data volume and compressing messages under communication constraints. In information fusion, we explore techniques under both ideal and non-ideal conditions, including those addressing heterogeneity, localization errors, latency, and packet loss. Finally, we summarize system-level approaches to support scalability in dense traffic scenarios. Compared with existing surveys, this paper introduces a new perspective by treating V2X communication as an information sensor and emphasizing the challenges of deploying cooperative perception in real-world intelligent transportation systems.","authors":["Zhiying Song","Tenghui Xie","Fuxi Wen","Jun Li"],"url":"https://arxiv.org/abs/2505.00747"}
{"created":"2025-05-05","title":"The Coral Protocol: Open Infrastructure Connecting The Internet of Agents","abstract":"The Coral Protocol is an open and decentralized collaboration infrastructure that enables communication, coordination, trust and payments for The Internet of Agents. It addresses the growing need for interoperability in a world where organizations are deploying multiple specialized AI agents that must work together across domains and vendors. As a foundational platform for multi-agent AI ecosystems, Coral establishes a common language and coordination framework allowing any agent to participate in complex workflows with others. Its design emphasizes broad compatibility, security, and vendor neutrality, ensuring that agent interactions are efficient and trustworthy. In particular, Coral introduces standardized messaging formats for agent communication, a modular coordination mechanism for orchestrating multi-agent tasks, and secure team formation capabilities for dynamically assembling trusted groups of agents. Together, these innovations position Coral Protocol as a cornerstone of the emerging \"Internet of Agents,\" unlocking new levels of automation, collective intelligence, and business value through open agent collaboration.","authors":["Roman J. Georgio","Caelum Forder","Suman Deb","Peter Carroll","\\\"Onder G\\\"urcan"],"url":"https://arxiv.org/abs/2505.00749"}
{"created":"2025-05-05","title":"GVPT -- A software for guided visual pitch tracking","abstract":"GVPT (Guided visual pitch tracking) is a publicly available, real-time pitch tracking software designed to guide and evaluate vocal pitch control using visual feedback. Developed for clinical and research applications, the system presents various visual target pitch contour and overlays the subject's pitch in real-time to promote accurate vocal reproduction. GVPT supports difficulty modification, session logging, and precise pitch tracking. The software enables voice pitch control exercise in both experimental and therapeutic settings.","authors":["Hyunjin Cho","Farhad Tabasi","Jeremy D. Greenlee","Rahul Singh"],"url":"https://arxiv.org/abs/2505.00750"}
{"created":"2025-05-05","title":"InstructAttribute: Fine-grained Object Attributes editing with Instruction","abstract":"Text-to-image (T2I) diffusion models, renowned for their advanced generative abilities, are extensively utilized in image editing applications, demonstrating remarkable effectiveness. However, achieving precise control over fine-grained attributes still presents considerable challenges. Existing image editing techniques either fail to modify the attributes of an object or struggle to preserve its structure and maintain consistency in other areas of the image. To address these challenges, we propose the Structure-Preserving and Attribute Amplification (SPAA), a training-free method which enables precise control over the color and material transformations of objects by editing the self-attention maps and cross-attention values. Furthermore, we constructed the Attribute Dataset, which encompasses nearly all colors and materials associated with various objects, by integrating multimodal large language models (MLLM) to develop an automated pipeline for data filtering and instruction labeling. Training on this dataset, we present our InstructAttribute, an instruction-based model designed to facilitate fine-grained editing of color and material attributes. Extensive experiments demonstrate that our method achieves superior performance in object-level color and material editing, outperforming existing instruction-based image editing approaches.","authors":["Xingxi Yin","Jingfeng Zhang","Zhi Li","Yicheng Li","Yin Zhang"],"url":"https://arxiv.org/abs/2505.00751"}
{"created":"2025-05-05","title":"DARTer: Dynamic Adaptive Representation Tracker for Nighttime UAV Tracking","abstract":"Nighttime UAV tracking presents significant challenges due to extreme illumination variations and viewpoint changes, which severely degrade tracking performance. Existing approaches either rely on light enhancers with high computational costs or introduce redundant domain adaptation mechanisms, failing to fully utilize the dynamic features in varying perspectives. To address these issues, we propose \\textbf{DARTer} (\\textbf{D}ynamic \\textbf{A}daptive \\textbf{R}epresentation \\textbf{T}racker), an end-to-end tracking framework designed for nighttime UAV scenarios. DARTer leverages a Dynamic Feature Blender (DFB) to effectively fuse multi-perspective nighttime features from static and dynamic templates, enhancing representation robustness. Meanwhile, a Dynamic Feature Activator (DFA) adaptively activates Vision Transformer layers based on extracted features, significantly improving efficiency by reducing redundant computations. Our model eliminates the need for complex multi-task loss functions, enabling a streamlined training process. Extensive experiments on multiple nighttime UAV tracking benchmarks demonstrate the superiority of DARTer over state-of-the-art trackers. These results confirm that DARTer effectively balances tracking accuracy and efficiency, making it a promising solution for real-world nighttime UAV tracking applications.","authors":["Xuzhao Li","Xuchen Li","Shiyu Hu"],"url":"https://arxiv.org/abs/2505.00752"}
{"created":"2025-05-05","title":"A Survey on Large Language Model based Human-Agent Systems","abstract":"Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers.","authors":["Henry Peng Zou","Wei-Chieh Huang","Yaozu Wu","Yankai Chen","Chunyu Miao","Hoang Nguyen","Yue Zhou","Weizhi Zhang","Liancheng Fang","Langzhou He","Yangning Li","Yuwei Cao","Dongyuan Li","Renhe Jiang","Philip S. Yu"],"url":"https://arxiv.org/abs/2505.00753"}
{"created":"2025-05-05","title":"P2P-Insole: Human Pose Estimation Using Foot Pressure Distribution and Motion Sensors","abstract":"This work presents P2P-Insole, a low-cost approach for estimating and visualizing 3D human skeletal data using insole-type sensors integrated with IMUs. Each insole, fabricated with e-textile garment techniques, costs under USD 1, making it significantly cheaper than commercial alternatives and ideal for large-scale production. Our approach uses foot pressure distribution, acceleration, and rotation data to overcome limitations, providing a lightweight, minimally intrusive, and privacy-aware solution. The system employs a Transformer model for efficient temporal feature extraction, enriched by first and second derivatives in the input stream. Including multimodal information, such as accelerometers and rotational measurements, improves the accuracy of complex motion pattern recognition. These facts are demonstrated experimentally, while error metrics show the robustness of the approach in various posture estimation tasks. This work could be the foundation for a low-cost, practical application in rehabilitation, injury prevention, and health monitoring while enabling further development through sensor optimization and expanded datasets.","authors":["Atsuya Watanabe","Ratna Aisuwarya","Lei Jing"],"url":"https://arxiv.org/abs/2505.00755"}
{"created":"2025-05-05","title":"Efficient On-Chip Implementation of 4D Radar-Based 3D Object Detection on Hailo-8L","abstract":"4D radar has attracted attention in autonomous driving due to its ability to enable robust 3D object detection even under adverse weather conditions. To practically deploy such technologies, it is essential to achieve real-time processing within low-power embedded environments. Addressing this, we present the first on-chip implementation of a 4D radar-based 3D object detection model on the Hailo-8L AI accelerator. Although conventional 3D convolutional neural network (CNN) architectures require 5D inputs, the Hailo-8L only supports 4D tensors, posing a significant challenge. To overcome this limitation, we introduce a tensor transformation method that reshapes 5D inputs into 4D formats during the compilation process, enabling direct deployment without altering the model structure. The proposed system achieves 46.47% AP_3D and 52.75% AP_BEV, maintaining comparable accuracy to GPU-based models while achieving an inference speed of 13.76 Hz. These results demonstrate the applicability of 4D radar-based perception technologies to autonomous driving systems.","authors":["Woong-Chan Byun","Dong-Hee Paek","Seung-Hyun Song","Seung-Hyun Kong"],"url":"https://arxiv.org/abs/2505.00757"}
{"created":"2025-05-05","title":"Multi-Modal Language Models as Text-to-Image Model Evaluators","abstract":"The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.","authors":["Jiahui Chen","Candace Ross","Reyhane Askari-Hemmat","Koustuv Sinha","Melissa Hall","Michal Drozdzal","Adriana Romero-Soriano"],"url":"https://arxiv.org/abs/2505.00759"}
{"created":"2025-05-05","title":"Person detection and re-identification in open-world settings of retail stores and public spaces","abstract":"Practical applications of computer vision in smart cities usually assume system integration and operation in challenging open-world environments. In the case of person re-identification task the main goal is to retrieve information whether the specific person has appeared in another place at a different time instance of the same video, or over multiple camera feeds. This typically assumes collecting raw data from video surveillance cameras in different places and under varying illumination conditions. In the considered open-world setting it also requires detection and localization of the person inside the analyzed video frame before the main re-identification step. With multi-person and multi-camera setups the system complexity becomes higher, requiring sophisticated tracking solutions and re-identification models. In this work we will discuss existing challenges in system design architectures, consider possible solutions based on different computer vision techniques, and describe applications of such systems in retail stores and public spaces for improved marketing analytics. In order to analyse sensitivity of person re-identification task under different open-world environments, a performance of one close to real-time solution will be demonstrated over several video captures and live camera feeds. Finally, based on conducted experiments we will indicate further research directions and possible system improvements.","authors":["Branko Brklja\\v{c}","Milan Brklja\\v{c}"],"url":"https://arxiv.org/abs/2505.00772"}
{"created":"2025-05-05","title":"Design, Integration, and Evaluation of a Dual-Arm Robotic System for High Throughput Tissue Sampling from Potato Tubers","abstract":"Manual tissue extraction from potato tubers for molecular pathogen detection is highly laborious. This study presents a machine-vision-guided, dual-arm coordinated inline robotic system integrating tuber grasping and tissue sampling mechanisms. Tubers are transported on a conveyor that halts when a YOLOv11-based vision system detects a tuber within the workspace of a one-prismatic-degree-of-freedom (P-DoF) robotic arm. This arm, equipped with a gripping end-effector, secures and positions the tuber for sampling. The second arm, a 3-P-DoF Cartesian manipulator with a biopsy punch-based end-effector, then performs tissue extraction guided by a YOLOv10-based vision system that identifies the sampling sites on the tuber such as eyes or stolon scars. The sampling involves four stages: insertion of the punch into the tuber, punch rotation for tissue detachment, biopsy punch retraction, and deposition of the tissue core onto a collection site. The system achieved an average positional error of 1.84 mm along the tuber surface and a depth deviation of 1.79 mm from a 7.00 mm target. The success rate for core extraction and deposition was 81.5%, with an average sampling cycle of 10.4 seconds. The total cost of the system components was under $1,900, demonstrating the system's potential as a cost-effective alternative to labor-intensive manual tissue sampling. Future work will focus on optimizing for multi-site sampling from a single tuber and validation in commercial settings.","authors":["Divyanth L. G.","Syed Usama Bin Sabir","Divya Rathore","Lav R. Khot","Chakradhar Mattupalli","Manoj Karkee"],"url":"https://arxiv.org/abs/2505.00774"}
{"created":"2025-05-05","title":"Reasoning Capabilities and Invariability of Large Language Models","abstract":"Large Language Models (LLMs) have shown remarkable capabilities in manipulating natural language across multiple applications, but their ability to handle simple reasoning tasks is often questioned. In this work, we aim to provide a comprehensive analysis of LLMs' reasoning competence, specifically focusing on their prompt dependency. In particular, we introduce a new benchmark dataset with a series of simple reasoning questions demanding shallow logical reasoning. Aligned with cognitive psychology standards, the questions are confined to a basic domain revolving around geometric figures, ensuring that responses are independent of any pre-existing intuition about the world and rely solely on deduction. An empirical analysis involving zero-shot and few-shot prompting across 24 LLMs of different sizes reveals that, while LLMs with over 70 billion parameters perform better in the zero-shot setting, there is still a large room for improvement. An additional test with chain-of-thought prompting over 22 LLMs shows that this additional prompt can aid or damage the performance of models, depending on whether the rationale is required before or after the answer.","authors":["Alessandro Raganato","Rafael Pe\\~naloza","Marco Viviani","Gabriella Pasi"],"url":"https://arxiv.org/abs/2505.00776"}
{"created":"2025-05-05","title":"Uncertainty-aware Latent Safety Filters for Avoiding Out-of-Distribution Failures","abstract":"Recent advances in generative world models have enabled classical safe control methods, such as Hamilton-Jacobi (HJ) reachability, to generalize to complex robotic systems operating directly from high-dimensional sensor observations. However, obtaining comprehensive coverage of all safety-critical scenarios during world model training is extremely challenging. As a result, latent safety filters built on top of these models may miss novel hazards and even fail to prevent known ones, overconfidently misclassifying risky out-of-distribution (OOD) situations as safe. To address this, we introduce an uncertainty-aware latent safety filter that proactively steers robots away from both known and unseen failures. Our key idea is to use the world model's epistemic uncertainty as a proxy for identifying unseen potential hazards. We propose a principled method to detect OOD world model predictions by calibrating an uncertainty threshold via conformal prediction. By performing reachability analysis in an augmented state space-spanning both the latent representation and the epistemic uncertainty-we synthesize a latent safety filter that can reliably safeguard arbitrary policies from both known and unseen safety hazards. In simulation and hardware experiments on vision-based control tasks with a Franka manipulator, we show that our uncertainty-aware safety filter preemptively detects potential unsafe scenarios and reliably proposes safe, in-distribution actions. Video results can be found on the project website at https://cmu-intentlab.github.io/UNISafe","authors":["Junwon Seo","Kensuke Nakamura","Andrea Bajcsy"],"url":"https://arxiv.org/abs/2505.00779"}
{"created":"2025-05-05","title":"Promises Made, Promises Kept: Safe Pareto Improvements via Ex Post Verifiable Commitments","abstract":"A safe Pareto improvement (SPI) [41] is a modification of a game that leaves all players better off with certainty. SPIs are typically proven under qualitative assumptions about the way different games are played. For example, we assume that strictly dominated strategies can be iteratively removed and that isomorphic games are played isomorphically. In this work, we study SPIs achieved through three types of ex post verifiable commitments -- promises about player behavior from which deviations can be detected by observing the game. First, we consider disarmament -- commitments not to play certain actions. Next, we consider SPIs based on token games. A token game is a game played by simply announcing an action (via cheap talk). As such, its outcome is intrinsically meaningless. However, we assume the players commit in advance to play specific (pure or correlated) strategy profiles in the original game as a function of the token game outcome. Under such commitments, the token game becomes a new, meaningful normal-form game. Finally, we consider default-conditional commitment: SPIs in settings where the players' default ways of playing the original game can be credibly revealed and hence the players can commit to act as a function of this default. We characterize the complexity of deciding whether SPIs exist in all three settings, giving a mixture of characterizations and efficient algorithms and NP- and Graph Isomorphism-hardness results.","authors":["Nathaniel Sauerberg","Caspar Oesterheld"],"url":"https://arxiv.org/abs/2505.00783"}
{"created":"2025-05-05","title":"Reconfigurable legged metamachines that run on autonomous modular legs","abstract":"Legged machines are becoming increasingly agile and adaptive but they have so far lacked the basic reconfigurability of legged animals, which have been rearranged and reshaped to fill millions of niches. Unlike their biological counterparts, legged machines have largely converged over the past decade to canonical quadrupedal and bipedal architectures that cannot be easily reconfigured to meet new tasks or recover from injury. Here we introduce autonomous modular legs: agile yet minimal, single-degree-of-freedom jointed links that can learn complex dynamic behaviors and may be freely attached to form legged metamachines at the meter scale. This enables rapid repair, redesign, and recombination of highly-dynamic modular agents that move quickly and acrobatically (non-quasistatically) through unstructured environments. Because each module is itself a complete agent, legged metamachines are able to sustain deep structural damage that would completely disable other legged robots. We also show how to encode the vast space of possible body configurations into a compact latent design genome that can be efficiently explored, revealing a wide diversity of novel legged forms.","authors":["Chen Yu","David Matthews","Jingxian Wang","Jing Gu","Douglas Blackiston","Michael Rubenstein","Sam Kriegman"],"url":"https://arxiv.org/abs/2505.00784"}
{"created":"2025-05-05","title":"AI-ready Snow Radar Echogram Dataset (SRED) for climate change monitoring","abstract":"Tracking internal layers in radar echograms with high accuracy is essential for understanding ice sheet dynamics and quantifying the impact of accelerated ice discharge in Greenland and other polar regions due to contemporary global climate warming. Deep learning algorithms have become the leading approach for automating this task, but the absence of a standardized and well-annotated echogram dataset has hindered the ability to test and compare algorithms reliably, limiting the advancement of state-of-the-art methods for the radar echogram layer tracking problem. This study introduces the first comprehensive ``deep learning ready'' radar echogram dataset derived from Snow Radar airborne data collected during the National Aeronautics and Space Administration Operation Ice Bridge (OIB) mission in 2012. The dataset contains 13,717 labeled and 57,815 weakly-labeled echograms covering diverse snow zones (dry, ablation, wet) with varying along-track resolutions. To demonstrate its utility, we evaluated the performance of five deep learning models on the dataset. Our results show that while current computer vision segmentation algorithms can identify and track snow layer pixels in echogram images, advanced end-to-end models are needed to directly extract snow depth and annual accumulation from echograms, reducing or eliminating post-processing. The dataset and accompanying benchmarking framework provide a valuable resource for advancing radar echogram layer tracking and snow accumulation estimation, advancing our understanding of polar ice sheets response to climate warming.","authors":["Oluwanisola Ibikunle","Hara Talasila","Debvrat Varshney","Jilu Li","John Paden","Maryam Rahnemoonfar"],"url":"https://arxiv.org/abs/2505.00786"}
{"created":"2025-05-05","title":"Constructing an Optimal Behavior Basis for the Option Keyboard","abstract":"Multi-task reinforcement learning aims to quickly identify solutions for new tasks with minimal or no additional interaction with the environment. Generalized Policy Improvement (GPI) addresses this by combining a set of base policies to produce a new one that is at least as good -- though not necessarily optimal -- as any individual base policy. Optimality can be ensured, particularly in the linear-reward case, via techniques that compute a Convex Coverage Set (CCS). However, these are computationally expensive and do not scale to complex domains. The Option Keyboard (OK) improves upon GPI by producing policies that are at least as good -- and often better. It achieves this through a learned meta-policy that dynamically combines base policies. However, its performance critically depends on the choice of base policies. This raises a key question: is there an optimal set of base policies -- an optimal behavior basis -- that enables zero-shot identification of optimal solutions for any linear tasks? We solve this open problem by introducing a novel method that efficiently constructs such an optimal behavior basis. We show that it significantly reduces the number of base policies needed to ensure optimality in new tasks. We also prove that it is strictly more expressive than a CCS, enabling particular classes of non-linear tasks to be solved optimally. We empirically evaluate our technique in challenging domains and show that it outperforms state-of-the-art approaches, increasingly so as task complexity increases.","authors":["Lucas N. Alegre","Ana L. C. Bazzan","Andr\\'e Barreto","Bruno C. da Silva"],"url":"https://arxiv.org/abs/2505.00787"}
{"created":"2025-05-05","title":"SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models","abstract":"Humans naturally understand 3D spatial relationships, enabling complex reasoning like predicting collisions of vehicles from different directions. Current large multimodal models (LMMs), however, lack of this capability of 3D spatial reasoning. This limitation stems from the scarcity of 3D training data and the bias in current model designs toward 2D data. In this paper, we systematically study the impact of 3D-informed data, architecture, and training setups, introducing SpatialLLM, a large multi-modal model with advanced 3D spatial reasoning abilities. To address data limitations, we develop two types of 3D-informed training datasets: (1) 3D-informed probing data focused on object's 3D location and orientation, and (2) 3D-informed conversation data for complex spatial relationships. Notably, we are the first to curate VQA data that incorporate 3D orientation relationships on real images. Furthermore, we systematically integrate these two types of training data with the architectural and training designs of LMMs, providing a roadmap for optimal design aimed at achieving superior 3D reasoning capabilities. Our SpatialLLM advances machines toward highly capable 3D-informed reasoning, surpassing GPT-4o performance by 8.7%. Our systematic empirical design and the resulting findings offer valuable insights for future research in this direction.","authors":["Wufei Ma","Luoxin Ye","Nessa McWeeney","Celso M de Melo","Alan Yuille","Jieneng Chen"],"url":"https://arxiv.org/abs/2505.00788"}
{"created":"2025-05-05","title":"Improving Routing in Sparse Mixture of Experts with Graph of Tokens","abstract":"Sparse Mixture of Experts (SMoE) has emerged as a key to achieving unprecedented scalability in deep learning. By activating only a small subset of parameters per sample, SMoE achieves an exponential increase in parameter counts while maintaining a constant computational overhead. However, SMoE models are susceptible to routing fluctuations--changes in the routing of a given input to its target expert--at the late stage of model training, leading to model non-robustness. In this work, we unveil the limitation of SMoE through the perspective of the probabilistic graphical model (PGM). Through this PGM framework, we highlight the independence in the expert-selection of tokens, which exposes the model to routing fluctuation and non-robustness. Alleviating this independence, we propose the novel Similarity-Aware (S)MoE, which considers interactions between tokens during expert selection. We then derive a new PGM underlying an (S)MoE-Attention block, going beyond just a single (S)MoE layer. Leveraging the token similarities captured by the attention matrix, we propose the innovative Attention-Aware (S)MoE, which employs the attention matrix to guide the routing of tokens to appropriate experts in (S)MoE. We theoretically prove that Similarity/Attention-Aware routing help reduce the entropy of expert selection, resulting in more stable token routing mechanisms. We empirically validate our models on various tasks and domains, showing significant improvements in reducing routing fluctuations, enhancing accuracy, and increasing model robustness over the baseline MoE-Transformer with token routing via softmax gating.","authors":["Tam Nguyen","Ngoc N. Tran","Khai Nguyen","Richard G. Baraniuk"],"url":"https://arxiv.org/abs/2505.00792"}
{"created":"2025-05-05","title":"Scalable Meta-Learning via Mixed-Mode Differentiation","abstract":"Gradient-based bilevel optimisation is a powerful technique with applications in hyperparameter optimisation, task adaptation, algorithm discovery, meta-learning more broadly, and beyond. It often requires differentiating through the gradient-based optimisation process itself, leading to \"gradient-of-a-gradient\" calculations with computationally expensive second-order and mixed derivatives. While modern automatic differentiation libraries provide a convenient way to write programs for calculating these derivatives, they oftentimes cannot fully exploit the specific structure of these problems out-of-the-box, leading to suboptimal performance. In this paper, we analyse such cases and propose Mixed-Flow Meta-Gradients, or MixFlow-MG -- a practical algorithm that uses mixed-mode differentiation to construct more efficient and scalable computational graphs yielding over 10x memory and up to 25% wall-clock time improvements over standard implementations in modern meta-learning setups.","authors":["Iurii Kemaev","Dan A Calian","Luisa M Zintgraf","Gregory Farquhar","Hado van Hasselt"],"url":"https://arxiv.org/abs/2505.00793"}
{"created":"2025-05-05","title":"Howard's Policy Iteration is Subexponential for Deterministic Markov Decision Problems with Rewards of Fixed Bit-size and Arbitrary Discount Factor","abstract":"Howard's Policy Iteration (HPI) is a classic algorithm for solving Markov Decision Problems (MDPs). HPI uses a \"greedy\" switching rule to update from any non-optimal policy to a dominating one, iterating until an optimal policy is found. Despite its introduction over 60 years ago, the best-known upper bounds on HPI's running time remain exponential in the number of states -- indeed even on the restricted class of MDPs with only deterministic transitions (DMDPs). Meanwhile, the tightest lower bound for HPI for MDPs with a constant number of actions per state is only linear. In this paper, we report a significant improvement: a subexponential upper bound for HPI on DMDPs, which is parameterised by the bit-size of the rewards, while independent of the discount factor. The same upper bound also applies to DMDPs with only two possible rewards (which may be of arbitrary size).","authors":["Dibyangshu Mukherjee","Shivaram Kalyanakrishnan"],"url":"https://arxiv.org/abs/2505.00795"}
{"created":"2025-05-05","title":"A New Semi-Discrete Finite-Volume Active Flux Method for Hyperbolic Conservation Laws","abstract":"In this work, we introduce a new active flux (AF) method for hyperbolic systems of conservation laws. Following an AF approach recently proposed in [{\\sc R. Abgrall}, Commun. Appl. Math. Comput., 5 (2023), pp. 370--402], we consider two different formulations of the studied system (the original conservative formulation and a primitive one containing nonconservative products), and discretize them on overlapping staggered meshes using two different numerical schemes. The novelty of our method is twofold. First, we introduce an original paradigm making use of overlapping finite-volume (FV) meshes over which cell averages of conservative and primitive variables are evolved using semi-discrete FV methods: The nonconservative system is discretized by a path-conservative central-upwind scheme and its solution is used to evaluate very simple numerical fluxes for the discretization of the original conservative system. Second, to ensure the nonlinear stability of the resulting AF method, we design a post-processing, which also guarantees a conservative coupling between the two sets of variables. We test the proposed semi-discrete FV AF method on a number of benchmarks for the one- and two-dimensional Euler equations of gas dynamics.","authors":["R\\'emi Abgrall","Alina Chertock","Alexander Kurganov","Lorenzo Micalizzi"],"url":"https://arxiv.org/abs/2505.00798"}
{"created":"2025-05-05","title":"Explanations as Bias Detectors: A Critical Study of Local Post-hoc XAI Methods for Fairness Exploration","abstract":"As Artificial Intelligence (AI) is increasingly used in areas that significantly impact human lives, concerns about fairness and transparency have grown, especially regarding their impact on protected groups. Recently, the intersection of explainability and fairness has emerged as an important area to promote responsible AI systems. This paper explores how explainability methods can be leveraged to detect and interpret unfairness. We propose a pipeline that integrates local post-hoc explanation methods to derive fairness-related insights. During the pipeline design, we identify and address critical questions arising from the use of explanations as bias detectors such as the relationship between distributive and procedural fairness, the effect of removing the protected attribute, the consistency and quality of results across different explanation methods, the impact of various aggregation strategies of local explanations on group fairness evaluations, and the overall trustworthiness of explanations as bias detectors. Our results show the potential of explanation methods used for fairness while highlighting the need to carefully consider the aforementioned critical aspects.","authors":["Vasiliki Papanikou","Danae Pla Karidi","Evaggelia Pitoura","Emmanouil Panagiotou","Eirini Ntoutsi"],"url":"https://arxiv.org/abs/2505.00802"}
{"created":"2025-05-05","title":"To Repair or Not to Repair? Investigating the Importance of AB-Cycles for the State-of-the-Art TSP Heuristic EAX","abstract":"The Edge Assembly Crossover (EAX) algorithm is the state-of-the-art heuristic for solving the Traveling Salesperson Problem (TSP). It regularly outperforms other methods, such as the Lin-Kernighan-Helsgaun heuristic (LKH), across diverse sets of TSP instances. Essentially, EAX employs a two-stage mechanism that focuses on improving the current solutions, first, at the local and, subsequently, at the global level. Although the second phase of the algorithm has been thoroughly studied, configured, and refined in the past, in particular, its first stage has hardly been examined.","authors":["Jonathan Heins","Darrell Whitley","Pascal Kerschke"],"url":"https://arxiv.org/abs/2505.00803"}
{"created":"2025-05-05","title":"Improved Approximation of Sensor Network Performance for Seabed Acoustic Sensors","abstract":"Sensor locations to detect Poisson-distributed targets, such as seabed sensors that detect shipping traffic, can be selected to maximize the so-called void probability, which is the probability of detecting all targets. Because evaluation of void probability is computationally expensive, we propose a new approximation of void probability that can greatly reduce the computational cost of selecting locations for a network of sensors. We build upon prior work that approximates void probability using Jensen's inequality. Our new approach better accommodates uncertainty in the (Poisson) target model and yields a sharper error bound. The proposed method is evaluated using historical ship traffic data from the Hampton Roads Channel, Virginia, demonstrating a reduction in the approximation error compared to the previous approach. The results validate the effectiveness of the improved approximation for maritime surveillance applications.","authors":["Mingyu Kim","Daniel J. Stilwell","Harun Yetkin","Jorge Jimenez"],"url":"https://arxiv.org/abs/2505.00804"}
{"created":"2025-05-05","title":"Advancing Wheat Crop Analysis: A Survey of Deep Learning Approaches Using Hyperspectral Imaging","abstract":"As one of the most widely cultivated and consumed crops, wheat is essential to global food security. However, wheat production is increasingly challenged by pests, diseases, climate change, and water scarcity, threatening yields. Traditional crop monitoring methods are labor-intensive and often ineffective for early issue detection. Hyperspectral imaging (HSI) has emerged as a non-destructive and efficient technology for remote crop health assessment. However, the high dimensionality of HSI data and limited availability of labeled samples present notable challenges. In recent years, deep learning has shown great promise in addressing these challenges due to its ability to extract and analysis complex structures. Despite advancements in applying deep learning methods to HSI data for wheat crop analysis, no comprehensive survey currently exists in this field. This review addresses this gap by summarizing benchmark datasets, tracking advancements in deep learning methods, and analyzing key applications such as variety classification, disease detection, and yield estimation. It also highlights the strengths, limitations, and future opportunities in leveraging deep learning methods for HSI-based wheat crop analysis. We have listed the current state-of-the-art papers and will continue tracking updating them in the following https://github.com/fadi-07/Awesome-Wheat-HSI-DeepLearning.","authors":["Fadi Abdeladhim Zidi","Abdelkrim Ouafi","Fares Bougourzi","Cosimo Distante","Abdelmalik Taleb-Ahmed"],"url":"https://arxiv.org/abs/2505.00805"}
{"created":"2025-05-05","title":"E-Graphs With Bindings","abstract":"Equality saturation, a technique for program optimisation and reasoning, has gained attention due to the resurgence of equality graphs (e-graphs). E-graphs represent equivalence classes of terms under rewrite rules, enabling simultaneous rewriting across a family of terms. However, they struggle in domains like $\\lambda$-calculus that involve variable binding, due to a lack of native support for bindings. Building on recent work interpreting e-graphs categorically as morphisms in semilattice-enriched symmetric monoidal categories, we extend this framework to closed symmetric monoidal categories to handle bindings. We provide a concrete combinatorial representation using hierarchical hypergraphs and introduce a corresponding double-pushout (DPO) rewriting mechanism. Finally, we establish the equivalence of term rewriting and DPO rewriting, with the key property that the combinatorial representation absorbs the equations of the symmetric monoidal category.","authors":["Aleksei Tiurin","Dan R. Ghica","Nick Hu"],"url":"https://arxiv.org/abs/2505.00807"}
{"created":"2025-05-05","title":"A Mathematical Philosophy of Explanations in Mechanistic Interpretability -- The Strange Science Part I.i","abstract":"Mechanistic Interpretability aims to understand neural networks through causal explanations. We argue for the Explanatory View Hypothesis: that Mechanistic Interpretability research is a principled approach to understanding models because neural networks contain implicit explanations which can be extracted and understood. We hence show that Explanatory Faithfulness, an assessment of how well an explanation fits a model, is well-defined. We propose a definition of Mechanistic Interpretability (MI) as the practice of producing Model-level, Ontic, Causal-Mechanistic, and Falsifiable explanations of neural networks, allowing us to distinguish MI from other interpretability paradigms and detail MI's inherent limits. We formulate the Principle of Explanatory Optimism, a conjecture which we argue is a necessary precondition for the success of Mechanistic Interpretability.","authors":["Kola Ayonrinde","Louis Jaburi"],"url":"https://arxiv.org/abs/2505.00808"}
{"created":"2025-05-05","title":"New Smoothness Indicator Within an Active Flux Framework","abstract":"In this work, we introduce a new smoothness indicator (SI), which is capable of detecting ``rough'' parts of the solutions computed by active flux (AF) methods for hyperbolic (systems of) conservation laws. The new SI is based on measuring the difference between the two sets of solutions (either cell averages and point values or cell averages on overlapping grids) evolved at each time step of AF methods. The key idea in the derivation of the new SI is that in the ``rough'' parts of the evolved solutions, the difference is ${\\cal O}(1)$, while in the smooth areas, it is proportional to the order of the underlying AF method. The performance of the new SI, that is, its ability to automatically and robustly detect ``rough'' parts of the computed solutions, is illustrated on several numerical examples, in which the one-dimensional Euler equations of gas dynamics are numerically solved by a recently introduced semi-discrete finite-volume AF method on overlapping grids.","authors":["Alina Chertock","Alexander Kurganov","Lorenzo Micalizzi"],"url":"https://arxiv.org/abs/2505.00809"}
{"created":"2025-05-05","title":"Scalable Unit Harmonization in Medical Informatics Using Bi-directional Transformers and Bayesian-Optimized BM25 and Sentence Embedding Retrieval","abstract":"Objective: To develop and evaluate a scalable methodology for harmonizing inconsistent units in large-scale clinical datasets, addressing a key barrier to data interoperability.","authors":["Jordi de la Torre"],"url":"https://arxiv.org/abs/2505.00810"}
{"created":"2025-05-05","title":"Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization","abstract":"Recent studies indicate that deep neural networks degrade in generalization performance under noisy supervision. Existing methods focus on isolating clean subsets or correcting noisy labels, facing limitations such as high computational costs, heavy hyperparameter tuning process, and coarse-grained optimization. To address these challenges, we propose a novel two-stage noisy learning framework that enables instance-level optimization through a dynamically weighted loss function, avoiding hyperparameter tuning. To obtain stable and accurate information about noise modeling, we introduce a simple yet effective metric, termed wrong event, which dynamically models the cleanliness and difficulty of individual samples while maintaining computational costs. Our framework first collects wrong event information and builds a strong base model. Then we perform noise-robust training on the base model, using a probabilistic model to handle the wrong event information of samples. Experiments on five synthetic and real-world LNL benchmarks demonstrate our method surpasses state-of-the-art methods in performance, achieves a nearly 75% reduction in computational time and improves model scalability.","authors":["Kuan Zhang","Chengliang Chai","Jingzhe Xu","Chi Zhang","Ye Yuan","Guoren Wang","Lei Cao"],"url":"https://arxiv.org/abs/2505.00812"}
{"created":"2025-05-05","title":"Knowledge-augmented Pre-trained Language Models for Biomedical Relation Extraction","abstract":"Automatic relationship extraction (RE) from biomedical literature is critical for managing the vast amount of scientific knowledge produced each year. In recent years, utilizing pre-trained language models (PLMs) has become the prevalent approach in RE. Several studies report improved performance when incorporating additional context information while fine-tuning PLMs for RE. However, variations in the PLMs applied, the databases used for augmentation, hyper-parameter optimization, and evaluation methods complicate direct comparisons between studies and raise questions about the generalizability of these findings. Our study addresses this research gap by evaluating PLMs enhanced with contextual information on five datasets spanning four relation scenarios within a consistent evaluation framework. We evaluate three baseline PLMs and first conduct extensive hyperparameter optimization. After selecting the top-performing model, we enhance it with additional data, including textual entity descriptions, relational information from knowledge graphs, and molecular structure encodings. Our findings illustrate the importance of i) the choice of the underlying language model and ii) a comprehensive hyperparameter optimization for achieving strong extraction performance. Although inclusion of context information yield only minor overall improvements, an ablation study reveals substantial benefits for smaller PLMs when such external data was included during fine-tuning.","authors":["Mario S\\\"anger","Ulf Leser"],"url":"https://arxiv.org/abs/2505.00814"}
{"created":"2025-05-05","title":"Aggregating empirical evidence from data strategy studies: a case on model quantization","abstract":"Background: As empirical software engineering evolves, more studies adopt data strategies$-$approaches that investigate digital artifacts such as models, source code, or system logs rather than relying on human subjects. Synthesizing results from such studies introduces new methodological challenges.","authors":["Santiago del Rey","Paulo S\\'ergio Medeiros dos Santos","Guilherme Horta Travassos","Xavier Franch","Silverio Mart\\'inez-Fern\\'andez"],"url":"https://arxiv.org/abs/2505.00816"}
{"created":"2025-05-05","title":"Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from Large Language Models","abstract":"Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.","authors":["Andrew Adiletta","Berk Sunar"],"url":"https://arxiv.org/abs/2505.00817"}
{"created":"2025-05-05","title":"Dual Filter: A Mathematical Framework for Inference using Transformer-like Architectures","abstract":"This paper presents a mathematical framework for causal nonlinear prediction in settings where observations are generated from an underlying hidden Markov model (HMM). Both the problem formulation and the proposed solution are motivated by the decoder-only transformer architecture, in which a finite sequence of observations (tokens) is mapped to the conditional probability of the next token. Our objective is not to construct a mathematical model of a transformer. Rather, our interest lies in deriving, from first principles, transformer-like architectures that solve the prediction problem for which the transformer is designed. The proposed framework is based on an original optimal control approach, where the prediction objective (MMSE) is reformulated as an optimal control problem. An analysis of the optimal control problem is presented leading to a fixed-point equation on the space of probability measures. To solve the fixed-point equation, we introduce the dual filter, an iterative algorithm that closely parallels the architecture of decoder-only transformers. These parallels are discussed in detail along with the relationship to prior work on mathematical modeling of transformers as transport on the space of probability measures. Numerical experiments are provided to illustrate the performance of the algorithm using parameter values used in researchscale transformer models.","authors":["Heng-Sheng Chang","Prashant G. Mehta"],"url":"https://arxiv.org/abs/2505.00818"}
{"created":"2025-05-05","title":"HMCF: A Human-in-the-loop Multi-Robot Collaboration Framework Based on Large Language Models","abstract":"Rapid advancements in artificial intelligence (AI) have enabled robots to performcomplex tasks autonomously with increasing precision. However, multi-robot systems (MRSs) face challenges in generalization, heterogeneity, and safety, especially when scaling to large-scale deployments like disaster response. Traditional approaches often lack generalization, requiring extensive engineering for new tasks and scenarios, and struggle with managing diverse robots. To overcome these limitations, we propose a Human-in-the-loop Multi-Robot Collaboration Framework (HMCF) powered by large language models (LLMs). LLMs enhance adaptability by reasoning over diverse tasks and robot capabilities, while human oversight ensures safety and reliability, intervening only when necessary. Our framework seamlessly integrates human oversight, LLM agents, and heterogeneous robots to optimize task allocation and execution. Each robot is equipped with an LLM agent capable of understanding its capabilities, converting tasks into executable instructions, and reducing hallucinations through task verification and human supervision. Simulation results show that our framework outperforms state-of-the-art task planning methods, achieving higher task success rates with an improvement of 4.76%. Real-world tests demonstrate its robust zero-shot generalization feature and ability to handle diverse tasks and environments with minimal human intervention.","authors":["Zhaoxing Li","Wenbo Wu","Yue Wang","Yanran Xu","William Hunt","Sebastian Stein"],"url":"https://arxiv.org/abs/2505.00820"}
{"created":"2025-05-05","title":"Should AI Mimic People? Understanding AI-Supported Writing Technology Among Black Users","abstract":"AI-supported writing technologies (AISWT) that provide grammatical suggestions, autocomplete sentences, or generate and rewrite text are now a regular feature integrated into many people's workflows. However, little is known about how people perceive the suggestions these tools provide. In this paper, we investigate how Black American users perceive AISWT, motivated by prior findings in natural language processing that highlight how the underlying large language models can contain racial biases. Using interviews and observational user studies with 13 Black American users of AISWT, we found a strong tradeoff between the perceived benefits of using AISWT to enhance their writing style and feeling like \"it wasn't built for us\". Specifically, participants reported AISWT's failure to recognize commonly used names and expressions in African American Vernacular English, experiencing its corrections as hurtful and alienating and fearing it might further minoritize their culture. We end with a reflection on the tension between AISWT that fail to include Black American culture and language, and AISWT that attempt to mimic it, with attention to accuracy, authenticity, and the production of social difference.","authors":["Jeffrey Basoah","Jay L. Cunningham","Erica Adams","Alisha Bose","Aditi Jain","Kaustubh Yadav","Zhengyang Yang","Katharina Reinecke","Daniela Rosner"],"url":"https://arxiv.org/abs/2505.00821"}
{"created":"2025-05-05","title":"Data-Driven Optical To Thermal Inference in Pool Boiling Using Generative Adversarial Networks","abstract":"Phase change plays a critical role in thermal management systems, yet quantitative characterization of multiphase heat transfer remains limited by the challenges of measuring temperature fields in chaotic, rapidly evolving flow regimes. While computational methods offer spatiotemporal resolution in idealized cases, replicating complex experimental conditions remains prohibitively difficult. Here, we present a data-driven framework that leverages a conditional generative adversarial network (CGAN) to infer temperature fields from geometric phase contours in a canonical pool boiling configuration where advanced data collection techniques are restricted. Using high-speed imaging data and simulation-informed training, our model demonstrates the ability to reconstruct temperature fields with errors below 6%. We further show that standard data augmentation strategies are effective in enhancing both accuracy and physical plausibility of the predicted maps across both simulation and experimental datasets when precise physical constraints are not applicable. Our results highlight the potential of deep generative models to bridge the gap between observable multiphase phenomena and underlying thermal transport, offering a powerful approach to augment and interpret experimental measurements in complex two-phase systems.","authors":["Qianxi Fu","Youngjoon Suh","Xiaojing Zhang","Yoonjin Won"],"url":"https://arxiv.org/abs/2505.00823"}
{"created":"2025-05-05","title":"Near-optimal Sensor Placement for Detecting Stochastic Target Trajectories in Barrier Coverage Systems","abstract":"This paper addresses the deployment of sensors for a 2-D barrier coverage system. The challenge is to compute near-optimal sensor placements for detecting targets whose trajectories follow a log-Gaussian Cox line process. We explore sensor deployment in a transformed space, where linear target trajectories are represented as points. While this space simplifies handling the line process, the spatial functions representing sensor performance (i.e. probability of detection) become less intuitive. To illustrate our approach, we focus on positioning sensors of the barrier coverage system on the seafloor to detect passing ships. Through numerical experiments using historical ship data, we compute sensor locations that maximize the probability all ship passing over the barrier coverage system are detected.","authors":["Mingyu Kim","Daniel J. Stilwell","Harun Yetkin","Jorge Jimenez"],"url":"https://arxiv.org/abs/2505.00825"}
{"created":"2025-05-05","title":"MIMIC-\\RNum{4}-Ext-22MCTS: A 22 Millions-Event Temporal Clinical Time-Series Dataset with Relative Timestamp for Risk Prediction","abstract":"Clinical risk prediction based on machine learning algorithms plays a vital role in modern healthcare. A crucial component in developing a reliable prediction model is collecting high-quality time series clinical events. In this work, we release such a dataset that consists of 22,588,586 Clinical Time Series events, which we term MIMIC-\\RNum{4}-Ext-22MCTS. Our source data are discharge summaries selected from the well-known yet unstructured MIMIC-IV-Note \\cite{Johnson2023-pg}. We then extract clinical events as short text span from the discharge summaries, along with the timestamps of these events as temporal information. The general-purpose MIMIC-IV-Note pose specific challenges for our work: it turns out that the discharge summaries are too lengthy for typical natural language models to process, and the clinical events of interest often are not accompanied with explicit timestamps. Therefore, we propose a new framework that works as follows: 1) we break each discharge summary into manageably small text chunks; 2) we apply contextual BM25 and contextual semantic search to retrieve chunks that have a high potential of containing clinical events; and 3) we carefully design prompts to teach the recently released Llama-3.1-8B \\cite{touvron2023llama} model to identify or infer temporal information of the chunks. We show that the obtained dataset is so informative and transparent that standard models fine-tuned on our dataset are achieving significant improvements in healthcare applications. In particular, the BERT model fine-tuned based on our dataset achieves 10\\% improvement in accuracy on medical question answering task, and 3\\% improvement in clinical trial matching task compared with the classic BERT. The GPT-2 model, fine-tuned on our dataset, produces more clinically reliable results for clinical questions.","authors":["Jing Wang","Xing Niu","Juyong Kim","Jie Shen","Tong Zhang","Jeremy C. Weiss"],"url":"https://arxiv.org/abs/2505.00827"}
{"created":"2025-05-05","title":"Intersectional Divergence: Measuring Fairness in Regression","abstract":"Research on fairness in machine learning has been mainly framed in the context of classification tasks, leaving critical gaps in regression. In this paper, we propose a seminal approach to measure intersectional fairness in regression tasks, going beyond the focus on single protected attributes from existing work to consider combinations of all protected attributes. Furthermore, we contend that it is insufficient to measure the average error of groups without regard for imbalanced domain preferences. To this end, we propose Intersectional Divergence (ID) as the first fairness measure for regression tasks that 1) describes fair model behavior across multiple protected attributes and 2) differentiates the impact of predictions in target ranges most relevant to users. We extend our proposal demonstrating how ID can be adapted into a loss function, IDLoss, and used in optimization problems. Through an extensive experimental evaluation, we demonstrate how ID allows unique insights into model behavior and fairness, and how incorporating IDLoss into optimization can considerably improve single-attribute and intersectional model fairness while maintaining a competitive balance in predictive performance.","authors":["Joe Germino","Nuno Moniz","Nitesh V. Chawla"],"url":"https://arxiv.org/abs/2505.00830"}
{"created":"2025-05-05","title":"SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation","abstract":"Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan -- a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics.","authors":["Quang P. M. Pham","Khoi T. N. Nguyen","Nhi H. Doan","Cuong A. Pham","Kentaro Inui","Dezhen Song"],"url":"https://arxiv.org/abs/2505.00831"}
{"created":"2025-05-05","title":"The Comparability of Model Fusion to Measured Data in Confuser Rejection","abstract":"Data collection has always been a major issue in the modeling and training of large deep learning networks, as no dataset can account for every slight deviation we might see in live usage. Collecting samples can be especially costly for Synthetic Aperture Radar (SAR), limiting the amount of unique targets and operating conditions we are able to observe from. To counter this lack of data, simulators have been developed utilizing the shooting and bouncing ray method to allow for the generation of synthetic SAR data on 3D models. While effective, the synthetically generated data does not perfectly correlate to the measured data leading to issues when training models solely on synthetic data. We aim to use computational power as a substitution for this lack of quality measured data, by ensembling many models trained on synthetic data. Synthetic data is also not complete, as we do not know what targets might be present in a live environment. Therefore we need to have our ensembling techniques account for these unknown targets by applying confuser rejection in which our models will reject unknown targets it is presented with, and only classify those it has been trained on.","authors":["Conor Flynn","Christopher Ebersole","Edmund Zelnio"],"url":"https://arxiv.org/abs/2505.00836"}
{"created":"2025-05-05","title":"IberFire -- a detailed creation of a spatio-temporal dataset for wildfire risk assessment in Spain","abstract":"Wildfires pose a critical environmental issue to ecosystems, economies, and public safety, particularly in Mediterranean regions such as Spain. Accurate predictive models rely on high-resolution spatio-temporal data to capture the complex interplay of environmental and anthropogenic factors. To address the lack of localised and fine-grained datasets in Spain, this work introduces IberFire, a spatio-temporal datacube at 1 km x 1 km x 1-day resolution covering mainland Spain and the Balearic Islands from December 2007 to December 2024. IberFire integrates 260 features across eight main categories: auxiliary features, fire history, geography, topography, meteorology, vegetation indices, human activity, and land cover. All features are derived from open-access sources, ensuring transparency and real-time applicability. The data processing pipeline was implemented entirely using open-source tools, and the codebase has been made publicly available. This work not only enhances spatio-temporal granularity and feature diversity compared to existing European datacubes but also provides a reproducible methodology for constructing similar datasets. IberFire supports advanced wildfire risk modelling through Machine Learning (ML) and Deep Learning (DL) techniques, enables climate pattern analysis and informs strategic planning in fire prevention and land management. The dataset is publicly available on Zenodo to promote open research and collaboration.","authors":["Julen Ercibengoa","Meritxell G\\'omez-Omella","Izaro Goienetxea"],"url":"https://arxiv.org/abs/2505.00837"}
{"created":"2025-05-05","title":"A stabilized march approach to adjoint-based sensitivity analysis of chaotic flows","abstract":"Adjoint-based sensitivity analysis is of interest in computational science due to its ability to compute sensitivities at a lower cost with respect to several design parameters. However, conventional sensitivity analysis methods fail in the presence of chaotic flows. Popular approaches to chaotic sensitivity analysis of flows involve the use of the shadowing trajectory. The state-of-the-art approach computes the shadowing trajectory by solving a least squares minimization problem, resulting in a space-time linear system of equations. The current paper computes the adjoint shadowing trajectory using the stabilized march, by specifying the adjoint boundary conditions instead of solving a minimization problem. This approach results in a space-time linear system that can be solved through a single backward substitution of order $\\mathcal{O}(n_u^2)$ with $n_u$ being the dimension of the unstable subspace. It is proven to compute sensitivities that converge to the true sensitivity for large integration times and that the error in the sensitivity due to the discretization is of the order of the local truncation error of the scheme. The approach is numerically verified on the Lorentz 63 and Kuramoto-Sivasinsky equations.","authors":["Pranshul Thakur","Siva Nadarajah"],"url":"https://arxiv.org/abs/2505.00838"}
{"created":"2025-05-05","title":"SMSAT: A Multimodal Acoustic Dataset and Deep Contrastive Learning Framework for Affective and Physiological Modeling of Spiritual Meditation","abstract":"Understanding how auditory stimuli influence emotional and physiological states is fundamental to advancing affective computing and mental health technologies. In this paper, we present a multimodal evaluation of the affective and physiological impacts of three auditory conditions, that is, spiritual meditation (SM), music (M), and natural silence (NS), using a comprehensive suite of biometric signal measures. To facilitate this analysis, we introduce the Spiritual, Music, Silence Acoustic Time Series (SMSAT) dataset, a novel benchmark comprising acoustic time series (ATS) signals recorded under controlled exposure protocols, with careful attention to demographic diversity and experimental consistency. To model the auditory induced states, we develop a contrastive learning based SMSAT audio encoder that extracts highly discriminative embeddings from ATS data, achieving 99.99% classification accuracy in interclass and intraclass evaluations. Furthermore, we propose the Calmness Analysis Model (CAM), a deep learning framework integrating 25 handcrafted and learned features for affective state classification across auditory conditions, attaining robust 99.99% classification accuracy. In contrast, pairwise t tests reveal significant deviations in cardiac response characteristics (CRC) between SM analysis via ANOVA inducing more significant physiological fluctuations. Compared to existing state of the art methods reporting accuracies up to 90%, the proposed model demonstrates substantial performance gains (up to 99%). This work contributes a validated multimodal dataset and a scalable deep learning framework for affective computing applications in stress monitoring, mental well-being, and therapeutic audio-based interventions.","authors":["Ahmad Suleman","Yazeed Alkhrijah","Misha Urooj Khan","Hareem Khan","Muhammad Abdullah Husnain Ali Faiz","Mohamad A. Alawad","Zeeshan Kaleem","Guan Gui"],"url":"https://arxiv.org/abs/2505.00839"}
{"created":"2025-05-05","title":"From Texts to Shields: Convergence of Large Language Models and Cybersecurity","abstract":"This report explores the convergence of large language models (LLMs) and cybersecurity, synthesizing interdisciplinary insights from network security, artificial intelligence, formal methods, and human-centered design. It examines emerging applications of LLMs in software and network security, 5G vulnerability analysis, and generative security engineering. The report highlights the role of agentic LLMs in automating complex tasks, improving operational efficiency, and enabling reasoning-driven security analytics. Socio-technical challenges associated with the deployment of LLMs -- including trust, transparency, and ethical considerations -- can be addressed through strategies such as human-in-the-loop systems, role-specific training, and proactive robustness testing. The report further outlines critical research challenges in ensuring interpretability, safety, and fairness in LLM-based systems, particularly in high-stakes domains. By integrating technical advances with organizational and societal considerations, this report presents a forward-looking research agenda for the secure and effective adoption of LLMs in cybersecurity.","authors":["Tao Li","Ya-Ting Yang","Yunian Pan","Quanyan Zhu"],"url":"https://arxiv.org/abs/2505.00841"}
{"created":"2025-05-05","title":"Fault-Tolerant Multi-Modal Localization of Multi-Robots on Matrix Lie Groups","abstract":"Consistent localization of cooperative multi-robot systems during navigation presents substantial challenges. This paper proposes a fault-tolerant, multi-modal localization framework for multi-robot systems on matrix Lie groups. We introduce novel stochastic operations to perform composition, differencing, inversion, averaging, and fusion of correlated and non-correlated estimates on Lie groups, enabling pseudo-pose construction for filter updates. The method integrates a combination of proprioceptive and exteroceptive measurements from inertial, velocity, and pose (pseudo-pose) sensors on each robot in an Extended Kalman Filter (EKF) framework. The prediction step is conducted on the Lie group $\\mathbb{SE}_2(3) \\times \\mathbb{R}^3 \\times \\mathbb{R}^3$, where each robot's pose, velocity, and inertial measurement biases are propagated. The proposed framework uses body velocity, relative pose measurements from fiducial markers, and inter-robot communication to provide scalable EKF update across the network on the Lie group $\\mathbb{SE}(3) \\times \\mathbb{R}^3$. A fault detection module is implemented, allowing the integration of only reliable pseudo-pose measurements from fiducial markers. We demonstrate the effectiveness of the method through experiments with a network of wheeled mobile robots equipped with inertial measurement units, wheel odometry, and ArUco markers. The comparison results highlight the proposed method's real-time performance, superior efficiency, reliability, and scalability in multi-robot localization, making it well-suited for large-scale robotic systems.","authors":["Mahboubeh Zarei","Robin Chhabra"],"url":"https://arxiv.org/abs/2505.00842"}
{"created":"2025-05-05","title":"OET: Optimization-based prompt injection Evaluation Toolkit","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, enabling their widespread adoption across various domains. However, their susceptibility to prompt injection attacks poses significant security risks, as adversarial inputs can manipulate model behavior and override intended instructions. Despite numerous defense strategies, a standardized framework to rigorously evaluate their effectiveness, especially under adaptive adversarial scenarios, is lacking. To address this gap, we introduce OET, an optimization-based evaluation toolkit that systematically benchmarks prompt injection attacks and defenses across diverse datasets using an adaptive testing framework. Our toolkit features a modular workflow that facilitates adversarial string generation, dynamic attack execution, and comprehensive result analysis, offering a unified platform for assessing adversarial robustness. Crucially, the adaptive testing framework leverages optimization methods with both white-box and black-box access to generate worst-case adversarial examples, thereby enabling strict red-teaming evaluations. Extensive experiments underscore the limitations of current defense mechanisms, with some models remaining susceptible even after implementing security enhancements.","authors":["Jinsheng Pan","Xiaogeng Liu","Chaowei Xiao"],"url":"https://arxiv.org/abs/2505.00843"}
{"created":"2025-05-05","title":"SeLR: Sparsity-enhanced Lagrangian Relaxation for Computation Offloading at the Edge","abstract":"This paper introduces a novel computational approach for offloading sensor data processing tasks to servers in edge networks for better accuracy and makespan. A task is assigned with one of several offloading options, each comprises a server, a route for uploading data to the server, and a service profile that specifies the performance and resource consumption at the server and in the network. This offline offloading and routing problem is formulated as mixed integer programming (MIP), which is non-convex and HP-hard due to the discrete decision variables associated to the offloading options. The novelty of our approach is to transform this non-convex problem into iterative convex optimization by relaxing integer decision variables into continuous space, combining primal-dual optimization for penalizing constraint violations and reweighted $L_1$-minimization for promoting solution sparsity, which achieves better convergence through a smoother path in a continuous search space. Compared to existing greedy heuristics, our approach can achieve a better Pareto frontier in accuracy and latency, scales better to larger problem instances, and can achieve a 7.72--9.17$\\times$ reduction in computational overhead of scheduling compared to the optimal solver in hierarchically organized edge networks with 300 nodes and 50--100 tasks.","authors":["Negar Erfaniantaghvayi","Zhongyuan Zhao","Kevin Chan","Ananthram Swami","Santiago Segarra"],"url":"https://arxiv.org/abs/2505.00848"}
{"created":"2025-05-05","title":"TherMod Communication: Low Power or Hot Air?","abstract":"The Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme leverages statistical physics to enable secure communication with zero average power flow in a wired channel. While the original KLJN scheme requires significant power for operation, a recent wireless modification, TherMod, proposed by Basar claims a \"low power\" implementation. This paper critically examines this claim. We explain that the additional components inherent in Basar's wireless adaptation substantially increase power consumption, rendering the \"low power\" assertion inappropriate. Furthermore, we clarify that the security claims of the original KLJN scheme do not directly translate to this wireless adaptation, implying significant security breach. Finally, the scheme looks identical one of the stealth communicators from 2005, which was shown not to be secure.","authors":["Christiana Chamon"],"url":"https://arxiv.org/abs/2505.00849"}
{"created":"2025-05-05","title":"ICQuant: Index Coding enables Low-bit LLM Quantization","abstract":"The rapid deployment of Large Language Models (LLMs) highlights the need for efficient low-bit post-training quantization (PTQ), due to their high memory costs. A key challenge in weight quantization is the presence of outliers, which inflate quantization ranges and lead to large errors. While a number of outlier suppression techniques have been proposed, they either: fail to effectively shrink the quantization range, or incur (relatively) high bit overhead. In this paper, we present ICQuant, a novel framework that leverages outlier statistics to design an efficient index coding scheme for outlier-aware weight-only quantization. Compared to existing outlier suppression techniques requiring $\\approx 1$ bit overhead to halve the quantization range, ICQuant requires only $\\approx 0.3$ bits; a significant saving in extreme compression regimes (e.g., 2-3 bits per weight). ICQuant can be used on top of any existing quantizers to eliminate outliers, improving the quantization quality. Using just 2.3 bits per weight and simple scalar quantizers, ICQuant improves the zero-shot accuracy of the 2-bit Llama3-70B model by up to 130% and 150% relative to QTIP and QuIP#; and it achieves comparable performance to the best-known fine-tuned quantizer (PV-tuning) without fine-tuning.","authors":["Xinlin Li","Osama Hanna","Christina Fragouli","Suhas Diggavi"],"url":"https://arxiv.org/abs/2505.00850"}
{"created":"2025-05-05","title":"LLM Ethics Benchmark: A Three-Dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models","abstract":"This study establishes a novel framework for systematically evaluating the moral reasoning capabilities of large language models (LLMs) as they increasingly integrate into critical societal domains. Current assessment methodologies lack the precision needed to evaluate nuanced ethical decision-making in AI systems, creating significant accountability gaps. Our framework addresses this challenge by quantifying alignment with human ethical standards through three dimensions: foundational moral principles, reasoning robustness, and value consistency across diverse scenarios. This approach enables precise identification of ethical strengths and weaknesses in LLMs, facilitating targeted improvements and stronger alignment with societal values. To promote transparency and collaborative advancement in ethical AI development, we are publicly releasing both our benchmark datasets and evaluation codebase at https://github.com/ The-Responsible-AI-Initiative/LLM_Ethics_Benchmark.git.","authors":["Junfeng Jiao","Saleh Afroogh","Abhejay Murali","Kevin Chen","David Atkinson","Amit Dhurandhar"],"url":"https://arxiv.org/abs/2505.00853"}
{"created":"2025-05-05","title":"Beyond the Mirror: Personal Analytics through Visual Juxtaposition with Other People's Data","abstract":"An individual's data can reveal facets of behavior and identity, but its interpretation is context dependent. We can easily identify various self-tracking applications that help people reflect on their lives. However, self-tracking confined to one person's data source may fall short in terms of objectiveness, and insights coming from various perspectives. To address this, we examine how those interpretations about a person's data can be augmented when the data are juxtaposed with that of others using anonymized online calendar logs from a schedule management app. We develop CALTREND, a visual analytics system that compares an individuals anonymized online schedule logs with using those from other people. Using CALTREND as a probe, we conduct a study with two domain experts, one in information technology and one in Korean herbal medicine. We report our observations on how comparative views help enrich the characterization of an individual based on the experts' comments. We find that juxtaposing personal data with others' can potentially lead to diverse interpretations of one dataset shaped by domain-specific mental models.","authors":["Sungbok Shin","Sunghyo Chung","Hyeon Jeon","Hyunwook Lee","Minje Choi","Taehun Kim","Jaehoon Choi","Sungahn Ko","Jaegul Choo"],"url":"https://arxiv.org/abs/2505.00855"}
{"created":"2025-05-05","title":"Duality on the Thermodynamics of the Kirchhoff-Law-Johnson-Noise (KLJN) Secure Key Exchange Scheme","abstract":"This study investigates a duality approach to information leak detection in the generalized Kirchhoff-Law-Johnson-Noise (KLJN) secure key exchange scheme. While previous work by Chamon and Kish sampled voltages at zero-current instances, this research explores sampling currents at zero-voltage crossings. The objective is to determine if this dual approach can reveal information leaks in non-equilibrium KLJN systems. Results indicate that the duality method successfully detects information leaks, further supporting the necessity of thermal equilibrium for unconditional security in KLJN systems.","authors":["Sarah Flanery","Anson Trapani","Christiana Chamon","Leyla Nazhandali"],"url":"https://arxiv.org/abs/2505.00858"}
{"created":"2025-05-05","title":"Are Minimal Radial Distortion Solvers Really Necessary for Relative Pose Estimation?","abstract":"Estimating the relative pose between two cameras is a fundamental step in many applications such as Structure-from-Motion. The common approach to relative pose estimation is to apply a minimal solver inside a RANSAC loop. Highly efficient solvers exist for pinhole cameras. Yet, (nearly) all cameras exhibit radial distortion. Not modeling radial distortion leads to (significantly) worse results. However, minimal radial distortion solvers are significantly more complex than pinhole solvers, both in terms of run-time and implementation efforts. This paper compares radial distortion solvers with two simple-to-implement approaches that do not use minimal radial distortion solvers: The first approach combines an efficient pinhole solver with sampled radial undistortion parameters, where the sampled parameters are used for undistortion prior to applying the pinhole solver. The second approach uses a state-of-the-art neural network to estimate the distortion parameters rather than sampling them from a set of potential values. Extensive experiments on multiple datasets, and different camera setups, show that complex minimal radial distortion solvers are not necessary in practice. We discuss under which conditions a simple sampling of radial undistortion parameters is preferable over calibrating cameras using a learning-based prior approach. Code and newly created benchmark for relative pose estimation under radial distortion are available at https://github.com/kocurvik/rdnet.","authors":["Viktor Kocur","Charalambos Tzamos","Yaqing Ding","Zuzana Berger Haladova","Torsten Sattler","Zuzana Kukelova"],"url":"https://arxiv.org/abs/2505.00866"}
{"created":"2025-05-05","title":"Deep Autoencoder-Based Constellation Design in Multiple Access Channels","abstract":"In multiple access channels (MAC), multiple users share a transmission medium to communicate with a common receiver. Traditional constellations like quadrature amplitude modulation are optimized for point-to-point systems and lack mechanisms to mitigate inter-user interference, leading to suboptimal performance in MAC environments. To address this, we propose a novel framework for constellation design in MAC that employs deep autoencoder (DAE)-based communication systems. This approach intelligently creates flexible constellations aware of inter-user interference, reducing symbol error rate and enhancing the constellation-constrained sum capacity of the channel. Comparisons against analytically derived constellations demonstrate that DAE-designed constellations consistently perform best or equal to the best across various system parameters. Furthermore, we apply the DAE to scenarios where no analytical solutions have been developed, such as with more than two users, demonstrating the adaptability of the model.","authors":["Stepan Gorelenkov","Mojtaba Vaezi"],"url":"https://arxiv.org/abs/2505.00868"}
{"created":"2025-05-05","title":"A Single-Bit Redundancy Framework for Multi-Dimensional Parametric Constraints","abstract":"Constrained coding plays a key role in optimizing performance and mitigating errors in applications such as storage and communication, where specific constraints on codewords are required. While non-parametric constraints have been well-studied, parametric constraints, which depend on sequence length, have traditionally been tackled with ad hoc solutions. Recent advances have introduced unified methods for parametric constrained coding. This paper extends these approaches to multidimensional settings, generalizing an iterative framework to efficiently encode arrays subject to parametric constraints. We demonstrate the application of the method to existing and new constraints, highlighting its versatility and potential for advanced storage systems.","authors":["Daniella Bar-Lev","Michael Shlizerman"],"url":"https://arxiv.org/abs/2505.00869"}
{"created":"2025-05-05","title":"IK Seed Generator for Dual-Arm Human-like Physicality Robot with Mobile Base","abstract":"Robots are strongly expected as a means of replacing human tasks. If a robot has a human-like physicality, the possibility of replacing human tasks increases. In the case of household service robots, it is desirable for them to be on a human-like size so that they do not become excessively large in order to coexist with humans in their operating environment. However, robots with size limitations tend to have difficulty solving inverse kinematics (IK) due to mechanical limitations, such as joint angle limitations. Conversely, if the difficulty coming from this limitation could be mitigated, one can expect that the use of such robots becomes more valuable. In numerical IK solver, which is commonly used for robots with higher degrees-of-freedom (DOF), the solvability of IK depends on the initial guess given to the solver. Thus, this paper proposes a method for generating a good initial guess for a numerical IK solver given the target hand configuration. For the purpose, we define the goodness of an initial guess using the scaled Jacobian matrix, which can calculate the manipulability index considering the joint limits. These two factors are related to the difficulty of solving IK. We generate the initial guess by optimizing the goodness using the genetic algorithm (GA). To enumerate much possible IK solutions, we use the reachability map that represents the reachable area of the robot hand in the arm-base coordinate system. We conduct quantitative evaluation and prove that using an initial guess that is judged to be better using the goodness value increases the probability that IK is solved. Finally, as an application of the proposed method, we show that by generating good initial guesses for IK a robot actually achieves three typical scenarios.","authors":["Jun Takamatsu","Atsushi Kanehira","Kazuhiro Sasabuchi","Naoki Wake","Katsushi Ikeuchi"],"url":"https://arxiv.org/abs/2505.00871"}
{"created":"2025-05-05","title":"Thoughts without Thinking: Reconsidering the Explanatory Value of Chain-of-Thought Reasoning in LLMs through Agentic Pipelines","abstract":"Agentic pipelines present novel challenges and opportunities for human-centered explainability. The HCXAI community is still grappling with how best to make the inner workings of LLMs transparent in actionable ways. Agentic pipelines consist of multiple LLMs working in cooperation with minimal human control. In this research paper, we present early findings from an agentic pipeline implementation of a perceptive task guidance system. Through quantitative and qualitative analysis, we analyze how Chain-of-Thought (CoT) reasoning, a common vehicle for explainability in LLMs, operates within agentic pipelines. We demonstrate that CoT reasoning alone does not lead to better outputs, nor does it offer explainability, as it tends to produce explanations without explainability, in that they do not improve the ability of end users to better understand systems or achieve their goals.","authors":["Ramesh Manuvinakurike","Emanuel Moss","Elizabeth Anne Watkins","Saurav Sahay","Giuseppe Raffa","Lama Nachman"],"url":"https://arxiv.org/abs/2505.00875"}
{"created":"2025-05-05","title":"Car Sensors Health Monitoring by Verification Based on Autoencoder and Random Forest Regression","abstract":"Driver assistance systems provide a wide range of crucial services, including closely monitoring the condition of vehicles. This paper showcases a groundbreaking sensor health monitoring system designed for the automotive industry. The ingenious system leverages cutting-edge techniques to process data collected from various vehicle sensors. It compares their outputs within the Electronic Control Unit (ECU) to evaluate the health of each sensor. To unravel the intricate correlations between sensor data, an extensive exploration of machine learning and deep learning methodologies was conducted. Through meticulous analysis, the most correlated sensor data were identified. These valuable insights were then utilized to provide accurate estimations of sensor values. Among the diverse learning methods examined, the combination of autoencoders for detecting sensor failures and random forest regression for estimating sensor values proved to yield the most impressive outcomes. A statistical model using the normal distribution has been developed to identify possible sensor failures proactively. By comparing the actual values of the sensors with their estimated values based on correlated sensors, faulty sensors can be detected early. When a defective sensor is detected, both the driver and the maintenance department are promptly alerted. Additionally, the system replaces the value of the faulty sensor with the estimated value obtained through analysis. This proactive approach was evaluated using data from twenty essential sensors in the Saipa's Quick vehicle's ECU, resulting in an impressive accuracy rate of 99\\%.","authors":["Sahar Torkhesari","Behnam Yousefimehr","Mehdi Ghatee"],"url":"https://arxiv.org/abs/2505.00876"}
{"created":"2025-05-05","title":"SynPAT: A System for Generating Synthetic Physical Theories with Data","abstract":"Automated means for discovering new physical laws of nature, starting from a given background theory and data, have recently emerged and are proving to have great potential to someday advance our understanding of the physical world. However, the fact there there are relatively few known theories in the physical sciences has made the training, testing and benchmarking of these systems difficult. To address these needs we have developed SynPAT, a system for generating synthetic physical theories, comprising a set of consistent axioms, together with noisy data that are either good fits to the axioms, or good fits to a subset of the axioms. We give a detailed description of the inner workings of SynPAT and its various capabilities. We also report on our benchmarking of three recent open-source symbolic regression systems using our generated theories and data.","authors":["Jonathan Lenchner","Karan Srivastava","Joao Goncalves","Lior Horesh"],"url":"https://arxiv.org/abs/2505.00878"}
{"created":"2025-05-05","title":"Inattentional Blindness with Augmented Reality HUDS: An On-road Study","abstract":"As the integration of augmented reality (AR) technology in head-up displays (HUDs) becomes more prevalent in vehicles, it is crucial to understand how to design and evaluate AR interfaces to ensure safety. With new AR displays capable of rendering images with larger field of views and at varying depths, the visual and cognitive separation between graphical and real-world visual stimuli will be increasingly more difficult to quantify as will drivers' ability to efficiently allocate visual attention between the two sets of stimuli. In this study, we present a user study that serves as a crucial first step in gaining insight into inattentional blindness while using AR in surface transportation, where understanding is currently limited. Our primary goal is to investigate how the visual demand of AR tasks influences drivers' ability to detect stimuli, and whether the nature of the stimuli itself plays a role in this effect. To address these questions, we designed an on-road user study aimed at producing a more realistic and ecologically valid understanding of the phenomenon.","authors":["Nayara de Oliveira Faria","Joseph L. Gabbard"],"url":"https://arxiv.org/abs/2505.00879"}
{"created":"2025-05-05","title":"Protocol-agnostic and Data-free Backdoor Attacks on Pre-trained Models in RF Fingerprinting","abstract":"While supervised deep neural networks (DNNs) have proven effective for device authentication via radio frequency (RF) fingerprinting, they are hindered by domain shift issues and the scarcity of labeled data. The success of large language models has led to increased interest in unsupervised pre-trained models (PTMs), which offer better generalization and do not require labeled datasets, potentially addressing the issues mentioned above. However, the inherent vulnerabilities of PTMs in RF fingerprinting remain insufficiently explored. In this paper, we thoroughly investigate data-free backdoor attacks on such PTMs in RF fingerprinting, focusing on a practical scenario where attackers lack access to downstream data, label information, and training processes. To realize the backdoor attack, we carefully design a set of triggers and predefined output representations (PORs) for the PTMs. By mapping triggers and PORs through backdoor training, we can implant backdoor behaviors into the PTMs, thereby introducing vulnerabilities across different downstream RF fingerprinting tasks without requiring prior knowledge. Extensive experiments demonstrate the wide applicability of our proposed attack to various input domains, protocols, and PTMs. Furthermore, we explore potential detection and defense methods, demonstrating the difficulty of fully safeguarding against our proposed backdoor attack.","authors":["Tianya Zhao","Ningning Wang","Junqing Zhang","Xuyu Wang"],"url":"https://arxiv.org/abs/2505.00881"}
{"created":"2025-05-05","title":"Towards Explainable Temporal User Profiling with LLMs","abstract":"Accurately modeling user preferences is vital not only for improving recommendation performance but also for enhancing transparency in recommender systems. Conventional user profiling methods, such as averaging item embeddings, often overlook the evolving, nuanced nature of user interests, particularly the interplay between short-term and long-term preferences. In this work, we leverage large language models (LLMs) to generate natural language summaries of users' interaction histories, distinguishing recent behaviors from more persistent tendencies. Our framework not only models temporal user preferences but also produces natural language profiles that can be used to explain recommendations in an interpretable manner. These textual profiles are encoded via a pre-trained model, and an attention mechanism dynamically fuses the short-term and long-term embeddings into a comprehensive user representation. Beyond boosting recommendation accuracy over multiple baselines, our approach naturally supports explainability: the interpretable text summaries and attention weights can be exposed to end users, offering insights into why specific items are suggested. Experiments on real-world datasets underscore both the performance gains and the promise of generating clearer, more transparent justifications for content-based recommendations.","authors":["Milad Sabouri","Masoud Mansoury","Kun Lin","Bamshad Mobasher"],"url":"https://arxiv.org/abs/2505.00886"}
{"created":"2025-05-05","title":"Rethinking Time Encoding via Learnable Transformation Functions","abstract":"Effectively modeling time information and incorporating it into applications or models involving chronologically occurring events is crucial. Real-world scenarios often involve diverse and complex time patterns, which pose significant challenges for time encoding methods. While previous methods focus on capturing time patterns, many rely on specific inductive biases, such as using trigonometric functions to model periodicity. This narrow focus on single-pattern modeling makes them less effective in handling the diversity and complexities of real-world time patterns. In this paper, we investigate to improve the existing commonly used time encoding methods and introduce Learnable Transformation-based Generalized Time Encoding (LeTE). We propose using deep function learning techniques to parameterize non-linear transformations in time encoding, making them learnable and capable of modeling generalized time patterns, including diverse and complex temporal dynamics. By enabling learnable transformations, LeTE encompasses previous methods as specific cases and allows seamless integration into a wide range of tasks. Through extensive experiments across diverse domains, we demonstrate the versatility and effectiveness of LeTE.","authors":["Xi Chen","Yateng Tang","Jiarong Xu","Jiawei Zhang","Siwei Zhang","Sijia Peng","Xuehao Zheng","Yun Xiong"],"url":"https://arxiv.org/abs/2505.00887"}
{"created":"2025-05-05","title":"Balancing Security and Liquidity: A Time-Weighted Snapshot Framework for DAO Governance Voting","abstract":"As new project upgrading the blockchain industry, novel forms of attack challenges developers to rethink about the design of their innovations. In the growth stage of the development, Decentralized Autonomous Organizations (DAO) introduces different approaches in managing fund through voting in governance tokens. However, relying on tokens as a weight for voting introduces opportunities for hackers to manipulate voting results through flash loan, allowing malicious proposals - fund withdrawal from DAO to hacker's wallet - to execute through the smart contract. In this research, we learned different defense mechanism against the flash loan attack, and their weakness in accessibility that compromise the security of different blockchain projects. Based on our observation, we propose a new defensing structure and apply it with cases.","authors":["Zayn Wang","Frank Pu","Vinci Cheung","Robert Hao"],"url":"https://arxiv.org/abs/2505.00888"}
{"created":"2025-05-05","title":"Drilling into Erasmus learning mobility flows between countries 2014-2024","abstract":"Analyzing the Erasmus mobility network, we illustrate typical problems and approaches in analyzing weighted networks. We propose alternative exploratory views on the network \"Erasmus+ learning mobility flows since 2014\". The network has 35 nodes (countries), is very dense, and the range of link weights (number of visits) is huge (from 1 to 217003). An increasing transformation is used to reduce the range. The traditional graph-based visualization is unreadable. To gain insight into the structure of a dense network, it can be reduced to a skeleton by removing less essential links and/or nodes. We have determined the 1-neighbors and 2-neighbors subnetworks. The 1-neighbors skeleton highlights Spain as the main attractor in the network. The 2-neighbors skeleton shows the dominant role of Spain, Germany, France, and Italy. The hubs and authorities, Pathfinder and Ps cores methods confirm these observations.","authors":["Vladimir Batagelj"],"url":"https://arxiv.org/abs/2505.00889"}
{"created":"2025-05-05","title":"Non-Adaptive Cryptanalytic Time-Space Lower Bounds via a Shearer-like Inequality for Permutations","abstract":"The power of adaptivity in algorithms has been intensively studied in diverse areas of theoretical computer science. In this paper, we obtain a number of sharp lower bound results which show that adaptivity provides a significant extra power in cryptanalytic time-space tradeoffs with (possibly unlimited) preprocessing time.","authors":["Itai Dinur","Nathan Keller","Avichai Marmor"],"url":"https://arxiv.org/abs/2505.00894"}
{"created":"2025-05-05","title":"Heterogeneous Memory Benchmarking Toolkit","abstract":"This paper presents an open-source kernel-level heterogeneous memory characterization framework (MemScope) for embedded systems that enables users to understand and precisely characterize the temporal behavior of all available memory modules under configurable contention stress scenarios. Since kernel-level provides a high degree of control over allocation, cache maintenance, $CPUs$, interrupts, and I/O device activity, seeking the most accurate way to benchmark heterogeneous memory subsystems, would be achieved by implementing it in the kernel. This gives us the privilege to directly map pieces of contiguous physical memory and instantiate allocators, allowing us to finely control cores to create and eliminate interference. Additionally, we can minimize noise and interruptions, guaranteeing more consistent and precise results compared to equivalent user-space solutions. Running our Framework on a Xilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability to precisely benchmark bandwidth and latency across various memory types, including PL-side DRAM and BRAM, in a multi-core system.","authors":["Golsana Ghaemi","Kazem Taram","Renato Mancuso"],"url":"https://arxiv.org/abs/2505.00901"}
{"created":"2025-05-05","title":"NeMo-Inspector: A Visualization Tool for LLM Generation Analysis","abstract":"Adapting Large Language Models (LLMs) to novel tasks and enhancing their overall capabilities often requires large, high-quality training datasets. Synthetic data, generated at scale, serves a valuable alternative when real-world data is scarce or difficult to obtain. However, ensuring the quality of synthetic datasets is challenging, as developers must manually inspect and refine numerous samples to identify errors and areas for improvement. This process is time-consuming and requires specialized tools. We introduce NeMo-Inspector, an open-source tool designed to simplify the analysis of synthetic datasets with integrated inference capabilities. We demonstrate its effectiveness through two real-world cases. Analysis and cleaning of the synthetically generated GSM-Plus dataset with NeMo-Inspector led to a significant decrease in low-quality samples from 46.99% to 19.51%. The tool also helped identify and correct generation errors in OpenMath models, improving accuracy by 1.92% on the MATH dataset and by 4.17% on the GSM8K dataset for a Meta-Llama-3-8B model fine-tuned on synthetic data generated from Nemotron-4-340B.","authors":["Daria Gitman","Igor Gitman","Evelina Bakhturina"],"url":"https://arxiv.org/abs/2505.00903"}
{"created":"2025-05-05","title":"Neural Networks Enabled Discovery On the Higher-Order Nonlinear Partial Differential Equation of Traffic Dynamics","abstract":"Modeling the traffic dynamics is essential for understanding and predicting the traffic spatiotemporal evolution. However, deriving the partial differential equation (PDE) models that capture these dynamics is challenging due to their potential high order property and nonlinearity. In this paper, we introduce a novel deep learning framework, \"TRAFFIC-PDE-LEARN\", designed to discover hidden PDE models of traffic network dynamics directly from measurement data. By harnessing the power of the neural network to approximate a spatiotemporal fundamental diagram that facilitates smooth estimation of partial derivatives with low-resolution loop detector data. Furthermore, the use of automatic differentiation enables efficient computation of the necessary partial derivatives through the chain and product rules, while sparse regression techniques facilitate the precise identification of physically interpretable PDE components. Tested on data from a real-world traffic network, our model demonstrates that the underlying PDEs governing traffic dynamics are both high-order and nonlinear. By leveraging the learned dynamics for prediction purposes, the results underscore the effectiveness of our approach and its potential to advance intelligent transportation systems.","authors":["Zihang Wei","Yunlong Zhang","Chenxi Liu","Yang Zhou"],"url":"https://arxiv.org/abs/2505.00904"}
{"created":"2025-05-05","title":"Co-Designing a Knowledge Graph Navigation Interface: A Participatory Approach","abstract":"Navigating and visualizing multilayered knowledge graphs remains a challenging, unresolved problem in information systems design. Building on our earlier study, which engaged end users in both the design and population of a domain-specific knowledge graph, we now focus on translating their insights into actionable interface guidelines. In this paper, we synthesize recommendations drawn from a participatory workshop with doctoral students. We then demonstrate how these recommendations inform the design of a prototype interface. Finally, we found that a participatory iterative design approach can help designers in decision making, leading to interfaces that are both innovative and user-centric. By combining user-driven requirements with proven visualization techniques, this paper presents a coherent framework for guiding future development of knowledge-graph navigation tools.","authors":["Stanislava Gardasevic","Manika Lamba","Jasmine S. Malone"],"url":"https://arxiv.org/abs/2505.00907"}
{"created":"2025-05-05","title":"Learning Neural Control Barrier Functions from Offline Data with Conservatism","abstract":"Safety filters, particularly those based on control barrier functions, have gained increased interest as effective tools for safe control of dynamical systems. Existing correct-by-construction synthesis algorithms, however, suffer from the curse of dimensionality. Deep learning approaches have been proposed in recent years to address this challenge. In this paper, we contribute to this line of work by proposing an algorithm for training control barrier functions from offline datasets. Our algorithm trains the filter to not only prevent the system from reaching unsafe states but also out-of-distribution ones, at which the filter would be unreliable. It is inspired by Conservative Q-learning, an offline reinforcement learning algorithm. We call its outputs Conservative Control Barrier Functions (CCBFs). Our empirical results demonstrate that CCBFs outperform existing methods in maintaining safety and out-of-distribution avoidance while minimally affecting task performance.","authors":["Ihab Tabbara","Hussein Sibai"],"url":"https://arxiv.org/abs/2505.00908"}
{"created":"2025-05-05","title":"Gaussian Process Policy Iteration with Additive Schwarz Acceleration for Forward and Inverse HJB and Mean Field Game Problems","abstract":"We propose a Gaussian Process (GP)-based policy iteration framework for addressing both forward and inverse problems in Hamilton--Jacobi--Bellman (HJB) equations and mean field games (MFGs). Policy iteration is formulated as an alternating procedure between solving the value function under a fixed control policy and updating the policy based on the resulting value function. By exploiting the linear structure of GPs for function approximation, each policy evaluation step admits an explicit closed-form solution, eliminating the need for numerical optimization. To improve convergence, we incorporate the additive Schwarz acceleration as a preconditioning step following each policy update. Numerical experiments demonstrate the effectiveness of Schwarz acceleration in improving computational efficiency.","authors":["Xianjin Yang","Jingguo Zhang"],"url":"https://arxiv.org/abs/2505.00909"}
{"created":"2025-05-05","title":"Towards a format for describing networks / 1. Networks and knowledge graphs","abstract":"The relationship between the concepts of network and knowledge graph is explored. A knowledge graph can be considered a special type of network. When using a knowledge graph, various networks can be obtained from it, and network analysis procedures can be applied to them. RDF is a formalization of the knowledge graph concept for the Semantic Web, but some of its solutions are also extensible to a format for describing general networks.","authors":["Vladimir Batagelj","Toma\\v{z} Pisanski","Iztok Savnik","Ana Slavec","Nino Ba\\v{s}i\\'c"],"url":"https://arxiv.org/abs/2505.00912"}
{"created":"2025-05-05","title":"Fine-Tuning without Performance Degradation","abstract":"Fine-tuning policies learned offline remains a major challenge in application domains. Monotonic performance improvement during \\emph{fine-tuning} is often challenging, as agents typically experience performance degradation at the early fine-tuning stage. The community has identified multiple difficulties in fine-tuning a learned network online, however, the majority of progress has focused on improving learning efficiency during fine-tuning. In practice, this comes at a serious cost during fine-tuning: initially, agent performance degrades as the agent explores and effectively overrides the policy learned offline. We show across a range of settings, many offline-to-online algorithms exhibit either (1) performance degradation or (2) slow learning (sometimes effectively no improvement) during fine-tuning. We introduce a new fine-tuning algorithm, based on an algorithm called Jump Start, that gradually allows more exploration based on online estimates of performance. Empirically, this approach achieves fast fine-tuning and significantly reduces performance degradations compared with existing algorithms designed to do the same.","authors":["Han Wang","Adam White","Martha White"],"url":"https://arxiv.org/abs/2505.00913"}
{"created":"2025-05-05","title":"Lower Bounds for Non-adaptive Local Computation Algorithms","abstract":"We study *non-adaptive* Local Computation Algorithms (LCA). A reduction of Parnas and Ron (TCS'07) turns any distributed algorithm into a non-adaptive LCA. Plugging known distributed algorithms, this leads to non-adaptive LCAs for constant approximations of maximum matching (MM) and minimum vertex cover (MVC) with complexity $\\Delta^{O(\\log \\Delta / \\log \\log \\Delta)}$, where $\\Delta$ is the maximum degree of the graph. Allowing adaptivity, this bound can be significantly improved to $\\text{poly}(\\Delta)$, but is such a gap necessary or are there better non-adaptive LCAs?","authors":["Amir Azarmehr","Soheil Behnezhad","Alma Ghafari","Madhu Sudan"],"url":"https://arxiv.org/abs/2505.00915"}
{"created":"2025-05-05","title":"Dynamic and Distributed Routing in IoT Networks based on Multi-Objective Q-Learning","abstract":"The last few decades have witnessed a rapid increase in IoT devices owing to their wide range of applications, such as smart healthcare monitoring systems, smart cities, and environmental monitoring. A critical task in IoT networks is sensing and transmitting information over the network. The IoT nodes gather data by sensing the environment and then transmit this data to a destination node via multi-hop communication, following some routing protocols. These protocols are usually designed to optimize possibly contradictory objectives, such as maximizing packet delivery ratio and energy efficiency. While most literature has focused on optimizing a static objective that remains unchanged, many real-world IoT applications require adapting to rapidly shifting priorities. For example, in monitoring systems, some transmissions are time-critical and require a high priority on low latency, while other transmissions are less urgent and instead prioritize energy efficiency. To meet such dynamic demands, we propose novel dynamic and distributed routing based on multiobjective Q-learning that can adapt to changes in preferences in real-time. Our algorithm builds on ideas from both multi-objective optimization and Q-learning. We also propose a novel greedy interpolation policy scheme to take near-optimal decisions for unexpected preference changes. The proposed scheme can approximate and utilize the Pareto-efficient solutions for dynamic preferences, thus utilizing past knowledge to adapt to unpredictable preferences quickly during runtime. Simulation results show that the proposed scheme outperforms state-of-the-art algorithms for various exploration strategies, preference variation patterns, and important metrics like overall reward, energy efficiency, and packet delivery ratio.","authors":["Shubham Vaishnav","Praveen Kumar Donta","Sindri Magn\\'usson"],"url":"https://arxiv.org/abs/2505.00918"}
{"created":"2025-05-05","title":"Towards a format for describing networks / 2. Format elements","abstract":"The key elements that a common format for describing networks should include are discussed.","authors":["Vladimir Batagelj","Toma\\v{z} Pisanski","Iztok Savnik","Ana Slavec","Nino Ba\\v{s}i\\'c"],"url":"https://arxiv.org/abs/2505.00921"}
{"created":"2025-05-05","title":"Cluster deletion and clique partitioning in graphs with bounded clique number","abstract":"The Cluster Deletion problem takes a graph $G$ as input and asks for a minimum size set of edges $X$ such that $G-X$ is the disjoint union of complete graphs. An equivalent formulation is the Clique Partition problem, which asks to find a partition of $V(G)$ into cliques such that the total number of edges is maximized.","authors":["Nicola Galesi","Tony Huynh","Fariba Ranjbar"],"url":"https://arxiv.org/abs/2505.00922"}
{"created":"2025-05-05","title":"Optimal Design of a Walking Robot: Analytical, Numerical, and Machine Learning Methods for Multicriteria Synthesis","abstract":"This paper addresses several critical stages of designing a walking robot, including optimal structural synthesis, introducing a novel 'rational' mechanical structure aimed at enhancing efficiency and simplifying control system, while addressing practical limitations observed in existing designs. The study includes development of novel multicriteria synthesis methods for achieving optimal leg design, integrating analytical and numerical methods. In addition, a method based on Non-dominated Sorting Genetic Algorithm II is presented. Turning modes are investigated, and for the first time, the isotropy criterion, typically applied to parallel manipulators, is used for optimizing walking robot parameters to ensure optimal force and motion transfer in all directions. Several physical prototypes are developed to experimentally validate the functionality of different mechanisms of the robot, including adaptation to the surface irregularities and navigation using LiDAR.","authors":["Arman Ibrayeva","Batyrkhan Omarov"],"url":"https://arxiv.org/abs/2505.00923"}
{"created":"2025-05-05","title":"MARS: Defending Unmanned Aerial Vehicles From Attacks on Inertial Sensors with Model-based Anomaly Detection and Recovery","abstract":"Unmanned Aerial Vehicles (UAVs) rely on measurements from Inertial Measurement Units (IMUs) to maintain stable flight. However, IMUs are susceptible to physical attacks, including acoustic resonant and electromagnetic interference attacks, resulting in immediate UAV crashes. Consequently, we introduce a Model-based Anomaly detection and Recovery System (MARS) that enables UAVs to quickly detect adversarial attacks on inertial sensors and achieve dynamic flight recovery. MARS features an attack-resilient state estimator based on the Extended Kalman Filter, which incorporates position, velocity, heading, and rotor speed measurements to reconstruct accurate attitude and angular velocity information for UAV control. Moreover, a statistical anomaly detection system monitors IMU sensor data, raising a system-level alert if an attack is detected. Upon receiving the alert, a multi-stage dynamic flight recovery strategy suspends the ongoing mission, stabilizes the drone in a hovering condition, and then resumes tasks under the resilient control. Experimental results in PX4 software-in-the-loop environments as well as real-world MARS-PX4 autopilot-equipped drones demonstrate the superiority of our approach over existing IMU-defense frameworks, showcasing the ability of the UAVs to survive attacks and complete the missions.","authors":["Haocheng Meng","Shaocheng Luo","Zhenyuan Liang","Qing Huang","Amir Khazraei","Miroslav Pajic"],"url":"https://arxiv.org/abs/2505.00924"}
{"created":"2025-05-05","title":"How Transformers Learn Regular Language Recognition: A Theoretical Study on Training Dynamics and Implicit Bias","abstract":"Language recognition tasks are fundamental in natural language processing (NLP) and have been widely used to benchmark the performance of large language models (LLMs). These tasks also play a crucial role in explaining the working mechanisms of transformers. In this work, we focus on two representative tasks in the category of regular language recognition, known as `even pairs' and `parity check', the aim of which is to determine whether the occurrences of certain subsequences in a given sequence are even. Our goal is to explore how a one-layer transformer, consisting of an attention layer followed by a linear layer, learns to solve these tasks by theoretically analyzing its training dynamics under gradient descent. While even pairs can be solved directly by a one-layer transformer, parity check need to be solved by integrating Chain-of-Thought (CoT), either into the inference stage of a transformer well-trained for the even pairs task, or into the training of a one-layer transformer. For both problems, our analysis shows that the joint training of attention and linear layers exhibits two distinct phases. In the first phase, the attention layer grows rapidly, mapping data sequences into separable vectors. In the second phase, the attention layer becomes stable, while the linear layer grows logarithmically and approaches in direction to a max-margin hyperplane that correctly separates the attention layer outputs into positive and negative samples, and the loss decreases at a rate of $O(1/t)$. Our experiments validate those theoretical results.","authors":["Ruiquan Huang","Yingbin Liang","Jing Yang"],"url":"https://arxiv.org/abs/2505.00926"}
{"created":"2025-05-05","title":"Virtual Force-Based Routing of Modular Agents on a Graph","abstract":"Modular vehicles have become an area of academic interest in the field of multi-agent systems. Modularity allows vehicles to connect and disconnect with each other mid-transit which provides a balance between efficiency and flexibility when solving complex and large scale tasks in urban or aerial transportation. This paper details a generalized scheme to route multiple modular agents on a graph to a predetermined set of target nodes. The objective is to visit all target nodes while incurring minimum resource expenditure. Agents that are joined together will incur the equivalent cost of a single agent, which is motivated by the logistical benefits of traffic reduction and increased fuel efficiency. To solve this problem, we introduce a heuristic algorithm that seeks to balance the optimality of the path that an agent takes and the cost benefit of joining agents. Our approach models the agents and targets as point charges, where the agents take the path of highest attractive force from its target node and neighboring agents. We validate our approach by simulating multiple modular agents along real-world transportation routes in the road network of Champaign-Urbana, Illinois, USA. For two vehicles, it performed equally compared to an existing modular-agent routing algorithm. Three agents were then routed using our method and the performance was benchmarked against non-modular agents using a simple shortest path policy where it performs better than the non-modular implementation 81 percent of the time. Moreover, we show that the proposed algorithm operates faster than existing routing methods for modular agents.","authors":["Adam Casselman","Manav Vora","Melkior Ornik"],"url":"https://arxiv.org/abs/2505.00928"}
{"created":"2025-05-05","title":"Compact Recurrent Transformer with Persistent Memory","abstract":"The Transformer architecture has shown significant success in many language processing and visual tasks. However, the method faces challenges in efficiently scaling to long sequences because the self-attention computation is quadratic with respect to the input length. To overcome this limitation, several approaches scale to longer sequences by breaking long sequences into a series of segments, restricting self-attention to local dependencies between tokens within each segment and using a memory mechanism to manage information flow between segments. However, these approached generally introduce additional compute overhead that restricts them from being used for applications where limited compute memory and power are of great concern (such as edge computing). We propose a novel and efficient Compact Recurrent Transformer (CRT), which combines shallow Transformer models that process short local segments with recurrent neural networks to compress and manage a single persistent memory vector that summarizes long-range global information between segments. We evaluate CRT on WordPTB and WikiText-103 for next-token-prediction tasks, as well as on the Toyota Smarthome video dataset for classification. CRT achieves comparable or superior prediction results to full-length Transformers in the language datasets while using significantly shorter segments (half or quarter size) and substantially reduced FLOPs. Our approach also demonstrates state-of-the-art performance on the Toyota Smarthome video dataset.","authors":["Edison Mucllari","Zachary Daniels","David Zhang","Qiang Ye"],"url":"https://arxiv.org/abs/2505.00929"}
{"created":"2025-05-05","title":"Robust Root Cause Diagnosis using In-Distribution Interventions","abstract":"Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today's cloud services and industrial operations. We propose In-Distribution Interventions (IDI), a novel algorithm that predicts root cause as nodes that meet two criteria: 1) **Anomaly:** root cause nodes should take on anomalous values; 2) **Fix:** had the root cause nodes assumed usual values, the target node would not have been anomalous. Prior methods of assessing the fix condition rely on counterfactuals inferred from a Structural Causal Model (SCM) trained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs yield unreliable counterfactual estimates. IDI overcomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. We present a theoretical analysis comparing and bounding the errors in assessing the fix condition using interventional and counterfactual estimates. We then conduct experiments by systematically varying the SCM's complexity to demonstrate the cases where IDI's interventional approach outperforms the counterfactual approach and vice versa. Experiments on both synthetic and PetShop RCD benchmark datasets demonstrate that \\our\\ consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. Code is released at https://github.com/nlokeshiisc/IDI_release.","authors":["Lokesh Nagalapatti","Ashutosh Srivastava","Sunita Sarawagi","Amit Sharma"],"url":"https://arxiv.org/abs/2505.00930"}
{"created":"2025-05-05","title":"Large Language Model-Driven Dynamic Assessment of Grammatical Accuracy in English Language Learner Writing","abstract":"This study investigates the potential for Large Language Models (LLMs) to scale-up Dynamic Assessment (DA). To facilitate such an investigation, we first developed DynaWrite-a modular, microservices-based grammatical tutoring application which supports multiple LLMs to generate dynamic feedback to learners of English. Initial testing of 21 LLMs, revealed GPT-4o and neural chat to have the most potential to scale-up DA in the language learning classroom. Further testing of these two candidates found both models performed similarly in their ability to accurately identify grammatical errors in user sentences. However, GPT-4o consistently outperformed neural chat in the quality of its DA by generating clear, consistent, and progressively explicit hints. Real-time responsiveness and system stability were also confirmed through detailed performance testing, with GPT-4o exhibiting sufficient speed and stability. This study shows that LLMs can be used to scale-up dynamic assessment and thus enable dynamic assessment to be delivered to larger groups than possible in traditional teacher-learner settings.","authors":["Timur Jaganov","John Blake","Juli\\'an Villegas","Nicholas Carr"],"url":"https://arxiv.org/abs/2505.00931"}
{"created":"2025-05-05","title":"A Self-Supervised Transformer for Unusable Shared Bike Detection","abstract":"The rapid expansion of bike-sharing systems (BSS) has greatly improved urban \"last-mile\" connectivity, yet large-scale deployments face escalating operational challenges, particularly in detecting faulty bikes. Existing detection approaches either rely on static model-based thresholds that overlook dynamic spatiotemporal (ST) usage patterns or employ supervised learning methods that struggle with label scarcity and class imbalance. To address these limitations, this paper proposes a novel Self-Supervised Transformer (SSTransformer) framework for automatically detecting unusable shared bikes, leveraging ST features extracted from GPS trajectories and trip records. The model incorporates a self-supervised pre-training strategy to enhance its feature extraction capabilities, followed by fine-tuning for efficient status recognition. In the pre-training phase, the Transformer encoder learns generalized representations of bike movement via a self-supervised objective; in the fine-tuning phase, the encoder is adapted to a downstream binary classification task. Comprehensive experiments on a real-world dataset of 10,730 bikes (1,870 unusable, 8,860 normal) from Chengdu, China, demonstrate that SSTransformer significantly outperforms traditional machine learning, ensemble learning, and deep learning baselines, achieving the best accuracy (97.81%), precision (0.8889), and F1-score (0.9358). This work highlights the effectiveness of self-supervised Transformer on ST data for capturing complex anomalies in BSS, paving the way toward more reliable and scalable maintenance solutions for shared mobility.","authors":["Yin Huang","Yongqi Dong","Youhua Tang","Alvaro Garc\\'ia Hernandez"],"url":"https://arxiv.org/abs/2505.00932"}
{"created":"2025-05-05","title":"TunnElQNN: A Hybrid Quantum-classical Neural Network for Efficient Learning","abstract":"Hybrid quantum-classical neural networks (HQCNNs) represent a promising frontier in machine learning, leveraging the complementary strengths of both models. In this work, we propose the development of TunnElQNN, a non-sequential architecture composed of alternating classical and quantum layers. Within the classical component, we employ the Tunnelling Diode Activation Function (TDAF), inspired by the I-V characteristics of quantum tunnelling. We evaluate the performance of this hybrid model on a synthetic dataset of interleaving half-circle for multi-class classification tasks with varying degrees of class overlap. The model is compared against a baseline hybrid architecture that uses the conventional ReLU activation function (ReLUQNN). Our results show that the TunnElQNN model consistently outperforms the ReLUQNN counterpart. Furthermore, we analyse the decision boundaries generated by TunnElQNN under different levels of class overlap and compare them to those produced by a neural network implementing TDAF within a fully classical architecture. These findings highlight the potential of integrating physics-inspired activation functions with quantum components to enhance the expressiveness and robustness of hybrid quantum-classical machine learning architectures.","authors":["A. H. Abbas"],"url":"https://arxiv.org/abs/2505.00933"}
{"created":"2025-05-05","title":"Autonomous Embodied Agents: When Robotics Meets Deep Learning Reasoning","abstract":"The increase in available computing power and the Deep Learning revolution have allowed the exploration of new topics and frontiers in Artificial Intelligence research. A new field called Embodied Artificial Intelligence, which places at the intersection of Computer Vision, Robotics, and Decision Making, has been gaining importance during the last few years, as it aims to foster the development of smart autonomous robots and their deployment in society. The recent availability of large collections of 3D models for photorealistic robotic simulation has allowed faster and safe training of learning-based agents for millions of frames and a careful evaluation of their behavior before deploying the models on real robotic platforms. These intelligent agents are intended to perform a certain task in a possibly unknown environment. To this end, during the training in simulation, the agents learn to perform continuous interactions with the surroundings, such as gathering information from the environment, encoding and extracting useful cues for the task, and performing actions towards the final goal; where every action of the agent influences the interactions. This dissertation follows the complete creation process of embodied agents for indoor environments, from their concept to their implementation and deployment. We aim to contribute to research in Embodied AI and autonomous agents, in order to foster future work in this field. We present a detailed analysis of the procedure behind implementing an intelligent embodied agent, comprehending a thorough description of the current state-of-the-art in literature, technical explanations of the proposed methods, and accurate experimental studies on relevant robotic tasks.","authors":["Roberto Bigazzi"],"url":"https://arxiv.org/abs/2505.00935"}
{"created":"2025-05-05","title":"CDFormer: Cross-Domain Few-Shot Object Detection Transformer Against Feature Confusion","abstract":"Cross-domain few-shot object detection (CD-FSOD) aims to detect novel objects across different domains with limited class instances. Feature confusion, including object-background confusion and object-object confusion, presents significant challenges in both cross-domain and few-shot settings. In this work, we introduce CDFormer, a cross-domain few-shot object detection transformer against feature confusion, to address these challenges. The method specifically tackles feature confusion through two key modules: object-background distinguishing (OBD) and object-object distinguishing (OOD). The OBD module leverages a learnable background token to differentiate between objects and background, while the OOD module enhances the distinction between objects of different classes. Experimental results demonstrate that CDFormer outperforms previous state-of-the-art approaches, achieving 12.9% mAP, 11.0% mAP, and 10.4% mAP improvements under the 1/5/10 shot settings, respectively, when fine-tuned.","authors":["Boyuan Meng","Xiaohan Zhang","Peilin Li","Zhe Wu","Yiming Li","Wenkai Zhao","Beinan Yu","Hui-Liang Shen"],"url":"https://arxiv.org/abs/2505.00938"}
{"created":"2025-05-05","title":"On The Metric Nature of (Differential) Logical Relations","abstract":"Differential logical relations are a method to measure distances between higher-order programs. They differ from standard methods based on program metrics in that differences between functional programs are themselves functions, relating errors in input with errors in output, this way providing a more fine grained, contextual, information. The aim of this paper is to clarify the metric nature of differential logical relations. While previous work has shown that these do not give rise, in general, to (quasi-)metric spaces nor to partial metric spaces, we show that the distance functions arising from such relations, that we call quasi-quasi-metrics, can be related to both quasi-metrics and partial metrics, the latter being also captured by suitable relational definitions. Moreover, we exploit such connections to deduce some new compositional reasoning principles for program differences.","authors":["Ugo Dal Lago","Naohiko Hoshino","Paolo Pistone"],"url":"https://arxiv.org/abs/2505.00939"}
{"created":"2025-05-05","title":"StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization","abstract":"When synthesizing multisource high-dimensional data, a key objective is to extract low-dimensional feature representations that effectively approximate the original features across different sources. Such general feature extraction facilitates the discovery of transferable knowledge, mitigates systematic biases such as batch effects, and promotes fairness. In this paper, we propose Stable Principal Component Analysis (StablePCA), a novel method for group distributionally robust learning of latent representations from high-dimensional multi-source data. A primary challenge in generalizing PCA to the multi-source regime lies in the nonconvexity of the fixed rank constraint, rendering the minimax optimization nonconvex. To address this challenge, we employ the Fantope relaxation, reformulating the problem as a convex minimax optimization, with the objective defined as the maximum loss across sources. To solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox algorithm with explicit closed-form updates. Theoretically, we establish the global convergence of the Mirror Prox algorithm, with the convergence rate provided from the optimization perspective. Furthermore, we offer practical criteria to assess how closely the solution approximates the original nonconvex formulation. Through extensive numerical experiments, we demonstrate StablePCA's high accuracy and efficiency in extracting robust low-dimensional representations across various finite-sample scenarios.","authors":["Zhenyu Wang","Molei Liu","Jing Lei","Francis Bach","Zijian Guo"],"url":"https://arxiv.org/abs/2505.00940"}
{"created":"2025-05-05","title":"FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection","abstract":"Time series anomaly detection is critical for system monitoring and risk identification, across various domains, such as finance and healthcare. However, for most reconstruction-based approaches, detecting anomalies remains a challenge due to the complexity of sequential patterns in time series data. On the one hand, reconstruction-based techniques are susceptible to computational deviation stemming from anomalies, which can lead to impure representations of normal sequence patterns. On the other hand, they often focus on the time-domain dependencies of time series, while ignoring the alignment of frequency information beyond the time domain. To address these challenges, we propose a novel Frequency-augmented Convolutional Transformer (FreCT). FreCT utilizes patch operations to generate contrastive views and employs an improved Transformer architecture integrated with a convolution module to capture long-term dependencies while preserving local topology information. The introduced frequency analysis based on Fourier transformation could enhance the model's ability to capture crucial characteristics beyond the time domain. To protect the training quality from anomalies and improve the robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and absolute error to optimize consistency information in both time and frequency domains. Extensive experiments on four public datasets demonstrate that FreCT outperforms existing methods in identifying anomalies.","authors":["Wenxin Zhang","Ding Xu","Guangzhen Yao","Xiaojian Lin","Renxiang Guan","Chengze Du","Renda Han","Xi Xuan","Cuicui Luo"],"url":"https://arxiv.org/abs/2505.00941"}
{"created":"2025-05-05","title":"SSRLBot: Designing and Developing an LLM-based Agent using Socially Shared Regulated Learning","abstract":"Large language model (LLM)-based agents are increasingly used to support human experts by streamlining complex tasks and offering actionable insights. However, their application in multi-professional decision-making, particularly in teamwork contexts, remains underexplored. This design-based study addresses that gap by developing LLM functions to enhance collaboration, grounded in the Socially Shared Regulation of Learning (SSRL) framework and applied to medical diagnostic teamwork. SSRL emphasizes metacognitive, cognitive, motivational, and emotional processes in shared learning, focusing on how teams manage these processes to improve decision-making. This paper introduces SSRLBot, a prototype chatbot designed to help team members reflect on both their diagnostic performance and key SSRL skills. Its core functions include summarizing dialogues, analyzing SSRL behaviors, evaluating diagnostic outcomes, annotating SSRL markers in conversation, assessing their impact on performance, and identifying interpersonal regulatory dynamics. We compare SSRLBot's capabilities with those of Gemini-1.5, GPT-3.5, and Deepseek-R1 in a case study. SSRLBot demonstrates stronger alignment with SSRL theory, offering detailed evaluations that link behaviors to regulatory dimensions and suggesting improvements for collaboration. By integrating SSRL theory with LLM capabilities, SSRLBot contributes a novel tool for enhancing team-based decision-making and collaborative learning in high-stakes environments, such as medical education.","authors":["Xiaoshan Huang","Jie Gao","Haolun Wu"],"url":"https://arxiv.org/abs/2505.00945"}
{"created":"2025-05-05","title":"Addressing Noise and Stochasticity in Fraud Detection for Service Networks","abstract":"Fraud detection is crucial in social service networks to maintain user trust and improve service network security. Existing spectral graph-based methods address this challenge by leveraging different graph filters to capture signals with different frequencies in service networks. However, most graph filter-based methods struggle with deriving clean and discriminative graph signals. On the one hand, they overlook the noise in the information propagation process, resulting in degradation of filtering ability. On the other hand, they fail to discriminate the frequency-specific characteristics of graph signals, leading to distortion of signals fusion. To address these issues, we develop a novel spectral graph network based on information bottleneck theory (SGNN-IB) for fraud detection in service networks. SGNN-IB splits the original graph into homophilic and heterophilic subgraphs to better capture the signals at different frequencies. For the first limitation, SGNN-IB applies information bottleneck theory to extract key characteristics of encoded representations. For the second limitation, SGNN-IB introduces prototype learning to implement signal fusion, preserving the frequency-specific characteristics of signals. Extensive experiments on three real-world datasets demonstrate that SGNN-IB outperforms state-of-the-art fraud detection methods.","authors":["Wenxin Zhang","Ding Xu","Xi Xuan","Lei Jiang","Guangzhen Yao","Renda Han","Xiangxiang Lang","Cuicui Luo"],"url":"https://arxiv.org/abs/2505.00946"}
{"created":"2025-05-05","title":"What Makes Teamwork Work? A Multimodal Case Study on Emotions and Diagnostic Expertise in an Intelligent Tutoring System","abstract":"Teamwork is pivotal in medical teamwork when professionals with diverse skills and emotional states collaborate to make critical decisions. This case study examines the interplay between emotions and professional skills in group decision-making during collaborative medical diagnosis within an Intelligent Tutoring System (ITS). By comparing verbal and physiological data between high-performing and low-performing teams of medical professionals working on a patient case within the ITS, alongside individuals' retrospective collaboration experiences, we employ multimodal data analysis to identify patterns in team emotional climate and their impact on diagnostic efficiency. Specifically, we investigate how emotion-driven dialogue and professional expertise influence both the information-seeking process and the final diagnostic decisions. Grounded in the socially shared regulation of learning framework and utilizing sentiment analysis, we found that social-motivational interactions are key drivers of a positive team emotional climate. Furthermore, through content analysis of dialogue and physiological signals to pinpoint emotional fluctuations, we identify episodes where knowledge exchange and skill acquisition are most likely to occur. Our findings offer valuable insights into optimizing group collaboration in medical contexts by harmonizing emotional dynamics with adaptive strategies for effective decision-making, ultimately enhancing diagnostic accuracy and teamwork effectiveness.","authors":["Xiaoshan Huang","Haolun Wu","Xue Liu","Susanne P. Lajoie"],"url":"https://arxiv.org/abs/2505.00948"}
{"created":"2025-05-05","title":"Llama-Nemotron: Efficient Reasoning Models","abstract":"We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.","authors":["Akhiad Bercovich","Itay Levy","Izik Golan","Mohammad Dabbah","Ran El-Yaniv","Omri Puny","Ido Galil","Zach Moshe","Tomer Ronen","Najeeb Nabwani","Ido Shahaf","Oren Tropp","Ehud Karpas","Ran Zilberstein","Jiaqi Zeng","Soumye Singhal","Alexander Bukharin","Yian Zhang","Tugrul Konuk","Gerald Shen","Ameya Sunil Mahabaleshwarkar","Bilal Kartal","Yoshi Suhara","Olivier Delalleau","Zijia Chen","Zhilin Wang","David Mosallanezhad","Adi Renduchintala","Haifeng Qian","Dima Rekesh","Fei Jia","Somshubra Majumdar","Vahid Noroozi","Wasi Uddin Ahmad","Sean Narenthiran","Aleksander Ficek","Mehrzad Samadi","Jocelyn Huang","Siddhartha Jain","Igor Gitman","Ivan Moshkov","Wei Du","Shubham Toshniwal","George Armstrong","Branislav Kisacanin","Matvei Novikov","Daria Gitman","Evelina Bakhturina","Jane Polak Scowcroft","John Kamalu","Dan Su","Kezhi Kong","Markus Kliegl","Rabeeh Karimi","Ying Lin","Sanjeev Satheesh","Jupinder Parmar","Pritam Gundecha","Brandon Norick","Joseph Jennings","Shrimai Prabhumoye","Syeda Nahida Akter","Mostofa Patwary","Abhinav Khattar","Deepak Narayanan","Roger Waleffe","Jimmy Zhang","Bor-Yiing Su","Guyue Huang","Terry Kong","Parth Chadha","Sahil Jain","Christine Harvey","Elad Segal","Jining Huang","Sergey Kashirsky","Robert McQueen","Izzy Putterman","George Lam","Arun Venkatesan","Sherry Wu","Vinh Nguyen","Manoj Kilaru","Andrew Wang","Anna Warno","Abhilash Somasamudramath","Sandip Bhaskar","Maka Dong","Nave Assaf","Shahar Mor","Omer Ullman Argov","Scot Junkin","Oleksandr Romanenko","Pedro Larroy","Monika Katariya","Marco Rovinelli","Viji Balas","Nicholas Edelman","Anahita Bhiwandiwalla","Muthu Subramaniam","Smita Ithape","Karthik Ramamoorthy","Yuting Wu","Suguna Varshini Velury","Omri Almog","Joyjit Daw","Denys Fridman","Erick Galinkin","Michael Evans","Katherine Luna","Leon Derczynski","Nikki Pope","Eileen Long","Seth Schneider","Guillermo Siman","Tomasz Grzegorzek","Pablo Ribalta","Monika Katariya","Joey Conway","Trisha Saar","Ann Guan","Krzysztof Pawelec","Shyamala Prayaga","Oleksii Kuchaiev","Boris Ginsburg","Oluwatobi Olabiyi","Kari Briski","Jonathan Cohen","Bryan Catanzaro","Jonah Alben","Yonatan Geifman","Eric Chung"],"url":"https://arxiv.org/abs/2505.00949"}
{"created":"2025-05-05","title":"Preserving Privacy and Utility in LLM-Based Product Recommendations","abstract":"Large Language Model (LLM)-based recommendation systems leverage powerful language models to generate personalized suggestions by processing user interactions and preferences. Unlike traditional recommendation systems that rely on structured data and collaborative filtering, LLM-based models process textual and contextual information, often using cloud-based infrastructure. This raises privacy concerns, as user data is transmitted to remote servers, increasing the risk of exposure and reducing control over personal information. To address this, we propose a hybrid privacy-preserving recommendation framework which separates sensitive from nonsensitive data and only shares the latter with the cloud to harness LLM-powered recommendations. To restore lost recommendations related to obfuscated sensitive data, we design a de-obfuscation module that reconstructs sensitive recommendations locally. Experiments on real-world e-commerce datasets show that our framework achieves almost the same recommendation utility with a system which shares all data with an LLM, while preserving privacy to a large extend. Compared to obfuscation-only techniques, our approach improves HR@10 scores and category distribution alignment, offering a better balance between privacy and recommendation quality. Furthermore, our method runs efficiently on consumer-grade hardware, making privacy-aware LLM-based recommendation systems practical for real-world use.","authors":["Tina Khezresmaeilzadeh","Jiang Zhang","Dimitrios Andreadis","Konstantinos Psounis"],"url":"https://arxiv.org/abs/2505.00951"}
{"created":"2025-05-05","title":"Enhancing User Sequence Modeling through Barlow Twins-based Self-Supervised Learning","abstract":"User sequence modeling is crucial for modern large-scale recommendation systems, as it enables the extraction of informative representations of users and items from their historical interactions. These user representations are widely used for a variety of downstream tasks to enhance users' online experience. A key challenge for learning these representations is the lack of labeled training data. While self-supervised learning (SSL) methods have emerged as a promising solution for learning representations from unlabeled data, many existing approaches rely on extensive negative sampling, which can be computationally expensive and may not always be feasible in real-world scenario. In this work, we propose an adaptation of Barlow Twins, a state-of-the-art SSL methods, to user sequence modeling by incorporating suitable augmentation methods. Our approach aims to mitigate the need for large negative sample batches, enabling effective representation learning with smaller batch sizes and limited labeled data. We evaluate our method on the MovieLens-1M, MovieLens-20M, and Yelp datasets, demonstrating that our method consistently outperforms the widely-used dual encoder model across three downstream tasks, achieving an 8%-20% improvement in accuracy. Our findings underscore the effectiveness of our approach in extracting valuable sequence-level information for user modeling, particularly in scenarios where labeled data is scarce and negative examples are limited.","authors":["Yuhan Liu","Lin Ning","Neo Wu","Karan Singhal","Philip Andrew Mansfield","Devora Berlowitz","Sushant Prakash","Bradley Green"],"url":"https://arxiv.org/abs/2505.00953"}
{"created":"2025-05-05","title":"Audio Personas: Augmenting Social Perception via Body-Anchored Audio Cues","abstract":"We introduce Audio Personas, enabling users to \"decorate\" themselves with body-anchored sounds in audio augmented reality. Like outfits, makeup, and fragrances, audio personas offer an alternative yet dynamic channel to augment face-to-face interactions. For instance, one can set their audio persona as rain sounds to reflect a bad mood, bee sounds to establish personal boundaries, or a playful \"woosh\" sound to mimic passing by someone like a breeze. To instantiate the concept, we implemented a headphone-based prototype with multi-user tracking and audio streaming. Our formative study with designers revealed that audio personas were preferred in public and semi-public-private spaces for managing social impressions (e.g., personality) and signaling current states (e.g., emotions). Our preregistered in-lab study with 64 participants showed that audio personas influenced how participants formed impressions. Individuals with positive audio personas were rated as more socially attractive, more likable, and less threatening than those with negative audio personas.","authors":["Yujie Tao","Libby Ye","Jeremy N. Bailenson","Sean Follmer"],"url":"https://arxiv.org/abs/2505.00956"}
{"created":"2025-05-05","title":"Extended Persistent Homology Distinguishes Simple and Complex Contagions with High Accuracy","abstract":"The social contagion literature makes a distinction between simple (independent cascade or bond percolation processes that pass infections through edges) and complex contagions (bootstrap percolation or threshold processes that require local reinforcement to spread). However, distinguishing simple and complex contagions using observational data poses a significant challenge in practice. Estimating population-level activation functions from observed contagion dynamics is hindered by confounding factors that influence adoptions (other than neighborhood interactions), as well as heterogeneity in individual behaviors and modeling variations that make it difficult to design appropriate null models for inferring contagion types. Here, we show that a new tool from topological data analysis (TDA), called extended persistent homology (EPH), when applied to contagion processes over networks, can effectively detect simple and complex contagion processes, as well as predict their parameters. We train classification and regression models using EPH-based topological summaries computed on simulated simple and complex contagion dynamics on three real-world network datasets and obtain high predictive performance over a wide range of contagion parameters and under a variety of informational constraints, including uncertainty in model parameters, noise, and partial observability of contagion dynamics. EPH captures the role of cycles of varying lengths in the observed contagion dynamics and offers a useful metric to classify contagion models and predict their parameters. Analyzing geometrical features of network contagion using TDA tools such as EPH can find applications in other network problems such as seeding, vaccination, and quarantine optimization, as well as network inference and reconstruction problems.","authors":["Vahid Shamsaddini","M. Amin Rahimian"],"url":"https://arxiv.org/abs/2505.00958"}
{"created":"2025-05-05","title":"The Open-Source BlackParrot-BedRock Cache Coherence System","abstract":"This dissertation revisits the topic of programmable cache coherence engines in the context of modern shared-memory multicore processors. First, the open-source BedRock cache coherence protocol is described. BedRock employs the canonical MOESIF coherence states and reduces implementation burden by eliminating transient coherence states from the protocol. The protocol's design complexity, concurrency, and verification effort are analyzed and compared to a canonical directory-based invalidate coherence protocol. Second, the architecture and microarchitecture of three separate cache coherence directories implementing the BedRock protocol within the BlackParrot 64-bit RISC-V multicore processor, collectively called BlackParrot-BedRock (BP-BedRock), are described. A fixed-function coherence directory engine implementation provides a baseline design for performance and area comparisons. A microcode-programmable coherence directory implementation demonstrates the feasibility of implementing a programmable coherence engine capable of maintaining sufficient protocol processing performance. A hybrid fixed-function and programmable coherence directory blends the protocol processing performance of the fixed-function design with the programmable flexibility of the microcode-programmable design. Collectively, the BedRock coherence protocol and its three BP-BedRock implementations demonstrate the feasibility and challenges of including programmable logic within the coherence system of modern shared-memory multicore processors, paving the way for future research into the application- and system-level benefits of programmable coherence engines.","authors":["Mark Unruh Wyse"],"url":"https://arxiv.org/abs/2505.00962"}
{"created":"2025-05-05","title":"Adaptive Branch-and-Bound Tree Exploration for Neural Network Verification","abstract":"Formal verification is a rigorous approach that can provably ensure the quality of neural networks, and to date, Branch and Bound (BaB) is the state-of-the-art that performs verification by splitting the problem as needed and applying off-the-shelf verifiers to sub-problems for improved performance. However, existing BaB may not be efficient, due to its naive way of exploring the space of sub-problems that ignores the \\emph{importance} of different sub-problems. To bridge this gap, we first introduce a notion of ``importance'' that reflects how likely a counterexample can be found with a sub-problem, and then we devise a novel verification approach, called ABONN, that explores the sub-problem space of BaB adaptively, in a Monte-Carlo tree search (MCTS) style. The exploration is guided by the ``importance'' of different sub-problems, so it favors the sub-problems that are more likely to find counterexamples. As soon as it finds a counterexample, it can immediately terminate; even though it cannot find, after visiting all the sub-problems, it can still manage to verify the problem. We evaluate ABONN with 552 verification problems from commonly-used datasets and neural network models, and compare it with the state-of-the-art verifiers as baseline approaches. Experimental evaluation shows that ABONN demonstrates speedups of up to $15.2\\times$ on MNIST and $24.7\\times$ on CIFAR-10. We further study the influences of hyperparameters to the performance of ABONN, and the effectiveness of our adaptive tree exploration.","authors":["Kota Fukuda","Guanqin Zhang","Zhenya Zhang","Yulei Sui","Jianjun Zhao"],"url":"https://arxiv.org/abs/2505.00963"}
{"created":"2025-05-05","title":"The AI Fairness Myth: A Position Paper on Context-Aware Bias","abstract":"Defining fairness in AI remains a persistent challenge, largely due to its deeply context-dependent nature and the lack of a universal definition. While numerous mathematical formulations of fairness exist, they sometimes conflict with one another and diverge from social, economic, and legal understandings of justice. Traditional quantitative definitions primarily focus on statistical comparisons, but they often fail to simultaneously satisfy multiple fairness constraints. Drawing on philosophical theories (Rawls' Difference Principle and Dworkin's theory of equality) and empirical evidence supporting affirmative action, we argue that fairness sometimes necessitates deliberate, context-aware preferential treatment of historically marginalized groups. Rather than viewing bias solely as a flaw to eliminate, we propose a framework that embraces corrective, intentional biases to promote genuine equality of opportunity. Our approach involves identifying unfairness, recognizing protected groups/individuals, applying corrective strategies, measuring impact, and iterating improvements. By bridging mathematical precision with ethical and contextual considerations, we advocate for an AI fairness paradigm that goes beyond neutrality to actively advance social justice.","authors":["Kessia Nepomuceno","Fabio Petrillo"],"url":"https://arxiv.org/abs/2505.00965"}
{"created":"2025-05-05","title":"SemSpaceFL: A Collaborative Hierarchical Federated Learning Framework for Semantic Communication in 6G LEO Satellites","abstract":"The advent of the sixth-generation (6G) wireless networks, enhanced by artificial intelligence, promises ubiquitous connectivity through Low Earth Orbit (LEO) satellites. These satellites are capable of collecting vast amounts of geographically diverse and real-time data, which can be immensely valuable for training intelligent models. However, limited inter-satellite communication and data privacy constraints hinder data collection on a single server for training. Therefore, we propose SemSpaceFL, a novel hierarchical federated learning (HFL) framework for LEO satellite networks, with integrated semantic communication capabilities. Our framework introduces a two-tier aggregation architecture where satellite models are first aggregated at regional gateways before final consolidation at a cloud server, which explicitly accounts for satellite mobility patterns and energy constraints. The key innovation lies in our novel aggregation approach, which dynamically adjusts the contribution of each satellite based on its trajectory and association with different gateways, which ensures stable model convergence despite the highly dynamic nature of LEO constellations. To further enhance communication efficiency, we incorporate semantic encoding-decoding techniques trained through the proposed HFL framework, which enables intelligent data compression while maintaining signal integrity. Our experimental results demonstrate that the proposed aggregation strategy achieves superior performance and faster convergence compared to existing benchmarks, while effectively managing the challenges of satellite mobility and energy limitations in dynamic LEO networks.","authors":["Loc X. Nguyen","Sheikh Salman Hassan","Yu Min Park","Yan Kyaw Tun","Zhu Han","Choong Seon Hong"],"url":"https://arxiv.org/abs/2505.00966"}
{"created":"2025-05-05","title":"A SCADE Model Verification Method Based on B-Model Transformation","abstract":"Due to the limitations of SCADE models in expressing and verifying abstract specifications in safety-critical systems, this study proposes a formal verification framework based on the B-Method. By establishing a semantic equivalence transformation mechanism from SCADE models to B models, a hierarchical mapping rule set is constructed, covering type systems, control flow structures, and state machines. This effectively addresses key technical challenges such as loop-equivalent transformation proof for high-order operators and modeling of temporal logic storage structures. The proposed method innovatively leverages the abstraction capabilities of B-Method in set theory and first-order logic, overcoming the constraints of native verification tools of SCADE in complex specification descriptions. It successfully verifies abstract specifications that are difficult to model directly in SCADE. Experimental results show that the transformed B models achieve a higher defect detection rate and improved verification efficiency in the ProB verification environment compared to the native verifier of SCADE, significantly enhancing the formal verification capability of safety-critical systems. This study provides a cross-model verification paradigm for embedded control systems in avionics, rail transportation, and other domains, demonstrating substantial engineering application value.","authors":["Xili Hou","Keming Wang","Huibing Zhao","Ruiyin Shi"],"url":"https://arxiv.org/abs/2505.00967"}
{"created":"2025-05-05","title":"Tree-Sliced Wasserstein Distance with Nonlinear Projection","abstract":"Tree-Sliced methods have recently emerged as an alternative to the traditional Sliced Wasserstein (SW) distance, replacing one-dimensional lines with tree-based metric spaces and incorporating a splitting mechanism for projecting measures. This approach enhances the ability to capture the topological structures of integration domains in Sliced Optimal Transport while maintaining low computational costs. Building on this foundation, we propose a novel nonlinear projectional framework for the Tree-Sliced Wasserstein (TSW) distance, substituting the linear projections in earlier versions with general projections, while ensuring the injectivity of the associated Radon Transform and preserving the well-definedness of the resulting metric. By designing appropriate projections, we construct efficient metrics for measures on both Euclidean spaces and spheres. Finally, we validate our proposed metric through extensive numerical experiments for Euclidean and spherical datasets. Applications include gradient flows, self-supervised learning, and generative models, where our methods demonstrate significant improvements over recent SW and TSW variants.","authors":["Thanh Tran","Viet-Hoang Tran","Thanh Chu","Trang Pham","Laurent El Ghaoui","Tam Le","Tan M. Nguyen"],"url":"https://arxiv.org/abs/2505.00968"}
{"created":"2025-05-05","title":"Real-time Two-tape Control System in Vine robots","abstract":"This paper focuses on how to make a growing Vine robot steer in different directions with a novel approach to real-time steering control by autonomously applying adhesive tape to induce a surface wrinkles. This enabling real-time directional control with arbitrary many turns while maintaining the robot's soft structure. This system feeds growing material external to the tube. The design achieves fixed-angle turns in 2D space. Through experimental validation, we demonstrate repeated 21-degree turns using a Dubins path planner with minimal error, establishing a foundation for more versatile Vine robot applications. This approach combines real-time control, multi-degree-of-freedom steering, and structural flexibility, addressing key challenges in soft robotics.","authors":["Hanmo Liu","Kayleen Smith","Zimu Yang","Mark Yim"],"url":"https://arxiv.org/abs/2505.00969"}
{"created":"2025-05-05","title":"LZD-style Compression Scheme with Truncation and Repetitions","abstract":"Lempel-Ziv-Double (LZD) is a variation of the LZ78 compression scheme that achieves better compression on repetitive datasets. Nevertheless, prior research has identified computational inefficiencies and a weakness in its compressibility for certain datasets. In this paper, we introduce LZD+, an enhancement of LZD, which enables expected linear-time online compression by allowing truncated references. To avoid the compressibility weakness exhibited by a lower bound example, we propose LZDR (LZD-runlength compressed), a further enhancement on top of LZD+, which introduces a repetition-based factorization rule while maintaining linear expected time complexity. The both time bounds can be de-randomized by a lookup data structure like a balanced search tree with a logarithmic dependency on the alphabet size. Additionally, we present three flexible parsing variants of LZDR that yield fewer factors in practice. Comprehensive benchmarking on standard corpora reveals that LZD+, LZDR, and its flexible variants outperform existing LZ-based methods in the number of factors while keeping competitive runtime efficiency. However, we note that the difference in the number of factors becomes marginal for large datasets like those of the Pizza&amp;Chili corpus.","authors":["Linus G\\\"otz","Dominik K\\\"oppl"],"url":"https://arxiv.org/abs/2505.00970"}
{"created":"2025-05-05","title":"Seeking to Collide: Online Safety-Critical Scenario Generation for Autonomous Driving with Retrieval Augmented Large Language Models","abstract":"Simulation-based testing is crucial for validating autonomous vehicles (AVs), yet existing scenario generation methods either overfit to common driving patterns or operate in an offline, non-interactive manner that fails to expose rare, safety-critical corner cases. In this paper, we introduce an online, retrieval-augmented large language model (LLM) framework for generating safety-critical driving scenarios. Our method first employs an LLM-based behavior analyzer to infer the most dangerous intent of the background vehicle from the observed state, then queries additional LLM agents to synthesize feasible adversarial trajectories. To mitigate catastrophic forgetting and accelerate adaptation, we augment the framework with a dynamic memorization and retrieval bank of intent-planner pairs, automatically expanding its behavioral library when novel intents arise. Evaluations using the Waymo Open Motion Dataset demonstrate that our model reduces the mean minimum time-to-collision from 1.62 to 1.08 s and incurs a 75% collision rate, substantially outperforming baselines.","authors":["Yuewen Mei","Tong Nie","Jian Sun","Ye Tian"],"url":"https://arxiv.org/abs/2505.00972"}
{"created":"2025-05-05","title":"A Minimax-MDP Framework with Future-imposed Conditions for Learning-augmented Problems","abstract":"We study a class of sequential decision-making problems with augmented predictions, potentially provided by a machine learning algorithm. In this setting, the decision-maker receives prediction intervals for unknown parameters that become progressively refined over time, and seeks decisions that are competitive with the hindsight optimal under all possible realizations of both parameters and predictions. We propose a minimax Markov Decision Process (minimax-MDP) framework, where the system state consists of an adversarially evolving environment state and an internal state controlled by the decision-maker. We introduce a set of future-imposed conditions that characterize the feasibility of minimax-MDPs and enable the design of efficient, often closed-form, robustly competitive policies. We illustrate the framework through three applications: multi-period inventory ordering with refining demand predictions, resource allocation with uncertain utility functions, and a multi-phase extension of the minimax-MDP applied to the inventory problem with time-varying ordering costs. Our results provide a tractable and versatile approach to robust online decision-making under predictive uncertainty.","authors":["Xin Chen","Yuze Chen","Yuan Zhou"],"url":"https://arxiv.org/abs/2505.00973"}
{"created":"2025-05-05","title":"On the Worst-Case Complexity of Gibbs Decoding for Reed--Muller Codes","abstract":"Reed--Muller (RM) codes are known to achieve capacity on binary symmetric channels (BSC) under the Maximum a Posteriori (MAP) decoder. However, it remains an open problem to design a capacity achieving polynomial-time RM decoder. Due to a lemma by Liu, Cuff, and Verd\\'u, it can be shown that decoding by sampling from the posterior distribution is also capacity-achieving for RM codes over BSC. The Gibbs decoder is one such Markov Chain Monte Carlo (MCMC) based method, which samples from the posterior distribution by flipping message bits according to the posterior, and can be modified to give other MCMC decoding methods. In this paper, we analyze the mixing time of the Gibbs decoder for RM codes. Our analysis reveals that the Gibbs decoder can exhibit slow mixing for certain carefully constructed sequences. This slow mixing implies that, in the worst-case scenario, the decoder requires super-polynomial time to converge to the desired posterior distribution.","authors":["Xuzhe Xia","Nicholas Kwan","Lele Wang"],"url":"https://arxiv.org/abs/2505.00974"}
{"created":"2025-05-05","title":"Generating Animated Layouts as Structured Text Representations","abstract":"Despite the remarkable progress in text-to-video models, achieving precise control over text elements and animated graphics remains a significant challenge, especially in applications such as video advertisements. To address this limitation, we introduce Animated Layout Generation, a novel approach to extend static graphic layouts with temporal dynamics. We propose a Structured Text Representation for fine-grained video control through hierarchical visual elements. To demonstrate the effectiveness of our approach, we present VAKER (Video Ad maKER), a text-to-video advertisement generation pipeline that combines a three-stage generation process with Unstructured Text Reasoning for seamless integration with LLMs. VAKER fully automates video advertisement generation by incorporating dynamic layout trajectories for objects and graphics across specific video frames. Through extensive evaluations, we demonstrate that VAKER significantly outperforms existing methods in generating video advertisements. Project Page: https://yeonsangshin.github.io/projects/Vaker","authors":["Yeonsang Shin","Jihwan Kim","Yumin Song","Kyungseung Lee","Hyunhee Chung","Taeyoung Na"],"url":"https://arxiv.org/abs/2505.00975"}
{"created":"2025-05-05","title":"Attack and defense techniques in large language models: A survey and new perspectives","abstract":"Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.","authors":["Zhiyu Liao","Kang Chen","Yuanguo Lin","Kangkang Li","Yunxuan Liu","Hefeng Chen","Xingwang Huang","Yuanhui Yu"],"url":"https://arxiv.org/abs/2505.00976"}
{"created":"2025-05-05","title":"A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts","abstract":"Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.","authors":["Yingquan Chen","Qianmu Li","Xiaocong Wu","Huifeng Li","Qing Chang"],"url":"https://arxiv.org/abs/2505.00977"}
{"created":"2025-05-05","title":"Synthesize-on-Graph: Knowledgeable Synthetic Data Generation for Continue Pre-training of Large Language Models","abstract":"Large Language Models (LLMs) have achieved remarkable success but remain data-inefficient, especially when learning from small, specialized corpora with limited and proprietary data. Existing synthetic data generation methods for continue pre-training focus on intra-document content and overlook cross-document knowledge associations, limiting content diversity and depth. We propose Synthetic-on-Graph (SoG), a synthetic data generation framework that incorporates cross-document knowledge associations for efficient corpus expansion. SoG constructs a context graph by extracting entities and concepts from the original corpus, representing cross-document associations, and employing a graph walk strategy for knowledge-associated sampling. This enhances synthetic data diversity and coherence, enabling models to learn complex knowledge structures and handle rare knowledge. To further improve synthetic data quality, we integrate Chain-of-Thought (CoT) and Contrastive Clarifying (CC) synthetic, enhancing reasoning processes and discriminative power. Experiments show that SoG outperforms the state-of-the-art (SOTA) method in a multi-hop document Q&amp;A dataset while performing comparably to the SOTA method on the reading comprehension task datasets, which also underscores the better generalization capability of SoG. Our work advances synthetic data generation and provides practical solutions for efficient knowledge acquisition in LLMs, especially in domains with limited data availability.","authors":["Xuhui Jiang","Shengjie Ma","Chengjin Xu","Cehao Yang","Liyu Zhang","Jian Guo"],"url":"https://arxiv.org/abs/2505.00979"}
{"created":"2025-05-05","title":"LMDepth: Lightweight Mamba-based Monocular Depth Estimation for Real-World Deployment","abstract":"Monocular depth estimation provides an additional depth dimension to RGB images, making it widely applicable in various fields such as virtual reality, autonomous driving and robotic navigation. However, existing depth estimation algorithms often struggle to effectively balance performance and computational efficiency, which poses challenges for deployment on resource-constrained devices. To address this, we propose LMDepth, a lightweight Mamba-based monocular depth estimation network, designed to reconstruct high-precision depth information while maintaining low computational overhead. Specifically, we propose a modified pyramid spatial pooling module that serves as a multi-scale feature aggregator and context extractor, ensuring global spatial information for accurate depth estimation. Moreover, we integrate multiple depth Mamba blocks into the decoder. Designed with linear computations, the Mamba Blocks enable LMDepth to efficiently decode depth information from global features, providing a lightweight alternative to Transformer-based architectures that depend on complex attention mechanisms. Extensive experiments on the NYUDv2 and KITTI datasets demonstrate the effectiveness of our proposed LMDepth. Compared to previous lightweight depth estimation methods, LMDepth achieves higher performance with fewer parameters and lower computational complexity (measured by GFLOPs). We further deploy LMDepth on an embedded platform with INT8 quantization, validating its practicality for real-world edge applications.","authors":["Jiahuan Long","Xin Zhou"],"url":"https://arxiv.org/abs/2505.00980"}
{"created":"2025-05-05","title":"Multi-agents based User Values Mining for Recommendation","abstract":"Recommender systems have rapidly evolved and become integral to many online services. However, existing systems sometimes produce unstable and unsatisfactory recommendations that fail to align with users' fundamental and long-term preferences. This is because they primarily focus on extracting shallow and short-term interests from user behavior data, which is inherently dynamic and challenging to model. Unlike these transient interests, user values are more stable and play a crucial role in shaping user behaviors, such as purchasing items and consuming content. Incorporating user values into recommender systems can help stabilize recommendation performance and ensure results better reflect users' latent preferences. However, acquiring user values is typically difficult and costly. To address this challenge, we leverage the strong language understanding, zero-shot inference, and generalization capabilities of Large Language Models (LLMs) to extract user values from users' historical interactions. Unfortunately, direct extraction using LLMs presents several challenges such as length constraints and hallucination. To overcome these issues, we propose ZOOM, a zero-shot multi-LLM collaborative framework for effective and accurate user value extraction. In ZOOM, we apply text summarization techniques to condense item content while preserving essential meaning. To mitigate hallucinations, ZOOM introduces two specialized agent roles: evaluators and supervisors, to collaboratively generate accurate user values. Extensive experiments on two widely used recommendation datasets with two state-of-the-art recommendation models demonstrate the effectiveness and generalization of our framework in automatic user value mining and recommendation performance improvement.","authors":["Lijian Chen","Wei Yuan","Tong Chen","Xiangyu Zhao","Nguyen Quoc Viet Hung","Hongzhi Yin"],"url":"https://arxiv.org/abs/2505.00981"}
{"created":"2025-05-05","title":"Accelerating Deep Neural Network Training via Distributed Hybrid Order Optimization","abstract":"Scaling deep neural network (DNN) training to more devices can reduce time-to-solution. However, it is impractical for users with limited computing resources. FOSI, as a hybrid order optimizer, converges faster than conventional optimizers by taking advantage of both gradient information and curvature information when updating the DNN model. Therefore, it provides a new chance for accelerating DNN training in the resource-constrained setting. In this paper, we explore its distributed design, namely DHO$_2$, including distributed calculation of curvature information and model update with partial curvature information to accelerate DNN training with a low memory burden. To further reduce the training time, we design a novel strategy to parallelize the calculation of curvature information and the model update on different devices. Experimentally, our distributed design can achieve an approximate linear reduction of memory burden on each device with the increase of the device number. Meanwhile, it achieves $1.4\\times\\sim2.1\\times$ speedup in the total training time compared with other distributed designs based on conventional first- and second-order optimizers.","authors":["Shunxian Gu","Chaoqun You","Bangbang Ren","Lailong Luo","Junxu Xia","Deke Guo"],"url":"https://arxiv.org/abs/2505.00982"}
{"created":"2025-05-05","title":"Toward Data-centric Directed Graph Learning: An Entropy-driven Approach","abstract":"The directed graph (digraph), as a generalization of undirected graphs, exhibits superior representation capability in modeling complex topology systems and has garnered considerable attention in recent years. Despite the notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage directed edges, they still fail to comprehensively delve into the abundant data knowledge concealed in the digraphs. This data-level limitation results in model-level sub-optimal predictive performance and underscores the necessity of further exploring the potential correlations between the directed edges (topology) and node profiles (feature and labels) from a data-centric perspective, thereby empowering model-centric neural networks with stronger encoding capabilities.","authors":["Xunkai Li","Zhengyu Wu","Kaichi Yu","Hongchao Qin","Guang Zeng","Rong-Hua Li","Guoren Wang"],"url":"https://arxiv.org/abs/2505.00983"}
{"created":"2025-05-05","title":"Position: Enough of Scaling LLMs! Lets Focus on Downscaling","abstract":"We challenge the dominant focus on neural scaling laws and advocate for a paradigm shift toward downscaling in the development of large language models (LLMs). While scaling laws have provided critical insights into performance improvements through increasing model and dataset size, we emphasize the significant limitations of this approach, particularly in terms of computational inefficiency, environmental impact, and deployment constraints. To address these challenges, we propose a holistic framework for downscaling LLMs that seeks to maintain performance while drastically reducing resource demands. This paper outlines practical strategies for transitioning away from traditional scaling paradigms, advocating for a more sustainable, efficient, and accessible approach to LLM development.","authors":["Ayan Sengupta","Yash Goel","Tanmoy Chakraborty"],"url":"https://arxiv.org/abs/2505.00985"}
{"created":"2025-05-05","title":"On-demand Test-time Adaptation for Edge Devices","abstract":"Continual Test-time adaptation (CTTA) continuously adapts the deployed model on every incoming batch of data. While achieving optimal accuracy, existing CTTA approaches present poor real-world applicability on resource-constrained edge devices, due to the substantial memory overhead and energy consumption. In this work, we first introduce a novel paradigm -- on-demand TTA -- which triggers adaptation only when a significant domain shift is detected. Then, we present OD-TTA, an on-demand TTA framework for accurate and efficient adaptation on edge devices. OD-TTA comprises three innovative techniques: 1) a lightweight domain shift detection mechanism to activate TTA only when it is needed, drastically reducing the overall computation overhead, 2) a source domain selection module that chooses an appropriate source model for adaptation, ensuring high and robust accuracy, 3) a decoupled Batch Normalization (BN) update scheme to enable memory-efficient adaptation with small batch sizes. Extensive experiments show that OD-TTA achieves comparable and even better performance while reducing the energy and computation overhead remarkably, making TTA a practical reality.","authors":["Xiao Ma","Young D. Kwon","Dong Ma"],"url":"https://arxiv.org/abs/2505.00986"}
{"created":"2025-05-05","title":"Destructive Interference: Encoding Loss in the Overlap","abstract":"Destructive Interference is a data visualization installation that representing the deaths and injuries caused by mass shootings in 2024 in the United States. I parametrically designed and fabricated an interlocking ring sculpture for each month of 2024; where the overall height corresponds to the level of violence in that month. Taller forms mark the deadliest months, while shorter ones reflect fewer casualties. Each inner ring encodes the number of people killed or injured, and each outer ring encodes the number of shootings and the number of days without them. The interlocking cylinders are powered via a motor to rotate, and lit from within. As the cylinders rotate, they cast overlapping shadows that represent those killed or injured by mass shootings. The goal of this work is to visualize otherwise overwhelming and disparate statistics in a way that is both physically present and emotionally resonant. By inviting viewers to step into and engage with these shadows, the piece creates space for reflection, conversation, and confrontation with the scale of this ongoing crisis.","authors":["Nik Aberle"],"url":"https://arxiv.org/abs/2505.00987"}
{"created":"2025-05-05","title":"The tape reconfiguration problem and its consequences for dominating set reconfiguration","abstract":"A dominating set of a graph $G=(V,E)$ is a set of vertices $D \\subseteq V$ whose closed neighborhood is $V$, i.e., $N[D]=V$. We view a dominating set as a collection of tokens placed on the vertices of $D$. In the token sliding variant of the Dominating Set Reconfiguration problem (TS-DSR), we seek to transform a source dominating set into a target dominating set in $G$ by sliding tokens along edges, and while maintaining a dominating set all along the transformation.","authors":["Nicolas Bousquet","Quentin Deschamps","Arnaud Mary","Amer E. Mouawad","Th\\'eo Pierron"],"url":"https://arxiv.org/abs/2505.00988"}
{"created":"2025-05-05","title":"VTS-LLM: Domain-Adaptive LLM Agent for Enhancing Awareness in Vessel Traffic Services through Natural Language","abstract":"Vessel Traffic Services (VTS) are essential for maritime safety and regulatory compliance through real-time traffic management. However, with increasing traffic complexity and the prevalence of heterogeneous, multimodal data, existing VTS systems face limitations in spatiotemporal reasoning and intuitive human interaction. In this work, we propose VTS-LLM Agent, the first domain-adaptive large LLM agent tailored for interactive decision support in VTS operations. We formalize risk-prone vessel identification as a knowledge-augmented Text-to-SQL task, combining structured vessel databases with external maritime knowledge. To support this, we construct a curated benchmark dataset consisting of a custom schema, domain-specific corpus, and a query-SQL test set in multiple linguistic styles. Our framework incorporates NER-based relational reasoning, agent-based domain knowledge injection, semantic algebra intermediate representation, and query rethink mechanisms to enhance domain grounding and context-aware understanding. Experimental results show that VTS-LLM outperforms both general-purpose and SQL-focused baselines under command-style, operational-style, and formal natural language queries, respectively. Moreover, our analysis provides the first empirical evidence that linguistic style variation introduces systematic performance challenges in Text-to-SQL modeling. This work lays the foundation for natural language interfaces in vessel traffic services and opens new opportunities for proactive, LLM-driven maritime real-time traffic management.","authors":["Sijin Sun","Liangbin Zhao","Ming Deng","Xiuju Fu"],"url":"https://arxiv.org/abs/2505.00989"}
{"created":"2025-05-05","title":"Identifying Root Cause of bugs by Capturing Changed Code Lines with Relational Graph Neural Networks","abstract":"The Just-In-Time defect prediction model helps development teams improve software quality and efficiency by assessing whether code changes submitted by developers are likely to introduce defects in real-time, allowing timely identification of potential issues during the commit stage. However, two main challenges exist in current work due to the reality that all deleted and added lines in bug-fixing commits may be related to the root cause of the introduced bug: 1) lack of effective integration of heterogeneous graph information, and 2) lack of semantic relationships between changed code lines. To address these challenges, we propose a method called RC-Detection, which utilizes relational graph convolutional network to capture the semantic relationships between changed code lines. RC-Detection is used to detect root-cause deletion lines in changed code lines, thereby identifying the root cause of introduced bugs in bug-fixing commits. To evaluate the effectiveness of RC-Detection, we used three datasets that contain high-quality bug-fixing and bug-introducing commits. Extensive experiments were conducted to evaluate the performance of our model by collecting data from 87 open-source projects, including 675 bug-fix commits. The experimental results show that, compared to the most advanced root cause detection methods, RC-Detection improved Recall@1, Recall@2, Recall@3, and MFR by at 4.107%, 5.113%, 4.289%, and 24.536%, respectively.","authors":["Jiaqi Zhang","Shikai Guo","Hui Li","Chenchen Li","Yu Chai","Rong Chen"],"url":"https://arxiv.org/abs/2505.00990"}
{"created":"2025-05-05","title":"DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning","abstract":"Dexterous manipulation has seen remarkable progress in recent years, with policies capable of executing many complex and contact-rich tasks in simulation. However, transferring these policies from simulation to real world remains a significant challenge. One important issue is the mismatch in low-level controller dynamics, where identical trajectories can lead to vastly different contact forces and behaviors when control parameters vary. Existing approaches often rely on manual tuning or controller randomization, which can be labor-intensive, task-specific, and introduce significant training difficulty. In this work, we propose a framework that jointly learns actions and controller parameters based on the historical information of both trajectory and controller. This adaptive controller adjustment mechanism allows the policy to automatically tune control parameters during execution, thereby mitigating the sim-to-real gap without extensive manual tuning or excessive randomization. Moreover, by explicitly providing controller parameters as part of the observation, our approach facilitates better reasoning over force interactions and improves robustness in real-world scenarios. Experimental results demonstrate that our method achieves improved transfer performance across a variety of dexterous tasks involving variable force conditions.","authors":["Shuqi Zhao","Ke Yang","Yuxin Chen","Chenran Li","Yichen Xie","Xiang Zhang","Changhao Wang","Masayoshi Tomizuka"],"url":"https://arxiv.org/abs/2505.00991"}
{"created":"2025-05-05","title":"Optimizing Indoor Farm Monitoring Efficiency Using UAV: Yield Estimation in a GNSS-Denied Cherry Tomato Greenhouse","abstract":"As the agricultural workforce declines and labor costs rise, robotic yield estimation has become increasingly important. While unmanned ground vehicles (UGVs) are commonly used for indoor farm monitoring, their deployment in greenhouses is often constrained by infrastructure limitations, sensor placement challenges, and operational inefficiencies. To address these issues, we develop a lightweight unmanned aerial vehicle (UAV) equipped with an RGB-D camera, a 3D LiDAR, and an IMU sensor. The UAV employs a LiDAR-inertial odometry algorithm for precise navigation in GNSS-denied environments and utilizes a 3D multi-object tracking algorithm to estimate the count and weight of cherry tomatoes. We evaluate the system using two dataset: one from a harvesting row and another from a growing row. In the harvesting-row dataset, the proposed system achieves 94.4\\% counting accuracy and 87.5\\% weight estimation accuracy within a 13.2-meter flight completed in 10.5 seconds. For the growing-row dataset, which consists of occluded unripened fruits, we qualitatively analyze tracking performance and highlight future research directions for improving perception in greenhouse with strong occlusions. Our findings demonstrate the potential of UAVs for efficient robotic yield estimation in commercial greenhouses.","authors":["Taewook Park","Jinwoo Lee","Hyondong Oh","Won-Jae Yun","Kyu-Wha Lee"],"url":"https://arxiv.org/abs/2505.00995"}
{"created":"2025-05-05","title":"Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis","abstract":"Human motion synthesis aims to generate plausible human motion sequences, which has raised widespread attention in computer animation. Recent score-based generative models (SGMs) have demonstrated impressive results on this task. However, their training process involves complex curvature trajectories, leading to unstable training process. In this paper, we propose a Deterministic-to-Stochastic Diverse Latent Feature Mapping (DSDFM) method for human motion synthesis. DSDFM consists of two stages. The first human motion reconstruction stage aims to learn the latent space distribution of human motions. The second diverse motion generation stage aims to build connections between the Gaussian distribution and the latent space distribution of human motions, thereby enhancing the diversity and accuracy of the generated human motions. This stage is achieved by the designed deterministic feature mapping procedure with DerODE and stochastic diverse output generation procedure with DivSDE.DSDFM is easy to train compared to previous SGMs-based methods and can enhance diversity without introducing additional training parameters.Through qualitative and quantitative experiments, DSDFM achieves state-of-the-art results surpassing the latest methods, validating its superiority in human motion synthesis.","authors":["Yu Hua","Weiming Liu","Gui Xu","Yaqing Hou","Yew-Soon Ong","Qiang Zhang"],"url":"https://arxiv.org/abs/2505.00998"}
{"created":"2025-05-05","title":"Togedule: Scheduling Meetings with Large Language Models and Adaptive Representations of Group Availability","abstract":"Scheduling is a perennial-and often challenging-problem for many groups. Existing tools are mostly static, showing an identical set of choices to everyone, regardless of the current status of attendees' inputs and preferences. In this paper, we propose Togedule, an adaptive scheduling tool that uses large language models to dynamically adjust the pool of choices and their presentation format. With the initial prototype, we conducted a formative study (N=10) and identified the potential benefits and risks of such an adaptive scheduling tool. Then, after enhancing the system, we conducted two controlled experiments, one each for attendees and organizers (total N=66). For each experiment, we compared scheduling with verbal messages, shared calendars, or Togedule. Results show that Togedule significantly reduces the cognitive load of attendees indicating their availability and improves the speed and quality of the decisions made by organizers.","authors":["Jaeyoon Song","Zahra Ashktorab","Thomas W. Malone"],"url":"https://arxiv.org/abs/2505.01000"}
{"created":"2025-05-05","title":"Photoshop Batch Rendering Using Actions for Stylistic Video Editing","abstract":"My project looks at an efficient workflow for creative image/video editing using Adobe Photoshop Actions tool and Batch Processing System. This innovative approach to video editing through Photoshop creates a fundamental shift to creative workflow management through the integration of industry-leading image manipulation with video editing techniques. Through systematic automation of Actions, users can achieve a simple and consistent application of visual edits across a string of images. This approach provides an alternative method to optimize productivity while ensuring uniform results across image collections through a post-processing pipeline.","authors":["Tessa De La Fuente"],"url":"https://arxiv.org/abs/2505.01001"}
{"created":"2025-05-05","title":"3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer","abstract":"Nowadays, Transformers and Graph Convolutional Networks (GCNs) are the prevailing techniques for 3D human pose estimation. However, Transformer-based methods either ignore the spatial neighborhood relationships between the joints when used for skeleton representations or disregard the local temporal patterns of the local joint movements in skeleton sequence modeling, while GCN-based methods often neglect the need for pose-specific representations. To address these problems, we propose a new method that exploits the graph modeling capability of GCN to represent each skeleton with multiple graphs of different orders, incorporated with a newly introduced Graph Order Attention module that dynamically emphasizes the most representative orders for each joint. The resulting spatial features of the sequence are further processed using a proposed temporal Body Aware Transformer that models the global body feature dependencies in the sequence with awareness of the local inter-skeleton feature dependencies of joints. Given that our 3D pose output aligns with the central 2D pose in the sequence, we improve the self-attention mechanism to be aware of the central pose while diminishing its focus gradually towards the first and the last poses. Extensive experiments on Human3.6m, MPIINF-3DHP, and HumanEva-I datasets demonstrate the effectiveness of the proposed method. Code and models are made available on Github.","authors":["Kamel Aouaidjia","Aofan Li","Wenhao Zhang","Chongsheng Zhang"],"url":"https://arxiv.org/abs/2505.01003"}
{"created":"2025-05-05","title":"Token-free Models for Sarcasm Detection","abstract":"Tokenization is a foundational step in most natural language processing (NLP) pipelines, yet it introduces challenges such as vocabulary mismatch and out-of-vocabulary issues. Recent work has shown that models operating directly on raw text at the byte or character level can mitigate these limitations. In this paper, we evaluate two token-free models, ByT5 and CANINE, on the task of sarcasm detection in both social media (Twitter) and non-social media (news headlines) domains. We fine-tune and benchmark these models against token-based baselines and state-of-the-art approaches. Our results show that ByT5-small and CANINE outperform token-based counterparts and achieve new state-of-the-art performance, improving accuracy by 0.77% and 0.49% on the News Headlines and Twitter Sarcasm datasets, respectively. These findings underscore the potential of token-free models for robust NLP in noisy and informal domains such as social media.","authors":["Sumit Mamtani","Maitreya Sonawane","Kanika Agarwal","Nishanth Sanjeev"],"url":"https://arxiv.org/abs/2505.01006"}
{"created":"2025-05-05","title":"Towards the Resistance of Neural Network Watermarking to Fine-tuning","abstract":"This paper proves a new watermarking method to embed the ownership information into a deep neural network (DNN), which is robust to fine-tuning. Specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised Fourier transform to extract frequency components from the convolutional filter. Additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. In this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. Preliminary experiments demonstrate the effectiveness of our method.","authors":["Ling Tang","Yuefeng Chen","Hui Xue","Quanshi Zhang"],"url":"https://arxiv.org/abs/2505.01007"}
{"created":"2025-05-05","title":"Where's the liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content","abstract":"The recent proliferation of photorealistic images created by generative models has sparked both excitement and concern, as these images are increasingly indistinguishable from real ones to the human eye. While offering new creative and commercial possibilities, the potential for misuse, such as in misinformation and fraud, highlights the need for effective detection methods. Current detection approaches often rely on access to model weights or require extensive collections of real image datasets, limiting their scalability and practical application in real world scenarios. In this work, we introduce a novel black box detection framework that requires only API access, sidestepping the need for model weights or large auxiliary datasets. Our approach leverages a corrupt and recover strategy: by masking part of an image and assessing the model ability to reconstruct it, we measure the likelihood that the image was generated by the model itself. For black-box models that do not support masked image inputs, we incorporate a cost efficient surrogate model trained to align with the target model distribution, enhancing detection capability. Our framework demonstrates strong performance, outperforming baseline methods by 4.31% in mean average precision across eight diffusion model variant datasets.","authors":["Haoyue Bai","Yiyou Sun","Wei Cheng","Haifeng Chen"],"url":"https://arxiv.org/abs/2505.01008"}
{"created":"2025-05-05","title":"Improving Large Language Model Planning with Action Sequence Similarity","abstract":"Planning is essential for artificial intelligence systems to look ahead and proactively determine a course of actions to reach objectives in the virtual and real world. Recent work on large language models (LLMs) sheds light on their planning capability in various tasks. However, it remains unclear what signals in the context influence the model performance. In this work, we explore how to improve the model planning capability through in-context learning (ICL), specifically, what signals can help select the exemplars. Through extensive experiments, we observe that commonly used problem similarity may result in false positives with drastically different plans, which can mislead the model. In response, we propose to sample and filter exemplars leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a two-stage pipeline that first re-samples high AS exemplars and then curates the selected exemplars with dynamic clustering on AS to achieve a balance of relevance and diversity. Our experimental result confirms that GRASE-DC achieves significant performance improvement on various planning tasks (up to ~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a validator, we are able to even boost the performance by 18.9% more.","authors":["Xinran Zhao","Hanie Sedghi","Bernd Bohnet","Dale Schuurmans","Azade Nova"],"url":"https://arxiv.org/abs/2505.01009"}
{"created":"2025-05-05","title":"On calculation of canonical decomposition of Tensor via the grid of local discrepancies","abstract":"The method for calculation of the canonical decomposition that approximates a tensor of high order is considered, which requires moderate computational resources. It is based on the replacement of the approximation error norm (global discrepancy functional) by the grid of local functionals (discrepancies computed on hyperplanes). The point of the global functional minimum in the space of the canonical decomposition cores is determined by the set of the stationary points of local functionals. In result, the estimation of the cores of the canonical decomposition is possible using Newton method applied point-wisely along coordinates and nodes. The discrepancies on the hyperplanes are calculated using Monte-Carlo method. Numerical tests on the approximation of sixth order tensors confirms the efficiency of the proposed approach.","authors":["A. K. Alekseev","A. E. Bondarev","Y. S. Pyatakova"],"url":"https://arxiv.org/abs/2505.01011"}
{"created":"2025-05-05","title":"Value Portrait: Understanding Values of LLMs with Human-aligned Benchmark","abstract":"The importance of benchmarks for assessing the values of language models has been pronounced due to the growing need of more authentic, human-aligned responses. However, existing benchmarks rely on human or machine annotations that are vulnerable to value-related biases. Furthermore, the tested scenarios often diverge from real-world contexts in which models are commonly used to generate text and express values. To address these issues, we propose the Value Portrait benchmark, a reliable framework for evaluating LLMs' value orientations with two key characteristics. First, the benchmark consists of items that capture real-life user-LLM interactions, enhancing the relevance of assessment results to real-world LLM usage and thus ecological validity. Second, each item is rated by human subjects based on its similarity to their own thoughts, and correlations between these ratings and the subjects' actual value scores are derived. This psychometrically validated approach ensures that items strongly correlated with specific values serve as reliable items for assessing those values. Through evaluating 27 LLMs with our benchmark, we find that these models prioritize Benevolence, Security, and Self-Direction values while placing less emphasis on Tradition, Power, and Achievement values. Also, our analysis reveals biases in how LLMs perceive various demographic groups, deviating from real human data.","authors":["Jongwook Han","Dongmin Choi","Woojung Song","Eun-Ju Lee","Yohan Jo"],"url":"https://arxiv.org/abs/2505.01015"}
{"created":"2025-05-05","title":"Fine-Tuning Without Forgetting: Adaptation of YOLOv8 Preserves COCO Performance","abstract":"The success of large pre-trained object detectors hinges on their adaptability to diverse downstream tasks. While fine-tuning is the standard adaptation method, specializing these models for challenging fine-grained domains necessitates careful consideration of feature granularity. The critical question remains: how deeply should the pre-trained backbone be fine-tuned to optimize for the specialized task without incurring catastrophic forgetting of the original general capabilities? Addressing this, we present a systematic empirical study evaluating the impact of fine-tuning depth. We adapt a standard YOLOv8n model to a custom, fine-grained fruit detection dataset by progressively unfreezing backbone layers (freeze points at layers 22, 15, and 10) and training. Performance was rigorously evaluated on both the target fruit dataset and, using a dual-head evaluation architecture, on the original COCO validation set. Our results demonstrate unequivocally that deeper fine-tuning (unfreezing down to layer 10) yields substantial performance gains (e.g., +10\\% absolute mAP50) on the fine-grained fruit task compared to only training the head. Strikingly, this significant adaptation and specialization resulted in negligible performance degradation (<0.1\\% absolute mAP difference) on the COCO benchmark across all tested freeze levels. We conclude that adapting mid-to-late backbone features is highly effective for fine-grained specialization. Critically, our results demonstrate this adaptation can be achieved without the commonly expected penalty of catastrophic forgetting, presenting a compelling case for exploring deeper fine-tuning strategies, particularly when targeting complex domains or when maximizing specialized performance is paramount.","authors":["Vishal Gandhi","Sagar Gandhi"],"url":"https://arxiv.org/abs/2505.01016"}
{"created":"2025-05-05","title":"Tightly Coupled Range Inertial Odometry and Mapping with Exact Point Cloud Downsampling","abstract":"In this work, to facilitate the real-time processing of multi-scan registration error minimization on factor graphs, we devise a point cloud downsampling algorithm based on coreset extraction. This algorithm extracts a subset of the residuals of input points such that the subset yields exactly the same quadratic error function as that of the original set for a given pose. This enables a significant reduction in the number of residuals to be evaluated without approximation errors at the sampling point. Using this algorithm, we devise a complete SLAM framework that consists of odometry estimation based on sliding window optimization and global trajectory optimization based on registration error minimization over the entire map, both of which can run in real time on a standard CPU. The experimental results demonstrate that the proposed framework outperforms state-of-the-art CPU-based SLAM frameworks without the use of GPU acceleration.","authors":["Kenji Koide","Aoki Takanose","Shuji Oishi","Masashi Yokozuka"],"url":"https://arxiv.org/abs/2505.01017"}
{"created":"2025-05-05","title":"Detecting the Root Cause Code Lines in Bug-Fixing Commits by Heterogeneous Graph Learning","abstract":"With the continuous growth in the scale and complexity of software systems, defect remediation has become increasingly difficult and costly. Automated defect prediction tools can proactively identify software changes prone to defects within software projects, thereby enhancing software development efficiency. However, existing work in heterogeneous and complex software projects continues to face challenges, such as struggling with heterogeneous commit structures and ignoring cross-line dependencies in code changes, which ultimately reduce the accuracy of defect identification. To address these challenges, we propose an approach called RC_Detector. RC_Detector comprises three main components: the bug-fixing graph construction component, the code semantic aggregation component, and the cross-line semantic retention component. The bug-fixing graph construction component identifies the code syntax structures and program dependencies within bug-fixing commits and transforms them into heterogeneous graph formats by converting the source code into vector representations. The code semantic aggregation component adapts to heterogeneous data by using heterogeneous attention to learn the hidden semantic representation of target code lines. The cross-line semantic retention component regulates propagated semantic information by using attenuation and reinforcement gates derived from old and new code semantic representations, effectively preserving cross-line semantic relationships. Extensive experiments were conducted to evaluate the performance of our model by collecting data from 87 open-source projects, including 675 bug-fixing commits. The experimental results demonstrate that our model outperforms state-of-the-art approaches, achieving significant improvements of 83.15%,96.83%,78.71%,74.15%,54.14%,91.66%,91.66%, and 34.82% in MFR, respectively, compared with the state-of-the-art approaches.","authors":["Liguo Ji","Shikai Guo","Lehuan Zhang","Hui Li","Yu Chai","Rong Chen"],"url":"https://arxiv.org/abs/2505.01022"}
{"created":"2025-05-05","title":"Adaptive Wizard for Removing Cross-Tier Misconfigurations in Active Directory","abstract":"Security vulnerabilities in Windows Active Directory (AD) systems are typically modeled using an attack graph and hardening AD systems involves an iterative workflow: security teams propose an edge to remove, and IT operations teams manually review these fixes before implementing the removal. As verification requires significant manual effort, we formulate an Adaptive Path Removal Problem to minimize the number of steps in this iterative removal process. In our model, a wizard proposes an attack path in each step and presents it as a set of multiple-choice options to the IT admin. The IT admin then selects one edge from the proposed set to remove. This process continues until the target $t$ is disconnected from source $s$ or the number of proposed paths reaches $B$. The model aims to optimize the human effort by minimizing the expected number of interactions between the IT admin and the security wizard. We first prove that the problem is $\\mathcal{\\#P}$-hard. We then propose a set of solutions including an exact algorithm, an approximate algorithm, and several scalable heuristics. Our best heuristic, called DPR, can operate effectively on larger-scale graphs compared to the exact algorithm and consistently outperforms the approximate algorithm across all graphs. We verify the effectiveness of our algorithms on several synthetic AD graphs and an AD attack graph collected from a real organization.","authors":["Huy Q. Ngo","Mingyu Guo","Hung Nguyen"],"url":"https://arxiv.org/abs/2505.01028"}
{"created":"2025-05-05","title":"Barriers to Employment: The Deaf Multimedia Authoring Tax","abstract":"This paper describes the challenges that deaf and hard of hearing people face with creating accessible multimedia content, such as portfolios, instructional videos and video presentations. Unlike content consumption, the process of content creation itself remains highly inaccessible, creating barriers to employment in all stages of recruiting, hiring, and carrying out assigned job duties. Overcoming these barriers incurs a \"deaf content creation tax\" that translates into requiring significant additional time and resources to produce content equivalent to what a non-disabled person would produce. We highlight this process and associated challenges through real-world examples experienced by the authors, and provide guidance and recommendations for addressing them.","authors":["C. Vogler","A. Glasser","R. Kushalnagar","M. Seita","M. Arroyo Chavez","K. Delk","P. DeVries","M. Feanny","B. Thompson","J. Waller"],"url":"https://arxiv.org/abs/2505.01030"}
{"created":"2025-05-05","title":"Edge-preserving Image Denoising via Multi-scale Adaptive Statistical Independence Testing","abstract":"Edge detection is crucial in image processing, but existing methods often produce overly detailed edge maps, affecting clarity. Fixed-window statistical testing faces issues like scale mismatch and computational redundancy. To address these, we propose a novel Multi-scale Adaptive Independence Testing-based Edge Detection and Denoising (EDD-MAIT), a Multi-scale Adaptive Statistical Testing-based edge detection and denoising method that integrates a channel attention mechanism with independence testing. A gradient-driven adaptive window strategy adjusts window sizes dynamically, improving detail preservation and noise suppression. EDD-MAIT achieves better robustness, accuracy, and efficiency, outperforming traditional and learning-based methods on BSDS500 and BIPED datasets, with improvements in F-score, MSE, PSNR, and reduced runtime. It also shows robustness against Gaussian noise, generating accurate and clean edge maps in noisy environments.","authors":["Ruyu Yan","Da-Qing Zhang"],"url":"https://arxiv.org/abs/2505.01032"}
{"created":"2025-05-05","title":"Do We Need a Detailed Rubric for Automated Essay Scoring using Large Language Models?","abstract":"This study investigates the necessity and impact of a detailed rubric in automated essay scoring (AES) using large language models (LLMs). While using rubrics are standard in LLM-based AES, creating detailed rubrics requires substantial ef-fort and increases token usage. We examined how different levels of rubric detail affect scoring accuracy across multiple LLMs using the TOEFL11 dataset. Our experiments compared three conditions: a full rubric, a simplified rubric, and no rubric, using four different LLMs (Claude 3.5 Haiku, Gemini 1.5 Flash, GPT-4o-mini, and Llama 3 70B Instruct). Results showed that three out of four models maintained similar scoring accuracy with the simplified rubric compared to the detailed one, while significantly reducing token usage. However, one model (Gemini 1.5 Flash) showed decreased performance with more detailed rubrics. The findings suggest that simplified rubrics may be sufficient for most LLM-based AES applications, offering a more efficient alternative without compromis-ing scoring accuracy. However, model-specific evaluation remains crucial as per-formance patterns vary across different LLMs.","authors":["Lui Yoshida"],"url":"https://arxiv.org/abs/2505.01035"}
{"created":"2025-05-05","title":"Stagnation in Evolutionary Algorithms: Convergence $\\neq$ Optimality","abstract":"In the evolutionary computation community, it is widely believed that stagnation impedes convergence in evolutionary algorithms, and that convergence inherently indicates optimality. However, this perspective is misleading. In this study, it is the first to highlight that the stagnation of an individual can actually facilitate the convergence of the entire population, and convergence does not necessarily imply optimality, not even local optimality. Convergence alone is insufficient to ensure the effectiveness of evolutionary algorithms. Several counterexamples are provided to illustrate this argument.","authors":["Xiaojun Zhou"],"url":"https://arxiv.org/abs/2505.01036"}
{"created":"2025-05-05","title":"Regular expressions over countable words","abstract":"We investigate the expressive power of regular expressions for languages of countable words and establish their expressive equivalence with logical and algebraic characterizations. Our goal is to extend the classical theory of regular languages - defined over finite words and characterized by automata, monadic second-order logic, and regular expressions - to the setting of countable words. In this paper, we introduce and study five classes of expressions: marked star-free expressions, marked expressions, power-free expressions, scatter-free expressions, and scatter expressions. We show that these expression classes characterize natural fragments of logic over countable words and possess decidable algebraic characterizations. As part of our algebraic analysis, we provide a precise description of the relevant classes in terms of their J-class structure. These results complete a triad of equivalences - between logic, algebra, and expressions - in this richer setting, thereby generalizing foundational results from classical formal language theory.","authors":["Thomas Colcombet","A V Sreejith"],"url":"https://arxiv.org/abs/2505.01039"}
{"created":"2025-05-05","title":"Edge Detection based on Channel Attention and Inter-region Independence Test","abstract":"Existing edge detection methods often suffer from noise amplification and excessive retention of non-salient details, limiting their applicability in high-precision industrial scenarios. To address these challenges, we propose CAM-EDIT, a novel framework that integrates Channel Attention Mechanism (CAM) and Edge Detection via Independence Testing (EDIT). The CAM module adaptively enhances discriminative edge features through multi-channel fusion, while the EDIT module employs region-wise statistical independence analysis (using Fisher's exact test and chi-square test) to suppress uncorrelated noise.Extensive experiments on BSDS500 and NYUDv2 datasets demonstrate state-of-the-art performance. Among the nine comparison algorithms, the F-measure scores of CAM-EDIT are 0.635 and 0.460, representing improvements of 19.2\\% to 26.5\\% over traditional methods (Canny, CannySR), and better than the latest learning based methods (TIP2020, MSCNGP). Noise robustness evaluations further reveal a 2.2\\% PSNR improvement under Gaussian noise compared to baseline methods. Qualitative results exhibit cleaner edge maps with reduced artifacts, demonstrating its potential for high-precision industrial applications.","authors":["Ru-yu Yan","Da-Qing Zhang"],"url":"https://arxiv.org/abs/2505.01040"}
{"created":"2025-05-05","title":"Global Optimality of Single-Timescale Actor-Critic under Continuous State-Action Space: A Study on Linear Quadratic Regulator","abstract":"Actor-critic methods have achieved state-of-the-art performance in various challenging tasks. However, theoretical understandings of their performance remain elusive and challenging. Existing studies mostly focus on practically uncommon variants such as double-loop or two-timescale stepsize actor-critic algorithms for simplicity. These results certify local convergence on finite state- or action-space only. We push the boundary to investigate the classic single-sample single-timescale actor-critic on continuous (infinite) state-action space, where we employ the canonical linear quadratic regulator (LQR) problem as a case study. We show that the popular single-timescale actor-critic can attain an epsilon-optimal solution with an order of epsilon to -2 sample complexity for solving LQR on the demanding continuous state-action space. Our work provides new insights into the performance of single-timescale actor-critic, which further bridges the gap between theory and practice.","authors":["Xuyang Chen","Jingliang Duan","Lin Zhao"],"url":"https://arxiv.org/abs/2505.01041"}
{"created":"2025-05-05","title":"Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities","abstract":"Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several components$\\unicode{x2013}$such as weights, activations, and gradients$\\unicode{x2013}$each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.","authors":["Zhiwei Hao","Jianyuan Guo","Li Shen","Yong Luo","Han Hu","Guoxia Wang","Dianhai Yu","Yonggang Wen","Dacheng Tao"],"url":"https://arxiv.org/abs/2505.01043"}
{"created":"2025-05-05","title":"Transforming physics-informed machine learning to convex optimization","abstract":"Physics-Informed Machine Learning (PIML) offers a powerful paradigm of integrating data with physical laws to address important scientific problems, such as parameter estimation, inferring hidden physics, equation discovery, and state prediction, etc. However, PIML still faces many serious optimization challenges that significantly restrict its applications. In this study, we propose a comprehensive framework that transforms PIML to convex optimization to overcome all these limitations, referred to as Convex-PIML. The linear combination of B-splines is utilized to approximate the data, promoting the convexity of the loss function. By replacing the non-convex components of the loss function with convex approximations, the problem is further converted into a sequence of successively refined approximated convex optimization problems. This conversion allows the use of well-established convex optimization algorithms, obtaining solutions effectively and efficiently. Furthermore, an adaptive knot optimization method based on error estimate is introduced to mitigate the spectral bias issue of PIML, further improving the performance. The proposed theoretically guaranteed framework is tested in scenarios with distinct types of physical prior. The results indicate that optimization problems are effectively solved in these scenarios, highlighting the potential of the framework for broad applications.","authors":["Letian Yi","Siyuan Yang","Ying Cui","Zhilu Lai"],"url":"https://arxiv.org/abs/2505.01047"}
{"created":"2025-05-05","title":"Capability-Based Multi-Tenant Access Management in Crowdsourced Drone Services","abstract":"We propose a capability-based access control method that leverages OAuth 2.0 and Verifiable Credentials (VCs) to share resources in crowdsourced drone services. VCs securely encode claims about entities, offering flexibility. However, standardized protocols for VCs are lacking, limiting their adoption. To address this, we integrate VCs into OAuth 2.0, creating a novel access token. This token encapsulates VCs using JSON Web Tokens (JWT) and employs JWT-based methods for proof of possession. Our method streamlines VC verification with JSON Web Signatures (JWS) requires only minor adjustments to current OAuth 2.0 systems. Furthermore, in order to increase security and efficiency in multi-tenant environments, we provide a novel protocol for VC creation that makes use of the OAuth 2.0 client credentials grant. Using VCs as access tokens enhances OAuth 2.0, supporting long-term use and efficient data management. This system aids bushfire management authorities by ensuring high availability, enhanced privacy, and improved data portability. It supports multi-tenancy, allowing drone operators to control data access policies in a decentralized environment.","authors":["Junaid Akram","Ali Anaissi","Awais Akram","Youcef Djenouri","Palash Ingle","Rutvij H. Jhaveri"],"url":"https://arxiv.org/abs/2505.01048"}
{"created":"2025-05-05","title":"Multi-Step Consistency Models: Fast Generation with Theoretical Guarantees","abstract":"Consistency models have recently emerged as a compelling alternative to traditional SDE based diffusion models, offering a significant acceleration in generation by producing high quality samples in very few steps. Despite their empirical success, a proper theoretic justification for their speed up is still lacking. In this work, we provide the analysis which bridges this gap, showing that given a consistency model which can map the input at a given time to arbitrary timestamps along the reverse trajectory, one can achieve KL divergence of order $ O(\\varepsilon^2) $ using only $ O\\left(\\log\\left(\\frac{d}{\\varepsilon}\\right)\\right) $ iterations with constant step size, where d is the data dimension. Additionally, under minimal assumptions on the data distribution an increasingly common setting in recent diffusion model analyses we show that a similar KL convergence guarantee can be obtained, with the number of steps scaling as $ O\\left(d \\log\\left(\\frac{d}{\\varepsilon}\\right)\\right) $. Going further, we also provide a theoretical analysis for estimation of such consistency models, concluding that accurate learning is feasible using small discretization steps, both in smooth and non smooth settings. Notably, our results for the non smooth case yield best in class convergence rates compared to existing SDE or ODE based analyses under minimal assumptions.","authors":["Nishant Jain","Xunpeng Huang","Yian Ma","Tong Zhang"],"url":"https://arxiv.org/abs/2505.01049"}
{"created":"2025-05-05","title":"Transferable Adversarial Attacks on Black-Box Vision-Language Models","abstract":"Vision Large Language Models (VLLMs) are increasingly deployed to offer advanced capabilities on inputs comprising both text and images. While prior research has shown that adversarial attacks can transfer from open-source to proprietary black-box models in text-only and vision-only contexts, the extent and effectiveness of such vulnerabilities remain underexplored for VLLMs. We present a comprehensive analysis demonstrating that targeted adversarial examples are highly transferable to widely-used proprietary VLLMs such as GPT-4o, Claude, and Gemini. We show that attackers can craft perturbations to induce specific attacker-chosen interpretations of visual information, such as misinterpreting hazardous content as safe, overlooking sensitive or restricted material, or generating detailed incorrect responses aligned with the attacker's intent. Furthermore, we discover that universal perturbations -- modifications applicable to a wide set of images -- can consistently induce these misinterpretations across multiple proprietary VLLMs. Our experimental results on object recognition, visual question answering, and image captioning show that this vulnerability is common across current state-of-the-art models, and underscore an urgent need for robust mitigations to ensure the safe and secure deployment of VLLMs.","authors":["Kai Hu","Weichen Yu","Li Zhang","Alexander Robey","Andy Zou","Chengming Xu","Haoqi Hu","Matt Fredrikson"],"url":"https://arxiv.org/abs/2505.01050"}
{"created":"2025-05-05","title":"GeloVec: Higher Dimensional Geometric Smoothing for Coherent Visual Feature Extraction in Image Segmentation","abstract":"This paper introduces GeloVec, a new CNN-based attention smoothing framework for semantic segmentation that addresses critical limitations in conventional approaches. While existing attention-backed segmentation methods suffer from boundary instability and contextual discontinuities during feature mapping, our framework implements a higher-dimensional geometric smoothing method to establish a robust manifold relationships between visually coherent regions. GeloVec combines modified Chebyshev distance metrics with multispatial transformations to enhance segmentation accuracy through stabilized feature extraction. The core innovation lies in the adaptive sampling weights system that calculates geometric distances in n-dimensional feature space, achieving superior edge preservation while maintaining intra-class homogeneity. The multispatial transformation matrix incorporates tensorial projections with orthogonal basis vectors, creating more discriminative feature representations without sacrificing computational efficiency. Experimental validation across multiple benchmark datasets demonstrates significant improvements in segmentation performance, with mean Intersection over Union (mIoU) gains of 2.1%, 2.7%, and 2.4% on Caltech Birds-200, LSDSC, and FSSD datasets respectively compared to state-of-the-art methods. GeloVec's mathematical foundation in Riemannian geometry provides theoretical guarantees on segmentation stability. Importantly, our framework maintains computational efficiency through parallelized implementation of geodesic transformations and exhibits strong generalization capabilities across disciplines due to the absence of information loss during transformations.","authors":["Boris Kriuk","Matey Yordanov"],"url":"https://arxiv.org/abs/2505.01057"}
{"created":"2025-05-05","title":"Model Tensor Planning","abstract":"Sampling-based model predictive control (MPC) offers strong performance in nonlinear and contact-rich robotic tasks, yet often suffers from poor exploration due to locally greedy sampling schemes. We propose \\emph{Model Tensor Planning} (MTP), a novel sampling-based MPC framework that introduces high-entropy control trajectory generation through structured tensor sampling. By sampling over randomized multipartite graphs and interpolating control trajectories with B-splines and Akima splines, MTP ensures smooth and globally diverse control candidates. We further propose a simple $\\beta$-mixing strategy that blends local exploitative and global exploratory samples within the modified Cross-Entropy Method (CEM) update, balancing control refinement and exploration. Theoretically, we show that MTP achieves asymptotic path coverage and maximum entropy in the control trajectory space in the limit of infinite tensor depth and width.","authors":["An T. Le","Khai Nguyen","Minh Nhat Vu","Jo\\~ao Carvalho","Jan Peters"],"url":"https://arxiv.org/abs/2505.01059"}
{"created":"2025-05-05","title":"Monotone Peridynamic Neural Operator for Nonlinear Material Modeling with Conditionally Unique Solutions","abstract":"Data-driven methods have emerged as powerful tools for modeling the responses of complex nonlinear materials directly from experimental measurements. Among these methods, the data-driven constitutive models present advantages in physical interpretability and generalizability across different boundary conditions/domain settings. However, the well-posedness of these learned models is generally not guaranteed a priori, which makes the models prone to non-physical solutions in downstream simulation tasks. In this study, we introduce monotone peridynamic neural operator (MPNO), a novel data-driven nonlocal constitutive model learning approach based on neural operators. Our approach learns a nonlocal kernel together with a nonlinear constitutive relation, while ensuring solution uniqueness through a monotone gradient network. This architectural constraint on gradient induces convexity of the learnt energy density function, thereby guaranteeing solution uniqueness of MPNO in small deformation regimes. To validate our approach, we evaluate MPNO's performance on both synthetic and real-world datasets. On synthetic datasets with manufactured kernel and constitutive relation, we show that the learnt model converges to the ground-truth as the measurement grid size decreases both theoretically and numerically. Additionally, our MPNO exhibits superior generalization capabilities than the conventional neural networks: it yields smaller displacement solution errors in down-stream tasks with new and unseen loadings. Finally, we showcase the practical utility of our approach through applications in learning a homogenized model from molecular dynamics data, highlighting its expressivity and robustness in real-world scenarios.","authors":["Jihong Wang","Xiaochuan Tian","Zhongqiang Zhang","Stewart Silling","Siavash Jafarzadeh","Yue Yu"],"url":"https://arxiv.org/abs/2505.01060"}
{"created":"2025-05-05","title":"Efficient Vocabulary-Free Fine-Grained Visual Recognition in the Age of Multimodal LLMs","abstract":"Fine-grained Visual Recognition (FGVR) involves distinguishing between visually similar categories, which is inherently challenging due to subtle inter-class differences and the need for large, expert-annotated datasets. In domains like medical imaging, such curated datasets are unavailable due to issues like privacy concerns and high annotation costs. In such scenarios lacking labeled data, an FGVR model cannot rely on a predefined set of training labels, and hence has an unconstrained output space for predictions. We refer to this task as Vocabulary-Free FGVR (VF-FGVR), where a model must predict labels from an unconstrained output space without prior label information. While recent Multimodal Large Language Models (MLLMs) show potential for VF-FGVR, querying these models for each test input is impractical because of high costs and prohibitive inference times. To address these limitations, we introduce \\textbf{Nea}rest-Neighbor Label \\textbf{R}efinement (NeaR), a novel approach that fine-tunes a downstream CLIP model using labels generated by an MLLM. Our approach constructs a weakly supervised dataset from a small, unlabeled training set, leveraging MLLMs for label generation. NeaR is designed to handle the noise, stochasticity, and open-endedness inherent in labels generated by MLLMs, and establishes a new benchmark for efficient VF-FGVR.","authors":["Hari Chandana Kuchibhotla","Sai Srinivas Kancheti","Abbavaram Gowtham Reddy","Vineeth N Balasubramanian"],"url":"https://arxiv.org/abs/2505.01064"}
{"created":"2025-05-05","title":"Good News for Script Kiddies? Evaluating Large Language Models for Automated Exploit Generation","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in code-related tasks, raising concerns about their potential for automated exploit generation (AEG). This paper presents the first systematic study on LLMs' effectiveness in AEG, evaluating both their cooperativeness and technical proficiency. To mitigate dataset bias, we introduce a benchmark with refactored versions of five software security labs. Additionally, we design an LLM-based attacker to systematically prompt LLMs for exploit generation. Our experiments reveal that GPT-4 and GPT-4o exhibit high cooperativeness, comparable to uncensored models, while Llama3 is the most resistant. However, no model successfully generates exploits for refactored labs, though GPT-4o's minimal errors highlight the potential for LLM-driven AEG advancements.","authors":["David Jin","Qian Fu","Yuekang Li"],"url":"https://arxiv.org/abs/2505.01065"}
{"created":"2025-05-05","title":"A Rusty Link in the AI Supply Chain: Detecting Evil Configurations in Model Repositories","abstract":"Recent advancements in large language models (LLMs) have spurred the development of diverse AI applications from code generation and video editing to text generation; however, AI supply chains such as Hugging Face, which host pretrained models and their associated configuration files contributed by the public, face significant security challenges; in particular, configuration files originally intended to set up models by specifying parameters and initial settings can be exploited to execute unauthorized code, yet research has largely overlooked their security compared to that of the models themselves; in this work, we present the first comprehensive study of malicious configurations on Hugging Face, identifying three attack scenarios (file, website, and repository operations) that expose inherent risks; to address these threats, we introduce CONFIGSCAN, an LLM-based tool that analyzes configuration files in the context of their associated runtime code and critical libraries, effectively detecting suspicious elements with low false positive rates and high accuracy; our extensive evaluation uncovers thousands of suspicious repositories and configuration files, underscoring the urgent need for enhanced security validation in AI model hosting platforms.","authors":["Ziqi Ding","Qian Fu","Junchen Ding","Gelei Deng","Yi Liu","Yuekang Li"],"url":"https://arxiv.org/abs/2505.01067"}
{"created":"2025-05-05","title":"Multimodal Transformers are Hierarchical Modal-wise Heterogeneous Graphs","abstract":"Multimodal Sentiment Analysis (MSA) is a rapidly developing field that integrates multimodal information to recognize sentiments, and existing models have made significant progress in this area. The central challenge in MSA is multimodal fusion, which is predominantly addressed by Multimodal Transformers (MulTs). Although act as the paradigm, MulTs suffer from efficiency concerns. In this work, from the perspective of efficiency optimization, we propose and prove that MulTs are hierarchical modal-wise heterogeneous graphs (HMHGs), and we introduce the graph-structured representation pattern of MulTs. Based on this pattern, we propose an Interlaced Mask (IM) mechanism to design the Graph-Structured and Interlaced-Masked Multimodal Transformer (GsiT). It is formally equivalent to MulTs which achieves an efficient weight-sharing mechanism without information disorder through IM, enabling All-Modal-In-One fusion with only 1/3 of the parameters of pure MulTs. A Triton kernel called Decomposition is implemented to ensure avoiding additional computational overhead. Moreover, it achieves significantly higher performance than traditional MulTs. To further validate the effectiveness of GsiT itself and the HMHG concept, we integrate them into multiple state-of-the-art models and demonstrate notable performance improvements and parameter reduction on widely used MSA datasets.","authors":["Yijie Jin","Junjie Peng","Xuanchao Lin","Haochen Yuan","Lan Wang","Cangzhi Zheng"],"url":"https://arxiv.org/abs/2505.01068"}
{"created":"2025-05-05","title":"Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits","abstract":"Knowledge distillation (KD) has become a powerful tool for training compact student models using larger, pretrained teacher models, often requiring less data and computational resources. Teacher models typically possess more layers and thus exhibit richer feature representations compared to their student counterparts. Furthermore, student models tend to learn simpler, surface-level features in their early layers. This discrepancy can increase errors in groups where labels spuriously correlate with specific input attributes, leading to a decline in group fairness even when overall accuracy remains comparable to the teacher. To mitigate these challenges, Early-Exit Neural Networks (EENNs), which enable predictions at multiple intermediate layers, have been employed. Confidence margins derived from these early exits have been utilized to reweight both cross-entropy and distillation losses on a per-instance basis. In this paper, we propose that leveraging Laplace approximation-based methods to obtain well-calibrated uncertainty estimates can also effectively reweight challenging instances and improve group fairness. We hypothesize that Laplace approximation offers a more robust identification of difficult or ambiguous instances compared to margin-based approaches. To validate our claims, we benchmark our approach using a Bert-based model on the MultiNLI dataset.","authors":["Edvin Fasth","Sagar Singh"],"url":"https://arxiv.org/abs/2505.01070"}
{"created":"2025-05-05","title":"Retrieval Augmented Learning: A Retrial-based Large Language Model Self-Supervised Learning and Autonomous Knowledge Generation","abstract":"The lack of domain-specific data in the pre-training of Large Language Models (LLMs) severely limits LLM-based decision systems in specialized applications, while post-training a model in the scenarios requires significant computational resources. In this paper, we present Retrial-Augmented Learning (RAL), a reward-free self-supervised learning framework for LLMs that operates without model training. By developing Retrieval-Augmented Generation (RAG) into a module for organizing intermediate data, we realized a three-stage autonomous knowledge generation of proposing a hypothesis, validating the hypothesis, and generating the knowledge. The method is evaluated in the LLM-PySC2 environment, a representative decision-making platform that combines sufficient complexity with domain-specific knowledge requirements. Experiments demonstrate that the proposed method effectively reduces hallucination by generating and utilizing validated knowledge, and increases decision-making performance at an extremely low cost. Meanwhile, the approach exhibits potential in out-of-distribution(OOD) tasks, robustness, and transferability, making it a cost-friendly but effective solution for decision-making problems and autonomous knowledge generation.","authors":["Zongyuan Li","Pengfei Li","Runnan Qi","Yanan Ni","Lumin Jiang","Hui Wu","Xuebo Zhang","Kuihua Huang","Xian Guo"],"url":"https://arxiv.org/abs/2505.01073"}
{"created":"2025-05-05","title":"Federated Adapter on Foundation Models: An Out-Of-Distribution Approach","abstract":"As foundation models gain prominence, Federated Foundation Models (FedFM) have emerged as a privacy-preserving approach to collaboratively fine-tune models in federated learning (FL) frameworks using distributed datasets across clients. A key challenge for FedFM, given the versatile nature of foundation models, is addressing out-of-distribution (OOD) generalization, where unseen tasks or clients may exhibit distribution shifts leading to suboptimal performance. Although numerous studies have explored OOD generalization in conventional FL, these methods are inadequate for FedFM due to the challenges posed by large parameter scales and increased data heterogeneity. To address these, we propose FedOA, which employs adapter-based parameter-efficient fine-tuning methods for efficacy and introduces personalized adapters with feature distance-based regularization to align distributions and guarantee OOD generalization for each client. Theoretically, we demonstrate that the conventional aggregated global model in FedFM inherently retains OOD generalization capabilities, and our proposed method enhances the personalized model's OOD generalization through regularization informed by the global model, with proven convergence under general non-convex settings. Empirically, the effectiveness of the proposed method is validated on benchmark datasets across various NLP tasks.","authors":["Yiyuan Yang","Guodong Long","Tianyi Zhou","Qinghua Lu","Shanshan Ye","Jing Jiang"],"url":"https://arxiv.org/abs/2505.01075"}
{"created":"2025-05-05","title":"Quasi-Static IRS: 3D Shaped Beamforming for Area Coverage Enhancement","abstract":"Intelligent reflecting surface (IRS) is a promising paradigm to reconfigure the wireless environment for enhanced communication coverage and quality. However, to compensate for the double pathloss effect, massive IRS elements are required, raising concerns on the scalability of cost and complexity. This paper introduces a new architecture of quasi-static IRS (QS-IRS), which tunes element phases via mechanical adjustment or manually re-arranging the array topology. QS-IRS relies on massive production/assembly of purely passive elements only, and thus is suitable for ultra low-cost and large-scale deployment to enhance long-term coverage. To achieve this end, an IRS-aided area coverage problem is formulated, which explicitly considers the element radiation pattern (ERP), with the newly introduced shape masks for the mainlobe, and the sidelobe constraints to reduce energy leakage. An alternating optimization (AO) algorithm based on the difference-of-convex (DC) and successive convex approximation (SCA) procedure is proposed, which achieves shaped beamforming with power gains close to that of the joint optimization algorithm, but with significantly reduced computational complexity.","authors":["Zhenyu Jiang","Xintong Chen","Jiangbin Lyu","Liqun Fu","Rui Zhang"],"url":"https://arxiv.org/abs/2505.01076"}
{"created":"2025-05-05","title":"Zero-Shot Document-Level Biomedical Relation Extraction via Scenario-based Prompt Design in Two-Stage with LLM","abstract":"With the advent of artificial intelligence (AI), many researchers are attempting to extract structured information from document-level biomedical literature by fine-tuning large language models (LLMs). However, they face significant challenges such as the need for expensive hardware, like high-performance GPUs and the high labor costs associated with annotating training datasets, especially in biomedical realm. Recent research on LLMs, such as GPT-4 and Llama3, has shown promising performance in zero-shot settings, inspiring us to explore a novel approach to achieve the same results from unannotated full documents using general LLMs with lower hardware and labor costs. Our approach combines two major stages: named entity recognition (NER) and relation extraction (RE). NER identifies chemical, disease and gene entities from the document with synonym and hypernym extraction using an LLM with a crafted prompt. RE extracts relations between entities based on predefined relation schemas and prompts. To enhance the effectiveness of prompt, we propose a five-part template structure and a scenario-based prompt design principles, along with evaluation method to systematically assess the prompts. Finally, we evaluated our approach against fine-tuning and pre-trained models on two biomedical datasets: ChemDisGene and CDR. The experimental results indicate that our proposed method can achieve comparable accuracy levels to fine-tuning and pre-trained models but with reduced human and hardware expenses.","authors":["Lei Zhao","Ling Kang","Quan Guo"],"url":"https://arxiv.org/abs/2505.01077"}
{"created":"2025-05-05","title":"Integration Matters for Learning PDEs with Backwards SDEs","abstract":"Backward stochastic differential equation (BSDE)-based deep learning methods provide an alternative to Physics-Informed Neural Networks (PINNs) for solving high-dimensional partial differential equations (PDEs), offering algorithmic advantages in settings such as stochastic optimal control, where the PDEs of interest are tied to an underlying dynamical system. However, existing BSDE-based solvers have empirically been shown to underperform relative to PINNs in the literature. In this paper, we identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme applied to short-horizon self-consistency BSDE losses, which shifts the optimization landscape off target. We find that this bias cannot be satisfactorily addressed through finer step sizes or longer self-consistency horizons. To properly handle this issue, we propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration. We show that our proposed approach completely eliminates the bias issues faced by EM integration. Furthermore, our empirical results show that our Heun-based BSDE method consistently outperforms EM-based variants and achieves competitive results with PINNs across multiple high-dimensional benchmarks. Our findings highlight the critical role of integration schemes in BSDE-based PDE solvers, an algorithmic detail that has received little attention thus far in the literature.","authors":["Sungje Park","Stephen Tu"],"url":"https://arxiv.org/abs/2505.01078"}
{"created":"2025-05-05","title":"Improving Editability in Image Generation with Layer-wise Memory","abstract":"Most real-world image editing tasks require multiple sequential edits to achieve desired results. Current editing approaches, primarily designed for single-object modifications, struggle with sequential editing: especially with maintaining previous edits along with adapting new objects naturally into the existing content. These limitations significantly hinder complex editing scenarios where multiple objects need to be modified while preserving their contextual relationships. We address this fundamental challenge through two key proposals: enabling rough mask inputs that preserve existing content while naturally integrating new elements and supporting consistent editing across multiple modifications. Our framework achieves this through layer-wise memory, which stores latent representations and prompt embeddings from previous edits. We propose Background Consistency Guidance that leverages memorized latents to maintain scene coherence and Multi-Query Disentanglement in cross-attention that ensures natural adaptation to existing content. To evaluate our method, we present a new benchmark dataset incorporating semantic alignment metrics and interactive editing scenarios. Through comprehensive experiments, we demonstrate superior performance in iterative image editing tasks with minimal user effort, requiring only rough masks while maintaining high-quality results throughout multiple editing steps.","authors":["Daneul Kim","Jaeah Lee","Jaesik Park"],"url":"https://arxiv.org/abs/2505.01079"}
{"created":"2025-05-05","title":"MADIL: An MDL-based Framework for Efficient Program Synthesis in the ARC Benchmark","abstract":"Artificial Intelligence (AI) has achieved remarkable success in specialized tasks but struggles with efficient skill acquisition and generalization. The Abstraction and Reasoning Corpus (ARC) benchmark evaluates intelligence based on minimal training requirements. While Large Language Models (LLMs) have recently improved ARC performance, they rely on extensive pre-training and high computational costs. We introduce MADIL (MDL-based AI), a novel approach leveraging the Minimum Description Length (MDL) principle for efficient inductive learning. MADIL performs pattern-based decomposition, enabling structured generalization. While its performance (7% at ArcPrize 2024) remains below LLM-based methods, it offers greater efficiency and interpretability. This paper details MADIL's methodology, its application to ARC, and experimental evaluations.","authors":["S\\'ebastien Ferr\\'e"],"url":"https://arxiv.org/abs/2505.01081"}
{"created":"2025-05-05","title":"DexFlow: A Unified Approach for Dexterous Hand Pose Retargeting and Interaction","abstract":"Despite advances in hand-object interaction modeling, generating realistic dexterous manipulation data for robotic hands remains a challenge. Retargeting methods often suffer from low accuracy and fail to account for hand-object interactions, leading to artifacts like interpenetration. Generative methods, lacking human hand priors, produce limited and unnatural poses. We propose a data transformation pipeline that combines human hand and object data from multiple sources for high-precision retargeting. Our approach uses a differential loss constraint to ensure temporal consistency and generates contact maps to refine hand-object interactions. Experiments show our method significantly improves pose accuracy, naturalness, and diversity, providing a robust solution for hand-object interaction modeling.","authors":["Xiaoyi Lin","Kunpeng Yao","Lixin Xu","Xueqiang Wang","Xuetao Li","Yuchen Wang","Miao Li"],"url":"https://arxiv.org/abs/2505.01083"}
{"created":"2025-05-05","title":"Artificial Intelligence in Government: Why People Feel They Lose Control","abstract":"The use of Artificial Intelligence (AI) in public administration is expanding rapidly, moving from automating routine tasks to deploying generative and agentic systems that autonomously act on goals. While AI promises greater efficiency and responsiveness, its integration into government functions raises concerns about fairness, transparency, and accountability. This article applies principal-agent theory (PAT) to conceptualize AI adoption as a special case of delegation, highlighting three core tensions: assessability (can decisions be understood?), dependency (can the delegation be reversed?), and contestability (can decisions be challenged?). These structural challenges may lead to a \"failure-by-success\" dynamic, where early functional gains obscure long-term risks to democratic legitimacy. To test this framework, we conducted a pre-registered factorial survey experiment across tax, welfare, and law enforcement domains. Our findings show that although efficiency gains initially bolster trust, they simultaneously reduce citizens' perceived control. When the structural risks come to the foreground, institutional trust and perceived control both drop sharply, suggesting that hidden costs of AI adoption significantly shape public attitudes. The study demonstrates that PAT offers a powerful lens for understanding the institutional and political implications of AI in government, emphasizing the need for policymakers to address delegation risks transparently to maintain public trust.","authors":["Alexander Wuttke","Adrian Rauchfleisch","Andreas Jungherr"],"url":"https://arxiv.org/abs/2505.01085"}
{"created":"2025-05-05","title":"Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation","abstract":"Generative models have revolutionized Artificial Intelligence (AI), particularly in multimodal applications. However, adapting these models to the medical domain poses unique challenges due to the complexity of medical data and the stringent need for clinical accuracy. In this work, we introduce a framework specifically designed for multimodal medical data generation. By enabling the generation of multi-view chest X-rays and their associated clinical report, it bridges the gap between general-purpose vision-language models and the specialized requirements of healthcare. Leveraging the MIMIC-CXR dataset, the proposed framework shows superior performance in generating high-fidelity images and semantically coherent reports. Our quantitative evaluation reveals significant results in terms of FID and BLEU scores, showcasing the quality of the generated data. Notably, our framework achieves comparable or even superior performance compared to real data on downstream disease classification tasks, underlining its potential as a tool for medical research and diagnostics. This study highlights the importance of domain-specific adaptations in enhancing the relevance and utility of generative models for clinical applications, paving the way for future advancements in synthetic multimodal medical data generation.","authors":["Daniele Molino","Francesco di Feola","Linlin Shen","Paolo Soda","Valerio Guarrasi"],"url":"https://arxiv.org/abs/2505.01091"}
{"created":"2025-05-05","title":"Multi-Objective Reinforcement Learning for Water Management","abstract":"Many real-world problems (e.g., resource management, autonomous driving, drug discovery) require optimizing multiple, conflicting objectives. Multi-objective reinforcement learning (MORL) extends classic reinforcement learning to handle multiple objectives simultaneously, yielding a set of policies that capture various trade-offs. However, the MORL field lacks complex, realistic environments and benchmarks. We introduce a water resource (Nile river basin) management case study and model it as a MORL environment. We then benchmark existing MORL algorithms on this task. Our results show that specialized water management methods outperform state-of-the-art MORL approaches, underscoring the scalability challenges MORL algorithms face in real-world scenarios.","authors":["Zuzanna Osika","Roxana Radelescu","Jazmin Zatarain Salazar","Frans Oliehoek","Pradeep K. Murukannaiah"],"url":"https://arxiv.org/abs/2505.01094"}
{"created":"2025-05-05","title":"Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages","abstract":"The integration of artificial intelligence in healthcare has opened new horizons for improving medical diagnostics and patient care. However, challenges persist in developing systems capable of generating accurate and contextually relevant radiology reports, particularly in low-resource languages. In this study, we present a comprehensive benchmark to evaluate the performance of instruction-tuned Vision-Language Models (VLMs) in the specialized task of radiology report generation across three low-resource languages: Italian, German, and Spanish. Employing the LLaVA architectural framework, we conducted a systematic evaluation of pre-trained models utilizing general datasets, domain-specific datasets, and low-resource language-specific datasets. In light of the unavailability of models that possess prior knowledge of both the medical domain and low-resource languages, we analyzed various adaptations to determine the most effective approach for these contexts. The results revealed that language-specific models substantially outperformed both general and domain-specific models in generating radiology reports, emphasizing the critical role of linguistic adaptation. Additionally, models fine-tuned with medical terminology exhibited enhanced performance across all languages compared to models with generic knowledge, highlighting the importance of domain-specific training. We also explored the influence of the temperature parameter on the coherence of report generation, providing insights for optimal model settings. Our findings highlight the importance of tailored language and domain-specific training for improving the quality and accuracy of radiological reports in multilingual settings. This research not only advances our understanding of VLMs adaptability in healthcare but also points to significant avenues for future investigations into model tuning and language-specific adaptations.","authors":["Marco Salm\\`e","Rosa Sicilia","Paolo Soda","Valerio Guarrasi"],"url":"https://arxiv.org/abs/2505.01096"}
{"created":"2025-05-05","title":"Nesterov Method for Asynchronous Pipeline Parallel Optimization","abstract":"Pipeline Parallelism (PP) enables large neural network training on small, interconnected devices by splitting the model into multiple stages. To maximize pipeline utilization, asynchronous optimization is appealing as it offers 100% pipeline utilization by construction. However, it is inherently challenging as the weights and gradients are no longer synchronized, leading to stale (or delayed) gradients. To alleviate this, we introduce a variant of Nesterov Accelerated Gradient (NAG) for asynchronous optimization in PP. Specifically, we modify the look-ahead step in NAG to effectively address the staleness in gradients. We theoretically prove that our approach converges at a sublinear rate in the presence of fixed delay in gradients. Our experiments on large-scale language modelling tasks using decoder-only architectures with up to 1B parameters, demonstrate that our approach significantly outperforms existing asynchronous methods, even surpassing the synchronous baseline.","authors":["Thalaiyasingam Ajanthan","Sameera Ramasinghe","Yan Zuo","Gil Avraham","Alexander Long"],"url":"https://arxiv.org/abs/2505.01099"}
{"created":"2025-05-05","title":"Semi-Centennial REDUCE","abstract":"We present a version of the REDUCE computer algebra system as it was in the early 1970s. We show how this historical version of REDUCE may be built and run in very modest present-day environments and outline some of its capabilities.","authors":["Arthur C. Norman","Stephen M. Watt"],"url":"https://arxiv.org/abs/2505.01103"}
{"created":"2025-05-05","title":"VSC: Visual Search Compositional Text-to-Image Diffusion Model","abstract":"Text-to-image diffusion models have shown impressive capabilities in generating realistic visuals from natural-language prompts, yet they often struggle with accurately binding attributes to corresponding objects, especially in prompts containing multiple attribute-object pairs. This challenge primarily arises from the limitations of commonly used text encoders, such as CLIP, which can fail to encode complex linguistic relationships and modifiers effectively. Existing approaches have attempted to mitigate these issues through attention map control during inference and the use of layout information or fine-tuning during training, yet they face performance drops with increased prompt complexity. In this work, we introduce a novel compositional generation method that leverages pairwise image embeddings to improve attribute-object binding. Our approach decomposes complex prompts into sub-prompts, generates corresponding images, and computes visual prototypes that fuse with text embeddings to enhance representation. By applying segmentation-based localization training, we address cross-attention misalignment, achieving improved accuracy in binding multiple attributes to objects. Our approaches outperform existing compositional text-to-image diffusion models on the benchmark T2I CompBench, achieving better image quality, evaluated by humans, and emerging robustness under scaling number of binding pairs in the prompt.","authors":["Do Huu Dat","Nam Hyeonu","Po-Yuan Mao","Tae-Hyun Oh"],"url":"https://arxiv.org/abs/2505.01104"}
{"created":"2025-05-05","title":"CoCoAFusE: Beyond Mixtures of Experts via Model Fusion","abstract":"Many learning problems involve multiple patterns and varying degrees of uncertainty dependent on the covariates. Advances in Deep Learning (DL) have addressed these issues by learning highly nonlinear input-output dependencies. However, model interpretability and Uncertainty Quantification (UQ) have often straggled behind. In this context, we introduce the Competitive/Collaborative Fusion of Experts (CoCoAFusE), a novel, Bayesian Covariates-Dependent Modeling technique. CoCoAFusE builds on the very philosophy behind Mixtures of Experts (MoEs), blending predictions from several simple sub-models (or \"experts\") to achieve high levels of expressiveness while retaining a substantial degree of local interpretability. Our formulation extends that of a classical Mixture of Experts by contemplating the fusion of the experts' distributions in addition to their more usual mixing (i.e., superimposition). Through this additional feature, CoCoAFusE better accommodates different scenarios for the intermediate behavior between generating mechanisms, resulting in tighter credible bounds on the response variable. Indeed, only resorting to mixing, as in classical MoEs, may lead to multimodality artifacts, especially over smooth transitions. Instead, CoCoAFusE can avoid these artifacts even under the same structure and priors for the experts, leading to greater expressiveness and flexibility in modeling. This new approach is showcased extensively on a suite of motivating numerical examples and a collection of real-data ones, demonstrating its efficacy in tackling complex regression problems where uncertainty is a key quantity of interest.","authors":["Aurelio Raffa Ugolini","Mara Tanelli","Valentina Breschi"],"url":"https://arxiv.org/abs/2505.01105"}
{"created":"2025-05-05","title":"Investigating Middle School Students Question-Asking and Answer-Evaluation Skills When Using ChatGPT for Science Investigation","abstract":"Generative AI (GenAI) tools such as ChatGPT allow users, including school students without prior AI expertise, to explore and address a wide range of tasks. Surveys show that most students aged eleven and older already use these tools for school-related activities. However, little is known about how they actually use GenAI and how it impacts their learning.","authors":["Rania Abdelghani","Kou Murayama","Celeste Kidd","H\\'el\\`ene Sauz\\'eon","Pierre-Yves Oudeyer"],"url":"https://arxiv.org/abs/2505.01106"}
{"created":"2025-05-05","title":"CIMFlow: An Integrated Framework for Systematic Design and Evaluation of Digital CIM Architectures","abstract":"Digital Compute-in-Memory (CIM) architectures have shown great promise in Deep Neural Network (DNN) acceleration by effectively addressing the \"memory wall\" bottleneck. However, the development and optimization of digital CIM accelerators are hindered by the lack of comprehensive tools that encompass both software and hardware design spaces. Moreover, existing design and evaluation frameworks often lack support for the capacity constraints inherent in digital CIM architectures. In this paper, we present CIMFlow, an integrated framework that provides an out-of-the-box workflow for implementing and evaluating DNN workloads on digital CIM architectures. CIMFlow bridges the compilation and simulation infrastructures with a flexible instruction set architecture (ISA) design, and addresses the constraints of digital CIM through advanced partitioning and parallelism strategies in the compilation flow. Our evaluation demonstrates that CIMFlow enables systematic prototyping and optimization of digital CIM architectures across diverse configurations, providing researchers and designers with an accessible platform for extensive design space exploration.","authors":["Yingjie Qi","Jianlei Yang","Yiou Wang","Yikun Wang","Dayu Wang","Ling Tang","Cenlin Duan","Xiaolin He","Weisheng Zhao"],"url":"https://arxiv.org/abs/2505.01107"}
{"created":"2025-05-05","title":"Towards an Interpretable Analysis for Estimating the Resolution Time of Software Issues","abstract":"Lately, software development has become a predominantly online process, as more teams host and monitor their projects remotely. Sophisticated approaches employ issue tracking systems like Jira, predicting the time required to resolve issues and effectively assigning and prioritizing project tasks. Several methods have been developed to address this challenge, widely known as bug-fix time prediction, yet they exhibit significant limitations. Most consider only textual issue data and/or use techniques that overlook the semantics and metadata of issues (e.g., priority or assignee expertise). Many also fail to distinguish actual development effort from administrative delays, including assignment and review phases, leading to estimates that do not reflect the true effort needed. In this work, we build an issue monitoring system that extracts the actual effort required to fix issues on a per-project basis. Our approach employs topic modeling to capture issue semantics and leverages metadata (components, labels, priority, issue type, assignees) for interpretable resolution time analysis. Final predictions are generated by an aggregated model, enabling contributors to make informed decisions. Evaluation across multiple projects shows the system can effectively estimate resolution time and provide valuable insights.","authors":["Dimitrios-Nikitas Nastos (Electrical and Computer Engineering Dept.","Aristotle University of Thessaloniki)","Themistoklis Diamantopoulos (Electrical and Computer Engineering Dept.","Aristotle University of Thessaloniki)","Davide Tosi (Department of Theoretical and Applied Sciences","Universita degli Studi dell Insubria)","Martina Tropeano (Department of Theoretical and Applied Sciences","Universita degli Studi dell Insubria)","Andreas L. Symeonidis (Electrical and Computer Engineering Dept.","Aristotle University of Thessaloniki)"],"url":"https://arxiv.org/abs/2505.01108"}
{"created":"2025-05-05","title":"Self-Supervision Enhances Instance-based Multiple Instance Learning Methods in Digital Pathology: A Benchmark Study","abstract":"Multiple Instance Learning (MIL) has emerged as the best solution for Whole Slide Image (WSI) classification. It consists of dividing each slide into patches, which are treated as a bag of instances labeled with a global label. MIL includes two main approaches: instance-based and embedding-based. In the former, each patch is classified independently, and then the patch scores are aggregated to predict the bag label. In the latter, bag classification is performed after aggregating patch embeddings. Even if instance-based methods are naturally more interpretable, embedding-based MILs have usually been preferred in the past due to their robustness to poor feature extractors. However, recently, the quality of feature embeddings has drastically increased using self-supervised learning (SSL). Nevertheless, many authors continue to endorse the superiority of embedding-based MIL. To investigate this further, we conduct 710 experiments across 4 datasets, comparing 10 MIL strategies, 6 self-supervised methods with 4 backbones, 4 foundation models, and various pathology-adapted techniques. Furthermore, we introduce 4 instance-based MIL methods never used before in the pathology domain. Through these extensive experiments, we show that with a good SSL feature extractor, simple instance-based MILs, with very few parameters, obtain similar or better performance than complex, state-of-the-art (SOTA) embedding-based MIL methods, setting new SOTA results on the BRACS and Camelyon16 datasets. Since simple instance-based MIL methods are naturally more interpretable and explainable to clinicians, our results suggest that more effort should be put into well-adapted SSL methods for WSI rather than into complex embedding-based MIL methods.","authors":["Ali Mammadov","Loic Le Folgoc","Julien Adam","Anne Buronfosse","Gilles Hayem","Guillaume Hocquet","Pietro Gori"],"url":"https://arxiv.org/abs/2505.01109"}
{"created":"2025-05-05","title":"MateICL: Mitigating Attention Dispersion in Large-Scale In-Context Learning","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in In-Context Learning (ICL). However, the fixed position length constraints in pre-trained models limit the number of demonstration examples. Recent efforts to extend context suffer from attention dispersion as the number of demonstrations increases. In this paper, we introduce Mitigating Attention Dispersion in large-scale ICL (MateICL) that enables LLMs to maintain effective self-attention as the context size grows. We first split the context into multiple windows, each filled to the model's context capacity, which are processed separately. Then, we introduce an additional layer to recalibrate the attention weights, prioritizing the query tokens as the number of demonstrations increases. Our empirical results show that MateICL can effectively leverage larger contexts to improve ICL performance. Compared to retrieval-based baselines, MateICL consistently achieves better performance without requiring an externally trained retrieval model. Despite recent advances in inference strategies (e.g., 32k token contexts), our results demonstrate that MateICL remains beneficial in computationally resource-constrained settings. The code is publicly available at https://github.com/amurtadha/MateICL.","authors":["Murtadha Ahmed","Wenbo","Liu yunfeng"],"url":"https://arxiv.org/abs/2505.01110"}
{"created":"2025-05-05","title":"Incorporating Inductive Biases to Energy-based Generative Models","abstract":"With the advent of score-matching techniques for model training and Langevin dynamics for sample generation, energy-based models (EBMs) have gained renewed interest as generative models. Recent EBMs usually use neural networks to define their energy functions. In this work, we introduce a novel hybrid approach that combines an EBM with an exponential family model to incorporate inductive bias into data modeling. Specifically, we augment the energy term with a parameter-free statistic function to help the model capture key data statistics. Like an exponential family model, the hybrid model aims to align the distribution statistics with data statistics during model training, even when it only approximately maximizes the data likelihood. This property enables us to impose constraints on the hybrid model. Our empirical study validates the hybrid model's ability to match statistics. Furthermore, experimental results show that data fitting and generation improve when suitable informative statistics are incorporated into the hybrid model.","authors":["Yukun Li","Li-Ping Liu"],"url":"https://arxiv.org/abs/2505.01111"}
{"created":"2025-05-05","title":"Learning Low-Dimensional Embeddings for Black-Box Optimization","abstract":"When gradient-based methods are impractical, black-box optimization (BBO) provides a valuable alternative. However, BBO often struggles with high-dimensional problems and limited trial budgets. In this work, we propose a novel approach based on meta-learning to pre-compute a reduced-dimensional manifold where optimal points lie for a specific class of optimization problems. When optimizing a new problem instance sampled from the class, black-box optimization is carried out in the reduced-dimensional space, effectively reducing the effort required for finding near-optimal solutions.","authors":["Riccardo Busetto","Manas Mejari","Marco Forgione","Alberto Bemporad","Dario Piga"],"url":"https://arxiv.org/abs/2505.01112"}
{"created":"2025-05-05","title":"NeuroLoc: Encoding Navigation Cells for 6-DOF Camera Localization","abstract":"Recently, camera localization has been widely adopted in autonomous robotic navigation due to its efficiency and convenience. However, autonomous navigation in unknown environments often suffers from scene ambiguity, environmental disturbances, and dynamic object transformation in camera localization. To address this problem, inspired by the biological brain navigation mechanism (such as grid cells, place cells, and head direction cells), we propose a novel neurobiological camera location method, namely NeuroLoc. Firstly, we designed a Hebbian learning module driven by place cells to save and replay historical information, aiming to restore the details of historical representations and solve the issue of scene fuzziness. Secondly, we utilized the head direction cell-inspired internal direction learning as multi-head attention embedding to help restore the true orientation in similar scenes. Finally, we added a 3D grid center prediction in the pose regression module to reduce the final wrong prediction. We evaluate the proposed NeuroLoc on commonly used benchmark indoor and outdoor datasets. The experimental results show that our NeuroLoc can enhance the robustness in complex environments and improve the performance of pose regression by using only a single image.","authors":["Xun Li","Jian Yang","Fenli Jia","Muyu Wang","Qi Wu","Jun Wu","Jinpeng Mi","Jilin Hu","Peidong Liang","Xuan Tang","Ke Li","Xiong You","Xian Wei"],"url":"https://arxiv.org/abs/2505.01113"}
{"created":"2025-05-05","title":"Exploring Equity of Climate Policies using Multi-Agent Multi-Objective Reinforcement Learning","abstract":"Addressing climate change requires coordinated policy efforts of nations worldwide. These efforts are informed by scientific reports, which rely in part on Integrated Assessment Models (IAMs), prominent tools used to assess the economic impacts of climate policies. However, traditional IAMs optimize policies based on a single objective, limiting their ability to capture the trade-offs among economic growth, temperature goals, and climate justice. As a result, policy recommendations have been criticized for perpetuating inequalities, fueling disagreements during policy negotiations. We introduce Justice, the first framework integrating IAM with Multi-Objective Multi-Agent Reinforcement Learning (MOMARL). By incorporating multiple objectives, Justice generates policy recommendations that shed light on equity while balancing climate and economic goals. Further, using multiple agents can provide a realistic representation of the interactions among the diverse policy actors. We identify equitable Pareto-optimal policies using our framework, which facilitates deliberative decision-making by presenting policymakers with the inherent trade-offs in climate and economic policy.","authors":["Palok Biswas","Zuzanna Osika","Isidoro Tamassia","Adit Whorra","Jazmin Zatarain-Salazar","Jan Kwakkel","Frans A. Oliehoek","Pradeep K. Murukannaiah"],"url":"https://arxiv.org/abs/2505.01115"}
{"created":"2025-05-05","title":"Evaluating the Impact of Data Cleaning on the Quality of Generated Pull Request Descriptions","abstract":"Pull Requests (PRs) are central to collaborative coding, summarizing code changes for reviewers. However, many PR descriptions are incomplete, uninformative, or have out-of-context content, compromising developer workflows and hindering AI-based generation models trained on commit messages and original descriptions as \"ground truth.\" This study examines the prevalence of \"noisy\" PRs and evaluates their impact on state-of-the-art description generation models. To do so, we propose four cleaning heuristics to filter noise from an initial dataset of 169K+ PRs drawn from 513 GitHub repositories. We train four models-BART, T5, PRSummarizer, and iTAPE-on both raw and cleaned datasets. Performance is measured via ROUGE-1, ROUGE-2, and ROUGE-L metrics, alongside a manual evaluation to assess description quality improvements from a human perspective. Cleaning the dataset yields significant gains: average F1 improvements of 8.6% (ROUGE-1), 8.7% (ROUGE-2), and 8.5% (ROUGE-L). Manual assessment confirms higher readability and relevance in descriptions generated by the best-performing model, BART when trained on cleaned data. Dataset refinement markedly enhances PR description generation, offering a foundation for more accurate AI-driven tools and guidelines to assist developers in crafting high-quality PR descriptions.","authors":["Kutay Tire","Berk \\c{C}akar","Eray T\\\"uz\\\"un"],"url":"https://arxiv.org/abs/2505.01120"}
{"created":"2025-05-05","title":"The Great Data Standoff: Researchers vs. Platforms Under the Digital Services Act","abstract":"To facilitate accountability and transparency, the Digital Services Act (DSA) sets up a process through which Very Large Online Platforms (VLOPs) need to grant vetted researchers access to their internal data (Article 40(4)). Operationalising such access is challenging for at least two reasons. First, data access is only available for research on systemic risks affecting European citizens, a concept with high levels of legal uncertainty. Second, data access suffers from an inherent standoff problem. Researchers need to request specific data but are not in a position to know all internal data processed by VLOPs, who, in turn, expect data specificity for potential access. In light of these limitations, data access under the DSA remains a mystery. To contribute to the discussion of how Article 40 can be interpreted and applied, we provide a concrete illustration of what data access can look like in a real-world systemic risk case study. We focus on the 2024 Romanian presidential election interference incident, the first event of its kind to trigger systemic risk investigations by the European Commission. During the elections, one candidate is said to have benefited from TikTok algorithmic amplification through a complex dis- and misinformation campaign. By analysing this incident, we can comprehend election-related systemic risk to explore practical research tasks and compare necessary data with available TikTok data. In particular, we make two contributions: (i) we combine insights from law, computer science and platform governance to shed light on the complexities of studying systemic risks in the context of election interference, focusing on two relevant factors: platform manipulation and hidden advertising; and (ii) we provide practical insights into various categories of available data for the study of TikTok, based on platform documentation, data donations and the Research API.","authors":["Catalina Goanta","Savvas Zannettou","Rishabh Kaushal","Jacob van de Kerkhof","Thales Bertaglia","Taylor Annabell","Haoyang Gui","Gerasimos Spanakis","Adriana Iamnitchi"],"url":"https://arxiv.org/abs/2505.01122"}
{"created":"2025-05-05","title":"Poster: Machine Learning for Vulnerability Detection as Target Oracle in Automated Fuzz Driver Generation","abstract":"In vulnerability detection, machine learning has been used as an effective static analysis technique, although it suffers from a significant rate of false positives. Contextually, in vulnerability discovery, fuzzing has been used as an effective dynamic analysis technique, although it requires manually writing fuzz drivers. Fuzz drivers usually target a limited subset of functions in a library that must be chosen according to certain criteria, e.g., the depth of a function, the number of paths. These criteria are verified by components called target oracles. In this work, we propose an automated fuzz driver generation workflow composed of: (1) identifying a likely vulnerable function by leveraging a machine learning for vulnerability detection model as a target oracle, (2) automatically generating fuzz drivers, (3) fuzzing the target function to find bugs which could confirm the vulnerability inferred by the target oracle. We show our method on an existing vulnerability in libgd, with a plan for large-scale evaluation.","authors":["Gianpietro Castiglione","Marcello Maugeri","Giampaolo Bella"],"url":"https://arxiv.org/abs/2505.01123"}
{"created":"2025-05-05","title":"Risk Analysis and Design Against Adversarial Actions","abstract":"Learning models capable of providing reliable predictions in the face of adversarial actions has become a central focus of the machine learning community in recent years. This challenge arises from observing that data encountered at deployment time often deviate from the conditions under which the model was trained. In this paper, we address deployment-time adversarial actions and propose a versatile, well-principled framework to evaluate the model's robustness against attacks of diverse types and intensities. While we initially focus on Support Vector Regression (SVR), the proposed approach extends naturally to the broad domain of learning via relaxed optimization techniques. Our results enable an assessment of the model vulnerability without requiring additional test data and operate in a distribution-free setup. These results not only provide a tool to enhance trust in the model's applicability but also aid in selecting among competing alternatives. Later in the paper, we show that our findings also offer useful insights for establishing new results within the out-of-distribution framework.","authors":["Marco C. Campi","Algo Car\\`e","Luis G. Crespo","Simone Garatti","Federico A. Ramponi"],"url":"https://arxiv.org/abs/2505.01130"}
{"created":"2025-05-05","title":"Remote Estimation over Packet-Dropping Wireless Channels with Partial State Information","abstract":"In this paper, we study the design of an optimal transmission policy for remote state estimation over packet-dropping wireless channels with imperfect channel state information. A smart sensor uses a Kalman filter to estimate the system state and transmits its information to a remote estimator. Our objective is to minimize the state estimation error and energy consumption by deciding whether to transmit new information or retransmit previously failed packets. To balance the trade-off between information freshness and reliability, the sensor applies a hybrid automatic repeat request protocol. We formulate this problem as a finite horizon partially observable Markov decision process with an augmented state-space that incorporates both the age of information and the unknown channel state. By defining an information state, we derive the dynamic programming equations for evaluating the optimal policy. This transmission policy is computed numerically using the point-based value iteration algorithm.","authors":["Ioannis Tzortzis","Evagoras Makridis","Charalambos D. Charalambous","Themistoklis Charalambous"],"url":"https://arxiv.org/abs/2505.01132"}
{"created":"2025-05-05","title":"Aggregation of Dependent Expert Distributions in Multimodal Variational Autoencoders","abstract":"Multimodal learning with variational autoencoders (VAEs) requires estimating joint distributions to evaluate the evidence lower bound (ELBO). Current methods, the product and mixture of experts, aggregate single-modality distributions assuming independence for simplicity, which is an overoptimistic assumption. This research introduces a novel methodology for aggregating single-modality distributions by exploiting the principle of consensus of dependent experts (CoDE), which circumvents the aforementioned assumption. Utilizing the CoDE method, we propose a novel ELBO that approximates the joint likelihood of the multimodal data by learning the contribution of each subset of modalities. The resulting CoDE-VAE model demonstrates better performance in terms of balancing the trade-off between generative coherence and generative quality, as well as generating more precise log-likelihood estimations. CoDE-VAE further minimizes the generative quality gap as the number of modalities increases. In certain cases, it reaches a generative quality similar to that of unimodal VAEs, which is a desirable property that is lacking in most current methods. Finally, the classification accuracy achieved by CoDE-VAE is comparable to that of state-of-the-art multimodal VAE models.","authors":["Rogelio A Mancisidor","Robert Jenssen","Shujian Yu","Michael Kampffmeyer"],"url":"https://arxiv.org/abs/2505.01134"}
{"created":"2025-05-05","title":"Dual-Forecaster: A Multimodal Time Series Model Integrating Descriptive and Predictive Texts","abstract":"Most existing single-modal time series models rely solely on numerical series, which suffer from the limitations imposed by insufficient information. Recent studies have revealed that multimodal models can address the core issue by integrating textual information. However, these models focus on either historical or future textual information, overlooking the unique contributions each plays in time series forecasting. Besides, these models fail to grasp the intricate relationships between textual and time series data, constrained by their moderate capacity for multimodal comprehension. To tackle these challenges, we propose Dual-Forecaster, a pioneering multimodal time series model that combines both descriptively historical textual information and predictive textual insights, leveraging advanced multimodal comprehension capability empowered by three well-designed cross-modality alignment techniques. Our comprehensive evaluations on fifteen multimodal time series datasets demonstrate that Dual-Forecaster is a distinctly effective multimodal time series model that outperforms or is comparable to other state-of-the-art models, highlighting the superiority of integrating textual information for time series forecasting. This work opens new avenues in the integration of textual information with numerical time series data for multimodal time series analysis.","authors":["Wenfa Wu","Guanyu Zhang","Zheng Tan","Yi Wang","Hongsheng Qi"],"url":"https://arxiv.org/abs/2505.01135"}
{"created":"2025-05-05","title":"CppSATD: A Reusable Self-Admitted Technical Debt Dataset in C++","abstract":"In software development, technical debt (TD) refers to suboptimal implementation choices made by the developers to meet urgent deadlines and limited resources, posing challenges for future maintenance. Self-Admitted Technical Debt (SATD) is a sub-type of TD, representing specific TD instances ``openly admitted'' by the developers and often expressed through source code comments. Previous research on SATD has focused predominantly on the Java programming language, revealing a significant gap in cross-language SATD. Such a narrow focus limits the generalizability of existing findings as well as SATD detection techniques across multiple programming languages. Our work addresses such limitation by introducing CppSATD, a dedicated C++ SATD dataset, comprising over 531,000 annotated comments and their source code contexts. Our dataset can serve as a foundation for future studies that aim to develop SATD detection methods in C++, generalize the existing findings to other languages, or contribute novel insights to cross-language SATD research.","authors":["Phuoc Pham","Murali Sridharan","Matteo Esposito","Valentina Lenarduzzi"],"url":"https://arxiv.org/abs/2505.01136"}
{"created":"2025-05-05","title":"Active Sybil Attack and Efficient Defense Strategy in IPFS DHT","abstract":"The InterPlanetary File System (IPFS) is a decentralized peer-to-peer (P2P) storage that relies on Kademlia, a Distributed Hash Table (DHT) structure commonly used in P2P systems for its proved scalability. However, DHTs are known to be vulnerable to Sybil attacks, in which a single entity controls multiple malicious nodes. Recent studies have shown that IPFS is affected by a passive content eclipse attack, leveraging Sybils, in which adversarial nodes hide received indexed information from other peers, making the content appear unavailable. Fortunately, the latest mitigation strategy coupling an attack detection based on statistical tests and a wider publication strategy upon detection was able to circumvent it.","authors":["V. H. M. Netto","T. Cholez","C. L. Ignat"],"url":"https://arxiv.org/abs/2505.01139"}
{"created":"2025-05-05","title":"ConflictSync: Bandwidth Efficient Synchronization of Divergent State","abstract":"State-based Conflict-free Replicated Data Types (CRDTs) are widely used in distributed systems to ensure high availability without coordination. However, their naive synchronization strategy - transmitting the full state - incurs high communication costs. Existing optimizations like delta-CRDTs reduce this overhead but rely on external metadata that must be garbage collected to prevent unbounded growth, at the cost of full state transmissions after network partitions.","authors":["Pedro Silva Gomes","Miguel Boaventura Rodrigues","Carlos Baquero"],"url":"https://arxiv.org/abs/2505.01144"}
{"created":"2025-05-05","title":"Towards Probabilistic Dynamic Security Assessment and Enhancement of Large Power Systems","abstract":"This paper proposes a novel methodology for probabilistic dynamic security assessment and enhancement of power systems that considers load and generation variability, N-2 contingencies, and uncertain cascade propagation caused by uncertain protection system behaviour. In this methodology, a database of likely operating conditions is generated via weather data, a market model and a model of operators' preventive actions. System states are sampled from this database and contingencies are applied to them to perform the security assessment. Rigorous statistical indicators are proposed to decide how many biased and unbiased samples to simulate to reach a target accuracy on the statistical error on the estimated risk from individual contingencies. Optionally, a screening of contingencies can be performed to limit the computational burden of the analysis. Finally, interpretable machine learning techniques are used to identify the root causes of the risk from critical contingencies, to ease the interpretation of the results, and to help with security enhancement. The method is demonstrated on the 73-bus reliability test system, and the scalability to large power systems (with thousands of buses) is also discussed.","authors":["Fr\\'ed\\'eric Sabot","Pierre-Etienne Labeau","Pierre Henneaux"],"url":"https://arxiv.org/abs/2505.01147"}
{"created":"2025-05-05","title":"Methodological Foundations for AI-Driven Survey Question Generation","abstract":"This paper presents a methodological framework for using generative AI in educational survey research. We explore how Large Language Models (LLMs) can generate adaptive, context-aware survey questions and introduce the Synthetic Question-Response Analysis (SQRA) framework, which enables iterative testing and refinement of AI-generated prompts prior to deployment with human participants. Guided by Activity Theory, we analyze how AI tools mediate participant engagement and learning, and we examine ethical issues such as bias, privacy, and transparency. Through sentiment, lexical, and structural analyses of both AI-to-AI and AI-to-human survey interactions, we evaluate the alignment and effectiveness of these questions. Our findings highlight the promise and limitations of AI-driven survey instruments, emphasizing the need for robust prompt engineering and validation to support trustworthy, scalable, and contextually relevant data collection in engineering education.","authors":["Ted K. Mburu","Kangxuan Rong","Campbell J. McColley","Alexandra Werth"],"url":"https://arxiv.org/abs/2505.01150"}
{"created":"2025-05-05","title":"Polarization Decomposition and Its Applications","abstract":"The polarization decomposition for arbitrary binary-input memoryless channels (BMCs) is investigated. By defining the polarization factor (PF) based on the conditional entropy of channel output under various input combinations, it is shown that the symmetric capacities of polarized subchannels can be expressed by the PF in the general form. We derive the explicit form of PF with respect to block length and subchannel index. Meanwhile, we propose an efficient calculation algorithm for the PF. Particularly, we demonstrate that any PF can correspond to a $n$-ary tree. Based on this structure, we design the calculation approach of the conditional entropy under different input relations. The proposed polarization algorithm provides both new theoretical insights and practical values such as visualization of polarization and polar code construction. Moreover, our approach first makes it possible to efficiently calculate the symmetric capacities of all subchannels for any BMCs.","authors":["Tianfu Qi","Jun Wang"],"url":"https://arxiv.org/abs/2505.01152"}
{"created":"2025-05-05","title":"Machine Learning for Physical Simulation Challenge Results and Retrospective Analysis: Power Grid Use Case","abstract":"This paper addresses the growing computational challenges of power grid simulations, particularly with the increasing integration of renewable energy sources like wind and solar. As grid operators must analyze significantly more scenarios in near real-time to prevent failures and ensure stability, traditional physical-based simulations become computationally impractical. To tackle this, a competition was organized to develop AI-driven methods that accelerate power flow simulations by at least an order of magnitude while maintaining operational reliability. This competition utilized a regional-scale grid model with a 30\\% renewable energy mix, mirroring the anticipated near-future composition of the French power grid. A key contribution of this work is through the use of LIPS (Learning Industrial Physical Systems), a benchmarking framework that evaluates solutions based on four critical dimensions: machine learning performance, physical compliance, industrial readiness, and generalization to out-of-distribution scenarios. The paper provides a comprehensive overview of the Machine Learning for Physical Simulation (ML4PhySim) competition, detailing the benchmark suite, analyzing top-performing solutions that outperformed traditional simulation methods, and sharing key organizational insights and best practices for running large-scale AI competitions. Given the promising results achieved, the study aims to inspire further research into more efficient, scalable, and sustainable power network simulation methodologies.","authors":["Milad Leyli-Abadi","J\\'er\\^ome Picault","Antoine Marot","Jean-Patrick Brunet","Agathe Gilain","Amarsagar Reddy Ramapuram Matavalam","Shaban Ghias Satti","Quingbin Jiang","Yang Liu","Dean Justin Ninalga"],"url":"https://arxiv.org/abs/2505.01156"}
{"created":"2025-05-05","title":"A Parameter-Driven Physics-Informed Neural Network Framework for Solving Two-Parameter Singular Perturbation Problems Involving Boundary Layers","abstract":"In this article, our goal is to solve two-parameter singular perturbation problems (SPPs) in one- and two-dimensions using an adapted Physics-Informed Neural Networks (PINNs) approach. Such problems are of major importance in engineering and sciences as it appears in control theory, fluid and gas dynamics, financial modelling and so on. Solutions of such problems exhibit boundary and/or interior layers, which make them difficult to handle. It has been validated in the literature that standard PINNs have low accuracy and can't handle such problems efficiently. Recently Cao et. al \\cite{cao2023physics} proposed a new parameter asymptotic PINNs (PA-PINNs) to solve one-parameter singularly perturbed convection-dominated problems. It was observed that PA-PINNs works better than standard PINNs and gPINNs in terms of accuracy, convergence and stability. In this article, for the first time robustness of PA-PINNs will be validated for solving two-parameter SPPs.","authors":["Pradanya Boro","Aayushman Raina","Srinivasan Natesan"],"url":"https://arxiv.org/abs/2505.01159"}
{"created":"2025-05-05","title":"TActiLE: Tiny Active LEarning for wearable devices","abstract":"Tiny Machine Learning (TinyML) algorithms have seen extensive use in recent years, enabling wearable devices to be not only connected but also genuinely intelligent by running machine learning (ML) computations directly on-device. Among such devices, smart glasses have particularly benefited from TinyML advancements. TinyML facilitates the on-device execution of the inference phase of ML algorithms on embedded and wearable devices, and more recently, it has expanded into On-device Learning (ODL), which allows both inference and learning phases to occur directly on the device. The application of ODL techniques to wearable devices is particularly compelling, as it enables the development of more personalized models that adapt based on the data of the user. However, one of the major challenges of ODL algorithms is the scarcity of labeled data collected on-device. In smart wearable contexts, requiring users to manually label large amounts of data is often impractical and could lead to user disengagement with the technology. To address this issue, this paper explores the application of Active Learning (AL) techniques, i.e., techniques that aim at minimizing the labeling effort, by actively selecting from a large quantity of unlabeled data only a small subset to be labeled and added to the training set of the algorithm. In particular, we propose TActiLE, a novel AL algorithm that selects from the stream of on-device sensor data the ones that would help the ML algorithm improve the most once coupled with labels provided by the user. TActiLE is the first Active Learning technique specifically designed for the TinyML context. We evaluate its effectiveness and efficiency through experiments on multiple image classification datasets. The results demonstrate its suitability for tiny and wearable devices.","authors":["Massimo Pavan","Claudio Galimberti","Manuel Roveri"],"url":"https://arxiv.org/abs/2505.01160"}
{"created":"2025-05-05","title":"On the Limitations of Steering in Language Model Alignment","abstract":"Steering vectors are a promising approach to aligning language model behavior at inference time. In this paper, we propose a framework to assess the limitations of steering vectors as alignment mechanisms. Using a framework of transformer hook interventions and antonym-based function vectors, we evaluate the role of prompt structure and context complexity in steering effectiveness. Our findings indicate that steering vectors are promising for specific alignment tasks, such as value alignment, but may not provide a robust foundation for general-purpose alignment in LLMs, particularly in complex scenarios. We establish a methodological foundation for future investigations into steering capabilities of reasoning models.","authors":["Chebrolu Niranjan","Kokil Jaidka","Gerard Christopher Yeo"],"url":"https://arxiv.org/abs/2505.01162"}
{"created":"2025-05-05","title":"Empirical Comparison of Lightweight Forecasting Models for Seasonal and Non-Seasonal Time Series","abstract":"Accurate time series forecasting is essential in many real-time applications that demand both high predictive accuracy and computational efficiency. This study provides an empirical comparison between a Polynomial Classifier and a Radial Basis Function Neural Network (RBFNN) across four real-world time series datasets (weather conditions, gold prices, crude oil prices, and beer production volumes) that cover both seasonal and nonseasonal patterns. Model performance is evaluated by forecasting accuracy (using Mean Absolute Error, Root Mean Squared Error, and Coefficient of Variation of Root Mean Squared Error) and computational time to assess each model's viability for real time forecasting. The results show that the PC yields more accurate and faster forecasts for non seasonal series, whereas the RBFNN performs better on series with pronounced seasonal patterns. From an interpretability standpoint, the polynomial model offers a simpler, more transparent structure (in contrast to the black box nature of neural network), which is advantageous for understanding and trust in real time decision making. The performance differences between PC and RBFNN are statistically significant, as confirmed by paired t tests and Wilcoxon signed rank tests. These findings provide practical guidance for model selection in time series forecasting, indicating that PC may be preferable for quick, interpretable forecasts in non-seasonal contexts, whereas RBFNN is superior for capturing complex seasonal behaviors","authors":["Thanh Son Nguyen","Dang Minh Duc Nguyen","Van Thanh Nguyen"],"url":"https://arxiv.org/abs/2505.01163"}
{"created":"2025-05-05","title":"CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in RAG Systems","abstract":"Modern embedding models capture both semantic and syntactic structures of queries, often mapping different queries to similar regions in vector space. This results in non-uniform cluster access patterns in disk-based vector search systems, particularly in Retrieval Augmented Generation (RAG) framework. While existing approaches optimize individual queries, they overlook the impact of cluster access patterns, failing to account for the locality effects of queries that access similar clusters. This oversight reduces cache efficiency and increases search latency due to excessive disk I/O. To address this, we introduce CaGR-RAG, a context-aware query grouping mechanism that organizes queries based on shared cluster access patterns. Additionally, it incorporates opportunistic cluster prefetching to minimize cache misses during transitions between query groups, further optimizing retrieval performance. Experimental results show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55% while consistently maintaining a higher cache hit ratio than the baseline.","authors":["Yeonwoo Jeong","Kyuli Park","Hyunji Cho","Sungyong Park"],"url":"https://arxiv.org/abs/2505.01164"}
{"created":"2025-05-05","title":"Harmonizing Intra-coherence and Inter-divergence in Ensemble Attacks for Adversarial Transferability","abstract":"The development of model ensemble attacks has significantly improved the transferability of adversarial examples, but this progress also poses severe threats to the security of deep neural networks. Existing methods, however, face two critical challenges: insufficient capture of shared gradient directions across models and a lack of adaptive weight allocation mechanisms. To address these issues, we propose a novel method Harmonized Ensemble for Adversarial Transferability (HEAT), which introduces domain generalization into adversarial example generation for the first time. HEAT consists of two key modules: Consensus Gradient Direction Synthesizer, which uses Singular Value Decomposition to synthesize shared gradient directions; and Dual-Harmony Weight Orchestrator which dynamically balances intra-domain coherence, stabilizing gradients within individual models, and inter-domain diversity, enhancing transferability across models. Experimental results demonstrate that HEAT significantly outperforms existing methods across various datasets and settings, offering a new perspective and direction for adversarial attack research.","authors":["Zhaoyang Ma","Zhihao Wu","Wang Lu","Xin Gao","Jinghang Yue","Taolin Zhang","Lipo Wang","Youfang Lin","Jing Wang"],"url":"https://arxiv.org/abs/2505.01168"}
{"created":"2025-05-05","title":"Distilling Two-Timed Flow Models by Separately Matching Initial and Terminal Velocities","abstract":"A flow matching model learns a time-dependent vector field $v_t(x)$ that generates a probability path $\\{ p_t \\}_{0 \\leq t \\leq 1}$ that interpolates between a well-known noise distribution ($p_0$) and the data distribution ($p_1$). It can be distilled into a \\emph{two-timed flow model} (TTFM) $\\phi_{s,x}(t)$ that can transform a sample belonging to the distribution at an initial time $s$ to another belonging to the distribution at a terminal time $t$ in one function evaluation. We present a new loss function for TTFM distillation called the \\emph{initial/terminal velocity matching} (ITVM) loss that extends the Lagrangian Flow Map Distillation (LFMD) loss proposed by Boffi et al. by adding redundant terms to match the initial velocities at time $s$, removing the derivative from the terminal velocity term at time $t$, and using a version of the model under training, stabilized by exponential moving averaging (EMA), to compute the target terminal average velocity. Preliminary experiments show that our loss leads to better few-step generation performance on multiple types of datasets and model architectures over baselines.","authors":["Pramook Khungurn","Pratch Piyawongwisal","Sira Sriswadi","Supasorn Suwajanakorn"],"url":"https://arxiv.org/abs/2505.01169"}
{"created":"2025-05-05","title":"Realizing Fully-Connected Layers Over the Air via Reconfigurable Intelligent Surfaces","abstract":"By leveraging the waveform superposition property of the multiple access channel, over-the-air computation (AirComp) enables the execution of digital computations through analog means in the wireless domain, leading to faster processing and reduced latency. In this paper, we propose a novel approach to implement a neural network (NN) consisting of digital fully connected (FC) layers using physically reconfigurable hardware. Specifically, we investigate reconfigurable intelligent surfaces (RISs)-assisted multiple-input multiple-output (MIMO) systems to emulate the functionality of a NN for over-the-air inference. In this setup, both the RIS and the transceiver are jointly configured to manipulate the ambient wireless propagation environment, effectively reproducing the adjustable weights of a digital FC layer. We refer to this new computational paradigm as \\textit{AirFC}. We formulate an imitation error minimization problem between the effective channel created by RIS and a target FC layer by jointly optimizing over-the-air parameters. To solve this non-convex optimization problem, an extremely low-complexity alternating optimization algorithm is proposed, where semi-closed-form/closed-form solutions for all optimization variables are derived. Simulation results show that the RIS-assisted MIMO-based AirFC can achieve competitive classification accuracy. Furthermore, it is also shown that a multi-RIS configuration significantly outperforms a single-RIS setup, particularly in line-of-sight (LoS)-dominated channels.","authors":["Meng Hua","Chenghong Bian","Haotian Wu","Deniz G\\\"und\\\"uz"],"url":"https://arxiv.org/abs/2505.01170"}
{"created":"2025-05-05","title":"An instrument to measure factors that constitute the socio-technical context of testing experience","abstract":"We consider testing a cooperative and social practice that is shaped by the tools developers use, the tests they write, and their mindsets and human needs. This work is one part of a project that explores the human- and socio-technical context of testing through the lens of those interwoven elements: test suite and tools as technical infrastructure and collaborative factors and motivation as mindset. Drawing on empirical observations of previous work, this survey examines how these factors relate to each other. We want to understand which combination of factors can help developers strive and make the most of their ambitions to leverage the potential that software testing practices have. In this report, we construct a survey instrument to measure the factors that constitute the socio-technical context of testing experience. In addition, we state our hypotheses about how these factors impact testing experience and explain the considerations and process that led to the construction of the survey questions.","authors":["Mark Swillus","Carolin Brandt","Andy Zaidman"],"url":"https://arxiv.org/abs/2505.01171"}
{"created":"2025-05-05","title":"FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis","abstract":"Long video generation involves generating extended videos using models trained on short videos, suffering from distribution shifts due to varying frame counts. It necessitates the use of local information from the original short frames to enhance visual and motion quality, and global information from the entire long frames to ensure appearance consistency. Existing training-free methods struggle to effectively integrate the benefits of both, as appearance and motion in videos are closely coupled, leading to motion inconsistency and visual quality. In this paper, we reveal that global and local information can be precisely decoupled into consistent appearance and motion intensity information by applying Principal Component Analysis (PCA), allowing for refined complementary integration of global consistency and local quality. With this insight, we propose FreePCA, a training-free long video generation paradigm based on PCA that simultaneously achieves high consistency and quality. Concretely, we decouple consistent appearance and motion intensity features by measuring cosine similarity in the principal component space. Critically, we progressively integrate these features to preserve original quality and ensure smooth transitions, while further enhancing consistency by reusing the mean statistics of the initial noise. Experiments demonstrate that FreePCA can be applied to various video diffusion models without requiring training, leading to substantial improvements. Code is available at https://github.com/JosephTiTan/FreePCA.","authors":["Jiangtong Tan","Hu Yu","Jie Huang","Jie Xiao","Feng Zhao"],"url":"https://arxiv.org/abs/2505.01172"}
{"created":"2025-05-05","title":"Self-moderation in the decentralized era: decoding blocking behavior on Bluesky","abstract":"Moderation and blocking behavior, both closely related to the mitigation of abuse and misinformation on social platforms, are fundamental mechanisms for maintaining healthy online communities. However, while centralized platforms typically employ top-down moderation, decentralized networks rely on users to self-regulate through mechanisms like blocking actions to safeguard their online experience. Given the novelty of the decentralized paradigm, addressing self-moderation is critical for understanding how community safety and user autonomy can be effectively balanced. This study examines user blocking on Bluesky, a decentralized social networking platform, providing a comprehensive analysis of over three months of user activity through the lens of blocking behaviour. We define profiles based on 86 features that describe user activity, content characteristics, and network interactions, addressing two primary questions: (1) Is the likelihood of a user being blocked inferable from their online behavior? and (2) What behavioral features are associated with an increased likelihood of being blocked? Our findings offer valuable insights and contribute with a robust analytical framework to advance research in moderation on decentralized social networks.","authors":["Carlo Bono","Nick Liu","Giuseppe Russo","Francesco Pierri"],"url":"https://arxiv.org/abs/2505.01174"}
{"created":"2025-05-05","title":"LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures","abstract":"As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.","authors":["Francisco Aguilera-Mart\\'inez","Fernando Berzal"],"url":"https://arxiv.org/abs/2505.01177"}
{"created":"2025-05-05","title":"Fast Flow-based Visuomotor Policies via Conditional Optimal Transport Couplings","abstract":"Diffusion and flow matching policies have recently demonstrated remarkable performance in robotic applications by accurately capturing multimodal robot trajectory distributions. However, their computationally expensive inference, due to the numerical integration of an ODE or SDE, limits their applicability as real-time controllers for robots. We introduce a methodology that utilizes conditional Optimal Transport couplings between noise and samples to enforce straight solutions in the flow ODE for robot action generation tasks. We show that naively coupling noise and samples fails in conditional tasks and propose incorporating condition variables into the coupling process to improve few-step performance. The proposed few-step policy achieves a 4% higher success rate with a 10x speed-up compared to Diffusion Policy on a diverse set of simulation tasks. Moreover, it produces high-quality and diverse action trajectories within 1-2 steps on a set of real-world robot tasks. Our method also retains the same training complexity as Diffusion Policy and vanilla Flow Matching, in contrast to distillation-based approaches.","authors":["Andreas Sochopoulos","Nikolay Malkin","Nikolaos Tsagkas","Jo\\~ao Moura","Michael Gienger","Sethu Vijayakumar"],"url":"https://arxiv.org/abs/2505.01179"}
{"created":"2025-05-05","title":"BS-tree: A gapped data-parallel B-tree","abstract":"We propose BS-tree, an in-memory implementation of the B+-tree that adopts the structure of the disk-based index (i.e., a balanced, multiway tree), setting the node size to a memory block that can be processed fast and in parallel using SIMD instructions. A novel feature of the BS-tree is that it enables gaps (unused positions) within nodes by duplicating key values. This allows (i) branchless SIMD search within each node, and (ii) branchless update operations in nodes without key shifting. We implement a frame of reference (FOR) compression mechanism, which allows nodes to have varying capacities, and can greatly decrease the memory footprint of BS-tree. We compare our approach to existing main-memory indices and learned indices under different workloads of queries and updates and demonstrate its robustness and superiority compared to previous work in single- and multi-threaded processing.","authors":["Dimitrios Tsitsigkos. Achilleas Michalopoulos","Nikos Mamoulis","Manolis Terrovitis"],"url":"https://arxiv.org/abs/2505.01180"}
{"created":"2025-05-05","title":"Explainable AI Based Diagnosis of Poisoning Attacks in Evolutionary Swarms","abstract":"Swarming systems, such as for example multi-drone networks, excel at cooperative tasks like monitoring, surveillance, or disaster assistance in critical environments, where autonomous agents make decentralized decisions in order to fulfill team-level objectives in a robust and efficient manner. Unfortunately, team-level coordinated strategies in the wild are vulnerable to data poisoning attacks, resulting in either inaccurate coordination or adversarial behavior among the agents. To address this challenge, we contribute a framework that investigates the effects of such data poisoning attacks, using explainable AI methods. We model the interaction among agents using evolutionary intelligence, where an optimal coalition strategically emerges to perform coordinated tasks. Then, through a rigorous evaluation, the swarm model is systematically poisoned using data manipulation attacks. We showcase the applicability of explainable AI methods to quantify the effects of poisoning on the team strategy and extract footprint characterizations that enable diagnosing. Our findings indicate that when the model is poisoned above 10%, non-optimal strategies resulting in inefficient cooperation can be identified.","authors":["Mehrdad Asadi","Roxana R\\u{a}dulescu","Ann Now\\'e"],"url":"https://arxiv.org/abs/2505.01181"}
{"created":"2025-05-05","title":"TSTMotion: Training-free Scene-awarenText-to-motion Generation","abstract":"Text-to-motion generation has recently garnered significant research interest, primarily focusing on generating human motion sequences in blank backgrounds. However, human motions commonly occur within diverse 3D scenes, which has prompted exploration into scene-aware text-to-motion generation methods. Yet, existing scene-aware methods often rely on large-scale ground-truth motion sequences in diverse 3D scenes, which poses practical challenges due to the expensive cost. To mitigate this challenge, we are the first to propose a \\textbf{T}raining-free \\textbf{S}cene-aware \\textbf{T}ext-to-\\textbf{Motion} framework, dubbed as \\textbf{TSTMotion}, that efficiently empowers pre-trained blank-background motion generators with the scene-aware capability. Specifically, conditioned on the given 3D scene and text description, we adopt foundation models together to reason, predict and validate a scene-aware motion guidance. Then, the motion guidance is incorporated into the blank-background motion generators with two modifications, resulting in scene-aware text-driven motion sequences. Extensive experiments demonstrate the efficacy and generalizability of our proposed framework. We release our code in \\href{https://tstmotion.github.io/}{Project Page}.","authors":["Ziyan Guo","Haoxuan Qu","Hossein Rahmani","Dewen Soh","Ping Hu","Qiuhong Ke","Jun Liu"],"url":"https://arxiv.org/abs/2505.01182"}
{"created":"2025-05-05","title":"Distributed Quantum Circuit Cutting for Hybrid Quantum-Classical High-Performance Computing","abstract":"Most quantum computers today are constrained by hardware limitations, particularly the number of available qubits, causing significant challenges for executing large-scale quantum algorithms. Circuit cutting has emerged as a key technique to overcome these limitations by decomposing large quantum circuits into smaller subcircuits that can be executed independently and later reconstructed. In this work, we introduce Qdislib, a distributed and flexible library for quantum circuit cutting, designed to seamlessly integrate with hybrid quantum-classical high-performance computing (HPC) systems. Qdislib employs a graph-based representation of quantum circuits to enable efficient partitioning, manipulation and execution, supporting both wire cutting and gate cutting techniques. The library is compatible with multiple quantum computing libraries, including Qiskit and Qibo, and leverages distributed computing frameworks to execute subcircuits across CPUs, GPUs, and quantum processing units (QPUs) in a fully parallelized manner. We present a proof of concept demonstrating how Qdislib enables the distributed execution of quantum circuits across heterogeneous computing resources, showcasing its potential for scalable quantum-classical workflows.","authors":["Mar Tejedor","Berta Casas","Javier Conejero","Alba Cervera-Lierta","Rosa M. Badia"],"url":"https://arxiv.org/abs/2505.01184"}
{"created":"2025-05-05","title":"EnviKal-Loc: Sub-10m Indoor LoRaWAN Localization using an Environmental-Aware Path Loss and Adaptive RSSI Smoothing","abstract":"LoRaWAN technology's extensive coverage positions it as a strong contender for large-scale IoT deployments. However, achieving sub-10 m accuracy in indoor localization remains challenging due to complex environmental conditions, multipath fading, and transient obstructions. This paper proposes a lightweight but robust approach combining adaptive filtering with an extended log-distance, multi-wall path loss and shadowing (PLS) model. Our methodology augments conventional models with critical LoRaWAN parameters (received signal strength indicator (RSSI), frequency, and signal-to-noise ratio (SNR)) and dynamic environmental indicators (temperature, humidity, carbon dioxide, particulate matter, and barometric pressure). An adaptive Kalman filter reduces RSSI fluctuations, isolating persistent trends from momentary noise. Using a six-month dataset of 1,328,334 field measurements, we evaluate three models: the baseline COST 231 multi-wall model (MWM), the baseline model augmented with environmental parameters (MWM-EP), and a forward-only adaptive Kalman-filtered RSSI version of the latter (MWM-EP-KF). Results confirm that the MWM-EP-KF achieves a mean absolute error (MAE) of 5.81 m, outperforming both the MWM-EP (10.56 m) and the baseline MWM framework (17.98 m). Environmental augmentation reduces systematic errors by 41.22%, while Kalman filtering significantly enhances robustness under high RSSI volatility by 42.63%, on average across all devices. These findings present an interpretable, efficient solution for precise indoor LoRaWAN localization in dynamically changing environments.","authors":["Nahshon Mokua Obiri","Kristof Van Laerhoven"],"url":"https://arxiv.org/abs/2505.01185"}
{"created":"2025-05-05","title":"Secure Cluster-Based Hierarchical Federated Learning in Vehicular Networks","abstract":"Hierarchical Federated Learning (HFL) has recently emerged as a promising solution for intelligent decision-making in vehicular networks, helping to address challenges such as limited communication resources, high vehicle mobility, and data heterogeneity. However, HFL remains vulnerable to adversarial and unreliable vehicles, whose misleading updates can significantly compromise the integrity and convergence of the global model. To address these challenges, we propose a novel defense framework that integrates dynamic vehicle selection with robust anomaly detection within a cluster-based HFL architecture, specifically designed to counter Gaussian noise and gradient ascent attacks. The framework performs a comprehensive reliability assessment for each vehicle by evaluating historical accuracy, contribution frequency, and anomaly records. Anomaly detection combines Z-score and cosine similarity analyses on model updates to identify both statistical outliers and directional deviations in model updates. To further refine detection, an adaptive thresholding mechanism is incorporated into the cosine similarity metric, dynamically adjusting the threshold based on the historical accuracy of each vehicle to enforce stricter standards for consistently high-performing vehicles. In addition, a weighted gradient averaging mechanism is implemented, which assigns higher weights to gradient updates from more trustworthy vehicles. To defend against coordinated attacks, a cross-cluster consistency check is applied to identify collaborative attacks in which multiple compromised clusters coordinate misleading updates. Together, these mechanisms form a multi-level defense strategy to filter out malicious contributions effectively. Simulation results show that the proposed algorithm significantly reduces convergence time compared to benchmark methods across both 1-hop and 3-hop topologies.","authors":["M. Saeid HaghighiFard","Sinem Coleri"],"url":"https://arxiv.org/abs/2505.01186"}
{"created":"2025-05-05","title":"Spectral Efficiency Analysis of Near-Field Holographic MIMO over Ricean Fading Channels","abstract":"With the denser distribution of antenna elements, stronger mutual coupling effects would kick in among antenna elements, which would eventually affect the communication performance. Meanwhile, as the holographic array usually has large physical size, the possibility of near-field communication increases. This paper investigates a near-field multi-user downlink HMIMO system and characterizes the spectral efficiency (SE) under the mutual coupling effect over Ricean fading channels. Both perfect and imperfect channel state information (CSI) scenarios are considered. (i) For the perfect CSI case, the mutual coupling and radiation efficiency model are first established. Then, the closed-form SE is derived under maximum ratio transmission (MRT). By comparing the SE between the cases with and without mutual coupling, it is unveiled that the system SE with mutual coupling might outperform that without mutual coupling in the low transmit power regime for a given aperture size. Moreover, it is also unveiled that the inter-user interference cannot be eliminated unless the physical size of the array increases to infinity. Fortunately, the additional distance term in the near-field channel can be exploited for the inter-user interference mitigation, especially for the worst case, where the users' angular positions overlap to a great extent. (ii) For the imperfect CSI case, the channel estimation error is considered for the derivation of the closed-form SE under MRT. It shows that in the low transmit power regime, the system SE can be enhanced by increasing the pilot power and the antenna element density, the latter of which will lead to severe mutual coupling. In the high transmit power regime, increasing the pilot power has a limited effect on improving the system SE. However, increasing the antenna element density remains highly beneficial for enhancing the system SE.","authors":["Mengyu Qian","Xidong Mu","Li You","Hyundong Shin","Michail Matthaiou"],"url":"https://arxiv.org/abs/2505.01187"}
{"created":"2025-05-05","title":"Continuous Aperture Array (CAPA)-Based Multi-Group Multicast Communications","abstract":"A continuous aperture array (CAPA)-based multi-group multicast communication system is investigated. An integral-based CAPA multi-group multicast beamforming design is formulated for the maximization of the system energy efficiency (EE), subject to a minimum multicast SE constraint of each user group and a total transmit power constraint. To address this non-econvex fractional programming problem, the Dinkelbach's method is employed. Within the Dinkelbach's framework, the non-convex group-wise multicast spectral efficiency (SE) constraint is first equivalently transformed into a tractable form with auxiliary variables. Then, an efficient block coordinate descent (BCD)-based algorithm is developed to solve the reformulated problem. The CAPA beamforming design subproblem can be optimally solved via the Lagrangian dual method and the calculus of variations (CoV) theory. It reveals that the optimal CAPA beamformer should be a combination of all the groups' user channels. To further reduce the computational complexity, a low-complexity zero-forcing (ZF)-based approach is proposed. The closed-form ZF CAPA beamformer is derived using each group's most representative user channel to mitigate the inter-group interference while ensuring the intra-group multicast performance. Then, the beamforming design subproblem in the BCD-based algorithm becomes a convex power allocation subproblem, which can be efficiently solved. Numerical results demonstrate that 1) the CAPA can significantly improve the EE compared to conventional spatially discrete arrays (SPDAs); 2) due to the enhanced spatial resolutions, increasing the aperture size of CAPA is not always beneficial for EE enhancement in multicast scenarios; and 3) wider user distributions of each group cause a significant EE degradation of CAPA compared to SPDA.","authors":["Mengyu Qian","Xidong Mu","Li You","Michail Matthaiou"],"url":"https://arxiv.org/abs/2505.01190"}
{"created":"2025-05-05","title":"Exploring the Impact of Explainable AI and Cognitive Capabilities on Users' Decisions","abstract":"Artificial Intelligence (AI) systems are increasingly used for decision-making across domains, raising debates over the information and explanations they should provide. Most research on Explainable AI (XAI) has focused on feature-based explanations, with less attention on alternative styles. Personality traits like the Need for Cognition (NFC) can also lead to different decision-making outcomes among low and high NFC individuals. We investigated how presenting AI information (prediction, confidence, and accuracy) and different explanation styles (example-based, feature-based, rule-based, and counterfactual) affect accuracy, reliance on AI, and cognitive load in a loan application scenario. We also examined low and high NFC individuals' differences in prioritizing XAI interface elements (loan attributes, AI information, and explanations), accuracy, and cognitive load. Our findings show that high AI confidence significantly increases reliance on AI while reducing cognitive load. Feature-based explanations did not enhance accuracy compared to other conditions. Although counterfactual explanations were less understandable, they enhanced overall accuracy, increasing reliance on AI and reducing cognitive load when AI predictions were correct. Both low and high NFC individuals prioritized explanations after loan attributes, leaving AI information as the least important. However, we found no significant differences between low and high NFC groups in accuracy or cognitive load, raising questions about the role of personality traits in AI-assisted decision-making. These findings highlight the need for user-centric personalization in XAI interfaces, incorporating diverse explanation styles and exploring multiple personality traits and other user characteristics to optimize human-AI collaboration.","authors":["Federico Maria Cau","Lucio Davide Spano"],"url":"https://arxiv.org/abs/2505.01192"}
{"created":"2025-05-05","title":"Going deep and going wide: Counting logic and homomorphism indistinguishability over graphs of bounded treedepth and treewidth","abstract":"We study the expressive power of first-order logic with counting quantifiers, especially the k-variable and quantifier-rank-q fragment C^k_q, using homomorphism indistinguishability. Recently, Dawar, Jakl, and Reggio~(2021) proved that two graphs satisfy the same C^k_q-sentences if and only if they are homomorphism indistinguishable over the class T^k_q of graphs admitting a k-pebble forest cover of depth q. After reproving this result using elementary means, we provide a graph-theoretic analysis of the graph class T^k_q. This allows us to separate T^k_q from the intersection TW_{k-1} \\cap TD_q, provided that q is sufficiently larger than k. Here TW_{k-1} is the class of all graphs of treewidth at most k-1 and TD_q is the class of all graphs of treedepth at most q.","authors":["Isolde Adler","Eva Fluck","Tim Seppelt","Gian Luca Spitzer"],"url":"https://arxiv.org/abs/2505.01193"}
{"created":"2025-05-05","title":"A Combinatorial Proof of Universal Optimality for Computing a Planar Convex Hull","abstract":"For a planar point set $P$, its convex hull is the smallest convex polygon that encloses all points in $P$. The construction of the convex hull from an array $I_P$ containing $P$ is a fundamental problem in computational geometry. By sorting $I_P$ in lexicographical order, one can construct the convex hull of $P$ in $O(n \\log n)$ time which is worst-case optimal. Standard worst-case analysis, however, has been criticized as overly coarse or pessimistic, and researchers search for more refined analyses.","authors":["Ivor van der Hoog","Eva Rotenberg","Daniel Rutschmann"],"url":"https://arxiv.org/abs/2505.01194"}
{"created":"2025-05-05","title":"A Secured Triad of IoT, Machine Learning, and Blockchain for Crop Forecasting in Agriculture","abstract":"To improve crop forecasting and provide farmers with actionable data-driven insights, we propose a novel approach integrating IoT, machine learning, and blockchain technologies. Using IoT, real-time data from sensor networks continuously monitor environmental conditions and soil nutrient levels, significantly improving our understanding of crop growth dynamics. Our study demonstrates the exceptional accuracy of the Random Forest model, achieving a 99.45\\% accuracy rate in predicting optimal crop types and yields, thereby offering precise crop projections and customized recommendations. To ensure the security and integrity of the sensor data used for these forecasts, we integrate the Ethereum blockchain, which provides a robust and secure platform. This ensures that the forecasted data remain tamper-proof and reliable. Stakeholders can access real-time and historical crop projections through an intuitive online interface, enhancing transparency and facilitating informed decision-making. By presenting multiple predicted crop scenarios, our system enables farmers to optimize production strategies effectively. This integrated approach promises significant advances in precision agriculture, making crop forecasting more accurate, secure, and user-friendly.","authors":["Najmus Sakib Sizan","Md. Abu Layek","Khondokar Fida Hasan"],"url":"https://arxiv.org/abs/2505.01196"}
{"created":"2025-05-05","title":"Gender Bias in Explainability: Investigating Performance Disparity in Post-hoc Methods","abstract":"While research on applications and evaluations of explanation methods continues to expand, fairness of the explanation methods concerning disparities in their performance across subgroups remains an often overlooked aspect. In this paper, we address this gap by showing that, across three tasks and five language models, widely used post-hoc feature attribution methods exhibit significant gender disparity with respect to their faithfulness, robustness, and complexity. These disparities persist even when the models are pre-trained or fine-tuned on particularly unbiased datasets, indicating that the disparities we observe are not merely consequences of biased training data. Our results highlight the importance of addressing disparities in explanations when developing and applying explainability methods, as these can lead to biased outcomes against certain subgroups, with particularly critical implications in high-stakes contexts. Furthermore, our findings underscore the importance of incorporating the fairness of explanations, alongside overall model fairness and explainability, as a requirement in regulatory frameworks.","authors":["Mahdi Dhaini","Ege Erdogan","Nils Feldhus","Gjergji Kasneci"],"url":"https://arxiv.org/abs/2505.01198"}
{"created":"2025-05-05","title":"CaReAQA: A Cardiac and Respiratory Audio Question Answering Model for Open-Ended Diagnostic Reasoning","abstract":"Medical audio signals, such as heart and lung sounds, play a crucial role in clinical diagnosis. However, analyzing these signals remains challenging: traditional methods rely on handcrafted features or supervised deep learning models that demand extensive labeled datasets, limiting their scalability and applicability. To address these issues, we propose CaReAQA, an audio-language model that integrates a foundation audio model with the reasoning capabilities of large language models, enabling clinically relevant, open-ended diagnostic responses. Alongside CaReAQA, we introduce CaReSound, a benchmark dataset of annotated medical audio recordings enriched with metadata and paired question-answer examples, intended to drive progress in diagnostic reasoning research. Evaluation results show that CaReAQA achieves 86.2% accuracy on open-ended diagnostic reasoning tasks, outperforming baseline models. It also generalizes well to closed-ended classification tasks, achieving an average accuracy of 56.9% on unseen datasets. Our findings show how audio-language integration and reasoning advances medical diagnostics, enabling efficient AI systems for clinical decision support.","authors":["Tsai-Ning Wang","Lin-Lin Chen","Neil Zeghidour","Aaqib Saeed"],"url":"https://arxiv.org/abs/2505.01199"}
{"created":"2025-05-05","title":"AGRO: An Autonomous AI Rover for Precision Agriculture","abstract":"Unmanned Ground Vehicles (UGVs) are emerging as a crucial tool in the world of precision agriculture. The combination of UGVs with machine learning allows us to find solutions for a range of complex agricultural problems. This research focuses on developing a UGV capable of autonomously traversing agricultural fields and capturing data. The project, known as AGRO (Autonomous Ground Rover Observer) leverages machine learning, computer vision and other sensor technologies. AGRO uses its capabilities to determine pistachio yields, performing self-localization and real-time environmental mapping while avoiding obstacles. The main objective of this research work is to automate resource-consuming operations so that AGRO can support farmers in making data-driven decisions. Furthermore, AGRO provides a foundation for advanced machine learning techniques as it captures the world around it.","authors":["Simar Ghumman","Fabio Di Troia","William Andreopoulos","Mark Stamp","Sanjit Rai"],"url":"https://arxiv.org/abs/2505.01200"}
{"created":"2025-05-05","title":"Efficient Vision-based Vehicle Speed Estimation","abstract":"This paper presents a computationally efficient method for vehicle speed estimation from traffic camera footage. Building upon previous work that utilizes 3D bounding boxes derived from 2D detections and vanishing point geometry, we introduce several improvements to enhance real-time performance. We evaluate our method in several variants on the BrnoCompSpeed dataset in terms of vehicle detection and speed estimation accuracy. Our extensive evaluation across various hardware platforms, including edge devices, demonstrates significant gains in frames per second (FPS) compared to the prior state-of-the-art, while maintaining comparable or improved speed estimation accuracy. We analyze the trade-off between accuracy and computational cost, showing that smaller models utilizing post-training quantization offer the best balance for real-world deployment. Our best performing model beats previous state-of-the-art in terms of median vehicle speed estimation error (0.58 km/h vs. 0.60 km/h), detection precision (91.02% vs 87.08%) and recall (91.14% vs. 83.32%) while also being 5.5 times faster.","authors":["Andrej Macko","Luk\\'a\\v{s} Gajdo\\v{s}ech","Viktor Kocur"],"url":"https://arxiv.org/abs/2505.01203"}
{"created":"2025-05-05","title":"Design for a Digital Twin in Clinical Patient Care","abstract":"Digital Twins hold great potential to personalize clinical patient care, provided the concept is translated to meet specific requirements dictated by established clinical workflows. We present a generalizable Digital Twin design combining knowledge graphs and ensemble learning to reflect the entire patient's clinical journey and assist clinicians in their decision-making. Such Digital Twins can be predictive, modular, evolving, informed, interpretable and explainable with applications ranging from oncology to epidemiology.","authors":["Anna-Katharina Nitschke (Physikalisches Institut","Universit\\\"at Heidelberg","Heidelberg","Germany)","Carlos Brandl (Physikalisches Institut","Universit\\\"at Heidelberg","Heidelberg","Germany)","Fabian Egersd\\\"orfer (Physikalisches Institut","Universit\\\"at Heidelberg","Heidelberg","Germany)","Magdalena G\\\"ortz (Junior Clinical Cooperation Unit 'Multiparametric Methods for Early Detection of Prostate Cancer'","German Cancer Research Center","Department of Urology","Heidelberg University Hospital","Heidelberg","Germany)","Markus Hohenfellner (Department of Urology","Heidelberg University Hospital","Heidelberg","Germany)","Matthias Weidem\\\"uller (Physikalisches Institut","Universit\\\"at Heidelberg","Heidelberg","Germany)"],"url":"https://arxiv.org/abs/2505.01206"}
{"created":"2025-05-05","title":"T-Graph: Enhancing Sparse-view Camera Pose Estimation by Pairwise Translation Graph","abstract":"Sparse-view camera pose estimation, which aims to estimate the 6-Degree-of-Freedom (6-DoF) poses from a limited number of images captured from different viewpoints, is a fundamental yet challenging problem in remote sensing applications. Existing methods often overlook the translation information between each pair of viewpoints, leading to suboptimal performance in sparse-view scenarios. To address this limitation, we introduce T-Graph, a lightweight, plug-and-play module to enhance camera pose estimation in sparse-view settings. T-graph takes paired image features as input and maps them through a Multilayer Perceptron (MLP). It then constructs a fully connected translation graph, where nodes represent cameras and edges encode their translation relationships. It can be seamlessly integrated into existing models as an additional branch in parallel with the original prediction, maintaining efficiency and ease of use. Furthermore, we introduce two pairwise translation representations, relative-t and pair-t, formulated under different local coordinate systems. While relative-t captures intuitive spatial relationships, pair-t offers a rotation-disentangled alternative. The two representations contribute to enhanced adaptability across diverse application scenarios, further improving our module's robustness. Extensive experiments on two state-of-the-art methods (RelPose++ and Forge) using public datasets (C03D and IMC PhotoTourism) validate both the effectiveness and generalizability of T-Graph. The results demonstrate consistent improvements across various metrics, notably camera center accuracy, which improves by 1% to 6% from 2 to 8 viewpoints.","authors":["Qingyu Xian","Weiqin Jiao","Hao Cheng","Berend Jan van der Zwaag","Yanqiu Huang"],"url":"https://arxiv.org/abs/2505.01207"}
{"created":"2025-05-05","title":"Enabling Training-Free Semantic Communication Systems with Generative Diffusion Models","abstract":"Semantic communication (SemCom) has recently emerged as a promising paradigm for next-generation wireless systems. Empowered by advanced artificial intelligence (AI) technologies, SemCom has achieved significant improvements in transmission quality and efficiency. However, existing SemCom systems either rely on training over large datasets and specific channel conditions or suffer from performance degradation under channel noise when operating in a training-free manner. To address these issues, we explore the use of generative diffusion models (GDMs) as training-free SemCom systems. Specifically, we design a semantic encoding and decoding method based on the inversion and sampling process of the denoising diffusion implicit model (DDIM), which introduces a two-stage forward diffusion process, split between the transmitter and receiver to enhance robustness against channel noise. Moreover, we optimize sampling steps to compensate for the increased noise level caused by channel noise. We also conduct a brief analysis to provide insights about this design. Simulations on the Kodak dataset validate that the proposed system outperforms the existing baseline SemCom systems across various metrics.","authors":["Shunpu Tang","Yuanyuan Jia","Qianqian Yang","Ruichen Zhang","Jihong Park","Dusit Niyato"],"url":"https://arxiv.org/abs/2505.01209"}
{"created":"2025-05-05","title":"A Space-Time Trade-off for Fast Self-Stabilizing Leader Election in Population Protocols","abstract":"We consider the problem of self-stabilizing leader election in the population model by Angluin, Aspnes, Diamadi, Fischer, and Peralta (JDistComp '06). The population model is a well-established and powerful model for asynchronous, distributed computation with a large number of applications. For self-stabilizing leader election, the population of $n$ anonymous agents, interacting in uniformly random pairs, must stabilize with a single leader from any possible initial configuration.","authors":["Henry Austin","Petra Berenbrink","Tom Friedetzky","Thorsten G\\\"otte","Lukas Hintze"],"url":"https://arxiv.org/abs/2505.01210"}
{"created":"2025-05-05","title":"High Dynamic Range Novel View Synthesis with Single Exposure","abstract":"High Dynamic Range Novel View Synthesis (HDR-NVS) aims to establish a 3D scene HDR model from Low Dynamic Range (LDR) imagery. Typically, multiple-exposure LDR images are employed to capture a wider range of brightness levels in a scene, as a single LDR image cannot represent both the brightest and darkest regions simultaneously. While effective, this multiple-exposure HDR-NVS approach has significant limitations, including susceptibility to motion artifacts (e.g., ghosting and blurring), high capture and storage costs. To overcome these challenges, we introduce, for the first time, the single-exposure HDR-NVS problem, where only single exposure LDR images are available during training. We further introduce a novel approach, Mono-HDR-3D, featuring two dedicated modules formulated by the LDR image formation principles, one for converting LDR colors to HDR counterparts, and the other for transforming HDR images to LDR format so that unsupervised learning is enabled in a closed loop. Designed as a meta-algorithm, our approach can be seamlessly integrated with existing NVS models. Extensive experiments show that Mono-HDR-3D significantly outperforms previous methods. Source code will be released.","authors":["Kaixuan Zhang","Hu Wang","Minxian Li","Mingwu Ren","Mao Ye","Xiatian Zhu"],"url":"https://arxiv.org/abs/2505.01212"}
{"created":"2025-05-05","title":"A Self-Healing and Fault-Tolerant Cloud-based Digital Twin Processing Management Model","abstract":"Digital twins, integral to cloud platforms, bridge physical and virtual worlds, fostering collaboration among stakeholders in manufacturing and processing. However, the cloud platforms face challenges like service outages, vulnerabilities, and resource contention, hindering critical digital twin application development. The existing research works have limited focus on reliability and fault tolerance in digital twin processing. In this context, this paper proposed a novel Self-healing and Faulttolerant cloud-based Digital Twin processing Management (SF-DTM) model. It employs collaborative digital twin tasks resource requirement estimation unit which utilizes newly devised Federated learning with cosine Similarity integration (SimiFed). Further, SF-DTM incorporates a self-healing fault-tolerance strategy employing a frequent sequence fault-prone pattern analytics unit for deciding the most admissible VM allocation. The implementation and evaluation of SF-DTM model using real traces demonstrates its effectiveness and resilience, revealing improved availability, higher Mean Time Between Failure (MTBF), and lower Mean Time To Repair (MTTR) compared with non-SF-DTM approaches, enhancing collaborative DT application management. SF-DTM improved the services availability up to 13.2% over non-SF-DTM-based DT processing.","authors":["Deepika Saxena","Ashutosh Kumar Singh"],"url":"https://arxiv.org/abs/2505.01215"}
{"created":"2025-05-05","title":"Quantitative Attractor Analysis of High-Capacity Kernel Logistic Regression Hopfield Networks","abstract":"Traditional Hopfield networks, using Hebbian learning, face severe storage capacity limits ($\\approx 0.14$ P/N) and spurious attractors. Kernel Logistic Regression (KLR) offers a non-linear approach, mapping patterns to high-dimensional feature spaces for improved separability. Our previous work showed KLR dramatically improves capacity and noise robustness over conventional methods. This paper quantitatively analyzes the attractor structures in KLR-trained networks via extensive simulations. We evaluated recall from diverse initial states across wide storage loads (up to 4.0 P/N) and noise levels. We quantified convergence rates and speed. Our analysis confirms KLR's superior performance: high capacity (up to 4.0 P/N) and robustness. The attractor landscape is remarkably \"clean,\" with near-zero spurious fixed points. Recall failures under high load/noise are primarily due to convergence to other learned patterns, not spurious ones. Dynamics are exceptionally fast (typically 1-2 steps for high-similarity states). This characterization reveals how KLR reshapes dynamics for high-capacity associative memory, highlighting its effectiveness and contributing to AM understanding.","authors":["Akira Tamamori"],"url":"https://arxiv.org/abs/2505.01218"}
{"created":"2025-05-05","title":"Tell me who its founders are and I'll tell you what your online community looks like: Online community founders' personality and community attributes","abstract":"Online communities are an increasingly important stakeholder for firms, and despite the growing body of research on them, much remains to be learned about them and about the factors that determine their attributes and sustainability. Whereas most of the literature focuses on predictors such as community activity, network structure, and platform interface, there is little research about behavioral and psychological aspects of community members and leaders. In the present study we focus on the personality traits of community founders as predictors of community attributes and sustainability. We develop a tool to estimate community members' Big Five personality traits from their social media text and use it to estimate the traits of 35,164 founders in 8,625 Reddit communities. We find support for most of our predictions about the relationships between founder traits and community sustainability and attributes, including the level of engagement within the community, aspects of its social network structure, and whether the founders themselves remain active in it.","authors":["Yaniv Dover","Shaul Oreg"],"url":"https://arxiv.org/abs/2505.01219"}
{"created":"2025-05-05","title":"Performance of Cell-Free Massive MIMO in Realistic Urban Propagation Environments","abstract":"While UE-centric cell-free massive MIMO (CF-mMIMO) provides high and uniform throughput performance under the assumption of a uniform propagation environment modeled by the log-distance path loss channel model, the performance under a realistic urban propagation environment is not yet fully addressed. In this paper we conduct the first comparative performance study of CF-mMIMO under both the widely assumed log-distance channel model and the realistic urban propagation environment obtained via raytracing using real 3D city layouts and practical AP locations. Our results show that with the raytracing channel model, CF-mMIMO cannot achieve as high and uniform throughput performance as observed with the log-distance channel model, putting into question the attractiveness in practice of CF-mMIMO for real urban deployments.","authors":["Yunlu Xiao","Ljiljana Simi\\'c"],"url":"https://arxiv.org/abs/2505.01222"}
{"created":"2025-05-05","title":"RD-UIE: Relation-Driven State Space Modeling for Underwater Image Enhancement","abstract":"Underwater image enhancement (UIE) is a critical preprocessing step for marine vision applications, where wavelength-dependent attenuation causes severe content degradation and color distortion. While recent state space models like Mamba show potential for long-range dependency modeling, their unfolding operations and fixed scan paths on 1D sequences fail to adapt to local object semantics and global relation modeling, limiting their efficacy in complex underwater environments. To address this, we enhance conventional Mamba with the sorting-based scanning mechanism that dynamically reorders scanning sequences based on statistical distribution of spatial correlation of all pixels. In this way, it encourages the network to prioritize the most informative components--structural and semantic features. Upon building this mechanism, we devise a Visually Self-adaptive State Block (VSSB) that harmonizes dynamic sorting of Mamba with input-dependent dynamic convolution, enabling coherent integration of global context and local relational cues. This exquisite design helps eliminate global focus bias, especially for widely distributed contents, which greatly weakens the statistical frequency. For robust feature extraction and refinement, we design a cross-feature bridge (CFB) to adaptively fuse multi-scale representations. These efforts compose the novel relation-driven Mamba framework for effective UIE (RD-UIE). Extensive experiments on underwater enhancement benchmarks demonstrate RD-UIE outperforms the state-of-the-art approach WMamba in both quantitative metrics and visual fidelity, averagely achieving 0.55 dB performance gain on the three benchmarks. Our code is available at https://github.com/kkoucy/RD-UIE/tree/main","authors":["Kui Jiang","Yan Luo","Junjun Jiang","Xin Xu","Fei Ma","Fei Yu"],"url":"https://arxiv.org/abs/2505.01224"}
{"created":"2025-05-05","title":"Core-Set Selection for Data-efficient Land Cover Segmentation","abstract":"The increasing accessibility of remotely sensed data and the potential of such data to inform large-scale decision-making has driven the development of deep learning models for many Earth Observation tasks. Traditionally, such models must be trained on large datasets. However, the common assumption that broadly larger datasets lead to better outcomes tends to overlook the complexities of the data distribution, the potential for introducing biases and noise, and the computational resources required for processing and storing vast datasets. Therefore, effective solutions should consider both the quantity and quality of data. In this paper, we propose six novel core-set selection methods for selecting important subsets of samples from remote sensing image segmentation datasets that rely on imagery only, labels only, and a combination of each. We benchmark these approaches against a random-selection baseline on three commonly used land cover classification datasets: DFC2022, Vaihingen, and Potsdam. In each of the datasets, we demonstrate that training on a subset of samples outperforms the random baseline, and some approaches outperform training on all available data. This result shows the importance and potential of data-centric learning for the remote sensing domain. The code is available at https://github.com/keillernogueira/data-centric-rs-classification/.","authors":["Keiller Nogueira","Akram Zaytar","Wanli Ma","Ribana Roscher","Ronny H\\\"ansch","Caleb Robinson","Anthony Ortiz","Simone Nsutezo","Rahul Dodhia","Juan M. Lavista Ferres","Oktay Karaku\\c{s}","Paul L. Rosin"],"url":"https://arxiv.org/abs/2505.01225"}
{"created":"2025-05-05","title":"Constructive solution of the common invariant cone problem","abstract":"Sets of $d\\times d$ matrices sharing a common invariant cone enjoy special properties, which are widely used in applications. However, finding this cone or even proving its existence/non-existence is hard. This problem is known to be algorithmically undecidable for general sets of matrices. We show that it can nevertheless be efficiently solved in practice. An algorithm that for a given finite set of matrices, either finds a common invariant cone or proves its non-existence is presented. Numerical results demonstrate that it works for a vast majority of matrix sets. The structure and properties of the minimal and maximal invariant cones are analyzed. Applications to dynamical systems and combinatorics are considered.","authors":["Thomas Mejstrik","Vladimiar Yu. Protasov"],"url":"https://arxiv.org/abs/2505.01229"}
{"created":"2025-05-05","title":"Bilateral Cognitive Security Games in Networked Control Systems under Stealthy Injection Attacks","abstract":"This paper studies a strategic security problem in networked control systems under stealthy false data injection attacks. The security problem is modeled as a bilateral cognitive security game between a defender and an adversary, each possessing cognitive reasoning abilities. The adversary with an adversarial cognitive ability strategically attacks some interconnections of the system with the aim of disrupting the network performance while remaining stealthy to the defender. Meanwhile, the defender with a defense cognitive ability strategically monitors some nodes to impose the stealthiness constraint with the purpose of minimizing the worst-case disruption caused by the adversary. Within the proposed bilateral cognitive security framework, the preferred cognitive levels of the two strategic agents are formulated in terms of two newly proposed concepts, cognitive mismatch and cognitive resonance. Moreover, we propose a method to compute the policies for the defender and the adversary with arbitrary cognitive abilities. A sufficient condition is established under which an increase in cognitive levels does not alter the policies for the defender and the adversary, ensuring convergence. The obtained results are validated through numerical simulations.","authors":["Anh Tung Nguyen","Quanyan Zhu","Andr\\'e Teixeira"],"url":"https://arxiv.org/abs/2505.01232"}
{"created":"2025-05-05","title":"Security Metrics for Uncertain Interconnected Systems under Stealthy Data Injection Attacks","abstract":"This paper quantifies the security of uncertain interconnected systems under stealthy data injection attacks. In particular, we consider a large-scale system composed of a certain subsystem interconnected with an uncertain subsystem, where only the input-output channels are accessible. An adversary is assumed to inject false data to maximize the performance loss of the certain subsystem while remaining undetected. By abstracting the uncertain subsystem as a class of admissible systems satisfying an $\\mathcal{L}_2$ gain constraint, the worst-case performance loss is obtained as the solution to a convex semi-definite program depending only on the certain subsystem dynamics and such an $\\mathcal{L}_2$ gain constraint. This solution is proved to serve as an upper bound for the actual worst-case performance loss when the model of the entire system is fully certain. The results are demonstrated through numerical simulations of the power transmission grid spanning Sweden and Northern Denmark.","authors":["Anh Tung Nguyen","Sribalaji C. Anand","Andr\\'e M. H. Teixeira"],"url":"https://arxiv.org/abs/2505.01233"}
{"created":"2025-05-05","title":"Robust Deep Learning-Based Physical Layer Communications: Strategies and Approaches","abstract":"Deep learning (DL) has emerged as a transformative technology with immense potential to reshape the sixth-generation (6G) wireless communication network. By utilizing advanced algorithms for feature extraction and pattern recognition, DL provides unprecedented capabilities in optimizing the network efficiency and performance, particularly in physical layer communications. Although DL technologies present the great potential, they also face significant challenges related to the robustness, which are expected to intensify in the complex and demanding 6G environment. Specifically, current DL models typically exhibit substantial performance degradation in dynamic environments with time-varying channels, interference of noise and different scenarios, which affect their effectiveness in diverse real-world applications. This paper provides a comprehensive overview of strategies and approaches for robust DL-based methods in physical layer communications. First we introduce the key challenges that current DL models face. Then we delve into a detailed examination of DL approaches specifically tailored to enhance robustness in 6G, which are classified into data-driven and model-driven strategies. Finally, we verify the effectiveness of these methods by case studies and outline future research directions.","authors":["Fenghao Zhu","Xinquan Wang","Chen Zhu","Tierui Gong","Zhaohui Yang","Chongwen Huang","Xiaoming Chen","Zhaoyang Zhang","M\\'erouane Debbah"],"url":"https://arxiv.org/abs/2505.01234"}
{"created":"2025-05-05","title":"Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting","abstract":"Online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. However, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. This paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. We propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. We show that our method restores the ideal observation by subtracting the learned error. We demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. Code, video results, and checkpoints are available at https://bbangsik13.github.io/OR2.","authors":["Youngsik Yun","Jeongmin Bae","Hyunseung Son","Seoha Kim","Hahyun Lee","Gun Bang","Youngjung Uh"],"url":"https://arxiv.org/abs/2505.01235"}
{"created":"2025-05-05","title":"CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment","abstract":"Recent advances in audio-visual learning have shown promising results in learning representations across modalities. However, most approaches rely on global audio representations that fail to capture fine-grained temporal correspondences with visual frames. Additionally, existing methods often struggle with conflicting optimization objectives when trying to jointly learn reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync as a simple yet effective extension of the original CAV-MAE framework for self-supervised audio-visual learning. We address three key challenges: First, we tackle the granularity mismatch between modalities by treating audio as a temporal sequence aligned with video frames, rather than using global representations. Second, we resolve conflicting optimization goals by separating contrastive and reconstruction objectives through dedicated global tokens. Third, we improve spatial localization by introducing learnable register tokens that reduce semantic load on patch tokens. We evaluate the proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on zero-shot retrieval, classification and localization tasks demonstrating state-of-the-art performance and outperforming more complex architectures.","authors":["Edson Araujo","Andrew Rouditchenko","Yuan Gong","Saurabhchand Bhati","Samuel Thomas","Brian Kingsbury","Leonid Karlinsky","Rogerio Feris","James R. Glass"],"url":"https://arxiv.org/abs/2505.01237"}
{"created":"2025-05-05","title":"EvalxNLP: A Framework for Benchmarking Post-Hoc Explainability Methods on NLP Models","abstract":"As Natural Language Processing (NLP) models continue to evolve and become integral to high-stakes applications, ensuring their interpretability remains a critical challenge. Given the growing variety of explainability methods and diverse stakeholder requirements, frameworks that help stakeholders select appropriate explanations tailored to their specific use cases are increasingly important. To address this need, we introduce EvalxNLP, a Python framework for benchmarking state-of-the-art feature attribution methods for transformer-based NLP models. EvalxNLP integrates eight widely recognized explainability techniques from the Explainable AI (XAI) literature, enabling users to generate and evaluate explanations based on key properties such as faithfulness, plausibility, and complexity. Our framework also provides interactive, LLM-based textual explanations, facilitating user understanding of the generated explanations and evaluation outcomes. Human evaluation results indicate high user satisfaction with EvalxNLP, suggesting it is a promising framework for benchmarking explanation methods across diverse user groups. By offering a user-friendly and extensible platform, EvalxNLP aims at democratizing explainability tools and supporting the systematic comparison and advancement of XAI techniques in NLP.","authors":["Mahdi Dhaini","Kafaite Zahra Hussain","Efstratios Zaradoukas","Gjergji Kasneci"],"url":"https://arxiv.org/abs/2505.01238"}
{"created":"2025-05-05","title":"mwBTFreddy: A Dataset for Flash Flood Damage Assessment in Urban Malawi","abstract":"This paper describes the mwBTFreddy dataset, a resource developed to support flash flood damage assessment in urban Malawi, specifically focusing on the impacts of Cyclone Freddy in 2023. The dataset comprises paired pre- and post-disaster satellite images sourced from Google Earth Pro, accompanied by JSON files containing labelled building annotations with geographic coordinates and damage levels (no damage, minor, major, or destroyed). Developed by the Kuyesera AI Lab at the Malawi University of Business and Applied Sciences, this dataset is intended to facilitate the development of machine learning models tailored to building detection and damage classification in African urban contexts. It also supports flood damage visualisation and spatial analysis to inform decisions on relocation, infrastructure planning, and emergency response in climate-vulnerable regions.","authors":["Evelyn Chapuma","Grey Mengezi","Lewis Msasa","Amelia Taylor"],"url":"https://arxiv.org/abs/2505.01242"}
{"created":"2025-05-05","title":"Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design","abstract":"Humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. In this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. This linear transformation allows us to carry out exact inference for the latent variables in factor analysis (FA) and mixtures of FA models of the scene. Further, this allows us to formulate and solve the choice of \"where to look next\" as a Bayesian experimental design problem using the Expected Information Gain criterion. Experiments on the Frey faces and MNIST datasets demonstrate the effectiveness of our models.","authors":["Christopher K. I. Williams"],"url":"https://arxiv.org/abs/2505.01249"}
{"created":"2025-05-05","title":"PHSafe: Disclosure Avoidance for the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC)","abstract":"This article describes the disclosure avoidance algorithm that the U.S. Census Bureau used to protect the 2020 Census Supplemental Demographic and Housing Characteristics File (S-DHC). The tabulations contain statistics of counts of U.S. persons living in certain types of households, including averages. The article describes the PHSafe algorithm, which is based on adding noise drawn from a discrete Gaussian distribution to the statistics of interest. We prove that the algorithm satisfies a well-studied variant of differential privacy, called zero-concentrated differential privacy. We then describe how the algorithm was implemented on Tumult Analytics and briefly outline the parameterization and tuning of the algorithm.","authors":["William Sexton","Skye Berghel","Bayard Carlson","Sam Haney","Luke Hartman","Michael Hay","Ashwin Machanavajjhala","Gerome Miklau","Amritha Pai","Simran Rajpal","David Pujol","Ruchit Shrestha","Daniel Simmons-Marengo"],"url":"https://arxiv.org/abs/2505.01254"}
{"created":"2025-05-05","title":"PREMISE: Matching-based Prediction for Accurate Review Recommendation","abstract":"We present PREMISE (PREdict with Matching ScorEs), a new architecture for the matching-based learning in the multimodal fields for the multimodal review helpfulness (MRHP) task. Distinct to previous fusion-based methods which obtains multimodal representations via cross-modal attention for downstream tasks, PREMISE computes the multi-scale and multi-field representations, filters duplicated semantics, and then obtained a set of matching scores as feature vectors for the downstream recommendation task. This new architecture significantly boosts the performance for such multimodal tasks whose context matching content are highly correlated to the targets of that task, compared to the state-of-the-art fusion-based methods. Experimental results on two publicly available datasets show that PREMISE achieves promising performance with less computational cost.","authors":["Wei Han","Hui Chen","Soujanya Poria"],"url":"https://arxiv.org/abs/2505.01255"}
{"created":"2025-05-05","title":"A First Runtime Analysis of NSGA-III on a Many-Objective Multimodal Problem: Provable Exponential Speedup via Stochastic Population Update","abstract":"The NSGA-III is a prominent algorithm in evolutionary many-objective optimization. It is well-suited for optimizing functions with more than three objectives, setting it apart from the classic NSGA-II. However, theoretical insights about NSGA-III of when and why it performs well are still in its early development. This paper addresses this point and conducts a rigorous runtime analysis of NSGA-III on the many-objective \\textsc{OneJumpZeroJump} benchmark (\\textsc{OjZj} for short), providing the first runtime bounds where the number of objectives is constant. We show that NSGA-III finds the Pareto front of \\textsc{OjZj} in time $O(n^{k+d/2}+ \\mu n \\ln(n))$ where $n$ is the problem size, $d$ is the number of objectives, $k$ is the gap size, a problem specific parameter, if its population size $\\mu \\in 2^{O(n)}$ is at least $(2n/d+1)^{d/2}$. Notably, NSGA-III is faster than NSGA-II by a factor of $\\mu/n^{d/2}$ for some $\\mu \\in \\omega(n^{d/2})$. We also show that a stochastic population update, proposed by Bian et al., provably guarantees a speedup of order $\\Theta((k/b)^{k-1})$ in the runtime where $b>0$ is a constant. To our knowledge, this is the first rigorous runtime analysis of NSGA-III on \\textsc{OjZj}. Proving these bounds requires a much deeper understanding of the population dynamics of NSGA-III than previous papers achieved.","authors":["Andre Opris"],"url":"https://arxiv.org/abs/2505.01256"}
{"created":"2025-05-05","title":"CAMELTrack: Context-Aware Multi-cue ExpLoitation for Online Multi-Object Tracking","abstract":"Online multi-object tracking has been recently dominated by tracking-by-detection (TbD) methods, where recent advances rely on increasingly sophisticated heuristics for tracklet representation, feature fusion, and multi-stage matching. The key strength of TbD lies in its modular design, enabling the integration of specialized off-the-shelf models like motion predictors and re-identification. However, the extensive usage of human-crafted rules for temporal associations makes these methods inherently limited in their ability to capture the complex interplay between various tracking cues. In this work, we introduce CAMEL, a novel association module for Context-Aware Multi-Cue ExpLoitation, that learns resilient association strategies directly from data, breaking free from hand-crafted heuristics while maintaining TbD's valuable modularity. At its core, CAMEL employs two transformer-based modules and relies on a novel association-centric training scheme to effectively model the complex interactions between tracked targets and their various association cues. Unlike end-to-end detection-by-tracking approaches, our method remains lightweight and fast to train while being able to leverage external off-the-shelf models. Our proposed online tracking pipeline, CAMELTrack, achieves state-of-the-art performance on multiple tracking benchmarks. Our code is available at https://github.com/TrackingLaboratory/CAMELTrack.","authors":["Vladimir Somers","Baptiste Standaert","Victor Joos","Alexandre Alahi","Christophe De Vleeschouwer"],"url":"https://arxiv.org/abs/2505.01257"}
{"created":"2025-05-05","title":"Enhancing Obsolescence Forecasting with Deep Generative Data Augmentation: A Semi-Supervised Framework for Low-Data Industrial Applications","abstract":"The challenge of electronic component obsolescence is particularly critical in systems with long life cycles. Various obsolescence management methods are employed to mitigate its impact, with obsolescence forecasting being a highly sought-after and prominent approach. As a result, numerous machine learning-based forecasting methods have been proposed. However, machine learning models require a substantial amount of relevant data to achieve high precision, which is lacking in the current obsolescence landscape in some situations. This work introduces a novel framework for obsolescence forecasting based on deep learning. The proposed framework solves the lack of available data through deep generative modeling, where new obsolescence cases are generated and used to augment the training dataset. The augmented dataset is then used to train a classical machine learning-based obsolescence forecasting model. To train classical forecasting models using augmented datasets, existing classical supervised-learning classifiers are adapted for semi-supervised learning within this framework. The proposed framework demonstrates state-of-the-art results on benchmarking datasets.","authors":["Elie Saad","Mariem Besbes","Marc Zolghadri","Victor Czmil","Claude Baron","Vincent Bourgeois"],"url":"https://arxiv.org/abs/2505.01261"}
{"created":"2025-05-05","title":"Thinking Outside the Template with Modular GP-GOMEA","abstract":"The goal in Symbolic Regression (SR) is to discover expressions that accurately map input to output data. Because often the intent is to understand these expressions, there is a trade-off between accuracy and the interpretability of expressions. GP-GOMEA excels at producing small SR expressions (increasing the potential for interpretability) with high accuracy, but requires a fixed tree template, which limits the types of expressions that can be evolved. This paper presents a modular representation for GP-GOMEA that allows multiple trees to be evolved simultaneously that can be used as (functional) subexpressions. While each tree individually is constrained to a (small) fixed tree template, the final expression, if expanded, can exhibit a much larger structure. Furthermore, the use of subexpressions decomposes the original regression problem and opens the possibility for enhanced interpretability through the piece-wise understanding of small subexpressions. We compare the performance of GP-GOMEA with and without modular templates on a variety of datasets. We find that our proposed approach generally outperforms single-template GP-GOMEA and can moreover uncover ground-truth expressions underlying synthetic datasets with modular subexpressions at a faster rate than GP-GOMEA without modular subexpressions.","authors":["Joe Harrison","Peter A. N. Bosman","Tanja Alderliesten"],"url":"https://arxiv.org/abs/2505.01262"}
{"created":"2025-05-05","title":"FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing","abstract":"Movie Dubbing aims to convert scripts into speeches that align with the given movie clip in both temporal and emotional aspects while preserving the vocal timbre of a given brief reference audio. Existing methods focus primarily on reducing the word error rate while ignoring the importance of lip-sync and acoustic quality. To address these issues, we propose a large language model (LLM) based flow matching architecture for dubbing, named FlowDubber, which achieves high-quality audio-visual sync and pronunciation by incorporating a large speech language model and dual contrastive aligning while achieving better acoustic quality via the proposed voice-enhanced flow matching than previous works. First, we introduce Qwen2.5 as the backbone of LLM to learn the in-context sequence from movie scripts and reference audio. Then, the proposed semantic-aware learning focuses on capturing LLM semantic knowledge at the phoneme level. Next, dual contrastive aligning (DCA) boosts mutual alignment with lip movement, reducing ambiguities where similar phonemes might be confused. Finally, the proposed Flow-based Voice Enhancing (FVE) improves acoustic quality in two aspects, which introduces an LLM-based acoustics flow matching guidance to strengthen clarity and uses affine style prior to enhance identity when recovering noise into mel-spectrograms via gradient vector field prediction. Extensive experiments demonstrate that our method outperforms several state-of-the-art methods on two primary benchmarks. The demos are available at {\\href{https://galaxycong.github.io/LLM-Flow-Dubber/}{\\textcolor{red}{https://galaxycong.github.io/LLM-Flow-Dubber/}}}.","authors":["Gaoxiang Cong","Liang Li","Jiadong Pan","Zhedong Zhang","Amin Beheshti","Anton van den Hengel","Yuankai Qi","Qingming Huang"],"url":"https://arxiv.org/abs/2505.01263"}
{"created":"2025-05-05","title":"Tight Runtime Guarantees From Understanding the Population Dynamics of the GSEMO Multi-Objective Evolutionary Algorithm","abstract":"The global simple evolutionary multi-objective optimizer (GSEMO) is a simple, yet often effective multi-objective evolutionary algorithm (MOEA). By only maintaining non-dominated solutions, it has a variable population size that automatically adjusts to the needs of the optimization process. The downside of the dynamic population size is that the population dynamics of this algorithm are harder to understand, resulting, e.g., in the fact that only sporadic tight runtime analyses exist. In this work, we significantly enhance our understanding of the dynamics of the GSEMO, in particular, for the classic CountingOnesCountingZeros (COCZ) benchmark. From this, we prove a lower bound of order $\\Omega(n^2 \\log n)$, for the first time matching the seminal upper bounds known for over twenty years. We also show that the GSEMO finds any constant fraction of the Pareto front in time $O(n^2)$, improving over the previous estimate of $O(n^2 \\log n)$ for the time to find the first Pareto optimum. Our methods extend to other classic benchmarks and yield, e.g., the first $\\Omega(n^{k+1})$ lower bound for the OJZJ benchmark in the case that the gap parameter is $k \\in \\{2,3\\}$. We are therefore optimistic that our new methods will be useful in future mathematical analyses of MOEAs.","authors":["Benjamin Doerr","Martin Krejca","Andre Opris"],"url":"https://arxiv.org/abs/2505.01266"}
{"created":"2025-05-05","title":"Diffusion-based Adversarial Purification from the Perspective of the Frequency Domain","abstract":"The diffusion-based adversarial purification methods attempt to drown adversarial perturbations into a part of isotropic noise through the forward process, and then recover the clean images through the reverse process. Due to the lack of distribution information about adversarial perturbations in the pixel domain, it is often unavoidable to damage normal semantics. We turn to the frequency domain perspective, decomposing the image into amplitude spectrum and phase spectrum. We find that for both spectra, the damage caused by adversarial perturbations tends to increase monotonically with frequency. This means that we can extract the content and structural information of the original clean sample from the frequency components that are less damaged. Meanwhile, theoretical analysis indicates that existing purification methods indiscriminately damage all frequency components, leading to excessive damage to the image. Therefore, we propose a purification method that can eliminate adversarial perturbations while maximizing the preservation of the content and structure of the original image. Specifically, at each time step during the reverse process, for the amplitude spectrum, we replace the low-frequency components of the estimated image's amplitude spectrum with the corresponding parts of the adversarial image. For the phase spectrum, we project the phase of the estimated image into a designated range of the adversarial image's phase spectrum, focusing on the low frequencies. Empirical evidence from extensive experiments demonstrates that our method significantly outperforms most current defense methods.","authors":["Gaozheng Pei","Ke Ma","Yingfei Sun","Qianqian Xu","Qingming Huang"],"url":"https://arxiv.org/abs/2505.01267"}
{"created":"2025-05-05","title":"Verifying Parameterized Networks Specified by Vertex-Replacement Graph Grammars","abstract":"We consider the parametric reachability problem (PRP) for families of networks described by vertex-replacement (VR) graph grammars, where network nodes run replicas of finite-state processes that communicate via binary handshaking. We show that the PRP problem for VR grammars can be effectively reduced to the PRP problem for hyperedge-replacement (HR) grammars at the cost of introducing extra edges for routing messages. This transformation is motivated by the existence of several parametric verification techniques for families of networks specified by HR grammars, or similar inductive formalisms. Our reduction enables applying the verification techniques for HR systems to systems with dense architectures, such as user-specified cliques and multi-partite graphs.","authors":["Radu Iosif","Arnaud Sangnier","Neven Villani"],"url":"https://arxiv.org/abs/2505.01269"}
{"created":"2025-05-05","title":"Anti-adversarial Learning: Desensitizing Prompts for Large Language Models","abstract":"With the widespread use of LLMs, preserving privacy in user prompts has become crucial, as prompts risk exposing privacy and sensitive data to the cloud LLMs. Traditional techniques like homomorphic encryption, secure multi-party computation, and federated learning face challenges due to heavy computational costs and user participation requirements, limiting their applicability in LLM scenarios. In this paper, we propose PromptObfus, a novel method for desensitizing LLM prompts. The core idea of PromptObfus is \"anti-adversarial\" learning, which perturbs privacy words in the prompt to obscure sensitive information while retaining the stability of model predictions. Specifically, PromptObfus frames prompt desensitization as a masked language modeling task, replacing privacy-sensitive terms with a [MASK] token. A desensitization model is trained to generate candidate replacements for each masked position. These candidates are subsequently selected based on gradient feedback from a surrogate model, ensuring minimal disruption to the task output. We demonstrate the effectiveness of our approach on three NLP tasks. Results show that PromptObfus effectively prevents privacy inference from remote LLMs while preserving task performance.","authors":["Xuan Li","Zhe Yin","Xiaodong Gu","Beijun Shen"],"url":"https://arxiv.org/abs/2505.01273"}
{"created":"2025-05-05","title":"MultiGran-STGCNFog: Towards Accurate and High-Throughput Inference for Multi-Granular Spatiotemporal Traffic Forecasting","abstract":"Accurate traffic forecasting and swift inference provision are essential for intelligent transportation systems. However, the present Graph Convolutional Network (GCN)-based approaches cannot extract and fuse multi-granular spatiotemporal features across various spatial and temporal scales sufficiently, proven to yield less accurate forecasts. Besides, additional feature extraction branches introduced in prior studies critically increased model complexity and extended inference time, making it challenging to provide fast inference for traffic forecasting. In this paper, we propose MultiGran-STGCNFog, an efficient fog distributed inference system with a novel traffic forecasting model that employs multi-granular spatiotemporal feature fusion on generated dynamic traffic graphs to fully capture interdependent traffic dynamics. The proposed scheduling algorithm GA-DPHDS, optimizing layer execution order and layer-device scheduling scheme simultaneously, contributes to considerable inference throughput improvement by leveraging heterogeneous fog devices in a pipelined manner. Extensive experiments on real-world datasets demonstrate the superiority of the proposed method over selected baselines.","authors":["Zhaoyan Wang","Xiangchi Song","In-Young Ko"],"url":"https://arxiv.org/abs/2505.01279"}
{"created":"2025-05-05","title":"A Physics-preserved Transfer Learning Method for Differential Equations","abstract":"While data-driven methods such as neural operator have achieved great success in solving differential equations (DEs), they suffer from domain shift problems caused by different learning environments (with data bias or equation changes), which can be alleviated by transfer learning (TL). However, existing TL methods adopted in DEs problems lack either generalizability in general DEs problems or physics preservation during training. In this work, we focus on a general transfer learning method that adaptively correct the domain shift and preserve physical information. Mathematically, we characterize the data domain as product distribution and the essential problems as distribution bias and operator bias. A Physics-preserved Optimal Tensor Transport (POTT) method that simultaneously admits generalizability to common DEs and physics preservation of specific problem is proposed to adapt the data-driven model to target domain utilizing the push-forward distribution induced by the POTT map. Extensive experiments demonstrate the superior performance, generalizability and physics preservation of the proposed POTT method.","authors":["Hao-Ran Yang","Chuan-Xian Ren"],"url":"https://arxiv.org/abs/2505.01281"}
{"created":"2025-05-05","title":"Micro-Patterns in Solidity Code","abstract":"Solidity is the predominant programming language for blockchain-based smart contracts, and its characteristics pose significant challenges for code analysis and maintenance. Traditional software analysis approaches, while effective for conventional programming languages, often fail to address Solidity-specific features such as gas optimization and security constraints.","authors":["Luca Ruschioni","Robert Shuttleworth","Rumyana Neykova","Barbara Re","Giuseppe Destefanis"],"url":"https://arxiv.org/abs/2505.01282"}
{"created":"2025-05-05","title":"Reduced-order structure-property linkages for stochastic metamaterials","abstract":"The capabilities of additive manufacturing have facilitated the design and production of mechanical metamaterials with diverse unit cell geometries. Establishing linkages between the vast design space of unit cells and their effective mechanical properties is critical for the efficient design and performance evaluation of such metamaterials. However, physics-based simulations of metamaterial unit cells across the entire design space are computationally expensive, necessitating a materials informatics framework to efficiently capture complex structure-property relationships. In this work, principal component analysis of 2-point correlation functions is performed to extract the salient features from a large dataset of randomly generated 2D metamaterials. Physics-based simulations are performed using a fast Fourier transform (FFT)-based homogenization approach to efficiently compute the homogenized effective elastic stiffness across the extensive unit cell designs. Subsequently, Gaussian process regression is used to generate reduced-order surrogates, mapping unit cell designs to their homogenized effective elastic constant. It is demonstrated that the adopted workflow enables a high-value low-dimensional representation of the voluminous stochastic metamaterial dataset, facilitating the construction of robust structure-property maps. Finally, an uncertainty-based active learning framework is utilized to train a surrogate model with a significantly smaller number of data points compared to the original full dataset. It is shown that a dataset as small as $0.61\\%$ of the entire dataset is sufficient to generate accurate and robust structure-property maps.","authors":["Hooman Danesh","Maruthi Annamaraju","Tim Brepols","Stefanie Reese","Surya R. Kalidindi"],"url":"https://arxiv.org/abs/2505.01283"}
{"created":"2025-05-05","title":"2DXformer: Dual Transformers for Wind Power Forecasting with Dual Exogenous Variables","abstract":"Accurate wind power forecasting can help formulate scientific dispatch plans, which is of great significance for maintaining the safety, stability, and efficient operation of the power system. In recent years, wind power forecasting methods based on deep learning have focused on extracting the spatiotemporal correlations among data, achieving significant improvements in forecasting accuracy. However, they exhibit two limitations. First, there is a lack of modeling for the inter-variable relationships, which limits the accuracy of the forecasts. Second, by treating endogenous and exogenous variables equally, it leads to unnecessary interactions between the endogenous and exogenous variables, increasing the complexity of the model. In this paper, we propose the 2DXformer, which, building upon the previous work's focus on spatiotemporal correlations, addresses the aforementioned two limitations. Specifically, we classify the inputs of the model into three types: exogenous static variables, exogenous dynamic variables, and endogenous variables. First, we embed these variables as variable tokens in a channel-independent manner. Then, we use the attention mechanism to capture the correlations among exogenous variables. Finally, we employ a multi-layer perceptron with residual connections to model the impact of exogenous variables on endogenous variables. Experimental results on two real-world large-scale datasets indicate that our proposed 2DXformer can further improve the performance of wind power forecasting. The code is available in this repository: \\href{https://github.com/jseaj/2DXformer}{https://github.com/jseaj/2DXformer}.","authors":["Yajuan Zhang","Jiahai Jiang","Yule Yan","Liang Yang","Ping Zhang"],"url":"https://arxiv.org/abs/2505.01286"}
{"created":"2025-05-05","title":"Shuffling Cards When You Are of Very Little Brain: Low Memory Generation of Permutations","abstract":"How can we generate a permutation of the numbers $1$ through $n$ so that it is hard to guess the next element given the history so far? The twist is that the generator of the permutation (the ``Dealer\") has limited memory, while the ``Guesser\" has unlimited memory. With unbounded memory (actually $n$ bits suffice), the Dealer can generate a truly random permutation where~$\\ln n$ is the expected number of correct guesses.","authors":["Boaz Menuhin","Moni Naor"],"url":"https://arxiv.org/abs/2505.01287"}
{"created":"2025-05-05","title":"ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow","abstract":"One of the central challenges preventing robots from acquiring complex manipulation skills is the prohibitive cost of collecting large-scale robot demonstrations. In contrast, humans are able to learn efficiently by watching others interact with their environment. To bridge this gap, we introduce semantic action flow as a core intermediate representation capturing the essential spatio-temporal manipulator-object interactions, invariant to superficial visual differences. We present ViSA-Flow, a framework that learns this representation self-supervised from unlabeled large-scale video data. First, a generative model is pre-trained on semantic action flows automatically extracted from large-scale human-object interaction video data, learning a robust prior over manipulation structure. Second, this prior is efficiently adapted to a target robot by fine-tuning on a small set of robot demonstrations processed through the same semantic abstraction pipeline. We demonstrate through extensive experiments on the CALVIN benchmark and real-world tasks that ViSA-Flow achieves state-of-the-art performance, particularly in low-data regimes, outperforming prior methods by effectively transferring knowledge from human video observation to robotic execution. Videos are available at https://visaflow-web.github.io/ViSAFLOW.","authors":["Changhe Chen","Quantao Yang","Xiaohao Xu","Nima Fazeli","Olov Andersson"],"url":"https://arxiv.org/abs/2505.01288"}
{"created":"2025-05-05","title":"Fine-grained Manipulation Attacks to Local Differential Privacy Protocols for Data Streams","abstract":"Local Differential Privacy (LDP) enables massive data collection and analysis while protecting end users' privacy against untrusted aggregators. It has been applied to various data types (e.g., categorical, numerical, and graph data) and application settings (e.g., static and streaming). Recent findings indicate that LDP protocols can be easily disrupted by poisoning or manipulation attacks, which leverage injected/corrupted fake users to send crafted data conforming to the LDP reports. However, current attacks primarily target static protocols, neglecting the security of LDP protocols in the streaming settings. Our research fills the gap by developing novel fine-grained manipulation attacks to LDP protocols for data streams. By reviewing the attack surfaces in existing algorithms, We introduce a unified attack framework with composable modules, which can manipulate the LDP estimated stream toward a target stream. Our attack framework can adapt to state-of-the-art streaming LDP algorithms with different analytic tasks (e.g., frequency and mean) and LDP models (event-level, user-level, w-event level). We validate our attacks theoretically and through extensive experiments on real-world datasets, and finally explore a possible defense mechanism for mitigating these attacks.","authors":["Xinyu Li","Xuebin Ren","Shusen Yang","Liang Shi","Chia-Mu Yu"],"url":"https://arxiv.org/abs/2505.01292"}
{"created":"2025-05-05","title":"A generalization of the Gauss-Seidel iteration method for generalized absolute value equations","abstract":"A parameter-free method, namely the generalization of the Gauss-Seidel (GGS) method, is developed to solve generalized absolute value equations. Convergence of the proposed method is analyzed. Numerical results are given to demonstrate the effectiveness and efficiency of the GGS method. Some results in the recent work of Edalatpour et al. \\cite{edhs2017} are extended.","authors":["Tingting Luo","Jiayu Liu","Cairong Chen","Linjie Chen","Changfeng Ma"],"url":"https://arxiv.org/abs/2505.01293"}
{"created":"2025-05-05","title":"Pattern formation using an intrinsic optimal control approach","abstract":"This paper investigates a pattern formation control problem for a multi-agent system modeled with given interaction topology, in which $m$ of the $n$ agents are chosen as leaders and consequently a control signal is added to each of the leaders. These agents interact with each other by Laplacian dynamics on a graph. The pattern formation control problem is formulated as an intrinsic infinite time-horizon linear quadratic optimal control problem, namely, no error information is incorporated in the objective function. Under mild conditions, we show the existence of the optimal control strategy and the convergence to the desired pattern formation. Based on the optimal control strategy, we propose a distributed control strategy to achieve the given pattern. Finally, numerical simulation is given to illustrate theoretical results.","authors":["Tianhao Li","Yibei Li","Zhixin Liu","Xiaoming Hu"],"url":"https://arxiv.org/abs/2505.01302"}
{"created":"2025-05-05","title":"Early Detection of Patient Deterioration from Real-Time Wearable Monitoring System","abstract":"Early detection of patient deterioration is crucial for reducing mortality rates. Heart rate data has shown promise in assessing patient health, and wearable devices offer a cost-effective solution for real-time monitoring. However, extracting meaningful insights from diverse heart rate data and handling missing values in wearable device data remain key challenges. To address these challenges, we propose TARL, an innovative approach that models the structural relationships of representative subsequences, known as shapelets, in heart rate time series. TARL creates a shapelet-transition knowledge graph to model shapelet dynamics in heart rate time series, indicating illness progression and potential future changes. We further introduce a transition-aware knowledge embedding to reinforce relationships among shapelets and quantify the impact of missing values, enabling the formulation of comprehensive heart rate representations. These representations capture explanatory structures and predict future heart rate trends, aiding early illness detection. We collaborate with physicians and nurses to gather ICU patient heart rate data from wearables and diagnostic metrics assessing illness severity for evaluating deterioration. Experiments on real-world ICU data demonstrate that TARL achieves both high reliability and early detection. A case study further showcases TARL's explainable detection process, highlighting its potential as an AI-driven tool to assist clinicians in recognizing early signs of patient deterioration.","authors":["Lo Pang-Yun Ting","Hong-Pei Chen","An-Shan Liu","Chun-Yin Yeh","Po-Lin Chen","Kun-Ta Chuang"],"url":"https://arxiv.org/abs/2505.01305"}
{"created":"2025-05-05","title":"Document Retrieval Augmented Fine-Tuning (DRAFT) for safety-critical software assessments","abstract":"Safety critical software assessment requires robust assessment against complex regulatory frameworks, a process traditionally limited by manual evaluation. This paper presents Document Retrieval-Augmented Fine-Tuning (DRAFT), a novel approach that enhances the capabilities of a large language model (LLM) for safety-critical compliance assessment. DRAFT builds upon existing Retrieval-Augmented Generation (RAG) techniques by introducing a novel fine-tuning framework that accommodates our dual-retrieval architecture, which simultaneously accesses both software documentation and applicable reference standards. To fine-tune DRAFT, we develop a semi-automated dataset generation methodology that incorporates variable numbers of relevant documents with meaningful distractors, closely mirroring real-world assessment scenarios. Experiments with GPT-4o-mini demonstrate a 7% improvement in correctness over the baseline model, with qualitative improvements in evidence handling, response structure, and domain-specific reasoning. DRAFT represents a practical approach to improving compliance assessment systems while maintaining the transparency and evidence-based reasoning essential in regulatory domains.","authors":["Regan Bolton","Mohammadreza Sheikhfathollahi","Simon Parkinson","Vanessa Vulovic","Gary Bamford","Dan Basher","Howard Parkinson"],"url":"https://arxiv.org/abs/2505.01307"}
{"created":"2025-05-05","title":"Desired Impedance Allocation for Robotic Systems","abstract":"Virtual Decomposition Control (VDC) has emerged as a powerful modular framework for real-world robotic control, particularly in contact-rich tasks. Despite its widespread use, VDC has been fundamentally limited to first-order impedance allocation, inherently neglecting the desired inertia due to the mathematical complexity of second-order behavior allocation. However, inertia is crucial, not only for shaping dynamic responses during contact phases, but also for enabling smooth acceleration and deceleration in trajectory tracking. Motivated by the growing demand for high-fidelity interaction control, this work introduces, for the first time in the VDC framework, a method to realize second-order impedance behavior. By redefining the required end-effector velocity and introducing a required acceleration and a pseudo-impedance term, we achieve second-order impedance control while preserving the modularity of VDC. Rigorous stability analysis confirms the robustness of the proposed controller. Experimental validation on a 7-degree-of-freedom haptic exoskeleton demonstrates superior tracking and contact performance compared to first-order methods. Notably, incorporating inertia enables stable interaction with environments up to 70% stiffer, highlighting the effectiveness of the approach in real-world contact-rich scenarios.","authors":["Mahdi Hejrati","Jouni Mattila"],"url":"https://arxiv.org/abs/2505.01308"}
{"created":"2025-05-05","title":"Enhancing SPARQL Query Rewriting for Complex Ontology Alignments","abstract":"SPARQL query rewriting is a fundamental mechanism for uniformly querying heterogeneous ontologies in the Linked Data Web. However, the complexity of ontology alignments, particularly rich correspondences (c : c), makes this process challenging. Existing approaches primarily focus on simple (s : s) and partially complex ( s : c) alignments, thereby overlooking the challenges posed by more expressive alignments. Moreover, the intricate syntax of SPARQL presents a barrier for non-expert users seeking to fully exploit the knowledge encapsulated in ontologies. This article proposes an innovative approach for the automatic rewriting of SPARQL queries from a source ontology to a target ontology, based on a user's need expressed in natural language. It leverages the principles of equivalence transitivity as well as the advanced capabilities of large language models such as GPT-4. By integrating these elements, this approach stands out for its ability to efficiently handle complex alignments, particularly (c : c) correspondences , by fully exploiting their expressiveness. Additionally, it facilitates access to aligned ontologies for users unfamiliar with SPARQL, providing a flexible solution for querying heterogeneous data.","authors":["Anicet Lepetit Ondo","Laurence Capus","Mamadou Bousso"],"url":"https://arxiv.org/abs/2505.01309"}
{"created":"2025-05-05","title":"A Factorized Probabilistic Model of the Semantics of Vague Temporal Adverbials Relative to Different Event Types","abstract":"Vague temporal adverbials, such as recently, just, and a long time ago, describe the temporal distance between a past event and the utterance time but leave the exact duration underspecified. In this paper, we introduce a factorized model that captures the semantics of these adverbials as probabilistic distributions. These distributions are composed with event-specific distributions to yield a contextualized meaning for an adverbial applied to a specific event. We fit the model's parameters using existing data capturing judgments of native speakers regarding the applicability of these vague temporal adverbials to events that took place a given time ago. Comparing our approach to a non-factorized model based on a single Gaussian distribution for each pair of event and temporal adverbial, we find that while both models have similar predictive power, our model is preferable in terms of Occam's razor, as it is simpler and has better extendability.","authors":["Svenja Kenneweg","J\\\"org Deigm\\\"oller","Julian Eggert","Philipp Cimiano"],"url":"https://arxiv.org/abs/2505.01311"}
{"created":"2025-05-05","title":"A Neural Architecture Search Method using Auxiliary Evaluation Metric based on ResNet Architecture","abstract":"This paper proposes a neural architecture search space using ResNet as a framework, with search objectives including parameters for convolution, pooling, fully connected layers, and connectivity of the residual network. In addition to recognition accuracy, this paper uses the loss value on the validation set as a secondary objective for optimization. The experimental results demonstrate that the search space of this paper together with the optimisation approach can find competitive network architectures on the MNIST, Fashion-MNIST and CIFAR100 datasets.","authors":["Shang Wang","Huanrong Tang","Jianquan Ouyang"],"url":"https://arxiv.org/abs/2505.01313"}
{"created":"2025-05-05","title":"A Transformer-based Neural Architecture Search Method","abstract":"This paper presents a neural architecture search method based on Transformer architecture, searching cross multihead attention computation ways for different number of encoder and decoder combinations. In order to search for neural network structures with better translation results, we considered perplexity as an auxiliary evaluation metric for the algorithm in addition to BLEU scores and iteratively improved each individual neural network within the population by a multi-objective genetic algorithm. Experimental results show that the neural network structures searched by the algorithm outperform all the baseline models, and that the introduction of the auxiliary evaluation metric can find better models than considering only the BLEU score as an evaluation metric.","authors":["Shang Wang","Huanrong Tang","Jianquan Ouyang"],"url":"https://arxiv.org/abs/2505.01314"}
{"created":"2025-05-05","title":"Helping Big Language Models Protect Themselves: An Enhanced Filtering and Summarization System","abstract":"The recent growth in the use of Large Language Models has made them vulnerable to sophisticated adversarial assaults, manipulative prompts, and encoded malicious inputs. Existing countermeasures frequently necessitate retraining models, which is computationally costly and impracticable for deployment. Without the need for retraining or fine-tuning, this study presents a unique defense paradigm that allows LLMs to recognize, filter, and defend against adversarial or malicious inputs on their own. There are two main parts to the suggested framework: (1) A prompt filtering module that uses sophisticated Natural Language Processing (NLP) techniques, including zero-shot classification, keyword analysis, and encoded content detection (e.g. base64, hexadecimal, URL encoding), to detect, decode, and classify harmful inputs; and (2) A summarization module that processes and summarizes adversarial research literature to give the LLM context-aware defense knowledge. This approach strengthens LLMs' resistance to adversarial exploitation by fusing text extraction, summarization, and harmful prompt analysis. According to experimental results, this integrated technique has a 98.71% success rate in identifying harmful patterns, manipulative language structures, and encoded prompts. By employing a modest amount of adversarial research literature as context, the methodology also allows the model to react correctly to harmful inputs with a larger percentage of jailbreak resistance and refusal rate. While maintaining the quality of LLM responses, the framework dramatically increases LLM's resistance to hostile misuse, demonstrating its efficacy as a quick and easy substitute for time-consuming, retraining-based defenses.","authors":["Sheikh Samit Muhaimin","Spyridon Mastorakis"],"url":"https://arxiv.org/abs/2505.01315"}
{"created":"2025-05-05","title":"Model See Model Do: Speech-Driven Facial Animation with Style Control","abstract":"Speech-driven 3D facial animation plays a key role in applications such as virtual avatars, gaming, and digital content creation. While existing methods have made significant progress in achieving accurate lip synchronization and generating basic emotional expressions, they often struggle to capture and effectively transfer nuanced performance styles. We propose a novel example-based generation framework that conditions a latent diffusion model on a reference style clip to produce highly expressive and temporally coherent facial animations. To address the challenge of accurately adhering to the style reference, we introduce a novel conditioning mechanism called style basis, which extracts key poses from the reference and additively guides the diffusion generation process to fit the style without compromising lip synchronization quality. This approach enables the model to capture subtle stylistic cues while ensuring that the generated animations align closely with the input speech. Extensive qualitative, quantitative, and perceptual evaluations demonstrate the effectiveness of our method in faithfully reproducing the desired style while achieving superior lip synchronization across various speech scenarios.","authors":["Yifang Pan","Karan Singh","Luiz Gustavo Hafemann"],"url":"https://arxiv.org/abs/2505.01319"}
{"created":"2025-05-05","title":"ABCO: Adaptive Bacterial Colony Optimisation","abstract":"This paper introduces a new optimisation algorithm, called Adaptive Bacterial Colony Optimisation (ABCO), modelled after the foraging behaviour of E. coli bacteria. The algorithm follows three stages--explore, exploit and reproduce--and is adaptable to meet the requirements of its applications. The performance of the proposed ABCO algorithm is compared to that of established optimisation algorithms--particle swarm optimisation (PSO) and ant colony optimisation (ACO)--on a set of benchmark functions. Experimental results demonstrate the benefits of the adaptive nature of the proposed algorithm: ABCO runs much faster than PSO and ACO while producing competitive results and outperforms PSO and ACO in a scenario where the running time is not crucial.","authors":["Barisi Kogam","Yevgeniya Kovalchuk","Mohamed Medhat Gaber"],"url":"https://arxiv.org/abs/2505.01320"}
{"created":"2025-05-05","title":"FreeInsert: Disentangled Text-Guided Object Insertion in 3D Gaussian Scene without Spatial Priors","abstract":"Text-driven object insertion in 3D scenes is an emerging task that enables intuitive scene editing through natural language. However, existing 2D editing-based methods often rely on spatial priors such as 2D masks or 3D bounding boxes, and they struggle to ensure consistency of the inserted object. These limitations hinder flexibility and scalability in real-world applications. In this paper, we propose FreeInsert, a novel framework that leverages foundation models including MLLMs, LGMs, and diffusion models to disentangle object generation from spatial placement. This enables unsupervised and flexible object insertion in 3D scenes without spatial priors. FreeInsert starts with an MLLM-based parser that extracts structured semantics, including object types, spatial relationships, and attachment regions, from user instructions. These semantics guide both the reconstruction of the inserted object for 3D consistency and the learning of its degrees of freedom. We leverage the spatial reasoning capabilities of MLLMs to initialize object pose and scale. A hierarchical, spatially aware refinement stage further integrates spatial semantics and MLLM-inferred priors to enhance placement. Finally, the appearance of the object is improved using the inserted-object image to enhance visual fidelity. Experimental results demonstrate that FreeInsert achieves semantically coherent, spatially precise, and visually realistic 3D insertions without relying on spatial priors, offering a user-friendly and flexible editing experience.","authors":["Chenxi Li","Weijie Wang","Qiang Li","Bruno Lepri","Nicu Sebe","Weizhi Nie"],"url":"https://arxiv.org/abs/2505.01322"}
{"created":"2025-05-05","title":"Proven Approximation Guarantees in Multi-Objective Optimization: SPEA2 Beats NSGA-II","abstract":"Together with the NSGA-II and SMS-EMOA, the strength Pareto evolutionary algorithm 2 (SPEA2) is one of the most prominent dominance-based multi-objective evolutionary algorithms (MOEAs). Different from the NSGA-II, it does not employ the crowding distance (essentially the distance to neighboring solutions) to compare pairwise non-dominating solutions but a complex system of $\\sigma$-distances that builds on the distances to all other solutions. In this work, we give a first mathematical proof showing that this more complex system of distances can be superior. More specifically, we prove that a simple steady-state SPEA2 can compute optimal approximations of the Pareto front of the OneMinMax benchmark in polynomial time. The best proven guarantee for a comparable variant of the NSGA-II only assures approximation ratios of roughly a factor of two, and both mathematical analyses and experiments indicate that optimal approximations are not found efficiently.","authors":["Yasser Alghouass","Benjamin Doerr","Martin S. Krejca","Mohammed Lagmah"],"url":"https://arxiv.org/abs/2505.01323"}
{"created":"2025-05-05","title":"TRAVELER: A Benchmark for Evaluating Temporal Reasoning across Vague, Implicit and Explicit References","abstract":"Understanding and resolving temporal references is essential in Natural Language Understanding as we often refer to the past or future in daily communication. Although existing benchmarks address a system's ability to reason about and resolve temporal references, systematic evaluation of specific temporal references remains limited. Towards closing this gap, we introduce TRAVELER, a novel synthetic benchmark dataset that follows a Question Answering paradigm and consists of questions involving temporal references with the corresponding correct answers. TRAVELER assesses models' abilities to resolve explicit, implicit relative to speech time, and vague temporal references. Beyond investigating the performance of state-of-the-art LLMs depending on the type of temporal reference, our benchmark also allows evaluation of performance in relation to the length of the set of events. For the category of vague temporal references, ground-truth answers were established via human surveys on Prolific, following a procedure similar to the one from Kenneweg et al. To demonstrate the benchmark's applicability, we evaluate four state-of-the-art LLMs using a question-answering task encompassing 3,300 questions. Our findings show that while the benchmarked LLMs can answer questions over event sets with a handful of events and explicit temporal references successfully, performance clearly deteriorates with larger event set length and when temporal references get less explicit. Notably, the vague question category exhibits the lowest performance across all models.","authors":["Svenja Kenneweg","J\\\"org Deigm\\\"oller","Philipp Cimiano","Julian Eggert"],"url":"https://arxiv.org/abs/2505.01325"}
{"created":"2025-05-05","title":"Constrained Network Adversarial Attacks: Validity, Robustness, and Transferability","abstract":"While machine learning has significantly advanced Network Intrusion Detection Systems (NIDS), particularly within IoT environments where devices generate large volumes of data and are increasingly susceptible to cyber threats, these models remain vulnerable to adversarial attacks. Our research reveals a critical flaw in existing adversarial attack methodologies: the frequent violation of domain-specific constraints, such as numerical and categorical limits, inherent to IoT and network traffic. This leads to up to 80.3% of adversarial examples being invalid, significantly overstating real-world vulnerabilities. These invalid examples, though effective in fooling models, do not represent feasible attacks within practical IoT deployments. Consequently, relying on these results can mislead resource allocation for defense, inflating the perceived susceptibility of IoT-enabled NIDS models to adversarial manipulation. Furthermore, we demonstrate that simpler surrogate models like Multi-Layer Perceptron (MLP) generate more valid adversarial examples compared to complex architectures such as CNNs and LSTMs. Using the MLP as a surrogate, we analyze the transferability of adversarial severity to other ML/DL models commonly used in IoT contexts. This work underscores the importance of considering both domain constraints and model architecture when evaluating and designing robust ML/DL models for security-critical IoT and network applications.","authors":["Anass Grini","Oumaima Taheri","Btissam El Khamlichi","Amal El Fallah-Seghrouchni"],"url":"https://arxiv.org/abs/2505.01328"}
{"created":"2025-05-05","title":"Power System Transition Planning: An Industry-Aligned Framework for Long-Term Optimization","abstract":"This work introduces the category of Power System Transition Planning optimization problem. It aims to shift power systems to emissions-free networks efficiently. Unlike comparable work, the framework presented here broadly applies to the industry's decision-making process. It defines a field-appropriate functional boundary focused on the economic efficiency of power systems. Namely, while imposing a wide range of planning factors in the decision space, the model maintains the structure and depth of conventional power system planning under uncertainty, which leads to a large-scale multistage stochastic programming formulation that encounters intractability in real-life cases. Thus, the framework simultaneously invokes high-performance computing defaultism. In this comprehensive exposition, we present a guideline model, comparing its scope to existing formulations, supported by a fully detailed example problem, showcasing the analytical value of the solution gained in a small test case. Then, the framework's viability for realistic applications is demonstrated by solving an extensive test case based on a realistic planning construct consistent with Alberta's power system practices for long-term planning studies. The framework resorts to Stochastic Dual Dynamic Programming as a decomposition method to achieve tractability, leveraging High-Performance Computing and parallel computation.","authors":["Ahmed Al-Shafei","Nima Amjady","Hamidreza Zareipour","Yankai Cao"],"url":"https://arxiv.org/abs/2505.01331"}
{"created":"2025-05-05","title":"Integration of Multi-Mode Preference into Home Energy Management System Using Deep Reinforcement Learning","abstract":"Home Energy Management Systems (HEMS) have emerged as a pivotal tool in the smart home ecosystem, aiming to enhance energy efficiency, reduce costs, and improve user comfort. By enabling intelligent control and optimization of household energy consumption, HEMS plays a significant role in bridging the gap between consumer needs and energy utility objectives. However, much of the existing literature construes consumer comfort as a mere deviation from the standard appliance settings. Such deviations are typically incorporated into optimization objectives via static weighting factors. These factors often overlook the dynamic nature of consumer behaviors and preferences. Addressing this oversight, our paper introduces a multi-mode Deep Reinforcement Learning-based HEMS (DRL-HEMS) framework, meticulously designed to optimize based on dynamic, consumer-defined preferences. Our primary goal is to augment consumer involvement in Demand Response (DR) programs by embedding dynamic multi-mode preferences tailored to individual appliances. In this study, we leverage a model-free, single-agent DRL algorithm to deliver a HEMS framework that is not only dynamic but also user-friendly. To validate its efficacy, we employed real-world data at 15-minute intervals, including metrics such as electricity price, ambient temperature, and appliances' power consumption. Our results show that the model performs exceptionally well in optimizing energy consumption within different preference modes. Furthermore, when compared to traditional algorithms based on Mixed-Integer Linear Programming (MILP), our model achieves nearly optimal performance while outperforming in computational efficiency.","authors":["Mohammed Sumayli","Olugbenga Moses Anubi"],"url":"https://arxiv.org/abs/2505.01332"}
{"created":"2025-05-05","title":"Enhancing Diversity in Parallel Agents: A Maximum State Entropy Exploration Story","abstract":"Parallel data collection has redefined Reinforcement Learning (RL), unlocking unprecedented efficiency and powering breakthroughs in large-scale real-world applications. In this paradigm, $N$ identical agents operate in $N$ replicas of an environment simulator, accelerating data collection by a factor of $N$. A critical question arises: \\textit{Does specializing the policies of the parallel agents hold the key to surpass the $N$ factor acceleration?} In this paper, we introduce a novel learning framework that maximizes the entropy of collected data in a parallel setting. Our approach carefully balances the entropy of individual agents with inter-agent diversity, effectively minimizing redundancies. The latter idea is implemented with a centralized policy gradient method, which shows promise when evaluated empirically against systems of identical agents, as well as synergy with batch RL techniques that can exploit data diversity. Finally, we provide an original concentration analysis that shows faster rates for specialized parallel sampling distributions, which supports our methodology and may be of independent interest.","authors":["Vincenzo De Paola","Riccardo Zamboni","Mirco Mutti","Marcello Restelli"],"url":"https://arxiv.org/abs/2505.01336"}
{"created":"2025-05-05","title":"Toward Teach and Repeat Across Seasonal Deep Snow Accumulation","abstract":"Teach and repeat is a rapid way to achieve autonomy in challenging terrain and off-road environments. A human operator pilots the vehicles to create a network of paths that are mapped and associated with odometry. Immediately after teaching, the system can drive autonomously within its tracks. This precision lets operators remain confident that the robot will follow a traversable route. However, this operational paradigm has rarely been explored in off-road environments that change significantly through seasonal variation. This paper presents preliminary field trials using lidar and radar implementations of teach and repeat. Using a subset of the data from the upcoming FoMo dataset, we attempted to repeat routes that were 4 days, 44 days, and 113 days old. Lidar teach and repeat demonstrated a stronger ability to localize when the ground points were removed. FMCW radar was often able to localize on older maps, but only with small deviations from the taught path. Additionally, we highlight specific cases where radar localization failed with recent maps due to the high pitch or roll of the vehicle. We highlight lessons learned during the field deployment and highlight areas to improve to achieve reliable teach and repeat with seasonal changes in the environment. Please follow the dataset at https://norlab-ulaval.github.io/FoMo-website for updates and information on the data release.","authors":["Mat\\v{e}j Boxan","Alexander Krawciw","Timothy D. Barfoot","Fran\\c{c}ois Pomerleau"],"url":"https://arxiv.org/abs/2505.01339"}
{"created":"2025-05-05","title":"Semantic Communication: From Philosophical Conceptions Towards a Mathematical Framework","abstract":"Semantic communication has emerged as a promising paradigm to address the challenges of next-generation communication networks. While some progress has been made in its conceptualization, fundamental questions remain unresolved. In this paper, we propose a probabilistic model for semantic communication that, unlike prior works primarily rooted in intuitions from human language, is grounded in a rigorous philosophical conception of information and its relationship with data as Constraining Affordances, mediated by Levels of Abstraction (LoA). This foundation not only enables the modeling of linguistic semantic communication but also provides a domain-independent definition of semantic content, extending its applicability beyond linguistic contexts. As the semantic communication problem involves a complex interplay of various factors, making it difficult to tackle in its entirety, we propose to orthogonalize it by classifying it into simpler sub-problems and approach the general problem step by step. Notably, we show that Shannon's framework constitutes a special case of semantic communication, in which each message conveys a single, unambiguous meaning. Consequently, the capacity in Shannon's model-defined as the maximum rate of reliably transmissible messages-coincides with the semantic capacity under this constrained scenario. In this paper, we specifically focus on the sub-problem where semantic ambiguity arises solely from physical channel noise and derive a lower bound for its semantic capacity, which reduces to Shannon's capacity in the corresponding special case. We also demonstrate that the achievable rate of all transmissible messages for reliable semantic communication, exceeds Shannon's capacity by the added term H(X|S).","authors":["Javad Gholipour","Rafael F. Schaefer","Gerhard P. Fettweis"],"url":"https://arxiv.org/abs/2505.01342"}
{"created":"2025-05-05","title":"BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing","abstract":"Large multi-modal models inevitably decay over time as facts change and previously learned information becomes outdated. Traditional approaches such as fine-tuning are often impractical for updating these models due to their size and complexity. Instead, direct knowledge editing within the models presents a more viable solution. Current model editing techniques, however, typically overlook the unique influence ranges of different facts, leading to compromised model performance in terms of both generality and locality. To address this issue, we introduce the concept of the generality-locality trade-off in multi-modal model editing. We develop a new model editing dataset named OKEDIT, specifically designed to effectively evaluate this trade-off. Building on this foundation, we propose BalancEdit, a novel method for balanced model editing that dynamically achieves an optimal balance between generality and locality. BalancEdit utilizes a unique mechanism that generates both positive and negative samples for each fact to accurately determine its influence scope and incorporates these insights into the model's latent space using a discrete, localized codebook of edits, without modifying the underlying model weights. To our knowledge, this is the first approach explicitly addressing the generality-locality trade-off in multi-modal model editing. Our comprehensive results confirm the effectiveness of BalancEdit, demonstrating minimal trade-offs while maintaining robust editing capabilities. Our code and dataset will be available.","authors":["Dongliang Guo","Mengxuan Hu","Zihan Guan","Thomas Hartvigsen","Sheng Li"],"url":"https://arxiv.org/abs/2505.01343"}
{"created":"2025-05-05","title":"How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets","abstract":"We consider binary classification restricted to a class of continuous piecewise linear functions whose decision boundaries are (possibly nonconvex) starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We investigate the expressivity of these function classes and describe the combinatorial and geometric structure of the loss landscape, most prominently the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and an exponential loss function. In particular, we give explicit bounds on the VC dimension of this model, and concretely describe the sublevel sets of the discrete loss as chambers in a hyperplane arrangement. For the exponential loss, we give sufficient conditions for the optimum to be unique, and describe the geometry of the optimum when varying the rate parameter of the underlying exponential probability distribution.","authors":["Marie-Charlotte Brandenburg","Katharina Jochemko"],"url":"https://arxiv.org/abs/2505.01346"}
{"created":"2025-05-05","title":"Learning Stabilizing Policies via an Unstable Subspace Representation","abstract":"We study the problem of learning to stabilize (LTS) a linear time-invariant (LTI) system. Policy gradient (PG) methods for control assume access to an initial stabilizing policy. However, designing such a policy for an unknown system is one of the most fundamental problems in control, and it may be as hard as learning the optimal policy itself. Existing work on the LTS problem requires large data as it scales quadratically with the ambient dimension. We propose a two-phase approach that first learns the left unstable subspace of the system and then solves a series of discounted linear quadratic regulator (LQR) problems on the learned unstable subspace, targeting to stabilize only the system's unstable dynamics and reduce the effective dimension of the control space. We provide non-asymptotic guarantees for both phases and demonstrate that operating on the unstable subspace reduces sample complexity. In particular, when the number of unstable modes is much smaller than the state dimension, our analysis reveals that LTS on the unstable subspace substantially speeds up the stabilization process. Numerical experiments are provided to support this sample complexity reduction achieved by our approach.","authors":["Leonardo F. Toso","Lintao Ye","James Anderson"],"url":"https://arxiv.org/abs/2505.01348"}
{"created":"2025-05-05","title":"Closing the Loop: A Systematic Review of Experience-Driven Game Adaptation","abstract":"Adaptive game systems aim to enrich player experiences by dynamically adjusting game content in response to user data. While extensive research has addressed content personalization and player experience modeling, the integration of these components into fully operational adaptive gameplay systems remains limited. This systematic review, conducted in accordance with PRISMA guidelines, analyzes 17 empirical studies published between January 2015 and May 2024, identifying and analyzing approaches that implement the complete experience-driven loop -- including player sensing, modeling, and content adaptation. Game telemetry remains the most prevalent sensing modality, although other non-invasive methods suitable for affective modeling -- such as facial expression analysis (FEA) and peripheral interaction data -- remain underutilized despite their potential for real-time emotional inference. Knowledge-based methods, such as rule-based systems and heuristics, dominate modeling and adaptation due to their interpretability and low resource demands, whereas machine learning approaches face challenges related to data availability and transparency. Despite their relevance to immersive and therapeutic experiences, affective states such as stress and anxiety remain largely ignored, as systems continue to favor performance over emotion-sensitive adaptation. These findings highlight a crucial research direction: advancing emotionally responsive game systems that move beyond performance optimization by incorporating underutilized sensing modalities -- such as FEA and peripheral interaction -- to enable real-time affect-driven personalization. Advancing in this direction holds strong potential to increase immersion, personalize gameplay, and support affect regulation across entertainment and therapeutic contexts.","authors":["Phil Lopes","Nuno Fachada","Maria Fonseca"],"url":"https://arxiv.org/abs/2505.01351"}
{"created":"2025-05-05","title":"Emerging Media Use and Acceptance of Digital Immortality: A Cluster Analysis among Chinese Young Generations","abstract":"The rapid technological advancements made the concept of digital immortality less fantastical and more plausible, sparking academic and industrial interest. Existing literature mainly discusses philosophical and societal aspects, lacking specific empirical observation. To address this gap, we conducted a study among Chinese youth to gauge their acceptance of digital immortality. Using cluster analysis, we classified participants into three groups: \"geeks,\" \"video game players,\" and \"laggards\" based on their media usage. Those most receptive to digital immortality, termed \"geeks\" tend to be male, with higher income levels, openness, conscientiousness, extensive engagement with emerging media technology, and surprisingly, more adhering to Buddhism and Daoism. Overall, this study examined media usage patterns and youth perspectives on digital immortality, shedding light on technology's role in shaping views on life and death. It highlights the importance of further research on the profound implications of digital immortality in the context of contemporary society.","authors":["Yi Mou","Jianfeng Lan","Jingyao Lu","Jilong Wang"],"url":"https://arxiv.org/abs/2505.01355"}
{"created":"2025-05-05","title":"Stabilizing Temporal Difference Learning via Implicit Stochastic Approximation","abstract":"Temporal Difference (TD) learning is a foundational algorithm in reinforcement learning (RL). For nearly forty years, TD learning has served as a workhorse for applied RL as well as a building block for more complex and specialized algorithms. However, despite its widespread use, it is not without drawbacks, the most prominent being its sensitivity to step size. A poor choice of step size can dramatically inflate the error of value estimates and slow convergence. Consequently, in practice, researchers must use trial and error in order to identify a suitable step size -- a process that can be tedious and time consuming. As an alternative, we propose implicit TD algorithms that reformulate TD updates into fixed-point equations. These updates are more stable and less sensitive to step size without sacrificing computational efficiency. Moreover, our theoretical analysis establishes asymptotic convergence guarantees and finite-time error bounds. Our results demonstrate their robustness and practicality for modern RL tasks, establishing implicit TD as a versatile tool for policy evaluation and value approximation.","authors":["Hwanwoo Kim","Panos Toulis","Eric Laber"],"url":"https://arxiv.org/abs/2505.01361"}
{"created":"2025-05-05","title":"Monitoring morphometric drift in lifelong learning segmentation of the spinal cord","abstract":"Morphometric measures derived from spinal cord segmentations can serve as diagnostic and prognostic biomarkers in neurological diseases and injuries affecting the spinal cord. While robust, automatic segmentation methods to a wide variety of contrasts and pathologies have been developed over the past few years, whether their predictions are stable as the model is updated using new datasets has not been assessed. This is particularly important for deriving normative values from healthy participants. In this study, we present a spinal cord segmentation model trained on a multisite $(n=75)$ dataset, including 9 different MRI contrasts and several spinal cord pathologies. We also introduce a lifelong learning framework to automatically monitor the morphometric drift as the model is updated using additional datasets. The framework is triggered by an automatic GitHub Actions workflow every time a new model is created, recording the morphometric values derived from the model's predictions over time. As a real-world application of the proposed framework, we employed the spinal cord segmentation model to update a recently-introduced normative database of healthy participants containing commonly used measures of spinal cord morphometry. Results showed that: (i) our model outperforms previous versions and pathology-specific models on challenging lumbar spinal cord cases, achieving an average Dice score of $0.95 \\pm 0.03$; (ii) the automatic workflow for monitoring morphometric drift provides a quick feedback loop for developing future segmentation models; and (iii) the scaling factor required to update the database of morphometric measures is nearly constant among slices across the given vertebral levels, showing minimum drift between the current and previous versions of the model monitored by the framework. The model is freely available in Spinal Cord Toolbox v7.0.","authors":["Enamundram Naga Karthik","Sandrine B\\'edard","Jan Valo\\v{s}ek","Christoph S. Aigner","Elise Bannier","Josef Bedna\\v{r}\\'ik","Virginie Callot","Anna Combes","Armin Curt","Gergely David","Falk Eippert","Lynn Farner","Michael G Fehlings","Patrick Freund","Tobias Granberg","Cristina Granziera","RHSCIR Network Imaging Group","Ulrike Horn","Tom\\'a\\v{s} Hor\\'ak","Suzanne Humphreys","Markus Hupp","Anne Kerbrat","Nawal Kinany","Shannon Kolind","Petr Kudli\\v{c}ka","Anna Lebret","Lisa Eunyoung Lee","Caterina Mainero","Allan R. Martin","Megan McGrath","Govind Nair","Kristin P. O'Grady","Jiwon Oh","Russell Ouellette","Nikolai Pfender","Dario Pfyffer","Pierre-Fran\\c{c}ois Pradat","Alexandre Prat","Emanuele Pravat\\`a","Daniel S. Reich","Ilaria Ricchi","Naama Rotem-Kohavi","Simon Schading-Sassenhausen","Maryam Seif","Andrew Smith","Seth A Smith","Grace Sweeney","Roger Tam","Anthony Traboulsee","Constantina Andrada Treaba","Charidimos Tsagkas","Zachary Vavasour","Dimitri Van De Ville","Kenneth Arnold Weber II","Sarath Chandar","Julien Cohen-Adad"],"url":"https://arxiv.org/abs/2505.01364"}
{"created":"2025-05-05","title":"Deep Learning-Enabled System Diagnosis in Microgrids: A Feature-Feedback GAN Approach","abstract":"The increasing integration of inverter-based resources (IBRs) and communication networks has brought both modernization and new vulnerabilities to the power system infrastructure. These vulnerabilities expose the system to internal faults and cyber threats, particularly False Data Injection (FDI) attacks, which can closely mimic real fault scenarios. Hence, this work presents a two-stage fault and cyberattack detection framework tailored for inverter-based microgrids. Stage 1 introduces an unsupervised learning model Feature Feedback Generative Adversarial Network (F2GAN), to distinguish between genuine internal faults and cyber-induced anomalies in microgrids. Compared to conventional GAN architectures, F2GAN demonstrates improved system diagnosis and greater adaptability to zero-day attacks through its feature-feedback mechanism. In Stage 2, supervised machine learning techniques, including Support Vector Machines (SVM), k-Nearest Neighbors (KNN), Decision Trees (DT), and Artificial Neural Networks (ANN) are applied to localize and classify faults within inverter switches, distinguishing between single-switch and multi-switch faults. The proposed framework is validated on a simulated microgrid environment, illustrating robust performance in detecting and classifying both physical and cyber-related disturbances in power electronic-dominated systems.","authors":["Swetha Rani Kasimalla","Kuchan Park","Junho Hong","Young-Jin Kim","HyoJong Lee"],"url":"https://arxiv.org/abs/2505.01366"}
{"created":"2025-05-05","title":"Binamix -- A Python Library for Generating Binaural Audio Datasets","abstract":"The increasing demand for spatial audio in applications such as virtual reality, immersive media, and spatial audio research necessitates robust solutions to generate binaural audio data sets for use in testing and validation. Binamix is an open-source Python library designed to facilitate programmatic binaural mixing using the extensive SADIE II Database, which provides Head Related Impulse Response (HRIR) and Binaural Room Impulse Response (BRIR) data for 20 subjects. The Binamix library provides a flexible and repeatable framework for creating large-scale spatial audio datasets, making it an invaluable resource for codec evaluation, audio quality metric development, and machine learning model training. A range of pre-built example scripts, utility functions, and visualization plots further streamline the process of custom pipeline creation. This paper presents an overview of the library's capabilities, including binaural rendering, impulse response interpolation, and multi-track mixing for various speaker layouts. The tools utilize a modified Delaunay triangulation technique to achieve accurate HRIR/BRIR interpolation where desired angles are not present in the data. By supporting a wide range of parameters such as azimuth, elevation, subject Impulse Responses (IRs), speaker layouts, mixing controls, and more, the library enables researchers to create large binaural datasets for any downstream purpose. Binamix empowers researchers and developers to advance spatial audio applications with reproducible methodologies by offering an open-source solution for binaural rendering and dataset generation. We release the library under the Apache 2.0 License at https://github.com/QxLabIreland/Binamix/","authors":["Dan Barry","Davoud Shariat Panah","Alessandro Ragano","Jan Skoglund","Andrew Hines"],"url":"https://arxiv.org/abs/2505.01369"}
{"created":"2025-05-05","title":"SimICD: A Closed-Loop Simulation Framework For ICD Therapy","abstract":"Virtual studies of ICD behaviour are crucial for testing device functionality in a controlled environment prior to clinical application. Although previous works have shown the viability of using in silico testing for diagnosis, there is a notable gap in available models that can simulate therapy progression decisions during arrhythmic episodes. This work introduces SimICD, a simulation tool which combines virtual ICD logic algorithms with cardiac electrophysiology simulations in a feedback loop, allowing the progression of ICD therapy protocols to be simulated for a range of tachy-arrhythmia episodes. Using a cohort of virtual patients, we demonstrate the ability of SimICD to simulate realistic cardiac signals and ICD responses that align with the logic of real-world devices, facilitating the reprogramming of ICD parameters to adapt to specific episodes.","authors":["Hannah Lydon","Milad Kazemi","Martin Bishop","Nicola Paoletti"],"url":"https://arxiv.org/abs/2505.01371"}
{"created":"2025-05-05","title":"Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii","abstract":"Mechanistic Interpretability (MI) aims to understand neural networks through causal explanations. Though MI has many explanation-generating methods, progress has been limited by the lack of a universal approach to evaluating explanations. Here we analyse the fundamental question \"What makes a good explanation?\" We introduce a pluralist Explanatory Virtues Framework drawing on four perspectives from the Philosophy of Science - the Bayesian, Kuhnian, Deutschian, and Nomological - to systematically evaluate and improve explanations in MI. We find that Compact Proofs consider many explanatory virtues and are hence a promising approach. Fruitful research directions implied by our framework include (1) clearly defining explanatory simplicity, (2) focusing on unifying explanations and (3) deriving universal principles for neural networks. Improved MI methods enhance our ability to monitor, predict, and steer AI systems.","authors":["Kola Ayonrinde","Louis Jaburi"],"url":"https://arxiv.org/abs/2505.01372"}
{"created":"2025-05-05","title":"An Efficient Real-Time Planning Method for Swarm Robotics Based on an Optimal Virtual Tube","abstract":"Swarm robotics navigating through unknown obstacle environments is an emerging research area that faces challenges. Performing tasks in such environments requires swarms to achieve autonomous localization, perception, decision-making, control, and planning. The limited computational resources of onboard platforms present significant challenges for planning and control. Reactive planners offer low computational demands and high re-planning frequencies but lack predictive capabilities, often resulting in local minima. Long-horizon planners, on the other hand, can perform multi-step predictions to reduce deadlocks but cost much computation, leading to lower re-planning frequencies. This paper proposes a real-time optimal virtual tube planning method for swarm robotics in unknown environments, which generates approximate solutions for optimal trajectories through affine functions. As a result, the computational complexity of approximate solutions is $O(n_t)$, where $n_t$ is the number of parameters in the trajectory, thereby significantly reducing the overall computational burden. By integrating reactive methods, the proposed method enables low-computation, safe swarm motion in unknown environments. The effectiveness of the proposed method is validated through several simulations and experiments.","authors":["Pengda Mao","Shuli Lv","Chen Min","Zhaolong Shen","Quan Quan"],"url":"https://arxiv.org/abs/2505.01380"}
{"created":"2025-05-05","title":"FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research","abstract":"We present FalconWing -- an open-source, ultra-lightweight (150 g) fixed-wing platform for autonomy research. The hardware platform integrates a small camera, a standard airframe, offboard computation, and radio communication for manual overrides. We demonstrate FalconWing's capabilities by developing and deploying a purely vision-based control policy for autonomous landing (without IMU or motion capture) using a novel real-to-sim-to-real learning approach. Our learning approach: (1) constructs a photorealistic simulation environment via 3D Gaussian splatting trained on real-world images; (2) identifies nonlinear dynamics from vision-estimated real-flight data; and (3) trains a multi-modal Vision Transformer (ViT) policy through simulation-only imitation learning. The ViT architecture fuses single RGB image with the history of control actions via self-attention, preserving temporal context while maintaining real-time 20 Hz inference. When deployed zero-shot on the hardware platform, this policy achieves an 80% success rate in vision-based autonomous landings. Together with the hardware specifications, we also open-source the system dynamics, the software for photorealistic simulator and the learning approach.","authors":["Yan Miao","Will Shen","Hang Cui","Sayan Mitra"],"url":"https://arxiv.org/abs/2505.01383"}
{"created":"2025-05-05","title":"Global Collinearity-aware Polygonizer for Polygonal Building Mapping in Remote Sensing","abstract":"This paper addresses the challenge of mapping polygonal buildings from remote sensing images and introduces a novel algorithm, the Global Collinearity-aware Polygonizer (GCP). GCP, built upon an instance segmentation framework, processes binary masks produced by any instance segmentation model. The algorithm begins by collecting polylines sampled along the contours of the binary masks. These polylines undergo a refinement process using a transformer-based regression module to ensure they accurately fit the contours of the targeted building instances. Subsequently, a collinearity-aware polygon simplification module simplifies these refined polylines and generate the final polygon representation. This module employs dynamic programming technique to optimize an objective function that balances the simplicity and fidelity of the polygons, achieving globally optimal solutions. Furthermore, the optimized collinearity-aware objective is seamlessly integrated into network training, enhancing the cohesiveness of the entire pipeline. The effectiveness of GCP has been validated on two public benchmarks for polygonal building mapping. Further experiments reveal that applying the collinearity-aware polygon simplification module to arbitrary polylines, without prior knowledge, enhances accuracy over traditional methods such as the Douglas-Peucker algorithm. This finding underscores the broad applicability of GCP. The code for the proposed method will be made available at https://github.com/zhu-xlab.","authors":["Fahong Zhang","Yilei Shi","Xiao Xiang Zhu"],"url":"https://arxiv.org/abs/2505.01385"}
{"created":"2025-05-05","title":"Carbon Aware Transformers Through Joint Model-Hardware Optimization","abstract":"The rapid growth of machine learning (ML) systems necessitates a more comprehensive evaluation of their environmental impact, particularly their carbon footprint, which comprises operational carbon from training and inference execution and embodied carbon from hardware manufacturing and its entire life-cycle. Despite the increasing importance of embodied emissions, there is a lack of tools and frameworks to holistically quantify and optimize the total carbon footprint of ML systems. To address this, we propose CATransformers, a carbon-aware architecture search framework that enables sustainability-driven co-optimization of ML models and hardware architectures. By incorporating both operational and embodied carbon metrics into early design space exploration of domain-specific hardware accelerators, CATransformers demonstrates that optimizing for carbon yields design choices distinct from those optimized solely for latency or energy efficiency. We apply our framework to multi-modal CLIP-based models, producing CarbonCLIP, a family of CLIP models achieving up to 17% reduction in total carbon emissions while maintaining accuracy and latency compared to state-of-the-art edge small CLIP baselines. This work underscores the need for holistic optimization methods to design high-performance, environmentally sustainable AI systems.","authors":["Irene Wang","Newsha Ardalani","Mostafa Elhoushi","Daniel Jiang","Samuel Hsia","Ekin Sumbul","Divya Mahajan","Carole-Jean Wu","Bilge Acun"],"url":"https://arxiv.org/abs/2505.01386"}
{"created":"2025-05-05","title":"Multimodal Doctor-in-the-Loop: A Clinically-Guided Explainable Framework for Predicting Pathological Response in Non-Small Cell Lung Cancer","abstract":"This study proposes a novel approach combining Multimodal Deep Learning with intrinsic eXplainable Artificial Intelligence techniques to predict pathological response in non-small cell lung cancer patients undergoing neoadjuvant therapy. Due to the limitations of existing radiomics and unimodal deep learning approaches, we introduce an intermediate fusion strategy that integrates imaging and clinical data, enabling efficient interaction between data modalities. The proposed Multimodal Doctor-in-the-Loop method further enhances clinical relevance by embedding clinicians' domain knowledge directly into the training process, guiding the model's focus gradually from broader lung regions to specific lesions. Results demonstrate improved predictive accuracy and explainability, providing insights into optimal data integration strategies for clinical applications.","authors":["Alice Natalina Caragliano","Claudia Tacconi","Carlo Greco","Lorenzo Nibid","Edy Ippolito","Michele Fiore","Giuseppe Perrone","Sara Ramella","Paolo Soda","Valerio Guarrasi"],"url":"https://arxiv.org/abs/2505.01390"}
{"created":"2025-05-05","title":"Learning and Transferring Physical Models through Derivatives","abstract":"We propose Derivative Learning (DERL), a supervised approach that models physical systems by learning their partial derivatives. We also leverage DERL to build physical models incrementally, by designing a distillation protocol that effectively transfers knowledge from a pre-trained to a student model. We provide theoretical guarantees that our approach can learn the true physical system, being consistent with the underlying physical laws, even when using empirical derivatives. DERL outperforms state-of-the-art methods in generalizing an ODE to unseen initial conditions and a parametric PDE to unseen parameters. We finally propose a method based on DERL to transfer physical knowledge across models by extending them to new portions of the physical domain and new range of PDE parameters. We believe this is the first attempt at building physical models incrementally in multiple stages.","authors":["Alessandro Trenta","Andrea Cossu","Davide Bacciu"],"url":"https://arxiv.org/abs/2505.01391"}
{"created":"2025-05-05","title":"The Proportional Veto Principle for Approval Ballots","abstract":"The proportional veto principle, which captures the idea that a candidate vetoed by a large group of voters should not be chosen, has been studied for ranked ballots in single-winner voting. We introduce a version of this principle for approval ballots, which we call flexible-voter representation (FVR). We show that while the approval voting rule and other natural scoring rules provide the optimal FVR guarantee only for some flexibility threshold, there exists a scoring rule that is FVR-optimal for all thresholds simultaneously. We also extend our results to multi-winner voting.","authors":["Daniel Halpern","Ariel D. Procaccia","Warut Suksompong"],"url":"https://arxiv.org/abs/2505.01395"}
{"created":"2025-05-05","title":"SIME: Enhancing Policy Self-Improvement with Modal-level Exploration","abstract":"Self-improvement requires robotic systems to initially learn from human-provided data and then gradually enhance their capabilities through interaction with the environment. This is similar to how humans improve their skills through continuous practice. However, achieving effective self-improvement is challenging, primarily because robots tend to repeat their existing abilities during interactions, often failing to generate new, valuable data for learning. In this paper, we identify the key to successful self-improvement: modal-level exploration and data selection. By incorporating a modal-level exploration mechanism during policy execution, the robot can produce more diverse and multi-modal interactions. At the same time, we select the most valuable trials and high-quality segments from these interactions for learning. We successfully demonstrate effective robot self-improvement on both simulation benchmarks and real-world experiments. The capability for self-improvement will enable us to develop more robust and high-success-rate robotic control strategies at a lower cost. Our code and experiment scripts are available at https://ericjin2002.github.io/SIME/","authors":["Yang Jin","Jun Lv","Wenye Yu","Hongjie Fang","Yong-Lu Li","Cewu Lu"],"url":"https://arxiv.org/abs/2505.01396"}
{"created":"2025-05-05","title":"Dynamic Robot Tool Use with Vision Language Models","abstract":"Tool use enhances a robot's task capabilities. Recent advances in vision-language models (VLMs) have equipped robots with sophisticated cognitive capabilities for tool-use applications. However, existing methodologies focus on elementary quasi-static tool manipulations or high-level tool selection while neglecting the critical aspect of task-appropriate tool grasping. To address this limitation, we introduce inverse Tool-Use Planning (iTUP), a novel VLM-driven framework that enables grounded fine-grained planning for versatile robotic tool use. Through an integrated pipeline of VLM-based tool and contact point grounding, position-velocity trajectory planning, and physics-informed grasp generation and selection, iTUP demonstrates versatility across (1) quasi-static and more challenging (2) dynamic and (3) cluster tool-use tasks. To ensure robust planning, our framework integrates stable and safe task-aware grasping by reasoning over semantic affordances and physical constraints. We evaluate iTUP and baselines on a comprehensive range of realistic tool use tasks including precision hammering, object scooping, and cluster sweeping. Experimental results demonstrate that iTUP ensures a thorough grounding of cognition and planning for challenging robot tool use across diverse environments.","authors":["Noah Trupin","Zixing Wang","Ahmed H. Qureshi"],"url":"https://arxiv.org/abs/2505.01399"}
{"created":"2025-05-05","title":"Predicting the Price of Gold in the Financial Markets Using Hybrid Models","abstract":"Predicting the price that has the least error and can provide the best and highest accuracy has been one of the most challenging issues and one of the most critical concerns among capital market activists and researchers. Therefore, a model that can solve problems and provide results with high accuracy is one of the topics of interest among researchers. In this project, using time series prediction models such as ARIMA to estimate the price, variables, and indicators related to technical analysis show the behavior of traders involved in involving psychological factors for the model. By linking all of these variables to stepwise regression, we identify the best variables influencing the prediction of the variable. Finally, we enter the selected variables as inputs to the artificial neural network. In other words, we want to call this whole prediction process the \"ARIMA_Stepwise Regression_Neural Network\" model and try to predict the price of gold in international financial markets. This approach is expected to be able to be used to predict the types of stocks, commodities, currency pairs, financial market indicators, and other items used in local and international financial markets. Moreover, a comparison between the results of this method and time series methods is also expressed. Finally, based on the results, it can be seen that the resulting hybrid model has the highest accuracy compared to the time series method, regression, and stepwise regression.","authors":["Mohammadhossein Rashidi","Mohammad Modarres"],"url":"https://arxiv.org/abs/2505.01402"}
{"created":"2025-05-05","title":"VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models","abstract":"The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. Code: Code: \\url{https://github.com/SPIN-UMass/VidStamp}","authors":["Mohammadreza Teymoorianfard","Shiqing Ma","Amir Houmansadr"],"url":"https://arxiv.org/abs/2505.01406"}
{"created":"2025-05-05","title":"Towards Optimal Deterministic LOCAL Algorithms on Trees","abstract":"While obtaining optimal algorithms for the most important problems in the LOCAL model has been one of the central goals in the area of distributed algorithms since its infancy, tight complexity bounds are elusive for many problems even when considering \\emph{deterministic} complexities on \\emph{trees}. We take a step towards remedying this issue by providing a way to relate the complexity of a problem $\\Pi$ on trees to its truly local complexity, which is the (asymptotically) smallest function $f$ such that $\\Pi$ can be solved in $O(f(\\Delta)+\\log^*n)$ rounds. More specifically, we develop a transformation that takes an algorithm $\\mathcal A$ for $\\Pi$ with a runtime of $O(f(\\Delta)+\\log^*n)$ rounds as input and transforms it into an $O(f(g(n))+\\log^* n)$-round algorithm $\\mathcal{A}'$ on trees, where $g$ is the function that satisfies $g(n)^{f(g(n))}=n$. If $f$ is the truly local complexity of $\\Pi$ (i.e., if $\\mathcal{A}$ is asymptotically optimal), then $\\mathcal{A}'$ is an asymptotically optimal algorithm on trees, conditioned on a natural assumption on the nature of the worst-case instances of $\\Pi$. Our transformation works for any member of a wide class of problems, including the most important symmetry-breaking problems. As an example of our transformation we obtain the first strongly sublogarithmic algorithm for $(\\text{edge-degree+1})$-edge coloring (and therefore also $(2\\Delta-1)$-edge coloring) on trees, exhibiting a runtime of $O(\\log^{12/13} n)$ rounds. This breaks through the $\\Omega(\\log n/\\log\\log n)$-barrier that is a fundamental lower bound for other symmetry-breaking problems such as maximal independent set or maximal matching (that already holds on trees), and proves a separation between these problems and the aforementioned edge coloring problems on trees. We extend a subset of our results to graphs of bounded arboricity.","authors":["Sebastian Brandt","Ananth Narayanan"],"url":"https://arxiv.org/abs/2505.01410"}
{"created":"2025-05-05","title":"Group Gaze-Sharing with Projection Displays","abstract":"The eyes play an important role in human collaboration. Mutual and shared gaze help communicate visual attention to each other or to a specific object of interest. Shared gaze was typically investigated for pair collaborations in remote settings and with people in virtual and augmented reality. With our work, we expand this line of research by a new technique to communicate gaze between groups in tabletop workshop scenarios. To achieve this communication, we use an approach based on projection mapping to unify gaze data from multiple participants into a common visualization space on a tabletop. We showcase our approach with a collaborative puzzle-solving task that displays shared visual attention on individual pieces and provides hints to solve the problem at hand.","authors":["Maurice Koch","Tobias Rau","Vladimir Mikheev","Seyda \\\"Oney","Michael Becher","Xiangyu Wang","Nelusa Pathmanathan","Patrick Gralka","Daniel Weiskopf","Kuno Kurzhals"],"url":"https://arxiv.org/abs/2505.01413"}
{"created":"2025-05-05","title":"How Effective are Large Time Series Models in Hydrology? A Study on Water Level Forecasting in Everglades","abstract":"The Everglades play a crucial role in flood and drought regulation, water resource planning, and ecosystem management in the surrounding regions. However, traditional physics-based and statistical methods for predicting water levels often face significant challenges, including high computational costs and limited adaptability to diverse or unforeseen conditions. Recent advancements in large time series models have demonstrated the potential to address these limitations, with state-of-the-art deep learning and foundation models achieving remarkable success in time series forecasting across various domains. Despite this progress, their application to critical environmental systems, such as the Everglades, remains underexplored. In this study, we fill the gap by investigating twelve task-specific models and five time series foundation models across six categories for a real-world application focused on water level prediction in the Everglades. Our primary results show that the foundation model, Chronos, significantly outperforms all other models while the remaining foundation models exhibit relatively poor performance. Moreover, the performance of task-specific models varies with the model architectures. Lastly, we discuss the possible reasons for the varying performance of models.","authors":["Rahuul Rangaraj","Jimeng Shi","Azam Shirali","Rajendra Paudel","Yanzhao Wu","Giri Narasimhan"],"url":"https://arxiv.org/abs/2505.01415"}
{"created":"2025-05-05","title":"Redundancy analysis using lcm-filtrations: networks, system signature and sensitivity evaluation","abstract":"We introduce the lcm-filtration and stepwise filtration, comparing their performance across various scenarios in terms of computational complexity, efficiency, and redundancy. The lcm-filtration often involves identical steps or ideals, leading to unnecessary computations. To address this, we analyse how stepwise filtration can effectively compute only the non-identical steps, offering a more efficient approach. We compare these filtrations in applications to networks, system signatures, and sensitivity analysis.","authors":["Fatemeh Mohammadi","Eduardo S\\'aenz-de-Cabez\\'on","Eduardo S\\'aenz-de-Cabez\\'on"],"url":"https://arxiv.org/abs/2505.01416"}
{"created":"2025-05-05","title":"Timely Tracking of a Wiener Process With Single Bit Quantization","abstract":"We consider the problem of timely tracking of a Wiener process via an energy-conserving sensor by utilizing a single bit quantization strategy under periodic sampling. Contrary to conventional single bit quantizers which only utilize the transmitted bit to convey information, in our codebook, we use an additional `$\\emptyset$' symbol to encode the event of \\emph{not transmitting}. Thus, our quantization functions are composed of three decision regions as opposed to the conventional two regions. First, we propose an optimum quantization method in which the optimum quantization functions are obtained by tracking the distributions of the quantization error. However, this method requires a high computational cost and might not be applicable for energy-conserving sensors. Thus, we propose two additional low complexity methods. In the last-bit aware method, three predefined quantization functions are available to both devices, and they switch the quantization function based on the last transmitted bit. With the Gaussian approximation method, we calculate a single quantization function by assuming that the quantization error can be approximated as Gaussian. While previous methods require a constant delay assumption, this method also works for random delay. We observe that all three methods perform similarly in terms of mean-squared error and transmission cost.","authors":["Osmail Cosandal","Sahan Liyanaarachchi","Sennur Ulukus"],"url":"https://arxiv.org/abs/2505.01419"}
{"created":"2025-05-05","title":"Evaluating Frontier Models for Stealth and Situational Awareness","abstract":"Recent work has demonstrated the plausibility of frontier AI models scheming -- knowingly and covertly pursuing an objective misaligned with its developer's intentions. Such behavior could be very hard to detect, and if present in future advanced systems, could pose severe loss of control risk. It is therefore important for AI developers to rule out harm from scheming prior to model deployment. In this paper, we present a suite of scheming reasoning evaluations measuring two types of reasoning capabilities that we believe are prerequisites for successful scheming: First, we propose five evaluations of ability to reason about and circumvent oversight (stealth). Second, we present eleven evaluations for measuring a model's ability to instrumentally reason about itself, its environment and its deployment (situational awareness). We demonstrate how these evaluations can be used as part of a scheming inability safety case: a model that does not succeed on these evaluations is almost certainly incapable of causing severe harm via scheming in real deployment. We run our evaluations on current frontier models and find that none of them show concerning levels of either situational awareness or stealth.","authors":["Mary Phuong","Roland S. Zimmermann","Ziyue Wang","David Lindner","Victoria Krakovna","Sarah Cogan","Allan Dafoe","Lewis Ho","Rohin Shah"],"url":"https://arxiv.org/abs/2505.01420"}
{"created":"2025-05-05","title":"Computational, Data-Driven, and Physics-Informed Machine Learning Approaches for Microstructure Modeling in Metal Additive Manufacturing","abstract":"Metal additive manufacturing enables unprecedented design freedom and the production of customized, complex components. However, the rapid melting and solidification dynamics inherent to metal AM processes generate heterogeneous, non-equilibrium microstructures that significantly impact mechanical properties and subsequent functionality. Predicting microstructure and its evolution across spatial and temporal scales remains a central challenge for process optimization and defect mitigation. While conventional experimental techniques and physics-based simulations provide a physical foundation and valuable insights, they face critical limitations. In contrast, data-driven machine learning offers an alternative prediction approach and powerful pattern recognition but often operate as black-box, lacking generalizability and physical consistency. To overcome these limitations, physics-informed machine learning, including physics-informed neural networks, has emerged as a promising paradigm by embedding governing physical laws into neural network architectures, thereby enhancing accuracy, transparency, data efficiency, and extrapolation capabilities. This work presents a comprehensive evaluation of modeling strategies for microstructure prediction in metal AM. The strengths and limitations of experimental, computational, and data-driven methods are analyzed in depth, and highlight recent advances in hybrid PIML frameworks that integrate physical knowledge with ML. Key challenges, such as data scarcity, multi-scale coupling, and uncertainty quantification, are discussed alongside future directions. Ultimately, this assessment underscores the importance of PIML-based hybrid approaches in enabling predictive, scalable, and physically consistent microstructure modeling for site-specific, microstructure-aware process control and the reliable production of high-performance AM components.","authors":["D. Patel","R. Sharma","Y. B. Guo"],"url":"https://arxiv.org/abs/2505.01424"}
{"created":"2025-05-05","title":"GENMO: A GENeralist Model for Human MOtion","abstract":"Human motion modeling traditionally separates motion generation and estimation into distinct tasks with specialized models. Motion generation models focus on creating diverse, realistic motions from inputs like text, audio, or keyframes, while motion estimation models aim to reconstruct accurate motion trajectories from observations like videos. Despite sharing underlying representations of temporal dynamics and kinematics, this separation limits knowledge transfer between tasks and requires maintaining separate models. We present GENMO, a unified Generalist Model for Human Motion that bridges motion estimation and generation in a single framework. Our key insight is to reformulate motion estimation as constrained motion generation, where the output motion must precisely satisfy observed conditioning signals. Leveraging the synergy between regression and diffusion, GENMO achieves accurate global motion estimation while enabling diverse motion generation. We also introduce an estimation-guided training objective that exploits in-the-wild videos with 2D annotations and text descriptions to enhance generative diversity. Furthermore, our novel architecture handles variable-length motions and mixed multimodal conditions (text, audio, video) at different time intervals, offering flexible control. This unified approach creates synergistic benefits: generative priors improve estimated motions under challenging conditions like occlusions, while diverse video data enhances generation capabilities. Extensive experiments demonstrate GENMO's effectiveness as a generalist framework that successfully handles multiple human motion tasks within a single model.","authors":["Jiefeng Li","Jinkun Cao","Haotian Zhang","Davis Rempe","Jan Kautz","Umar Iqbal","Ye Yuan"],"url":"https://arxiv.org/abs/2505.01425"}
{"created":"2025-05-05","title":"Advancing Software Security and Reliability in Cloud Platforms through AI-based Anomaly Detection","abstract":"Continuous Integration/Continuous Deployment (CI/CD) is fundamental for advanced software development, supporting faster and more efficient delivery of code changes into cloud environments. However, security issues in the CI/CD pipeline remain challenging, and incidents (e.g., DDoS, Bot, Log4j, etc.) are happening over the cloud environments. While plenty of literature discusses static security testing and CI/CD practices, only a few deal with network traffic pattern analysis to detect different cyberattacks. This research aims to enhance CI/CD pipeline security by implementing anomaly detection through AI (Artificial Intelligence) support. The goal is to identify unusual behaviour or variations from network traffic patterns in pipeline and cloud platforms. The system shall integrate into the workflow to continuously monitor pipeline activities and cloud infrastructure. Additionally, it aims to explore adaptive response mechanisms to mitigate the detected anomalies or security threats. This research employed two popular network traffic datasets, CSE-CIC-IDS2018 and CSE-CIC-IDS2017. We implemented a combination of Convolution Neural Network(CNN) and Long Short-Term Memory (LSTM) to detect unusual traffic patterns. We achieved an accuracy of 98.69% and 98.30% and generated log files in different CI/CD pipeline stages that resemble the network anomalies affected to address security challenges in modern DevOps practices, contributing to advancing software security and reliability.","authors":["Sabbir M. Saleh","Ibrahim Mohammed Sayem","Nazim Madhavji","John Steinbacher"],"url":"https://arxiv.org/abs/2411.09200"}
{"created":"2025-05-05","title":"Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task","abstract":"Recognizing safety violations in construction environments is critical yet remains underexplored in computer vision. Existing models predominantly rely on 2D object detection, which fails to capture the complexities of real-world violations due to: (i) an oversimplified task formulation treating violation recognition merely as object detection, (ii) inadequate validation under realistic conditions, (iii) absence of standardized baselines, and (iv) limited scalability from the unavailability of synthetic dataset generators for diverse construction scenarios. To address these challenges, we introduce Safe-Construct, the first framework that reformulates violation recognition as a 3D multi-view engagement task, leveraging scene-level worker-object context and 3D spatial understanding. We also propose the Synthetic Indoor Construction Site Generator (SICSG) to create diverse, scalable training data, overcoming data limitations. Safe-Construct achieves a 7.6% improvement over state-of-the-art methods across four violation types. We rigorously evaluate our approach in near-realistic settings, incorporating four violations, four workers, 14 objects, and challenging conditions like occlusions (worker-object, worker-worker) and variable illumination (back-lighting, overexposure, sunlight). By integrating 3D multi-view spatial understanding and synthetic data generation, Safe-Construct sets a new benchmark for scalable and robust safety monitoring in high-risk industries. Project Website: https://Safe-Construct.github.io/Safe-Construct","authors":["Aviral Chharia","Tianyu Ren","Tomotake Furuhata","Kenji Shimada"],"url":"https://arxiv.org/abs/2504.10880"}
{"created":"2025-05-05","title":"TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models","abstract":"Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.","authors":["Mihai Nadas","Laura Diosan","Andrei Piscoran","Andreea Tomescu"],"url":"https://arxiv.org/abs/2504.20605"}
{"created":"2025-05-05","title":"Notes on the discretization of TV-norm regularized inverse potential problems","abstract":"We describe a method to discretize optimization problems arising in the regularization of linear inverse problem having compact forward operator defined on 3-D valed measures, compactly supported on a fixed set. The criterion is a quadratic residual attached to the data, with an additive penalization of the total variation of the measure.","authors":["L Baratchart (FACTAS)","D P Hardin","C Villalobos-Guill\\'en"],"url":"https://arxiv.org/abs/2505.00710"}
{"created":"2025-05-05","title":"QEGS: A Mathematica Package for the Analysis of Quantum Extended Games","abstract":"Quantum games have attracted much attention in recent years due to their ability to solve decision-making dilemmas. The aim of this study is to extend previous work on quantum games by introducing a Mathematica package QEGS (Quantum Extension Game Solver) dedicated to the study of quantum extensions of classical $2\\times2$ games based on the EWL scheme. The package generates all possible game extensions with one or two unitary strategies, which are invariant with respect to isomorphic transformations of the initial games. The package includes a number of functions to study these extensions, such as determining their Nash equilibria in pure strategies, eliminating dominated strategies, or computing maximin strategies. Independently of quantum extensions, these functions can also be used to analyze classical games. Reporting to a pdf is available. The discussion includes an outline of future research directions, such as the exploration of mixed-strategy Nash equilibria and potential real-world applications in fields like quantum computing and secure communications.","authors":["Krzysztof Grzanka","Anna Gorczyca-Goraj","Piotr Fr\\k{a}ckiewicz","Marek Szopa"],"url":"https://arxiv.org/abs/2505.00714"}
{"created":"2025-05-05","title":"Productive Quantum Programming Needs Better Abstract Machines","abstract":"An effective, accessible abstraction hierarchy has made using and programming computers possible for people across all disciplines. Establishing such a hierarchy for quantum programming is an outstanding challenge, especially due to a proliferation of different conventions and the rapid pace of innovation. One critical portion of the hierarchy is the abstract machine, the layer that separates a programmer's mental model of the hardware from its physical realization. Drawing on historical parallels in classical computing, we explain why having the \"right\" quantum abstract machine (QAM) is essential for making progress in the field and propose a novel framework for evaluating QAMs based on a set of desirable criteria. These criteria capture aspects of a QAM such as universality, compactness, expressiveness, and composability, which aid in the representation of quantum programs. By defining this framework we take steps toward defining an optimal QAM. We further apply our framework to survey the landscape of existing proposals, draw comparisons, and assess them based on our criteria. While these proposals share many common strengths, we find that each falls short of our ideal. Our framework and our findings set a direction for subsequent efforts to define a future QAM that is both straightforward to map to a variety of quantum computers, and provides a stable abstraction for quantum software development.","authors":["Santiago N\\'u\\~nez-Corrales","Olivia Di Matteo","John Dumbell","Marcus Edwards","Edoardo Giusto","Scott Pakin","Vlad Stirbu"],"url":"https://arxiv.org/abs/2505.00718"}
{"created":"2025-05-05","title":"Leveraging Depth and Attention Mechanisms for Improved RGB Image Inpainting","abstract":"Existing deep learning-based image inpainting methods typically rely on convolutional networks with RGB images to reconstruct images. However, relying exclusively on RGB images may neglect important depth information, which plays a critical role in understanding the spatial and structural context of a scene. Just as human vision leverages stereo cues to perceive depth, incorporating depth maps into the inpainting process can enhance the model's ability to reconstruct images with greater accuracy and contextual awareness. In this paper, we propose a novel approach that incorporates both RGB and depth images for enhanced image inpainting. Our models employ a dual encoder architecture, where one encoder processes the RGB image and the other handles the depth image. The encoded features from both encoders are then fused in the decoder using an attention mechanism, effectively integrating the RGB and depth representations. We use two different masking strategies, line and square, to test the robustness of the model under different types of occlusions. To further analyze the effectiveness of our approach, we use Gradient-weighted Class Activation Mapping (Grad-CAM) visualizations to examine the regions of interest the model focuses on during inpainting. We show that incorporating depth information alongside the RGB image significantly improves the reconstruction quality. Through both qualitative and quantitative comparisons, we demonstrate that the depth-integrated model outperforms the baseline, with attention mechanisms further enhancing inpainting performance, as evidenced by multiple evaluation metrics and visualization.","authors":["Jin Hyun Park","Harine Choi","Praewa Pitiphat"],"url":"https://arxiv.org/abs/2505.00735"}
{"created":"2025-05-05","title":"Modeling and Analyzing Urban Networks and Amenities with OSMnx","abstract":"OSMnx is a Python package for downloading, modeling, analyzing, and visualizing urban networks and any other geospatial features from OpenStreetMap data. A large and growing body of literature uses it to conduct scientific studies across the disciplines of geography, urban planning, transport engineering, computer science, and others. The OSMnx project has recently developed and implemented many new features, modeling capabilities, and analytical methods. The package now encompasses substantially more functionality than was previously documented in the literature. This article introduces OSMnx's modern capabilities, usage, and design -- in addition to the scientific theory and logic underlying them. It shares lessons learned in geospatial software development and reflects on open science's implications for urban modeling and analysis.","authors":["Geoff Boeing"],"url":"https://arxiv.org/abs/2505.00736"}
{"created":"2025-05-05","title":"A Survey on 3D Reconstruction Techniques in Plant Phenotyping: From Classical Methods to Neural Radiance Fields (NeRF), 3D Gaussian Splatting (3DGS), and Beyond","abstract":"Plant phenotyping plays a pivotal role in understanding plant traits and their interactions with the environment, making it crucial for advancing precision agriculture and crop improvement. 3D reconstruction technologies have emerged as powerful tools for capturing detailed plant morphology and structure, offering significant potential for accurate and automated phenotyping. This paper provides a comprehensive review of the 3D reconstruction techniques for plant phenotyping, covering classical reconstruction methods, emerging Neural Radiance Fields (NeRF), and the novel 3D Gaussian Splatting (3DGS) approach. Classical methods, which often rely on high-resolution sensors, are widely adopted due to their simplicity and flexibility in representing plant structures. However, they face challenges such as data density, noise, and scalability. NeRF, a recent advancement, enables high-quality, photorealistic 3D reconstructions from sparse viewpoints, but its computational cost and applicability in outdoor environments remain areas of active research. The emerging 3DGS technique introduces a new paradigm in reconstructing plant structures by representing geometry through Gaussian primitives, offering potential benefits in both efficiency and scalability. We review the methodologies, applications, and performance of these approaches in plant phenotyping and discuss their respective strengths, limitations, and future prospects (https://github.com/JiajiaLi04/3D-Reconstruction-Plants). Through this review, we aim to provide insights into how these diverse 3D reconstruction techniques can be effectively leveraged for automated and high-throughput plant phenotyping, contributing to the next generation of agricultural technology.","authors":["Jiajia Li","Xinda Qi","Seyed Hamidreza Nabaei","Meiqi Liu","Dong Chen","Xin Zhang","Xunyuan Yin","Zhaojian Li"],"url":"https://arxiv.org/abs/2505.00737"}
{"created":"2025-05-05","title":"XeMap: Contextual Referring in Large-Scale Remote Sensing Environments","abstract":"Advancements in remote sensing (RS) imagery have provided high-resolution detail and vast coverage, yet existing methods, such as image-level captioning/retrieval and object-level detection/segmentation, often fail to capture mid-scale semantic entities essential for interpreting large-scale scenes. To address this, we propose the conteXtual referring Map (XeMap) task, which focuses on contextual, fine-grained localization of text-referred regions in large-scale RS scenes. Unlike traditional approaches, XeMap enables precise mapping of mid-scale semantic entities that are often overlooked in image-level or object-level methods. To achieve this, we introduce XeMap-Network, a novel architecture designed to handle the complexities of pixel-level cross-modal contextual referring mapping in RS. The network includes a fusion layer that applies self- and cross-attention mechanisms to enhance the interaction between text and image embeddings. Furthermore, we propose a Hierarchical Multi-Scale Semantic Alignment (HMSA) module that aligns multiscale visual features with the text semantic vector, enabling precise multimodal matching across large-scale RS imagery. To support XeMap task, we provide a novel, annotated dataset, XeMap-set, specifically tailored for this task, overcoming the lack of XeMap datasets in RS imagery. XeMap-Network is evaluated in a zero-shot setting against state-of-the-art methods, demonstrating superior performance. This highlights its effectiveness in accurately mapping referring regions and providing valuable insights for interpreting large-scale RS environments.","authors":["Yuxi Li","Lu Si","Yujie Hou","Chengaung Liu","Bin Li","Hongjian Fang","Jun Zhang"],"url":"https://arxiv.org/abs/2505.00738"}
{"created":"2025-05-05","title":"JFlow: Model-Independent Spherical Jeans Analysis using Equivariant Continuous Normalizing Flows","abstract":"The kinematics of stars in dwarf spheroidal galaxies have been studied to understand the structure of dark matter halos. However, the kinematic information of these stars is often limited to celestial positions and line-of-sight velocities, making full phase space analysis challenging. Conventional methods rely on projected analytic phase space density models with several parameters and infer dark matter halo structures by solving the spherical Jeans equation. In this paper, we introduce an unsupervised machine learning method for solving the spherical Jeans equation in a model-independent way as a first step toward model-independent analysis of dwarf spheroidal galaxies. Using equivariant continuous normalizing flows, we demonstrate that spherically symmetric stellar phase space densities and velocity dispersions can be estimated without model assumptions. As a proof of concept, we apply our method to Gaia challenge datasets for spherical models and measure dark matter mass densities given velocity anisotropy profiles. Our method can identify halo structures accurately, even with a small number of tracer stars.","authors":["Sung Hak Lim","Kohei Hayashi","Shun'ichi Horigome","Shigeki Matsumoto","Mihoko M. Nojiri"],"url":"https://arxiv.org/abs/2505.00763"}
{"created":"2025-05-05","title":"Dynamical System Parameter Path Optimization using Persistent Homology","abstract":"Nonlinear dynamical systems are complex and typically only simple systems can be analytically studied. In applications, these systems are usually defined with a set of tunable parameters and as the parameters are varied the system response undergoes significant topological changes or bifurcations. In a high dimensional parameter space, it is difficult to determine which direction to vary the system parameters to achieve a desired system response or state. In this paper, we introduce a new approach for optimally navigating a dynamical system parameter space that is rooted in topological data analysis. Specifically we use the differentiability of persistence diagrams to define a topological language for intuitively promoting or deterring different topological features in the state space response of a dynamical system and use gradient descent to optimally move from one point in the parameter space to another. The end result is a path in this space that guides the system to a set of parameters that yield the desired topological features defined by the loss function. We show a number of examples by applying the methods to different dynamical systems and scenarios to demonstrate how to promote different features and how to choose the hyperparameters to achieve different outcomes.","authors":["Max M. Chumley","Firas A. Khasawneh"],"url":"https://arxiv.org/abs/2505.00782"}
{"created":"2025-05-05","title":"Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions","abstract":"A clustered adaptive intervention (cAI) is a pre-specified sequence of decision rules that guides practitioners on how best - and based on which measures - to tailor cluster-level intervention to improve outcomes at the level of individuals within the clusters. A clustered sequential multiple assignment randomized trial (cSMART) is a type of trial that is used to inform the empirical development of a cAI. The most common type of secondary aim in a cSMART focuses on assessing causal effect moderation by candidate tailoring variables. We introduce a clustered Q-learning framework with the M-out-of-N Cluster Bootstrap using data from a cSMART to evaluate whether a set of candidate tailoring variables may be useful in defining an optimal cAI. This approach could construct confidence intervals (CI) with near-nominal coverage to assess parameters indexing the causal effect moderation function. Specifically, it allows reliable inferences concerning the utility of candidate tailoring variables in constructing a cAI that maximizes a mean end-of-study outcome even when \"non-regularity\", a well-known challenge exists. Simulations demonstrate the numerical performance of the proposed method across varying non-regularity conditions and investigate the impact of varying number of clusters and intra-cluster correlation coefficient on CI coverage. Methods are applied on ADEPT dataset to inform the construction of a clinic-level cAI for improving evidence-based practice in treating mood disorders.","authors":["Yao Song","Kelly Speth","Amy Kilbourne","Andrew Quanbeck","Daniel Almirall","Lu Wang"],"url":"https://arxiv.org/abs/2505.00822"}
{"created":"2025-05-05","title":"Multi-site modelling and reconstruction of past extreme skew surges along the French Atlantic coast","abstract":"Appropriate modelling of extreme skew surges is crucial, particularly for coastal risk management. Our study focuses on modelling extreme skew surges along the French Atlantic coast, with a particular emphasis on investigating the extremal dependence structure between stations. We employ the peak-over-threshold framework, where a multivariate extreme event is defined whenever at least one location records a large value, though not necessarily all stations simultaneously. A novel method for determining an appropriate level (threshold) above which observations can be classified as extreme is proposed. Two complementary approaches are explored. First, the multivariate generalized Pareto distribution is employed to model extremes, leveraging its properties to derive a generative model that predicts extreme skew surges at one station based on observed extremes at nearby stations. Second, a novel extreme regression framework is assessed for point predictions. This specific regression framework enables accurate point predictions using only the \"angle\" of input variables, i.e. input variables divided by their norms. The ultimate objective is to reconstruct historical skew surge time series at stations with limited data. This is achieved by integrating extreme skew surge data from stations with longer records, such as Brest and Saint-Nazaire, which provide over 150 years of observations.","authors":["Nathan Huet","Philippe Naveau","Anne Sabourin"],"url":"https://arxiv.org/abs/2505.00835"}
{"created":"2025-05-05","title":"On the emergence of numerical instabilities in Next Generation Reservoir Computing","abstract":"Next Generation Reservoir Computing (NGRC) is a low-cost machine learning method for forecasting chaotic time series from data. However, ensuring the dynamical stability of NGRC models during autonomous prediction remains a challenge. In this work, we uncover a key connection between the numerical conditioning of the NGRC feature matrix -- formed by polynomial evaluations on time-delay coordinates -- and the long-term NGRC dynamics. Merging tools from numerical linear algebra and ergodic theory of dynamical systems, we systematically study how the feature matrix conditioning varies across hyperparameters. We demonstrate that the NGRC feature matrix tends to be ill-conditioned for short time lags and high-degree polynomials. Ill-conditioning amplifies sensitivity to training data perturbations, which can produce unstable NGRC dynamics. We evaluate the impact of different numerical algorithms (Cholesky, SVD, and LU) for solving the regularized least-squares problem.","authors":["Edmilson Roque dos Santos","Erik Bollt"],"url":"https://arxiv.org/abs/2505.00846"}
{"created":"2025-05-05","title":"Platoon Coordination and Leader Selection in Mixed Transportation Systems via Dynamic Programming","abstract":"With the growing penetration of electric trucks, freight transportation is transitioning toward a mixed system comprising both fuel-powered and electric trucks. Enhancing truck platoon formation in such a heterogeneous environment presents new challenges. This paper investigates the hub-based platoon coordination problem in a mixed truck fleet, where the focus is to optimize the trucks' waiting times, charging amounts for electric trucks, and platoon leader assignments. The objective is to maximize the overall platoon revenue of the fleet while accounting for the associated waiting and charging costs. We formulate the problem as a mixed-integer linear program and present a dynamic programming approach to compute its sub-optimal solution efficiently. The proposed method operates in polynomial time, ensuring scalable computational efficiency. Simulation studies involving 1,000 trucks traveling between two hubs in Sweden demonstrate the effectiveness and scalability of the proposed approach.","authors":["Ying Wang","Ting Bai","Andreas A. Malikopoulos"],"url":"https://arxiv.org/abs/2505.00847"}
{"created":"2025-05-05","title":"Prime Integer Matrices","abstract":"This paper introduces prime integer matrices and its properties. It provides a simple way to construct families of pairwise co-prime integer matrices, that may have applications in multidimensional co-prime sensing and multidimensional Chinese remainder theorem.","authors":["Xiang-Gen Xia","Guangpu Guo"],"url":"https://arxiv.org/abs/2505.00862"}
{"created":"2025-05-05","title":"Rigidity of polytopes with edge length and coplanarity constraints","abstract":"We investigate a novel setting for polytope rigidity, where a flex must preserve edge lengths and the planarity of faces, but is allowed to change the shapes of faces. For instance, the regular cube is flexible in this notion. We present techniques for constructing flexible polytopes and find that flexibility seems to be an exceptional property. Based on this observation, we introduce a notion of generic realizations for polytopes and conjecture that convex polytopes are generically rigid in dimension $d\\geq 3$. We prove this conjecture in dimension $d=3$. Motivated by our findings we also pose several questions that are intended to inspire future research into this notion of polytope rigidity.","authors":["Matthias Himmelmann","Bernd Schulze","Martin Winter"],"url":"https://arxiv.org/abs/2505.00874"}
{"created":"2025-05-05","title":"The Alicki-Fannes-Winter technique in the quasi-classical settings: advanced version and its applications","abstract":"We describe an advanced version of the AFW-technique proposed in [Lett. Math. Phys., 113, 121 (2023)],[Lobachevskii J. Math., 44(6), 2169 (2023)] which allows us to obtain lower semicontinuity bounds, continuity bounds and local lower bounds for characteristics of quantum systems and discrete random variables.","authors":["M. E. Shirokov"],"url":"https://arxiv.org/abs/2505.00882"}
{"created":"2025-05-05","title":"Quantum Computing in Industrial Environments: Where Do We Stand and Where Are We Headed?","abstract":"This article explores the current state and future prospects of quantum computing in industrial environments. Firstly, it describes three main paradigms in this field of knowledge: gate-based quantum computers, quantum annealers, and tensor networks. The article also examines specific industrial applications, such as bin packing, job shop scheduling, and route planning for robots and vehicles. These applications demonstrate the potential of quantum computing to solve complex problems in the industry. The article concludes by presenting a vision of the directions the field will take in the coming years, also discussing the current limitations of quantum technology. Despite these limitations, quantum computing is emerging as a powerful tool to address industrial challenges in the future.","authors":["Eneko Osaba","I\\~nigo Perez Delgado","Alejandro Mata Ali","Pablo Miranda-Rodriguez","Aitor Moreno Fdez de Leceta","Luka Carmona Rivas"],"url":"https://arxiv.org/abs/2505.00891"}
{"created":"2025-05-05","title":"Multivariate Conformal Selection","abstract":"Selecting high-quality candidates from large datasets is critical in applications such as drug discovery, precision medicine, and alignment of large language models (LLMs). While Conformal Selection (CS) provides rigorous uncertainty quantification, it is limited to univariate responses and scalar criteria. To address this issue, we propose Multivariate Conformal Selection (mCS), a generalization of CS designed for multivariate response settings. Our method introduces regional monotonicity and employs multivariate nonconformity scores to construct conformal p-values, enabling finite-sample False Discovery Rate (FDR) control. We present two variants: mCS-dist, using distance-based scores, and mCS-learn, which learns optimal scores via differentiable optimization. Experiments on simulated and real-world datasets demonstrate that mCS significantly improves selection power while maintaining FDR control, establishing it as a robust framework for multivariate selection tasks.","authors":["Tian Bai","Yue Zhao","Xiang Yu","Archer Y. Yang"],"url":"https://arxiv.org/abs/2505.00917"}
{"created":"2025-05-05","title":"Enhancing Realism in Holographic Augmented Reality Displays through Occlusion Handling","abstract":"In this paper, an occlusion-capable holographic augmented-reality (AR) display is proposed, and its ability to enhance AR imagery through occlusion is demonstrated. Holographic displays can generate ideal three-dimensional (3D) virtual images and have recently shown rapid advancements, particularly in noise reduction through learning-based approaches. However, these displays still face challenges in improving image quality for AR scenarios because holographic virtual images are simply superimposed onto the real world, leading to a loss of contrast and visibility. To address this, an occlusion optics, which can mask designated areas of the real world, is incorporated into holographic AR displays. The proposed system employs a folded 4f system with a digital micromirror device and sequentially operates as both a real-world mask and an active Fourier filter. This approach transforms traditionally translucent holographic images into perceptually opaque ones while simultaneously eliminating unwanted noise terms from pixelated holographic displays. Furthermore, active Fourier filtering expands the virtual image field of view through time-multiplexed operation and supports a novel binary hologram optimization algorithm that performs especially well for sparse virtual content. The implementation successfully achieves opaque holographic 3D image presentation, significantly improving contrast and image quality while producing highly realistic 3D AR scenes with optically cast shadows.","authors":["Woongseob Han","Chanseul Lee","Jae-Hyeung Park"],"url":"https://arxiv.org/abs/2505.00942"}
{"created":"2025-05-05","title":"DOLCE: Decomposing Off-Policy Evaluation/Learning into Lagged and Current Effects","abstract":"Off-policy evaluation (OPE) and off-policy learning (OPL) for contextual bandit policies leverage historical data to evaluate and optimize a target policy. Most existing OPE/OPL methods--based on importance weighting or imputation--assume common support between the target and logging policies. When this assumption is violated, these methods typically require unstable extrapolation, truncation, or conservative strategies for individuals outside the common support assumption. However, such approaches can be inadequate in settings where explicit evaluation or optimization for such individuals is required. To address this issue, we propose DOLCE: Decomposing Off-policy evaluation/learning into Lagged and Current Effects, a novel estimator that leverages contextual information from multiple time points to decompose rewards into lagged and current effects. By incorporating both past and present contexts, DOLCE effectively handles individuals who violate the common support assumption. We show that the proposed estimator is unbiased under two assumptions--local correctness and conditional independence. Our experiments demonstrate that DOLCE achieves substantial improvements in OPE and OPL, particularly as the proportion of individuals outside the common support assumption increases.","authors":["Shu Tamano","Masanori Nojima"],"url":"https://arxiv.org/abs/2505.00961"}
{"created":"2025-05-05","title":"Quantum Support Vector Regression for Robust Anomaly Detection","abstract":"Anomaly Detection (AD) is critical in data analysis, particularly within the domain of IT security. In recent years, Machine Learning (ML) algorithms have emerged as a powerful tool for AD in large-scale data. In this study, we explore the potential of quantum ML approaches, specifically quantum kernel methods, for the application to robust AD. We build upon previous work on Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a comprehensive benchmark on IBM quantum hardware using eleven datasets. Our results demonstrate that QSVR achieves strong classification performance and even outperforms the noiseless simulation on two of these datasets. Moreover, we investigate the influence of - in the NISQ-era inevitable - quantum noise on the performance of the QSVR. Our findings reveal that the model exhibits robustness to depolarizing, phase damping, phase flip, and bit flip noise, while amplitude damping and miscalibration noise prove to be more disruptive. Finally, we explore the domain of Quantum Adversarial Machine Learning and demonstrate that QSVR is highly vulnerable to adversarial attacks and that noise does not improve the adversarial robustness of the model.","authors":["Kilian Tscharke","Maximilian Wendlinger","Sebastian Issel","Pascal Debus"],"url":"https://arxiv.org/abs/2505.01012"}
{"created":"2025-05-05","title":"Characterization and Learning of Causal Graphs from Hard Interventions","abstract":"A fundamental challenge in the empirical sciences involves uncovering causal structure through observation and experimentation. Causal discovery entails linking the conditional independence (CI) invariances in observational data to their corresponding graphical constraints via d-separation. In this paper, we consider a general setting where we have access to data from multiple experimental distributions resulting from hard interventions, as well as potentially from an observational distribution. By comparing different interventional distributions, we propose a set of graphical constraints that are fundamentally linked to Pearl's do-calculus within the framework of hard interventions. These graphical constraints associate each graphical structure with a set of interventional distributions that are consistent with the rules of do-calculus. We characterize the interventional equivalence class of causal graphs with latent variables and introduce a graphical representation that can be used to determine whether two causal graphs are interventionally equivalent, i.e., whether they are associated with the same family of hard interventional distributions, where the elements of the family are indistinguishable using the invariances from do-calculus. We also propose a learning algorithm to integrate multiple datasets from hard interventions, introducing new orientation rules. The learning objective is a tuple of augmented graphs which entails a set of causal graphs. We also prove the soundness of the proposed algorithm.","authors":["Zihan Zhou","Muhammad Qasim Elahi","Murat Kocaoglu"],"url":"https://arxiv.org/abs/2505.01037"}
{"created":"2025-05-05","title":"Snakes in the Plane: Controllable Gliders in a Nanomagnetic Metamaterial","abstract":"The magnetic metamaterials known as Artificial Spin Ice (ASI) are promising candidates for neuromorphic computing, composed of vast numbers of interacting nanomagnets arranged in the plane. Every computing device requires the ability to transform, transmit and store information. While ASI excel at data transformation, reliable transmission and storage has proven difficult to achieve. Here, we take inspiration from the Cellular Automaton (CA), an abstract computing model reminiscent of ASI. In CAs, information transmission and storage can be realised by the ``glider'', a simple structure capable of propagating while maintaining its form. Employing an evolutionary algorithm, we search for gliders in pinwheel ASI and present the simplest glider discovered: the ``snake''. Driven by a global field protocol, the snake moves strictly in one direction, determined by its orientation. We demonstrate the snake, both in simulation and experimentally, and analyse the mechanism behind its motion. The snake provides a means of manipulating a magnetic texture in an ASI with resolution on the order of 100 nm, which could in turn be utilised to precisely control other magnetic phenomena. The integration of data transmission, storage and modification into the same magnetic substrate unlocks the potential for ultra-low power computing devices.","authors":["Arthur Penty","Johannes H. Jensen","Ida Breivik","Anders Str{\\o}mberg","Erik Folven","Gunnar Tufte"],"url":"https://arxiv.org/abs/2505.01116"}
{"created":"2025-05-05","title":"On Simulating Thin-Film Processes at the Atomic Scale Using Machine Learned Force Fields","abstract":"Atomistic modeling of thin-film processes provides an avenue not only for discovering key chemical mechanisms of the processes but also to extract quantitative metrics on the events and reactions taking place at the gas-surface interface. Molecular dynamics (MD) is a powerful computational method to study the evolution of a process at the atomic scale, but studies of industrially relevant processes usually require suitable force fields, which are in general not available for all processes of interest. However, machine learned force fields (MLFF) are conquering the field of computational materials and surface science. In this paper, we demonstrate how to efficiently build MLFFs suitable for process simulations and provide two examples for technologically relevant processes: precursor pulse in the atomic layer deposition of HfO2 and atomic layer etching of MoS2.","authors":["S. Kondati Natarajan","J. Schneider","N. Pandey","J. Wellendorff","S. Smidstrup"],"url":"https://arxiv.org/abs/2505.01118"}
{"created":"2025-05-05","title":"Robustness and uncertainty of direct numerical simulation under the influence of rounding and noise","abstract":"Numerical precision in large-scale scientific computations has become an emerging topic due to recent developments in computer hardware. Lower floating point precision offers the potential for significant performance improvements, but the uncertainty added from reducing the numerical precision is a major obstacle for it to reach prevalence in high-fidelity simulations of turbulence. In the present work, the impact of reducing the numerical precision under different rounding schemes is investigated and compared to the presence of white noise in the simulation data to obtain statistical averages of different quantities in the flow. To investigate how this impacts the simulation, an experimental methodology to assess the impact of these sources of uncertainty is proposed, in which each realization $u^i$ at time $t_i$ is perturbed, either by constraining the flow to a coarser discretization of the phase space (corresponding to low precision formats rounded with deterministic and stochastic rounding) or by perturbing the flow with white noise with a uniform distribution. The purpose of this approach is to assess the limiting factors for precision, and how robust a direct numerical simulation (DNS) is to noise and numerical precision. Our results indicate that for low-Re turbulent channel flow, stochastic rounding and noise impacts the results significantly less than deterministic rounding, indicating potential benefits of stochastic rounding over conventional round-to-nearest. We find that to capture the probability density function of the velocity change in time, the floating point precision is especially important in regions with small relative velocity changes and low turbulence intensity, but less important in regions with large velocity gradients and variations such as in the near-wall region.","authors":["Martin Karp","Niclas Jansson","Saleh Rezaeiravesh","Stefano Markidis","Philipp Schlatter"],"url":"https://arxiv.org/abs/2505.01140"}
{"created":"2025-05-05","title":"Uncovering complementary information sharing in spider monkey collective foraging using higher-order spatial networks","abstract":"Collectives are often able to process information in a distributed fashion, surpassing each individual member's processing capacity. In fission-fusion dynamics, where group members come together and split from others often, sharing complementary information about uniquely known foraging areas could allow a group to track a heterogenous foraging environment better than any group member on its own. We analyse the partial overlaps between individual core ranges, which we assume represent the knowledge of an individual during a given season. We identify sets of individuals whose overlap shows a balance between redundantly and uniquely known portions and we use simplicial complexes to represent these higher-order interactions. The structure of the simplicial complexes shows holes in various dimensions, revealing complementarity in the foraging information that is being shared. We propose that the complex spatial networks arising from fission-fusion dynamics allow for adaptive, collective processing of foraging information in dynamic environments.","authors":["Gabriel Ramos-Fernandez","Ross S. Walker","Matthew J. Silk","Denis Boyer","Sandra E. Smith-Aguilar"],"url":"https://arxiv.org/abs/2505.01167"}
{"created":"2025-05-05","title":"A flexible Bayesian non-parametric mixture model reveals multiple dependencies of swap errors in visual working memory","abstract":"Human behavioural data in psychophysics has been used to elucidate the underlying mechanisms of many cognitive processes, such as attention, sensorimotor integration, and perceptual decision making. Visual working memory has particularly benefited from this approach: analyses of VWM errors have proven crucial for understanding VWM capacity and coding schemes, in turn constraining neural models of both. One poorly understood class of VWM errors are swap errors, whereby participants recall an uncued item from memory. Swap errors could arise from erroneous memory encoding, noisy storage, or errors at retrieval time - previous research has mostly implicated the latter two. However, these studies made strong a priori assumptions on the detailed mechanisms and/or parametric form of errors contributed by these sources. Here, we pursue a data-driven approach instead, introducing a Bayesian non-parametric mixture model of swap errors (BNS) which provides a flexible descriptive model of swapping behaviour, such that swaps are allowed to depend on both the probed and reported features of every stimulus item. We fit BNS to the trial-by-trial behaviour of human participants and show that it recapitulates the strong dependence of swaps on cue similarity in multiple datasets. Critically, BNS reveals that this dependence coexists with a non-monotonic modulation in the report feature dimension for a random dot motion direction-cued, location-reported dataset. The form of the modulation inferred by BNS opens new questions about the importance of memory encoding in causing swap errors in VWM, a distinct source to the previously suggested binding and cueing errors. Our analyses, combining qualitative comparisons of the highly interpretable BNS parameter structure with rigorous quantitative model comparison and recovery methods, show that previous interpretations of swap errors may have been incomplete.","authors":["Puria Radmard","Paul M. Bays","M\\'at\\'e Lengyel"],"url":"https://arxiv.org/abs/2505.01178"}
{"created":"2025-05-05","title":"Gaussian Differential Private Bootstrap by Subsampling","abstract":"Bootstrap is a common tool for quantifying uncertainty in data analysis. However, besides additional computational costs in the application of the bootstrap on massive data, a challenging problem in bootstrap based inference under Differential Privacy consists in the fact that it requires repeated access to the data. As a consequence, bootstrap based differentially private inference requires a significant increase of the privacy budget, which on the other hand comes with a substantial loss in statistical accuracy.","authors":["Holger Dette","Carina Graw"],"url":"https://arxiv.org/abs/2505.01197"}
{"created":"2025-05-05","title":"Can Foundation Models Really Segment Tumors? A Benchmarking Odyssey in Lung CT Imaging","abstract":"Accurate lung tumor segmentation is crucial for improving diagnosis, treatment planning, and patient outcomes in oncology. However, the complexity of tumor morphology, size, and location poses significant challenges for automated segmentation. This study presents a comprehensive benchmarking analysis of deep learning-based segmentation models, comparing traditional architectures such as U-Net and DeepLabV3, self-configuring models like nnUNet, and foundation models like MedSAM, and MedSAM~2. Evaluating performance across two lung tumor segmentation datasets, we assess segmentation accuracy and computational efficiency under various learning paradigms, including few-shot learning and fine-tuning. The results reveal that while traditional models struggle with tumor delineation, foundation models, particularly MedSAM~2, outperform them in both accuracy and computational efficiency. These findings underscore the potential of foundation models for lung tumor segmentation, highlighting their applicability in improving clinical workflows and patient outcomes.","authors":["Elena Mulero Ayll\\'on","Massimiliano Mantegna","Linlin Shen","Paolo Soda","Valerio Guarrasi","Matteo Tortora"],"url":"https://arxiv.org/abs/2505.01239"}
{"created":"2025-05-05","title":"Asymptotic Linear Convergence of ADMM for Isotropic TV Norm Compressed Sensing","abstract":"We prove an explicit local linear rate for ADMM solving the isotropic Total Variation (TV) norm compressed sensing problem in multiple dimensions, by analyzing the auxiliary variable in the equivalent Douglas-Rachford splitting on a dual problem. Numerical verification on large 3D problems and real MRI data will be shown. Though the proven rate is not sharp, it is close to the observed ones in numerical tests.","authors":["Emmanuel Gil Torres","Matt Jacobs","Xiangxiong Zhang"],"url":"https://arxiv.org/abs/2505.01240"}
{"created":"2025-05-05","title":"A CFL-type Condition and Theoretical Insights for Discrete-Time Sparse Full-Order Model Inference","abstract":"In this work, we investigate the data-driven inference of a discrete-time dynamical system via a sparse Full-Order Model (sFOM). We first formulate the involved Least Squares (LS) problem and discuss the need for regularization, indicating a connection between the typically employed $l_2$ regularization and the stability of the inferred discrete-time sFOM. We then provide theoretical insights considering the consistency and stability properties of the inferred numerical schemes that form the sFOM and exemplify them via illustrative, 1D test cases of linear diffusion and linear advection. For linear advection, we analytically derive a \"sampling CFL\" condition, which dictates a bound for the ratio of spatial and temporal discretization steps in the training data that ensures stability of the inferred sFOM. Finally, we investigate the sFOM inference for two nonlinear problems, namely a 2D Burgers' test case and the incompressible flow in an oscillating lid driven cavity, and draw connections between the theoretical findings and the properties of the inferred, nonlinear sFOMs.","authors":["Leonidas Gkimisis","S\\\"uleyman Y{\\i}ld{\\i}z","Peter Benner","Thomas Richter"],"url":"https://arxiv.org/abs/2505.01244"}
{"created":"2025-05-05","title":"A Provably Convergent Plug-and-Play Framework for Stochastic Bilevel Optimization","abstract":"Bilevel optimization has recently attracted significant attention in machine learning due to its wide range of applications and advanced hierarchical optimization capabilities. In this paper, we propose a plug-and-play framework, named PnPBO, for developing and analyzing stochastic bilevel optimization methods. This framework integrates both modern unbiased and biased stochastic estimators into the single-loop bilevel optimization framework introduced in [9], with several improvements. In the implementation of PnPBO, all stochastic estimators for different variables can be independently incorporated, and an additional moving average technique is applied when using an unbiased estimator for the upper-level variable. In the theoretical analysis, we provide a unified convergence and complexity analysis for PnPBO, demonstrating that the adaptation of various stochastic estimators (including PAGE, ZeroSARAH, and mixed strategies) within the PnPBO framework achieves optimal sample complexity, comparable to that of single-level optimization. This resolves the open question of whether the optimal complexity bounds for solving bilevel optimization are identical to those for single-level optimization. Finally, we empirically validate our framework, demonstrating its effectiveness on several benchmark problems and confirming our theoretical findings.","authors":["Tianshu Chu","Dachuan Xu","Wei Yao","Chengming Yu","Jin Zhang"],"url":"https://arxiv.org/abs/2505.01258"}
{"created":"2025-05-05","title":"How much to Dereverberate? Low-Latency Single-Channel Speech Enhancement in Distant Microphone Scenarios","abstract":"Dereverberation is an important sub-task of Speech Enhancement (SE) to improve the signal's intelligibility and quality. However, it remains challenging because the reverberation is highly correlated with the signal. Furthermore, the single-channel SE literature has predominantly focused on rooms with short reverb times (typically under 1 second), smaller rooms (under volumes of 1000 cubic meters) and relatively short distances (up to 2 meters). In this paper, we explore real-time low-latency single-channel SE under distant microphone scenarios, such as 5 to 10 meters, and focus on conference rooms and theatres, with larger room dimensions and reverberation times. Such a setup is useful for applications such as lecture demonstrations, drama, and to enhance stage acoustics. First, we show that single-channel SE in such challenging scenarios is feasible. Second, we investigate the relationship between room volume and reverberation time, and demonstrate its importance when randomly simulating room impulse responses. Lastly, we show that for dereverberation with short decay times, preserving early reflections before decaying the transfer function of the room improves overall signal quality.","authors":["Satvik Venkatesh","Philip Coleman","Arthur Benilov","Simon Brown","Selim Sheta","Frederic Roskam"],"url":"https://arxiv.org/abs/2505.01338"}
{"created":"2025-05-05","title":"Differentiable Nonlinear Model Predictive Control","abstract":"The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. While approaches presented in the machine learning community are limited to convex or unconstrained formulations, this paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. The publication is accompanied by an efficient open-source implementation within the framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solver mpc.pytorch.","authors":["Jonathan Frey","Katrin Baumg\\\"artner","Gianluca Frison","Dirk Reinhardt","Jasper Hoffmann","Leonard Fichtner","Sebastien Gros","Moritz Diehl"],"url":"https://arxiv.org/abs/2505.01353"}
{"created":"2025-05-05","title":"Provable Efficiency of Guidance in Diffusion Models for General Data Distribution","abstract":"Diffusion models have emerged as a powerful framework for generative modeling, with guidance techniques playing a crucial role in enhancing sample quality. Despite their empirical success, a comprehensive theoretical understanding of the guidance effect remains limited. Existing studies only focus on case studies, where the distribution conditioned on each class is either isotropic Gaussian or supported on a one-dimensional interval with some extra conditions. How to analyze the guidance effect beyond these case studies remains an open question. Towards closing this gap, we make an attempt to analyze diffusion guidance under general data distributions. Rather than demonstrating uniform sample quality improvement, which does not hold in some distributions, we prove that guidance can improve the whole sample quality, in the sense that the average reciprocal of the classifier probability decreases with the existence of guidance. This aligns with the motivation of introducing guidance.","authors":["Gen Li","Yuchen Jiao"],"url":"https://arxiv.org/abs/2505.01382"}
{"created":"2025-05-05","title":"Negative Stepsizes Make Gradient-Descent-Ascent Converge","abstract":"Efficient computation of min-max problems is a central question in optimization, learning, games, and controls. Arguably the most natural algorithm is gradient-descent-ascent (GDA). However, since the 1970s, conventional wisdom has argued that GDA fails to converge even on simple problems. This failure spurred an extensive literature on modifying GDA with additional building blocks such as extragradients, optimism, momentum, anchoring, etc. In contrast, we show that GDA converges in its original form by simply using a judicious choice of stepsizes.","authors":["Henry Shugart","Jason M. Altschuler"],"url":"https://arxiv.org/abs/2505.01423"}
{"created":"2025-05-05","title":"Notes on Theory of Distributed Systems","abstract":"Notes for the Yale course CPSC 465/565 Theory of Distributed Systems.","authors":["James Aspnes"],"url":"https://arxiv.org/abs/2001.04235"}
{"created":"2025-05-05","title":"Minimum mean-squared error estimation with bandit feedback","abstract":"We consider the problem of sequentially learning to estimate, in the mean squared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by observing only $m < K$ of its entries in each round. We propose two MSE estimators, and analyze their concentration properties. The first estimator is non-adaptive, as it is tied to a predetermined $m$-subset and lacks the flexibility to transition to alternative subsets. The second estimator, which is derived using a regression framework, is adaptive and exhibits better concentration bounds in comparison to the first estimator. We frame the MSE estimation problem with bandit feedback, where the objective is to find the MSE-optimal subset with high confidence. We propose a variant of the successive elimination algorithm to solve this problem. We also derive a minimax lower bound to understand the fundamental limit on the sample complexity of this problem.","authors":["Ayon Ghosh","L. A. Prashanth","Dipayan Sen","Aditya Gopalan"],"url":"https://arxiv.org/abs/2203.16810"}
{"created":"2025-05-05","title":"Identifying discreditable firms in a large-scale ownership network","abstract":"Violations of laws and regulations about food safety, production safety, quality standard and environmental protection, or negative consequences from loan, guarantee and pledge contracts, may result in operating and credit risks of firms. The above illegal or trust-breaking activities are collectively called discreditable activities, and firms with discreditable activities are named as discreditable firms. Identification of discreditable firms is of great significance for investment attraction, bank lending, equity investment, supplier selection, job seeking, and so on. In this paper, we collect registration records of about 113 million Chinese firms and construct an ownership network with about 6 million nodes, where each node is a firm who has invested at least one firm or has been invested by at least one firm. Analysis of publicly available records of discreditable activities show strong network effect, namely the probability of a firm to be discreditable is remarkably higher than the average probability given the fact that one of its investors or investees is discreditable. In comparison, for the risk of being a discreditable firm, an investee has higher impact than an investor in average. The impact of a firm on surrounding firms decays along with the increasing topological distance, analogous to the well-known \"three degrees of separation\" phenomenon. The uncovered correlation of discreditable activities can be considered as a representative example of network effect, in addition to the propagation of diseases, opinions and human behaviors. Lastly, we show that the utilization of the network effect largely improves the accuracy of the algorithm to identify discreditable firms.","authors":["Tao Zhou","Yan-Li Lee","Qian Li","Duanbing Chen","Wenbo Xie","Tong Wu","Tu Zeng"],"url":"https://arxiv.org/abs/2211.14316"}
{"created":"2025-05-05","title":"Deep Reinforcement Learning for Traffic Light Control in Intelligent Transportation Systems","abstract":"Smart traffic lights in intelligent transportation systems (ITSs) are envisioned to greatly increase traffic efficiency and reduce congestion. Deep reinforcement learning (DRL) is a promising approach to adaptively control traffic lights based on the real-time traffic situation in a road network. However, conventional methods may suffer from poor scalability. In this paper, we investigate deep reinforcement learning to control traffic lights, and both theoretical analysis and numerical experiments show that the intelligent behavior ``greenwave\" (i.e., a vehicle will see a progressive cascade of green lights, and not have to brake at any intersection) emerges naturally a grid road network, which is proved to be the optimal policy in an avenue with multiple cross streets. As a first step, we use two DRL algorithms for the traffic light control problems in two scenarios. In a single road intersection, we verify that the deep Q-network (DQN) algorithm delivers a thresholding policy; and in a grid road network, we adopt the deep deterministic policy gradient (DDPG) algorithm. Secondly, numerical experiments show that the DQN algorithm delivers the optimal control, and the DDPG algorithm with passive observations has the capability to produce on its own a high-level intelligent behavior in a grid road network, namely, the ``greenwave\" policy emerges. We also verify the ``greenwave\" patterns in a $5 \\times 10$ grid road network. Thirdly, the ``greenwave\" patterns demonstrate that DRL algorithms produce favorable solutions since the ``greenwave\" policy shown in experiment results is proved to be optimal in a specified traffic model (an avenue with multiple cross streets). The delivered policies both in a single road intersection and a grid road network demonstrate the scalability of DRL algorithms.","authors":["Ming Zhu","Xiao-Yang Liu","Sem Borst","Anwar Walid"],"url":"https://arxiv.org/abs/2302.03669"}
{"created":"2025-05-05","title":"Deterministic Nonsmooth Nonconvex Optimization","abstract":"We study the complexity of optimizing nonsmooth nonconvex Lipschitz functions by producing $(\\delta,\\epsilon)$-stationary points. Several recent works have presented randomized algorithms that produce such points using $\\tilde O(\\delta^{-1}\\epsilon^{-3})$ first-order oracle calls, independent of the dimension $d$. It has been an open problem as to whether a similar result can be obtained via a deterministic algorithm. We resolve this open problem, showing that randomization is necessary to obtain a dimension-free rate. In particular, we prove a lower bound of $\\Omega(d)$ for any deterministic algorithm. Moreover, we show that unlike smooth or convex optimization, access to function values is required for any deterministic algorithm to halt within any finite time.","authors":["Michael I. Jordan","Guy Kornowski","Tianyi Lin","Ohad Shamir","Manolis Zampetakis"],"url":"https://arxiv.org/abs/2302.08300"}
{"created":"2025-05-05","title":"The First and Second-Order Asymptotics of Covert Communication over AWGN Channels","abstract":"This paper investigates the asymptotics of the maximal throughput of communication over AWGN channels by $n$ channel uses under a covert constraint in terms of an upper bound $\\delta$ of Kullback-Leibler divergence (KL divergence). It is shown that the first and second order asymptotics of the maximal throughput are $\\sqrt{n\\delta \\log e}$ and $(2)^{1/2}(n\\delta)^{1/4}(\\log e)^{3/4}\\cdot Q^{-1}(\\epsilon)$, respectively. The technique we use in the achievability is quasi-$\\varepsilon$-neighborhood notion from information geometry. For finite blocklength $n$, the generating distributions are chosen to be a family of truncated Gaussian distributions with decreasing variances. The law of decreasing is carefully designed so that it maximizes the throughput at the main channel in the asymptotic sense under the condition that the output distributions satisfy the covert constraint. For the converse, the optimality of Gaussian distribution for minimizing KL divergence under the second order moment constraint is extended from dimension $1$ to dimension $n$. Based on that, we establish an upper bound on the average power of the code to satisfy the covert constraint, which further leads to the direct converse bound in terms of covert metric.","authors":["Xinchun Yu","Shuangqing Wei","Shao-Lun Huang","Xiao-Ping Zhang"],"url":"https://arxiv.org/abs/2305.17924"}
{"created":"2025-05-05","title":"An Adaptive Method for Weak Supervision with Drifting Data","abstract":"We introduce an adaptive method with formal quality guarantees for weak supervision in a non-stationary setting. Our goal is to infer the unknown labels of a sequence of data by using weak supervision sources that provide independent noisy signals of the correct classification for each data point. This setting includes crowdsourcing and programmatic weak supervision. We focus on the non-stationary case, where the accuracy of the weak supervision sources can drift over time, e.g., because of changes in the underlying data distribution. Due to the drift, older data could provide misleading information to infer the label of the current data point. Previous work relied on a priori assumptions on the magnitude of the drift to decide how much data to use from the past. In contrast, our algorithm does not require any assumptions on the drift, and it adapts based on the input by dynamically varying its window size. In particular, at each step, our algorithm estimates the current accuracies of the weak supervision sources by identifying a window of past observations that guarantees a near-optimal minimization of the trade-off between the error due to the variance of the estimation and the error due to the drift. Experiments on synthetic and real-world labelers show that our approach adapts to the drift.","authors":["Alessio Mazzetto","Reza Esfandiarpoor","Akash Singirikonda","Eli Upfal","Stephen H. Bach"],"url":"https://arxiv.org/abs/2306.01658"}
{"created":"2025-05-05","title":"VitalVideos-Europe: A dataset of face videos with PPG and blood pressure ground truths","abstract":"We collected a large dataset consisting of 850 unique participants. For every participant we recorded two 30 second uncompressed videos, synchronized PPG waveforms and a single blood pressure measurement. Gender, age and skin color were also registered for every participant. The dataset includes roughly equal numbers of males and females, as well as participants of all ages. While the skin color distribution could have been more balanced, the dataset contains individuals from every skin color. The data was collected in a diverse set of locations to ensure a wide variety of backgrounds and lighting conditions. In an effort to assist in the research and development of remote vital sign measurement we are now opening up access to this dataset.","authors":["Pieter-Jan Toye"],"url":"https://arxiv.org/abs/2306.11891"}
{"created":"2025-05-05","title":"Killing Two Birds with One Stone: Malicious Package Detection in NPM and PyPI using a Single Model of Malicious Behavior Sequence","abstract":"Open-source software (OSS) supply chain enlarges the attack surface, which makes package registries attractive targets for attacks. Recently, package registries NPM and PyPI have been flooded with malicious packages. The effectiveness of existing malicious NPM and PyPI package detection approaches is hindered by two challenges. The first challenge is how to leverage the knowledge of malicious packages from different ecosystems in a unified way such that multi-lingual malicious package detection can be feasible. The second challenge is how to model malicious behavior in a sequential way such that maliciousness can be precisely captured. To address the two challenges, we propose and implement Cerebro to detect malicious packages in NPM and PyPI. We curate a feature set based on a high-level abstraction of malicious behavior to enable multi-lingual knowledge fusing. We organize extracted features into a behavior sequence to model sequential malicious behavior. We fine-tune the BERT model to understand the semantics of malicious behavior. Extensive evaluation has demonstrated the effectiveness of Cerebro over the state-of-the-art as well as the practically acceptable efficiency. Cerebro has successfully detected 306 and 196 new malicious packages in PyPI and NPM, and received 385 thank letters from the official PyPI and NPM teams.","authors":["Junan Zhang","Kaifeng Huang","Yiheng Huang","Bihuan Chen","Ruisi Wang","Chong Wang","Xin Peng"],"url":"https://arxiv.org/abs/2309.02637"}
{"created":"2025-05-05","title":"Stacked Intelligent Metasurfaces for Multiuser Downlink Beamforming in the Wave Domain","abstract":"Intelligent metasurface has recently emerged as a promising technology that enables the customization of wireless environments by harnessing large numbers of low-cost reconfigurable scattering elements. However, prior studies have predominantly focused on single-layer metasurfaces, which have limitations in terms of wave-domain processing capabilities due to practical hardware limitations. In contrast, this paper introduces a novel stacked intelligent metasurface (SIM) design. Specifically, we investigate the integration of SIM into the downlink of a multiuser multiple-input single-output (MISO) communication system, where an SIM, consisting of a multilayer metasurface structure, is deployed at the base station (BS) to facilitate transmit beamforming in the electromagnetic wave domain. This eliminates the need for conventional digital beamforming and high-resolution digital-to-analog converters at the BS. To this end, an optimization problem is formulated to maximize the sum rate of all user equipments by jointly optimizing the transmit power allocation at the BS and the wave-based beamforming at the SIM, subject to constraints on the transmit power budget and discrete phase shifts. Furthermore, we propose a computationally efficient algorithm for solving the formulated joint optimization problem and elaborate on the potential benefits of employing SIM in wireless networks. Numerical results are illustrated to corroborate the effectiveness of the proposed SIM-enabled wave-based beamforming design and to evaluate the performance improvement achieved by the proposed algorithm compared to various benchmark schemes. It is demonstrated that considering the same number of transmit antennas, the proposed SIM-based system achieves about 200\\% improvement in terms of sum rate compared to conventional MISO systems. The code for this paper is available at \\url{https://github.com/JianchengAn}.","authors":["Jiancheng An","Marco Di Renzo","M\\'erouane Debbah","H. Vincent Poor","Chau Yuen"],"url":"https://arxiv.org/abs/2309.02687"}
{"created":"2025-05-05","title":"Automating the Generation of Prompts for LLM-based Action Choice in PDDL Planning","abstract":"Large language models (LLMs) have revolutionized a large variety of NLP tasks. An active debate is to what extent they can do reasoning and planning. Prior work has assessed the latter in the specific context of PDDL planning, based on manually converting three PDDL domains into natural language (NL) prompts. Here we automate this conversion step, showing how to leverage an LLM to automatically generate NL prompts from PDDL input. Our automatically generated NL prompts result in similar LLM-planning performance as the previous manually generated ones. Beyond this, the automation enables us to run much larger experiments, providing for the first time a broad evaluation of LLM planning performance in PDDL. Our NL prompts yield better performance than PDDL prompts and simple template-based NL prompts. Compared to symbolic planners, LLM planning lags far behind; but in some domains, our best LLM configuration scales up further than A$^\\star$ using LM-cut.","authors":["Katharina Stein","Daniel Fi\\v{s}er","J\\\"org Hoffmann","Alexander Koller"],"url":"https://arxiv.org/abs/2311.09830"}
{"created":"2025-05-05","title":"Quantum Circuit Mutants: Empirical Analysis and Recommendations","abstract":"As a new research area, quantum software testing lacks systematic testing benchmarks to assess testing techniques' effectiveness. Recently, some open-source benchmarks and mutation analysis tools have emerged. However, there is insufficient evidence on how various quantum circuit characteristics (e.g., circuit depth, number of quantum gates), algorithms (e.g., Quantum Approximate Optimization Algorithm), and mutation characteristics (e.g., mutation operators) affect the detection of mutants in quantum circuits. Studying such relations is important to systematically design faulty benchmarks with varied attributes (e.g., the difficulty in detecting a seeded fault) to facilitate assessing the cost-effectiveness of quantum software testing techniques efficiently. To this end, we present a large-scale empirical evaluation with more than 700K faulty benchmarks (quantum circuits) generated by mutating 382 real-world quantum circuits. Based on the results, we provide valuable insights for researchers to define systematic quantum mutation analysis techniques. We also provide a tool to recommend mutants to users based on chosen characteristics (e.g., a quantum algorithm type) and the required difficulty of detecting mutants. Finally, we also provide faulty benchmarks that can already be used to assess the cost-effectiveness of quantum software testing techniques.","authors":["E\\~naut Mendiluze Usandizaga","Tao Yue","Paolo Arcaini","Shaukat Ali"],"url":"https://arxiv.org/abs/2311.16913"}
{"created":"2025-05-05","title":"Generating synthetic data for neural operators","abstract":"Recent advances in the literature show promising potential of deep learning methods, particularly neural operators, in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, existing data-driven approaches often rely on training data produced by numerical PDE solvers (e.g., finite difference or finite element methods). We introduce a \"backward\" data generation method that avoids solving the PDE numerically: by randomly sampling candidate solutions $u_j$ from the appropriate solution space (e.g., $H_0^1(\\Omega)$), we compute the corresponding right-hand side $f_j$ directly from the equation by differentiation. This produces training pairs ${(f_j, u_j)}$ by computing derivatives rather than solving a PDE numerically for each data point, enabling fast, large-scale data generation consisting of exact solutions. Experiments indicate that models trained on this synthetic data generalize well when tested on data produced by standard solvers. While the idea is simple, we hope this method will expand the potential of neural PDE solvers that do not rely on classical numerical solvers to generate their data.","authors":["Erisa Hasani","Rachel A. Ward"],"url":"https://arxiv.org/abs/2401.02398"}
{"created":"2025-05-05","title":"Timeout Asynchronous Session Types: Safe Asynchronous Mixed-Choice For Timed Interactions","abstract":"Mixed-choice has long been barred from models of asynchronous communication since it compromises the decidability of key properties of communicating finite-state machines. Session types inherit this restriction, which precludes them from fully modelling timeouts -- a core property of web and cloud services. To address this deficiency, we present (binary) Timeout Asynchronous Session Types (TOAST) as an extension to (binary) asynchronous timed session types, that permits mixed-choice. TOAST deploys timing constraints to regulate the use of mixed-choice so as to preserve communication safety. We provide a new behavioural semantics for TOAST which guarantees progress in the presence of mixed-choice. Building upon TOAST, we provide a calculus featuring process timers which is capable of modelling timeouts using a receive-after pattern, much like Erlang, and capture the correspondence with TOAST specifications via a type system for which we prove subject reduction.","authors":["Jonah Pears","Laura Bocchi","Maurizio Murgia","Andy King"],"url":"https://arxiv.org/abs/2401.11197"}
{"created":"2025-05-05","title":"Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks","abstract":"Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a comprehensive interpretation of intertwined quantitative and qualitative features, demonstrating its usefulness and effectiveness via case studies on formulating concept stocks and extensive interviews with domain experts.","authors":["Wong Kam-Kwai","Yan Luo","Xuanwu Yue","Wei Chen","Huamin Qu"],"url":"https://arxiv.org/abs/2402.08978"}
{"created":"2025-05-05","title":"Visual Concept-driven Image Generation with Text-to-Image Diffusion Model","abstract":"Text-to-image (TTI) diffusion models have demonstrated impressive results in generating high-resolution images of complex and imaginative scenes. Recent approaches have further extended these methods with personalization techniques that allow them to integrate user-illustrated concepts (e.g., the user him/herself) using a few sample image illustrations. However, the ability to generate images with multiple interacting concepts, such as human subjects, as well as concepts that may be entangled in one, or across multiple, image illustrations remains illusive. In this work, we propose a concept-driven TTI personalization framework that addresses these core challenges. We build on existing works that learn custom tokens for user-illustrated concepts, allowing those to interact with existing text tokens in the TTI model. However, importantly, to disentangle and better learn the concepts in question, we jointly learn (latent) segmentation masks that disentangle these concepts in user-provided image illustrations. We do so by introducing an Expectation Maximization (EM)-like optimization procedure where we alternate between learning the custom tokens and estimating (latent) masks encompassing corresponding concepts in user-supplied images. We obtain these masks based on cross-attention, from within the U-Net parameterized latent diffusion model and subsequent DenseCRF optimization. We illustrate that such joint alternating refinement leads to the learning of better tokens for concepts and, as a by-product, latent masks. We illustrate the benefits of the proposed approach qualitatively and quantitatively with several examples and use cases that can combine three or more entangled concepts.","authors":["Tanzila Rahman","Shweta Mahajan","Hsin-Ying Lee","Jian Ren","Sergey Tulyakov","Leonid Sigal"],"url":"https://arxiv.org/abs/2402.11487"}
{"created":"2025-05-05","title":"Rethinking Scientific Summarization Evaluation: Grounding Explainable Metrics on Facet-aware Benchmark","abstract":"The summarization capabilities of pretrained and large language models (LLMs) have been widely validated in general areas, but their use in scientific corpus, which involves complex sentences and specialized knowledge, has been less assessed. This paper presents conceptual and experimental analyses of scientific summarization, highlighting the inadequacies of traditional evaluation methods, such as $n$-gram, embedding comparison, and QA, particularly in providing explanations, grasping scientific concepts, or identifying key content. Subsequently, we introduce the Facet-aware Metric (FM), employing LLMs for advanced semantic matching to evaluate summaries based on different aspects. This facet-aware approach offers a thorough evaluation of abstracts by decomposing the evaluation task into simpler subtasks.Recognizing the absence of an evaluation benchmark in this domain, we curate a Facet-based scientific summarization Dataset (FD) with facet-level annotations. Our findings confirm that FM offers a more logical approach to evaluating scientific summaries. In addition, fine-tuned smaller models can compete with LLMs in scientific contexts, while LLMs have limitations in learning from in-context information in scientific domains. This suggests an area for future enhancement of LLMs.","authors":["Xiuying Chen","Tairan Wang","Qingqing Zhu","Taicheng Guo","Shen Gao","Zhiyong Lu","Xin Gao","Xiangliang Zhang"],"url":"https://arxiv.org/abs/2402.14359"}
{"created":"2025-05-05","title":"FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning","abstract":"Finetuning large language models (LLMs) is essential for task adaptation, yet serving stacks today isolate inference and finetuning on separate GPU clusters -- wasting resources and under-utilizing hardware. We introduce FlexLLM, the first system to co-serve LLM inference and PEFT-based finetuning on shared GPUs by fusing computation at the token level. The static compilation optimizations in FlexLLM -- dependent parallelization and graph pruning significantly shrink activation memory, leading to end-to-end GPU memory savings by up to 80%. At runtime, a novel token-level finetuning mechanism paired with a hybrid token scheduler dynamically interleaves inference and training tokens within each co-serving iteration, meeting strict latency SLOs while maximizing utilization. In end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B, FlexLLM sustains the inference SLO requirements up to 20 req/s, and improves finetuning throughput by 1.9-4.8x under heavy inference workloads and 2.5-6.8x under light loads, preserving over 76% of peak finetuning progress even at peak demand. The source code of FlexLLM is publicly available at https://github.com/flexflow/FlexFlow/.","authors":["Gabriele Oliaro","Xupeng Miao","Xinhao Cheng","Vineeth Kada","Ruohan Gao","Yingyi Huang","Remi Delacourt","April Yang","Yingcheng Wang","Mengdi Wu","Colin Unger","Zhihao Jia"],"url":"https://arxiv.org/abs/2402.18789"}
{"created":"2025-05-05","title":"Wiki-TabNER: Integrating Named Entity Recognition into Wikipedia Tables","abstract":"Interest in solving table interpretation tasks has grown over the years, yet it still relies on existing datasets that may be overly simplified. This is potentially reducing the effectiveness of the dataset for thorough evaluation and failing to accurately represent tables as they appear in the real-world. To enrich the existing benchmark datasets, we extract and annotate a new, more challenging dataset. The proposed Wiki-TabNER dataset features complex tables containing several entities per cell, with named entities labeled using DBpedia classes. This dataset is specifically designed to address named entity recognition (NER) task within tables, but it can also be used as a more challenging dataset for evaluating the entity linking task. In this paper we describe the distinguishing features of the Wiki-TabNER dataset and the labeling process. In addition, we propose a prompting framework for evaluating the new large language models on the within tables NER task. Finally, we perform qualitative analysis to gain insights into the challenges encountered by the models and to understand the limitations of the proposed~dataset.","authors":["Aneta Koleva","Martin Ringsquandl","Ahmed Hatem","Thomas Runkler","Volker Tresp"],"url":"https://arxiv.org/abs/2403.04577"}
{"created":"2025-05-05","title":"Improving Continual Learning Performance and Efficiency with Auxiliary Classifiers","abstract":"Continual learning is crucial for applying machine learning in challenging, dynamic, and often resource-constrained environments. However, catastrophic forgetting - overwriting previously learned knowledge when new information is acquired - remains a major challenge. In this work, we examine the intermediate representations in neural network layers during continual learning and find that such representations are less prone to forgetting, highlighting their potential to accelerate computation. Motivated by these findings, we propose to use auxiliary classifiers(ACs) to enhance performance and demonstrate that integrating ACs into various continual learning methods consistently improves accuracy across diverse evaluation settings, yielding an average 10% relative gain. We also leverage the ACs to reduce the average cost of the inference by 10-60% without compromising accuracy, enabling the model to return the predictions before computing all the layers. Our approach provides a scalable and efficient solution for continual learning.","authors":["Filip Szatkowski","Yaoyue Zheng","Fei Yang","Bart{\\l}omiej Twardowski","Tomasz Trzci\\'nski","Joost van de Weijer"],"url":"https://arxiv.org/abs/2403.07404"}
{"created":"2025-05-05","title":"P-Hologen: An End-to-End Generative Framework for Phase-Only Holograms","abstract":"Holography stands at the forefront of visual technology, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Although generative models have been extensively explored in the image domain, their application to holograms remains relatively underexplored due to the inherent complexity of phase learning. Exploiting generative models for holograms offers exciting opportunities for advancing innovation and creativity, such as semantic-aware hologram generation and editing. Currently, the most viable approach for utilizing generative models in the hologram domain involves integrating an image-based generative model with an image-to-hologram conversion model, which comes at the cost of increased computational complexity and inefficiency. To tackle this problem, we introduce P-Hologen, the first end-to-end generative framework designed for phase-only holograms (POHs). P-Hologen employs vector quantized variational autoencoders to capture the complex distributions of POHs. It also integrates the angular spectrum method into the training process, constructing latent spaces for complex phase data using strategies from the image processing domain. Extensive experiments demonstrate that P-Hologen achieves superior quality and computational efficiency compared to the existing methods. Furthermore, our model generates high-quality unseen, diverse holographic content from its learned latent space without requiring pre-existing images. Our work paves the way for new applications and methodologies in holographic content creation, opening a new era in the exploration of generative holographic content. The code for our paper is publicly available on https://github.com/james0223/P-Hologen.","authors":["JooHyun Park","YuJin Jeon","HuiYong Kim","SeungHwan Baek","HyeongYeop Kang"],"url":"https://arxiv.org/abs/2404.01330"}
{"created":"2025-05-05","title":"On Reducing the Execution Latency of Superconducting Quantum Processors via Quantum Job Scheduling","abstract":"Quantum computing has gained considerable attention, especially after the arrival of the Noisy Intermediate-Scale Quantum (NISQ) era. Quantum processors and cloud services have been made world-wide increasingly available. Unfortunately, jobs on existing quantum processors are often executed in series, and the workload could be heavy to the processor. Typically, one has to wait for hours or even longer to obtain the result of a single quantum job on public quantum cloud due to long queue time. In fact, as the scale grows, the qubit utilization rate of the serial execution mode will further diminish, causing the waste of quantum resources. In this paper, to our best knowledge for the first time, the Quantum Job Scheduling Problem (QJSP) is formulated and introduced, and we accordingly aim to improve the utility efficiency of quantum resources. Specifically, a noise-aware quantum job scheduler (NAQJS) concerning the circuit width, number of measurement shots, and submission time of quantum jobs is proposed to reduce the execution latency. We conduct extensive experiments on a simulated Qiskit noise model, as well as on the Xiaohong (from QuantumCTek) superconducting quantum processor. Numerical results show the effectiveness in both the QPU time and turnaround time.","authors":["Wenjie Wu","Yiquan Wang","Ge Yan","Yuming Zhao","Bo Zhang","Junchi Yan"],"url":"https://arxiv.org/abs/2404.07882"}
{"created":"2025-05-05","title":"InspectorRAGet: An Introspection Platform for RAG Evaluation","abstract":"Large Language Models (LLM) have become a popular approach for implementing Retrieval Augmented Generation (RAG) systems, and a significant amount of effort has been spent on building good models and metrics. In spite of increased recognition of the need for rigorous evaluation of RAG systems, few tools exist that go beyond the creation of model output and automatic calculation. We present InspectorRAGet, an introspection platform for performing a comprehensive analysis of the quality of RAG system output. InspectorRAGet allows the user to analyze aggregate and instance-level performance of RAG systems, using both human and algorithmic metrics as well as annotator quality. InspectorRAGet is suitable for multiple use cases and is available publicly to the community. A live instance of the platform is available at https://ibm.biz/InspectorRAGet.","authors":["Kshitij Fadnis","Siva Sankalp Patel","Odellia Boni","Yannis Katsis","Sara Rosenthal","Benjamin Sznajder","Marina Danilevsky"],"url":"https://arxiv.org/abs/2404.17347"}
{"created":"2025-05-05","title":"Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?","abstract":"Vision and language model (VLM) decoders are currently the best-performing architectures on multimodal tasks. Next to answers, they are able to produce natural language explanations, either in post-hoc or CoT settings. However, it is not clear to what extent they are using the input vision and text modalities when generating answers or explanations. In this work, we investigate if VLMs rely on their input modalities differently when they produce explanations as opposed to answers. We also evaluate the self-consistency of VLM decoders in both post-hoc and CoT explanation settings, by extending existing unimodal tests and measures to VLM decoders. We find that most tested VLMs are less self-consistent than LLMs. Text contributions in all tested VL decoders are more important than image contributions in all examined tasks. However, when comparing explanation generation to answer generation, the contributions of images are significantly stronger for generating explanations compared to answers. This difference is even larger in CoT compared to post-hoc explanations. Lastly, we provide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE benchmark, which before was restricted to VL encoders. We find that the tested VL decoders still struggle with most phenomena tested by VALSE.","authors":["Letitia Parcalabescu","Anette Frank"],"url":"https://arxiv.org/abs/2404.18624"}
{"created":"2025-05-05","title":"Machine learning of continuous and discrete variational ODEs with convergence guarantee and uncertainty quantification","abstract":"The article introduces a method to learn dynamical systems that are governed by Euler--Lagrange equations from data. The method is based on Gaussian process regression and identifies continuous or discrete Lagrangians and is, therefore, structure preserving by design. A rigorous proof of convergence as the distance between observation data points converges to zero and lower bounds for convergence rates are provided. Next to convergence guarantees, the method allows for quantification of model uncertainty, which can provide a basis of adaptive sampling techniques. We provide efficient uncertainty quantification of any observable that is linear in the Lagrangian, including of Hamiltonian functions (energy) and symplectic structures, which is of interest in the context of system identification. The article overcomes major practical and theoretical difficulties related to the ill-posedness of the identification task of (discrete) Lagrangians through a careful design of geometric regularisation strategies and through an exploit of a relation to convex minimisation problems in reproducing kernel Hilbert spaces.","authors":["Christian Offen"],"url":"https://arxiv.org/abs/2404.19626"}
{"created":"2025-05-05","title":"Part-aware Shape Generation with Latent 3D Diffusion of Neural Voxel Fields","abstract":"This paper presents a novel latent 3D diffusion model for the generation of neural voxel fields, aiming to achieve accurate part-aware structures. Compared to existing methods, there are two key designs to ensure high-quality and accurate part-aware generation. On one hand, we introduce a latent 3D diffusion process for neural voxel fields, enabling generation at significantly higher resolutions that can accurately capture rich textural and geometric details. On the other hand, a part-aware shape decoder is introduced to integrate the part codes into the neural voxel fields, guiding the accurate part decomposition and producing high-quality rendering results. Through extensive experimentation and comparisons with state-of-the-art methods, we evaluate our approach across four different classes of data. The results demonstrate the superior generative capabilities of our proposed method in part-aware shape generation, outperforming existing state-of-the-art methods.","authors":["Yuhang Huang","SHilong Zou","Xinwang Liu","Kai Xu"],"url":"https://arxiv.org/abs/2405.00998"}
{"created":"2025-05-05","title":"Logical Characterizations of Recurrent Graph Neural Networks with Reals and Floats","abstract":"In pioneering work from 2019, Barcel\\'o and coauthors identified logics that precisely match the expressive power of constant iteration-depth graph neural networks (GNNs) relative to properties definable in first-order logic. In this article, we give exact logical characterizations of recurrent GNNs in two scenarios: (1) in the setting with floating-point numbers and (2) with reals. For floats, the formalism matching recurrent GNNs is a rule-based modal logic with counting, while for reals we use a suitable infinitary modal logic, also with counting. These results give exact matches between logics and GNNs in the recurrent setting without relativising to a background logic in either case, but using some natural assumptions about floating-point arithmetic. Applying our characterizations, we also prove that, relative to graph properties definable in monadic second-order logic (MSO), our infinitary and rule-based logics are equally expressive. This implies that recurrent GNNs with reals and floats have the same expressive power over MSO-definable properties and shows that, for such properties, also recurrent GNNs with reals are characterized by a (finitary!) rule-based modal logic. In the general case, in contrast, the expressive power with floats is weaker than with reals. In addition to logic-oriented results, we also characterize recurrent GNNs, with both reals and floats, via distributed automata, drawing links to distributed computing models.","authors":["Veeti Ahvonen","Damian Heiman","Antti Kuusisto","Carsten Lutz"],"url":"https://arxiv.org/abs/2405.14606"}
{"created":"2025-05-05","title":"Robust Classification by Coupling Data Mollification with Label Smoothing","abstract":"Introducing training-time augmentations is a key technique to enhance generalization and prepare deep neural networks against test-time corruptions. Inspired by the success of generative diffusion models, we propose a novel approach of coupling data mollification, in the form of image noising and blurring, with label smoothing to align predicted label confidences with image degradation. The method is simple to implement, introduces negligible overheads, and can be combined with existing augmentations. We demonstrate improved robustness and uncertainty quantification on the corrupted image benchmarks of CIFAR, TinyImageNet and ImageNet datasets.","authors":["Markus Heinonen","Ba-Hien Tran","Michael Kampffmeyer","Maurizio Filippone"],"url":"https://arxiv.org/abs/2406.01494"}
{"created":"2025-05-05","title":"Position-based Rogue Access Point Detection","abstract":"Rogue Wi-Fi access point (AP) attacks can lead to data breaches and unauthorized access. Existing rogue AP detection methods and tools often rely on channel state information (CSI) or received signal strength indicator (RSSI), but they require specific hardware or achieve low detection accuracy. On the other hand, AP positions are typically fixed, and Wi-Fi can support indoor positioning of user devices. Based on this position information, the mobile platform can check if one (or more) AP in range is rogue. The inclusion of a rogue AP would in principle result in a wrong estimated position. Thus, the idea to use different subsets of APs: the positions computed based on subsets that include a rogue AP will be significantly different from those that do not. Our scheme contains two components: subset generation and position validation. First, we generate subsets of RSSIs from APs, which are then utilized for positioning, similar to receiver autonomous integrity monitoring (RAIM). Second, the position estimates, along with uncertainties, are combined into a Gaussian mixture, to check for inconsistencies by evaluating the overlap of the Gaussian components. Our comparative analysis, conducted on a real-world dataset with three types of attacks and synthetic RSSIs integrated, demonstrates a substantial improvement in rogue AP detection accuracy.","authors":["Wenjie Liu","Panos Papadimitratos"],"url":"https://arxiv.org/abs/2406.01927"}
{"created":"2025-05-05","title":"Linear Recurrence Sequence Automata and the Addition of Abstract Numeration Systems","abstract":"Abstract numeration systems encode natural numbers using radix ordered words of an infinite regular language and linear recurrence sequences play a key role in their valuation. Sequence automata, which are deterministic finite automata with an additional linear recurrence sequence on each transition, are introduced to compute various $\\mathbb{Z}$-rational non commutative formal series in abstract numeration systems. Under certain Pisot conditions on the recurrence sequences, the support of these series is regular. This property can be leveraged to derive various synchronized relations including a deterministic finite automaton that computes the addition relation of various Dumont-Thomas numeration systems and deterministic finite automata converting between various numeration systems. A practical implementation for Walnut is provided.","authors":["Olivier Carton","Jean-Michel Couvreur","Martin Delacourt","Nicolas Ollinger"],"url":"https://arxiv.org/abs/2406.09868"}
{"created":"2025-05-05","title":"Tree-Sliced Wasserstein Distance: A Geometric Perspective","abstract":"Many variants of Optimal Transport (OT) have been developed to address its heavy computation. Among them, notably, Sliced Wasserstein (SW) is widely used for application domains by projecting the OT problem onto one-dimensional lines, and leveraging the closed-form expression of the univariate OT to reduce the computational burden. However, projecting measures onto low-dimensional spaces can lead to a loss of topological information. To mitigate this issue, in this work, we propose to replace one-dimensional lines with a more intricate structure, called tree systems. This structure is metrizable by a tree metric, which yields a closed-form expression for OT problems on tree systems. We provide an extensive theoretical analysis to formally define tree systems with their topological properties, introduce the concept of splitting maps, which operate as the projection mechanism onto these structures, then finally propose a novel variant of Radon transform for tree systems and verify its injectivity. This framework leads to an efficient metric between measures, termed Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL). By conducting a variety of experiments on gradient flows, image style transfer, and generative models, we illustrate that our proposed approach performs favorably compared to SW and its variants.","authors":["Viet-Hoang Tran","Trang Pham","Tho Tran","Minh Khoi Nguyen Nhat","Thanh Chu","Tam Le","Tan M. Nguyen"],"url":"https://arxiv.org/abs/2406.13725"}
{"created":"2025-05-05","title":"Timely and Painless Breakups: Off-the-grid Blind Message Recovery and Users' Demixing","abstract":"In the near future, the Internet of Things will interconnect billions of devices, forming a vast network where users sporadically transmit short messages through multi-path wireless channels. These channels are characterized by the superposition of a small number of scaled and delayed copies of Dirac spikes. At the receiver, the observed signal is a sum of these convolved signals, and the task is to find the amplitudes, continuous-indexed delays, and transmitted messages from a single signal. This task is inherently ill-posed without additional assumptions on the channel or messages. In this work, we assume the channel exhibits sparsity in the delay domain and that i.i.d. random linear encoding is applied to the messages at the devices. Leveraging these assumptions, we propose a semidefinite programming optimization capable of simultaneously recovering both messages and the delay parameters of the channels from only a single received signal. Our theoretical analysis establishes that the required number of samples at the receiver scales proportionally to the sum-product of sparsity and message length of all users, aligning with the degrees of freedom in the proposed convex optimization framework. Numerical experiments confirm the efficacy of the proposed method in accurately estimating closely-spaced delay parameters and recovering messages.","authors":["Sajad Daei","Saeed Razavikia","Mikael Skoglund","Gabor Fodor","Carlo Fischione"],"url":"https://arxiv.org/abs/2406.17393"}
{"created":"2025-05-05","title":"Quadratic Differentiable Optimization For The Maximum Independent Set Problem","abstract":"Combinatorial Optimization (CO) addresses many important problems, including the challenging Maximum Independent Set (MIS) problem. Alongside exact and heuristic solvers, differentiable approaches have emerged, often using continuous relaxations of ReLU-based or quadratic objectives. Noting that an MIS in a graph is a Maximum Clique (MC) in its complement, we propose a new quadratic formulation for MIS by incorporating an MC term, improving convergence and exploration. We show that every maximal independent set corresponds to a local minimizer, derive conditions with respect to the MIS size, and characterize stationary points. To tackle the non-convexity of the objective, we propose optimizing several initializations in parallel using momentum-based gradient descent, complemented by an efficient MIS checking criterion derived from our theory. We dub our method as **p**arallelized **C**lique-Informed **Q**uadratic **O**ptimization for MIS (**pCQO-MIS**). Our experimental results demonstrate the effectiveness of the proposed method compared to exact, heuristic, sampling, and data-centric approaches. Notably, our method avoids the out-of-distribution tuning and reliance on (un)labeled data required by data-centric methods, while achieving superior MIS sizes and competitive runtime relative to their inference time. Additionally, a key advantage of pCQO-MIS is that, unlike exact and heuristic solvers, the runtime scales only with the number of nodes in the graph, not the number of edges. Our code is available at the GitHub repository \\href{https://github.com/ledenmat/pCQO-mis-benchmark/tree/refactor}{{{pCQO-MIS}}}","authors":["Ismail Alkhouri","Cedric Le Denmat","Yingjie Li","Cunxi Yu","Jia Liu","Rongrong Wang","Alvaro Velasquez"],"url":"https://arxiv.org/abs/2406.19532"}
{"created":"2025-05-05","title":"Stacked Intelligent Metasurfaces for Wireless Communications: Applications and Challenges","abstract":"The rapid growth of wireless communications has created a significant demand for high throughput, seamless connectivity, and extremely low latency. To meet these goals, a novel technology -- stacked intelligent metasurfaces (SIMs) -- has been developed to perform signal processing by directly utilizing electromagnetic waves, thus achieving incredibly fast computing speed while reducing hardware requirements. In this article, we provide an overview of SIM technology, including its underlying hardware, benefits, and exciting applications in wireless communications. Specifically, we examine the utilization of SIMs in realizing transmit beamforming and semantic encoding in the wave domain. Additionally, channel estimation in SIM-aided communication systems is discussed. Finally, we highlight potential research opportunities and identify key challenges for deploying SIMs in wireless networks to motivate future research.","authors":["Hao Liu","Jiancheng An","Xing Jia","Lu Gan","George K. Karagiannidis","Bruno Clerckx","Mehdi Bennis","M\\'erouane Debbah","Tie Jun Cui"],"url":"https://arxiv.org/abs/2407.03566"}
{"created":"2025-05-05","title":"Beamforming Design for Joint Target Sensing and Proactive Eavesdropping","abstract":"This work studies the beamforming design in the joint target sensing and proactive eavesdropping (JTSAPE) system. The JTSAPE base station (BS) receives the information transmitted by the illegal transmitter and transmits the waveform for target sensing. The shared waveform also serves as artificial noise to interfere with the illegal receiver, thereby achieving proactive eavesdropping. We firstly optimize the transmitting beam of the BS to maximize the eavesdropping signal-to-interference-plus-noise ratio or minimize the target estimation parameter Cram{\\'{e}}r-Rao bound, respectively. Then, the joint optimization of proactive eavesdropping and target sensing is investigated, and the normalized weighted optimization problem is formulated. To address the complexity of the original problem, the formulated problem is decomposed into two subproblems: proactive eavesdropping and target sensing, which are solved by the semi-definite relaxation technique. Furthermore, the scenario in which the quality of the eavesdropping channel is stronger than that of the illegal channel is considered. We utilize the sequential rank-one constraint relaxation method and iteration technique to obtain the high-quality suboptimal solution of the beam transmit covariance matrix. Numerical simulation shows the effectiveness of our proposed algorithm.","authors":["Qian Dan","Hongjiang Lei","Ki-Hong Park","Gaofeng Pan","Mohamed-Slim Alouini"],"url":"https://arxiv.org/abs/2407.06521"}
{"created":"2025-05-05","title":"The syzygy distinguisher","abstract":"We present a new distinguisher for alternant and Goppa codes, whose complexity is subexponential in the error-correcting capability, hence better than that of generic decoding algorithms. Moreover it does not suffer from the strong regime limitations of the previous distinguishers or structure recovery algorithms: in particular, it applies to the codes used in the Classic McEliece candidate for postquantum cryptography standardization. The invariants that allow us to distinguish are graded Betti numbers of the homogeneous coordinate ring of a shortening of the dual code.","authors":["Hugues Randriambololona"],"url":"https://arxiv.org/abs/2407.15740"}
{"created":"2025-05-05","title":"Visual-Friendly Concept Protection via Selective Adversarial Perturbations","abstract":"Personalized concept generation by tuning diffusion models with a few images raises potential legal and ethical concerns regarding privacy and intellectual property rights. Researchers attempt to prevent malicious personalization using adversarial perturbations. However, previous efforts have mainly focused on the effectiveness of protection while neglecting the visibility of perturbations. They utilize global adversarial perturbations, which introduce noticeable alterations to original images and significantly degrade visual quality. In this work, we propose the Visual-Friendly Concept Protection (VCPro) framework, which prioritizes the protection of key concepts chosen by the image owner through adversarial perturbations with lower perceptibility. To ensure these perturbations are as inconspicuous as possible, we introduce a relaxed optimization objective to identify the least perceptible yet effective adversarial perturbations, solved using the Lagrangian multiplier method. Qualitative and quantitative experiments validate that VCPro achieves a better trade-off between the visibility of perturbations and protection effectiveness, effectively prioritizing the protection of target concepts in images with less perceptible perturbations.","authors":["Xiaoyue Mi","Fan Tang","Juan Cao","Peng Li","Yang Liu"],"url":"https://arxiv.org/abs/2408.08518"}
{"created":"2025-05-05","title":"MoDeGPT: Modular Decomposition for Large Language Model Compression","abstract":"Large Language Models (LLMs) have reshaped the landscape of artificial intelligence by demonstrating exceptional performance across various tasks. However, substantial computational requirements make their deployment challenging on devices with limited resources. Recently, compression methods using low-rank matrix techniques have shown promise, yet these often lead to degraded accuracy or introduce significant overhead in parameters and inference latency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition (MoDeGPT), a novel structured compression framework that does not need recovery fine-tuning while resolving the above drawbacks. MoDeGPT partitions the Transformer block into modules comprised of matrix pairs and reduces the hidden dimensions via reconstructing the module-level outputs. MoDeGPT is developed based on a theoretical framework that utilizes three well-established matrix decomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD -- and applies them to our redefined transformer modules. Our comprehensive experiments show MoDeGPT, without backward propagation, matches or surpasses previous structured compression methods that rely on gradient information, and saves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3 and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30% compression rates. Moreover, the compression can be done on a single GPU within a few hours and increases the inference throughput by up to 46%.","authors":["Chi-Heng Lin","Shangqian Gao","James Seale Smith","Abhishek Patel","Shikhar Tuli","Yilin Shen","Hongxia Jin","Yen-Chang Hsu"],"url":"https://arxiv.org/abs/2408.09632"}
{"created":"2025-05-05","title":"Towards Aligned Data Removal via Twin Machine Unlearning","abstract":"Modern privacy regulations have spurred the evolution of machine unlearning, a technique that enables the removal of data from an already trained ML model without requiring retraining from scratch. Previous unlearning methods tend to induce the model to achieve lowest classification accuracy on the removal data. Nonetheless, the authentic objective of machine unlearning is to align the unlearned model with the gold model, i.e., achieving the same classification accuracy as the gold model. For this purpose, we present a Twin Machine Unlearning (TMU) approach, where a twin unlearning problem is defined corresponding to the original unlearning problem. As a results, the generalization-label predictor trained on the twin problem can be transferred to the original problem, facilitating aligned data removal. Comprehensive empirical experiments illustrate that our approach significantly enhances the alignment between the unlearned model and the gold model. Meanwhile, our method allows data removal without compromising the model accuracy.","authors":["Haoxuan Ji","Zheng Lin","Yuyao Sun","Gao Fei","Yuhang Wang","Haichang Gao","Zhenxing Niu"],"url":"https://arxiv.org/abs/2408.11433"}
{"created":"2025-05-05","title":"Differential Confounding Privacy and Inverse Composition","abstract":"Differential privacy (DP) has become the gold standard for privacy-preserving data analysis, but its applicability can be limited in scenarios involving complex dependencies between sensitive information and datasets. To address this, we introduce \\textit{differential confounding privacy} (DCP), a specialized form of the Pufferfish privacy (PP) framework that generalizes DP by accounting for broader relationships between sensitive information and datasets. DCP adopts the $(\\epsilon, \\delta)$-indistinguishability framework to quantify privacy loss. We show that while DCP mechanisms retain privacy guarantees under composition, they lack the graceful compositional properties of DP. To overcome this, we propose an \\textit{Inverse Composition (IC)} framework, where a leader-follower model optimally designs a privacy strategy to achieve target guarantees without relying on worst-case privacy proofs, such as sensitivity calculation. Experimental results validate IC's effectiveness in managing privacy budgets and ensuring rigorous privacy guarantees under composition.","authors":["Tao Zhang","Bradley A. Malin","Netanel Raviv","Yevgeniy Vorobeychik"],"url":"https://arxiv.org/abs/2408.12010"}
{"created":"2025-05-05","title":"REFFLY: Melody-Constrained Lyrics Editing Model","abstract":"Automatic melody-to-lyric (M2L) generation aims to create lyrics that align with a given melody. While most previous approaches generate lyrics from scratch, revision, editing plain text draft to fit it into the melody, offers a much more flexible and practical alternative. This enables broad applications, such as generating lyrics from flexible inputs (keywords, themes, or full text that needs refining to be singable), song translation (preserving meaning across languages while keeping the melody intact), or style transfer (adapting lyrics to different genres). This paper introduces REFFLY (REvision Framework For LYrics), the first revision framework for editing and generating melody-aligned lyrics. We train the lyric revision module using our curated synthesized melody-aligned lyrics dataset, enabling it to transform plain text into lyrics that align with a given melody. To further enhance the revision ability, we propose training-free heuristics aimed at preserving both semantic meaning and musical consistency throughout the editing process. Experimental results demonstrate the effectiveness of REFFLY across various tasks (e.g. lyrics generation, song translation), showing that our model outperforms strong baselines, including Lyra (Tian et al., 2023) and GPT-4, by 25% in both musicality and text quality.","authors":["Songyan Zhao","Bingxuan Li","Yufei Tian","Nanyun Peng"],"url":"https://arxiv.org/abs/2409.00292"}
{"created":"2025-05-05","title":"UDGS-SLAM : UniDepth Assisted Gaussian Splatting for Monocular SLAM","abstract":"Recent advancements in monocular neural depth estimation, particularly those achieved by the UniDepth network, have prompted the investigation of integrating UniDepth within a Gaussian splatting framework for monocular SLAM. This study presents UDGS-SLAM, a novel approach that eliminates the necessity of RGB-D sensors for depth estimation within Gaussian splatting framework. UDGS-SLAM employs statistical filtering to ensure local consistency of the estimated depth and jointly optimizes camera trajectory and Gaussian scene representation parameters. The proposed method achieves high-fidelity rendered images and low ATERMSE of the camera trajectory. The performance of UDGS-SLAM is rigorously evaluated using the TUM RGB-D dataset and benchmarked against several baseline methods, demonstrating superior performance across various scenarios. Additionally, an ablation study is conducted to validate design choices and investigate the impact of different network backbone encoders on system performance.","authors":["Mostafa Mansour","Ahmed Abdelsalam","Ari Happonen","Jari Porras","Esa Rahtu"],"url":"https://arxiv.org/abs/2409.00362"}
{"created":"2025-05-05","title":"Asynchronous Stochastic Approximation and Average-Reward Reinforcement Learning","abstract":"This paper studies asynchronous stochastic approximation (SA) algorithms and their theoretical application to reinforcement learning in semi-Markov decision processes (SMDPs) with an average-reward criterion. We first extend Borkar and Meyn's stability proof method to accommodate more general noise conditions, yielding broader convergence guarantees for asynchronous SA. To sharpen the convergence analysis, we further examine shadowing properties in the asynchronous setting, building on a dynamical-systems approach of Hirsch and Bena\\\"{i}m. Leveraging these SA results, we establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. Moreover, to make full use of these SA results in this application, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework, and we address them with novel arguments in the stability and convergence analysis of RVI Q-learning.","authors":["Huizhen Yu","Yi Wan","Richard S. Sutton"],"url":"https://arxiv.org/abs/2409.03915"}
{"created":"2025-05-05","title":"Reward Guidance for Reinforcement Learning Tasks Based on Large Language Models: The LMGT Framework","abstract":"The inherent uncertainty in the environmental transition model of Reinforcement Learning (RL) necessitates a delicate balance between exploration and exploitation. This balance is crucial for optimizing computational resources to accurately estimate expected rewards for the agent. In scenarios with sparse rewards, such as robotic control systems, achieving this balance is particularly challenging. However, given that many environments possess extensive prior knowledge, learning from the ground up in such contexts may be redundant. To address this issue, we propose Language Model Guided reward Tuning (LMGT), a novel, sample-efficient framework. LMGT leverages the comprehensive prior knowledge embedded in Large Language Models (LLMs) and their proficiency in processing non-standard data forms, such as wiki tutorials. By utilizing LLM-guided reward shifts, LMGT adeptly balances exploration and exploitation, thereby guiding the agent's exploratory behavior and enhancing sample efficiency. We have rigorously evaluated LMGT across various RL tasks and evaluated it in the embodied robotic environment Housekeep. Our results demonstrate that LMGT consistently outperforms baseline methods. Furthermore, the findings suggest that our framework can substantially reduce the computational resources required during the RL training phase.","authors":["Yongxin Deng","Xihe Qiu","Jue Chen","Xiaoyu Tan"],"url":"https://arxiv.org/abs/2409.04744"}
{"created":"2025-05-05","title":"Differentially Private High-Dimensional Approximate Range Counting, Revisited","abstract":"Locality Sensitive Filters are known for offering a quasi-linear space data structure with rigorous guarantees for the Approximate Near Neighbor search (ANN) problem. Building on Locality Sensitive Filters, we derive a simple data structure for the Approximate Near Neighbor Counting (ANNC) problem under differential privacy (DP). Moreover, we provide a simple analysis leveraging a connection with concomitant statistics and extreme value theory. Our approach produces a simple data structure with a tunable parameter that regulates a trade-off between space-time and utility. Through this trade-off, our data structure achieves the same performance as the recent findings of Andoni et al. (NeurIPS 2023) while offering better utility at the cost of higher space and query time. In addition, we provide a more efficient algorithm under pure $\\varepsilon$-DP and elucidate the connection between ANN and differentially private ANNC. As a side result, the paper provides a more compact description and analysis of Locality Sensitive Filters for Fair Near Neighbor Search, improving a previous result in Aum\\\"{u}ller et al. (TODS 2022).","authors":["Martin Aum\\\"uller","Fabrizio Boninsegna","Francesco Silvestri"],"url":"https://arxiv.org/abs/2409.07187"}
{"created":"2025-05-05","title":"Generative Semantic Communication via Textual Prompts: Latency Performance Tradeoffs","abstract":"This paper develops an edge-device collaborative Generative Semantic Communications (Gen SemCom) framework leveraging pre-trained Multi-modal/Vision Language Models (M/VLMs) for ultra-low-rate semantic communication via textual prompts. The proposed framework optimizes the use of M/VLMs on the wireless edge/device to generate high-fidelity textual prompts through visual captioning/question answering, which are then transmitted over a wireless channel for SemCom. Specifically, we develop a multi-user Gen SemCom framework using pre-trained M/VLMs, and formulate a joint optimization problem of prompt generation offloading, communication and computation resource allocation to minimize the latency and maximize the resulting semantic quality. Due to the nonconvex nature of the problem with highly coupled discrete and continuous variables, we decompose it as a two-level problem and propose a low-complexity swap/leaving/joining (SLJ)-based matching algorithm. Simulation results demonstrate significant performance improvements over the conventional semanticunaware/non-collaborative offloading benchmarks.","authors":["Mengmeng Ren","Li Qiao","Long Yang","Zhen Gao","Jian Chen","Mahdi Boloursaz Mashhadi","Pei Xiao","Rahim Tafazolli","Mehdi Bennis"],"url":"https://arxiv.org/abs/2409.09715"}
{"created":"2025-05-05","title":"Leveraging Symmetry to Accelerate Learning of Trajectory Tracking Controllers for Free-Flying Robotic Systems","abstract":"Tracking controllers enable robotic systems to accurately follow planned reference trajectories. In particular, reinforcement learning (RL) has shown promise in the synthesis of controllers for systems with complex dynamics and modest online compute budgets. However, the poor sample efficiency of RL and the challenges of reward design make training slow and sometimes unstable, especially for high-dimensional systems. In this work, we leverage the inherent Lie group symmetries of robotic systems with a floating base to mitigate these challenges when learning tracking controllers. We model a general tracking problem as a Markov decision process (MDP) that captures the evolution of both the physical and reference states. Next, we prove that symmetry in the underlying dynamics and running costs leads to an MDP homomorphism, a mapping that allows a policy trained on a lower-dimensional \"quotient\" MDP to be lifted to an optimal tracking controller for the original system. We compare this symmetry-informed approach to an unstructured baseline, using Proximal Policy Optimization (PPO) to learn tracking controllers for three systems: the Particle (a forced point mass), the Astrobee (a fullyactuated space robot), and the Quadrotor (an underactuated system). Results show that a symmetry-aware approach both accelerates training and reduces tracking error at convergence.","authors":["Jake Welde","Nishanth Rao","Pratik Kunapuli","Dinesh Jayaraman","Vijay Kumar"],"url":"https://arxiv.org/abs/2409.11238"}
{"created":"2025-05-05","title":"Parameterised Holant Problems","abstract":"We investigate the complexity of parameterised holant problems p-$\\mathrm{Holant}(\\mathcal{S})$ for families of signatures $\\mathcal{S}$. The parameterised holant framework was introduced by Curticapean in 2015 as a counter-part to the classical theory of holographic reductions and algorithms and it constitutes an extensive family of coloured and weighted counting constraint satisfaction problems on graph-like structures, encoding as special cases various well-studied counting problems in parameterised and fine-grained complexity theory such as counting edge-colourful $k$-matchings, graph-factors, Eulerian orientations or, subgraphs with weighted degree constraints. We establish an exhaustive complexity trichotomy along the set of signatures $\\mathcal{S}$: Depending on $\\mathcal{S}$, p-$\\mathrm{Holant}(\\mathcal{S})$ is: (1) solvable in FPT-near-linear time (i.e. $f(k)\\cdot \\tilde{\\mathcal{O}}(|x|)$); (2) solvable in \"FPT-matrix-multiplication time\" (i.e. $f(k)\\cdot {\\mathcal{O}}(n^{\\omega})$) but not solvable in FPT-near-linear time unless the Triangle Conjecture fails; or (3) #W[1]-complete and no significant improvement over brute force is possible unless ETH fails. This classification reveals a significant and surprising gap in the complexity landscape of parameterised Holants: Not only is every instance either fixed-parameter tractable or #W[1]-complete, but additionally, every FPT instance is solvable in time $f(k)\\cdot {\\mathcal{O}}(n^{\\omega})$.","authors":["Panagiotis Aivasiliotis","Andreas G\\\"obel","Marc Roth","Johannes Schmitt"],"url":"https://arxiv.org/abs/2409.13579"}
{"created":"2025-05-05","title":"Cyber Food Swamps: Investigating the Impacts of Online-to-Offline Food Delivery Platforms on Healthy Food Choices","abstract":"Online-to-offline (O2O) food delivery platforms have greatly expanded urban residents' access to a wide range of food options by allowing convenient ordering from distant food outlets. However, concerns persist regarding the nutritional quality of delivered food, particularly as the impact of O2O food delivery platforms on users' healthy food remains unclear. This study leverages large-scale empirical data from a leading O2O delivery platform to comprehensively analyze online food choice behaviors and how they are influenced by the online exposure to fast food restaurants, i.e., online food environment. Our analyses reveal significant variations in food preferences across demographic groups and city sizes, where male, low-income, and younger users are more likely to order fast food via O2O platforms. Besides, we also perform a comparative analysis on the food exposure differences in offline and online environments, confirming that the extended service ranges of O2O platforms can create larger \"cyber food swamps\". Furthermore, regression analysis highlights that a higher ratio of fast food orders is associated with \"cyber food swamps\", areas characterized by a higher proportion of accessible fast food restaurants. A 10% increase in this proportion raises the probability of ordering fast food by 22.0%. Moreover, a quasi-natural experiment substantiates the long-term causal effect of online food environment changes on healthy food choices. These findings underscore the need for O2O food delivery platforms to address the health implications of online food choice exposure, offering critical insights for stakeholders aiming to improve dietary health among urban populations.","authors":["Yunke Zhang","Yiran Fan","Peijie Liu","Fengli Xu","Yong Li"],"url":"https://arxiv.org/abs/2409.16601"}
{"created":"2025-05-05","title":"Slowly Scaling Per-Record Differential Privacy","abstract":"We develop formal privacy mechanisms for releasing statistics from data with many outlying values, such as income data. These mechanisms ensure that a per-record differential privacy guarantee degrades slowly in the protected records' influence on the statistics being released.","authors":["Brian Finley","Anthony M Caruso","Justin C Doty","Ashwin Machanavajjhala","Mikaela R Meyer","David Pujol","William Sexton","Zachary Terner"],"url":"https://arxiv.org/abs/2409.18118"}
{"created":"2025-05-05","title":"Closed-Loop Long-Horizon Robotic Planning via Equilibrium Sequence Modeling","abstract":"In the endeavor to make autonomous robots take actions, task planning is a major challenge that requires translating high-level task descriptions to long-horizon action sequences. Despite recent advances in language model agents, they remain prone to planning errors and limited in their ability to plan ahead. To address these limitations in robotic planning, we advocate a self-refining scheme that iteratively refines a draft plan until an equilibrium is reached. Remarkably, this process can be optimized end-to-end from an analytical perspective without the need to curate additional verifiers or reward models, allowing us to train self-refining planners in a simple supervised learning fashion. Meanwhile, a nested equilibrium sequence modeling procedure is devised for efficient closed-loop planning that incorporates useful feedback from the environment (or an internal world model). Our method is evaluated on the VirtualHome-Env benchmark, showing advanced performance with improved scaling w.r.t. inference-time computation. Code is available at https://github.com/Singularity0104/equilibrium-planner.","authors":["Jinghan Li","Zhicheng Sun","Yadong Mu"],"url":"https://arxiv.org/abs/2410.01440"}
{"created":"2025-05-05","title":"HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration","abstract":"Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\\%$ latency reduction (i.e., $2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$. Remarkably, our image-free approach reduces training time by $25\\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.","authors":["Yushi Huang","Zining Wang","Ruihao Gong","Jing Liu","Xinjie Zhang","Jinyang Guo","Xianglong Liu","Jun Zhang"],"url":"https://arxiv.org/abs/2410.01723"}
{"created":"2025-05-05","title":"$X^2$-DFD: A framework for eXplainable and eXtendable Deepfake Detection","abstract":"Detecting deepfakes has become an important task. Most existing detection methods provide only real/fake predictions without offering human-comprehensible explanations. Recent studies leveraging MLLMs for deepfake detection have shown improvements in explainability. However, the performance of pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of understanding of their capabilities for this task and strategies to enhance them. In this work, we empirically assess the strengths and weaknesses of MLLMs specifically in deepfake detection via forgery features analysis. Building on these assessments, we propose a novel framework called ${X}^2$-DFD, consisting of three core modules. The first module, Model Feature Assessment (MFA), measures the detection capabilities of forgery features intrinsic to MLLMs, and gives a descending ranking of these features. The second module, Strong Feature Strengthening (SFS), enhances the detection and explanation capabilities by fine-tuning the MLLM on a dataset constructed based on the top-ranked features. The third module, Weak Feature Supplementing (WFS), improves the fine-tuned MLLM's capabilities on lower-ranked features by integrating external dedicated deepfake detectors. To verify the effectiveness of this framework, we further present a practical implementation, where an automated forgery features generation, evaluation, and ranking procedure is designed for MFA module; an automated generation procedure of the fine-tuning dataset containing real and fake images with explanations based on top-ranked features is developed for SFS model; an external conventional deepfake detector focusing on blending artifact, which corresponds to a low detection capability in the pre-trained MLLM, is integrated for WFS module. Experiments show that our approach enhances both detection and explanation performance.","authors":["Yize Chen","Zhiyuan Yan","Siwei Lyu","Baoyuan Wu"],"url":"https://arxiv.org/abs/2410.06126"}
{"created":"2025-05-05","title":"Stabilization of Predator-Prey Age-Structured Hyperbolic PDE when Harvesting both Species is Inevitable","abstract":"Populations do not only interact over time but also age over time. It is therefore common to model them as age-structured PDEs, where age is the space variable. Since the models also involve integrals over age, both in the birth process and in the interaction among species, they are in fact integro-partial differential equations (IPDEs) with positive states. To regulate the population densities to desired profiles, harvesting is used as input. But non-discriminating harvesting, where wanting to repress one species will inevitably repress the other species as well, the positivity restriction on the input (no insertion of population), and the multiplicative nature of harvesting, makes control challenging even for ODE versions of such dynamics, let alone for their IPDE versions on an infinite-dimensional nonnegative state space.","authors":["Carina Veil","Miroslav Krsti\\'c","Iasson Karafyllis","Mamadou Diagne","Oliver Sawodny"],"url":"https://arxiv.org/abs/2410.06823"}
{"created":"2025-05-05","title":"Offline Model-Based Optimization by Learning to Rank","abstract":"Offline model-based optimization (MBO) aims to identify a design that maximizes a black-box function using only a fixed, pre-collected dataset of designs and their corresponding scores. A common approach in offline MBO is to train a regression-based surrogate model by minimizing mean squared error (MSE) and then find the best design within this surrogate model by different optimizers (e.g., gradient ascent). However, a critical challenge is the risk of out-of-distribution errors, i.e., the surrogate model may typically overestimate the scores and mislead the optimizers into suboptimal regions. Prior works have attempted to address this issue in various ways, such as using regularization techniques and ensemble learning to enhance the robustness of the model, but it still remains. In this paper, we argue that regression models trained with MSE are not well-aligned with the primary goal of offline MBO, which is to select promising designs rather than to predict their scores precisely. Notably, if a surrogate model can maintain the order of candidate designs based on their relative score relationships, it can produce the best designs even without precise predictions. To validate it, we conduct experiments to compare the relationship between the quality of the final designs and MSE, finding that the correlation is really very weak. In contrast, a metric that measures order-maintaining quality shows a significantly stronger correlation. Based on this observation, we propose learning a ranking-based model that leverages learning to rank techniques to prioritize promising designs based on their relative scores. We show that the generalization error on ranking loss can be well bounded. Empirical results across diverse tasks demonstrate the superior performance of our proposed ranking-based models than twenty existing methods.","authors":["Rong-Xi Tan","Ke Xue","Shen-Huan Lyu","Haopu Shang","Yao Wang","Yaoyuan Wang","Sheng Fu","Chao Qian"],"url":"https://arxiv.org/abs/2410.11502"}
{"created":"2025-05-05","title":"Caging in Time: A Framework for Robust Object Manipulation under Uncertainties and Limited Robot Perception","abstract":"Real-world object manipulation has been commonly challenged by physical uncertainties and perception limitations. Being an effective strategy, while caging configuration-based manipulation frameworks have successfully provided robust solutions, they are not broadly applicable due to their strict requirements on the availability of multiple robots, widely distributed contacts, or specific geometries of robots or objects.","authors":["Gaotian Wang","Kejia Ren","Andrew S. Morgan","Kaiyu Hang"],"url":"https://arxiv.org/abs/2410.16481"}
{"created":"2025-05-05","title":"EmoGene: Audio-Driven Emotional 3D Talking-Head Generation","abstract":"Audio-driven talking-head generation is a crucial and useful technology for virtual human interaction and film-making. While recent advances have focused on improving image fidelity and lip synchronization, generating accurate emotional expressions remains underexplored. In this paper, we introduce EmoGene, a novel framework for synthesizing high-fidelity, audio-driven video portraits with accurate emotional expressions. Our approach employs a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks, which are concatenated with emotional embedding in a motion-to-emotion module to produce emotional landmarks. These landmarks drive a Neural Radiance Fields (NeRF)-based emotion-to-video module to render realistic emotional talking-head videos. Additionally, we propose a pose sampling method to generate natural idle-state (non-speaking) videos for silent audio inputs. Extensive experiments demonstrate that EmoGene outperforms previous methods in generating high-fidelity emotional talking-head videos.","authors":["Wenqing Wang","Yun Fu"],"url":"https://arxiv.org/abs/2410.17262"}
{"created":"2025-05-05","title":"Learning Transparent Reward Models via Unsupervised Feature Selection","abstract":"In complex real-world tasks such as robotic manipulation and autonomous driving, collecting expert demonstrations is often more straightforward than specifying precise learning objectives and task descriptions. Learning from expert data can be achieved through behavioral cloning or by learning a reward function, i.e., inverse reinforcement learning. The latter allows for training with additional data outside the training distribution, guided by the inferred reward function. We propose a novel approach to construct compact and transparent reward models from automatically selected state features. These inferred rewards have an explicit form and enable the learning of policies that closely match expert behavior by training standard reinforcement learning algorithms from scratch. We validate our method's performance in various robotic environments with continuous and high-dimensional state spaces. Webpage: \\url{https://sites.google.com/view/transparent-reward}.","authors":["Daulet Baimukashev","Gokhan Alcan","Kevin Sebastian Luck","Ville Kyrki"],"url":"https://arxiv.org/abs/2410.18608"}
{"created":"2025-05-05","title":"DivShift: Exploring Domain-Specific Distribution Shifts in Large-Scale, Volunteer-Collected Biodiversity Datasets","abstract":"Large-scale, volunteer-collected datasets of community-identified natural world imagery like iNaturalist have enabled marked performance gains for fine-grained visual classification of species using machine learning methods. However, such data -- sometimes referred to as citizen science data -- are opportunistic and lack a structured sampling strategy. This volunteer-collected biodiversity data contains geographic, temporal, taxonomic, observers, and sociopolitical biases that can have significant effects on biodiversity model performance, but whose impacts are unclear for fine-grained species recognition performance. Here we introduce Diversity Shift (DivShift), a framework for quantifying the effects of domain-specific distribution shifts on machine learning model performance. To diagnose the performance effects of biases specific to volunteer-collected biodiversity data, we also introduce DivShift - North American West Coast (DivShift-NAWC), a curated dataset of almost 7.5 million iNaturalist images across the western coast of North America partitioned across five types of expert-verified bias. We compare species recognition performance across these bias partitions using a diverse variety of species- and ecosystem-focused accuracy metrics. We observe that these biases confound model performance less than expected from the underlying label distribution shift, and that more data leads to better model performance but the magnitude of these improvements are bias-specific. These findings imply that while the structure within natural world images provides generalization improvements for biodiversity monitoring tasks, the biases present in volunteer-collected biodiversity data can also affect model performance; thus these models should be used with caution in downstream biodiversity monitoring tasks.","authors":["Elena Sierra","Lauren E. Gillespie","Salim Soltani","Moises Exposito-Alonso","Teja Kattenborn"],"url":"https://arxiv.org/abs/2410.19816"}
{"created":"2025-05-05","title":"Random Policy Enables In-Context Reinforcement Learning within Trust Horizons","abstract":"Pretrained foundation models have exhibited extraordinary in-context learning performance, allowing zero-shot generalization to new tasks not encountered during pretraining. In the case of reinforcement learning (RL), in-context RL (ICRL) emerges when pretraining FMs on decision-making problems in an autoregressive-supervised manner. Nevertheless, current state-of-the-art ICRL algorithms, like Algorithm Distillation, Decision Pretrained Transformer and Decision Importance Transformer, impose stringent requirements on the pretraining dataset concerning the source policies, context information, and action labels. Notably, these algorithms either demand optimal policies or require varying degrees of well-trained behavior policies for all pretraining environments. This significantly hinders the application of ICRL to real-world scenarios, where acquiring optimal or well-trained policies for a substantial volume of real-world training environments can be intractable. To overcome this challenge, we introduce a novel approach, termed State-Action Distillation (SAD), that allows to generate an effective pretraining dataset guided solely by random policies. In particular, SAD selects query states and corresponding action labels by distilling outstanding state-action pairs from the entire state and action spaces by using random policies within a trust horizon, and then inherits the classical autoregressive-supervised mechanism during pretraining. To the best of our knowledge, this is the first work that enables effective ICRL under random policies and random contexts. We also establish quantitative analysis of the trustworthiness as well as the performance guarantees of SAD. Moreover, our empirical results across multiple popular ICRL benchmark environments demonstrate that, on average, SAD outperforms the best baseline by 236.3% in the offline evaluation and by 135.2% in the online evaluation.","authors":["Weiqin Chen","Santiago Paternain"],"url":"https://arxiv.org/abs/2410.19982"}
{"created":"2025-05-05","title":"Agentic Feedback Loop Modeling Improves Recommendation and User Simulation","abstract":"Large language model-based agents are increasingly applied in the recommendation field due to their extensive knowledge and strong planning capabilities. While prior research has primarily focused on enhancing either the recommendation agent or the user agent individually, the collaborative interaction between the two has often been overlooked. Towards this research gap, we propose a novel framework that emphasizes the feedback loop process to facilitate the collaboration between the recommendation agent and the user agent. Specifically, the recommendation agent refines its understanding of user preferences by analyzing the feedback from the user agent on the item recommendation. Conversely, the user agent further identifies potential user interests based on the items and recommendation reasons provided by the recommendation agent. This iterative process enhances the ability of both agents to infer user behaviors, enabling more effective item recommendations and more accurate user simulations. Extensive experiments on three datasets demonstrate the effectiveness of the agentic feedback loop: the agentic feedback loop yields an average improvement of 11.52% over the single recommendation agent and 21.12% over the single user agent. Furthermore, the results show that the agentic feedback loop does not exacerbate popularity or position bias, which are typically amplified by the real-world feedback loop, highlighting its robustness. The source code is available at https://github.com/Lanyu0303/AFL.","authors":["Shihao Cai","Jizhi Zhang","Keqin Bao","Chongming Gao","Qifan Wang","Fuli Feng","Xiangnan He"],"url":"https://arxiv.org/abs/2410.20027"}
{"created":"2025-05-05","title":"LLM-PySC2: Starcraft II learning environment for Large Language Models","abstract":"The tremendous potential has been demonstrated by large language models (LLMs) in intelligent decision-making problems, with unprecedented capabilities shown across diverse applications ranging from gaming AI systems to complex strategic planning frameworks. However, the StarCraft II platform, which has been widely adopted for validating decision-making algorithms in the past decade, has not yet provided substantial support for this emerging domain. To address issues that LLMs cannot interface with the hundreds of actions of the pysc2 backend and the lack of native support for multi-agent (MA) collaboration, we propose the LLM-PySC2 environment. This is the first environment that offers LLMs the complete pysc2 action space with sufficient multi-modal information and game Wiki knowledge. With an asynchronous query architecture, the environment efficiently interacts with LLMs that maintain a constant latency regardless of the scale of the agents' population. In the experiments, we evaluated LLMs' decision-making performance in both the macro-decision and micro-operation scenarios, with traditional StarCraft II Multi-Agent Challenge (SMAC) tasks and a series of new proposed. Results indicate that LLMs possess the potential to achieve victories in complex scenarios but cannot constantly generate correct decisions, especially in the recovered pysc2 action space and MA settings. Without task-relevant instructions, the pre-trained models suffer from issues such as hallucinations and inefficient collaboration. Our findings suggest that StarCraft II still challenges in the era of large models, revealing that there is a lot to do to develop an advanced LLM decision-making system, and the proposed LLM-PySC2 environment will support future development of LLM-based decision-making solutions.","authors":["Zongyuan Li","Yanan Ni","Runnan Qi","Lumin Jiang","Chang Lu","Xiaojie Xu","Xiangbei Liu","Pengfei Li","Yunzheng Guo","Zhe Ma","Huanyu Li","Hui Wu","Xian Guo","Kuihua Huang","Xuebo Zhang"],"url":"https://arxiv.org/abs/2411.05348"}
{"created":"2025-05-05","title":"StiffGIPC: Advancing GPU IPC for stiff affine-deformable simulation","abstract":"Incremental Potential Contact (IPC) is a widely used, robust, and accurate method for simulating complex frictional contact behaviors. However, achieving high efficiency remains a major challenge, particularly as material stiffness increases, which leads to slower Preconditioned Conjugate Gradient (PCG) convergence, even with the state-of-the-art preconditioners. In this paper, we propose a fully GPU-optimized IPC simulation framework capable of handling materials across a wide range of stiffnesses, delivering consistent high performance and scalability with up to 10x speedup over state-of-the-art GPU IPC methods. Our framework introduces three key innovations: 1) A novel connectivity-enhanced Multilevel Additive Schwarz (MAS) preconditioner on the GPU, designed to efficiently capture both stiff and soft elastodynamics and improve PCG convergence at a reduced preconditioning cost. 2) A C2-continuous cubic energy with an analytic eigensystem for strain limiting, enabling more parallel-friendly simulations of stiff membranes, such as cloth, without membrane locking. 3) For extremely stiff behaviors where elastic waves are barely visible, we employ affine body dynamics (ABD) with a hash-based multi-layer reduction strategy for fast Hessian assembly and efficient affine-deformable coupling. We conduct extensive performance analyses and benchmark studies to compare our framework against state-of-the-art methods and alternative design choices. Our system consistently delivers the fastest performance across soft, stiff, and hybrid simulation scenarios, even in cases with high resolution, large deformations, and high-speed impacts. Our framework will be fully open-sourced upon acceptance.","authors":["Kemeng Huang","Xinyu Lu","Huancheng Lin","Taku Komura","Minchen Li"],"url":"https://arxiv.org/abs/2411.06224"}
{"created":"2025-05-05","title":"An Efficient Matrix Multiplication Algorithm for Accelerating Inference in Binary and Ternary Neural Networks","abstract":"Despite their tremendous success and versatility, Deep Neural Networks (DNNs) such as Large Language Models (LLMs) suffer from inference inefficiency and rely on advanced computational infrastructure. To address these challenges and make these models more accessible and cost-effective, in this paper, we propose algorithms to improve the inference time and memory efficiency of DNNs with binary and ternary weight matrices. Particularly focusing on matrix multiplication as the bottleneck operation of inference, we observe that, once trained, the weight matrices of a model no longer change. This allows us to preprocess these matrices and create indices that help reduce the storage requirements by a logarithmic factor while enabling our efficient inference algorithms. Specifically, for a $n\\times n$ weight matrix, our efficient algorithm guarantees a time complexity of $O(\\frac{n^2}{\\log n})$, a logarithmic factor improvement over the standard vector-matrix multiplication. Besides theoretical analysis, we conduct extensive experiments to evaluate the practical efficiency of our algorithms. Our results confirm the superiority of our approach both with respect to time and memory, as we observed a reduction in the multiplication time up to 29x and memory usage up to 6x. When applied to LLMs, our experiments show up to a 5.24x speedup in the inference time.","authors":["Mohsen Dehghankar","Mahdi Erfanian","Abolfazl Asudeh"],"url":"https://arxiv.org/abs/2411.06360"}
{"created":"2025-05-05","title":"Locally Private Sampling with Public Data","abstract":"Local differential privacy (LDP) is increasingly employed in privacy-preserving machine learning to protect user data before sharing it with an untrusted aggregator. Most LDP methods assume that users possess only a single data record, which is a significant limitation since users often gather extensive datasets (e.g., images, text, time-series data) and frequently have access to public datasets. To address this limitation, we propose a locally private sampling framework that leverages both the private and public datasets of each user. Specifically, we assume each user has two distributions: $p$ and $q$ that represent their private dataset and the public dataset, respectively. The objective is to design a mechanism that generates a private sample approximating $p$ while simultaneously preserving $q$. We frame this objective as a minimax optimization problem using $f$-divergence as the utility measure. We fully characterize the minimax optimal mechanisms for general $f$-divergences provided that $p$ and $q$ are discrete distributions. Remarkably, we demonstrate that this optimal mechanism is universal across all $f$-divergences. Experiments validate the effectiveness of our minimax optimal sampler compared to the state-of-the-art locally private sampler.","authors":["Behnoosh Zamanlooy","Mario Diaz","Shahab Asoodeh"],"url":"https://arxiv.org/abs/2411.08791"}
{"created":"2025-05-05","title":"AC-Informed DC Optimal Transmission Switching Problems via Parameter Optimization","abstract":"Optimal Transmission Switching (OTS) problems minimize operational costs while treating both the transmission line energization statuses and generator setpoints as decision variables. The combination of nonlinearities from an AC power flow model and discrete variables associated with line statuses makes AC-OTS a computationally challenging Mixed-Integer Nonlinear Program (MINLP). To address these challenges, the DC power flow approximation is often used to obtain a DC-OTS formulation expressed as a Mixed-Integer Linear Program (MILP). However, this approximation often leads to suboptimal or infeasible switching decisions when evaluated with an AC power flow model. This paper proposes an enhanced DC-OTS formulation that leverages techniques for training machine learning models to optimize the DC power flow model's parameters. By optimally selecting parameter values that align flows in the DC power flow model with apparent power flows -- incorporating both real and reactive components -- from AC Optimal Power Flow (OPF) solutions, our method more accurately captures line congestion behavior. Integrating these optimized parameters into the DC-OTS formulation significantly improves the accuracy of switching decisions and reduces discrepancies between DC-OTS and AC-OTS solutions. We compare our optimized DC-OTS model against traditional OTS approaches, including DC-OTS, Linear Programming AC (LPAC)-OTS, and Quadratic Convex (QC)-OTS. Numerical results show that switching decisions from our model yield better performance when evaluated using an AC power flow model, with up to $44\\%$ cost reductions in some cases.","authors":["Babak Taheri","Daniel K. Molzahn"],"url":"https://arxiv.org/abs/2411.10528"}
{"created":"2025-05-05","title":"Dynamic Dimensioning of Frequency Containment Reserves: The Case of the Nordic Grid","abstract":"One of the main responsibilities of a Transmission System Operator (TSO) operating an electric grid is to maintain a designated frequency (e.g., 50 Hz in Europe). To achieve this, TSOs have created several products called frequency-supporting ancillary services. The Frequency Containment Reserve (FCR) is one of these ancillary service products. This article focuses on the TSO problem of determining the volume procured for FCR. Specifically, we investigate the potential benefits and impact on grid security when transitioning from a traditionally \\textit{static} procurement method to a \\textit{dynamic} strategy for FCR volume. We take the Nordic synchronous area in Europe as a case study and use a diffusion model to capture its frequency development. We introduce a controlled mean reversal parameter to assess changes in FCR obligations, in particular for the Nordic FCR-N ancillary service product. We establish closed-form expressions for exceedance probabilities and use historical frequency data as input to calibrate the model. We show that a dynamic dimensioning approach for FCR has the potential to significantly reduce the exceedance probabilities (up to $37\\%$) while maintaining the total yearly procured FCR volume equal to that of the current static approach. Alternatively, a dynamic dimensioning approach could significantly increase security at limited extra cost.","authors":["J\\\"obke Janssen","Alessandro Zocca","Bert Zwart","Jalal Kazempour"],"url":"https://arxiv.org/abs/2411.11093"}
{"created":"2025-05-05","title":"Quantifying Haptic Affection of Car Door through Data-Driven Analysis of Force Profile","abstract":"Haptic affection plays a crucial role in user experience, particularly in the automotive industry where the tactile quality of components can influence customer satisfaction. This study aims to accurately predict the affective property of a car door by only watching the force or torque profile of it when opening. To this end, a deep learning model is designed to capture the underlying relationships between force profiles and user-defined adjective ratings, providing insights into the door-opening experience. The dataset employed in this research includes force profiles and user adjective ratings collected from six distinct car models, reflecting a diverse set of door-opening characteristics and tactile feedback. The model's performance is assessed using Leave-One-Out Cross-Validation, a method that measures its generalization capability on unseen data. The results demonstrate that the proposed model achieves a high level of prediction accuracy, indicating its potential in various applications related to haptic affection and design optimization in the automotive industry.","authors":["Mudassir Ibrahim Awan","Ahsan Raza","Waseem Hassan","Ki-Uk Kyung","Seokhee Jeon"],"url":"https://arxiv.org/abs/2411.11382"}
{"created":"2025-05-05","title":"HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments","abstract":"We study the problem of robot navigation in dense and interactive crowds with environmental constraints such as corridors and furniture. Previous methods fail to consider all types of interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different components in the environment and propose a heterogeneous spatio-temporal (st) graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous st-graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions among entities through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success and efficiency in challenging navigation scenarios. Furthermore, we demonstrate that our pipeline achieves better zero-shot generalization capability than previous works when the densities of humans and obstacles change. More videos are available at https://sites.google.com/view/crowdnav-height/home.","authors":["Shuijing Liu","Haochen Xia","Fatemeh Cheraghi Pouria","Kaiwen Hong","Neeloy Chakraborty","Zichao Hu","Joydeep Biswas","Katherine Driggs-Campbell"],"url":"https://arxiv.org/abs/2411.12150"}
{"created":"2025-05-05","title":"Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models","abstract":"Large Language Models (LLMs) demonstrate enhanced capabilities and reliability by reasoning more, evolving from Chain-of-Thought prompting to product-level solutions like OpenAI o1. Despite various efforts to improve LLM reasoning, high-quality long-chain reasoning data and optimized training pipelines still remain inadequately explored in vision-language tasks. In this paper, we present Insight-V, an early effort to 1) scalably produce long and robust reasoning data for complex multi-modal tasks, and 2) an effective training pipeline to enhance the reasoning capabilities of multi-modal large language models (MLLMs). Specifically, to create long and structured reasoning data without human labor, we design a two-step pipeline with a progressive strategy to generate sufficiently long and diverse reasoning paths and a multi-granularity assessment method to ensure data quality. We observe that directly supervising MLLMs with such long and complex reasoning data will not yield ideal reasoning ability. To tackle this problem, we design a multi-agent system consisting of a reasoning agent dedicated to performing long-chain reasoning and a summary agent trained to judge and summarize reasoning results. We further incorporate an iterative DPO algorithm to enhance the reasoning agent's generation stability and quality. Based on the popular LLaVA-NeXT model and our stronger base MLLM, we demonstrate significant performance gains across challenging multi-modal benchmarks requiring visual reasoning. Benefiting from our multi-agent system, Insight-V can also easily maintain or improve performance on perception-focused multi-modal tasks.","authors":["Yuhao Dong","Zuyan Liu","Hai-Long Sun","Jingkang Yang","Winston Hu","Yongming Rao","Ziwei Liu"],"url":"https://arxiv.org/abs/2411.14432"}
{"created":"2025-05-05","title":"Learning Lifted STRIPS Models from Action Traces Alone: A Simple, General, and Scalable Solution","abstract":"Learning STRIPS action models from action traces alone is a challenging problem as it involves learning the domain predicates as well. In this work, a novel approach is introduced which, like the well-known LOCM systems, is scalable, but like SAT approaches, is sound and complete. Furthermore, the approach is general and imposes no restrictions on the hidden domain or the number or arity of the predicates. The new learning method is based on an \\emph{efficient, novel test} that checks whether the assumption that a predicate is affected by a set of action patterns, namely, actions with specific argument positions, is consistent with the traces. The predicates and action patterns that pass the test provide the basis for the learned domain that is then easily completed with preconditions and static predicates. The new method is studied theoretically and experimentally. For the latter, the method is evaluated on traces and graphs obtained from standard classical domains like the 8-puzzle, which involve hundreds of thousands of states and transitions. The learned representations are then verified on larger instances.","authors":["Jonas G\\\"osgens","Niklas Jansen","Hector Geffner"],"url":"https://arxiv.org/abs/2411.14995"}
{"created":"2025-05-05","title":"FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation","abstract":"Contact-rich tasks present significant challenges for robotic manipulation policies due to the complex dynamics of contact and the need for precise control. Vision-based policies often struggle with the skill required for such tasks, as they typically lack critical contact feedback modalities like force/torque information. To address this issue, we propose FoAR, a force-aware reactive policy that combines high-frequency force/torque sensing with visual inputs to enhance the performance in contact-rich manipulation. Built upon the RISE policy, FoAR incorporates a multimodal feature fusion mechanism guided by a future contact predictor, enabling dynamic adjustment of force/torque data usage between non-contact and contact phases. Its reactive control strategy also allows FoAR to accomplish contact-rich tasks accurately through simple position control. Experimental results demonstrate that FoAR significantly outperforms all baselines across various challenging contact-rich tasks while maintaining robust performance under unexpected dynamic disturbances. Project website: https://tonyfang.net/FoAR/","authors":["Zihao He","Hongjie Fang","Jingjing Chen","Hao-Shu Fang","Cewu Lu"],"url":"https://arxiv.org/abs/2411.15753"}
{"created":"2025-05-05","title":"Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks","abstract":"Vision Language Models (VLMs) can produce unintended and harmful content when exposed to adversarial attacks, particularly because their vision capabilities create new vulnerabilities. Existing defenses, such as input preprocessing, adversarial training, and response evaluation-based methods, are often impractical for real-world deployment due to their high costs. To address this challenge, we propose ASTRA, an efficient and effective defense by adaptively steering models away from adversarial feature directions to resist VLM attacks. Our key procedures involve finding transferable steering vectors representing the direction of harmful response and applying adaptive activation steering to remove these directions at inference time. To create effective steering vectors, we randomly ablate the visual tokens from the adversarial images and identify those most strongly associated with jailbreaks. These tokens are then used to construct steering vectors. During inference, we perform the adaptive steering method that involves the projection between the steering vectors and calibrated activation, resulting in little performance drops on benign inputs while strongly avoiding harmful outputs under adversarial inputs. Extensive experiments across multiple models and baselines demonstrate our state-of-the-art performance and high efficiency in mitigating jailbreak risks. Additionally, ASTRA exhibits good transferability, defending against unseen attacks (i.e., structured-based attack, perturbation-based attack with project gradient descent variants, and text-only attack). Our code is available at \\url{https://github.com/ASTRAL-Group/ASTRA}.","authors":["Han Wang","Gang Wang","Huan Zhang"],"url":"https://arxiv.org/abs/2411.16721"}
{"created":"2025-05-05","title":"RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training","abstract":"Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.","authors":["Raktim Gautam Goswami","Prashanth Krishnamurthy","Yann LeCun","Farshad Khorrami"],"url":"https://arxiv.org/abs/2411.17662"}
{"created":"2025-05-05","title":"Does Self-Attention Need Separate Weights in Transformers?","abstract":"The success of self-attention lies in its ability to capture long-range dependencies and enhance context understanding, but it is limited by its computational complexity and challenges in handling sequential data with inherent directionality. This work introduces a shared weight self-attention-based BERT model that only learns one weight matrix for (Key, Value, and Query) representations instead of three individual matrices for each of them. Our shared weight attention reduces the training parameter size by more than half and training time by around one-tenth. Furthermore, we demonstrate higher prediction accuracy on small tasks of GLUE over the BERT baseline and in particular a generalization power on noisy and out-of-domain data. Experimental results indicate that our shared self-attention method achieves a parameter size reduction of 66.53% in the attention block. In the GLUE dataset, the shared weight self-attention-based BERT model demonstrates accuracy improvements of 0.38%, 5.81%, and 1.06% over the standard, symmetric, and pairwise attention-based BERT models, respectively. The model and source code are available at Anonymous.","authors":["Md Kowsher","Nusrat Jahan Prottasha","Chun-Nam Yu","Ozlem Ozmen Garibay","Niloofar Yousefi"],"url":"https://arxiv.org/abs/2412.00359"}
{"created":"2025-05-05","title":"Competition Dynamics Shape Algorithmic Phases of In-Context Learning","abstract":"In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to novel tasks using merely the inputted context. This has motivated a series of papers that analyze tractable synthetic domains and postulate precise mechanisms that may underlie ICL. However, the use of relatively distinct setups that often lack a sequence modeling nature to them makes it unclear how general the reported insights from such studies are. Motivated by this, we propose a synthetic sequence modeling task that involves learning to simulate a finite mixture of Markov chains. As we show, models trained on this task reproduce most well-known results on ICL, hence offering a unified setting for studying the concept. Building on this setup, we demonstrate we can explain a model's behavior by decomposing it into four broad algorithms that combine a fuzzy retrieval vs. inference approach with either unigram or bigram statistics of the context. These algorithms engage in a competition dynamics to dominate model behavior, with the precise experimental conditions dictating which algorithm ends up superseding others: e.g., we find merely varying context size or amount of training yields (at times sharp) transitions between which algorithm dictates the model behavior, revealing a mechanism that explains the transient nature of ICL. In this sense, we argue ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability. This also implies that making general claims about ICL that hold universally across all settings may be infeasible.","authors":["Core Francisco Park","Ekdeep Singh Lubana","Itamar Pres","Hidenori Tanaka"],"url":"https://arxiv.org/abs/2412.01003"}
{"created":"2025-05-05","title":"You KAN Do It in a Single Shot: Plug-and-Play Methods with Single-Instance Priors","abstract":"The use of Plug-and-Play (PnP) methods has become a central approach for solving inverse problems, with denoisers serving as regularising priors that guide optimisation towards a clean solution. In this work, we introduce KAN-PnP, an optimisation framework that incorporates Kolmogorov-Arnold Networks (KANs) as denoisers within the Plug-and-Play (PnP) paradigm. KAN-PnP is specifically designed to solve inverse problems with single-instance priors, where only a single noisy observation is available, eliminating the need for large datasets typically required by traditional denoising methods. We show that KANs, based on the Kolmogorov-Arnold representation theorem, serve effectively as priors in such settings, providing a robust approach to denoising. We prove that the KAN denoiser is Lipschitz continuous, ensuring stability and convergence in optimisation algorithms like PnP-ADMM, even in the context of single-shot learning. Additionally, we provide theoretical guarantees for KAN-PnP, demonstrating its convergence under key conditions: the convexity of the data fidelity term, Lipschitz continuity of the denoiser, and boundedness of the regularisation functional. These conditions are crucial for stable and reliable optimisation. Our experimental results show, on super-resolution and joint optimisation, that KAN-PnP outperforms exiting methods, delivering superior performance in single-shot learning with minimal data. The method exhibits strong convergence properties, achieving high accuracy with fewer iterations.","authors":["Yanqi Cheng","Carola-Bibiane Sch\\\"onlieb","Angelica I Aviles-Rivero"],"url":"https://arxiv.org/abs/2412.06204"}
{"created":"2025-05-05","title":"When Every Token Counts: Optimal Segmentation for Low-Resource Language Models","abstract":"Traditional greedy tokenization methods have been a critical step in Natural Language Processing (NLP), influencing how text is converted into tokens and directly impacting model performance. While subword tokenizers like Byte-Pair Encoding (BPE) are widely used, questions remain about their optimality across model scales and languages. In this work, we demonstrate through extensive experiments that an optimal BPE configuration significantly reduces token count compared to greedy segmentation, yielding improvements in token-saving percentages and performance benefits, particularly for smaller models. We evaluate tokenization performance across various intrinsic and extrinsic tasks, including generation and classification. Our findings suggest that compression-optimized tokenization strategies could provide substantial advantages for multilingual and low-resource language applications, highlighting a promising direction for further research and inclusive NLP.","authors":["Bharath Raj","Garvit Suri","Vikrant Dewangan","Raghav Sonavane"],"url":"https://arxiv.org/abs/2412.06926"}
{"created":"2025-05-05","title":"A Causal World Model Underlying Next Token Prediction: Exploring GPT in a Controlled Environment","abstract":"Do generative pre-trained transformer (GPT) models, trained only to predict the next token, implicitly learn a world model from which a sequence is generated one token at a time? We address this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT models, at inference time, can be utilized for zero-shot causal structure learning for input sequences and present a confidence score. Empirical evaluation is conducted in a controlled environment using the setup and rules of the Othello and Chess strategy games. A GPT, pre-trained on real-world games played with the intention of winning, is tested on out-of-distribution synthetic data consisting of sequences of random legal moves. We find that the GPT model is likely to generate legal next moves for out-of-distribution sequences for which a causal structure is encoded in the attention mechanism with high confidence. In cases for which the GPT model generates illegal moves it also fails to capture any causal structure.","authors":["Raanan Y. Rohekar","Yaniv Gurwicz","Sungduk Yu","Estelle Aflalo","Vasudev Lal"],"url":"https://arxiv.org/abs/2412.07446"}
{"created":"2025-05-05","title":"AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework","abstract":"Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multi-agent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-of-Clauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation...","authors":["Meihao Fan","Ju Fan","Nan Tang","Lei Cao","Guoliang Li","Xiaoyong Du"],"url":"https://arxiv.org/abs/2412.10422"}
{"created":"2025-05-05","title":"On the specific solutions of reduced biquaternion equality constrained least squares problem and their relative forward error bound","abstract":"This study focuses on addressing the challenge of solving the reduced biquaternion equality constrained least squares (RBLSE) problem. We develop algebraic techniques to derive real and complex solutions for the RBLSE problem by utilizing the real and complex forms of reduced biquaternion matrices. Furthermore, we propose algorithms and provide a detailed analysis of their computational complexity for finding special solutions to the RBLSE problem. A perturbation analysis is conducted, establishing an upper bound for the relative forward error of these solutions. This analysis ensures the accuracy and stability of the solutions in the presence of data perturbations, which is crucial for practical applications where errors arising from input inaccuracies can cause deviations between computed and true solutions. Numerical examples are presented to validate the proposed algorithms, demonstrate their effectiveness, and verify the accuracy of the established upper bound for the relative forward errors. These findings lay the groundwork for exploring applications in 3D and 4D algebra such as robotics, signal, and image processing, expanding their impact on practical and emerging domains.","authors":["Sk. Safique Ahmad","Neha Bhadala"],"url":"https://arxiv.org/abs/2412.11059"}
{"created":"2025-05-05","title":"DriveGPT: Scaling Autoregressive Behavior Models for Driving","abstract":"We present DriveGPT, a scalable behavior model for autonomous driving. We model driving as a sequential decision-making task, and learn a transformer model to predict future agent states as tokens in an autoregressive fashion. We scale up our model parameters and training data by multiple orders of magnitude, enabling us to explore the scaling properties in terms of dataset size, model parameters, and compute. We evaluate DriveGPT across different scales in a planning task, through both quantitative metrics and qualitative examples, including closed-loop driving in complex real-world scenarios. In a separate prediction task, DriveGPT outperforms state-of-the-art baselines and exhibits improved performance by pretraining on a large-scale dataset, further validating the benefits of data scaling.","authors":["Xin Huang","Eric M. Wolff","Paul Vernaza","Tung Phan-Minh","Hongge Chen","David S. Hayden","Mark Edmonds","Brian Pierce","Xinxin Chen","Pratik Elias Jacob","Xiaobai Chen","Chingiz Tairbekov","Pratik Agarwal","Tianshi Gao","Yuning Chai","Siddhartha Srinivasa"],"url":"https://arxiv.org/abs/2412.14415"}
{"created":"2025-05-05","title":"ICLR: In-Context Learning of Representations","abstract":"Recent work has demonstrated that semantics specified by pretraining data influence how representations of different concepts are organized in a large language model (LLM). However, given the open-ended nature of LLMs, e.g., their ability to in-context learn, we can ask whether models alter these pretraining semantics to adopt alternative, context-specified ones. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, do models reorganize their representations in accordance with these novel semantics? To answer this question, we take inspiration from the theory of conceptual role semantics and define a toy \"graph tracing\" task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.) and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization from pretrained semantic representations to in-context representations aligned with the graph structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, providing evidence towards an implicit optimization process to infer context-specified semantics. Overall, our findings indicate scaling context-size can flexibly re-organize model representations, possibly unlocking novel capabilities.","authors":["Core Francisco Park","Andrew Lee","Ekdeep Singh Lubana","Yongyi Yang","Maya Okawa","Kento Nishi","Martin Wattenberg","Hidenori Tanaka"],"url":"https://arxiv.org/abs/2501.00070"}
{"created":"2025-05-05","title":"Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention","abstract":"Human cognition is punctuated by abrupt, spontaneous shifts between topics-driven by emotional, contextual, or associative cues-a phenomenon known as spontaneous thought in neuroscience. In contrast, self-attention based models depend on structured patterns over their inputs to predict each next token, lacking spontaneity. Motivated by this distinction, we characterize spontaneous topic changes in self-attention architectures, revealing both their similarities and their divergences from spontaneous human thought. First, we establish theoretical results under a simplified, single-layer self-attention model with suitable conditions by defining the topic as a set of Token Priority Graphs (TPGs). Specifically, we demonstrate that (1) the model maintains the priority order of tokens related to the input topic, (2) a spontaneous topic change can occur only if lower-priority tokens outnumber all higher-priority tokens of the input topic, and (3) unlike human cognition, the longer context length or the more ambiguous input topic reduces the likelihood of spontaneous change. Second, we empirically validate that these dynamics persist in modern, state-of-the-art LLMs, underscoring a fundamental disparity between human cognition and AI behaviour in the context of spontaneous topic changes. To the best of our knowledge, no prior work has explored these questions with a focus as closely aligned to human thought.","authors":["Mumin Jia","Jairo Diaz-Rodriguez"],"url":"https://arxiv.org/abs/2501.06382"}
{"created":"2025-05-05","title":"ADAM: An AI Reasoning and Bioinformatics Model for Alzheimer's Disease Detection and Microbiome-Clinical Data Integration","abstract":"Alzheimer's Disease Analysis Model (ADAM) is a multi-agent reasoning large language model (LLM) framework designed to integrate and analyze multimodal data, including microbiome profiles, clinical datasets, and external knowledge bases, to enhance the understanding and classification of Alzheimer's disease (AD). By leveraging the agentic system with LLM, ADAM produces insights from diverse data sources and contextualizes the findings with literature-driven evidence. A comparative evaluation with XGBoost revealed a significantly improved mean F1 score and significantly reduced variance for ADAM, highlighting its robustness and consistency, particularly when utilizing human biological data. Although currently tailored for binary classification tasks with two data modalities, future iterations will aim to incorporate additional data types, such as neuroimaging and peripheral biomarkers, and expand them to predict disease progression, thereby broadening ADAM's scalability and applicability in AD research and diagnostic applications.","authors":["Ziyuan Huang","Vishaldeep Kaur Sekhon","Roozbeh Sadeghian","Maria L. Vaida","Cynthia Jo","Doyle Ward","Vanni Bucci","John P. Haran"],"url":"https://arxiv.org/abs/2501.08324"}
{"created":"2025-05-05","title":"Test-time regression: a unifying framework for designing sequence models with associative memory","abstract":"Sequence models lie at the heart of modern deep learning. However, rapid advancements have produced a diversity of seemingly unrelated architectures, such as Transformers and recurrent alternatives. In this paper, we introduce a unifying framework to understand and derive these sequence models, inspired by the empirical importance of associative recall, the capability to retrieve contextually relevant tokens. We formalize associative recall as a two-step process, memorization and retrieval, casting memorization as a regression problem. Layers that combine these two steps perform associative recall via ``test-time regression'' over its input tokens. Prominent layers, including linear attention, state-space models, fast-weight programmers, online learners, and softmax attention, arise as special cases defined by three design choices: the regression weights, the regressor function class, and the test-time optimization algorithm. Our approach clarifies how linear attention fails to capture inter-token correlations and offers a mathematical justification for the empirical effectiveness of query-key normalization in softmax attention. Further, it illuminates unexplored regions within the design space, which we use to derive novel higher-order generalizations of softmax attention. Beyond unification, our work bridges sequence modeling with classic regression methods, a field with extensive literature, paving the way for developing more powerful and theoretically principled architectures.","authors":["Ke Alexander Wang","Jiaxin Shi","Emily B. Fox"],"url":"https://arxiv.org/abs/2501.12352"}
{"created":"2025-05-05","title":"A Rate-Distortion Framework for Summarization","abstract":"This paper introduces an information-theoretic framework for text summarization. We define the summarizer rate-distortion function and show that it provides a fundamental lower bound on summarizer performance. We describe an iterative procedure, similar to Blahut-Arimoto algorithm, for computing this function. To handle real-world text datasets, we also propose a practical method that can calculate the summarizer rate-distortion function with limited data. Finally, we empirically confirm our theoretical results by comparing the summarizer rate-distortion function with the performances of different summarizers used in practice.","authors":["Enes Arda","Aylin Yener"],"url":"https://arxiv.org/abs/2501.13100"}
{"created":"2025-05-05","title":"Collaborative Coded Caching for Partially Connected Networks","abstract":"Coded caching leverages the differences in user cache memories to achieve gains that scale with the total cache size, alleviating network congestion due to high-quality content requests. Additionally, distributing transmitters over a wide area can mitigate the adverse effects of path loss. In this work, we consider a partially connected network where the channel between distributed transmitters (helpers) and users is modeled as a distributed multiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a novel delivery scheme consisting of two phases: partitioning and transmission. In the partitioning phase, users with identical cache profiles are partitioned into the minimum number of sets, such that users within each set can successfully decode their desired message from a joint transmission enabled by MIMO precoding. To optimally partition the users, we employ the branch and bound method. In the transmission phase, each partition is treated as a single entity, and codewords are multicast to partitions with distinct cache profiles. The proposed delivery scheme is applicable to any partially connected network, and while the partitioning is optimal, the overall delivery scheme, including transmission, is heuristic. Interestingly, simulation results show that its performance closely approximates that of the fully connected optimal solution.","authors":["Kagan Akcay","Eleftherios Lampiris","MohammadJavad Salehi","Giuseppe Caire"],"url":"https://arxiv.org/abs/2501.13298"}
{"created":"2025-05-05","title":"Transfer Learning of Surrogate Models via Domain Affine Transformation Across Synthetic and Real-World Benchmarks","abstract":"Surrogate models are frequently employed as efficient substitutes for the costly execution of real-world processes. However, constructing a high-quality surrogate model often demands extensive data acquisition. A solution to this issue is to transfer pre-trained surrogate models for new tasks, provided that certain invariances exist between tasks. This study focuses on transferring non-differentiable surrogate models (e.g., random forests) from a source function to a target function, where we assume their domains are related by an unknown affine transformation, using only a limited amount of transfer data points evaluated on the target. Previous research attempts to tackle this challenge for differentiable models, e.g., Gaussian process regression, which minimizes the empirical loss on the transfer data by tuning the affine transformations. In this paper, we extend the previous work to the random forest and assess its effectiveness on a widely-used artificial problem set - Black-Box Optimization Benchmark (BBOB) testbed, and on four real-world transfer learning problems. The results highlight the significant practical advantages of the proposed method, particularly in reducing both the data requirements and computational costs of training surrogate models for complex real-world scenarios.","authors":["Shuaiqun Pan","Diederick Vermetten","Manuel L\\'opez-Ib\\'a\\~nez","Thomas B\\\"ack","Hao Wang"],"url":"https://arxiv.org/abs/2501.14012"}
{"created":"2025-05-05","title":"From Bugs to Benefits: Improving User Stories by Leveraging Crowd Knowledge with CrUISE-AC","abstract":"Costs for resolving software defects increase exponentially in late stages. Incomplete or ambiguous requirements are one of the biggest sources for defects, since stakeholders might not be able to communicate their needs or fail to share their domain specific knowledge. Combined with insufficient developer experience, teams are prone to constructing incorrect or incomplete features. To prevent this, requirements engineering has to explore knowledge sources beyond stakeholder interviews. Publicly accessible issue trackers for systems within the same application domain hold essential information on identified weaknesses, edge cases, and potential error sources, all documented by actual users. Our research aims at (1) identifying, and (2) leveraging such issues to improve an agile requirements artifact known as a \"user story\". We present CrUISE-AC (Crowd and User Informed Suggestion Engine for Acceptance Criteria) as a fully automated method that investigates issues and generates non-trivial additional acceptance criteria for a given user story by employing NLP techniques and an ensemble of LLMs. CrUISE- AC was evaluated by five independent experts in two distinct business domains. Our findings suggest that issue trackers hold valuable information pertinent to requirements engineering. Our evaluation shows that 80-82% of the generated acceptance criteria add relevant requirements to the user stories. Limitations are the dependence on accessible input issues and the fact that we do not check generated criteria for being conflict-free or non-overlapping with criteria from other user stories.","authors":["Stefan Schwedt","Thomas Str\\\"oder"],"url":"https://arxiv.org/abs/2501.15181"}
{"created":"2025-05-05","title":"chebgreen: Learning and Interpolating Continuous Empirical Green's Functions from Data","abstract":"In this work, we present a mesh-independent, data-driven library, chebgreen, to mathematically model one-dimensional systems, possessing an associated control parameter, and whose governing partial differential equation is unknown. The proposed method learns an Empirical Green's Function for the associated, but hidden, boundary value problem, in the form of a Rational Neural Network from which we subsequently construct a bivariate representation in a Chebyshev basis. We uncover the Green's function, at an unseen control parameter value, by interpolating the left and right singular functions within a suitable library, expressed as points on a manifold of Quasimatrices, while the associated singular values are interpolated with Lagrange polynomials.","authors":["Harshwardhan Praveen","Jacob Brown","Christopher Earls"],"url":"https://arxiv.org/abs/2501.18715"}
{"created":"2025-05-05","title":"Project-and-Fuse: Improving RGB-D Semantic Segmentation via Graph Convolution Networks","abstract":"Most existing RGB-D semantic segmentation methods focus on the feature level fusion, including complex cross-modality and cross-scale fusion modules. However, these methods may cause misalignment problem in the feature fusion process and counter-intuitive patches in the segmentation results. Inspired by the popular pixel-node-pixel pipeline, we propose to 1) fuse features from two modalities in a late fusion style, during which the geometric feature injection is guided by texture feature prior; 2) employ Graph Neural Networks (GNNs) on the fused feature to alleviate the emergence of irregular patches by inferring patch relationship. At the 3D feature extraction stage, we argue that traditional CNNs are not efficient enough for depth maps. So, we encode depth map into normal map, after which CNNs can easily extract object surface tendencies.At projection matrix generation stage, we find the existence of Biased-Assignment and Ambiguous-Locality issues in the original pipeline. Therefore, we propose to 1) adopt the Kullback-Leibler Loss to ensure no missing important pixel features, which can be viewed as hard pixel mining process; 2) connect regions that are close to each other in the Euclidean space as well as in the semantic space with larger edge weights so that location informations can been considered. Extensive experiments on two public datasets, NYU-DepthV2 and SUN RGB-D, have shown that our approach can consistently boost the performance of RGB-D semantic segmentation task.","authors":["Xiaoyan Jiang","Bohan Wang","Xinlong Wan","Shanshan Chen","Hamido Fujita","Hanan Abd. Al Juaid"],"url":"https://arxiv.org/abs/2501.18851"}
{"created":"2025-05-05","title":"TableMaster: A Recipe to Advance Table Understanding with Language Models","abstract":"Tables serve as a fundamental format for representing structured relational data. While current language models (LMs) excel at many text-based tasks, they still face challenges in table understanding due to the complex characteristics of tabular data, such as their structured nature. In this paper, we aim to enhance LMs for improved table understanding. We identify four key challenges: 1) difficulty in locating target data, 2) deficiency in table semantics, 3) numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in symbolic reasoning. To address these issues, we propose TableMaster, a recipe and comprehensive framework that integrates multiple solutions to overcome these obstacles. TableMaster first extracts relevant table content and verbalizes it with enriched semantic context. Additionally, we introduce adaptive reasoning, a flexible approach that dynamically adjusts between textual and symbolic reasoning, tailoring the reasoning process to each query. Extensive analyses and experiments demonstrate our findings and the effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines.","authors":["Lang Cao","Hanbing Liu"],"url":"https://arxiv.org/abs/2501.19378"}
{"created":"2025-05-05","title":"From Foresight to Forethought: VLM-In-the-Loop Policy Steering via Latent Alignment","abstract":"While generative robot policies have demonstrated significant potential in learning complex, multimodal behaviors from demonstrations, they still exhibit diverse failures at deployment-time. Policy steering offers an elegant solution to reducing the chance of failure by using an external verifier to select from low-level actions proposed by an imperfect generative policy. Here, one might hope to use a Vision Language Model (VLM) as a verifier, leveraging its open-world reasoning capabilities. However, off-the-shelf VLMs struggle to understand the consequences of low-level robot actions as they are represented fundamentally differently than the text and images the VLM was trained on. In response, we propose FOREWARN, a novel framework to unlock the potential of VLMs as open-vocabulary verifiers for runtime policy steering. Our key idea is to decouple the VLM's burden of predicting action outcomes (foresight) from evaluation (forethought). For foresight, we leverage a latent world model to imagine future latent states given diverse low-level action plans. For forethought, we align the VLM with these predicted latent states to reason about the consequences of actions in its native representation--natural language--and effectively filter proposed plans. We validate our framework across diverse robotic manipulation tasks, demonstrating its ability to bridge representational gaps and provide robust, generalizable policy steering. Videos can be found on the project website: https://yilin-wu98.github.io/forewarn/.","authors":["Yilin Wu","Ran Tian","Gokul Swamy","Andrea Bajcsy"],"url":"https://arxiv.org/abs/2502.01828"}
{"created":"2025-05-05","title":"CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing","abstract":"Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel Collaborative Inference with Token-lEvel Routing (CITER) framework that enables efficient collaboration between small and large language models (SLMs \\& LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. Our data and code are available at https://github.com/aiming-lab/CITER.","authors":["Wenhao Zheng","Yixiao Chen","Weitong Zhang","Souvik Kundu","Yun Li","Zhengzhong Liu","Eric P. Xing","Hongyi Wang","Huaxiu Yao"],"url":"https://arxiv.org/abs/2502.01976"}
{"created":"2025-05-05","title":"Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation","abstract":"Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation. Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information. However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process. To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations. Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features. This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC. Extensive experiments and analyses on the CortexBench and LIBERO benchmarks demonstrate significant performance improvements with IB, underscoring the importance of reducing input data redundancy and highlighting its practical value for more practical applications. Project Page: https://baishuanghao.github.io/BC-IB.github.io.","authors":["Shuanghao Bai","Wanqi Zhou","Pengxiang Ding","Wei Zhao","Donglin Wang","Badong Chen"],"url":"https://arxiv.org/abs/2502.02853"}
{"created":"2025-05-05","title":"Interaction-Aware Gaussian Weighting for Clustered Federated Learning","abstract":"Federated Learning (FL) emerged as a decentralized paradigm to train models while preserving privacy. However, conventional FL struggles with data heterogeneity and class imbalance, which degrade model performance. Clustered FL balances personalization and decentralized training by grouping clients with analogous data distributions, enabling improved accuracy while adhering to privacy constraints. This approach effectively mitigates the adverse impact of heterogeneity in FL. In this work, we propose a novel clustered FL method, FedGWC (Federated Gaussian Weighting Clustering), which groups clients based on their data distribution, allowing training of a more robust and personalized model on the identified clusters. FedGWC identifies homogeneous clusters by transforming individual empirical losses to model client interactions with a Gaussian reward mechanism. Additionally, we introduce the Wasserstein Adjusted Score, a new clustering metric for FL to evaluate cluster cohesion with respect to the individual class distribution. Our experiments on benchmark datasets show that FedGWC outperforms existing FL algorithms in cluster quality and classification accuracy, validating the efficacy of our approach.","authors":["Alessandro Licciardi","Davide Leo","Eros Fan\\`i","Barbara Caputo","Marco Ciccone"],"url":"https://arxiv.org/abs/2502.03340"}
{"created":"2025-05-05","title":"AL-PINN: Active Learning-Driven Physics-Informed Neural Networks for Efficient Sample Selection in Solving Partial Differential Equations","abstract":"Physics-Informed Neural Networks (PINNs) have emerged as a promising approach for solving Partial Differential Equations (PDEs) by incorporating physical constraints into deep learning models. However, standard PINNs often require a large number of training samples to achieve high accuracy, leading to increased computational costs. To address this issue, we propose Active Learning-Driven PINNs (AL-PINN), which integrates Uncertainty Quantification (UQ) and Active Learning (AL) strategies to optimize sample selection dynamically.","authors":["Keon Vin Park"],"url":"https://arxiv.org/abs/2502.03963"}
{"created":"2025-05-05","title":"End-to-End Learning Framework for Solving Non-Markovian Optimal Control","abstract":"Integer-order calculus often falls short in capturing the long-range dependencies and memory effects found in many real-world processes. Fractional calculus addresses these gaps via fractional-order integrals and derivatives, but fractional-order dynamical systems pose substantial challenges in system identification and optimal control due to the lack of standard control methodologies. In this paper, we theoretically derive the optimal control via linear quadratic regulator (LQR) for fractional-order linear time-invariant (FOLTI) systems and develop an end-to-end deep learning framework based on this theoretical foundation. Our approach establishes a rigorous mathematical model, derives analytical solutions, and incorporates deep learning to achieve data-driven optimal control of FOLTI systems. Our key contributions include: (i) proposing an innovative system identification method control strategy for FOLTI systems, (ii) developing the first end-to-end data-driven learning framework, Fractional-Order Learning for Optimal Control (FOLOC), that learns control policies from observed trajectories, and (iii) deriving a theoretical analysis of sample complexity to quantify the number of samples required for accurate optimal control in complex real-world problems. Experimental results indicate that our method accurately approximates fractional-order system behaviors without relying on Gaussian noise assumptions, pointing to promising avenues for advanced optimal control.","authors":["Xiaole Zhang","Peiyu Zhang","Xiongye Xiao","Shixuan Li","Vasileios Tzoumas","Vijay Gupta","Paul Bogdan"],"url":"https://arxiv.org/abs/2502.04649"}
{"created":"2025-05-05","title":"Nonlinear manifold approximation using compositional polynomial networks","abstract":"We consider the problem of approximating a subset $M$ of a Hilbert space $X$ by a low-dimensional manifold $M_n$, using samples from $M$. We propose a nonlinear approximation method where $M_n $ is defined as the range of a smooth nonlinear decoder $D$ defined on $\\mathbb{R}^n$ with values in a possibly high-dimensional linear space $X_N$, and a linear encoder $E$ which associates to an element from $ M$ its coefficients $E(u)$ on a basis of a $n$-dimensional subspace $X_n \\subset X_N$, where $X_n$ and $X_N$ are optimal or near to optimal linear spaces, depending on the selected error measure. The linearity of the encoder allows to easily obtain the parameters $E(u)$ associated with a given element $u$ in $M$. The proposed decoder is a polynomial map from $\\mathbb{R}^n$ to $X_N$ which is obtained by a tree-structured composition of polynomial maps, estimated sequentially from samples in $M$. Rigorous error and stability analyses are provided, as well as an adaptive strategy for constructing a decoder that guarantees an approximation of the set $M$ with controlled mean-squared or wort-case errors, and a controlled stability (Lipschitz continuity) of the encoder and decoder pair. We demonstrate the performance of our method through numerical experiments.","authors":["Antoine Bensalah","Anthony Nouy","Joel Soffo"],"url":"https://arxiv.org/abs/2502.05088"}
{"created":"2025-05-05","title":"MELON: Provable Indirect Prompt Injection Defense via Masked Re-execution and Tool Comparison","abstract":"Recent research has explored that LLM agents are vulnerable to indirect prompt injection (IPI) attacks, where malicious tasks embedded in tool-retrieved information can redirect the agent to take unauthorized actions. Existing defenses against IPI have significant limitations: either require essential model training resources, lack effectiveness against sophisticated attacks, or harm the normal utilities. We present MELON (Masked re-Execution and TooL comparisON), a novel IPI defense. Our approach builds on the observation that under a successful attack, the agent's next action becomes less dependent on user tasks and more on malicious tasks. Following this, we design MELON to detect attacks by re-executing the agent's trajectory with a masked user prompt modified through a masking function. We identify an attack if the actions generated in the original and masked executions are similar. We also include three key designs to reduce the potential false positives and false negatives. Extensive evaluation on the IPI benchmark AgentDojo demonstrates that MELON outperforms SOTA defenses in both attack prevention and utility preservation. Moreover, we show that combining MELON with a SOTA prompt augmentation defense (denoted as MELON-Aug) further improves its performance. We also conduct a detailed ablation study to validate our key designs.","authors":["Kaijie Zhu","Xianjun Yang","Jindong Wang","Wenbo Guo","William Yang Wang"],"url":"https://arxiv.org/abs/2502.05174"}
{"created":"2025-05-05","title":"Towards a Foundation Model for Physics-Informed Neural Networks: Multi-PDE Learning with Active Sampling","abstract":"Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs) by embedding physical laws into neural network training. However, traditional PINN models are typically designed for single PDEs, limiting their generalizability across different physical systems. In this work, we explore the potential of a foundation PINN model capable of solving multiple PDEs within a unified architecture. We investigate the efficacy of a single PINN framework trained on four distinct PDEs-the Simple Harmonic Oscillator (SHO), the 1D Heat Equation, the 1D Wave Equation, and the 2D Laplace Equation, demonstrating its ability to learn diverse physical dynamics.","authors":["Keon Vin Park"],"url":"https://arxiv.org/abs/2502.07425"}
{"created":"2025-05-05","title":"AT-Drone: Benchmarking Adaptive Teaming in Multi-Drone Pursuit","abstract":"Adaptive teaming-the capability of agents to effectively collaborate with unfamiliar teammates without prior coordination-is widely explored in virtual video games but overlooked in real-world multi-robot contexts. Yet, such adaptive collaboration is crucial for real-world applications, including border surveillance, search-and-rescue, and counter-terrorism operations. To address this gap, we introduce AT-Drone, the first dedicated benchmark explicitly designed to facilitate comprehensive training and evaluation of adaptive teaming strategies in multi-drone pursuit scenarios. AT-Drone makes the following key contributions: (1) An adaptable simulation environment configurator that enables intuitive and rapid setup of adaptive teaming multi-drone pursuit tasks, including four predefined pursuit environments. (2) A streamlined real-world deployment pipeline that seamlessly translates simulation insights into practical drone evaluations using edge devices and Crazyflie drones. (3) A novel algorithm zoo integrated with a distributed training framework, featuring diverse algorithms explicitly tailored, for the first time, to multi-pursuer and multi-evader settings. (4) Standardized evaluation protocols with newly designed unseen drone zoos, explicitly designed to rigorously assess the performance of adaptive teaming. Comprehensive experimental evaluations across four progressively challenging multi-drone pursuit scenarios confirm AT-Drone's effectiveness in advancing adaptive teaming research. Real-world drone experiments further validate its practical feasibility and utility for realistic robotic operations. Videos, code and weights are available at \\url{https://sites.google.com/view/at-drone}.","authors":["Yang Li","Junfan Chen","Feng Xue","Jiabin Qiu","Wenbin Li","Qingrui Zhang","Ying Wen","Wei Pan"],"url":"https://arxiv.org/abs/2502.09762"}
{"created":"2025-05-05","title":"Human-centered explanation does not fit all: The interplay of sociotechnical, cognitive, and individual factors in the effect AI explanations in algorithmic decision-making","abstract":"Recent XAI studies have investigated what constitutes a \\textit{good} explanation in AI-assisted decision-making. Despite the widely accepted human-friendly properties of explanations, such as contrastive and selective, existing studies have yielded inconsistent findings. To address these gaps, our study focuses on the cognitive dimensions of explanation evaluation, by evaluating six explanations with different contrastive strategies and information selectivity and scrutinizing factors behind their valuation process. Our analysis results find that contrastive explanations are not the most preferable or understandable in general; Rather, different contrastive and selective explanations were appreciated to a different extent based on who they are, when, how, and what to explain -- with different level of cognitive load and engagement and sociotechnical contexts. Given these findings, we call for a nuanced view of explanation strategies, with implications for designing AI interfaces to accommodate individual and contextual differences in AI-assisted decision-making.","authors":["Yongsu Ahn","Yu-Ru Lin","Malihe Alikhani","Eunjeong Cheon"],"url":"https://arxiv.org/abs/2502.12354"}
{"created":"2025-05-05","title":"Design and Implementation of a Dual Uncrewed Surface Vessel Platform for Bathymetry Research under High-flow Conditions","abstract":"Bathymetry, the study of underwater topography, relies on sonar mapping of submerged structures. These measurements, critical for infrastructure health monitoring, often require expensive instrumentation. The high financial risk associated with sensor damage or vessel loss creates a reluctance to deploy uncrewed surface vessels (USVs) for bathymetry. However, the crewed-boat bathymetry operations, are costly, pose hazards to personnel, and frequently fail to achieve the stable conditions necessary for bathymetry data collection, especially under high currents. Further research is essential to advance autonomous control, navigation, and data processing technologies, with a particular focus on bathymetry. There is a notable lack of accessible hardware platforms that allow for integrated research in both bathymetry-focused autonomous control and navigation, as well as data evaluation and processing. This paper addresses this gap through the design and implementation of two complementary USV systems tailored for uncrewed bathymetry research. This includes a low-cost USV for Navigation And Control research (NAC-USV) and a second, high-end USV equipped with a high-resolution multi-beam sonar and the associated hardware for Bathymetry data quality Evaluation and Post-processing research (BEP-USV). The NAC-USV facilitates the investigation of autonomous, fail-safe navigation and control, emphasizing the stability requirements for high-quality bathymetry data collection while minimizing the risk to equipment. The BEP-USV, which mirrors the NAC-USV hardware, is then used for additional control validation and in-depth exploration of bathymetry data evaluation and post-processing methodologies. We detail the design and implementation of both systems, and open source the design. Furthermore, we demonstrate the system's effectiveness in a range of operational scenarios.","authors":["Dinesh Kumar","Amin Ghorbanpour","Kin Yen","Iman Soltani"],"url":"https://arxiv.org/abs/2502.12539"}
{"created":"2025-05-05","title":"Minimalist Leader Election Under Weak Communication","abstract":"We propose a protocol to solve Leader Election within weak communication models such as the beeping model or the stone-age model. Unlike most previous work, our algorithm operates on only six states, does not require unique identifiers, and assumes no prior knowledge of the network's size or topology, i.e., it is uniform. We show that under our protocol, the system almost surely converges to a configuration in which a single node is in a leader state. With high probability, this occurs in fewer than $O(D^2 \\log n)$ rounds, where $D$ is the network diameter. We also show that this can be decreased to $O(D \\log n)$ when a constant factor approximation of $D$ is known. The main drawbacks of our approach are a $\\Tilde{\\Omega}(D)$ overhead in the running time compared to algorithms with stronger requirements, and the fact that nodes are unaware of when a single-leader configuration is reached. Nevertheless, the minimal assumptions and natural appeal of our solution make it particularly well-suited for implementation in the simplest distributed systems, especially biological ones.","authors":["Robin Vacus","Isabella Ziccardi"],"url":"https://arxiv.org/abs/2502.12697"}
{"created":"2025-05-05","title":"Activation Steering in Neural Theorem Provers","abstract":"Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.","authors":["Shashank Kirtania"],"url":"https://arxiv.org/abs/2502.15507"}
{"created":"2025-05-05","title":"Adversarial Combinatorial Semi-bandits with Graph Feedback","abstract":"In combinatorial semi-bandits, a learner repeatedly selects from a combinatorial decision set of arms, receives the realized sum of rewards, and observes the rewards of the individual selected arms as feedback. In this paper, we extend this framework to include \\emph{graph feedback}, where the learner observes the rewards of all neighboring arms of the selected arms in a feedback graph $G$. We establish that the optimal regret over a time horizon $T$ scales as $\\widetilde{\\Theta}(S\\sqrt{T}+\\sqrt{\\alpha ST})$, where $S$ is the size of the combinatorial decisions and $\\alpha$ is the independence number of $G$. This result interpolates between the known regrets $\\widetilde\\Theta(S\\sqrt{T})$ under full information (i.e., $G$ is complete) and $\\widetilde\\Theta(\\sqrt{KST})$ under the semi-bandit feedback (i.e., $G$ has only self-loops), where $K$ is the total number of arms. A key technical ingredient is to realize a convexified action using a random decision vector with negative correlations. We also show that online stochastic mirror descent (OSMD) that only realizes convexified actions in expectation is suboptimal.","authors":["Yuxiao Wen"],"url":"https://arxiv.org/abs/2502.18826"}
{"created":"2025-05-05","title":"Towards Multimodal Large-Language Models for Parent-Child Interaction: A Focus on Joint Attention","abstract":"Joint attention is a critical component of early speech-language development and a key indicator of effective parent-child interaction. However, research on detecting and analysing joint attention remains limited, particularly for Multimodal Large Language Models (MLLMs). This study evaluates MLLMs' ability to comprehend joint attention by analysing 26 parent-child interaction videos annotated by two speech-language pathologists. These annotations identify strong and poor joint attention segments, serving as benchmarks for evaluating the models' interpretive capabilities. Our findings reveal that current MLLMs struggle to accurately interpret joint attention due to a lack of nuanced understanding of child-initiated eye contact, a crucial component of joint attention dynamics. This study highlights the importance of incorporating detailed eye contact to enhance MLLMs' multimodal reasoning. Addressing these gaps is essential for future research to advance the use of MLLMs in analysing and supporting parent-child interactions.","authors":["Weiyan Shi","Viet Hai Le","Kenny Tsu Wei Choo"],"url":"https://arxiv.org/abs/2502.19877"}
{"created":"2025-05-05","title":"Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention","abstract":"Soybean leaf disease detection is critical for agricultural productivity but faces challenges due to visually similar symptoms and limited interpretability in conventional methods. While Convolutional Neural Networks (CNNs) excel in spatial feature extraction, they often neglect inter-image relational dependencies, leading to misclassifications. This paper proposes an interpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework that synergizes MobileNetV2 for localized feature extraction and GraphSAGE for relational modeling. The framework constructs a graph where nodes represent leaf images, with edges defined by cosine similarity-based adjacency matrices and adaptive neighborhood sampling. This design captures fine-grained lesion features and global symptom patterns, addressing inter-class similarity challenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAM visualizations, generating heatmaps to highlight disease-influential regions. Evaluated on a dataset of ten soybean leaf diseases, the model achieves $97.16\\%$ accuracy, surpassing standalone CNNs ($\\le95.04\\%$) and traditional machine learning models ($\\le77.05\\%$). Ablation studies validate the sequential architecture's superiority over parallel or single-model configurations. With only 2.3 million parameters, the lightweight MobileNetV2-GraphSAGE combination ensures computational efficiency, enabling real-time deployment in resource-constrained environments. The proposed approach bridges the gap between accurate classification and practical applicability, offering a robust, interpretable tool for agricultural diagnostics while advancing CNN-GNN integration in plant pathology research.","authors":["Md Abrar Jahin","Soudeep Shahriar","M. F. Mridha","Md. Jakir Hossen","Nilanjan Dey"],"url":"https://arxiv.org/abs/2503.01284"}
{"created":"2025-05-05","title":"ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation","abstract":"This paper presents ArticuBot, in which a single learned policy enables a robotics system to open diverse categories of unseen articulated objects in the real world. This task has long been challenging for robotics due to the large variations in the geometry, size, and articulation types of such objects. Our system, Articubot, consists of three parts: generating a large number of demonstrations in physics-based simulation, distilling all generated demonstrations into a point cloud-based neural policy via imitation learning, and performing zero-shot sim2real transfer to real robotics systems. Utilizing sampling-based grasping and motion planning, our demonstration generalization pipeline is fast and effective, generating a total of 42.3k demonstrations over 322 training articulated objects. For policy learning, we propose a novel hierarchical policy representation, in which the high-level policy learns the sub-goal for the end-effector, and the low-level policy learns how to move the end-effector conditioned on the predicted goal. We demonstrate that this hierarchical approach achieves much better object-level generalization compared to the non-hierarchical version. We further propose a novel weighted displacement model for the high-level policy that grounds the prediction into the existing 3D structure of the scene, outperforming alternative policy representations. We show that our learned policy can zero-shot transfer to three different real robot settings: a fixed table-top Franka arm across two different labs, and an X-Arm on a mobile base, opening multiple unseen articulated objects across two labs, real lounges, and kitchens. Videos and code can be found on our project website: https://articubot.github.io/.","authors":["Yufei Wang","Ziyu Wang","Mino Nakura","Pratik Bhowal","Chia-Liang Kuo","Yi-Ting Chen","Zackory Erickson","David Held"],"url":"https://arxiv.org/abs/2503.03045"}
{"created":"2025-05-05","title":"AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons","abstract":"Scaling up robotic imitation learning for real-world applications requires efficient and scalable demonstration collection methods. While teleoperation is effective, it depends on costly and inflexible robot platforms. In-the-wild demonstrations offer a promising alternative, but existing collection devices have key limitations: handheld setups offer limited observational coverage, and whole-body systems often require fine-tuning with robot data due to domain gaps. To address these challenges, we present AirExo-2, a low-cost exoskeleton system for large-scale in-the-wild data collection, along with several adaptors that transform collected data into pseudo-robot demonstrations suitable for policy learning. We further introduce RISE-2, a generalizable imitation learning policy that fuses 3D spatial and 2D semantic perception for robust manipulations. Experiments show that RISE-2 outperforms prior state-of-the-art methods on both in-domain and generalization evaluations. Trained solely on adapted in-the-wild data produced by AirExo-2, the RISE-2 policy achieves comparable performance to the policy trained with teleoperated data, highlighting the effectiveness and potential of AirExo-2 for scalable and generalizable imitation learning.","authors":["Hongjie Fang","Chenxi Wang","Yiming Wang","Jingjing Chen","Shangning Xia","Jun Lv","Zihao He","Xiyan Yi","Yunhan Guo","Xinyu Zhan","Lixin Yang","Weiming Wang","Cewu Lu","Hao-Shu Fang"],"url":"https://arxiv.org/abs/2503.03081"}
{"created":"2025-05-05","title":"A unified approach for degree bound estimates of linear differential operators","abstract":"We identify a common scheme in several existing algorithms addressing computational problems on linear differential equations with polynomial coefficients. These algorithms reduce to computing a linear relation between vectors obtained as iterates of a simple differential operator known as pseudo-linear map.","authors":["Louis Gaillard"],"url":"https://arxiv.org/abs/2503.03337"}
{"created":"2025-05-05","title":"Generative AI in Academic Writing: A Comparison of DeepSeek, Qwen, ChatGPT, Gemini, Llama, Mistral, and Gemma","abstract":"DeepSeek v3, developed in China, was released in December 2024, followed by Alibaba's Qwen 2.5 Max in January 2025 and Qwen3 235B in April 2025. These free and open-source models offer significant potential for academic writing and content creation. This study evaluates their academic writing performance by comparing them with ChatGPT, Gemini, Llama, Mistral, and Gemma. There is a critical gap in the literature concerning how extensively these tools can be utilized and their potential to generate original content in terms of quality, readability, and effectiveness. Using 40 papers on Digital Twin and Healthcare, texts were generated through AI tools based on posed questions and paraphrased abstracts. The generated content was analyzed using plagiarism detection, AI detection, word count comparisons, semantic similarity, and readability assessments. Results indicate that paraphrased abstracts showed higher plagiarism rates, while question-based responses also exceeded acceptable levels. AI detection tools consistently identified all outputs as AI-generated. Word count analysis revealed that all chatbots produced a sufficient volume of content. Semantic similarity tests showed a strong overlap between generated and original texts. However, readability assessments indicated that the texts were insufficient in terms of clarity and accessibility. This study comparatively highlights the potential and limitations of popular and latest large language models for academic writing. While these models generate substantial and semantically accurate content, concerns regarding plagiarism, AI detection, and readability must be addressed for their effective use in scholarly work.","authors":["Omer Aydin","Enis Karaarslan","Fatih Safa Erenay","Nebojsa Bacanin"],"url":"https://arxiv.org/abs/2503.04765"}
{"created":"2025-05-05","title":"Generating Building-Level Heat Demand Time Series by Combining Occupancy Simulations and Thermal Modeling","abstract":"Despite various efforts, decarbonizing the heating sector remains a significant challenge. To tackle it by smart planning, the availability of highly resolved heating demand data is key. Several existing models provide heating demand only for specific applications. Typically, they either offer time series for a larger area or annual demand data on a building level, but not both simultaneously. Additionally, the diversity in heating demand across different buildings is often not considered. To address these limitations, this paper presents a novel method for generating temporally resolved heat demand time series at the building level using publicly available data. The approach integrates a thermal building model with stochastic occupancy simulations that account for variability in user behavior. As a result, the tool serves as a cost-effective resource for cross-sectoral energy system planning and policy development, particularly with a focus on the heating sector. The obtained data can be used to assess the impact of renovation and retrofitting strategies, or to analyze district heating expansion. To illustrate the potential applications of this approach, we conducted a case study in Puertollano (Spain), where we prepared a dataset of heating demand with hourly resolution for each of 9,298 residential buildings. This data was then used to compare two different pathways for the thermal renovation of these buildings. By relying on publicly available data, this method can be adapted and applied to various European regions, offering broad usability in energy system optimization and analysis of decarbonization strategies.","authors":["Simon Malacek","Jos\\'e Portela","Yannick Marcus Werner","Sonja Wogrin"],"url":"https://arxiv.org/abs/2503.05427"}
{"created":"2025-05-05","title":"Transfer Learning for LQR Control","abstract":"In this paper, we study a transfer learning framework for Linear Quadratic Regulator (LQR) control, where (i) the dynamics of the system of interest (target system) are unknown and only a short trajectory of impulse responses from the target system is provided, and (ii) impulse responses are available from $N$ source systems with different dynamics. We show that the LQR controller can be learned from a sufficiently long trajectory of impulse responses. Further, a transferable mode set can be identified using the available data from source systems and the target system, enabling the reconstruction of the target system's impulse responses for controller design. By leveraging data from source systems, we show that the sample complexity for synthesizing the LQR controller can be reduced by $50 \\%$. Algorithms and numerical examples are provided to demonstrate the implementation of the proposed transfer control framework.","authors":["Taosha Guo","Fabio Pasqualetti"],"url":"https://arxiv.org/abs/2503.06755"}
{"created":"2025-05-05","title":"Prompt Inversion Attack against Collaborative Inference of Large Language Models","abstract":"Large language models (LLMs) have been widely applied for their remarkable capability of content generation. However, the practical use of open-source LLMs is hindered by high resource requirements, making deployment expensive and limiting widespread development. The collaborative inference is a promising solution for this problem, in which users collaborate by each hosting a subset of layers and transmitting intermediate activation. Many companies are building collaborative inference platforms to reduce LLM serving costs, leveraging users' underutilized GPUs. Despite widespread interest in collaborative inference within academia and industry, the privacy risks associated with LLM collaborative inference have not been well studied. This is largely because of the challenge posed by inverting LLM activation due to its strong non-linearity.","authors":["Wenjie Qu","Yuguang Zhou","Yongji Wu","Tingsong Xiao","Binhang Yuan","Yiming Li","Jiaheng Zhang"],"url":"https://arxiv.org/abs/2503.09022"}
{"created":"2025-05-05","title":"\"Silent Is Not Actually Silent\": An Investigation of Toxicity on Bug Report Discussion","abstract":"Toxicity in bug report discussions poses significant challenges to the collaborative dynamics of open-source software development. Bug reports are crucial for identifying and resolving defects, yet their inherently problem-focused nature and emotionally charged context make them susceptible to toxic interactions. This study explores toxicity in GitHub bug reports through a qualitative analysis of 203 bug threads, including 81 toxic ones. Our findings reveal that toxicity frequently arises from misaligned perceptions of bug severity and priority, unresolved frustrations with tools, and lapses in professional communication. These toxic interactions not only derail productive discussions but also reduce the likelihood of actionable outcomes, such as linking issues with pull requests. Our preliminary findings offer actionable recommendations to improve bug resolution by mitigating toxicity.","authors":["Mia Mohammad Imran","Jaydeb Sarker"],"url":"https://arxiv.org/abs/2503.10072"}
{"created":"2025-05-05","title":"Learning-Based MPC for Fuel Efficient Control of Autonomous Vehicles with Discrete Gear Selection","abstract":"Co-optimization of both vehicle speed and gear position via model predictive control (MPC) has been shown to offer benefits for fuel-efficient autonomous driving. However, optimizing both the vehicle's continuous dynamics and discrete gear positions may be too computationally intensive for a real-time implementation. This work proposes a learning-based MPC scheme to address this issue. A policy is trained to select and fix the gear positions across the prediction horizon of the MPC controller, leaving a significantly simpler continuous optimization problem to be solved online. In simulation, the proposed approach is shown to have a significantly lower computation burden and a comparable performance, with respect to pure MPC-based co-optimization.","authors":["Samuel Mallick","Gianpietro Battocletti","Qizhang Dong","Azita Dabiri","Bart De Schutter"],"url":"https://arxiv.org/abs/2503.11359"}
{"created":"2025-05-05","title":"MAVEN: Multi-modal Attention for Valence-Arousal Emotion Network","abstract":"Dynamic emotion recognition in the wild remains challenging due to the transient nature of emotional expressions and temporal misalignment of multi-modal cues. Traditional approaches predict valence and arousal and often overlook the inherent correlation between these two dimensions. The proposed Multi-modal Attention for Valence-Arousal Emotion Network (MAVEN) integrates visual, audio, and textual modalities through a bi-directional cross-modal attention mechanism. MAVEN uses modality-specific encoders to extract features from synchronized video frames, audio segments, and transcripts, predicting emotions in polar coordinates following Russell's circumplex model. The evaluation of the Aff-Wild2 dataset using MAVEN achieved a concordance correlation coefficient (CCC) of 0.3061, surpassing the ResNet-50 baseline model with a CCC of 0.22. The multistage architecture captures the subtle and transient nature of emotional expressions in conversational videos and improves emotion recognition in real-world situations. The code is available at: https://github.com/Vrushank-Ahire/MAVEN_8th_ABAW","authors":["Vrushank Ahire","Kunal Shah","Mudasir Nazir Khan","Nikhil Pakhale","Lownish Rai Sookha","M. A. Ganaie","Abhinav Dhall"],"url":"https://arxiv.org/abs/2503.12623"}
{"created":"2025-05-05","title":"Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings","abstract":"Abusive behavior is common on online social networks, and forces the hosts of such platforms to find new solutions to address this problem. Various methods have been proposed to automate this task in the past decade. Most of them rely on the exchanged content, but ignore the structure and dynamics of the conversation, which could provide some relevant information. In this article, we propose to use representation learning methods to automatically produce embeddings of this textual content and of the conversational graphs depicting message exchanges. While the latter could be enhanced by including additional information on top of the raw conversational structure, no method currently exists to learn wholegraph representations using simultaneously edge directions, weights, signs, and vertex attributes. We propose two such methods to fill this gap in the literature. We experiment with 5 textual and 13 graph embedding methods, and apply them to a dataset of online messages annotated for abuse detection. Our best results achieve an F -measure of 81.02 using text alone and 80.61 using graphs alone. We also combine both modalities of information (text and graphs) through three fusion strategies, and show that this strongly improves abuse detection performance, increasing the F -measure to 87.06. Finally, we identify which specific engineered features are captured by the embedding methods under consideration. These features have clear interpretations and help explain what information the representation learning methods deem discriminative.","authors":["No\\'e Cecillon (LIA)","Vincent Labatut (LIA)","Richard Dufour (LS2N - \\'equipe TALN)"],"url":"https://arxiv.org/abs/2503.12994"}
{"created":"2025-05-05","title":"ADAPT: An Autonomous Forklift for Construction Site Operation","abstract":"Efficient material logistics play a critical role in controlling costs and schedules in the construction industry. However, manual material handling remains prone to inefficiencies, delays, and safety risks. Autonomous forklifts offer a promising solution to streamline on-site logistics, reducing reliance on human operators and mitigating labor shortages. This paper presents the development and evaluation of ADAPT (Autonomous Dynamic All-terrain Pallet Transporter), a fully autonomous off-road forklift designed for construction environments. Unlike structured warehouse settings, construction sites pose significant challenges, including dynamic obstacles, unstructured terrain, and varying weather conditions. To address these challenges, our system integrates AI-driven perception techniques with traditional approaches for decision making, planning, and control, enabling reliable operation in complex environments. We validate the system through extensive real-world testing, comparing its continuous performance against an experienced human operator across various weather conditions. Our findings demonstrate that autonomous outdoor forklifts can operate near human-level performance, offering a viable path toward safer and more efficient construction logistics.","authors":["Johannes Huemer","Markus Murschitz","Matthias Sch\\\"orghuber","Lukas Reisinger","Thomas Kadiofsky","Christoph Weidinger","Mario Niedermeyer","Benedikt Widy","Marcel Zeilinger","Csaba Beleznai","Tobias Gl\\\"uck","Andreas Kugi","Patrik Zips"],"url":"https://arxiv.org/abs/2503.14331"}
{"created":"2025-05-05","title":"A Study on Human-Swarm Interaction: A Framework for Assessing Situation Awareness and Task Performance","abstract":"This paper introduces a framework for human swarm interaction studies that measures situation awareness in dynamic environments. A tablet-based interface was developed for a user study by implementing the concepts introduced in the framework, where operators guided a robotic swarm in a single-target search task, marking hazardous cells unknown to the swarm. Both subjective and objective situation awareness measures were used, with task performance evaluated based on how close the robots were to the target. The framework enabled a structured investigation of the role of situation awareness in human swarm interaction, leading to key findings such as improved task performance across attempts, showing the interface was learnable, centroid active robot position proved to be a useful task performance metric for assessing situation awareness, perception and projection played a key role in task performance, highlighting their importance in interface design and objective situation awareness influenced both subjective situation awareness and task performance, emphasizing the need for interfaces that emphasise objective situation awareness. These findings validate our framework as a structured approach for integrating situation awareness concepts into human swarm interaction studies, offering a systematic way to assess situation awareness and task performance. The framework can be applied to other swarming studies to evaluate interface learnability, identify meaningful task performance metrics, and refine interface designs to enhance situation awareness, ultimately improving human swarm interaction in dynamic environments.","authors":["Wasura D. Wattearachchi","Erandi Lakshika","Kathryn Kasmarik","Michael Barlow"],"url":"https://arxiv.org/abs/2503.14810"}
{"created":"2025-05-05","title":"Synthesizing Grid Data with Cyber Resilience and Privacy Guarantees","abstract":"Differential privacy (DP) provides a principled approach to synthesizing data (e.g., loads) from real-world power systems while limiting the exposure of sensitive information. However, adversaries may exploit synthetic data to calibrate cyberattacks on the source grids. To control these risks, we propose new DP algorithms for synthesizing data that provide the source grids with both cyber resilience and privacy guarantees. The algorithms incorporate both normal operation and attack optimization models to balance the fidelity of synthesized data and cyber resilience. The resulting post-processing optimization is reformulated as a robust optimization problem, which is compatible with the exponential mechanism of DP to moderate its computational burden.","authors":["Shengyang Wu","Vladimir Dvorkin"],"url":"https://arxiv.org/abs/2503.14877"}
{"created":"2025-05-05","title":"WiCross: Indoor Human Zone-Crossing Detection Using Commodity WiFi Devices","abstract":"Detecting whether a target crosses the given zone (e.g., a door) can enable various practical applications in smart homes, including intelligent security and people counting. The traditional infrared-based approach only covers a line and can be easily cracked. In contrast, reusing the ubiquitous WiFi devices deployed in homes has the potential to cover a larger area of interest as WiFi signals are scattered throughout the entire space. By detecting the walking direction (i.e., approaching and moving away) with WiFi signal strength change, existing work can identify the behavior of crossing between WiFi transceiver pair. However, this method mistakenly classifies the turn-back behavior as crossing behavior, resulting in a high false alarm rate. In this paper, we propose WiCross, which can accurately distinguish the turn-back behavior with the phase statistics pattern of WiFi signals and thus robustly identify whether the target crosses the area between the WiFi transceiver pair. We implement WiCross with commercial WiFi devices and extensive experiments demonstrate that WiCross can achieve an accuracy higher than 95\\% with a false alarm rate of less than 5%.","authors":["Weiyan Shi","Xuanzhi Wang","Kai Niu","Leye Wang","Daqing Zhang"],"url":"https://arxiv.org/abs/2503.20331"}
{"created":"2025-05-05","title":"LLPut: Investigating Large Language Models for Bug Report-Based Input Generation","abstract":"Failure-inducing inputs play a crucial role in diagnosing and analyzing software bugs. Bug reports typically contain these inputs, which developers extract to facilitate debugging. Since bug reports are written in natural language, prior research has leveraged various Natural Language Processing (NLP) techniques for automated input extraction. With the advent of Large Language Models (LLMs), an important research question arises: how effectively can generative LLMs extract failure-inducing inputs from bug reports? In this paper, we propose LLPut, a technique to empirically evaluate the performance of three open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in extracting relevant inputs from bug reports. We conduct an experimental evaluation on a dataset of 206 bug reports to assess the accuracy and effectiveness of these models. Our findings provide insights into the capabilities and limitations of generative LLMs in automated bug diagnosis.","authors":["Alif Al Hasan","Subarna Saha","Mia Mohammad Imran","Tarannum Shaila Zaman"],"url":"https://arxiv.org/abs/2503.20578"}
{"created":"2025-05-05","title":"Reflections on Diversity: A Real-time Virtual Mirror for Inclusive 3D Face Transformations","abstract":"Real-time 3D face manipulation has significant applications in virtual reality, social media and human-computer interaction. This paper introduces a novel system, which we call Mirror of Diversity (MOD), that combines Generative Adversarial Networks (GANs) for texture manipulation and 3D Morphable Models (3DMMs) for facial geometry to achieve realistic face transformations that reflect various demographic characteristics, emphasizing the beauty of diversity and the universality of human features. As participants sit in front of a computer monitor with a camera positioned above, their facial characteristics are captured in real time and can further alter their digital face reconstruction with transformations reflecting different demographic characteristics, such as gender and ethnicity (e.g., a person from Africa, Asia, Europe). Another feature of our system, which we call Collective Face, generates an averaged face representation from multiple participants' facial data. A comprehensive evaluation protocol is implemented to assess the realism and demographic accuracy of the transformations. Qualitative feedback is gathered through participant questionnaires, which include comparisons of MOD transformations with similar filters on platforms like Snapchat and TikTok. Additionally, quantitative analysis is conducted using a pretrained Convolutional Neural Network that predicts gender and ethnicity, to validate the accuracy of demographic transformations.","authors":["Paraskevi Valergaki","Antonis Argyros","Giorgos Giannakakis","Anastasios Roussos"],"url":"https://arxiv.org/abs/2503.20819"}
{"created":"2025-05-05","title":"SHIFT SNARE: Uncovering Secret Keys in FALCON via Single-Trace Analysis","abstract":"This paper presents a novel singletrace sidechannel attack on FALCON a latticebased postquantum digital signature protocol recently approved for standardization by NIST We target the discrete Gaussian sampling operation within FALCONs key generation scheme and demonstrate that a single power trace is sufficient to mount a successful attack Notably negating the results of a 63bit rightshift operation on 64bit secret values leaks critical information about the assignment of 1 versus 0 to intermediate coefficients during sampling These leaks enable full recovery of the secret key We demonstrate a groundup approach to the attack on an ARM CortexM4 microcontroller executing both the reference and optimized implementations from FALCONs NIST round 3 software package We successfully recovered all of the secret polynomials in FALCON We further quantify the attackers success rate using a univariate Gaussian template model providing generalizable guarantees Statistical analysis with over 500000 tests reveals a percoefficient success rate of 999999999478 and a fullkey recovery rate of 9999994654 for FALCON512 We verify that this vulnerability is present in all implementations included in FALCONs NIST submission package This highlights the vulnerability of current software implementations to singletrace attacks and underscores the urgent need for singletrace resilient software in embedded systems","authors":["Jinyi Qiu","Aydin Aysu"],"url":"https://arxiv.org/abs/2504.00320"}
{"created":"2025-05-05","title":"Improving Multiresource Job Scheduling with Markovian Service Rate Policies","abstract":"Modern cloud computing workloads are composed of multiresource jobs that require a variety of computational resources in order to run, such as CPU cores, memory, disk space, or hardware accelerators. A single cloud server can typically run many multiresource jobs in parallel, but only if the server has sufficient resources to satisfy the demands of every job. A scheduling policy must therefore select sets of multiresource jobs to run in parallel in order to minimize the mean response time across jobs -- the average time from when a job arrives to the system until it is completed. Unfortunately, achieving low response times by selecting sets of jobs that fully utilize the available server resources has proven to be a difficult problem.","authors":["Zhongrui Chen","Isaac Grosof","Benjamin Berg"],"url":"https://arxiv.org/abs/2504.08094"}
{"created":"2025-05-05","title":"Should you use LLMs to simulate opinions? Quality checks for early-stage deliberation","abstract":"The emergent capabilities of large language models (LLMs) have sparked interest in assessing their ability to simulate human opinions in a variety of contexts, potentially serving as surrogates for human subjects in opinion surveys. However, previous evaluations of this capability have depended heavily on costly, domain-specific human survey data, and mixed empirical results about LLM effectiveness create uncertainty for managers about whether investing in this technology is justified in early-stage research. To address these challenges, we introduce a series of quality checks to support early-stage deliberation about the viability of using LLMs for simulating human opinions. These checks emphasize logical constraints, model stability, and alignment with stakeholder expectations of model outputs, thereby reducing dependence on human-generated data in the initial stages of evaluation. We demonstrate the usefulness of the proposed quality control tests in the context of AI-assisted content moderation, an application that both advocates and critics of LLMs' capabilities to simulate human opinion see as a desirable potential use case. None of the tested models passed all quality control checks, revealing several failure modes. We conclude by discussing implications of these failure modes and recommend how organizations can utilize our proposed tests for prompt engineering and in their risk management practices when considering the use of LLMs for opinion simulation. We make our crowdsourced dataset of claims with human and LLM annotations publicly available for future research.","authors":["Terrence Neumann","Maria De-Arteaga","Sina Fazelpour"],"url":"https://arxiv.org/abs/2504.08954"}
{"created":"2025-05-05","title":"MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation","abstract":"We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features.","authors":["Changhao Li","Yu Xin","Xiaowei Zhou","Ariel Shamir","Hao Zhang","Ligang Liu","Ruizhen Hu"],"url":"https://arxiv.org/abs/2504.09149"}
{"created":"2025-05-05","title":"Online Model Order Reduction of Linear Systems via $(\\gamma,\\delta)$-Similarity","abstract":"Model order reduction aims to determine a low-order approximation of high-order models with least possible approximation errors. For application to physical systems, it is crucial that the reduced order model (ROM) is robust to any disturbance that acts on the full order model (FOM) -- in the sense that the output of the ROM remains a good approximation of that of the FOM, even in the presence of such disturbances. In this work, we present a framework for online model order reduction for a class of continuous-time linear systems that ensures this property for any $\\mathcal{L}_2$ disturbance. Apart from robustness to disturbances in this sense, the proposed framework also displays other desirable properties for model order reduction: (1) a provable bound on the error defined as the $L_2$ norm of the difference between the output of the ROM and FOM, (2) preservation of stability, (3) compositionality properties and a provable error bound for arbitrary interconnected systems, (4) a provable bound on the output of the FOM when the controller designed for the ROM is used with the FOM, and finally, (5) compatibility with existing approaches such as balanced truncation and moment matching. Property (4) does not require computation of any gap metric and property (5) is beneficial as existing approaches can also be equipped with some of the preceding properties. The theoretical results are corroborated on numerical case studies, including on a building model.","authors":["Shivam Bajaj","Carolyn L. Beck","Vijay Gupta"],"url":"https://arxiv.org/abs/2504.10437"}
{"created":"2025-05-05","title":"Accuracy is Not Agreement: Expert-Aligned Evaluation of Crash Narrative Classification Models","abstract":"This study investigates the relationship between deep learning (DL) model accuracy and expert agreement in classifying crash narratives. We evaluate five DL models -- including BERT variants, USE, and a zero-shot classifier -- against expert labels and narratives, and extend the analysis to four large language models (LLMs): GPT-4, LLaMA 3, Qwen, and Claude. Our findings reveal an inverse relationship: models with higher technical accuracy often show lower agreement with human experts, while LLMs demonstrate stronger expert alignment despite lower accuracy. We use Cohen's Kappa and Principal Component Analysis (PCA) to quantify and visualize model-expert agreement, and employ SHAP analysis to explain misclassifications. Results show that expert-aligned models rely more on contextual and temporal cues than location-specific keywords. These findings suggest that accuracy alone is insufficient for safety-critical NLP tasks. We argue for incorporating expert agreement into model evaluation frameworks and highlight the potential of LLMs as interpretable tools in crash analysis pipelines.","authors":["Sudesh Ramesh Bhagat","Ibne Farabi Shihab","Anuj Sharma"],"url":"https://arxiv.org/abs/2504.13068"}
{"created":"2025-05-05","title":"DConAD: A Differencing-based Contrastive Representation Learning Framework for Time Series Anomaly Detection","abstract":"Time series anomaly detection holds notable importance for risk identification and fault detection across diverse application domains. Unsupervised learning methods have become popular because they have no requirement for labels. However, due to the challenges posed by the multiplicity of abnormal patterns, the sparsity of anomalies, and the growth of data scale and complexity, these methods often fail to capture robust and representative dependencies within the time series for identifying anomalies. To enhance the ability of models to capture normal patterns of time series and avoid the retrogression of modeling ability triggered by the dependencies on high-quality prior knowledge, we propose a differencing-based contrastive representation learning framework for time series anomaly detection (DConAD). Specifically, DConAD generates differential data to provide additional information about time series and utilizes transformer-based architecture to capture spatiotemporal dependencies, which enhances the robustness of unbiased representation learning ability. Furthermore, DConAD implements a novel KL divergence-based contrastive learning paradigm that only uses positive samples to avoid deviation from reconstruction and deploys the stop-gradient strategy to compel convergence. Extensive experiments on five public datasets show the superiority and effectiveness of DConAD compared with nine baselines. The code is available at https://github.com/shaieesss/DConAD.","authors":["Wenxin Zhang","Xiaojian Lin","Wenjun Yu","Guangzhen Yao","jingxiang Zhong","Yu Li","Renda Han","Songcheng Xu","Hao Shi","Cuicui Luo"],"url":"https://arxiv.org/abs/2504.14204"}
{"created":"2025-05-05","title":"MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video","abstract":"We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a physically-inspired Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both global camera and local object motion. Our MoBGS framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent advanced methods (DyBluRF and Deblur4DGS), achieving state-of-the-art performance for dynamic NVS under motion blur.","authors":["Minh-Quan Viet Bui","Jongmin Park","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"url":"https://arxiv.org/abs/2504.15122"}
{"created":"2025-05-05","title":"An Automated Pipeline for Few-Shot Bird Call Classification: A Case Study with the Tooth-Billed Pigeon","abstract":"This paper presents an automated one-shot bird call classification pipeline designed for rare species absent from large publicly available classifiers like BirdNET and Perch. While these models excel at detecting common birds with abundant training data, they lack options for species with only 1-3 known recordings-a critical limitation for conservationists monitoring the last remaining individuals of endangered birds. To address this, we leverage the embedding space of large bird classification networks and develop a classifier using cosine similarity, combined with filtering and denoising preprocessing techniques, to optimize detection with minimal training data. We evaluate various embedding spaces using clustering metrics and validate our approach in both a simulated scenario with Xeno-Canto recordings and a real-world test on the critically endangered tooth-billed pigeon (Didunculus strigirostris), which has no existing classifiers and only three confirmed recordings. The final model achieved 1.0 recall and 0.95 accuracy in detecting tooth-billed pigeon calls, making it practical for use in the field. This open-source system provides a practical tool for conservationists seeking to detect and monitor rare species on the brink of extinction.","authors":["Abhishek Jana","Moeumu Uili","James Atherton","Mark O'Brien","Joe Wood","Leandra Brickson"],"url":"https://arxiv.org/abs/2504.16276"}
{"created":"2025-05-05","title":"Network Sampling: An Overview and Comparative Analysis","abstract":"Network sampling is a crucial technique for analyzing large or partially observable networks. However, the effectiveness of different sampling methods can vary significantly depending on the context. In this study, we empirically compare representative methods from three main categories: node-based, edge-based, and exploration-based sampling. We used two real-world datasets for our analysis: a scientific collaboration network and a temporal message-sending network. Our results indicate that no single sampling method consistently outperforms the others in both datasets. Although advanced methods tend to provide better accuracy on static networks, they often perform poorly on temporal networks, where simpler techniques can be more effective. These findings suggest that the best sampling strategy depends not only on the structural characteristics of the network but also on the specific metrics that need to be preserved or analyzed. Our work offers practical insights for researchers in choosing sampling approaches that are tailored to different types of networks and analytical objectives.","authors":["Quoc Chuong Nguyen"],"url":"https://arxiv.org/abs/2504.17701"}
{"created":"2025-05-05","title":"Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy","abstract":"To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available: github.com/fangzr/TOC-Edge-Aerial.","authors":["Zhengru Fang","Zhenghao Liu","Jingjing Wang","Senkang Hu","Yu Guo","Yiqin Deng","Yuguang Fang"],"url":"https://arxiv.org/abs/2504.18317"}
{"created":"2025-05-05","title":"Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization","abstract":"The Internet of Vehicles (IoV) is transforming transportation by enhancing connectivity and enabling autonomous driving. However, this increased interconnectivity introduces new security vulnerabilities. Bot malware and cyberattacks pose significant risks to Connected and Autonomous Vehicles (CAVs), as demonstrated by real-world incidents involving remote vehicle system compromise. To address these challenges, we propose an edge-based Intrusion Detection System (IDS) that monitors network traffic to and from CAVs. Our detection model is based on a meta-ensemble classifier capable of recognizing known (Nday) attacks and detecting previously unseen (zero-day) attacks. The approach involves training multiple Isolation Forest (IF) models on Multi-access Edge Computing (MEC) servers, with each IF specialized in identifying a specific type of botnet attack. These IFs, either trained locally or shared by other MEC nodes, are then aggregated using a Particle Swarm Optimization (PSO) based stacking strategy to construct a robust meta-classifier. The proposed IDS has been evaluated on a vehicular botnet dataset, achieving an average detection rate of 92.80% for N-day attacks and 77.32% for zero-day attacks. These results highlight the effectiveness of our solution in detecting both known and emerging threats, providing a scalable and adaptive defense mechanism for CAVs within the IoV ecosystem.","authors":["Abdelaziz Amara Korba","Nour Elislem Karabadji","Yacine Ghamri-Doudane"],"url":"https://arxiv.org/abs/2504.18814"}
{"created":"2025-05-05","title":"Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events","abstract":"Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.","authors":["Pouya Shaeri","Yasaman Mohammadpour","Alimohammad Beigi","Ariane Middel","Huan Liu"],"url":"https://arxiv.org/abs/2504.18837"}
{"created":"2025-05-05","title":"A Multi-Language Perspective on the Robustness of LLM Code Generation","abstract":"Large language models have gained significant traction and popularity in recent times, extending their usage to code-generation tasks. While this field has garnered considerable attention, the exploration of testing and evaluating the robustness of code generation models remains an ongoing endeavor. Previous studies have primarily focused on code generation models specifically for the Python language, overlooking other widely used programming languages. In this research, we conduct a comprehensive comparative analysis to assess the robustness performance of several prominent code generation models. Furthermore, we investigate how their performance varies across different programming languages. To accomplish this, we introduce perturbations in four key areas of the prompt: DocString, function name, syntax, and format. We have compiled and released a dedicated dataset for this purpose. This work presents our experimental findings, shedding light on the performance of code generation models in various scenarios.","authors":["Fazle Rabbi","Zishuo Ding","Jinqiu Yang"],"url":"https://arxiv.org/abs/2504.19108"}
{"created":"2025-05-05","title":"Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments","abstract":"Task robust adaptation is a long-standing pursuit in sequential decision-making. Some risk-averse strategies, e.g., the conditional value-at-risk principle, are incorporated in domain randomization or meta reinforcement learning to prioritize difficult tasks in optimization, which demand costly intensive evaluations. The efficiency issue prompts the development of robust active task sampling to train adaptive policies, where risk-predictive models are used to surrogate policy evaluation. This work characterizes the optimization pipeline of robust active task sampling as a Markov decision process, posits theoretical and practical insights, and constitutes robustness concepts in risk-averse scenarios. Importantly, we propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making. Extensive experiments show that PDTS unlocks the potential of robust active task sampling, significantly improves the zero-shot and few-shot adaptation robustness in challenging tasks, and even accelerates the learning process under certain scenarios. Our project website is at https://thu-rllab.github.io/PDTS_project_page.","authors":["Yun Qu","Qi Cheems Wang","Yixiu Mao","Yiqin Lv","Xiangyang Ji"],"url":"https://arxiv.org/abs/2504.19139"}
{"created":"2025-05-05","title":"ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery","abstract":"Adverse weather conditions challenge safe transportation, necessitating robust real-time weather detection from traffic camera imagery. We propose a novel framework combining CycleGAN-based domain adaptation with efficient contrastive learning to enhance weather classification, particularly in low-light nighttime conditions. Our approach leverages the lightweight SigLIP-2 model, which employs pairwise sigmoid loss to reduce computational demands, integrated with CycleGAN to transform nighttime images into day-like representations while preserving weather cues. Evaluated on an Iowa Department of Transportation dataset, the baseline EVA-02 model with CLIP achieves a per-class overall accuracy of 96.55\\% across three weather conditions (No Precipitation, Rain, Snow) and a day/night overall accuracy of 96.55\\%, but shows a significant day-night gap (97.21\\% day vs.\\ 63.40\\% night). With CycleGAN, EVA-02 improves to 97.01\\% per-class accuracy and 96.85\\% day/night accuracy, boosting nighttime performance to 82.45\\%. Our Vision-SigLIP-2 + Text-SigLIP-2 + CycleGAN + Contrastive configuration excels in nighttime scenarios, achieving the highest nighttime accuracy of 85.90\\%, with 94.00\\% per-class accuracy and 93.35\\% day/night accuracy. This model reduces training time by 89\\% (from 6 hours to 40 minutes) and inference time by 80\\% (from 15 seconds to 3 seconds) compared to EVA-02. By narrowing the day-night performance gap from 33.81 to 8.90 percentage points, our framework provides a scalable, efficient solution for all-weather classification using existing camera infrastructure.","authors":["Anush Lakshman Sivaraman","Kojo Adu-Gyamfi","Ibne Farabi Shihab","Anuj Sharma"],"url":"https://arxiv.org/abs/2504.19684"}
{"created":"2025-05-05","title":"Simple Finite-Length Achievability and Converse Bounds for the Deletion Channel and the Insertion Channel","abstract":"We develop upper bounds on code size for independent and identically distributed deletion (insertion) channel for given code length and target frame error probability. The bounds are obtained as a variation of a general converse bound, which, though available for any channel, is inefficient and not easily computable without a good reference distribution over the output alphabet. We obtain a reference output distribution for a general finite-input finite-output channel and provide a simple formula for the converse bound on the capacity employing this distribution. We then evaluate the bound for the deletion channel with a finite block length and show that the resulting upper bound on the code side is tighter than that for a binary erasure channel, which is the only alternative converse bound for this finite-length setting. Also, we provide the similar results for the insertion channel.","authors":["Ruslan Morozov","Tolga Mete Duman"],"url":"https://arxiv.org/abs/2504.20961"}
{"created":"2025-05-05","title":"A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage","abstract":"Sanitizing sensitive text data typically involves removing personally identifiable information (PII) or generating synthetic data under the assumption that these methods adequately protect privacy; however, their effectiveness is often only assessed by measuring the leakage of explicit identifiers but ignoring nuanced textual markers that can lead to re-identification. We challenge the above illusion of privacy by proposing a new framework that evaluates re-identification attacks to quantify individual privacy risks upon data release. Our approach shows that seemingly innocuous auxiliary information -- such as routine social activities -- can be used to infer sensitive attributes like age or substance use history from sanitized data. For instance, we demonstrate that Azure's commercial PII removal tool fails to protect 74\\% of information in the MedQA dataset. Although differential privacy mitigates these risks to some extent, it significantly reduces the utility of the sanitized text for downstream tasks. Our findings indicate that current sanitization techniques offer a \\textit{false sense of privacy}, highlighting the need for more robust methods that protect against semantic-level information leakage.","authors":["Rui Xin","Niloofar Mireshghallah","Shuyue Stella Li","Michael Duan","Hyunwoo Kim","Yejin Choi","Yulia Tsvetkov","Sewoong Oh","Pang Wei Koh"],"url":"https://arxiv.org/abs/2504.21035"}
{"created":"2025-05-05","title":"Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection","abstract":"With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort.","authors":["Suwen Yang","Lei Shi"],"url":"https://arxiv.org/abs/2504.21357"}
{"created":"2025-05-05","title":"A Unified QoS-Aware Multiplexing Framework for Next Generation Immersive Communication with Legacy Wireless Applications","abstract":"Immersive communication, including emerging augmented reality, virtual reality, and holographic telepresence, has been identified as a key service for enabling next-generation wireless applications. To align with legacy wireless applications, such as enhanced mobile broadband or ultra-reliable low-latency communication, network slicing has been widely adopted. However, attempting to statistically isolate the above types of wireless applications through different network slices may lead to throughput degradation and increased queue backlog. To address these challenges, we establish a unified QoS-aware framework that supports immersive communication and legacy wireless applications simultaneously. Based on the Lyapunov drift theorem, we transform the original long-term throughput maximization problem into an equivalent short-term throughput maximization weighted by virtual queue length. Moreover, to cope with the challenges introduced by the interaction between large-timescale network slicing and short-timescale resource allocation, we propose an adaptive adversarial slicing (Ad2S) scheme for networks with invarying channel statistics. To track the network channel variations, we also propose a measurement extrapolation-Kalman filter (ME-KF)-based method and refine our scheme into Ad2S-non-stationary refinement (Ad2S-NR). Through extended numerical examples, we demonstrate that our proposed schemes achieve 3.86 Mbps throughput improvement and 63.96% latency reduction with 24.36% convergence time reduction. Within our framework, the trade-off between total throughput and user service experience can be achieved by tuning systematic parameters.","authors":["Jihong Li","Shunqing Zhang","Tao Yu","Guangjin Pan","Kaixuan Huang","Xiaojing Chen","Yanzan Sun","Junyu Liu","Jiandong Li","Derrick Wing Kwan Ng"],"url":"https://arxiv.org/abs/2504.21444"}
{"created":"2025-05-05","title":"Fast Sign Retrieval via Sub-band Convolution: An Elementary Extension of Binary Classification","abstract":"To efficiently compress the sign information of images, we address a sign retrieval problem for the block-wise discrete cosine transformation (DCT): reconstruction of the signs of DCT coefficients from their amplitudes. To this end, we propose a fast sign retrieval method on the basis of binary classification machine learning. We first introduce 3D representations of the amplitudes and signs, where we pack amplitudes/signs belonging to the same frequency band into a 2D slice, referred to as the sub-band block. We then retrieve the signs from the 3D amplitudes via binary classification, where each sign is regarded as a binary label. We implement a binary classification algorithm using convolutional neural networks, which are advantageous for efficiently extracting features in the 3D amplitudes. Experimental results demonstrate that our method achieves accurate sign retrieval with an overwhelmingly low computation cost.","authors":["Fuma Ito","Chihiro Tsutake","Keita Takahashi","Toshiaki Fujii"],"url":"https://arxiv.org/abs/2504.21632"}
{"created":"2025-05-05","title":"Mode and Ridge Estimation in Euclidean and Directional Product Spaces: A Mean Shift Approach","abstract":"The set of local modes and density ridge lines are important summary characteristics of the data-generating distribution. In this work, we focus on estimating local modes and density ridges from point cloud data in a product space combining two or more Euclidean and/or directional metric spaces. Specifically, our approach extends the (subspace constrained) mean shift algorithm to such product spaces, addressing potential challenges in the generalization process. We establish the algorithmic convergence of the proposed methods, along with practical implementation guidelines. Experiments on simulated and real-world datasets demonstrate the effectiveness of our proposed methods.","authors":["Yikun Zhang","Yen-Chi Chen"],"url":"https://arxiv.org/abs/2110.08505"}
{"created":"2025-05-05","title":"A Normal Map-Based Proximal Stochastic Gradient Method: Convergence and Identification Properties","abstract":"The proximal stochastic gradient method (PSGD) is one of the state-of-the-art approaches for stochastic composite-type problems. In contrast to its deterministic counterpart, PSGD has been found to have difficulties with the correct identification of underlying substructures (such as supports, low rank patterns, or active constraints) and it does not possess a finite-time manifold identification property. Existing solutions rely on convexity assumptions or on the additional usage of variance reduction techniques. In this paper, we address these limitations and present a simple variant of PSGD based on Robinson's normal map. The proposed normal map-based proximal stochastic gradient method (NSGD) is shown to converge globally, i.e., accumulation points of the generated iterates correspond to stationary points almost surely. In addition, we establish complexity bounds for NSGD that match the known results for PSGD and we prove that NSGD can almost surely identify active manifolds in finite-time in a general nonconvex setting. Our derivations are built on almost sure iterate convergence guarantees and utilize analysis techniques based on the Kurdyka-Lojasiewicz inequality.","authors":["Junwen Qiu","Li Jiang","Andre Milzarek"],"url":"https://arxiv.org/abs/2305.05828"}
{"created":"2025-05-05","title":"Volumetric medical image segmentation through dual self-distillation in U-shaped networks","abstract":"U-shaped networks and its variants have demonstrated exceptional results for medical image segmentation. In this paper, we propose a novel dual self-distillation (DSD) framework in U-shaped networks for volumetric medical image segmentation. DSD distills knowledge from the ground-truth segmentation labels to the decoder layers. Additionally, DSD also distills knowledge from the deepest decoder and encoder layer to the shallower decoder and encoder layers respectively of a single U-shaped network. DSD is a general training strategy that could be attached to the backbone architecture of any U-shaped network to further improve its segmentation performance. We attached DSD on several state-of-the-art U-shaped backbones, and extensive experiments on various public 3D medical image segmentation datasets (cardiac substructure, brain tumor and Hippocampus) demonstrated significant improvement over the same backbones without DSD. On average, after attaching DSD to the U-shaped backbones, we observed an increase of 2.82\\%, 4.53\\% and 1.3\\% in Dice similarity score, a decrease of 7.15 mm, 6.48 mm and 0.76 mm in the Hausdorff distance, for cardiac substructure, brain tumor and Hippocampus segmentation, respectively. These improvements were achieved with negligible increase in the number of trainable parameters and training time. Our proposed DSD framework also led to significant qualitative improvements for cardiac substructure, brain tumor and Hippocampus segmentation over the U-shaped backbones. The source code is publicly available at https://github.com/soumbane/DualSelfDistillation.","authors":["Soumyanil Banerjee","Nicholas Summerfield","Ming Dong","Carri Glide-Hurst"],"url":"https://arxiv.org/abs/2306.03271"}
{"created":"2025-05-05","title":"Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm","abstract":"Graphical models have exhibited their performance in numerous tasks ranging from biological analysis to recommender systems. However, graphical models with hub nodes are computationally difficult to fit, particularly when the dimension of the data is large. To efficiently estimate the hub graphical models, we introduce a two-phase algorithm. The proposed algorithm first generates a good initial point via a dual alternating direction method of multipliers (ADMM), and then warm starts a semismooth Newton (SSN) based augmented Lagrangian method (ALM) to compute a solution that is accurate enough for practical tasks. We fully excavate the sparsity structure of the generalized Jacobian arising from the hubs in the graphical models, which ensures that the algorithm can obtain a nice solution very efficiently. Comprehensive experiments on both synthetic data and real data show that it obviously outperforms the existing state-of-the-art algorithms. In particular, in some high dimensional tasks, it can save more than 70\\% of the execution time, meanwhile still achieves a high-quality estimation.","authors":["Chengjing Wang","Peipei Tang","Wenling He","Meixia Lin"],"url":"https://arxiv.org/abs/2308.08852"}
{"created":"2025-05-05","title":"A Quadratic Speedup in Finding Nash Equilibria of Quantum Zero-Sum Games","abstract":"Recent developments in domains such as non-local games, quantum interactive proofs, and quantum generative adversarial networks have renewed interest in quantum game theory and, specifically, quantum zero-sum games. Central to classical game theory is the efficient algorithmic computation of Nash equilibria, which represent optimal strategies for both players. In 2008, Jain and Watrous proposed the first classical algorithm for computing equilibria in quantum zero-sum games using the Matrix Multiplicative Weight Updates (MMWU) method to achieve a convergence rate of $\\mathcal{O}(d/\\epsilon^2)$ iterations to $\\epsilon$-Nash equilibria in the $4^d$-dimensional spectraplex. In this work, we propose a hierarchy of quantum optimization algorithms that generalize MMWU via an extra-gradient mechanism. Notably, within this proposed hierarchy, we introduce the Optimistic Matrix Multiplicative Weights Update (OMMWU) algorithm and establish its average-iterate convergence complexity as $\\mathcal{O}(d/\\epsilon)$ iterations to $\\epsilon$-Nash equilibria. This quadratic speed-up relative to Jain and Watrous' original algorithm sets a new benchmark for computing $\\epsilon$-Nash equilibria in quantum zero-sum games.","authors":["Francisca Vasconcelos","Emmanouil-Vasileios Vlatakis-Gkaragkounis","Panayotis Mertikopoulos","Georgios Piliouras","Michael I. Jordan"],"url":"https://arxiv.org/abs/2311.10859"}
{"created":"2025-05-05","title":"Multivariate Density Estimation via Variance-Reduced Sketching","abstract":"Multivariate density estimation is of great interest in various scientific and engineering disciplines. In this work, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate multivariate density functions with a reduced curse of dimensionality. Our VRS framework conceptualizes multivariate functions as infinite-size matrices/tensors, and facilitates a new sketching technique motivated by the numerical linear algebra literature to reduce the variance in density estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network density estimators and classical kernel methods in numerous distribution models. Additionally, we offer theoretical justifications for VRS to support its ability to deliver density estimation with a reduced curse of dimensionality.","authors":["Yifan Peng","Yuehaw Khoo","Daren Wang"],"url":"https://arxiv.org/abs/2401.11646"}
{"created":"2025-05-05","title":"Employing Federated Learning for Training Autonomous HVAC Systems","abstract":"Buildings account for 40% of global energy consumption. A considerable portion of building energy consumption stems from heating, ventilation, and air conditioning (HVAC), and thus implementing smart, energy-efficient HVAC systems has the potential to significantly impact the course of climate change. In recent years, model-free reinforcement learning algorithms have been increasingly assessed for this purpose due to their ability to learn and adapt purely from experience. They have been shown to outperform classical controllers in terms of energy cost and consumption, as well as thermal comfort. However, their weakness lies in their relatively poor data efficiency, requiring long periods of training to reach acceptable policies, making them inapplicable to real-world controllers directly. In this paper, we demonstrate that using federated learning to train the reinforcement learning controller of HVAC systems can improve the learning speed, as well as improve their ability to generalize, which in turn facilitates transfer learning to unseen building environments. In our setting, a global control policy is learned by aggregating local policies trained on multiple data centers located in different climate zones. The goal of the policy is to minimize energy consumption and maximize thermal comfort. We perform experiments evaluating three different optimizers for local policy training, as well as three different federated learning algorithms against two alternative baselines. Our experiments show that these effects lead to a faster learning speed, as well as greater generalization capabilities in the federated policy compared to any individually trained policy. Furthermore, the learning stability is significantly improved, with the learning process and performance of the federated policy being less sensitive to the choice of parameters and the inherent randomness of reinforcement learning.","authors":["Fredrik Hagstr\\\"om","Vikas Garg","Fabricio Oliveira"],"url":"https://arxiv.org/abs/2405.00389"}
{"created":"2025-05-05","title":"Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE","abstract":"This paper shows that dimensionality reduction methods such as UMAP and t-SNE, can be approximately recast as MAP inference methods corresponding to a model introduced in Ravuri et al. (2023), that describes the graph Laplacian (an estimate of the data precision matrix) using a Wishart distribution, with a mean given by a non-linear covariance function evaluated on the latents. This interpretation offers deeper theoretical and semantic insights into such algorithms, and forging a connection to Gaussian process latent variable models by showing that well-known kernels can be used to describe covariances implied by graph Laplacians. We also introduce tools with which similar dimensionality reduction methods can be studied.","authors":["Aditya Ravuri","Neil D. Lawrence"],"url":"https://arxiv.org/abs/2405.17412"}
{"created":"2025-05-05","title":"Exponential Quantum Advantage for Pathfinding in Regular Sunflower Graphs","abstract":"Finding problems that allow for superpolynomial quantum speedup is one of the most important tasks in quantum computation. A key challenge is identifying problem structures that can only be exploited by quantum mechanics. In this paper, we find a class of graphs that allows for exponential quantum-classical separation for the pathfinding problem with the adjacency list oracle, and this class of graphs is named regular sunflower graphs. We prove that, with high probability, a regular sunflower graph of degree at least $7$ is a mild expander graph, that is, the spectral gap of the graph Laplacian is at least inverse polylogarithmic in the graph size.","authors":["Jianqiang Li","Yu Tong"],"url":"https://arxiv.org/abs/2407.14398"}
{"created":"2025-05-05","title":"Mesh-based Super-Resolution of Fluid Flows with Multiscale Graph Neural Networks","abstract":"A graph neural network (GNN) approach is introduced in this work which enables mesh-based three-dimensional super-resolution of fluid flows. In this framework, the GNN is designed to operate not on the full mesh-based field at once, but on localized meshes of elements (or cells) directly. To facilitate mesh-based GNN representations in a manner similar to spectral (or finite) element discretizations, a baseline GNN layer (termed a message passing layer, which updates local node properties) is modified to account for synchronization of coincident graph nodes, rendering compatibility with commonly used element-based mesh connectivities. The architecture is multiscale in nature, and is comprised of a combination of coarse-scale and fine-scale message passing layer sequences (termed processors) separated by a graph unpooling layer. The coarse-scale processor embeds a query element (alongside a set number of neighboring coarse elements) into a single latent graph representation using coarse-scale synchronized message passing over the element neighborhood, and the fine-scale processor leverages additional message passing operations on this latent graph to correct for interpolation errors. Demonstration studies are performed using hexahedral mesh-based data from Taylor-Green Vortex and backward-facing step flow simulations at Reynolds numbers of 1600 and 3200. Through analysis of both global and local errors, the results ultimately show how the GNN is able to produce accurate super-resolved fields compared to targets in both coarse-scale and multiscale model configurations. Reconstruction errors for fixed architectures were found to increase in proportion to the Reynolds number. Geometry extrapolation studies on a separate cavity flow configuration show promising cross-mesh capabilities of the super-resolution strategy.","authors":["Shivam Barwey","Pinaki Pal","Saumil Patel","Riccardo Balin","Bethany Lusch","Venkatram Vishwanath","Romit Maulik","Ramesh Balakrishnan"],"url":"https://arxiv.org/abs/2409.07769"}
{"created":"2025-05-05","title":"Deep Kernel Posterior Learning under Infinite Variance Prior Weights","abstract":"Neal (1996) proved that infinitely wide shallow Bayesian neural networks (BNN) converge to Gaussian processes (GP), when the network weights have bounded prior variance. Cho & Saul (2009) provided a useful recursive formula for deep kernel processes for relating the covariance kernel of each layer to the layer immediately below. Moreover, they worked out the form of the layer-wise covariance kernel in an explicit manner for several common activation functions. Recent works, including Aitchison et al. (2021), have highlighted that the covariance kernels obtained in this manner are deterministic and hence, precludes any possibility of representation learning, which amounts to learning a non-degenerate posterior of a random kernel given the data. To address this, they propose adding artificial noise to the kernel to retain stochasticity, and develop deep kernel inverse Wishart processes. Nonetheless, this artificial noise injection could be critiqued in that it would not naturally emerge in a classic BNN architecture under an infinite-width limit. To address this, we show that a Bayesian deep neural network, where each layer width approaches infinity, and all network weights are elliptically distributed with infinite variance, converges to a process with $\\alpha$-stable marginals in each layer that has a conditionally Gaussian representation. These conditional random covariance kernels could be recursively linked in the manner of Cho & Saul (2009), even though marginally the process exhibits stable behavior, and hence covariances are not even necessarily defined. We also provide useful generalizations of the recent results of Lor\\'ia & Bhadra (2024) on shallow networks to multi-layer networks, and remedy the computational burden of their approach. The computational and statistical benefits over competing approaches stand out in simulations and in demonstrations on benchmark data sets.","authors":["Jorge Lor\\'ia","Anindya Bhadra"],"url":"https://arxiv.org/abs/2410.01284"}
{"created":"2025-05-05","title":"Reinforcement learning with learned gadgets to tackle hard quantum problems on real hardware","abstract":"Designing quantum circuits for specific tasks is challenging due to the exponential growth of the state space. We introduce gadget reinforcement learning (GRL), which integrates reinforcement learning with program synthesis to automatically generate and incorporate composite gates (gadgets) into the action space. This enhances the exploration of parameterized quantum circuits (PQCs) for complex tasks like approximating ground states of quantum Hamiltonians, an NP-hard problem. We evaluate GRL using the transverse field Ising model under typical computational budgets (e.g., 2- 3 days of GPU runtime). Our results show improved accuracy, hardware compatibility and scalability. GRL exhibits robust performance as the size and complexity of the problem increases, even with constrained computational resources. By integrating gadget extraction, GRL facilitates the discovery of reusable circuit components tailored for specific hardware, bridging the gap between algorithmic design and practical implementation. This makes GRL a versatile framework for optimizing quantum circuits with applications in hardware-specific optimizations and variational quantum algorithms. The code is available at: https://github.com/Aqasch/Gadget_RL","authors":["Akash Kundu","Leopoldo Sarra"],"url":"https://arxiv.org/abs/2411.00230"}
{"created":"2025-05-05","title":"Scaling of Stochastic Normalizing Flows in $\\mathrm{SU}(3)$ lattice gauge theory","abstract":"Non-equilibrium Markov Chain Monte Carlo (NE-MCMC) simulations provide a well-understood framework based on Jarzynski's equality to sample from a target probability distribution. By driving a base probability distribution out of equilibrium, observables are computed without the need to thermalize. If the base distribution is characterized by mild autocorrelations, this approach provides a way to mitigate critical slowing down. Out-of-equilibrium evolutions share the same framework of flow-based approaches and they can be naturally combined into a novel architecture called Stochastic Normalizing Flows (SNFs). In this work we present the first implementation of SNFs for $\\mathrm{SU}(3)$ lattice gauge theory in 4 dimensions, defined by introducing gauge-equivariant layers between out-of-equilibrium Monte Carlo updates. The core of our analysis is focused on the promising scaling properties of this architecture with the degrees of freedom of the system, which are directly inherited from NE-MCMC. Finally, we discuss how systematic improvements of this approach can realistically lead to a general and yet efficient sampling strategy at fine lattice spacings for observables affected by long autocorrelation times.","authors":["Andrea Bulgarelli","Elia Cellini","Alessandro Nada"],"url":"https://arxiv.org/abs/2412.00200"}
{"created":"2025-05-05","title":"A Physics-Inspired Deep Learning Framework with Polar Coordinate Attention for Ptychographic Imaging","abstract":"Ptychographic imaging confronts inherent challenges in applying deep learning for phase retrieval from diffraction patterns. Conventional neural architectures, both convolutional neural networks and Transformer-based methods, are optimized for natural images with Euclidean spatial neighborhood-based inductive biases that exhibit geometric mismatch with the concentric coherent patterns characteristic of diffraction data in reciprocal space. In this paper, we present PPN, a physics-inspired deep learning network with Polar Coordinate Attention (PoCA) for ptychographic imaging, that aligns neural inductive biases with diffraction physics through a dual-branch architecture separating local feature extraction from non-local coherence modeling. It consists of a PoCA mechanism that replaces Euclidean spatial priors with physically consistent radial-angular correlations. PPN outperforms existing end-to-end models, with spectral and spatial analysis confirming its greater preservation of high-frequency details. Notably, PPN maintains robust performance compared to iterative methods even at low overlap ratios, making it well suited for high-throughput imaging in real-world acquisition scenarios for samples with consistent structural characteristics.","authors":["Han Yue","Jun Cheng","Yu-Xuan Ren","Chien-Chun Chen","Grant A. van Riessen","Philip Heng Wai Leong","Steve Feng Shu"],"url":"https://arxiv.org/abs/2412.06806"}
{"created":"2025-05-05","title":"Convex Data-Driven Contraction With Riemannian Metrics","abstract":"The growing complexity of dynamical systems and advances in data collection necessitates robust data-driven control strategies without explicit system identification and robust synthesis. Data-driven stability has been explored in linear and nonlinear systems, often by turning the problem into a linear or positive semidefinite program. This paper focuses on a new emerging property called contractivity, which refers to the exponential convergence of all system trajectories toward each other under a specified metric. Data-driven closed loop contractivity has been studied for the case of the 2-norm and assuming nonlinearities are Lipschitz bounded in subsets of n dimensional euclidean space. We extend the analysis by considering Riemannian metrics for polynomial dynamics. The key to our derivation is to leverage the convex criteria for closed-loop contraction and duality results to efficiently check infinite dimensional membership constraints. Numerical examples demonstrate the effectiveness of the proposed method for both linear and nonlinear systems, highlighting its potential for robust data-driven contraction.","authors":["Andreas Oliveira","Jian Zheng","Mario Sznaier"],"url":"https://arxiv.org/abs/2412.20283"}
{"created":"2025-05-05","title":"I-trustworthy Models. A framework for trustworthiness evaluation of probabilistic classifiers","abstract":"As probabilistic models continue to permeate various facets of our society and contribute to scientific advancements, it becomes a necessity to go beyond traditional metrics such as predictive accuracy and error rates and assess their trustworthiness. Grounded in the competence-based theory of trust, this work formalizes I-trustworthy framework -- a novel framework for assessing the trustworthiness of probabilistic classifiers for inference tasks by linking local calibration to trustworthiness. To assess I-trustworthiness, we use the local calibration error (LCE) and develop a method of hypothesis-testing. This method utilizes a kernel-based test statistic, Kernel Local Calibration Error (KLCE), to test local calibration of a probabilistic classifier. This study provides theoretical guarantees by offering convergence bounds for an unbiased estimator of KLCE. Additionally, we present a diagnostic tool designed to identify and measure biases in cases of miscalibration. The effectiveness of the proposed test statistic is demonstrated through its application to both simulated and real-world datasets. Finally, LCE of related recalibration methods is studied, and we provide evidence of insufficiency of existing methods to achieve I-trustworthiness.","authors":["Ritwik Vashistha","Arya Farahi"],"url":"https://arxiv.org/abs/2501.15617"}
{"created":"2025-05-05","title":"RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior","abstract":"Denoising diffusion probabilistic models (DDPMs) can be utilized for recovering a clean signal from its degraded observation(s) by conditioning the model on the degraded signal. The degraded signals are themselves contaminated versions of the clean signals; due to this correlation, they may encompass certain useful information about the target clean data distribution. However, existing adoption of the standard Gaussian as the prior distribution in turn discards such information, resulting in sub-optimal performance. In this paper, we propose to improve conditional DDPMs for signal restoration by leveraging a more informative prior that is jointly learned with the diffusion model. The proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the variational autoencoder framework and exploits the correlation between the degraded and clean signals to encode a better diffusion prior. On speech and image restoration tasks, we show that RestoreGrad demonstrates faster convergence (5-10 times fewer training steps) to achieve better quality of restored signals over existing DDPM baselines, and improved robustness to using fewer sampling steps in inference time (2-2.5 times fewer), advocating the advantages of leveraging jointly learned prior for efficiency improvements in the diffusion process.","authors":["Ching-Hua Lee","Chouchang Yang","Jaejin Cho","Yashas Malur Saidutta","Rakshith Sharma Srinivasa","Yilin Shen","Hongxia Jin"],"url":"https://arxiv.org/abs/2502.13574"}
{"created":"2025-05-05","title":"YARE-GAN: Yet Another Resting State EEG-GAN","abstract":"In this study, we implement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generate multi-channel resting-state EEG data and assess the quality of the synthesized signals through both visual and feature-based evaluations. Our results indicate that the model effectively captures the statistical and spectral characteristics of real EEG data, although challenges remain in replicating high-frequency oscillations in the frontal region. Additionally, we demonstrate that the Critic's learned representations can be reused for gender classification task, achieving an out-of-sample accuracy, significantly better than a shuffled-label baseline and a model trained directly on EEG data. These findings suggest that generative models can serve not only as EEG data generators but also as unsupervised feature extractors, reducing the need for manual feature engineering. This study highlights the potential of GAN-based unsupervised learning for EEG analysis, suggesting avenues for more data-efficient deep learning applications in neuroscience.","authors":["Yeganeh Farahzadi","Morteza Ansarinia","Zoltan Kekecs"],"url":"https://arxiv.org/abs/2503.02636"}
{"created":"2025-05-05","title":"Technical Insights and Legal Considerations for Advancing Federated Learning in Bioinformatics","abstract":"Federated learning leverages data across institutions to improve clinical discovery while complying with data-sharing restrictions and protecting patient privacy. As the evolution of biobanks in genetics and systems biology has proved, accessing more extensive and varied data pools leads to a faster and more robust exploration and translation of results. More widespread use of federated learning may have the same impact in bioinformatics, allowing access to many combinations of genotypic, phenotypic and environmental information that are undercovered or not included in existing biobanks. This paper reviews the methodological, infrastructural and legal issues that academic and clinical institutions must address before implementing it. Finally, we provide recommendations for the reliable use of federated learning and its effective translation into clinical practice.","authors":["Daniele Malpetti","Marco Scutari","Francesco Gualdi","Jessica van Setten","Sander van der Laan","Saskia Haitjema","Aaron Mark Lee","Isabelle Hering","Francesca Mangili"],"url":"https://arxiv.org/abs/2503.09649"}
{"created":"2025-05-05","title":"Optimal classification with outcome performativity","abstract":"I consider the problem of classifying individual behavior in a simple setting of outcome performativity where the behavior the algorithm seeks to classify is itself dependent on the algorithm. I show in this context that the most accurate classifier is either a threshold or a negative threshold rule. A threshold rule offers the \"good\" classification to those individuals more likely to have engaged in a desirable behavior, while a negative threshold rule offers the \"good\" outcome to those less likely to have engaged in the desirable behavior. While seemingly pathological, I show that a negative threshold rule can maximize classification accuracy when outcomes are performative. I provide an example of such a classifier, and extend the analysis to more general algorithm objectives. A key takeaway is that optimal classification can negatively correlate with signal information, yielding adverse downstream effects on individual behavior.","authors":["Elizabeth Maggie Penn"],"url":"https://arxiv.org/abs/2504.06127"}
{"created":"2025-05-05","title":"Deciphering scrolls with tomography: A training experiment","abstract":"The recovery of severely damaged ancient written documents has proven to be a major challenge for many scientists, mainly due to the impracticality of physical unwrapping them. Non-destructive techniques, such as X-ray computed tomography (CT), combined with computer vision algorithms, have emerged as a means of facilitating the virtual reading of the hidden contents of the damaged documents. This paper proposes an educational laboratory aimed at simulating the entire process of acquisition and virtual recovery of the ancient works. We have developed an experimental setup that uses visible light to replace the detrimental X-rays, and a didactic software pipeline that allows students to virtually reconstruct a transparent rolled sheet with printed text on it, the wrapped scroll.","authors":["Sonia Foschiatti","Axel Kittenberger","Otmar Scherzer"],"url":"https://arxiv.org/abs/2504.11485"}
{"created":"2025-05-05","title":"Asynchronous Push-sum Dual Gradient Algorithm in Distributed Model Predictive Control","abstract":"This paper studies the distributed model predictive control (DMPC) problem for distributed discrete-time linear systems with both local and global constraints over directed communication networks. We establish an optimization problem to formulate the DMPC policy, including the design of terminal ingredients. To cope with the global constraint, we transform the primal optimization problem into its dual problem. Then, we propose a novel asynchronous push-sum dual gradient (APDG) algorithm with an adaptive step-size scheme to solve this dual problem in a fully asynchronous distributed manner. The proposed algorithm does not require synchronous waiting and any form of coordination, which greatly improves solving efficiency. We theoretically prove that the APDG algorithm converges at an R-linear rate as long as the step-size does not exceed the designed upper bound. Furthermore, we develop a distributed termination criterion to terminate the APDG algorithm when its output solution satisfies the specified suboptimality and the global constraint, thereby avoiding an infinite number of iterations. The recursive feasibility and the stability of the closed-loop system are also established. Finally, a numerical example clarifies and validates theoretical findings.","authors":["Pengbiao Wang","Xuemei Ren","Dongdong Zheng"],"url":"https://arxiv.org/abs/2504.18941"}
{"created":"2025-05-05","title":"Provably faster randomized and quantum algorithms for $k$-means clustering via uniform sampling","abstract":"The $k$-means algorithm (Lloyd's algorithm) is a widely used method for clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that each iteration requires time linear in the number of data points, which can be expensive in big data applications. This was improved in recent works proposing quantum and quantum-inspired classical algorithms to approximate the $k$-means algorithm locally, in time depending only logarithmically on the number of data points (along with data dependent parameters) [$q$-means: A quantum algorithm for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash, NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this work, we describe a simple randomized mini-batch $k$-means algorithm and a quantum algorithm inspired by the classical algorithm. We prove worse-case guarantees that significantly improve upon the bounds for previous algorithms. Our improvements are due to a careful use of uniform sampling, which preserves certain symmetries of the $k$-means problem that are not preserved in previous algorithms that use data norm-based sampling.","authors":["Tyler Chen","Archan Ray","Akshay Seshadri","Dylan Herman","Bao Bach","Pranav Deshpande","Abhishek Som","Niraj Kumar","Marco Pistoia"],"url":"https://arxiv.org/abs/2504.20982"}
{"created":"2025-05-05","title":"Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations","abstract":"The design of novel materials hinges on the understanding of structure-property relationships. However, in recent times, our capability to synthesize a large number of materials has outpaced our speed at characterizing them. While the overall chemical constituents can be readily known during synthesis, the structural evolution and characterization of newly synthesized samples remains a bottleneck for the ultimate goal of high throughput nanomaterials discovery. Thus, scalable methods for crystal symmetry determination that can analyze a large volume of material samples within a short time-frame are especially needed. Kikuchi diffraction in the SEM is a promising technique for this due to its sensitivity to dynamical scattering, which may provide information beyond just the seven crystal systems and fourteen Bravais lattices. After diffraction patterns are collected from material samples, deep learning methods may be able to classify the space group symmetries using the patterns as input, which paired with the elemental composition, would help enable the determination of the crystal structure. To investigate the feasibility of this solution, neural networks were trained to predict the space group type of background corrected EBSD patterns. Our networks were first trained and tested on an artificial dataset of EBSD patterns of 5,148 different cubic phases, created through physics-based dynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised deep learning-based domain adaptation method, was utilized to train neural networks to make predictions for experimental EBSD patterns. We introduce a relabeling scheme, which enables our models to achieve accuracy scores higher than 90% on simulated and experimental data, suggesting that neural networks are capable of making predictions of crystal symmetry from an EBSD pattern.","authors":["Alfred Yan","Muhammad Nur Talha Kilic","Gert Nolze","Ankit Agrawal","Alok Choudhary","Roberto dos Reis","Vinayak Dravid"],"url":"https://arxiv.org/abs/2504.21331"}
