{"created":"2025-04-29","title":"Optimal tables for asymmetric numeral systems","abstract":"We present several algorithms to generate tables for asymmetric numeral systems and prove that they are optimal in terms of discrepancy. In turn, this gives rise to the strongest proven bound on entropy loss. We further give improved theoretical bounds for the entropy loss in tabled asymmetric numeral systems and a brief empirical evaluation of the stream variant.","authors":["Raphael S. Steiner","Mirko De Vita","Endri Bezati"],"url":"https://arxiv.org/abs/2504.18541"}
{"created":"2025-04-29","title":"Critical Challenges and Guidelines in Evaluating Synthetic Tabular Data: A Systematic Review","abstract":"Generating synthetic tabular data can be challenging, however evaluation of their quality is just as challenging, if not more. This systematic review sheds light on the critical importance of rigorous evaluation of synthetic health data to ensure reliability, relevance, and their appropriate use. Based on screening of 1766 papers and a detailed review of 101 papers we identified key challenges, including lack of consensus on evaluation methods, improper use of evaluation metrics, limited input from domain experts, inadequate reporting of dataset characteristics, and limited reproducibility of results. In response, we provide several guidelines on the generation and evaluation of synthetic data, to allow the community to unlock and fully harness the transformative potential of synthetic data and accelerate innovation.","authors":["Nazia Nafis","Inaki Esnaola","Alvaro Martinez-Perez","Maria-Cruz Villa-Uriol","Venet Osmani"],"url":"https://arxiv.org/abs/2504.18544"}
{"created":"2025-04-29","title":"Low-Bit Integerization of Vision Transformers using Operand Reodering for Efficient Hardware","abstract":"Pre-trained vision transformers have achieved remarkable performance across various visual tasks but suffer from expensive computational and memory costs. While model quantization reduces memory usage by lowering precision, these models still incur significant computational overhead due to the dequantization before matrix operations. In this work, we analyze the computation graph and propose an integerization process based on operation reordering. Specifically, the process delays dequantization until after matrix operations. This enables integerized matrix multiplication and linear module by directly processing the quantized input. To validate our approach, we synthesize the self-attention module of ViT on a systolic array-based hardware. Experimental results show that our low-bit inference reduces per-PE power consumption for linear layer and matrix multiplication, bridging the gap between quantized models and efficient inference.","authors":["Ching-Yi Lin","Sahil Shah"],"url":"https://arxiv.org/abs/2504.18547"}
{"created":"2025-04-29","title":"RDI: An adversarial robustness evaluation metric for deep neural networks based on sample clustering features","abstract":"Deep neural networks (DNNs) are highly susceptible to adversarial samples, raising concerns about their reliability in safety-critical tasks. Currently, methods of evaluating adversarial robustness are primarily categorized into attack-based and certified robustness evaluation approaches. The former not only relies on specific attack algorithms but also is highly time-consuming, while the latter due to its analytical nature, is typically difficult to implement for large and complex models. A few studies evaluate model robustness based on the model's decision boundary, but they suffer from low evaluation accuracy. To address the aforementioned issues, we propose a novel adversarial robustness evaluation metric, Robustness Difference Index (RDI), which is based on sample clustering features. RDI draws inspiration from clustering evaluation by analyzing the intra-class and inter-class distances of feature vectors separated by the decision boundary to quantify model robustness. It is attack-independent and has high computational efficiency. Experiments show that, RDI demonstrates a stronger correlation with the gold-standard adversarial robustness metric of attack success rate (ASR). The average computation time of RDI is only 1/30 of the evaluation method based on the PGD attack. Our open-source code is available at: https://anonymous.4open.science/r/RDI-B1DA.","authors":["Jialei Song","Xingquan Zuo","Feiyang Wang","Hai Huang","Tianle Zhang"],"url":"https://arxiv.org/abs/2504.18556"}
{"created":"2025-04-29","title":"Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages","abstract":"Large Language Models (LLMs) have exhibited impressive natural language processing capabilities but often perpetuate social biases inherent in their training data. To address this, we introduce MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by enabling systematic multilingual bias testing. MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings. In this study, we evaluate the effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six languages -- including two low-resource languages -- focusing on seven sensitive categories of discrimination.","authors":["Alessio Buscemi","C\\'edric Lothritz","Sergio Morales","Marcos Gomez-Vazquez","Robert Claris\\'o","Jordi Cabot","German Castignani"],"url":"https://arxiv.org/abs/2504.18560"}
{"created":"2025-04-29","title":"Deep Learning with Pretrained 'Internal World' Layers: A Gemma 3-Based Modular Architecture for Wildfire Prediction","abstract":"Deep learning models, especially large Transformers, carry substantial \"memory\" in their intermediate layers -- an \\emph{internal world} that encodes a wealth of relational and contextual knowledge. This work harnesses that internal world for wildfire occurrence prediction by introducing a modular architecture built upon Gemma 3, a state-of-the-art multimodal model. Rather than relying on Gemma 3's original embedding and positional encoding stacks, we develop a custom feed-forward module that transforms tabular wildfire features into the hidden dimension required by Gemma 3's mid-layer Transformer blocks. We freeze these Gemma 3 sub-layers -- thus preserving their pretrained representation power -- while training only the smaller input and output networks. This approach minimizes the number of trainable parameters and reduces the risk of overfitting on limited wildfire data, yet retains the benefits of Gemma 3's broad knowledge. Evaluations on a Moroccan wildfire dataset demonstrate improved predictive accuracy and robustness compared to standard feed-forward and convolutional baselines. Ablation studies confirm that the frozen Transformer layers consistently contribute to better representations, underscoring the feasibility of reusing large-model mid-layers as a learned internal world. Our findings suggest that strategic modular reuse of pretrained Transformers can enable more data-efficient and interpretable solutions for critical environmental applications such as wildfire risk management.","authors":["Ayoub Jadouli","Chaker El Amrani"],"url":"https://arxiv.org/abs/2504.18562"}
{"created":"2025-04-29","title":"Backdoor Defense in Diffusion Models via Spatial Attention Unlearning","abstract":"Text-to-image diffusion models are increasingly vulnerable to backdoor attacks, where malicious modifications to the training data cause the model to generate unintended outputs when specific triggers are present. While classification models have seen extensive development of defense mechanisms, generative models remain largely unprotected due to their high-dimensional output space, which complicates the detection and mitigation of subtle perturbations. Defense strategies for diffusion models, in particular, remain under-explored. In this work, we propose Spatial Attention Unlearning (SAU), a novel technique for mitigating backdoor attacks in diffusion models. SAU leverages latent space manipulation and spatial attention mechanisms to isolate and remove the latent representation of backdoor triggers, ensuring precise and efficient removal of malicious effects. We evaluate SAU across various types of backdoor attacks, including pixel-based and style-based triggers, and demonstrate its effectiveness in achieving 100% trigger removal accuracy. Furthermore, SAU achieves a CLIP score of 0.7023, outperforming existing methods while preserving the model's ability to generate high-quality, semantically aligned images. Our results show that SAU is a robust, scalable, and practical solution for securing text-to-image diffusion models against backdoor attacks.","authors":["Abha Jha","Ashwath Vaithinathan Aravindan","Matthew Salaway","Atharva Sandeep Bhide","Duygu Nur Yaldiz"],"url":"https://arxiv.org/abs/2504.18563"}
{"created":"2025-04-29","title":"DualBreach: Efficient Dual-Jailbreaking via Target-Driven Initialization and Multi-Target Optimization","abstract":"Recent research has focused on exploring the vulnerabilities of Large Language Models (LLMs), aiming to elicit harmful and/or sensitive content from LLMs. However, due to the insufficient research on dual-jailbreaking -- attacks targeting both LLMs and Guardrails, the effectiveness of existing attacks is limited when attempting to bypass safety-aligned LLMs shielded by guardrails. Therefore, in this paper, we propose DualBreach, a target-driven framework for dual-jailbreaking. DualBreach employs a Target-driven Initialization (TDI) strategy to dynamically construct initial prompts, combined with a Multi-Target Optimization (MTO) method that utilizes approximate gradients to jointly adapt the prompts across guardrails and LLMs, which can simultaneously save the number of queries and achieve a high dual-jailbreaking success rate. For black-box guardrails, DualBreach either employs a powerful open-sourced guardrail or imitates the target black-box guardrail by training a proxy model, to incorporate guardrails into the MTO process.","authors":["Xinzhe Huang","Kedong Xiu","Tianhang Zheng","Churui Zeng","Wangze Ni","Zhan Qiin","Kui Ren","Chun Chen"],"url":"https://arxiv.org/abs/2504.18564"}
{"created":"2025-04-29","title":"RepliBench: Evaluating the autonomous replication capabilities of language model agents","abstract":"Uncontrollable autonomous replication of language model agents poses a critical safety risk. To better understand this risk, we introduce RepliBench, a suite of evaluations designed to measure autonomous replication capabilities. RepliBench is derived from a decomposition of these capabilities covering four core domains: obtaining resources, exfiltrating model weights, replicating onto compute, and persisting on this compute for long periods. We create 20 novel task families consisting of 86 individual tasks. We benchmark 5 frontier models, and find they do not currently pose a credible threat of self-replication, but succeed on many components and are improving rapidly. Models can deploy instances from cloud compute providers, write self-propagating programs, and exfiltrate model weights under simple security setups, but struggle to pass KYC checks or set up robust and persistent agent deployments. Overall the best model we evaluated (Claude 3.7 Sonnet) has a >50% pass@10 score on 15/20 task families, and a >50% pass@10 score for 9/20 families on the hardest variants. These findings suggest autonomous replication capability could soon emerge with improvements in these remaining areas or with human assistance.","authors":["Sid Black","Asa Cooper Stickland","Jake Pencharz","Oliver Sourbut","Michael Schmatz","Jay Bailey","Ollie Matthews","Ben Millwood","Alex Remedios","Alan Cooney"],"url":"https://arxiv.org/abs/2504.18565"}
{"created":"2025-04-29","title":"Feature Selection via GANs (GANFS): Enhancing Machine Learning Models for DDoS Mitigation","abstract":"Distributed Denial of Service (DDoS) attacks represent a persistent and evolving threat to modern networked systems, capable of causing large-scale service disruptions. The complexity of such attacks, often hidden within high-dimensional and redundant network traffic data, necessitates robust and intelligent feature selection techniques for effective detection. Traditional methods such as filter-based, wrapper-based, and embedded approaches, each offer strengths but struggle with scalability or adaptability in complex attack environments. In this study, we explore these existing techniques through a detailed comparative analysis and highlight their limitations when applied to large-scale DDoS detection tasks. Building upon these insights, we introduce a novel Generative Adversarial Network-based Feature Selection (GANFS) method that leverages adversarial learning dynamics to identify the most informative features. By training a GAN exclusively on attack traffic and employing a perturbation-based sensitivity analysis on the Discriminator, GANFS effectively ranks feature importance without relying on full supervision. Experimental evaluations using the CIC-DDoS2019 dataset demonstrate that GANFS not only improves the accuracy of downstream classifiers but also enhances computational efficiency by significantly reducing feature dimensionality. These results point to the potential of integrating generative learning models into cybersecurity pipelines to build more adaptive and scalable detection systems.","authors":["Harsh Patel"],"url":"https://arxiv.org/abs/2504.18566"}
{"created":"2025-04-29","title":"Lecture Notes on Algorithmic Information Theory","abstract":"Algorithmic information theory roots the concept of information in computation rather than probability. These lecture notes were constructed in conjunction with the graduate course I taught at Universit\\`a della Svizzera italiana in the spring of 2023. The course is intended for graduate students and researchers seeking a self-contained journey from the foundations of computability theory to prefix complexity and the information-theoretic limits of formal systems. My exposition ignores boundaries between computer science, mathematics, physics, and philosophy -- an approach I consider essential when explaining inherently multidisciplinary fields. Lecture recordings are available online. Among other topics, the notes cover bit strings, codes, Shannon information theory, computability theory, the universal Turing machine, the Halting Problem, Rice's Theorem, plain algorithmic complexity, the Invariance Theorem, incompressibility, Solomonoff's induction, self-delimiting Turing machines, prefix algorithmic complexity, the halting probability Omega, Chaitin's Incompleteness Theorem, The Coding Theorem, lower semi-computable semi-measures, and the chain rule for algorithmic complexity.","authors":["Charles Alexandre B\\'edard"],"url":"https://arxiv.org/abs/2504.18568"}
{"created":"2025-04-29","title":"Large Language Model Empowered Privacy-Protected Framework for PHI Annotation in Clinical Notes","abstract":"The de-identification of private information in medical data is a crucial process to mitigate the risk of confidentiality breaches, particularly when patient personal details are not adequately removed before the release of medical records. Although rule-based and learning-based methods have been proposed, they often struggle with limited generalizability and require substantial amounts of annotated data for effective performance. Recent advancements in large language models (LLMs) have shown significant promise in addressing these issues due to their superior language comprehension capabilities. However, LLMs present challenges, including potential privacy risks when using commercial LLM APIs and high computational costs for deploying open-source LLMs locally. In this work, we introduce LPPA, an LLM-empowered Privacy-Protected PHI Annotation framework for clinical notes, targeting the English language. By fine-tuning LLMs locally with synthetic notes, LPPA ensures strong privacy protection and high PHI annotation accuracy. Extensive experiments demonstrate LPPA's effectiveness in accurately de-identifying private information, offering a scalable and efficient solution for enhancing patient privacy protection.","authors":["Guanchen Wu","Linzhi Zheng","Han Xie","Zhen Xiang","Jiaying Lu","Darren Liu","Delgersuren Bold","Bo Li","Xiao Hu","Carl Yang"],"url":"https://arxiv.org/abs/2504.18569"}
{"created":"2025-04-29","title":"Residual-Evasive Attacks on ADMM in Distributed Optimization","abstract":"This paper presents two attack strategies designed to evade detection in ADMM-based systems by preventing significant changes to the residual during the attacked iteration. While many detection algorithms focus on identifying false data injection through residual changes, we show that our attacks remain undetected by keeping the residual largely unchanged. The first strategy uses a random starting point combined with Gram-Schmidt orthogonalization to ensure stealth, with potential for refinement by enhancing the orthogonal component to increase system disruption. The second strategy builds on the first, targeting financial gains by manipulating reactive power and pushing the system to its upper voltage limit, exploiting operational constraints. The effectiveness of the proposed attack-resilient mechanism is demonstrated through case studies on the IEEE 14-bus system. A comparison of the two strategies, along with commonly used naive attacks, reveals trade-offs between simplicity, detectability, and effectiveness, providing insights into ADMM system vulnerabilities. These findings underscore the need for more robust monitoring algorithms to protect against advanced attack strategies.","authors":["Sabrina Bruckmeier","Huadong Mo","James Qin"],"url":"https://arxiv.org/abs/2504.18570"}
{"created":"2025-04-29","title":"Intelligent Detection of Non-Essential IoT Traffic on the Home Gateway","abstract":"The rapid expansion of Internet of Things (IoT) devices, particularly in smart home environments, has introduced considerable security and privacy concerns due to their persistent connectivity and interaction with cloud services. Despite advancements in IoT security, effective privacy measures remain uncovered, with existing solutions often relying on cloud-based threat detection that exposes sensitive data or outdated allow-lists that inadequately restrict non-essential network traffic. This work presents ML-IoTrim, a system for detecting and mitigating non-essential IoT traffic (i.e., not influencing the device operations) by analyzing network behavior at the edge, leveraging Machine Learning to classify network destinations. Our approach includes building a labeled dataset based on IoT device behavior and employing a feature-extraction pipeline to enable a binary classification of essential vs. non-essential network destinations. We test our framework in a consumer smart home setup with IoT devices from five categories, demonstrating that the model can accurately identify and block non-essential traffic, including previously unseen destinations, without relying on traditional allow-lists. We implement our solution on a home access point, showing the framework has strong potential for scalable deployment, supporting near-real-time traffic classification in large-scale IoT environments with hundreds of devices. This research advances privacy-aware traffic control in smart homes, paving the way for future developments in IoT device privacy.","authors":["Fabio Palmese","Anna Maria Mandalari","Hamed Haddadi","Alessandro Enrico Cesare Redondi"],"url":"https://arxiv.org/abs/2504.18571"}
{"created":"2025-04-29","title":"BELL: Benchmarking the Explainability of Large Language Models","abstract":"Large Language Models have demonstrated remarkable capabilities in natural language processing, yet their decision-making processes often lack transparency. This opaqueness raises significant concerns regarding trust, bias, and model performance. To address these issues, understanding and evaluating the interpretability of LLMs is crucial. This paper introduces a standardised benchmarking technique, Benchmarking the Explainability of Large Language Models, designed to evaluate the explainability of large language models.","authors":["Syed Quiser Ahmed","Bharathi Vokkaliga Ganesh","Jagadish Babu P","Karthick Selvaraj","ReddySiva Naga Parvathi Devi","Sravya Kappala"],"url":"https://arxiv.org/abs/2504.18572"}
{"created":"2025-04-29","title":"Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism","abstract":"SSMs offer efficient processing of long sequences with fixed state sizes, but struggle with algorithmic tasks like retrieving past context. In this work, we examine how such in-context retrieval operates within Transformer- and SSM-based language models. We find that both architectures develop the same fundamental Gather-and-Aggregate (G&amp;A) mechanism. A Gather Head first identifies and extracts relevant information from the context, which an Aggregate Head then integrates into a final representation. Across both model types, G&amp;A concentrates in just a few heads, making them critical bottlenecks even for benchmarks that require a basic form of retrieval. For example, disabling a single Gather or Aggregate Head of a pruned Llama-3.1-8B degrades its ability to retrieve the correct answer letter in MMLU, reducing accuracy from 66% to 25%. This finding suggests that in-context retrieval can obscure the limited knowledge demands of certain tasks. Despite strong MMLU performance with retrieval intact, the pruned model fails on other knowledge tests. Similar G&amp;A dependencies exist in GSM8K, BBH, and dialogue tasks. Given the significance of G&amp;A in performance, we show that retrieval challenges in SSMs manifest in how they implement G&amp;A, leading to smoother attention patterns rather than the sharp token transitions that effective G&amp;A relies on. Thus, while a gap exists between Transformers and SSMs in implementing in-context retrieval, it is confined to a few heads, not the entire model. This insight suggests a unified explanation for performance differences between Transformers and SSMs while also highlighting ways to combine their strengths. For example, in pretrained hybrid models, attention components naturally take on the role of Aggregate Heads. Similarly, in a pretrained pure SSM, replacing a single G&amp;A head with an attention-based variant significantly improves retrieval.","authors":["Aviv Bick","Eric Xing","Albert Gu"],"url":"https://arxiv.org/abs/2504.18574"}
{"created":"2025-04-29","title":"WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks","abstract":"Web navigation AI agents use language-and-vision foundation models to enhance productivity but these models are known to be susceptible to indirect prompt injections that get them to follow instructions different from the legitimate user's. Existing explorations of this threat applied to web agents often focus on a single isolated adversarial goal, test with injected instructions that are either too easy or not truly malicious, and often give the adversary unreasonable access. In order to better focus adversarial research, we construct a new benchmark called WASP (Web Agent Security against Prompt injection attacks) that introduces realistic web agent hijacking objectives and an isolated environment to test them in that does not affect real users or the live web. As part of WASP, we also develop baseline attacks against three popular web agentic systems (VisualWebArena, Claude Computer Use, and Operator) instantiated with various state-of-the-art models. Our evaluation shows that even AI agents backed by models with advanced reasoning capabilities and by models with instruction hierarchy mitigations are susceptible to low-effort human-written prompt injections. However, the realistic objectives in WASP also allow us to observe that agents are currently not capable enough to complete the goals of attackers end-to-end. Agents begin executing the adversarial instruction between 16 and 86% of the time but only achieve the goal between 0 and 17% of the time. Based on these findings, we argue that adversarial researchers should demonstrate stronger attacks that more consistently maintain control over the agent given realistic constraints on the adversary's power.","authors":["Ivan Evtimov","Arman Zharmagambetov","Aaron Grattafiori","Chuan Guo","Kamalika Chaudhuri"],"url":"https://arxiv.org/abs/2504.18575"}
{"created":"2025-04-29","title":"DriVerse: Navigation World Model for Driving Simulation via Multimodal Trajectory Prompting and Motion Alignment","abstract":"This paper presents DriVerse, a generative model for simulating navigation-driven driving scenes from a single image and a future trajectory. Previous autonomous driving world models either directly feed the trajectory or discrete control signals into the generation pipeline, leading to poor alignment between the control inputs and the implicit features of the 2D base generative model, which results in low-fidelity video outputs. Some methods use coarse textual commands or discrete vehicle control signals, which lack the precision to guide fine-grained, trajectory-specific video generation, making them unsuitable for evaluating actual autonomous driving algorithms. DriVerse introduces explicit trajectory guidance in two complementary forms: it tokenizes trajectories into textual prompts using a predefined trend vocabulary for seamless language integration, and converts 3D trajectories into 2D spatial motion priors to enhance control over static content within the driving scene. To better handle dynamic objects, we further introduce a lightweight motion alignment module, which focuses on the inter-frame consistency of dynamic pixels, significantly enhancing the temporal coherence of moving elements over long sequences. With minimal training and no need for additional data, DriVerse outperforms specialized models on future video generation tasks across both the nuScenes and Waymo datasets. The code and models will be released to the public.","authors":["Xiaofan Li","Chenming Wu","Zhao Yang","Zhihao Xu","Dingkang Liang","Yumeng Zhang","Ji Wan","Jun Wang"],"url":"https://arxiv.org/abs/2504.18576"}
{"created":"2025-04-29","title":"Defending Against Intelligent Attackers at Large Scales","abstract":"We investigate the scale of attack and defense mathematically in the context of AI's possible effect on cybersecurity. For a given target today, highly scaled cyber attacks such as from worms or botnets typically all fail or all succeed. Here, we consider the effect of scale if those attack agents were intelligent and creative enough to act independently such that each attack attempt was different from the others or such that attackers could learn from their successes and failures. We find that small increases in the number or quality of defenses can compensate for exponential increases in the number of independent attacks and for exponential speedups.","authors":["Andrew J. Lohn"],"url":"https://arxiv.org/abs/2504.18577"}
{"created":"2025-04-29","title":"An Artificial Intelligence-Based Framework for Predicting Emergency Department Overcrowding: Development and Evaluation Study","abstract":"Background: Emergency department (ED) overcrowding remains a major challenge, causing delays in care and increased operational strain. Hospital management often reacts to congestion after it occurs. Machine learning predictive modeling offers a proactive approach by forecasting patient flow metrics, such as waiting count, to improve resource planning and hospital efficiency.","authors":["Orhun Vural","Bunyamin Ozaydin","Khalid Y. Aram","James Booth","Brittany F. Lindsey","Abdulaziz Ahmed"],"url":"https://arxiv.org/abs/2504.18578"}
{"created":"2025-04-29","title":"ZipR1: Reinforcing Token Sparsity in MLLMs","abstract":"Sparse attention mechanisms aim to reduce computational overhead by selectively processing a subset of salient tokens while preserving model performance. Despite the effectiveness of such designs, how to actively encourage token sparsity of well-posed MLLMs remains under-explored, which fundamentally limits the achievable acceleration effect during inference. In this paper, we propose a simple RL-based post-training method named \\textbf{ZipR1} that treats the token reduction ratio as the efficiency reward and answer accuracy as the performance reward.","authors":["Feng Chen","Yefei He","Lequan Lin","Jing Liu","Bohan Zhuang","Qi Wu"],"url":"https://arxiv.org/abs/2504.18579"}
{"created":"2025-04-29","title":"Parameter-Efficient Checkpoint Merging via Metrics-Weighted Averaging","abstract":"Checkpoint merging is a technique for combining multiple model snapshots into a single superior model, potentially reducing training time for large language models. This paper explores checkpoint merging in the context of parameter-efficient fine-tuning (PEFT), where only small adapter modules (e.g. LoRA) are trained. We propose Metrics-Weighted Averaging (MWA), a simple yet effective method to merge model checkpoints by weighting their parameters according to performance metrics. In particular, we investigate weighting by training loss and by training steps, under the intuition that lower-loss or later-step checkpoints are more valuable. We introduce a formula with a penalty factor to adjust weight distribution, requiring only one hyperparameter regardless of the number of checkpoints. Experiments on three fine-tuning tasks (mathematical reasoning, preference alignment, and general instruction tuning) show that MWA consistently produces merged models that outperform the naive uniform average of checkpoints. Notably, loss-weighted merging often yields the best results, delivering up to 5% higher task accuracy than the baseline uniform merge and even surpassing the final individual checkpoint's performance. These findings validate checkpoint merging for PEFT and demonstrate that a metric-driven weighting heuristic can efficiently boost model performance with minimal computational overhead.","authors":["Shi Jie Yu","Sehyun Choi"],"url":"https://arxiv.org/abs/2504.18580"}
{"created":"2025-04-29","title":"Enhancing Privacy in Semantic Communication over Wiretap Channels leveraging Differential Privacy","abstract":"Semantic communication (SemCom) improves transmission efficiency by focusing on task-relevant information. However, transmitting semantic-rich data over insecure channels introduces privacy risks. This paper proposes a novel SemCom framework that integrates differential privacy (DP) mechanisms to protect sensitive semantic features. This method employs the generative adversarial network (GAN) inversion technique to extract disentangled semantic features and uses neural networks (NNs) to approximate the DP application and removal processes, effectively mitigating the non-invertibility issue of DP. Additionally, an NN-based encryption scheme is introduced to strengthen the security of channel inputs. Simulation results demonstrate that the proposed approach effectively prevents eavesdroppers from reconstructing sensitive information by generating chaotic or fake images, while ensuring high-quality image reconstruction for legitimate users. The system exhibits robust performance across various privacy budgets and channel conditions, achieving an optimal balance between privacy protection and reconstruction fidelity.","authors":["Weixuan Chen","Shunpu Tang","Qianqian Yang","Zhiguo Shi","Dusit Niyato"],"url":"https://arxiv.org/abs/2504.18581"}
{"created":"2025-04-29","title":"Speaker Diarization for Low-Resource Languages Through Wav2vec Fine-Tuning","abstract":"Speaker diarization is a fundamental task in speech processing that involves dividing an audio stream by speaker. Although state-of-the-art models have advanced performance in high-resource languages, low-resource languages such as Kurdish pose unique challenges due to limited annotated data, multiple dialects and frequent code-switching. In this study, we address these issues by training the Wav2Vec 2.0 self-supervised learning model on a dedicated Kurdish corpus. By leveraging transfer learning, we adapted multilingual representations learned from other languages to capture the phonetic and acoustic characteristics of Kurdish speech. Relative to a baseline method, our approach reduced the diarization error rate by seven point two percent and improved cluster purity by thirteen percent. These findings demonstrate that enhancements to existing models can significantly improve diarization performance for under-resourced languages. Our work has practical implications for developing transcription services for Kurdish-language media and for speaker segmentation in multilingual call centers, teleconferencing and video-conferencing systems. The results establish a foundation for building effective diarization systems in other understudied languages, contributing to greater equity in speech technology.","authors":["Abdulhady Abas Abdullah","Sarkhel H. Taher Karim","Sara Azad Ahmed","Kanar R. Tariq","Tarik A. Rashid"],"url":"https://arxiv.org/abs/2504.18582"}
{"created":"2025-04-29","title":"PARD: Accelerating LLM Inference with Low-Cost PARallel Draft Model Adaptation","abstract":"The autoregressive nature of large language models (LLMs) limits inference speed. Each forward pass generates only a single token and is often bottlenecked by memory bandwidth. Speculative decoding alleviates this issue using a draft-then-verify approach to accelerate token generation. However, the overhead introduced during the draft phase and the training cost of the draft model limit the efficiency and adaptability of speculative decoding. In this work, we introduce PARallel Draft (PARD), a novel speculative decoding method that enables low-cost adaptation of autoregressive draft models into parallel draft models. PARD enhances inference efficiency by predicting multiple future tokens in a single forward pass of the draft phase, and incorporates a conditional drop token method to accelerate training. Its target-independence property allows a single draft model to be applied to an entire family of different models, minimizing the adaptation cost. Our proposed conditional drop token method can improves draft model training efficiency by 3x. On our optimized inference framework, PARD accelerates LLaMA3.1-8B inference by 4.08x, achieving 311.5 tokens per second.","authors":["Zihao An","Huajun Bai","Ziqiong Liu","Dong Li","Emad Barsoum"],"url":"https://arxiv.org/abs/2504.18583"}
{"created":"2025-04-29","title":"A Decade of You Only Look Once (YOLO) for Object Detection","abstract":"This review marks the tenth anniversary of You Only Look Once (YOLO), one of the most influential frameworks in real-time object detection. Over the past decade, YOLO has evolved from a streamlined detector into a diverse family of architectures characterized by efficient design, modular scalability, and cross-domain adaptability. The paper presents a technical overview of the main versions, highlights key architectural trends, and surveys the principal application areas in which YOLO has been adopted. It also addresses evaluation practices, ethical considerations, and potential future directions for the framework's continued development. The analysis aims to provide a comprehensive and critical perspective on YOLO's trajectory and ongoing transformation.","authors":["Leo Thomas Ramos","Angel D. Sappa"],"url":"https://arxiv.org/abs/2504.18586"}
{"created":"2025-04-29","title":"Training Large Language Models to Reason via EM Policy Gradient","abstract":"Recently, foundation models such as OpenAI's O1 and O3, along with DeepSeek's R1, have demonstrated strong reasoning capacities and problem-solving skills acquired through large-scale reinforcement learning (RL), with wide applications in mathematics, coding, science, intelligent agents, and virtual assistants. In this work, we introduce an off-policy reinforcement learning algorithm, EM Policy Gradient, aimed at enhancing LLM reasoning by optimizing expected return over reasoning trajectories. We frame the reasoning task as an Expectation-Maximization (EM) optimization problem, alternating between sampling diverse rationale trajectories and performing reward-guided fine-tuning. Unlike PPO and GRPO, which rely on complex importance weights and heuristic clipping, our method provides a simpler, more principled off-policy policy gradient approach, eliminating these complexities while maintaining strong performance. We evaluate the effectiveness of EM Policy Gradient on the GSM8K and MATH (HARD) datasets, where it achieves performance comparable to or slightly surpassing the state-of-the-art GRPO, while offering additional advantages in scalability, simplicity, and reasoning conciseness. Moreover, models fine-tuned with our method exhibit cognitive behaviors, such as sub-problem decomposition, self-verification, and backtracking, highlighting its potential to enhance both the interpretability and robustness of LLM reasoning.","authors":["Tianbing Xu"],"url":"https://arxiv.org/abs/2504.18587"}
{"created":"2025-04-29","title":"Dynamic QoS Prediction via a Non-Negative Tensor Snowflake Factorization","abstract":"Dynamic quality of service (QoS) data exhibit rich temporal patterns in user-service interactions, which are crucial for a comprehensive understanding of user behavior and service conditions in Web service. As the number of users and services increases, there is a large amount of unobserved QoS data, which significantly affects users'choice of services. To predict unobserved QoS data, we propose a Non-negative Snowflake Factorization of tensors model. This method designs a snowflake core tensor to enhance the model's learning capability. Additionally, it employs a single latent factor-based, nonnegative multiplication update on tensor (SLF-NMUT) for parameter learning. Empirical results demonstrate that the proposed model more accurately learns dynamic user-service interaction patterns, thereby yielding improved predictions for missing QoS data.","authors":["YongHui Xia","Lan Wang","Hao Wu"],"url":"https://arxiv.org/abs/2504.18588"}
{"created":"2025-04-29","title":"Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency","abstract":"Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements.","authors":["Zhikai Wang","Jiashuo Sun","Wenqi Zhang","Zhiqiang Hu","Xin Li","Fan Wang","Deli Zhao"],"url":"https://arxiv.org/abs/2504.18589"}
{"created":"2025-04-29","title":"A multilevel approach to accelerate the training of Transformers","abstract":"In this article, we investigate the potential of multilevel approaches to accelerate the training of transformer architectures. Using an ordinary differential equation (ODE) interpretation of these architectures, we propose an appropriate way of varying the discretization of these ODE Transformers in order to accelerate the training. We validate our approach experimentally by a comparison with the standard training procedure.","authors":["Guillaume Lauga (OCKHAM)","Ma\\\"el Chaumette (OCKHAM)","Edgar Desainte-Mar\\'eville (OCKHAM)","\\'Etienne Lasalle (OCKHAM)","Arthur Lebeurrier (OCKHAM)"],"url":"https://arxiv.org/abs/2504.18590"}
{"created":"2025-04-29","title":"Geometry aware inference of steady state PDEs using Equivariant Neural Fields representations","abstract":"Recent advances in Neural Fields have enabled powerful, discretization-invariant methods for learning neural operators that approximate solutions of Partial Differential Equations (PDEs) on general geometries. Building on these developments, we introduce enf2enf, an encoder--decoder methodology for predicting steady-state Partial Differential Equations with non-parameterized geometric variability, based on recently proposed Equivariant Neural Field architectures. In enf2enf, input geometries are encoded into latent point cloud embeddings that inherently preserve geometric grounding and capture local phenomena. The resulting representations are then combined with global parameters and directly decoded into continuous output fields, thus efficiently modeling the coupling between geometry and physics. By leveraging the inductive biases of locality and translation invariance, our approach is able to capture fine-scale physical features as well as complex shape variations, thereby enhancing generalization and physical compliance. Extensive experiments on a high-fidelity aerodynamic dataset, a hyper-elastic material benchmark, and multi-element airfoil geometries, demonstrate that the proposed model achieves superior or competitive performance compared to state-of-the-art graph based, operator learning, and neural field methods. Notably, our method supports real time inference and zero-shot super-resolution, enabling efficient training on low-resolution meshes while maintaining high accuracy on full-scale discretizations.","authors":["Giovanni Catalani","Michael Bauerheim","Fr\\'ed\\'eric Tost","Xavier Bertrand","Joseph Morlier"],"url":"https://arxiv.org/abs/2504.18591"}
{"created":"2025-04-29","title":"Severity Classification of Chronic Obstructive Pulmonary Disease in Intensive Care Units: A Semi-Supervised Approach Using MIMIC-III Dataset","abstract":"Chronic obstructive pulmonary disease (COPD) represents a significant global health burden, where precise severity assessment is particularly critical for effective clinical management in intensive care unit (ICU) settings. This study introduces an innovative machine learning framework for COPD severity classification utilizing the MIMIC-III critical care database, thereby expanding the applications of artificial intelligence in critical care medicine. Our research developed a robust classification model incorporating key ICU parameters such as blood gas measurements and vital signs, while implementing semi-supervised learning techniques to effectively utilize unlabeled data and enhance model performance. The random forest classifier emerged as particularly effective, demonstrating exceptional discriminative capability with 92.51% accuracy and 0.98 ROC AUC in differentiating between mild-to-moderate and severe COPD cases. This machine learning approach provides clinicians with a practical, accurate, and efficient tool for rapid COPD severity evaluation in ICU environments, with significant potential to improve both clinical decision-making processes and patient outcomes. Future research directions should prioritize external validation across diverse patient populations and integration with clinical decision support systems to optimize COPD management in critical care settings.","authors":["Akram Shojaei","Mehdi Delrobaei"],"url":"https://arxiv.org/abs/2504.18593"}
{"created":"2025-04-29","title":"A Simple DropConnect Approach to Transfer-based Targeted Attack","abstract":"We study the problem of transfer-based black-box attack, where adversarial samples generated using a single surrogate model are directly applied to target models. Compared with untargeted attacks, existing methods still have lower Attack Success Rates (ASRs) in the targeted setting, i.e., the obtained adversarial examples often overfit the surrogate model but fail to mislead other models. In this paper, we hypothesize that the pixels or features in these adversarial examples collaborate in a highly dependent manner to maximize the success of an adversarial attack on the surrogate model, which we refer to as perturbation co-adaptation. Then, we propose to Mitigate perturbation Co-adaptation by DropConnect (MCD) to enhance transferability, by creating diverse variants of surrogate model at each optimization iteration. We conduct extensive experiments across various CNN- and Transformer-based models to demonstrate the effectiveness of MCD. In the challenging scenario of transferring from a CNN-based model to Transformer-based models, MCD achieves 13% higher average ASRs compared with state-of-the-art baselines. MCD boosts the performance of self-ensemble methods by bringing in more diversification across the variants while reserving sufficient semantic information for each variant. In addition, MCD attains the highest performance gain when scaling the compute of crafting adversarial examples.","authors":["Tongrui Su","Qingbin Li","Shengyu Zhu","Wei Chen","Xueqi Cheng"],"url":"https://arxiv.org/abs/2504.18594"}
{"created":"2025-04-29","title":"EnviroPiNet: A Physics-Guided AI Model for Predicting Biofilter Performance","abstract":"Environmental biotechnologies, such as drinking water biofilters, rely on complex interactions between microbial communities and their surrounding physical-chemical environments. Predicting the performance of these systems is challenging due to high-dimensional, sparse datasets that lack diversity and fail to fully capture system behaviour. Accurate predictive models require innovative, science-guided approaches. In this study, we present the first application of Buckingham Pi theory to modelling biofilter performance. This dimensionality reduction technique identifies meaningful, dimensionless variables that enhance predictive accuracy and improve model interpretability. Using these variables, we developed the Environmental Buckingham Pi Neural Network (EnviroPiNet), a physics-guided model benchmarked against traditional data-driven methods, including Principal Component Analysis (PCA) and autoencoder neural networks. Our findings demonstrate that the EnviroPiNet model achieves an R^2 value of 0.9236 on the testing dataset, significantly outperforming PCA and autoencoder methods. The Buckingham Pi variables also provide insights into the physical and chemical relationships governing biofilter behaviour, with implications for system design and optimization. This study highlights the potential of combining physical principles with AI approaches to model complex environmental systems characterized by sparse, high-dimensional datasets.","authors":["Uzma","Fabien Cholet","Domenic Quinn","Cindy Smith","Siming You","William Sloan"],"url":"https://arxiv.org/abs/2504.18595"}
{"created":"2025-04-29","title":"Optimizing the Privacy-Utility Balance using Synthetic Data and Configurable Perturbation Pipelines","abstract":"This paper explores the strategic use of modern synthetic data generation and advanced data perturbation techniques to enhance security, maintain analytical utility, and improve operational efficiency when managing large datasets, with a particular focus on the Banking, Financial Services, and Insurance (BFSI) sector. We contrast these advanced methods encompassing generative models like GANs, sophisticated context-aware PII transformation, configurable statistical perturbation, and differential privacy with traditional anonymization approaches.","authors":["Anantha Sharma","Swetha Devabhaktuni","Eklove Mohan"],"url":"https://arxiv.org/abs/2504.18596"}
{"created":"2025-04-29","title":"Accurate BGV Parameters Selection: Accounting for Secret and Public Key Dependencies in Average-Case Analysis","abstract":"The Brakerski-Gentry-Vaikuntanathan (BGV) scheme is one of the most significant fully homomorphic encryption (FHE) schemes. It belongs to a class of FHE schemes whose security is based on the presumed intractability of the Learning with Errors (LWE) problem and its ring variant (RLWE). Such schemes deal with a quantity, called noise, which increases each time a homomorphic operation is performed. Specifically, in order for the scheme to work properly, it is essential that the noise remains below a certain threshold throughout the process. For BGV, this threshold strictly depends on the ciphertext modulus, which is one of the initial parameters whose selection heavily affects both the efficiency and security of the scheme. In this paper, we provide a new method to estimate noise growth, closely aligning with experimental results and forming the basis for parameter selection that ensures correctness and improves efficiency.","authors":["Beatrice Biasioli","Chiara Marcolla","Nadir Murru","Matilda Urani"],"url":"https://arxiv.org/abs/2504.18597"}
{"created":"2025-04-29","title":"BadMoE: Backdooring Mixture-of-Experts LLMs via Optimizing Routing Triggers and Infecting Dormant Experts","abstract":"Mixture-of-Experts (MoE) have emerged as a powerful architecture for","authors":["Qingyue Wang","Qi Pang","Xixun Lin","Shuai Wang","Daoyuan Wu"],"url":"https://arxiv.org/abs/2504.18598"}
{"created":"2025-04-29","title":"A Hybrid Framework for Real-Time Data Drift and Anomaly Identification Using Hierarchical Temporal Memory and Statistical Tests","abstract":"Data Drift is the phenomenon where the generating model behind the data changes over time. Due to data drift, any model built on the past training data becomes less relevant and inaccurate over time. Thus, detecting and controlling for data drift is critical in machine learning models. Hierarchical Temporal Memory (HTM) is a machine learning model developed by Jeff Hawkins, inspired by how the human brain processes information. It is a biologically inspired model of memory that is similar in structure to the neocortex, and whose performance is claimed to be comparable to state of the art models in detecting anomalies in time series data. Another unique benefit of HTMs is its independence from training and testing cycle; all the learning takes place online with streaming data and no separate training and testing cycle is required. In sequential learning paradigm, Sequential Probability Ratio Test (SPRT) offers some unique benefit for online learning and inference. This paper proposes a novel hybrid framework combining HTM and SPRT for real-time data drift detection and anomaly identification. Unlike existing data drift methods, our approach eliminates frequent retraining and ensures low false positive rates. HTMs currently work with one dimensional or univariate data. In a second study, we also propose an application of HTM in multidimensional supervised scenario for anomaly detection by combining the outputs of multiple HTM columns, one for each dimension of the data, through a neural network. Experimental evaluations demonstrate that the proposed method outperforms conventional drift detection techniques like the Kolmogorov-Smirnov (KS) test, Wasserstein distance, and Population Stability Index (PSI) in terms of accuracy, adaptability, and computational efficiency. Our experiments also provide insights into optimizing hyperparameters for real-time deployment in domains such as Telecom.","authors":["Subhadip Bandyopadhyay","Joy Bose","Sujoy Roy Chowdhury"],"url":"https://arxiv.org/abs/2504.18599"}
{"created":"2025-04-29","title":"QuantBench: Benchmarking AI Methods for Quantitative Investment","abstract":"The field of artificial intelligence (AI) in quantitative investment has seen significant advancements, yet it lacks a standardized benchmark aligned with industry practices. This gap hinders research progress and limits the practical application of academic innovations. We present QuantBench, an industrial-grade benchmark platform designed to address this critical need. QuantBench offers three key strengths: (1) standardization that aligns with quantitative investment industry practices, (2) flexibility to integrate various AI algorithms, and (3) full-pipeline coverage of the entire quantitative investment process. Our empirical studies using QuantBench reveal some critical research directions, including the need for continual learning to address distribution shifts, improved methods for modeling relational financial data, and more robust approaches to mitigate overfitting in low signal-to-noise environments. By providing a common ground for evaluation and fostering collaboration between researchers and practitioners, QuantBench aims to accelerate progress in AI for quantitative investment, similar to the impact of benchmark platforms in computer vision and natural language processing.","authors":["Saizhuo Wang","Hao Kong","Jiadong Guo","Fengrui Hua","Yiyan Qi","Wanyun Zhou","Jiahao Zheng","Xinyu Wang","Lionel M. Ni","Jian Guo"],"url":"https://arxiv.org/abs/2504.18600"}
{"created":"2025-04-29","title":"The Philosophic Turn for AI Agents: Replacing centralized digital rhetoric with decentralized truth-seeking","abstract":"In the face of rapidly advancing AI technology, individuals will increasingly rely on AI agents to navigate life's growing complexities, raising critical concerns about maintaining both human agency and autonomy. This paper addresses a fundamental dilemma posed by AI decision-support systems: the risk of either becoming overwhelmed by complex decisions, thus losing agency, or having autonomy compromised by externally controlled choice architectures reminiscent of ``nudging'' practices. While the ``nudge'' framework, based on the use of choice-framing to guide individuals toward presumed beneficial outcomes, initially appeared to preserve liberty, at AI-driven scale, it threatens to erode autonomy. To counteract this risk, the paper proposes a philosophic turn in AI design. AI should be constructed to facilitate decentralized truth-seeking and open-ended inquiry, mirroring the Socratic method of philosophical dialogue. By promoting individual and collective adaptive learning, such AI systems would empower users to maintain control over their judgments, augmenting their agency without undermining autonomy. The paper concludes by outlining essential features for autonomy-preserving AI systems, sketching a path toward AI systems that enhance human judgment rather than undermine it.","authors":["Philipp Koralus"],"url":"https://arxiv.org/abs/2504.18601"}
{"created":"2025-04-29","title":"Beyond Platforms -- Growing Distributed Transaction Networks for Digital Commerce","abstract":"Many of today's IT infrastructures are proprietary platforms, like WhatsApp or Amazon. In some domains, like healthcare or finance, governments often take a strong regulatory role or even own the infrastructure. However, the biggest IT Infrastructure, the Internet itself, is run, evolved and governed in a cooperative manner. Decentralised architectures provide a number of advantages: They are potentially more inclusive for small players; more resilient in case of adversarial events, and seem to generate more innovation. However, we do not have much knowledge on how to evolve, adapt and govern decentralised infrastructures. This article reports empirical research on the development and governance of the Beckn Protocol, a protocol for decentralised transactions, and the successful development of domain-specific adaptations, their implementation and scaling. It explores how the architecture and governance support local innovation for specific business domains and how the domain-specific innovations and need feedback into the evolution of the protocol itself. The research applied a case study approach, combining interviews, document and code analysis. The article shows the possibility of such a decentralised approach to IT Infrastructures. It identifies a number of generativity mechanisms, socio-technical arrangements of the architecture, community support and governance that support adoption, innovation, and scaling it. It emphasises the governance of both the evolution of the open source specifications and software and how this relates to the governance of the conduct of network participants in operational networks. Finally, it emphasises the importance of feedback loops to both provide input for technical evolution and to recognise misconduct and develop means to address it.","authors":["Yvonne Dittrich","Kim Peiter J{\\o}rgensen","Ravi Prakash","Willard Rafnsson","Jonas Kastberg Hinrichsen"],"url":"https://arxiv.org/abs/2504.18602"}
{"created":"2025-04-29","title":"Toward Personalizing Quantum Computing Education: An Evolutionary LLM-Powered Approach","abstract":"Quantum computing education faces significant challenges due to its complexity and the limitations of current tools; this paper introduces a novel Intelligent Teaching Assistant for quantum computing education and details its evolutionary design process. The system combines a knowledge-graph-augmented architecture with two specialized Large Language Model (LLM) agents: a Teaching Agent for dynamic interaction, and a Lesson Planning Agent for lesson plan generation. The system is designed to adapt to individual student needs, with interactions meticulously tracked and stored in a knowledge graph. This graph represents student actions, learning resources, and relationships, aiming to enable reasoning about effective learning pathways. We describe the implementation of the system, highlighting the challenges encountered and the solutions implemented, including introducing a dual-agent architecture where tasks are separated, all coordinated through a central knowledge graph that maintains system awareness, and a user-facing tag system intended to mitigate LLM hallucination and improve user control. Preliminary results illustrate the system's potential to capture rich interaction data, dynamically adapt lesson plans based on student feedback via a tag system in simulation, and facilitate context-aware tutoring through the integrated knowledge graph, though systematic evaluation is required.","authors":["Iizalaarab Elhaimeur","Nikos Chrisochoides"],"url":"https://arxiv.org/abs/2504.18603"}
{"created":"2025-04-29","title":"A Cognitive-Mechanistic Human Reliability Analysis Framework: A Nuclear Power Plant Case Study","abstract":"Traditional human reliability analysis (HRA) methods, such as IDHEAS-ECA, rely on expert judgment and empirical rules that often overlook the cognitive underpinnings of human error. Moreover, conducting human-in-the-loop experiments for advanced nuclear power plants is increasingly impractical due to novel interfaces and limited operational data. This study proposes a cognitive-mechanistic framework (COGMIF) that enhances the IDHEAS-ECA methodology by integrating an ACT-R-based human digital twin (HDT) with TimeGAN-augmented simulation. The ACT-R model simulates operator cognition, including memory retrieval, goal-directed procedural reasoning, and perceptual-motor execution, under high-fidelity scenarios derived from a high-temperature gas-cooled reactor (HTGR) simulator. To overcome the resource constraints of large-scale cognitive modeling, TimeGAN is trained on ACT-R-generated time-series data to produce high-fidelity synthetic operator behavior datasets. These simulations are then used to drive IDHEAS-ECA assessments, enabling scalable, mechanism-informed estimation of human error probabilities (HEPs). Comparative analyses with SPAR-H and sensitivity assessments demonstrate the robustness and practical advantages of the proposed COGMIF. Finally, procedural features are mapped onto a Bayesian network to quantify the influence of contributing factors, revealing key drivers of operational risk. This work offers a credible and computationally efficient pathway to integrate cognitive theory into industrial HRA practices.","authors":["Xingyu Xiao","Peng Chen","Jiejuan Tong","Shunshun Liu","Hongru Zhao","Jun Zhao","Qianqian Jia","Jingang Liang","Haitao Wang"],"url":"https://arxiv.org/abs/2504.18604"}
{"created":"2025-04-29","title":"ECG Identity Authentication in Open-set with Multi-model Pretraining and Self-constraint Center & Irrelevant Sample Repulsion Learning","abstract":"Electrocardiogram (ECG) signal exhibits inherent uniqueness, making it a promising biometric modality for identity authentication. As a result, ECG authentication has gained increasing attention in recent years. However, most existing methods focus primarily on improving authentication accuracy within closed-set settings, with limited research addressing the challenges posed by open-set scenarios. In real-world applications, identity authentication systems often encounter a substantial amount of unseen data, leading to potential security vulnerabilities and performance degradation. To address this issue, we propose a robust ECG identity authentication system that maintains high performance even in open-set settings. Firstly, we employ a multi-modal pretraining framework, where ECG signals are paired with textual reports derived from their corresponding fiducial features to enhance the representational capacity of the signal encoder. During fine-tuning, we introduce Self-constraint Center Learning and Irrelevant Sample Repulsion Learning to constrain the feature distribution, ensuring that the encoded representations exhibit clear decision boundaries for classification. Our method achieves 99.83% authentication accuracy and maintains a False Accept Rate as low as 5.39% in the presence of open-set samples. Furthermore, across various open-set ratios, our method demonstrates exceptional stability, maintaining an Open-set Classification Rate above 95%.","authors":["Mingyu Dong","Zhidong Zhao","Hao Wang","Yefei Zhang","Yanjun Deng"],"url":"https://arxiv.org/abs/2504.18608"}
{"created":"2025-04-29","title":"Periodic Online Testing for Sparse Systolic Tensor Arrays","abstract":"Modern Machine Learning (ML) applications often benefit from structured sparsity, a technique that efficiently reduces model complexity and simplifies handling of sparse data in hardware. Sparse systolic tensor arrays - specifically designed to accelerate these structured-sparse ML models - play a pivotal role in enabling efficient computations. As ML is increasingly integrated into safety-critical systems, it is of paramount importance to ensure the reliability of these systems. This paper introduces an online error-checking technique capable of detecting and locating permanent faults within sparse systolic tensor arrays before computation begins. The new technique relies on merely four test vectors and exploits the weight values already loaded within the systolic array to comprehensively test the system. Fault-injection campaigns within the gate-level netlist, while executing three well-established Convolutional Neural Networks (CNN), validate the efficiency of the proposed approach, which is shown to achieve very high fault coverage, while incurring minimal performance and area overheads.","authors":["Christodoulos Peltekis","Chrysostomos Nicopoulos","Giorgos Dimitrakopoulos"],"url":"https://arxiv.org/abs/2504.18628"}
{"created":"2025-04-29","title":"Fairness Is More Than Algorithms: Racial Disparities in Time-to-Recidivism","abstract":"Racial disparities in recidivism remain a persistent challenge within the criminal justice system, increasingly exacerbated by the adoption of algorithmic risk assessment tools. Past works have primarily focused on bias induced by these tools, treating recidivism as a binary outcome. Limited attention has been given to non-algorithmic factors (including socioeconomic ones) in driving racial disparities from a systemic perspective. To that end, this work presents a multi-stage causal framework to investigate the advent and extent of disparities by considering time-to-recidivism rather than a simple binary outcome. The framework captures interactions among races, the algorithm, and contextual factors. This work introduces the notion of counterfactual racial disparity and offers a formal test using survival analysis that can be conducted with observational data to assess if differences in recidivism arise from algorithmic bias, contextual factors, or their interplay. In particular, it is formally established that if sufficient statistical evidence for differences across racial groups is observed, it would support rejecting the null hypothesis that non-algorithmic factors (including socioeconomic ones) do not affect recidivism. An empirical study applying this framework to the COMPAS dataset reveals that short-term recidivism patterns do not exhibit racial disparities when controlling for risk scores. However, statistically significant disparities emerge with longer follow-up periods, particularly for low-risk groups. This suggests that factors beyond algorithmic scores, possibly structural disparities in housing, employment, and social support, may accumulate and exacerbate recidivism risks over time. This underscores the need for policy interventions extending beyond algorithmic improvements to address broader influences on recidivism trajectories.","authors":["Jessy Xinyi Han","Kristjan Greenewald","Devavrat Shah"],"url":"https://arxiv.org/abs/2504.18629"}
{"created":"2025-04-29","title":"Research on Personalized Medical Intervention Strategy Generation System based on Group Relative Policy Optimization and Time-Series Data Fusion","abstract":"With the timely formation of personalized intervention plans based on high-dimensional heterogeneous time series information becoming an important challenge in the medical field today, electronic medical records, wearables, and other multi-source medical data are increasingly generated and diversified. In this work, we develop a system to generate personalized medical intervention strategies based on Group Relative Policy Optimization (GRPO) and Time-Series Data Fusion. First, by incorporating relative policy constraints among the groups during policy gradient updates, we adaptively balance individual and group gains. To improve the robustness and interpretability of decision-making, a multi-layer neural network structure is employed to group-code patient characteristics. Second, for the rapid multi-modal fusion of multi-source heterogeneous time series, a multi-channel neural network combined with a self-attention mechanism is used for dynamic feature extraction. Key feature screening and aggregation are achieved through a differentiable gating network. Finally, a collaborative search process combining a genetic algorithm and Monte Carlo tree search is proposed to find the ideal intervention strategy, achieving global optimization. Experimental results show significant improvements in accuracy, coverage, and decision-making benefits compared with existing methods.","authors":["Dingxin Lu","Shurui Wu","Xinyi Huang"],"url":"https://arxiv.org/abs/2504.18631"}
{"created":"2025-04-29","title":"A Gradient-Optimized TSK Fuzzy Framework for Explainable Phishing Detection","abstract":"Phishing attacks represent an increasingly sophisticated and pervasive threat to individuals and organizations, causing significant financial losses, identity theft, and severe damage to institutional reputations. Existing phishing detection methods often struggle to simultaneously achieve high accuracy and explainability, either failing to detect novel attacks or operating as opaque black-box models. To address this critical gap, we propose a novel phishing URL detection system based on a first-order Takagi-Sugeno-Kang (TSK) fuzzy inference model optimized through gradient-based techniques. Our approach intelligently combines the interpretability and human-like reasoning capabilities of fuzzy logic with the precision and adaptability provided by gradient optimization methods, specifically leveraging the Adam optimizer for efficient parameter tuning. Experiments conducted using a comprehensive dataset of over 235,000 URLs demonstrate rapid convergence, exceptional predictive performance (accuracy averaging 99.95% across 5 cross-validation folds, with a perfect AUC i.e. 1.00). Furthermore, optimized fuzzy rules and membership functions improve interoperability, clearly indicating how the model makes decisions - an essential feature for cybersecurity applications. This high-performance, transparent, and interpretable phishing detection framework significantly advances current cybersecurity defenses, providing practitioners with accurate and explainable decision-making tools.","authors":["Lohith Srikanth Pentapalli","Jon Salisbury","Josette Riep","Kelly Cohen"],"url":"https://arxiv.org/abs/2504.18636"}
{"created":"2025-04-29","title":"Span-Level Hallucination Detection for LLM-Generated Answers","abstract":"Detecting spans of hallucination in LLM-generated answers is crucial for improving factual consistency. This paper presents a span-level hallucination detection framework for the SemEval-2025 Shared Task, focusing on English and Arabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose the answer into atomic roles, which are then compared with a retrieved reference context obtained via question-based LLM prompting. Using a DeBERTa-based textual entailment model, we evaluate each role semantic alignment with the retrieved context. The entailment scores are further refined through token-level confidence measures derived from output logits, and the combined scores are used to detect hallucinated spans. Experiments on the Mu-SHROOM dataset demonstrate competitive performance. Additionally, hallucinated spans have been verified through fact-checking by prompting GPT-4 and LLaMA. Our findings contribute to improving hallucination detection in LLM-generated responses.","authors":["Passant Elchafei","Mervet Abu-Elkheir"],"url":"https://arxiv.org/abs/2504.18639"}
{"created":"2025-04-29","title":"Worst-Case and Average-Case Hardness of Hypercycle and Database Problems","abstract":"In this paper we present tight lower-bounds and new upper-bounds for hypergraph and database problems. We give tight lower-bounds for finding minimum hypercycles. We give tight lower-bounds for a substantial regime of unweighted hypercycle. We also give a new faster algorithm for longer unweighted hypercycles. We give a worst-case to average-case reduction from detecting a subgraph of a hypergraph in the worst-case to counting subgraphs of hypergraphs in the average-case. We demonstrate two applications of this worst-case to average-case reduction, which result in average-case lower bounds for counting hypercycles in random hypergraphs and queries in average-case databases. Our tight upper and lower bounds for hypercycle detection in the worst-case have immediate implications for the average-case via our worst-case to average-case reductions.","authors":["Cheng-Hao Fu","Andrea Lincoln","Rene Reyes"],"url":"https://arxiv.org/abs/2504.18640"}
{"created":"2025-04-29","title":"Raptr: Prefix Consensus for Robust High-Performance BFT","abstract":"In this paper, we present Raptr--a Byzantine fault-tolerant state machine replication (BFT SMR) protocol that combines strong robustness with high throughput, while attaining near-optimal theoretical latency. Raptr delivers exceptionally low latency and high throughput under favorable conditions, and it degrades gracefully in the presence of Byzantine faults and network attacks.","authors":["Andrei Tonkikh","Balaji Arun","Zhuolun Xiang","Zekun Li","Alexander Spiegelman"],"url":"https://arxiv.org/abs/2504.18649"}
{"created":"2025-04-29","title":"Unsupervised outlier detection to improve bird audio dataset labels","abstract":"The Xeno-Canto bird audio repository is an invaluable resource for those interested in vocalizations and other sounds made by birds around the world. This is particularly the case for machine learning researchers attempting to improve on the bird species recognition accuracy of classification models. However, the task of extracting labeled datasets from the recordings found in this crowd-sourced repository faces several challenges. One challenge of particular significance to machine learning practitioners is that one bird species label is applied to each audio recording, but frequently other sounds are also captured including other bird species, other animal sounds, anthropogenic and other ambient sounds. These non-target bird species sounds can result in dataset labeling discrepancies referred to as label noise. In this work we present a cleaning process consisting of audio preprocessing followed by dimensionality reduction and unsupervised outlier detection (UOD) to reduce the label noise in a dataset derived from Xeno-Canto recordings. We investigate three neural network dimensionality reduction techniques: two flavors of convolutional autoencoders and variational deep embedding (VaDE (Jiang, 2017)). While both methods show some degree of effectiveness at detecting outliers for most bird species datasets, we found significant variation in the performance of the methods from one species to the next. We believe that the results of this investigation demonstrate that the application of our cleaning process can meaningfully reduce the label noise of bird species datasets derived from Xeno-Canto audio repository but results vary across species.","authors":["Bruce Collins"],"url":"https://arxiv.org/abs/2504.18650"}
{"created":"2025-04-29","title":"Exploring a Large Language Model for Transforming Taxonomic Data into OWL: Lessons Learned and Implications for Ontology Development","abstract":"Managing scientific names in ontologies that represent species taxonomies is challenging due to the ever-evolving nature of these taxonomies. Manually maintaining these names becomes increasingly difficult when dealing with thousands of scientific names. To address this issue, this paper investigates the use of ChatGPT-4 to automate the development of the :Organism module in the Agricultural Product Types Ontology (APTO) for species classification. Our methodology involved leveraging ChatGPT-4 to extract data from the GBIF Backbone API and generate OWL files for further integration in APTO. Two alternative approaches were explored: (1) issuing a series of prompts for ChatGPT-4 to execute tasks via the BrowserOP plugin and (2) directing ChatGPT-4 to design a Python algorithm to perform analogous tasks. Both approaches rely on a prompting method where we provide instructions, context, input data, and an output indicator. The first approach showed scalability limitations, while the second approach used the Python algorithm to overcome these challenges, but it struggled with typographical errors in data handling. This study highlights the potential of Large language models like ChatGPT-4 to streamline the management of species names in ontologies. Despite certain limitations, these tools offer promising advancements in automating taxonomy-related tasks and improving the efficiency of ontology development.","authors":["Filipi Miranda Soares","Antonio Mauro Saraiva","Lu\\'is Ferreira Pires","Luiz Olavo Bonino da Silva Santos","Dilvan de Abreu Moreira","Fernando Elias Corr\\^ea","Kelly Rosa Braghetto","Debora Pignatari Drucker","Alexandre Cl\\'audio Botazzo Delbem"],"url":"https://arxiv.org/abs/2504.18651"}
{"created":"2025-04-29","title":"Performance Analysis and Experimental Validation of UAV Corridor-Assisted Networks","abstract":"Unmanned aerial vehicle (UAV) corridor-assisted communication networks are expected to expand significantly in the upcoming years driven by several technological, regulatory, and societal trends. In this new type of networks, accurate and realistic channel models are essential for designing reliable, efficient, and secure communication systems. In this paper, an analytical framework is presented that is based on one-dimensional (1D) finite point processes, namely the binomial point process (BPP) and the finite homogeneous Poisson point process (HPPP), to model the spatial locations of UAV-Base Stations (UAV-BSs). To this end, the shadowing conditions experienced in the UAV-BS-to-ground users links are accurately considered in a realistic maximum power-based user association policy. Subsequently, coverage probability analysis under the two spatial models is conducted, and exact-form expressions are derived. In an attempt to reduce the analytical complexity of the derived expressions, a dominant interferer-based approach is also investigated. Finally, the main outcomes of this paper are extensively validated by empirical data collected in an air-to-ground measurement campaign. To the best of the authors' knowledge, this is the first work to experimentally verify a generic spatial model by jointly considering the random spatial and shadowing characteristics of a UAV-assisted air-to-ground network.","authors":["Harris K. Armeniakos","Viktor Nikolaidis","Petros S. Bithas","Konstantinos Maliatsos","Athanasios G. Kanatas"],"url":"https://arxiv.org/abs/2504.18654"}
{"created":"2025-04-29","title":"The Big Send-off: High Performance Collectives on GPU-based Supercomputers","abstract":"We evaluate the current state of collective communication on GPU-based supercomputers for large language model (LLM) training at scale. Existing libraries such as RCCL and Cray-MPICH exhibit critical limitations on systems such as Frontier -- Cray-MPICH underutilizes network and compute resources, while RCCL suffers from severe scalability issues. To address these challenges, we introduce PCCL, a communication library with highly optimized implementations of all-gather and reduce-scatter operations tailored for distributed deep learning workloads. PCCL is designed to maximally utilize all available network and compute resources and to scale efficiently to thousands of GPUs. It achieves substantial performance improvements, delivering 6-33x speedups over RCCL and 28-70x over Cray-MPICH for all-gather on 2048 GCDs of Frontier. These gains translate directly to end-to-end performance: in large-scale GPT-3-style training, PCCL provides up to 60% and 40% speedups over RCCL for 7B and 13B parameter models, respectively.","authors":["Siddharth Singh","Mahua Singh","Abhinav Bhatele"],"url":"https://arxiv.org/abs/2504.18658"}
{"created":"2025-04-29","title":"M2R2: MulitModal Robotic Representation for Temporal Action Segmentation","abstract":"Temporal action segmentation (TAS) has long been a key area of research in both robotics and computer vision. In robotics, algorithms have primarily focused on leveraging proprioceptive information to determine skill boundaries, with recent approaches in surgical robotics incorporating vision. In contrast, computer vision typically relies on exteroceptive sensors, such as cameras. Existing multimodal TAS models in robotics integrate feature fusion within the model, making it difficult to reuse learned features across different models. Meanwhile, pretrained vision-only feature extractors commonly used in computer vision struggle in scenarios with limited object visibility. In this work, we address these challenges by proposing M2R2, a multimodal feature extractor tailored for TAS, which combines information from both proprioceptive and exteroceptive sensors. We introduce a novel pretraining strategy that enables the reuse of learned features across multiple TAS models. Our method achieves state-of-the-art performance on the REASSEMBLE dataset, a challenging multimodal robotic assembly dataset, outperforming existing robotic action segmentation models by 46.6%. Additionally, we conduct an extensive ablation study to evaluate the contribution of different modalities in robotic TAS tasks.","authors":["Daniel Sliwowski","Dongheui Lee"],"url":"https://arxiv.org/abs/2504.18662"}
{"created":"2025-04-29","title":"Co-Training with Active Contrastive Learning and Meta-Pseudo-Labeling on 2D Projections for Deep Semi-Supervised Learning","abstract":"A major challenge that prevents the training of DL models is the limited availability of accurately labeled data. This shortcoming is highlighted in areas where data annotation becomes a time-consuming and error-prone task. In this regard, SSL tackles this challenge by capitalizing on scarce labeled and abundant unlabeled data; however, SoTA methods typically depend on pre-trained features and large validation sets to learn effective representations for classification tasks. In addition, the reduced set of labeled data is often randomly sampled, neglecting the selection of more informative samples. Here, we present active-DeepFA, a method that effectively combines CL, teacher-student-based meta-pseudo-labeling and AL to train non-pretrained CNN architectures for image classification in scenarios of scarcity of labeled and abundance of unlabeled data. It integrates DeepFA into a co-training setup that implements two cooperative networks to mitigate confirmation bias from pseudo-labels. The method starts with a reduced set of labeled samples by warming up the networks with supervised CL. Afterward and at regular epoch intervals, label propagation is performed on the 2D projections of the networks' deep features. Next, the most reliable pseudo-labels are exchanged between networks in a cross-training fashion, while the most meaningful samples are annotated and added into the labeled set. The networks independently minimize an objective loss function comprising supervised contrastive, supervised and semi-supervised loss components, enhancing the representations towards image classification. Our approach is evaluated on three challenging biological image datasets using only 5% of labeled samples, improving baselines and outperforming six other SoTA methods. In addition, it reduces annotation effort by achieving comparable results to those of its counterparts with only 3% of labeled data.","authors":["David Aparco-Cardenas","Jancarlo F. Gomes","Alexandre X. Falc\\~ao","Pedro J. de Rezende"],"url":"https://arxiv.org/abs/2504.18666"}
{"created":"2025-04-29","title":"Collaborative Object Transportation in Space via Impact Interactions","abstract":"We present a planning and control approach for collaborative transportation of objects in space by a team of robots. Object and robots in microgravity environments are not subject to friction but are instead free floating. This property is key to how we approach the transportation problem: the passive objects are controlled by impact interactions with the controlled robots. In particular, given a high-level Signal Temporal Logic (STL) specification of the transportation task, we synthesize motion plans for the robots to maximize the specification satisfaction in terms of spatial STL robustness. Given that the physical impact interactions are complex and hard to model precisely, we also present an alternative formulation maximizing the permissible uncertainty in a simplified kinematic impact model. We define the full planning and control stack required to solve the object transportation problem; an offline planner, an online replanner, and a low-level model-predictive control scheme for each of the robots. We show the method in a high-fidelity simulator for a variety of scenarios and present experimental validation of 2-robot, 1-object scenarios on a freeflyer platform.","authors":["Joris Verhagen","Jana Tumova"],"url":"https://arxiv.org/abs/2504.18667"}
{"created":"2025-04-29","title":"Exploring the Potential of Latent Embeddings for Sea Ice Characterization using ICESat-2 Data","abstract":"The Ice, Cloud, and Elevation Satellite-2 (ICESat-2) provides high-resolution measurements of sea ice height. Recent studies have developed machine learning methods on ICESat-2 data, primarily focusing on surface type classification. However, the heavy reliance on manually collected labels requires significant time and effort for supervised learning, as it involves cross-referencing track measurements with overlapping background optical imagery. Additionally, the coincidence of ICESat-2 tracks with background images is relatively rare due to the different overpass patterns and atmospheric conditions. To address these limitations, this study explores the potential of unsupervised autoencoder on unlabeled data to derive latent embeddings. We develop autoencoder models based on Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNN) to reconstruct topographic sequences from ICESat-2 and derive embeddings. We then apply Uniform Manifold Approximation and Projection (UMAP) to reduce dimensions and visualize the embeddings. Our results show that embeddings from autoencoders preserve the overall structure but generate relatively more compact clusters compared to the original ICESat-2 data, indicating the potential of embeddings to lessen the number of required labels samples.","authors":["Daehyeon Han","Morteza Karimzadeh"],"url":"https://arxiv.org/abs/2504.18668"}
{"created":"2025-04-29","title":"Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction","abstract":"Mild Traumatic Brain Injury (TBI) detection presents significant challenges due to the subtle and often ambiguous presentation of symptoms in medical imaging, making accurate diagnosis a complex task. To address these challenges, we propose Proof-of-TBI, a medical diagnosis support system that integrates multiple fine-tuned vision-language models with the OpenAI-o3 reasoning large language model (LLM). Our approach fine-tunes multiple vision-language models using a labeled dataset of TBI MRI scans, training them to diagnose TBI symptoms effectively. The predictions from these models are aggregated through a consensus-based decision-making process. The system evaluates the predictions from all fine-tuned vision language models using the OpenAI-o3 reasoning LLM, a model that has demonstrated remarkable reasoning performance, to produce the most accurate final diagnosis. The LLM Agents orchestrates interactions between the vision-language models and the reasoning LLM, managing the final decision-making process with transparency, reliability, and automation. This end-to-end decision-making workflow combines the vision-language model consortium with the OpenAI-o3 reasoning LLM, enabled by custom prompt engineering by the LLM agents. The prototype for the proposed platform was developed in collaboration with the U.S. Army Medical Research team in Newport News, Virginia, incorporating five fine-tuned vision-language models. The results demonstrate the transformative potential of combining fine-tuned vision-language model inputs with the OpenAI-o3 reasoning LLM to create a robust, secure, and highly accurate diagnostic system for mild TBI prediction. To the best of our knowledge, this research represents the first application of fine-tuned vision-language models integrated with a reasoning LLM for TBI prediction tasks.","authors":["Ross Gore","Eranga Bandara","Sachin Shetty","Alberto E. Musto","Pratip Rana","Ambrosio Valencia-Romero","Christopher Rhea","Lobat Tayebi","Heather Richter","Atmaram Yarlagadda","Donna Edmonds","Steven Wallace","Donna Broshek"],"url":"https://arxiv.org/abs/2504.18671"}
{"created":"2025-04-29","title":"Can Third-parties Read Our Emotions?","abstract":"Natural Language Processing tasks that aim to infer an author's private states, e.g., emotions and opinions, from their written text, typically rely on datasets annotated by third-party annotators. However, the assumption that third-party annotators can accurately capture authors' private states remains largely unexamined. In this study, we present human subjects experiments on emotion recognition tasks that directly compare third-party annotations with first-party (author-provided) emotion labels. Our findings reveal significant limitations in third-party annotations-whether provided by human annotators or large language models (LLMs)-in faithfully representing authors' private states. However, LLMs outperform human annotators nearly across the board. We further explore methods to improve third-party annotation quality. We find that demographic similarity between first-party authors and third-party human annotators enhances annotation performance. While incorporating first-party demographic information into prompts leads to a marginal but statistically significant improvement in LLMs' performance. We introduce a framework for evaluating the limitations of third-party annotations and call for refined annotation practices to accurately represent and model authors' private states.","authors":["Jiayi Li","Yingfan Zhou","Pranav Narayanan Venkit","Halima Binte Islam","Sneha Arya","Shomir Wilson","Sarah Rajtmajer"],"url":"https://arxiv.org/abs/2504.18673"}
{"created":"2025-04-29","title":"A Hybrid Framework for Efficient Koopman Operator Learning","abstract":"Koopman analysis of a general dynamics system provides a linear Koopman operator and an embedded eigenfunction space, enabling the application of standard techniques from linear analysis. However, in practice, deriving exact operators and mappings for the observable space is intractable, and deriving an approximation or expressive subset of these functions is challenging. Programmatic methods often rely on system-specific parameters and may scale poorly in both time and space, while learning-based approaches depend heavily on difficult-to-know hyperparameters, such as the dimension of the observable space. To address the limitations of both methods, we propose a hybrid framework that uses semidefinite programming to find a representation of the linear operator, then learns an approximate mapping into and out of the space that the operator propagates. This approach enables efficient learning of the operator and explicit mappings while reducing the need for specifying the unknown structure ahead of time.","authors":["Alexander Estornell","Leonard Jung","Alenna Spiro","Mario Sznaier","Michael Everett"],"url":"https://arxiv.org/abs/2504.18676"}
{"created":"2025-04-29","title":"Empirical Bernstein and betting confidence intervals for randomized quasi-Monte Carlo","abstract":"Randomized quasi-Monte Carlo (RQMC) methods estimate the mean of a random variable by sampling an integrand at $n$ equidistributed points. For scrambled digital nets, the resulting variance is typically $\\tilde O(n^{-\\theta})$ where $\\theta\\in[1,3]$ depends on the smoothness of the integrand and $\\tilde O$ neglects logarithmic factors. While RQMC can be far more accurate than plain Monte Carlo (MC) it remains difficult to get confidence intervals on RQMC estimates. We investigate some empirical Bernstein confidence intervals (EBCI) and hedged betting confidence intervals (HBCI), both from Waudby-Smith and Ramdas (2024), when the random variable of interest is subject to known bounds. When there are $N$ integrand evaluations partitioned into $R$ independent replicates of $n=N/R$ RQMC points, and the RQMC variance is $\\Theta(n^{-\\theta})$, then an oracle minimizing the width of a Bennett confidence interval would choose $n =\\Theta(N^{1/(\\theta+1)})$. The resulting intervals have a width that is $\\Theta(N^{-\\theta/(\\theta+1)})$. Our empirical investigations had optimal values of $n$ grow slowly with $N$, HBCI intervals that were usually narrower than the EBCI ones, and optimal values of $n$ for HBCI that were equal to or smaller than the ones for the oracle.","authors":["Aadit Jain","Fred J. Hickernell","Art B. Owen","Aleksei G. Sorokin"],"url":"https://arxiv.org/abs/2504.18677"}
{"created":"2025-04-29","title":"SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models","abstract":"Interpreting object-referential language and grounding objects in 3D with spatial relations and attributes is essential for robots operating alongside humans. However, this task is often challenging due to the diversity of scenes, large number of fine-grained objects, and complex free-form nature of language references. Furthermore, in the 3D domain, obtaining large amounts of natural language training data is difficult. Thus, it is important for methods to learn from little data and zero-shot generalize to new environments. To address these challenges, we propose SORT3D, an approach that utilizes rich object attributes from 2D data and merges a heuristics-based spatial reasoning toolbox with the ability of large language models (LLMs) to perform sequential reasoning. Importantly, our method does not require text-to-3D data for training and can be applied zero-shot to unseen environments. We show that SORT3D achieves state-of-the-art performance on complex view-dependent grounding tasks on two benchmarks. We also implement the pipeline to run real-time on an autonomous vehicle and demonstrate that our approach can be used for object-goal navigation on previously unseen real-world environments. All source code for the system pipeline is publicly released at https://github.com/nzantout/SORT3D .","authors":["Nader Zantout","Haochen Zhang","Pujith Kachana","Jinkai Qiu","Ji Zhang","Wenshan Wang"],"url":"https://arxiv.org/abs/2504.18684"}
{"created":"2025-04-29","title":"GeoFINDR: Practical Approach to Verify Cloud Instances Geolocation in Multicloud","abstract":"In multicloud environments, where legal obligations, technical constraints and economic interests are at stake, it is of interest to stakeholders to be able to locate cloud data or the cloud instance where data are decrypted for processing, making it particularly vulnerable. This paper proposes an original and practical delay-based approach, called GeoFINDR, to locate a cloud instance, e.g. a Virtual Machine (VM), over the Internet, based on RIPE Atlas landmarks. First, the assumed threat model and assumptions are more realistic than in existing solutions, e.g. VM-scale localization in multicloud environments, a Cloud Service Provider (CSP) lying about the VM's location. Second, the originality of the approach lies in four original ideas: (1) geolocalization is performed from the VM, (2) a Greedy algorithm selects a first set LM_A of distributed audit landmarks in the vicinity of the declared area, (3) a sectorization algorithm identifies a set LM_S of other landmarks with distance delay behavior similar to that of the VM to estimate the sector of the VM, and (4) the estimated location of the VM is calculated as the barycenter position of the LM_S landmarks. An open source tool is published on GitHub and experiments show that localization accuracy can be as high as 22.1km, under unfavorable conditions where the CSP lies about the location of the VM.","authors":["Said Ider","Maryline Laurent"],"url":"https://arxiv.org/abs/2504.18685"}
{"created":"2025-04-29","title":"A Unified MDL-based Binning and Tensor Factorization Framework for PDF Estimation","abstract":"Reliable density estimation is fundamental for numerous applications in statistics and machine learning. In many practical scenarios, data are best modeled as mixtures of component densities that capture complex and multimodal patterns. However, conventional density estimators based on uniform histograms often fail to capture local variations, especially when the underlying distribution is highly nonuniform. Furthermore, the inherent discontinuity of histograms poses challenges for tasks requiring smooth derivatives, such as gradient-based optimization, clustering, and nonparametric discriminant analysis. In this work, we present a novel non-parametric approach for multivariate probability density function (PDF) estimation that utilizes minimum description length (MDL)-based binning with quantile cuts. Our approach builds upon tensor factorization techniques, leveraging the canonical polyadic decomposition (CPD) of a joint probability tensor. We demonstrate the effectiveness of our method on synthetic data and a challenging real dry bean classification dataset.","authors":["Mustafa Musab","Joseph K. Chege","Arie Yeredor","Martin Haardt"],"url":"https://arxiv.org/abs/2504.18686"}
{"created":"2025-04-29","title":"Transformational Creativity in Science: A Graphical Theory","abstract":"Creative processes are typically divided into three types: combinatorial, exploratory, and transformational. Here, we provide a graphical theory of transformational scientific creativity, synthesizing Boden's insight that transformational creativity arises from changes in the \"enabling constraints\" of a conceptual space and Kuhn's structure of scientific revolutions as resulting from paradigm shifts. We prove that modifications made to axioms of our graphical model have the most transformative potential and then illustrate how several historical instances of transformational creativity can be captured by our framework.","authors":["Samuel Schapiro","Jonah Black","Lav R. Varshney"],"url":"https://arxiv.org/abs/2504.18687"}
{"created":"2025-04-29","title":"HierSum: A Global and Local Attention Mechanism for Video Summarization","abstract":"Video summarization creates an abridged version (i.e., a summary) that provides a quick overview of the video while retaining pertinent information. In this work, we focus on summarizing instructional videos and propose a method for breaking down a video into meaningful segments, each corresponding to essential steps in the video. We propose \\textbf{HierSum}, a hierarchical approach that integrates fine-grained local cues from subtitles with global contextual information provided by video-level instructions. Our approach utilizes the ``most replayed\" statistic as a supervisory signal to identify critical segments, thereby improving the effectiveness of the summary. We evaluate on benchmark datasets such as TVSum, BLiSS, Mr.HiSum, and the WikiHow test set, and show that HierSum consistently outperforms existing methods in key metrics such as F1-score and rank correlation. We also curate a new multi-modal dataset using WikiHow and EHow videos and associated articles containing step-by-step instructions. Through extensive ablation studies, we demonstrate that training on this dataset significantly enhances summarization on the target datasets.","authors":["Apoorva Beedu","Irfan Essa"],"url":"https://arxiv.org/abs/2504.18689"}
{"created":"2025-04-29","title":"From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions","abstract":"Background and Context. The increasing integration of large language models (LLMs) in computing education presents an emerging challenge in understanding how students use LLMs and craft prompts to solve computational tasks. Prior research has used both qualitative and quantitative methods to analyze prompting behavior, but these approaches lack scalability or fail to effectively capture the semantic evolution of prompts. Objective. In this paper, we investigate whether students prompts can be systematically analyzed using propositional logic constraints. We examine whether this approach can identify patterns in prompt evolution, detect struggling students, and provide insights into effective and ineffective strategies. Method. We introduce Prompt2Constraints, a novel method that translates students prompts into logical constraints. The constraints are able to represent the intent of the prompts in succinct and quantifiable ways. We used this approach to analyze a dataset of 1,872 prompts from 203 students solving introductory programming tasks. Findings. We find that while successful and unsuccessful attempts tend to use a similar number of constraints overall, when students fail, they often modify their prompts more significantly, shifting problem-solving strategies midway. We also identify points where specific interventions could be most helpful to students for refining their prompts. Implications. This work offers a new and scalable way to detect students who struggle in solving natural language programming tasks. This work could be extended to investigate more complex tasks and integrated into programming tools to provide real-time support.","authors":["Ali Alfageeh","Sadegh AlMahdi Kazemi Zarkouei","Daye Nam","Daniel Prol","Matin Amoozadeh","Souti Chattopadhyay","James Prather","Paul Denny","Juho Leinonen","Michael Hilton","Sruti Srinivasa Ragavan","Mohammad Amin Alipour"],"url":"https://arxiv.org/abs/2504.18691"}
{"created":"2025-04-29","title":"Learning-Based Modeling of Soft Actuators Using Euler Spiral-Inspired Curvature","abstract":"Soft robots, distinguished by their inherent compliance and continuum structures, present unique modeling challenges, especially when subjected to significant external loads such as gravity and payloads. In this study, we introduce an innovative data-driven modeling framework leveraging an Euler spiral-inspired shape representations to accurately describe the complex shapes of soft continuum actuators. Based on this representation, we develop neural network-based forward and inverse models to effectively capture the nonlinear behavior of a fiber-reinforced pneumatic bending actuator. Our forward model accurately predicts the actuator's deformation given inputs of pressure and payload, while the inverse model reliably estimates payloads from observed actuator shapes and known pressure inputs. Comprehensive experimental validation demonstrates the effectiveness and accuracy of our proposed approach. Notably, the augmented Euler spiral-based forward model achieves low average positional prediction errors of 3.38%, 2.19%, and 1.93% of the actuator length at the one-third, two-thirds, and tip positions, respectively. Furthermore, the inverse model demonstrates precision of estimating payloads with an average error as low as 0.72% across the tested range. These results underscore the potential of our method to significantly enhance the accuracy and predictive capabilities of modeling frameworks for soft robotic systems.","authors":["Yu Mei","Shangyuan Yuan","Xinda Qi","Preston Fairchild","Xiaobo Tan"],"url":"https://arxiv.org/abs/2504.18692"}
{"created":"2025-04-29","title":"Technical Challenges in Maintaining Tax Prep Software with Large Language Models","abstract":"As the US tax law evolves to adapt to ever-changing politico-economic realities, tax preparation software plays a significant role in helping taxpayers navigate these complexities. The dynamic nature of tax regulations poses a significant challenge to accurately and timely maintaining tax software artifacts. The state-of-the-art in maintaining tax prep software is time-consuming and error-prone as it involves manual code analysis combined with an expert interpretation of tax law amendments. We posit that the rigor and formality of tax amendment language, as expressed in IRS publications, makes it amenable to automatic translation to executable specifications (code). Our research efforts focus on identifying, understanding, and tackling technical challenges in leveraging Large Language Models (LLMs), such as ChatGPT and Llama, to faithfully extract code differentials from IRS publications and automatically integrate them with the prior version of the code to automate tax prep software maintenance.","authors":["Sina Gogani-Khiabani","Varsha Dewangan","Nina Olson","Ashutosh Trivedi","Saeid Tizpaz-Niari"],"url":"https://arxiv.org/abs/2504.18693"}
{"created":"2025-04-29","title":"Active Few-Shot Learning for Vertex Classification Starting from an Unlabeled Dataset","abstract":"Despite the ample availability of graph data, obtaining vertex labels is a tedious and expensive task. Therefore, it is desirable to learn from a few labeled vertices only. Existing few-shot learners assume a class oracle, which provides labeled vertices for a desired class. However, such an oracle is not available in a real-world setting, i.e., when drawing a vertex for labeling it is unknown to which class the vertex belongs. Few-shot learners are often combined with prototypical networks, while classical semi-supervised vertex classification uses discriminative models, e.g., Graph Convolutional Networks (GCN). In this paper, we train our models by iteratively prompting a human annotator with vertices to annotate. We perform three experiments where we continually relax our assumptions. First, we assume a class oracle, i.e., the human annotator is provided with an equal number of vertices to label for each class. We denote this as \"Balanced Sampling''. In the subsequent experiment, \"Unbalanced Sampling,'' we replace the class oracle with $k$-medoids clustering and draw vertices to label from the clusters. In the last experiment, the \"Unknown Number of Classes,'' we no longer assumed we knew the number and distribution of classes. Our results show that prototypical models outperform discriminative models in all experiments when fewer than $20$ samples per class are available. While dropping the assumption of the class oracle for the \"Unbalanced Sampling'' experiment reduces the performance of the GCN by $9\\%$, the prototypical network loses only $1\\%$ on average. For the \"Unknown Number of Classes'' experiment, the average performance for both models decreased further by $1\\%$.","authors":["Felix Burr","Marcel Hoffmann","Ansgar Scherp"],"url":"https://arxiv.org/abs/2504.18696"}
{"created":"2025-04-29","title":"Robust Push Recovery on Bipedal Robots: Leveraging Multi-Domain Hybrid Systems with Reduced-Order Model Predictive Control","abstract":"In this paper, we present a novel control framework to achieve robust push recovery on bipedal robots while locomoting. The key contribution is the unification of hybrid system models of locomotion with a reduced-order model predictive controller determining: foot placement, step timing, and ankle control. The proposed reduced-order model is an augmented Linear Inverted Pendulum model with zero moment point coordinates; this is integrated within a model predictive control framework for robust stabilization under external disturbances. By explicitly leveraging the hybrid dynamics of locomotion, our approach significantly improves stability and robustness across varying walking heights, speeds, step durations, and is effective for both flat-footed and more complex multi-domain heel-to-toe walking patterns. The framework is validated with high-fidelity simulation on Cassie, a 3D underactuated robot, showcasing real-time feasibility and substantially improved stability. The results demonstrate the robustness of the proposed method in dynamic environments.","authors":["Min Dai","Aaron D. Ames"],"url":"https://arxiv.org/abs/2504.18698"}
{"created":"2025-04-29","title":"Fast Singular-Kernel Convolution on General Non-Smooth Domains via Truncated Fourier Filtering","abstract":"The rapid and accurate evaluation of convolutions with singular kernels plays crucial roles in a wide range of scientific and engineering applications. Building on the recently introduced Truncated Fourier Filtering method for smooth kernels, this work presents a fast, high-order numerical methodology that extends the approach to singular kernels and non-smooth domains. The method relies on truncated Fourier expansions of a prescribed order $F$ for the characteristic function of the integration domain, as well as expansions for the products of characteristic functions and singular functions. This version of the article is a preliminary draft, shared to facilitate early dissemination. The complete version will be available shortly.","authors":["Oscar Bruno","Jinghao Cao"],"url":"https://arxiv.org/abs/2504.18699"}
{"created":"2025-04-29","title":"Codetations: Intelligent, Persistent Notes and UIs for Programs and Other Documents","abstract":"Software developers maintain extensive mental models of code they produce and its context, often relying on memory to retrieve or reconstruct design decisions, edge cases, and debugging experiences. These missing links and data obstruct both developers and, more recently, large language models (LLMs) working with unfamiliar code. We present Codetations, a system that helps developers contextualize documents with rich notes and tools. Unlike previous approaches, notes in Codetations stay outside the document to prevent code clutter, attaching to spans in the document using a hybrid edit-tracking/LLM-based method. Their content is dynamic, interactive, and synchronized with code changes. A worked example shows that relevant notes with interactively-collected data improve LLM performance during code repair. In our user evaluation, developers praised these properties and saw significant potential in annotation types that we generated with an LLM in just a few minutes.","authors":["Edward Misback","Erik Vank","Zachary Tatlock","Steven Tanimoto"],"url":"https://arxiv.org/abs/2504.18702"}
{"created":"2025-04-29","title":"An Interactive Debugger for Rust Trait Errors","abstract":"Compiler diagnostics for type inference failures are notoriously bad, and type classes only make the problem worse. By introducing a complex search process during inference, type classes can lead to wholly inscrutable or useless errors. We describe a system, Argus, for interactively visualizing type class inferences to help programmers debug inference failures, applied specifically to Rust's trait system. The core insight of Argus is to avoid the traditional model of compiler diagnostics as one-size-fits-all, instead providing the programmer with different views on the search tree corresponding to different debugging goals. Argus carefully uses defaults to improve debugging productivity, including interface design (e.g., not showing full paths of types by default) and heuristics (e.g., sorting obligations based on the expected complexity of fixing them). We evaluated Argus in a user study where $N = 25$ participants debugged type inference failures in realistic Rust programs, finding that participants using Argus correctly localized $2.2\\times$ as many faults and localized $3.3\\times$ faster compared to not using Argus.","authors":["Gavin Gray (Brown University)","Will Crichton (Brown University)","Shriram Krishnamurthi (Brown University)"],"url":"https://arxiv.org/abs/2504.18704"}
{"created":"2025-04-29","title":"On Queueing Theory for Large-Scale CI/CD Pipelines Optimization","abstract":"Continuous Integration and Continuous Deployment (CI/CD) pipelines are central to modern software development. In large organizations, the high volume of builds and tests creates bottlenecks, especially under shared infrastructure. This article proposes a modeling framework based on queueing theory to optimize large-scale CI/CD workflows. We formalize the system using classical $M/M/c$ queueing models and discuss strategies to minimize delays and infrastructure costs. Our approach integrates theoretical results with practical techniques, including dynamic scaling and prioritization of CI/CD tasks.","authors":["Gr\\'egory Bournassenko"],"url":"https://arxiv.org/abs/2504.18705"}
{"created":"2025-04-29","title":"Decentralized Fusion of 3D Extended Object Tracking based on a B-Spline Shape Model","abstract":"Extended Object Tracking (EOT) exploits the high resolution of modern sensors for detailed environmental perception. Combined with decentralized fusion, it contributes to a more scalable and robust perception system. This paper investigates the decentralized fusion of 3D EOT using a B-spline curve based model. The spline curve is used to represent the side-view profile, which is then extruded with a width to form a 3D shape. We use covariance intersection (CI) for the decentralized fusion and discuss the challenge of applying it to EOT. We further evaluate the tracking result of the decentralized fusion with simulated and real datasets of traffic scenarios. We show that the CI-based fusion can significantly improve the tracking performance for sensors with unfavorable perspective.","authors":["Longfei Han","Klaus Kefferp\\\"utz","J\\\"urgen Beyerer"],"url":"https://arxiv.org/abs/2504.18708"}
{"created":"2025-04-29","title":"Explicit neural network classifiers for non-separable data","abstract":"We fully characterize a large class of feedforward neural networks in terms of truncation maps. As an application, we show how a ReLU neural network can implement a feature map which separates concentric data.","authors":["Patr\\'icia Mu\\~noz Ewald"],"url":"https://arxiv.org/abs/2504.18710"}
{"created":"2025-04-29","title":"Certifiably-Correct Mapping for Safe Navigation Despite Odometry Drift","abstract":"Accurate perception, state estimation and mapping are essential for safe robotic navigation as planners and controllers rely on these components for safety-critical decisions. However, existing mapping approaches often assume perfect pose estimates, an unrealistic assumption that can lead to incorrect obstacle maps and therefore collisions. This paper introduces a framework for certifiably-correct mapping that ensures that the obstacle map correctly classifies obstacle-free regions despite the odometry drift in vision-based localization systems (VIO}/SLAM). By deflating the safe region based on the incremental odometry error at each timestep, we ensure that the map remains accurate and reliable locally around the robot, even as the overall odometry error with respect to the inertial frame grows unbounded.","authors":["Devansh R. Agrawal","Taekyung Kim","Rajiv Govindjee","Trushant Adeshara","Jiangbo Yu","Anurekha Ravikumar","Dimitra Panagou"],"url":"https://arxiv.org/abs/2504.18713"}
{"created":"2025-04-29","title":"From Good to Great: Improving Memory Tiering Performance Through Parameter Tuning","abstract":"Memory tiering systems achieve memory scaling by adding multiple tiers of memory wherein different tiers have different access latencies and bandwidth. For maximum performance, frequently accessed (hot) data must be placed close to the host in faster tiers and infrequently accessed (cold) data can be placed in farther slower memory tiers. Existing tiering solutions employ heuristics and pre-configured thresholds to make data placement and migration decisions. Unfortunately, these systems fail to adapt to different workloads and the underlying hardware, so perform sub-optimally.","authors":["Konstantinos Kanellis","Sujay Yadalam","Fanchao Chen","Michael Swift","Shivaram Venkataraman"],"url":"https://arxiv.org/abs/2504.18714"}
{"created":"2025-04-29","title":"Spatial Speech Translation: Translating Across Space With Binaural Hearables","abstract":"Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.","authors":["Tuochao Chen","Qirui Wang","Runlin He","Shyam Gollakota"],"url":"https://arxiv.org/abs/2504.18715"}
{"created":"2025-04-29","title":"Building UD Cairo for Old English in the Classroom","abstract":"In this paper we present a sample treebank for Old English based on the UD Cairo sentences, collected and annotated as part of a classroom curriculum in Historical Linguistics. To collect the data, a sample of 20 sentences illustrating a range of syntactic constructions in the world's languages, we employ a combination of LLM prompting and searches in authentic Old English data. For annotation we assigned sentences to multiple students with limited prior exposure to UD, whose annotations we compare and adjudicate. Our results suggest that while current LLM outputs in Old English do not reflect authentic syntax, this can be mitigated by post-editing, and that although beginner annotators do not possess enough background to complete the task perfectly, taken together they can produce good results and learn from the experience. We also conduct preliminary parsing experiments using Modern English training data, and find that although performance on Old English is poor, parsing on annotated features (lemma, hyperlemma, gloss) leads to improved performance.","authors":["Lauren Levine","Junghyun Min","Amir Zeldes"],"url":"https://arxiv.org/abs/2504.18718"}
{"created":"2025-04-29","title":"Vysics: Object Reconstruction Under Occlusion by Fusing Vision and Contact-Rich Physics","abstract":"We introduce Vysics, a vision-and-physics framework for a robot to build an expressive geometry and dynamics model of a single rigid body, using a seconds-long RGBD video and the robot's proprioception. While the computer vision community has built powerful visual 3D perception algorithms, cluttered environments with heavy occlusions can limit the visibility of objects of interest. However, observed motion of partially occluded objects can imply physical interactions took place, such as contact with a robot or the environment. These inferred contacts can supplement the visible geometry with \"physible geometry,\" which best explains the observed object motion through physics. Vysics uses a vision-based tracking and reconstruction method, BundleSDF, to estimate the trajectory and the visible geometry from an RGBD video, and an odometry-based model learning method, Physics Learning Library (PLL), to infer the \"physible\" geometry from the trajectory through implicit contact dynamics optimization. The visible and \"physible\" geometries jointly factor into optimizing a signed distance function (SDF) to represent the object shape. Vysics does not require pretraining, nor tactile or force sensors. Compared with vision-only methods, Vysics yields object models with higher geometric accuracy and better dynamics prediction in experiments where the object interacts with the robot and the environment under heavy occlusion. Project page: https://vysics-vision-and-physics.github.io/","authors":["Bibit Bianchini","Minghan Zhu","Mengti Sun","Bowen Jiang","Camillo J. Taylor","Michael Posa"],"url":"https://arxiv.org/abs/2504.18719"}
{"created":"2025-04-29","title":"Appa: Bending Weather Dynamics with Latent Diffusion Models for Global Data Assimilation","abstract":"Deep learning has transformed weather forecasting by improving both its accuracy and computational efficiency. However, before any forecast can begin, weather centers must identify the current atmospheric state from vast amounts of observational data. To address this challenging problem, we introduce Appa, a score-based data assimilation model producing global atmospheric trajectories at 0.25-degree resolution and 1-hour intervals. Powered by a 1.5B-parameter spatio-temporal latent diffusion model trained on ERA5 reanalysis data, Appa can be conditioned on any type of observations to infer the posterior distribution of plausible state trajectories, without retraining. Our unified probabilistic framework flexibly tackles multiple inference tasks -- reanalysis, filtering, and forecasting -- using the same model, eliminating the need for task-specific architectures or training procedures. Experiments demonstrate physical consistency on a global scale and good reconstructions from observations, while showing competitive forecasting skills. Our results establish latent score-based data assimilation as a promising foundation for future global atmospheric modeling systems.","authors":["G\\'er\\^ome Andry","Fran\\c{c}ois Rozet","Sacha Lewin","Omer Rochman","Victor Mangeleer","Matthias Pirlet","Elise Faulx","Marilaure Gr\\'egoire","Gilles Louppe"],"url":"https://arxiv.org/abs/2504.18720"}
{"created":"2025-04-29","title":"MODP: Multi Objective Directional Prompting","abstract":"Recent advances in large language models (LLMs) have led to their popularity across multiple use-cases. However, prompt engineering, the process for optimally utilizing such models, remains approximation-driven and subjective. Most of the current research on prompt engineering focuses on task-specific optimization, while neglecting the behavior of the LLM under consideration during prompt development. This paper introduces MODP -- Multi Objective Directional Prompting, a framework based on two key concepts: 1) multi-objectivity: the importance of considering an LLM's intrinsic behavior as an additional objective in prompt development, and 2) directional prompting: a metrics-driven method for prompt engineering to ensure development of robust and high-precision prompts. We demonstrate the effectiveness of our proposed ideas on a summarization task, using a synthetically created dataset, achieving a 26% performance gain over initial prompts. Finally, we apply MODP to develop prompts for Dell's Next Best Action support tool, which is now in production and is used by more than 10,000 internal support agents and serving millions of customers worldwide.","authors":["Aashutosh Nema","Samaksh Gulati","Evangelos Giakoumakis","Bipana Thapaliya"],"url":"https://arxiv.org/abs/2504.18722"}
{"created":"2025-04-29","title":"World Food Atlas Project","abstract":"A coronavirus pandemic is forcing people to be \"at home\" all over the world. In a life of hardly ever going out, we would have realized how the food we eat affects our bodies. What can we do to know our food more and control it better? To give us a clue, we are trying to build a World Food Atlas (WFA) that collects all the knowledge about food in the world. In this paper, we present two of our trials. The first is the Food Knowledge Graph (FKG), which is a graphical representation of knowledge about food and ingredient relationships derived from recipes and food nutrition data. The second is the FoodLog Athl and the RecipeLog that are applications for collecting people's detailed records about food habit. We also discuss several problems that we try to solve to build the WFA by integrating these two ideas.","authors":["Ali Rostami","Z Xie","A Ishino","Y Yamakata","K Aizawa","Ramesh Jain"],"url":"https://arxiv.org/abs/2504.18727"}
{"created":"2025-04-29","title":"Multimodal graph representation learning for website generation based on visual sketch","abstract":"The Design2Code problem, which involves converting digital designs into functional source code, is a significant challenge in software development due to its complexity and time-consuming nature. Traditional approaches often struggle with accurately interpreting the intricate visual details and structural relationships inherent in webpage designs, leading to limitations in automation and efficiency. In this paper, we propose a novel method that leverages multimodal graph representation learning to address these challenges. By integrating both visual and structural information from design sketches, our approach enhances the accuracy and efficiency of code generation, particularly in producing semantically correct and structurally sound HTML code. We present a comprehensive evaluation of our method, demonstrating significant improvements in both accuracy and efficiency compared to existing techniques. Extensive evaluation demonstrates significant improvements of multimodal graph learning over existing techniques, highlighting the potential of our method to revolutionize design-to-code automation. Code available at https://github.com/HySonLab/Design2Code","authors":["Tung D. Vu","Chung Hoang","Truong-Son Hy"],"url":"https://arxiv.org/abs/2504.18729"}
{"created":"2025-04-29","title":"A Unified Alternating Optimization Framework for Joint Sensor and Actuator Configuration in LQG Systems","abstract":"This paper fills a gap in the literature by considering a joint sensor and actuator configuration problem under the linear quadratic Gaussian (LQG) performance without assuming a predefined set of candidate components. Different from the existing research, which primarily focuses on selecting or placing sensors and actuators from a fixed group, we consider a more flexible formulation where these components must be designed from scratch, subject to general-form configuration costs and constraints. To address this challenge, we first analytically characterize the gradients of the LQG performance with respect to the sensor and actuator matrices using algebraic Riccati equations. Subsequently, we derive first-order optimality conditions based on the Karush-Kuhn-Tucker (KKT) analysis and develop a unified alternating direction method of multipliers (ADMM)-based alternating optimization framework to address the general-form sensor and actuator configuration problem. Furthermore, we investigate three representative scenarios: sparsity promoting configuration, low-rank promoting configuration, and structure-constrained configuration. For each scenario, we provide in-depth analysis and develop tailored computational schemes. The proposed framework ensures numerical efficiency and adaptability to various design constraints and configuration costs, making it well-suited for integration into numerical solvers.","authors":["Nachuan Yang","Yuzhe Li","Ling Shi","Tongwen Chen"],"url":"https://arxiv.org/abs/2504.18731"}
{"created":"2025-04-29","title":"A convergent algorithm for mean curvature flow of surfaces with Dirichlet boundary conditions","abstract":"We establish convergence results for a spatial semidiscretization of Mean Curvature Flow (MCF) for surfaces with fixed boundaries.","authors":["B\\'arbara Solange Ivaniszyn","Pedro Morin","M. Sebasti\\'an Pauletti"],"url":"https://arxiv.org/abs/2504.18734"}
{"created":"2025-04-29","title":"TLoRA: Tri-Matrix Low-Rank Adaptation of Large Language Models","abstract":"We propose TLoRA, a novel tri-matrix low-rank adaptation method that decomposes weight updates into three matrices: two fixed random matrices and one trainable matrix, combined with a learnable, layer-wise scaling factor. This tri-matrix design enables TLoRA to achieve highly efficient parameter adaptation while introducing minimal additional computational overhead. Through extensive experiments on the GLUE benchmark, we demonstrate that TLoRA achieves comparable performance to existing low-rank methods such as LoRA and Adapter-based techniques, while requiring significantly fewer trainable parameters. Analyzing the adaptation dynamics, we observe that TLoRA exhibits Gaussian-like weight distributions, stable parameter norms, and scaling factor variability across layers, further highlighting its expressive power and adaptability. Additionally, we show that TLoRA closely resembles LoRA in its eigenvalue distributions, parameter norms, and cosine similarity of updates, underscoring its ability to effectively approximate LoRA's adaptation behavior. Our results establish TLoRA as a highly efficient and effective fine-tuning method for LLMs, offering a significant step forward in resource-efficient model adaptation.","authors":["Tanvir Islam"],"url":"https://arxiv.org/abs/2504.18735"}
{"created":"2025-04-29","title":"EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers","abstract":"We study the task of automatically finding evidence relevant to hypotheses in biomedical papers. Finding relevant evidence is an important step when researchers investigate scientific hypotheses. We introduce EvidenceBench to measure models performance on this task, which is created by a novel pipeline that consists of hypothesis generation and sentence-by-sentence annotation of biomedical papers for relevant evidence, completely guided by and faithfully following existing human experts judgment. We demonstrate the pipeline's validity and accuracy with multiple sets of human-expert annotations. We evaluated a diverse set of language models and retrieval systems on the benchmark and found that model performances still fall significantly short of the expert level on this task. To show the scalability of our proposed pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated papers with hypotheses to facilitate model training and development. Both datasets are available at https://github.com/EvidenceBench/EvidenceBench","authors":["Jianyou Wang","Weili Cao","Kaicheng Wang","Xiaoyue Wang","Ashish Dalvi","Gino Prasad","Qishan Liang","Hsuan-lin Her","Ming Wang","Qin Yang","Gene W. Yeo","David E. Neal","Maxim Khan","Christopher D. Rosin","Ramamohan Paturi","Leon Bergen"],"url":"https://arxiv.org/abs/2504.18736"}
{"created":"2025-04-29","title":"A Review of 3D Object Detection with Vision-Language Models","abstract":"This review provides a systematic analysis of comprehensive survey of 3D object detection with vision-language models(VLMs) , a rapidly advancing area at the intersection of 3D vision and multimodal AI. By examining over 100 research papers, we provide the first systematic analysis dedicated to 3D object detection with vision-language models. We begin by outlining the unique challenges of 3D object detection with vision-language models, emphasizing differences from 2D detection in spatial reasoning and data complexity. Traditional approaches using point clouds and voxel grids are compared to modern vision-language frameworks like CLIP and 3D LLMs, which enable open-vocabulary detection and zero-shot generalization. We review key architectures, pretraining strategies, and prompt engineering methods that align textual and 3D features for effective 3D object detection with vision-language models. Visualization examples and evaluation benchmarks are discussed to illustrate performance and behavior. Finally, we highlight current challenges, such as limited 3D-language datasets and computational demands, and propose future research directions to advance 3D object detection with vision-language models. >Object Detection, Vision-Language Models, Agents, VLMs, LLMs, AI","authors":["Ranjan Sapkota","Konstantinos I Roumeliotis","Rahul Harsha Cheppally","Marco Flores Calero","Manoj Karkee"],"url":"https://arxiv.org/abs/2504.18738"}
{"created":"2025-04-29","title":"Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes","abstract":"This work presents the first finite-time analysis for the last-iterate convergence of average-reward Q-learning with an asynchronous implementation. A key feature of the algorithm we study is the use of adaptive stepsizes, which serve as local clocks for each state-action pair. We show that the iterates generated by this Q-learning algorithm converge at a rate of $O(1/k)$ (in the mean-square sense) to the optimal relative Q-function in the span seminorm. Moreover, by adding a centering step to the algorithm, we further establish pointwise mean-square convergence to a centered optimal relative Q-function, also at a rate of $O(1/k)$. To prove these results, we show that adaptive stepsizes are necessary, as without them, the algorithm fails to converge to the correct target. In addition, adaptive stepsizes can be interpreted as a form of implicit importance sampling that counteracts the effects of asynchronous updates.","authors":["Zaiwei Chen"],"url":"https://arxiv.org/abs/2504.18743"}
{"created":"2025-04-29","title":"Dream-Box: Object-wise Outlier Generation for Out-of-Distribution Detection","abstract":"Deep neural networks have demonstrated great generalization capabilities for tasks whose training and test sets are drawn from the same distribution. Nevertheless, out-of-distribution (OOD) detection remains a challenging task that has received significant attention in recent years. Specifically, OOD detection refers to the detection of instances that do not belong to the training distribution, while still having good performance on the in-distribution task (e.g., classification or object detection). Recent work has focused on generating synthetic outliers and using them to train an outlier detector, generally achieving improved OOD detection than traditional OOD methods. In this regard, outliers can be generated either in feature or pixel space. Feature space driven methods have shown strong performance on both the classification and object detection tasks, at the expense that the visualization of training outliers remains unknown, making further analysis on OOD failure modes challenging. On the other hand, pixel space outlier generation techniques enabled by diffusion models have been used for image classification using, providing improved OOD detection performance and outlier visualization, although their adaption to the object detection task is as yet unexplored. We therefore introduce Dream-Box, a method that provides a link to object-wise outlier generation in the pixel space for OOD detection. Specifically, we use diffusion models to generate object-wise outliers that are used to train an object detector for an in-distribution task and OOD detection. Our method achieves comparable performance to previous traditional methods while being the first technique to provide concrete visualization of generated OOD objects.","authors":["Brian K. S. Isaac-Medina","Toby P. Breckon"],"url":"https://arxiv.org/abs/2504.18746"}
{"created":"2025-04-29","title":"Covert Communication Over a Quantum MAC with a Helper","abstract":"We study covert classical communication over a quantum multiple-access channel (MAC) with a helper. Specifically, we consider three transmitters, where one transmitter helps the other two transmitters communicate covertly with a receiver. We demonstrate the feasibility of achieving a positive covert rate over this channel and establish an achievable rate region. Our result recovers as a special case known results for classical communication over classical MACs with a degraded message set, classical communication over quantum MACs, and classical communication over MACs with a helper. To the best of our knowledge, our result is the first to achieve covert communication with positive rates over both classical and quantum MACs.","authors":["Hassan ZivariFard","R\\'emi A. Chou","Xiaodong Wang"],"url":"https://arxiv.org/abs/2504.18747"}
{"created":"2025-04-29","title":"Generative Product Recommendations for Implicit Superlative Queries","abstract":"In Recommender Systems, users often seek the best products through indirect, vague, or under-specified queries, such as \"best shoes for trail running\". Such queries, also referred to as implicit superlative queries, pose a significant challenge for standard retrieval and ranking systems as they lack an explicit mention of attributes and require identifying and reasoning over complex factors. We investigate how Large Language Models (LLMs) can generate implicit attributes for ranking as well as reason over them to improve product recommendations for such queries. As a first step, we propose a novel four-point schema for annotating the best product candidates for superlative queries called SUPERB, paired with LLM-based product annotations. We then empirically evaluate several existing retrieval and ranking approaches on our new dataset, providing insights and discussing their integration into real-world e-commerce production systems.","authors":["Kaustubh D. Dhole","Nikhita Vedula","Saar Kuzi","Giuseppe Castellucci","Eugene Agichtein","Shervin Malmasi"],"url":"https://arxiv.org/abs/2504.18748"}
{"created":"2025-04-29","title":"Multi-Stage Boundary-Aware Transformer Network for Action Segmentation in Untrimmed Surgical Videos","abstract":"Understanding actions within surgical workflows is essential for evaluating post-operative outcomes. However, capturing long sequences of actions performed in surgical settings poses challenges, as individual surgeons have their unique approaches shaped by their expertise, leading to significant variability. To tackle this complex problem, we focused on segmentation with precise boundaries, a demanding task due to the inherent variability in action durations and the subtle transitions often observed in untrimmed videos. These transitions, marked by ambiguous starting and ending points, complicate the segmentation process. Traditional models, such as MS-TCN, which depend on large receptive fields, frequently face challenges of over-segmentation (resulting in fragmented segments) or under-segmentation (merging distinct actions). Both of these issues negatively impact the quality of segmentation. To overcome these challenges, we present the Multi-Stage Boundary-Aware Transformer Network (MSBATN) with hierarchical sliding window attention, designed to enhance action segmentation. Our proposed approach incorporates a novel unified loss function that treats action classification and boundary detection as distinct yet interdependent tasks. Unlike traditional binary boundary detection methods, our boundary voting mechanism accurately identifies start and end points by leveraging contextual information. Extensive experiments using three challenging surgical datasets demonstrate the superior performance of the proposed method, achieving state-of-the-art results in F1 scores at thresholds of 25% and 50%, while also delivering comparable performance in other metrics.","authors":["Rezowan Shuvo","M S Mekala","Eyad Elyan"],"url":"https://arxiv.org/abs/2504.18756"}
{"created":"2025-04-29","title":"High-order Graph Neural Networks with Common Neighbor Awareness for Link Prediction","abstract":"Link prediction is a fundamental task in dynamic graph learning (DGL), inherently shaped by the topology of the DG. Recent advancements in dynamic graph neural networks (DGNN), primarily by modeling the relationships among nodes via a message passing scheme, have significantly improved link prediction performance. However, DGNNs heavily rely on the pairwise node interactions, which neglect the common neighbor interaction in DGL. To address this limitation, we propose a High-order Graph Neural Networks with Common Neighbor Awareness (HGNN-CNA) for link prediction with two-fold ideas: a) estimating correlation score by considering multi-hop common neighbors for capturing the complex interaction between nodes; b) fusing the correlation into the message-passing process to consider common neighbor interaction directly in DGL. Experimental results on three real DGs demonstrate that the proposed HGNN-CNA acquires a significant accuracy gain over several state-of-the-art models on the link prediction task.","authors":["Ling Wang","Minglian Han"],"url":"https://arxiv.org/abs/2504.18758"}
{"created":"2025-04-29","title":"Beyond Isolation: Towards an Interactionist Perspective on Human Cognitive Bias and AI Bias","abstract":"Isolated perspectives have often paved the way for great scientific discoveries. However, many breakthroughs only emerged when moving away from singular views towards interactions. Discussions on Artificial Intelligence (AI) typically treat human and AI bias as distinct challenges, leaving their dynamic interplay and compounding potential largely unexplored. Recent research suggests that biased AI can amplify human cognitive biases, while well-calibrated systems might help mitigate them. In this position paper, I advocate for transcending beyond separate treatment of human and AI biases and instead focus on their interaction effects. I argue that a comprehensive framework, one that maps (compound human-AI) biases to mitigation strategies, is essential for understanding and protecting human cognition, and I outline concrete steps for its development.","authors":["Nick von Felten"],"url":"https://arxiv.org/abs/2504.18759"}
{"created":"2025-04-29","title":"SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning","abstract":"Large Language Models (LLMs) are powerful but often require extensive fine-tuning and large datasets for specialized domains like law. General-purpose pre-training may not capture legal nuances, and acquiring sufficient legal data is challenging. We introduce SynLexLM, a novel approach to efficiently pre-train a legal LLM. Our method employs curriculum learning, progressing from simple to complex legal texts and queries, combined with synthetic data augmentation using models like Gemini Pro to address data scarcity. We aim to achieve improved performance on legal benchmarks (BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned versions. Preliminary work involves generating synthetic QA pairs reflecting legal reasoning. This work aims to enhance legal document analysis and research tools, potentially democratizing access to advanced legal AI.","authors":["Ojasw Upadhyay","Abishek Saravankumar","Ayman Ismail"],"url":"https://arxiv.org/abs/2504.18762"}
{"created":"2025-04-29","title":"A Vision for Auto Research with LLM Agents","abstract":"This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes.","authors":["Chengwei Liu","Chong Wang","Jiayue Cao","Jingquan Ge","Kun Wang","Lvye Zhang","Ming-Ming Cheng","Penghai Zhao","Tianlin Li","Xiaojun Jia","Xiang Li","Xinfeng Li","Yang Liu","Yebo Feng","Yihao Huang","Yijia Xu","Yuqiang Sun","Zhenhong Zhou","Zhengzi Xu"],"url":"https://arxiv.org/abs/2504.18765"}
{"created":"2025-04-29","title":"Dynamic Action Interpolation: A Universal Approach for Accelerating Reinforcement Learning with Expert Guidance","abstract":"Reinforcement learning (RL) suffers from severe sample inefficiency, especially during early training, requiring extensive environmental interactions to perform competently. Existing methods tend to solve this by incorporating prior knowledge, but introduce significant architectural and implementation complexity. We propose Dynamic Action Interpolation (DAI), a universal yet straightforward framework that interpolates expert and RL actions via a time-varying weight $\\alpha(t)$, integrating into any Actor-Critic algorithm with just a few lines of code and without auxiliary networks or additional losses. Our theoretical analysis shows that DAI reshapes state visitation distributions to accelerate value function learning while preserving convergence guarantees. Empirical evaluations across MuJoCo continuous control tasks demonstrate that DAI improves early-stage performance by over 160\\% on average and final performance by more than 50\\%, with the Humanoid task showing a 4$\\times$ improvement early on and a 2$\\times$ gain at convergence. These results challenge the assumption that complex architectural modifications are necessary for sample-efficient reinforcement learning.","authors":["Wenjun Cao"],"url":"https://arxiv.org/abs/2504.18766"}
{"created":"2025-04-29","title":"Minimum Cost Nowhere-zero Flows and Cut-balanced Orientations","abstract":"Flows and colorings are disparate concepts in graph algorithms -- the former is tractable while the latter is intractable. Tutte introduced the concept of nowhere-zero flows to unify these two concepts. Jaeger showed that nowhere-zero flows are equivalent to cut-balanced orientations. Motivated by connections between nowhere-zero flows, cut-balanced orientations, Nash-Williams' well-balanced orientations, and postman problems, we study optimization versions of nowhere-zero flows and cut-balanced orientations. Given a bidirected graph with asymmetric costs on two orientations of each edge, we study the min cost nowhere-zero $k$-flow problem and min cost $k$-cut-balanced orientation problem. We show that both problems are NP-hard to approximate within any finite factor. Given the strong inapproximability result, we design bicriteria approximations for both problems: we obtain a $(6,6)$-approximation to the min cost nowhere-zero $k$-flow and a $(k,6)$-approximation to the min cost $k$-cut-balanced orientation. For the case of symmetric costs (where the costs of both orientations are the same for every edge), we show that the nowhere-zero $k$-flow problem remains NP-hard and admits a $3$-approximation.","authors":["Karthekeyan Chandrasekaran","Siyue Liu","R. Ravi"],"url":"https://arxiv.org/abs/2504.18767"}
{"created":"2025-04-29","title":"TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians","abstract":"The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.","authors":["Letian Huang","Dongwei Ye","Jialin Dan","Chengzhi Tao","Huiwen Liu","Kun Zhou","Bo Ren","Yuanqi Li","Yanwen Guo","Jie Guo"],"url":"https://arxiv.org/abs/2504.18768"}
{"created":"2025-04-29","title":"PyViT-FUSE: A Foundation Model for Multi-Sensor Earth Observation Data","abstract":"We propose PyViT-FUSE, a foundation model for earth observation data explicitly designed to handle multi-modal imagery by learning to fuse an arbitrary number of mixed-resolution input bands into a single representation through an attention mechanism. The learned patch tokens are further processed by a stack of vision transformers with a novel pyramidal structure. We train the model on a globally sampled dataset in a self-supervised manner, leveraging core concepts of the SwAV algorithm. We show the interpretability of the fusion mechanism by visualization of the attention scores and the models applicability to downstream tasks.","authors":["Manuel Weber","Carly Beneke"],"url":"https://arxiv.org/abs/2504.18770"}
{"created":"2025-04-29","title":"Performance of Machine Learning Classifiers for Anomaly Detection in Cyber Security Applications","abstract":"This work empirically evaluates machine learning models on two imbalanced public datasets (KDDCUP99 and Credit Card Fraud 2013). The method includes data preparation, model training, and evaluation, using an 80/20 (train/test) split. Models tested include eXtreme Gradient Boosting (XGB), Multi Layer Perceptron (MLP), Generative Adversarial Network (GAN), Variational Autoencoder (VAE), and Multiple-Objective Generative Adversarial Active Learning (MO-GAAL), with XGB and MLP further combined with Random-Over-Sampling (ROS) and Self-Paced-Ensemble (SPE). Evaluation involves 5-fold cross-validation and imputation techniques (mean, median, and IterativeImputer) with 10, 20, 30, and 50 % missing data. Findings show XGB and MLP outperform generative models. IterativeImputer results are comparable to mean and median, but not recommended for large datasets due to increased complexity and execution time. The code used is publicly available on GitHub (github.com/markushaug/acr-25).","authors":["Markus Haug","Gissel Velarde"],"url":"https://arxiv.org/abs/2504.18771"}
{"created":"2025-04-29","title":"Depth as Points: Center Point-based Depth Estimation","abstract":"The perception of vehicles and pedestrians in urban scenarios is crucial for autonomous driving. This process typically involves complicated data collection, imposes high computational and hardware demands. To address these limitations, we first develop a highly efficient method for generating virtual datasets, which enables the creation of task- and scenario-specific datasets in a short time. Leveraging this method, we construct the virtual depth estimation dataset VirDepth, a large-scale, multi-task autonomous driving dataset. Subsequently, we propose CenterDepth, a lightweight architecture for monocular depth estimation that ensures high operational efficiency and exhibits superior performance in depth estimation tasks with highly imbalanced height-scale distributions. CenterDepth integrates global semantic information through the innovative Center FC-CRFs algorithm, aggregates multi-scale features based on object key points, and enables detection-based depth estimation of targets. Experiments demonstrate that our proposed method achieves superior performance in terms of both computational speed and prediction accuracy.","authors":["Zhiheng Tu","Xinjian Huang","Yong He","Ruiyang Zhou","Bo Du","Weitao Wu"],"url":"https://arxiv.org/abs/2504.18773"}
{"created":"2025-04-29","title":"ThinkFL: Self-Refining Failure Localization for Microservice Systems via Reinforcement Fine-Tuning","abstract":"As modern microservice systems grow increasingly popular and complex-often consisting of hundreds or even thousands of fine-grained, interdependent components-they are becoming more susceptible to frequent and subtle failures. Ensuring system reliability therefore hinges on accurate and efficient failure localization. Traditional failure localization approaches based on small models lack the flexibility to adapt to diverse failure scenarios, while recent LLM-based methods suffer from two major limitations: they often rely on rigid invocation workflows that constrain the model's ability to dynamically explore optimal localization paths, and they require resource-intensive inference, making them cost-prohibitive for real-world deployment. To address these challenges, we explore the use of reinforcement fine-tuning to equip lightweight LLMs with reasoning and self-refinement capabilities, significantly improving the cost-effectiveness and adaptability of LLM-based failure localization. We begin with an empirical study to identify three key capabilities essential for accurate localization. Building on these insights, we propose a progressive multi-stage GRPO fine-tuning framework, which integrates a multi-factor failure localization grader and a recursion-of-thought actor module. The resulting model, ThinkFL, not only outperforms existing state-of-the-art LLMs and baseline methods in localization accuracy but also reduces end-to-end localization latency from minutes to seconds, demonstrating strong potential for real-world applications.","authors":["Lingzhe Zhang","Yunpeng Zhai","Tong Jia","Chiming Duan","Siyu Yu","Jinyang Gao","Bolin Ding","Zhonghai Wu","Ying Li"],"url":"https://arxiv.org/abs/2504.18776"}
{"created":"2025-04-29","title":"Evaluating AI-Driven Automated Map Digitization in QGIS","abstract":"Map digitization is an important process that converts maps into digital formats that can be used for further analysis. This process typically requires a deep human involvement because of the need for interpretation and decision-making when translating complex features. With the advancement of artificial intelligence, there is an alternative to conducting map digitization with the help of machine learning techniques. Deepness, or Deep Neural Remote Sensing, is an advanced AI-driven tool designed and integrated as a plugin in QGIS application. This research focuses on assessing the effectiveness of Deepness in automated digitization. This study analyses AI-generated digitization results from Google Earth imagery and compares them with digitized outputs from OpenStreetMap (OSM) to evaluate performance.","authors":["Diana Febrita"],"url":"https://arxiv.org/abs/2504.18777"}
{"created":"2025-04-29","title":"Design, Contact Modeling, and Collision-inclusive Planning of a Dual-stiffness Aerial RoboT (DART)","abstract":"Collision-resilient quadrotors have gained significant attention given their potential for operating in cluttered environments and leveraging impacts to perform agile maneuvers. However, existing designs are typically single-mode: either safeguarded by propeller guards that prevent deformation or deformable but lacking rigidity, which is crucial for stable flight in open environments. This paper introduces DART, a Dual-stiffness Aerial RoboT, that adapts its post-collision response","authors":["Yogesh Kumar","Karishma Patnaik","Wenlong Zhang"],"url":"https://arxiv.org/abs/2504.18780"}
{"created":"2025-04-29","title":"IoT Botnet Detection: Application of Vision Transformer to Classification of Network Flow Traffic","abstract":"Despite the demonstrated effectiveness of transformer models in NLP, and image and video classification, the available tools for extracting features from captured IoT network flow packets fail to capture sequential patterns in addition to the absence of spatial patterns consequently limiting transformer model application. This work introduces a novel preprocessing method to adapt transformer models, the vision transformer (ViT) in particular, for IoT botnet attack detection using network flow packets. The approach involves feature extraction from .pcap files and transforming each instance into a 1-channel 2D image shape, enabling ViT-based classification. Also, the ViT model was enhanced to allow use any classifier besides Multilayer Perceptron (MLP) that was deployed in the initial ViT paper. Models including the conventional feed forward Deep Neural Network (DNN), LSTM and Bidirectional-LSTM (BLSTM) demonstrated competitive performance in terms of precision, recall, and F1-score for multiclass-based attack detection when evaluated on two IoT attack datasets.","authors":["Hassan Wasswa","Timothy Lynar","Aziida Nanyonga","Hussein Abbass"],"url":"https://arxiv.org/abs/2504.18781"}
{"created":"2025-04-29","title":"CAMeL: Cross-modality Adaptive Meta-Learning for Text-based Person Retrieval","abstract":"Text-based person retrieval aims to identify specific individuals within an image database using textual descriptions. Due to the high cost of annotation and privacy protection, researchers resort to synthesized data for the paradigm of pretraining and fine-tuning. However, these generated data often exhibit domain biases in both images and textual annotations, which largely compromise the scalability of the pre-trained model. Therefore, we introduce a domain-agnostic pretraining framework based on Cross-modality Adaptive Meta-Learning (CAMeL) to enhance the model generalization capability during pretraining to facilitate the subsequent downstream tasks. In particular, we develop a series of tasks that reflect the diversity and complexity of real-world scenarios, and introduce a dynamic error sample memory unit to memorize the history for errors encountered within multiple tasks. To further ensure multi-task adaptation, we also adopt an adaptive dual-speed update strategy, balancing fast adaptation to new tasks and slow weight updates for historical tasks. Albeit simple, our proposed model not only surpasses existing state-of-the-art methods on real-world benchmarks, including CUHK-PEDES, ICFG-PEDES, and RSTPReid, but also showcases robustness and scalability in handling biased synthetic images and noisy text annotations. Our code is available at https://github.com/Jahawn-Wen/CAMeL-reID.","authors":["Hang Yu","Jiahao Wen","Zhedong Zheng"],"url":"https://arxiv.org/abs/2504.18782"}
{"created":"2025-04-29","title":"Secret Breach Detection in Source Code with Large Language Models","abstract":"Background: Leaking sensitive information, such as API keys, tokens, and credentials, in source code remains a persistent security threat. Traditional regex and entropy-based tools often generate high false positives due to limited contextual understanding. Aims: This work aims to enhance secret detection in source code using large language models (LLMs), reducing false positives while maintaining high recall. We also evaluate the feasibility of using fine-tuned, smaller models for local deployment. Method: We propose a hybrid approach combining regex-based candidate extraction with LLM-based classification. We evaluate pre-trained and fine-tuned variants of various Large Language Models on a benchmark dataset from 818 GitHub repositories. Various prompting strategies and efficient fine-tuning methods are employed for both binary and multiclass classification. Results: The fine-tuned LLaMA-3.1 8B model achieved an F1-score of 0.9852 in binary classification, outperforming regex-only baselines. For multiclass classification, Mistral-7B reached 0.982 accuracy. Fine-tuning significantly improved performance across all models. Conclusions: Fine-tuned LLMs offer an effective and scalable solution for secret detection, greatly reducing false positives. Open-source models provide a practical alternative to commercial APIs, enabling secure and cost-efficient deployment in development workflows.","authors":["Md Nafiu Rahman","Sadif Ahmed","Zahin Wahab","S M Sohan","Rifat Shahriyar"],"url":"https://arxiv.org/abs/2504.18784"}
{"created":"2025-04-29","title":"ALF: Advertiser Large Foundation Model for Multi-Modal Advertiser Understanding","abstract":"We present ALF (Advertiser Large Foundation model), a multi-modal transformer architecture for understanding advertiser behavior and intent across text, image, video and structured data modalities. Through contrastive learning and multi-task optimization, ALF creates unified advertiser representations that capture both content and behavioral patterns. Our model achieves state-of-the-art performance on critical tasks including fraud detection, policy violation identification, and advertiser similarity matching. In production deployment, ALF reduces false positives by 90% while maintaining 99.8% precision on abuse detection tasks. The architecture's effectiveness stems from its novel combination of multi-modal transformations, inter-sample attention mechanism, spectrally normalized projections, and calibrated probabilistic outputs.","authors":["Santosh Rajagopalan","Jonathan Vronsky","Songbai Yan","S. Alireza Golestaneh","Shubhra Chandra","Min Zhou"],"url":"https://arxiv.org/abs/2504.18785"}
{"created":"2025-04-29","title":"Contracts: A unified lens on congestion control robustness, fairness, congestion, and generality","abstract":"Congestion control algorithms (CCAs) operate in partially observable environments, lacking direct visibility into link capacities, or competing flows. To ensure fair sharing of network resources, CCAs communicate their fair share through observable signals. For instance, Reno's fair share is encoded as $\\propto 1/\\sqrt{\\texttt{loss rate}}$. We call such communication mechanisms \\emph{contracts}. We show that the design choice of contracts fixes key steady-state performance metrics, including robustness to errors in congestion signals, fairness, amount of congestion (e.g., delay, loss), and generality (e.g., range of supported link rates). This results in fundamental tradeoffs between these metrics. We also discover some properties of contracts that describe CCA design pitfalls that can lead to starvation (extreme unfairness). We empirically validate our findings and discuss their implications on CCA design and network measurement.","authors":["Anup Agarwal","Venkat Arun","Srinivasan Seshan"],"url":"https://arxiv.org/abs/2504.18786"}
{"created":"2025-04-29","title":"Coherence-based Approximate Derivatives via Web of Affine Spaces Optimization","abstract":"Computing derivatives is a crucial subroutine in computer science and related fields as it provides a local characterization of a function's steepest directions of ascent or descent. In this work, we recognize that derivatives are often not computed in isolation; conversely, it is quite common to compute a \\textit{sequence} of derivatives, each one somewhat related to the last. Thus, we propose accelerating derivative computation by reusing information from previous, related calculations-a general strategy known as \\textit{coherence}. We introduce the first instantiation of this strategy through a novel approach called the Web of Affine Spaces (WASP) Optimization. This approach provides an accurate approximation of a function's derivative object (i.e. gradient, Jacobian matrix, etc.) at the current input within a sequence. Each derivative within the sequence only requires a small number of forward passes through the function (typically two), regardless of the number of function inputs and outputs. We demonstrate the efficacy of our approach through several numerical experiments, comparing it with alternative derivative computation methods on benchmark functions. We show that our method significantly improves the performance of derivative computation on small to medium-sized functions, i.e., functions with approximately fewer than 500 combined inputs and outputs. Furthermore, we show that this method can be effectively applied in a robotics optimization context. We conclude with a discussion of the limitations and implications of our work. Open-source code, visual explanations, and videos are located at the paper website: \\href{https://apollo-lab-yale.github.io/25-RSS-WASP-website/}{https://apollo-lab-yale.github.io/25-RSS-WASP-website/}.","authors":["Daniel Rakita","Chen Liang","Qian Wang"],"url":"https://arxiv.org/abs/2504.18790"}
{"created":"2025-04-29","title":"Nonconvex Linear System Identification with Minimal State Representation","abstract":"Low-order linear System IDentification (SysID) addresses the challenge of estimating the parameters of a linear dynamical system from finite samples of observations and control inputs with minimal state representation. Traditional approaches often utilize Hankel-rank minimization, which relies on convex relaxations that can require numerous, costly singular value decompositions (SVDs) to optimize. In this work, we propose two nonconvex reformulations to tackle low-order SysID (i) Burer-Monterio (BM) factorization of the Hankel matrix for efficient nuclear norm minimization, and (ii) optimizing directly over system parameters for real, diagonalizable systems with an atomic norm style decomposition. These reformulations circumvent the need for repeated heavy SVD computations, significantly improving computational efficiency. Moreover, we prove that optimizing directly over the system parameters yields lower statistical error rates, and lower sample complexities that do not scale linearly with trajectory length like in Hankel-nuclear norm minimization. Additionally, while our proposed formulations are nonconvex, we provide theoretical guarantees of achieving global optimality in polynomial time. Finally, we demonstrate algorithms that solve these nonconvex programs and validate our theoretical claims on synthetic data.","authors":["Uday Kiran Reddy Tadipatri","Benjamin D. Haeffele","Joshua Agterberg","Ingvar Ziemann","Ren\\'e Vidal"],"url":"https://arxiv.org/abs/2504.18791"}
{"created":"2025-04-29","title":"STDArm: Transferring Visuomotor Policies From Static Data Training to Dynamic Robot Manipulation","abstract":"Recent advances in mobile robotic platforms like quadruped robots and drones have spurred a demand for deploying visuomotor policies in increasingly dynamic environments. However, the collection of high-quality training data, the impact of platform motion and processing delays, and limited onboard computing resources pose significant barriers to existing solutions. In this work, we present STDArm, a system that directly transfers policies trained under static conditions to dynamic platforms without extensive modifications.","authors":["Yifan Duan","Heng Li","Yilong Wu","Wenhao Yu","Xinran Zhang","Yedong Shen","Jianmin Ji","Yanyong Zhang"],"url":"https://arxiv.org/abs/2504.18792"}
{"created":"2025-04-29","title":"Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations","abstract":"The rapid adoption of AI-powered applications demands high-performance, scalable, and efficient cloud database solutions, as traditional architectures often struggle with AI-driven workloads requiring real-time data access, vector search, and low-latency queries. This paper explores how cloud-native databases enable AI-driven applications by leveraging purpose-built technologies such as vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores (Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and PostgreSQL). It presents architectural patterns for integrating AI workloads with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with LLMs, real-time data pipelines, AI-driven query optimization, and embeddings-based search. Performance benchmarks, scalability considerations, and cost-efficient strategies are evaluated to guide the design of AI-enabled applications. Real-world case studies from industries such as healthcare, finance, and customer experience illustrate how enterprises utilize cloud databases to enhance AI capabilities while ensuring security, governance, and compliance with enterprise and regulatory standards. By providing a comprehensive analysis of AI and cloud database integration, this paper serves as a practical guide for researchers, architects, and enterprises to build next-generation AI applications that optimize performance, scalability, and cost efficiency in cloud environments.","authors":["Santosh Bhupathi"],"url":"https://arxiv.org/abs/2504.18793"}
{"created":"2025-04-29","title":"Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots","abstract":"Hierarchical reinforcement learning (HRL) is hypothesized to be able to take advantage of the inherent hierarchy in robot learning tasks with sparse reward schemes, in contrast to more traditional reinforcement learning algorithms. In this research, hierarchical reinforcement learning is evaluated and contrasted with standard reinforcement learning in complex navigation tasks. We evaluate unique characteristics of HRL, including their ability to create sub-goals and the termination function. We constructed experiments to test the differences between PPO and HRL, different ways of creating sub-goals, manual vs automatic sub-goal creation, and the effects of the frequency of termination on performance. These experiments highlight the advantages of HRL and how it achieves these advantages.","authors":["Brendon Johnson","Alfredo Weitzenfeld"],"url":"https://arxiv.org/abs/2504.18794"}
{"created":"2025-04-29","title":"A Survey on Multimodal Music Emotion Recognition","abstract":"Multimodal music emotion recognition (MMER) is an emerging discipline in music information retrieval that has experienced a surge in interest in recent years. This survey provides a comprehensive overview of the current state-of-the-art in MMER. Discussing the different approaches and techniques used in this field, the paper introduces a four-stage MMER framework, including multimodal data selection, feature extraction, feature processing, and final emotion prediction. The survey further reveals significant advancements in deep learning methods and the increasing importance of feature fusion techniques. Despite these advancements, challenges such as the need for large annotated datasets, datasets with more modalities, and real-time processing capabilities remain. This paper also contributes to the field by identifying critical gaps in current research and suggesting potential directions for future research. The gaps underscore the importance of developing robust, scalable, a interpretable models for MMER, with implications for applications in music recommendation systems, therapeutic tools, and entertainment.","authors":["Rashini Liyanarachchi","Aditya Joshi","Erik Meijering"],"url":"https://arxiv.org/abs/2504.18799"}
{"created":"2025-04-29","title":"Video CLIP Model for Multi-View Echocardiography Interpretation","abstract":"Echocardiography involves recording videos of the heart using ultrasound, enabling clinicians to evaluate its condition. Recent advances in large-scale vision-language models (VLMs) have garnered attention for automating the interpretation of echocardiographic videos. However, most existing VLMs proposed for medical interpretation thus far rely on single-frame (i.e., image) inputs. Consequently, these image-based models often exhibit lower diagnostic accuracy for conditions identifiable through cardiac motion. Moreover, echocardiographic videos are recorded from various views that depend on the direction of ultrasound emission, and certain views are more suitable than others for interpreting specific conditions. Incorporating multiple views could potentially yield further improvements in accuracy. In this study, we developed a video-language model that takes five different views and full video sequences as input, training it on pairs of echocardiographic videos and clinical reports from 60,747 cases. Our experiments demonstrate that this expanded approach achieves higher interpretation accuracy than models trained with only single-view videos or with still images.","authors":["Ryo Takizawa","Satoshi Kodera","Tempei Kabayama","Ryo Matsuoka","Yuta Ando","Yuto Nakamura","Haruki Settai","Norihiko Takeda"],"url":"https://arxiv.org/abs/2504.18800"}
{"created":"2025-04-29","title":"Can We Enhance Bug Report Quality Using LLMs?: An Empirical Study of LLM-Based Bug Report Generation","abstract":"Bug reports contain the information developers need to triage and fix software bugs. However, unclear, incomplete, or ambiguous information may lead to delays and excessive manual effort spent on bug triage and resolution. In this paper, we explore whether Instruction fine-tuned Large Language Models (LLMs) can automatically transform casual, unstructured bug reports into high-quality, structured bug reports adhering to a standard template. We evaluate three open-source instruction-tuned LLMs (\\emph{Qwen 2.5, Mistral, and Llama 3.2}) against ChatGPT-4o, measuring performance on established metrics such as CTQRS, ROUGE, METEOR, and SBERT. Our experiments show that fine-tuned Qwen 2.5 achieves a CTQRS score of \\textbf{77%}, outperforming both fine-tuned Mistral (\\textbf{71%}), Llama 3.2 (\\textbf{63%}) and ChatGPT in 3-shot learning (\\textbf{75%}). Further analysis reveals that Llama 3.2 shows higher accuracy of detecting missing fields particularly Expected Behavior and Actual Behavior, while Qwen 2.5 demonstrates superior performance in capturing Steps-to-Reproduce, with an F1 score of 76%. Additional testing of the models on other popular projects (e.g., Eclipse, GCC) demonstrates that our approach generalizes well, achieving up to \\textbf{70%} CTQRS in unseen projects' bug reports. These findings highlight the potential of instruction fine-tuning in automating structured bug report generation, reducing manual effort for developers and streamlining the software maintenance process.","authors":["Jagrit Acharya","Gouri Ginde"],"url":"https://arxiv.org/abs/2504.18804"}
{"created":"2025-04-29","title":"Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation","abstract":"Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.","authors":["Jong Inn Park","Maanas Taneja","Qianwen Wang","Dongyeop Kang"],"url":"https://arxiv.org/abs/2504.18805"}
{"created":"2025-04-29","title":"BugsRepo: A Comprehensive Curated Dataset of Bug Reports, Comments and Contributors Information from Bugzilla","abstract":"Bug reports help software development teams enhance software quality, yet their utility is often compromised by unclear or incomplete information. This issue not only hinders developers' ability to quickly understand and resolve bugs but also poses significant challenges for various software maintenance prediction systems, such as bug triaging, severity prediction, and bug report summarization. To address this issue, we introduce \\textnormal{{\\fontfamily{ppl}\\selectfont BugsRepo}}, a multifaceted dataset derived from Mozilla projects that offers three key components to support a wide range of software maintenance tasks. First, it includes a Bug report meta-data & Comments dataset with detailed records for 119,585 fixed or closed and resolved bug reports, capturing fields like severity, creation time, status, and resolution to provide rich contextual insights. Second, {\\fontfamily{ppl}\\selectfont BugsRepo} features a contributor information dataset comprising 19,351 Mozilla community members, enriched with metadata on user roles, activity history, and contribution metrics such as the number of bugs filed, comments made, and patches reviewed, thus offering valuable information for tasks like developer recommendation. Lastly, the dataset provides a structured bug report subset of 10,351 well-structured bug reports, complete with steps to reproduce, actual behavior, and expected behavior. After this initial filter, a secondary filtering layer is applied using the CTQRS scale. By integrating static metadata, contributor statistics, and detailed comment threads, {\\fontfamily{ppl}\\selectfont BugsRepo} presents a holistic view of each bug's history, supporting advancements in automated bug report analysis, which can enhance the efficiency and effectiveness of software maintenance processes.","authors":["Jagrit Acharya","Gouri Ginde"],"url":"https://arxiv.org/abs/2504.18806"}
{"created":"2025-04-29","title":"Clones in the Machine: A Feminist Critique of Agency in Digital Cloning","abstract":"This paper critiques digital cloning in academic research, highlighting how it exemplifies AI solutionism. Digital clones, which replicate user data to simulate behavior, are often seen as scalable tools for behavioral insights. However, this framing obscures ethical concerns around consent, agency, and representation. Drawing on feminist theories of agency, the paper argues that digital cloning oversimplifies human complexity and risks perpetuating systemic biases. To address these issues, it proposes decentralized data repositories and dynamic consent models, promoting ethical, context-aware AI practices that challenge the reductionist logic of AI solutionism","authors":["Si\\^an Brooke"],"url":"https://arxiv.org/abs/2504.18807"}
{"created":"2025-04-29","title":"Audio-Driven Talking Face Video Generation with Joint Uncertainty Learning","abstract":"Talking face video generation with arbitrary speech audio is a significant challenge within the realm of digital human technology. The previous studies have emphasized the significance of audio-lip synchronization and visual quality. Currently, limited attention has been given to the learning of visual uncertainty, which creates several issues in existing systems, including inconsistent visual quality and unreliable performance across different input conditions. To address the problem, we propose a Joint Uncertainty Learning Network (JULNet) for high-quality talking face video generation, which incorporates a representation of uncertainty that is directly related to visual error. Specifically, we first design an uncertainty module to individually predict the error map and uncertainty map after obtaining the generated image. The error map represents the difference between the generated image and the ground truth image, while the uncertainty map is used to predict the probability of incorrect estimates. Furthermore, to match the uncertainty distribution with the error distribution through a KL divergence term, we introduce a histogram technique to approximate the distributions. By jointly optimizing error and uncertainty, the performance and robustness of our model can be enhanced. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking face video generation compared to previous methods.","authors":["Yifan Xie","Fei Ma","Yi Bin","Ying He","Fei Yu"],"url":"https://arxiv.org/abs/2504.18810"}
{"created":"2025-04-29","title":"SynFuzz: Leveraging Fuzzing of Netlist to Detect Synthesis Bugs","abstract":"In the evolving landscape of integrated circuit (IC) design, the increasing complexity of modern processors and intellectual property (IP) cores has introduced new challenges in ensuring design correctness and security. The recent advancements in hardware fuzzing techniques have shown their efficacy in detecting hardware bugs and vulnerabilities at the RTL abstraction level of hardware. However, they suffer from several limitations, including an inability to address vulnerabilities introduced during synthesis and gate-level transformations. These methods often fail to detect issues arising from library adversaries, where compromised or malicious library components can introduce backdoors or unintended behaviors into the design. In this paper, we present a novel hardware fuzzer, SynFuzz, designed to overcome the limitations of existing hardware fuzzing frameworks. SynFuzz focuses on fuzzing hardware at the gate-level netlist to identify synthesis bugs and vulnerabilities that arise during the transition from RTL to the gate-level. We analyze the intrinsic hardware behaviors using coverage metrics specifically tailored for the gate-level. Furthermore, SynFuzz implements differential fuzzing to uncover bugs associated with EDA libraries. We evaluated SynFuzz on popular open-source processors and IP designs, successfully identifying 7 new synthesis bugs. Additionally, by exploiting the optimization settings of EDA tools, we performed a compromised library mapping attack (CLiMA), creating a malicious version of hardware designs that remains undetectable by traditional verification methods. We also demonstrate how SynFuzz overcomes the limitations of the industry-standard formal verification tool, Cadence Conformal, providing a more robust and comprehensive approach to hardware verification.","authors":["Raghul Saravanan","Sudipta Paria","Aritra Dasgupta","Venkat Nitin Patnala","Swarup Bhunia","Sai Manoj P D"],"url":"https://arxiv.org/abs/2504.18812"}
{"created":"2025-04-29","title":"Automated Routing-Informed Placement for Large-Scale Photonic Integrated Circuits","abstract":"As technology advances, photonic integrated circuits (PICs) are rapidly scaling in size and complexity, with modern designs integrating thousands of components. However, the analog custom layout nature of photonics, the curvy waveguide structures, and single-layer routing resources impose stringent physical constraints, such as minimum bend radii and waveguide crossing penalties, which make manual layout the de facto standard. This manual process takes weeks to complete and is error-prone, which is fundamentally unscalable for large-scale PIC systems. Existing automation solutions have adopted force-directed placement on small benchmarks with tens of components, with limited routability and scalability. To fill this fundamental gap in the electronic-photonic design automation (EPDA) toolchain, we present the first GPU-accelerated, routing-informed placement framework. It features an asymmetric bending-aware wirelength function with explicit modeling of waveguide routing congestion and crossings for routability maximization. Meanwhile, conditional projection is employed to gradually enforce a variety of user-defined layout constraints, including alignment, spacing, etc. This constrained optimization is accelerated and stabilized by a custom blockwise adaptive Nesterov-accelerated optimizer, ensuring stable and high-quality convergence. Compared to existing methods, our method can generate high-quality layouts for large-scale PICs with an average routing success rate of 94.79% across all benchmarks within minutes. By tightly coupling placement with physical-aware routing, our method establishes a new paradigm for automated PIC design, bringing intelligent, scalable layout synthesis to the forefront of next-generation EPDA. We will open-source our code.","authors":["Hongjian Zhou","Haoyu Yang","Gangi Nicholas","Haoxing Ren","Huang Rena","Jiaqi Gu"],"url":"https://arxiv.org/abs/2504.18813"}
{"created":"2025-04-29","title":"Zero-Day Botnet Attack Detection in IoV: A Modular Approach Using Isolation Forests and Particle Swarm Optimization","abstract":"The Internet of Vehicles (IoV) is transforming transportation by enhancing connectivity and enabling autonomous driving. However, this increased interconnectivity introduces new security vulnerabilities. Bot malware and cyberattacks pose significant risks to Connected and Autonomous Vehicles (CAVs), as demonstrated by real-world incidents involving remote vehicle system compromise. To address these challenges, we propose an edge-based Intrusion Detection System (IDS) that monitors network traffic to and from CAVs. Our detection model is based on a meta-ensemble classifier capable of recognizing known (Nday) attacks and detecting previously unseen (zero-day) attacks. The approach involves training multiple Isolation Forest (IF) models on Multi-access Edge Computing (MEC) servers, with each IF specialized in identifying a specific type of botnet attack. These IFs, either trained locally or shared by other MEC nodes, are then aggregated using a Particle Swarm Optimization (PSO) based stacking strategy to construct a robust meta-classifier. The proposed IDS has been evaluated on a vehicular botnet dataset, achieving an average detection rate of 92.80% for N-day attacks and 77.32% for zero-day attacks. These results highlight the effectiveness of our solution in detecting both known and emerging threats, providing a scalable and adaptive defense mechanism for CAVs within the IoV ecosystem.","authors":["Abdelaziz Amara korba","Nour Elislem Karabadji","Yacine Ghamri-Doudane"],"url":"https://arxiv.org/abs/2504.18814"}
{"created":"2025-04-29","title":"Understanding Decentralized Social Feed Curation on Mastodon","abstract":"As centralized social media platforms face growing concerns, more users are seeking greater control over their social feeds and turning to decentralized alternatives such as Mastodon. The decentralized nature of Mastodon creates unique opportunities for customizing feeds, yet user perceptions and curation strategies on these platforms remain unknown. This paper presents findings from a two-part interview study with 21 Mastodon users, exploring how they perceive, interact with, and manage their current feeds, and how we can better empower users to personalize their feeds on Mastodon. We use the qualitative findings of the first part of the study to guide the creation of Braids, a web-based prototype for feed curation. Results from the second part of our study, using Braids, highlighted opportunities and challenges for future research, particularly in using seamful design to enhance people's acceptance of algorithmic curation and nuanced trade-offs between machine learning-based and rule-based curation algorithms. To optimize user experience, we also discuss the tension between creating new apps and building add-ons in the decentralized social media realm.","authors":["Yuhan Liu","Emmy Song","Owen Xingjian Zhang","Jewel Merriman","Lei Zhang","Andr\\'es Monroy-Hern\\'andez"],"url":"https://arxiv.org/abs/2504.18817"}
{"created":"2025-04-29","title":"Frequency-Integrated Transformer for Arbitrary-Scale Super-Resolution","abstract":"Methods based on implicit neural representation have demonstrated remarkable capabilities in arbitrary-scale super-resolution (ASSR) tasks, but they neglect the potential value of the frequency domain, leading to sub-optimal performance. We proposes a novel network called Frequency-Integrated Transformer (FIT) to incorporate and utilize frequency information to enhance ASSR performance. FIT employs Frequency Incorporation Module (FIM) to introduce frequency information in a lossless manner and Frequency Utilization Self-Attention module (FUSAM) to efficiently leverage frequency information by exploiting spatial-frequency interrelationship and global nature of frequency. FIM enriches detail characterization by incorporating frequency information through a combination of Fast Fourier Transform (FFT) with real-imaginary mapping. In FUSAM, Interaction Implicit Self-Attention (IISA) achieves cross-domain information synergy by interacting spatial and frequency information in subspace, while Frequency Correlation Self-attention (FCSA) captures the global context by computing correlation in frequency. Experimental results demonstrate FIT yields superior performance compared to existing methods across multiple benchmark datasets. Visual feature map proves the superiority of FIM in enriching detail characterization. Frequency error map validates IISA productively improve the frequency fidelity. Local attribution map validates FCSA effectively captures global context.","authors":["Xufei Wang","Fei Ge","Jinchen Zhu","Mingjian Zhang","Qi Wu","Jifeng Ren Shizhuang Weng"],"url":"https://arxiv.org/abs/2504.18818"}
{"created":"2025-04-29","title":"Preserving Seasonal and Trend Information: A Variational Autoencoder-Latent Space Arithmetic Based Approach for Non-stationary Learning","abstract":"AI models have garnered significant research attention towards predictive task automation. However, a stationary training environment is an underlying assumption for most models and such models simply do not work on non-stationary data since a stationary relationship is learned. The existing solutions propose making data stationary prior to model training and evaluation. This leads to loss of trend and seasonal patterns which are vital components for learning temporal dependencies of the system under study. This research aims to address this limitation by proposing a method for enforcing stationary behaviour within the latent space while preserving trend and seasonal information. The method deploys techniques including Differencing, Time-series decomposition, and Latent Space Arithmetic (LSA), to learn information vital for efficient approximation of trend and seasonal information which is then stored as embeddings within the latent space of a Variational Autoencoder (VAE). The approach's ability to preserve trend and seasonal information was evaluated on two time-series non-stationary datasets. For predictive performance evaluation, four deep learning models were trained on the latent vector representations of the datasets after application of the proposed method and all models produced competitive results in comparison with state-of-the-art techniques using RMSE as the performance metric.","authors":["Hassan Wasswa","Aziida Nanyonga","Timothy Lynar"],"url":"https://arxiv.org/abs/2504.18819"}
{"created":"2025-04-29","title":"Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning","abstract":"In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.","authors":["Teeradaj Racharak","Chaiyong Ragkhitwetsagul","Chommakorn Sontesadisai","Thanwadee Sunetnanta"],"url":"https://arxiv.org/abs/2504.18827"}
{"created":"2025-04-29","title":"Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy","abstract":"Generalizable dexterous grasping with suitable grasp types is a fundamental skill for intelligent robots. Developing such skills requires a large-scale and high-quality dataset that covers numerous grasp types (i.e., at least those categorized by the GRASP taxonomy), but collecting such data is extremely challenging. Existing automatic grasp synthesis methods are often limited to specific grasp types or object categories, hindering scalability. This work proposes an efficient pipeline capable of synthesizing contact-rich, penetration-free, and physically plausible grasps for any grasp type, object, and articulated hand. Starting from a single human-annotated template for each hand and grasp type, our pipeline tackles the complicated synthesis problem with two stages: optimize the object to fit the hand template first, and then locally refine the hand to fit the object in simulation. To validate the synthesized grasps, we introduce a contact-aware control strategy that allows the hand to apply the appropriate force at each contact point to the object. Those validated grasps can also be used as new grasp templates to facilitate future synthesis. Experiments show that our method significantly outperforms previous type-unaware grasp synthesis baselines in simulation. Using our algorithm, we construct a dataset containing 10.7k objects and 9.5M grasps, covering 31 grasp types in the GRASP taxonomy. Finally, we train a type-conditional generative model that successfully performs the desired grasp type from single-view object point clouds, achieving an 82.3% success rate in real-world experiments. Project page: https://pku-epic.github.io/Dexonomy.","authors":["Jiayi Chen","Yubin Ke","Lin Peng","He Wang"],"url":"https://arxiv.org/abs/2504.18829"}
{"created":"2025-04-29","title":"Aerial Robots Persistent Monitoring and Target Detection: Deployment and Assessment in the Field","abstract":"In this manuscript, we present a distributed algorithm for multi-robot persistent monitoring and target detection. In particular, we propose a novel solution that effectively integrates the Time-inverted Kuramoto model, three-dimensional Lissajous curves, and Model Predictive Control. We focus on the implementation of this algorithm on aerial robots, addressing the practical challenges involved in deploying our approach under real-world conditions. Our method ensures an effective and robust solution that maintains operational efficiency even in the presence of what we define as type I and type II failures. Type I failures refer to short-time disruptions, such as tracking errors and communication delays, while type II failures account for long-time disruptions, including malicious attacks, severe communication failures, and battery depletion. Our approach guarantees persistent monitoring and target detection despite these challenges. Furthermore, we validate our method with extensive field experiments involving up to eleven aerial robots, demonstrating the effectiveness, resilience, and scalability of our solution.","authors":["Manuel Boldrer","Vit Kratky","Martin Saska"],"url":"https://arxiv.org/abs/2504.18832"}
{"created":"2025-04-29","title":"Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events","abstract":"Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.","authors":["Pouya Shaeri","Yasaman Mohammadpour","Alimohammad Beigi","Ariane Middel","Huan Liu"],"url":"https://arxiv.org/abs/2504.18837"}
{"created":"2025-04-29","title":"Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks","abstract":"Large Language Models (LLMs) are advancing at an amazing speed and have become indispensable across academia, industry, and daily applications. To keep pace with the status quo, this survey probes the core challenges that the rise of LLMs poses for evaluation. We identify and analyze two pivotal transitions: (i) from task-specific to capability-based evaluation, which reorganizes benchmarks around core competencies such as knowledge, reasoning, instruction following, multi-modal understanding, and safety; and (ii) from manual to automated evaluation, encompassing dynamic dataset curation and \"LLM-as-a-judge\" scoring.","authors":["Yixin Cao","Shibo Hong","Xinze Li","Jiahao Ying","Yubo Ma","Haiyuan Liang","Yantao Liu","Zijun Yao","Xiaozhi Wang","Dan Huang","Wenxuan Zhang","Lifu Huang","Muhao Chen","Lei Hou","Qianru Sun","Xingjun Ma","Zuxuan Wu","Min-Yen Kan","David Lo","Qi Zhang","Heng Ji","Jing Jiang","Juanzi Li","Aixin Sun","Xuanjing Huang","Tat-Seng Chua","Yu-Gang Jiang"],"url":"https://arxiv.org/abs/2504.18838"}
{"created":"2025-04-29","title":"Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning","abstract":"Large language models (LLMs) are rapidly changing various domains. However, their capabilities in handling conversational breakdowns still require an in-depth exploration. This paper addresses the challenge of detecting and mitigating dialogue breakdowns within LLM-driven conversational systems. While powerful models from OpenAI and Anthropic excel in many dialogue tasks, they can still produce incoherent or contradictory responses, commonly referred to as breakdowns, which undermine user trust. To tackle this, we propose an approach that combines specialized fine-tuning with advanced prompting strategies, including few-shot learning, chain-of-thought reasoning, and analogical prompting. In particular, we fine-tune a small 8B model and demonstrate its robust classification and calibration capabilities in English and Japanese dialogue. We also validate its generalization on the BETOLD dataset, achieving a 7\\% accuracy improvement over its base model. Furthermore, we introduce a real-time deployment architecture that selectively escalates suspicious responses to more resource-intensive frontier models only when breakdowns are detected, significantly cutting operational expenses and energy consumption. Experimental results show our method surpasses prior state-of-the-art specialized classifiers while also narrowing performance gaps between smaller open-source models and large proprietary ones. Our approach offers a scalable solution for robust conversational AI in high-impact domains by combining efficiency, interpretability, and reliability.","authors":["Abdellah Ghassel","Xianzhi Li","Xiaodan Zhu"],"url":"https://arxiv.org/abs/2504.18839"}
{"created":"2025-04-29","title":"Swarming in the Wild: A Distributed Communication-less Lloyd-based Algorithm dealing with Uncertainties","abstract":"In this work, we present a distributed algorithm for swarming in complex environments that operates with no communication, no a priori information about the environment, and using only onboard sensing and computation capabilities. We provide sufficient conditions to guarantee that each robot reaches its goal region in a finite time, avoiding collisions with obstacles and other robots without exceeding a desired maximum distance from a predefined set of neighbors (flocking constraint). In addition, we show how the proposed algorithm can deal with tracking errors and onboard sensing errors without violating safety and proximity constraints, still providing the conditions for having convergence towards the goal region. To validate the approach, we provide experiments in the field. We tested our algorithm in GNSS-denied environments i.e., a dense forest, where fully autonomous aerial robots swarmed safely to the desired destinations, by relying only on onboard sensors, i.e., without a communication network. This work marks the initial deployment of a fully distributed system where there is no communication between the robots, nor reliance on any global localization system, which at the same time it ensures safety and convergence towards the goal within such complex environments.","authors":["Manuel Boldrer","Vit Kratky","Viktor Walter","Martin Saska"],"url":"https://arxiv.org/abs/2504.18840"}
{"created":"2025-04-29","title":"A Microgravity Simulation Experimental Platform For Small Space Robots In Orbit","abstract":"This study describes the development and validation of a novel microgravity experimental platform that is mainly applied to small robots such as modular self-reconfigurable robots. This platform mainly consists of an air supply system, a microporous platform and glass. By supplying air to the microporous platform to form an air film, the influence of the weight of the air foot and the ventilation hose of traditional air-float platforms on microgravity experiments is solved. The contribution of this work is to provide a platform with less external interference for microgravity simulation experiments on small robots.","authors":["Hang Luo","Nanlin Zhou","Haoxiang Zhang","Kai Han","Ning Zhao","Zhiyuan Yang","Jian Qi","Sikai Zhao","Jie Zhao","Yanhe Zhu"],"url":"https://arxiv.org/abs/2504.18842"}
{"created":"2025-04-29","title":"A Group Theoretic Construction of Batch Codes","abstract":"Batch codes serve as critical tools for load balancing in distributed storage systems. While numerous constructions exist for specific batch sizes t, current methodologies predominantly rely on code dimension parameters, limiting their adaptability. Practical implementations, however, demand versatile batch code designs capable of accommodating arbitrary batch sizes-a challenge that remains understudied in the literature. This paper introduces a novel framework for constructing batch codes through finite groups and their subgroup structures, building on the quasi-uniform group code framework proposed by Chan et al. By leveraging algebraic properties of groups, the proposed method enables systematic code construction, streamlined decoding procedures, and efficient reconstruction of information symbols. Unlike traditional linear codes, quasi-uniform codes exhibit broader applicability due to their inherent structural flexibility.","authors":["Eldho K. Thomas"],"url":"https://arxiv.org/abs/2504.18844"}
{"created":"2025-04-29","title":"Introducing Interval Neural Networks for Uncertainty-Aware System Identification","abstract":"System Identification (SysID) is crucial for modeling and understanding dynamical systems using experimental data. While traditional SysID methods emphasize linear models, their inability to fully capture nonlinear dynamics has driven the adoption of Deep Learning (DL) as a more powerful alternative. However, the lack of uncertainty quantification (UQ) in DL-based models poses challenges for reliability and safety, highlighting the necessity of incorporating UQ. This paper introduces a systematic framework for constructing and learning Interval Neural Networks (INNs) to perform UQ in SysID tasks. INNs are derived by transforming the learnable parameters (LPs) of pre-trained neural networks into interval-valued LPs without relying on probabilistic assumptions. By employing interval arithmetic throughout the network, INNs can generate Prediction Intervals (PIs) that capture target coverage effectively. We extend Long Short-Term Memory (LSTM) and Neural Ordinary Differential Equations (Neural ODEs) into Interval LSTM (ILSTM) and Interval NODE (INODE) architectures, providing the mathematical foundations for their application in SysID. To train INNs, we propose a DL framework that integrates a UQ loss function and parameterization tricks to handle constraints arising from interval LPs. We introduce novel concept \"elasticity\" for underlying uncertainty causes and validate ILSTM and INODE in SysID experiments, demonstrating their effectiveness.","authors":["Mehmet Ali Ferah","Tufan Kumbasar"],"url":"https://arxiv.org/abs/2504.18845"}
{"created":"2025-04-29","title":"Imitation Learning for Autonomous Driving: Insights from Real-World Testing","abstract":"This work focuses on the design of a deep learning-based autonomous driving system deployed and tested on the real-world MIT Racecar to assess its effectiveness in driving scenarios. The Deep Neural Network (DNN) translates raw image inputs into real-time steering commands in an end-to-end learning fashion, following the imitation learning framework. The key design challenge is to ensure that DNN predictions are accurate and fast enough, at a high sampling frequency, and result in smooth vehicle operation under different operating conditions. In this study, we design and compare various DNNs, to identify the most effective approach for real-time autonomous driving. In designing the DNNs, we adopted an incremental design approach that involved enhancing the model capacity and dataset to address the challenges of real-world driving scenarios. We designed a PD system, CNN, CNN-LSTM, and CNN-NODE, and evaluated their performance on the real-world MIT Racecar. While the PD system handled basic lane following, it struggled with sharp turns and lighting variations. The CNN improved steering but lacked temporal awareness, which the CNN-LSTM addressed as it resulted in smooth driving performance. The CNN-NODE performed similarly to the CNN-LSTM in handling driving dynamics, yet with slightly better driving performance. The findings of this research highlight the importance of iterative design processes in developing robust DNNs for autonomous driving applications. The experimental video is available at https://www.youtube.com/watch?v=FNNYgU--iaY.","authors":["Hidayet Ersin Dursun","Yusuf G\\\"uven","Tufan Kumbasar"],"url":"https://arxiv.org/abs/2504.18847"}
{"created":"2025-04-29","title":"Theoretical Framework for Tempered Fractional Gradient Descent: Application to Breast Cancer Classification","abstract":"This paper introduces Tempered Fractional Gradient Descent (TFGD), a novel optimization framework that synergizes fractional calculus with exponential tempering to enhance gradient-based learning. Traditional gradient descent methods often suffer from oscillatory updates and slow convergence in high-dimensional, noisy landscapes. TFGD addresses these limitations by incorporating a tempered memory mechanism, where historical gradients are weighted by fractional coefficients $|w_j| = \\binom{\\alpha}{j}$ and exponentially decayed via a tempering parameter $\\lambda$. Theoretical analysis establishes TFGD's convergence guarantees: in convex settings, it achieves an $\\mathcal{O}(1/K)$ rate with alignment coefficient $d_{\\alpha,\\lambda} = (1 - e^{-\\lambda})^{-\\alpha}$, while stochastic variants attain $\\mathcal{O}(1/k^\\alpha)$ error decay. The algorithm maintains $\\mathcal{O}(n)$ time complexity equivalent to SGD, with memory overhead scaling as $\\mathcal{O}(d/\\lambda)$ for parameter dimension $d$. Empirical validation on the Breast Cancer Wisconsin dataset demonstrates TFGD's superiority, achieving 98.25\\% test accuracy (vs. 92.11\\% for SGD) and 2$\\times$ faster convergence. The tempered memory mechanism proves particularly effective in medical classification tasks, where feature correlations benefit from stable gradient averaging. These results position TFGD as a robust alternative to conventional optimizers in both theoretical and applied machine learning.","authors":["Omar Naifar"],"url":"https://arxiv.org/abs/2504.18849"}
{"created":"2025-04-29","title":"When2Call: When (not) to Call Tools","abstract":"Leveraging external tools is a key feature for modern Language Models (LMs) to expand their capabilities and integrate them into existing systems. However, existing benchmarks primarily focus on the accuracy of tool calling -- whether the correct tool is called with the correct parameters -- and less on evaluating when LMs should (not) call tools. We develop a new benchmark, When2Call, which evaluates tool-calling decision-making: when to generate a tool call, when to ask follow-up questions and when to admit the question can't be answered with the tools provided. We find that state-of-the-art tool-calling LMs show significant room for improvement on When2Call, indicating the importance of this benchmark. We also develop a training set for When2Call and leverage the multiple-choice nature of the benchmark to develop a preference optimization training regime, which shows considerably more improvement than traditional fine-tuning. We release the benchmark and training data as well as evaluation scripts at https://github.com/NVIDIA/When2Call.","authors":["Hayley Ross","Ameya Sunil Mahabaleshwarkar","Yoshi Suhara"],"url":"https://arxiv.org/abs/2504.18851"}
{"created":"2025-04-29","title":"Cross Far- and Near-Field Beam Management Technologies in Millimeter-Wave and Terahertz MIMO Systems","abstract":"The evolution of wireless communication toward next-generation networks introduces unprecedented demands on data rates, latency, and connectivity. To meet these requirements, two key trends have emerged: the use of higher communication frequencies to provide broader bandwidth, and the deployment of massive multiple-input multiple-output systems with large antenna arrays to compensate for propagation losses and enhance spatial multiplexing. These advancements significantly extend the Rayleigh distance, enabling near-field (NF) propagation alongside the traditional far-field (FF) regime. As user communication distances dynamically span both FF and NF regions, cross-field (CF) communication has also emerged as a practical consideration. Beam management (BM)-including beam scanning, channel state information estimation, beamforming, and beam tracking-plays a central role in maintaining reliable directional communications. While most existing BM techniques are developed for FF channels, recent works begin to address the unique characteristics of NF and CF regimes. This survey presents a comprehensive review of BM techniques from the perspective of propagation fields. We begin by building the basic through analyzing the modeling of FF, NF, and CF channels, along with the associated beam patterns for alignment. Then, we categorize BM techniques by methodologies, and discuss their operational differences across propagation regimes, highlighting how field-dependent channel characteristics influence design tradeoffs and implementation complexity. In addition, for each BM method, we identify open challenges and future research directions, including extending FF methods to NF or CF scenarios, developing unified BM strategies for field-agnostic deployment, and designing low-overhead BM solutions for dynamic environments.","authors":["Yuhang Chen","Heyin Shen","Chong Han"],"url":"https://arxiv.org/abs/2504.18855"}
{"created":"2025-04-29","title":"Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation","abstract":"In Computational Pathology (CPath), the introduction of Vision-Language Models (VLMs) has opened new avenues for research, focusing primarily on aligning image-text pairs at a single magnification level. However, this approach might not be sufficient for tasks like cancer subtype classification, tissue phenotyping, and survival analysis due to the limited level of detail that a single-resolution image can provide. Addressing this, we propose a novel multi-resolution paradigm leveraging Whole Slide Images (WSIs) to extract histology patches at multiple resolutions and generate corresponding textual descriptions through advanced CPath VLM. We introduce visual-textual alignment at multiple resolutions as well as cross-resolution alignment to establish more effective text-guided visual representations. Cross-resolution alignment using a multimodal encoder enhances the model's ability to capture context from multiple resolutions in histology images. Our model aims to capture a broader range of information, supported by novel loss functions, enriches feature representation, improves discriminative ability, and enhances generalization across different resolutions. Pre-trained on a comprehensive TCGA dataset with 34 million image-language pairs at various resolutions, our fine-tuned model outperforms state-of-the-art (SOTA) counterparts across multiple datasets and tasks, demonstrating its effectiveness in CPath. The code is available on GitHub at: https://github.com/BasitAlawode/MR-PLIP","authors":["Shahad Albastaki","Anabia Sohail","Iyyakutti Iyappan Ganapathi","Basit Alawode","Asim Khan","Sajid Javed","Naoufel Werghi","Mohammed Bennamoun","Arif Mahmood"],"url":"https://arxiv.org/abs/2504.18856"}
{"created":"2025-04-29","title":"Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation","abstract":"Large Language Models (LLMs) often struggle to process and generate coherent context when the number of input tokens exceeds the pre-trained length. Recent advancements in long-context extension have significantly expanded the context window of LLMs but require expensive overhead to train the large-scale models with longer context. In this work, we propose Dimension-Wise Positional Embeddings Manipulation (DPE), a training-free framework to extrapolate the context window of LLMs by diving into RoPE's different hidden dimensions. Instead of manipulating all dimensions equally, DPE detects the effective length for every dimension and finds the key dimensions for context extension. We reuse the original position indices with their embeddings from the pre-trained model and manipulate the key dimensions' position indices to their most effective lengths. In this way, DPE adjusts the pre-trained models with minimal modifications while ensuring that each dimension reaches its optimal state for extrapolation. DPE significantly surpasses well-known baselines such as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of 128k tokens without continual training and integrates seamlessly with Flash Attention 2. In addition to its impressive extrapolation capability, DPE also dramatically improves the models' performance within training length, such as Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When compared with commercial models, Llama 3.1 70B with DPE even achieves better performance than GPT-4-128K.","authors":["Yi Lu","Wanxu Zhao","Xin Zhou","Chenxin An","Chenglong Wang","Shuo Li","Yuming Yang","Jun Zhao","Tao Ji","Tao Gui","Qi Zhang","Xuanjing Huang"],"url":"https://arxiv.org/abs/2504.18857"}
{"created":"2025-04-29","title":"Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle","abstract":"Context: ChatGPT and other large language models (LLMs) are widely used across healthcare, business, economics, engineering, and software engineering (SE). Despite their popularity, concerns persist about their reliability, especially their error rates across domains and the software development lifecycle (SDLC).","authors":["Vahid Garousi"],"url":"https://arxiv.org/abs/2504.18858"}
{"created":"2025-04-29","title":"Diffeomorphic Obstacle Avoidance for Contractive Dynamical Systems via Implicit Representations","abstract":"Ensuring safety and robustness of robot skills is becoming crucial as robots are required to perform increasingly complex and dynamic tasks. The former is essential when performing tasks in cluttered environments, while the latter is relevant to overcome unseen task situations. This paper addresses the challenge of ensuring both safety and robustness in dynamic robot skills learned from demonstrations. Specifically, we build on neural contractive dynamical systems to provide robust extrapolation of the learned skills, while designing a full-body obstacle avoidance strategy that preserves contraction stability via diffeomorphic transforms. This is particularly crucial in complex environments where implicit scene representations, such as Signed Distance Fields (SDFs), are necessary. To this end, our framework called Signed Distance Field Diffeomorphic Transform, leverages SDFs and flow-based diffeomorphisms to achieve contraction-preserving obstacle avoidance. We thoroughly evaluate our framework on synthetic datasets and several real-world robotic tasks in a kitchen environment. Our results show that our approach locally adapts the learned contractive vector field while staying close to the learned dynamics and without introducing highly-curved motion paths, thus outperforming several state-of-the-art methods.","authors":["Ken-Joel Simmoteit","Philipp Schillinger","Leonel Rozo"],"url":"https://arxiv.org/abs/2504.18860"}
{"created":"2025-04-29","title":"Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras","abstract":"The need for accurate and non-intrusive flow measurement methods has led to the widespread adoption of Particle Image Velocimetry (PIV), a powerful diagnostic tool in fluid motion estimation. This study investigates the tremendous potential of spike cameras (a type of ultra-high-speed, high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike Imaging Velocimetry (SIV), designed specifically for highly turbulent and intricate flow fields. To aggregate motion features from the spike stream while minimizing information loss, we incorporate a Detail-Preserving Hierarchical Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to extract contextual features from highly complex fluid flows. Furthermore, we present a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), which provides labeled data for three challenging fluid dynamics scenarios. Our proposed method achieves superior performance compared to existing baseline methods on PSSD. The datasets and our implementation of SIV are open-sourced in the supplementary materials.","authors":["Yunzhong Zhang","Bo Xiong","You Zhou","Changqing Su","Zhen Cheng","Zhaofei Yu","Xun Cao","Tiejun Huang"],"url":"https://arxiv.org/abs/2504.18864"}
{"created":"2025-04-29","title":"PiercingEye: Dual-Space Video Violence Detection with Hyperbolic Vision-Language Guidance","abstract":"Existing weakly supervised video violence detection (VVD) methods primarily rely on Euclidean representation learning, which often struggles to distinguish visually similar yet semantically distinct events due to limited hierarchical modeling and insufficient ambiguous training samples. To address this challenge, we propose PiercingEye, a novel dual-space learning framework that synergizes Euclidean and hyperbolic geometries to enhance discriminative feature representation. Specifically, PiercingEye introduces a layer-sensitive hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to progressively model event hierarchies, and a cross-space attention mechanism to facilitate complementary feature interactions between Euclidean and hyperbolic spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage large language models to generate logic-guided ambiguous event descriptions, enabling explicit supervision through a hyperbolic vision-language contrastive loss that prioritizes high-confusion samples via dynamic similarity-aware weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks demonstrate that PiercingEye achieves state-of-the-art performance, with particularly strong results on a newly curated ambiguous event subset, validating its superior capability in fine-grained violence detection.","authors":["Jiaxu Leng","Zhanjie Wu","Mingpi Tan","Mengjingcheng Mo","Jiankang Zheng","Qingqing Li","Ji Gan","Xinbo Gao"],"url":"https://arxiv.org/abs/2504.18866"}
{"created":"2025-04-29","title":"Approximating Nash Equilibria in General-Sum Games via Meta-Learning","abstract":"Nash equilibrium is perhaps the best-known solution concept in game theory. Such a solution assigns a strategy to each player which offers no incentive to unilaterally deviate. While a Nash equilibrium is guaranteed to always exist, the problem of finding one in general-sum games is PPAD-complete, generally considered intractable. Regret minimization is an efficient framework for approximating Nash equilibria in two-player zero-sum games. However, in general-sum games, such algorithms are only guaranteed to converge to a coarse-correlated equilibrium (CCE), a solution concept where players can correlate their strategies. In this work, we use meta-learning to minimize the correlations in strategies produced by a regret minimizer. This encourages the regret minimizer to find strategies that are closer to a Nash equilibrium. The meta-learned regret minimizer is still guaranteed to converge to a CCE, but we give a bound on the distance to Nash equilibrium in terms of our meta-loss. We evaluate our approach in general-sum imperfect information games. Our algorithms provide significantly better approximations of Nash equilibria than state-of-the-art regret minimization techniques.","authors":["David Sychrovsk\\'y","Christopher Solinas","Revan MacQueen","Kevin Wang","James R. Wright","Nathan R. Sturtevant","Michael Bowling"],"url":"https://arxiv.org/abs/2504.18868"}
{"created":"2025-04-29","title":"WLTCL: Wide Field-of-View 3-D LiDAR Truck Compartment Automatic Localization System","abstract":"As an essential component of logistics automation, the automated loading system is becoming a critical technology for enhancing operational efficiency and safety. Precise automatic positioning of the truck compartment, which serves as the loading area, is the primary step in automated loading. However, existing methods have difficulty adapting to truck compartments of various sizes, do not establish a unified coordinate system for LiDAR and mobile manipulators, and often exhibit reliability issues in cluttered environments. To address these limitations, our study focuses on achieving precise automatic positioning of key points in large, medium, and small fence-style truck compartments in cluttered scenarios. We propose an innovative wide field-of-view 3-D LiDAR vehicle compartment automatic localization system. For vehicles of various sizes, this system leverages the LiDAR to generate high-density point clouds within an extensive field-of-view range. By incorporating parking area constraints, our vehicle point cloud segmentation method more effectively segments vehicle point clouds within the scene. Our compartment key point positioning algorithm utilizes the geometric features of the compartments to accurately locate the corner points, providing stackable spatial regions. Extensive experiments on our collected data and public datasets demonstrate that this system offers reliable positioning accuracy and reduced computational resource consumption, leading to its application and promotion in relevant fields.","authors":["Guodong Sun","Mingjing Li","Dingjie Liu","Mingxuan Liu","Bo Wu","Yang Zhang"],"url":"https://arxiv.org/abs/2504.18870"}
{"created":"2025-04-29","title":"Latent Adversarial Training Improves the Representation of Refusal","abstract":"Recent work has shown that language models' refusal behavior is primarily encoded in a single direction in their latent space, making it vulnerable to targeted attacks. Although Latent Adversarial Training (LAT) attempts to improve robustness by introducing noise during training, a key question remains: How does this noise-based training affect the underlying representation of refusal behavior? Understanding this encoding is crucial for evaluating LAT's effectiveness and limitations, just as the discovery of linear refusal directions revealed vulnerabilities in traditional supervised safety fine-tuning (SSFT).","authors":["Alexandra Abbas","Nora Petrova","Helios Ael Lyons","Natalia Perez-Campanero"],"url":"https://arxiv.org/abs/2504.18872"}
{"created":"2025-04-29","title":"Generative to Agentic AI: Survey, Conceptualization, and Challenges","abstract":"Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It constitutes the next major step in the evolution of AI with much stronger reasoning and interaction capabilities that enable more autonomous behavior to tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI has seen widespread adoption, giving users firsthand experience. However, the distinction between Agentic AI and GenAI remains less well understood. To address this gap, our survey is structured in two parts. In the first part, we compare GenAI and Agentic AI using existing literature, discussing their key characteristics, how Agentic AI remedies limitations of GenAI, and the major steps in GenAI's evolution toward Agentic AI. This section is intended for a broad audience, including academics in both social sciences and engineering, as well as industry professionals. It provides the necessary insights to comprehend novel applications that are possible with Agentic AI but not with GenAI. In the second part, we deep dive into novel aspects of Agentic AI, including recent developments and practical concerns such as defining agents. Finally, we discuss several challenges that could serve as a future research agenda, while cautioning against risks that can emerge when exceeding human intelligence.","authors":["Johannes Schneider"],"url":"https://arxiv.org/abs/2504.18875"}
{"created":"2025-04-29","title":"Welfare and Beyond in Multi-Agent Contracts","abstract":"A principal delegates a project to a team $S$ from a pool of $n$ agents. The project's value if all agents in $S$ exert costly effort is $f(S)$. To incentivize the agents to participate, the principal assigns each agent $i\\in S$ a share $\\rho_i\\in [0,1]$ of the project's final value (i.e., designs $n$ linear contracts). The shares must be feasible -- their sum should not exceed $1$. It is well-understood how to design these contracts to maximize the principal's own expected utility, but what if the goal is to coordinate the agents toward maximizing social welfare?","authors":["Gil Aharoni","Martin Hoefer","Inbal Talgam-Cohen"],"url":"https://arxiv.org/abs/2504.18876"}
{"created":"2025-04-29","title":"TSRM: A Lightweight Temporal Feature Encoding Architecture for Time Series Forecasting and Imputation","abstract":"We introduce a temporal feature encoding architecture called Time Series Representation Model (TSRM) for multivariate time series forecasting and imputation. The architecture is structured around CNN-based representation layers, each dedicated to an independent representation learning task and designed to capture diverse temporal patterns, followed by an attention-based feature extraction layer and a merge layer, designed to aggregate extracted features. The architecture is fundamentally based on a configuration that is inspired by a Transformer encoder, with self-attention mechanisms at its core. The TSRM architecture outperforms state-of-the-art approaches on most of the seven established benchmark datasets considered in our empirical evaluation for both forecasting and imputation tasks. At the same time, it significantly reduces complexity in the form of learnable parameters. The source code is available at https://github.com/RobertLeppich/TSRM.","authors":["Robert Leppich","Michael Stenger","Daniel Grillmeyer","Vanessa Borst","Samuel Kounev"],"url":"https://arxiv.org/abs/2504.18878"}
{"created":"2025-04-29","title":"Reshaping MOFs Text Mining with a Dynamic Multi-Agent Framework of Large Language Agents","abstract":"The mining of synthesis conditions for metal-organic frameworks (MOFs) is a significant focus in materials science. However, identifying the precise synthesis conditions for specific MOFs within the vast array of possibilities presents a considerable challenge. Large Language Models (LLMs) offer a promising solution to this problem. We leveraged the capabilities of LLMs, specifically gpt-4o-mini, as core agents to integrate various MOF-related agents, including synthesis, attribute, and chemical information agents. This integration culminated in the development of MOFh6, an LLM tool designed to streamline the MOF synthesis process. MOFh6 allows users to query in multiple formats, such as submitting scientific literature, or inquiring about specific MOF codes or structural properties. The tool analyzes these queries to provide optimal synthesis conditions and generates model files for density functional theory pre modeling. We believe MOFh6 will enhance efficiency in the MOF synthesis of all researchers.","authors":["Zuhong Lin","Daoyuan Ren","Kai Ran","Sun Jing","Xiaotiang Huang","Haiyang He","Pengxu Pan","Xiaohang Zhang","Ying Fang","Tianying Wang","Minli Wu","Zhanglin Li","Xiaochuan Zhang","Haipu Li","Jingjing Yao"],"url":"https://arxiv.org/abs/2504.18880"}
{"created":"2025-04-29","title":"TSCAN: Context-Aware Uplift Modeling via Two-Stage Training for Online Merchant Business Diagnosis","abstract":"A primary challenge in ITE estimation is sample selection bias. Traditional approaches utilize treatment regularization techniques such as the Integral Probability Metrics (IPM), re-weighting, and propensity score modeling to mitigate this bias. However, these regularizations may introduce undesirable information loss and limit the performance of the model. Furthermore, treatment effects vary across different external contexts, and the existing methods are insufficient in fully interacting with and utilizing these contextual features. To address these issues, we propose a Context-Aware uplift model based on the Two-Stage training approach (TSCAN), comprising CAN-U and CAN-D sub-models. In the first stage, we train an uplift model, called CAN-U, which includes the treatment regularizations of IPM and propensity score prediction, to generate a complete dataset with counterfactual uplift labels. In the second stage, we train a model named CAN-D, which utilizes an isotonic output layer to directly model uplift effects, thereby eliminating the reliance on the regularization components. CAN-D adaptively corrects the errors estimated by CAN-U through reinforcing the factual samples, while avoiding the negative impacts associated with the aforementioned regularizations. Additionally, we introduce a Context-Aware Attention Layer throughout the two-stage process to manage the interactions between treatment, merchant, and contextual features, thereby modeling the varying treatment effect in different contexts. We conduct extensive experiments on two real-world datasets to validate the effectiveness of TSCAN. Ultimately, the deployment of our model for real-world merchant diagnosis on one of China's largest online food ordering platforms validates its practical utility and impact.","authors":["Hangtao Zhang","Zhe Li","Kairui Zhang"],"url":"https://arxiv.org/abs/2504.18881"}
{"created":"2025-04-29","title":"SPD Learning for Covariance-Based Neuroimaging Analysis: Perspectives, Methods, and Challenges","abstract":"Neuroimaging provides a critical framework for characterizing brain activity by quantifying connectivity patterns and functional architecture across modalities. While modern machine learning has significantly advanced our understanding of neural processing mechanisms through these datasets, decoding task-specific signatures must contend with inherent neuroimaging constraints, for example, low signal-to-noise ratios in raw electrophysiological recordings, cross-session non-stationarity, and limited sample sizes. This review focuses on machine learning approaches for covariance-based neuroimaging data, where often symmetric positive definite (SPD) matrices under full-rank conditions encode inter-channel relationships. By equipping the space of SPD matrices with Riemannian metrics (e.g., affine-invariant or log-Euclidean), their space forms a Riemannian manifold enabling geometric analysis. We unify methodologies operating on this manifold under the SPD learning framework, which systematically leverages the SPD manifold's geometry to process covariance features, thereby advancing brain imaging analytics.","authors":["Ce Ju","Reinmar J. Kobler","Antoine Collas","Motoaki Kawanabe","Cuntai Guan","Bertrand Thirion"],"url":"https://arxiv.org/abs/2504.18882"}
{"created":"2025-04-29","title":"LiLIS: Enhancing Big Spatial Data Processing with Lightweight Distributed Learned Index","abstract":"The efficient management of big spatial data is crucial for location-based services, particularly in smart cities. However, existing systems such as Simba and Sedona, which incorporate distributed spatial indexing, still incur substantial index construction overheads, rendering them far from optimal for real-time analytics. Recent studies demonstrate that learned indices can achieve high efficiency through well-designed machine learning models, but how to design a learned index for distributed spatial analytics remains unaddressed. In this paper, we present LiLIS, a Lightweight Distributed Learned Index for big spatial data. LiLIS combines machine-learned search strategies with spatial-aware partitioning within a distributed framework, and efficiently implements common spatial queries, including point query, range query, k-nearest neighbors (kNN), and spatial joins. Extensive experimental results over real-world and synthetic datasets show that LiLIS outperforms state-of-the-art big spatial data analytics by $2-3$ orders of magnitude for most spatial queries, and the index building achieves $1.5-2\\times$ speed-up. The code is available at https://github.com/SWUFE-DB-Group/learned-index-spark.","authors":["Zhongpu Chen","Wanjun Hao","Ziang Zeng","Long Shi","Yi Wen","Zhi-Jie Wang","Yu Zhao"],"url":"https://arxiv.org/abs/2504.18883"}
{"created":"2025-04-29","title":"A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification","abstract":"With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%.","authors":["Junichiro Niimi"],"url":"https://arxiv.org/abs/2504.18884"}
{"created":"2025-04-29","title":"Exploiting Multiple Representations: 3D Face Biometrics Fusion with Application to Surveillance","abstract":"3D face reconstruction (3DFR) algorithms are based on specific assumptions tailored to the limits and characteristics of the different application scenarios. In this study, we investigate how multiple state-of-the-art 3DFR algorithms can be used to generate a better representation of subjects, with the final goal of improving the performance of face recognition systems in challenging uncontrolled scenarios. We also explore how different parametric and non-parametric score-level fusion methods can exploit the unique strengths of multiple 3DFR algorithms to enhance biometric recognition robustness. With this goal, we propose a comprehensive analysis of several face recognition systems across diverse conditions, such as varying distances and camera setups, intra-dataset and cross-dataset, to assess the robustness of the proposed ensemble method. The results demonstrate that the distinct information provided by different 3DFR algorithms can alleviate the problem of generalizing over multiple application scenarios. In addition, the present study highlights the potential of advanced fusion strategies to enhance the reliability of 3DFR-based face recognition systems, providing the research community with key insights to exploit them in real-world applications effectively. Although the experiments are carried out in a specific face verification setup, our proposed fusion-based 3DFR methods may be applied to other tasks around face biometrics that are not strictly related to identity recognition.","authors":["Simone Maurizio La Cava","Roberto Casula","Sara Concas","Giulia Orr\\`u","Ruben Tolosana","Martin Drahansky","Julian Fierrez","Gian Luca Marcialis"],"url":"https://arxiv.org/abs/2504.18886"}
{"created":"2025-04-29","title":"Closed-Form Expressions for I/O Relation in Zak-OTFS with Different Delay-Doppler Filters","abstract":"The transceiver operations in the delay-Doppler (DD) domain in Zak-OTFS modulation, including DD domain filtering at the transmitter and receiver, involve twisted convolution operation. The twisted convolution operations give rise to multiple integrals in the end-to-end DD domain input-output (I/O) relation. The I/O relation plays a crucial role in performance evaluation and algorithm development for transceiver implementation. In this paper, we derive discrete DD domain closed-form expressions for the I/O relation and noise covariance in Zak-OTFS. We derive these expressions for sinc and Gaussian pulse shaping DD filters at the transmitter (Tx). On the receiver (Rx) side, three types of DD filters are considered, viz., $(i)$ Rx filter identical to Tx filter (referred to as `identical filtering'), $(ii)$ Rx filter matched to the Tx filter (referred to as `matched filtering'), and $(iii)$ Rx filter matched to both Tx filter and channel response (referred to as `channel matched filtering'). For all the above cases, except for the case of sinc identical filtering, we derive exact I/O relation and noise covariance expressions in closed-form. For the sinc identical filtering case, we derive approximate closed-form expressions which are shown to be accurate. Using the derived closed-form expressions, we evaluate the bit error performance of Zak-OTFS for different Tx/Rx filter configurations. Our results using Vehicular-A (Veh-A) channel model with fractional DDs show that, while matched filtering achieves slightly better or almost same performance as identical filtering, channel matched filtering achieves the best performance among the three.","authors":["Arpan Das","Fathima Jesbin","Ananthanarayanan Chockalingam"],"url":"https://arxiv.org/abs/2504.18887"}
{"created":"2025-04-29","title":"Which kind of research papers influence policymaking","abstract":"This study examines the use of evidence in policymaking by analysing a range of journal and article attributes, as well as online engagement metrics. It employs a large-scale citation analysis of nearly 150,000 articles covering diverse policy topics. The findings highlight that scholarly citations exert the strongest positive influence on policy citations. Articles from journals with a higher citation impact and larger Mendeley readership are cited more frequently in policy documents. Other online engagements, such as news and blog mentions, also boost policy citations, while mentions on social media X have a negative effect. The finding that highly cited and widely read papers are also frequently referenced in policy documents likely reflects the perception among policymakers that such research is more trustworthy. In contrast, papers that derive their influence primarily from social media tend to be cited less often in policy contexts.","authors":["Pablo Dorta-Gonz\\'alez"],"url":"https://arxiv.org/abs/2504.18889"}
{"created":"2025-04-29","title":"Effect of perceived preprint effectiveness and research intensity on posting behaviour","abstract":"Open science is increasingly recognised worldwide, with preprint posting emerging as a key strategy. This study explores the factors influencing researchers' adoption of preprint publication, particularly the perceived effectiveness of this practice and research intensity indicators such as publication and review frequency. Using open data from a comprehensive survey with 5,873 valid responses, we conducted regression analyses to control for demographic variables. Researchers' productivity, particularly the number of journal articles and books published, greatly influences the frequency of preprint deposits. The perception of the effectiveness of preprints follows this. Preprints are viewed positively in terms of early access to new research, but negatively in terms of early feedback. Demographic variables, such as gender and the type of organisation conducting the research, do not have a significant impact on the production of preprints when other factors are controlled for. However, the researcher's discipline, years of experience and geographical region generally have a moderate effect on the production of preprints. These findings highlight the motivations and barriers associated with preprint publication and provide insights into how researchers perceive the benefits and challenges of this practice within the broader context of open science.","authors":["Pablo Dorta-Gonz\\'alez","Mar\\'ia Isabel Dorta-Gonz\\'alez"],"url":"https://arxiv.org/abs/2504.18896"}
{"created":"2025-04-29","title":"Hierarchical Temporal Logic Task and Motion Planning for Multi-Robot Systems","abstract":"Task and motion planning (TAMP) for multi-robot systems, which integrates discrete task planning with continuous motion planning, remains a challenging problem in robotics. Existing TAMP approaches often struggle to scale effectively for multi-robot systems with complex specifications, leading to infeasible solutions and prolonged computation times. This work addresses the TAMP problem in multi-robot settings where tasks are specified using expressive hierarchical temporal logic and task assignments are not pre-determined. Our approach leverages the efficiency of hierarchical temporal logic specifications for task-level planning and the optimization-based graph of convex sets method for motion-level planning, integrating them within a product graph framework. At the task level, we convert hierarchical temporal logic specifications into a single graph, embedding task allocation within its edges. At the motion level, we represent the feasible motions of multiple robots through convex sets in the configuration space, guided by a sampling-based motion planner. This formulation allows us to define the TAMP problem as a shortest path search within the product graph, where efficient convex optimization techniques can be applied. We prove that our approach is both sound and complete under mild assumptions. Additionally, we extend our framework to cooperative pick-and-place tasks involving object handovers between robots. We evaluate our method across various high-dimensional multi-robot scenarios, including simulated and real-world environments with quadrupeds, robotic arms, and automated conveyor systems. Our results show that our approach outperforms existing methods in execution time and solution optimality while effectively scaling with task complexity.","authors":["Zhongqi Wei","Xusheng Luo","Changliu Liu"],"url":"https://arxiv.org/abs/2504.18899"}
{"created":"2025-04-29","title":"Adaptive Nonlinear Elimination Preconditioning for Transport in Fractured Porous Media","abstract":"Sequential implicit (SI) formulations are gaining increasing interest due to their ability to decouple reservoir simulation problems into distinct flow and transport subproblems, allowing for the use of specialized solvers tailored to each. This separation often improves solver efficiency and flexibility, especially in weakly coupled systems. However, for fractured reservoirs, even the decoupled subproblems may generate nonlinearly stiff systems. This is specifically evident in the transport subproblem, where fracture-induced non-linearity imbalances often lead to poor Newton convergence, including failed iterations and frequent timestep cuts. To address this challenge, we propose and investigate an adaptive Nonlinear Elimination (NE) preconditioned exact Newton algorithm specifically tailored to transport subproblems that arise from the sequential splitting of two-phase flow in fractured porous media. The proposed method is evaluated through a series of waterflooding test cases involving discrete fracture networks. The adaptive NE-preconditioned algorithm consistently demonstrates improved convergence behavior and computational efficiency compared to standard Newton.","authors":["Omar Chaabi","Mohammed Al-Kobaisi"],"url":"https://arxiv.org/abs/2504.18900"}
{"created":"2025-04-29","title":"Transformer-Empowered Actor-Critic Reinforcement Learning for Sequence-Aware Service Function Chain Partitioning","abstract":"In the forthcoming era of 6G networks, characterized by unprecedented data rates, ultra-low latency, and extensive connectivity, effective management of Virtualized Network Functions (VNFs) is essential. VNFs are software-based counterparts of traditional hardware devices that facilitate flexible and scalable service provisioning. Service Function Chains (SFCs), structured as ordered sequences of VNFs, are pivotal in orchestrating complex network services. Nevertheless, partitioning SFCs across multi-domain network infrastructures presents substantial challenges due to stringent latency constraints and limited resource availability. Conventional optimization-based methods typically exhibit low scalability, whereas existing data-driven approaches often fail to adequately balance computational efficiency with the capability to effectively account for dependencies inherent in SFCs. To overcome these limitations, we introduce a Transformer-empowered actor-critic framework specifically designed for sequence-aware SFC partitioning. By utilizing the self-attention mechanism, our approach effectively models complex inter-dependencies among VNFs, facilitating coordinated and parallelized decision-making processes. Additionally, we enhance training stability and convergence using $\\epsilon$-LoPe exploration strategy as well as Asymptotic Return Normalization. Comprehensive simulation results demonstrate that the proposed methodology outperforms existing state-of-the-art solutions in terms of long-term acceptance rates, resource utilization efficiency, and scalability, while achieving rapid inference. This study not only advances intelligent network orchestration by delivering a scalable and robust solution for SFC partitioning within emerging 6G environments, but also bridging recent advancements in Large Language Models (LLMs) with the optimization of next-generation networks.","authors":["Cyril Shih-Huan Hsu","Anestis Dalgkitsis","Chrysa Papagianni","Paola Grosso"],"url":"https://arxiv.org/abs/2504.18902"}
{"created":"2025-04-29","title":"Numerical analysis of an H(div)-conforming divergence-free discontinuous Galerkin method with a second-order explicit Runge-Kutta scheme for the incompressible Euler equations","abstract":"In this paper, we present an error analysis for an \\( H(\\text{div}) \\)-conforming divergence-free discontinuous Galerkin (DG) method, combined with a second-order explicit Runge-Kutta (RK) scheme, for the incompressible Euler equations. This work extends the error analysis of the second-order explicit Runge-Kutta discontinuous Galerkin (RKDG) type methods to incompressible flows, in which the exactly divergence-free constraint introduces additional challenges to the analysis. For smooth solutions, we rigorously derive an a priori error estimate of $O(h^{k+1 / 2}+\\tau^2)$ under a restrictive CFL condition $\\tau \\lesssim h^{4 / 3}$ for polynomials of degree $k \\geq 1$, where $h$ and $\\tau$ are the mesh size and time step size, respectively. For the case of linear polynomials, we further investigate whether existing analytical techniques can relax the restrictive CFL condition to a standard CFL condition $\\tau \\lesssim h$. It is demonstrated that the exactly divergence-free constraint prevents the application of these techniques. We conjecture that the error estimates for linear polynomials cannot be derived under a standard CFL condition. Numerical experiments are conducted, supporting our analytical results and the conjecture for linear polynomials.","authors":["Yongbin Han","Yanren Hou","Xuehua Zhao"],"url":"https://arxiv.org/abs/2504.18903"}
{"created":"2025-04-29","title":"RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning","abstract":"Data scaling and standardized evaluation benchmarks have driven significant advances in natural language processing and computer vision. However, robotics faces unique challenges in scaling data and establishing evaluation protocols. Collecting real-world data is resource-intensive and inefficient, while benchmarking in real-world scenarios remains highly complex. Synthetic data and simulation offer promising alternatives, yet existing efforts often fall short in data quality, diversity, and benchmark standardization. To address these challenges, we introduce RoboVerse, a comprehensive framework comprising a simulation platform, a synthetic dataset, and unified benchmarks. Our simulation platform supports multiple simulators and robotic embodiments, enabling seamless transitions between different environments. The synthetic dataset, featuring high-fidelity physics and photorealistic rendering, is constructed through multiple approaches. Additionally, we propose unified benchmarks for imitation learning and reinforcement learning, enabling evaluation across different levels of generalization. At the core of the simulation platform is MetaSim, an infrastructure that abstracts diverse simulation environments into a universal interface. It restructures existing simulation environments into a simulator-agnostic configuration system, as well as an API aligning different simulator functionalities, such as launching simulation environments, loading assets with initial states, stepping the physics engine, etc. This abstraction ensures interoperability and extensibility. Comprehensive experiments demonstrate that RoboVerse enhances the performance of imitation learning, reinforcement learning, world model learning, and sim-to-real transfer. These results validate the reliability of our dataset and benchmarks, establishing RoboVerse as a robust solution for advancing robot learning.","authors":["Haoran Geng","Feishi Wang","Songlin Wei","Yuyang Li","Bangjun Wang","Boshi An","Charlie Tianyue Cheng","Haozhe Lou","Peihao Li","Yen-Jen Wang","Yutong Liang","Dylan Goetting","Chaoyi Xu","Haozhe Chen","Yuxi Qian","Yiran Geng","Jiageng Mao","Weikang Wan","Mingtong Zhang","Jiangran Lyu","Siheng Zhao","Jiazhao Zhang","Jialiang Zhang","Chengyang Zhao","Haoran Lu","Yufei Ding","Ran Gong","Yuran Wang","Yuxuan Kuang","Ruihai Wu","Baoxiong Jia","Carlo Sferrazza","Hao Dong","Siyuan Huang","Yue Wang","Jitendra Malik","Pieter Abbeel"],"url":"https://arxiv.org/abs/2504.18904"}
{"created":"2025-04-29","title":"Fairness-aware Dynamic Hosting Capacity and the Impacts of Strategic Solar PV Curtailment","abstract":"Rapid deployment of distributed energy resources (DERs), such as solar photovoltaics (PV), poses a risk to the distribution grid under high penetration. Therefore, studying hosting capacity (HC) limits considering grid physics and demand variability is crucial. This paper introduces an improved framework for determining the HC of radial distribution networks by enhancing an existing convex inner approximation (CIA) approach. The proposed method achieves a more accurate and larger inner approximation, resulting in better HC limits. We also consider time-varying demand and the design of objective functions to ensure equitable access to grid resources. A case study with solar PV integration is conducted using a modified IEEE-37 radial network to examine the impact of increased PV capacity, demonstrating that with no more than 5% annual solar PV energy curtailed, it is possible to increase solar PV hosting capacity by at least 50% with no negative grid impacts and a net positive economic impact when accounted for the cost of carbon. Results show that fair allocation methods can lead to higher net profits and reduced PV curtailment and CO2.","authors":["Beyzanur Aydin","Rebecca Holt","Mads Almassalkhi"],"url":"https://arxiv.org/abs/2504.18905"}
{"created":"2025-04-29","title":"Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness","abstract":"Unauthorized screen capturing and dissemination pose severe security threats such as data leakage and information theft. Several studies propose robust watermarking methods to track the copyright of Screen-Camera (SC) images, facilitating post-hoc certification against infringement. These techniques typically employ heuristic mathematical modeling or supervised neural network fitting as the noise layer, to enhance watermarking robustness against SC. However, both strategies cannot fundamentally achieve an effective approximation of SC noise. Mathematical simulation suffers from biased approximations due to the incomplete decomposition of the noise and the absence of interdependence among the noise components. Supervised networks require paired data to train the noise-fitting model, and it is difficult for the model to learn all the features of the noise. To address the above issues, we propose Simulation-to-Real (S2R). Specifically, an unsupervised noise layer employs unpaired data to learn the discrepancy between the modeling simulated noise distribution and the real-world SC noise distribution, rather than directly learning the mapping from sharp images to real-world images. Learning this transformation from simulation to reality is inherently simpler, as it primarily involves bridging the gap in noise distributions, instead of the complex task of reconstructing fine-grained image details. Extensive experimental results validate the efficacy of the proposed method, demonstrating superior watermark robustness and generalization compared to those of state-of-the-art methods.","authors":["Yufeng Wu","Xin Liao","Baowei Wang","Han Fang","Xiaoshuai Wu","Guiling Wang"],"url":"https://arxiv.org/abs/2504.18906"}
{"created":"2025-04-29","title":"Kinship Verification through a Forest Neural Network","abstract":"Early methods used face representations in kinship verification, which are less accurate than joint representations of parents' and children's facial images learned from scratch. We propose an approach featuring graph neural network concepts to utilize face representations and have comparable results to joint representation algorithms. Moreover, we designed the structure of the classification module and introduced a new combination of losses to engage the center loss gradually in training our network. Additionally, we conducted experiments on KinFaceW-I and II, demonstrating the effectiveness of our approach. We achieved the best result on KinFaceW-II, an average improvement of nearly 1.6 for all kinship types, and we were near the best on KinFaceW-I. The code is available at https://github.com/ali-nazari/Kinship-Verification","authors":["Ali Nazari","Mohsen Ebrahimi Moghaddam","Omidreza Borzoei"],"url":"https://arxiv.org/abs/2504.18910"}
{"created":"2025-04-29","title":"Inferring Questions from Programming Screenshots","abstract":"The integration of generative AI into developer forums like Stack Overflow presents an opportunity to enhance problem-solving by allowing users to post screenshots of code or Integrated Development Environments (IDEs) instead of traditional text-based queries. This study evaluates the effectiveness of various large language models (LLMs), specifically LLAMA, GEMINI, and GPT-4o in interpreting such visual inputs. We employ prompt engineering techniques, including in-context learning, chain-of-thought prompting, and few-shot learning, to assess each model's responsiveness and accuracy. Our findings show that while GPT-4o shows promising capabilities, achieving over 60% similarity to baseline questions for 51.75% of the tested images, challenges remain in obtaining consistent and accurate interpretations for more complex images. This research advances our understanding of the feasibility of using generative AI for image-centric problem-solving in developer communities, highlighting both the potential benefits and current limitations of this approach while envisioning a future where visual-based debugging copilot tools become a reality.","authors":["Faiz Ahmed","Xuchen Tan","Folajinmi Adewole","Suprakash Datta","Maleknaz Nayebi"],"url":"https://arxiv.org/abs/2504.18912"}
{"created":"2025-04-29","title":"On Symmetric Lanczos Quadrature for Trace Estimation","abstract":"The Golub-Welsch algorithm computes Gauss quadrature rules with the nodes and weights generated from the symmetric tridiagonal matrix in the Lanczos process. While symmetric Lanczos quadrature (in exact arithmetic) theoretically reduces computational costs, its practical feasibility for trace estimation remains uncertain. This paper resolves this ambiguity by establishing sufficient and necessary conditions for the symmetry of Lanczos quadratrure. For matrices of Jordan-Wielandt type, we provide guidance on selecting initial vectors for the Lanczos algorithm that guarantees symmetric quadrature nodes and weights. More importantly, regarding Estrada index computations in bipartite graphs or directed ones, our method would not only save computational costs, but also ensure the unbiasedness of trace estimators.","authors":["Wenhao Li","Zongyuan Han","Shengxin Zhu"],"url":"https://arxiv.org/abs/2504.18913"}
{"created":"2025-04-29","title":"Factor Analysis with Correlated Topic Model for Multi-Modal Data","abstract":"Integrating various data modalities brings valuable insights into underlying phenomena. Multimodal factor analysis (FA) uncovers shared axes of variation underlying different simple data modalities, where each sample is represented by a vector of features. However, FA is not suited for structured data modalities, such as text or single cell sequencing data, where multiple data points are measured per each sample and exhibit a clustering structure. To overcome this challenge, we introduce FACTM, a novel, multi-view and multi-structure Bayesian model that combines FA with correlated topic modeling and is optimized using variational inference. Additionally, we introduce a method for rotating latent factors to enhance interpretability with respect to binary features. On text and video benchmarks as well as real-world music and COVID-19 datasets, we demonstrate that FACTM outperforms other methods in identifying clusters in structured data, and integrating them with simple modalities via the inference of shared, interpretable factors.","authors":["Ma{\\l}gorzata {\\L}az\\k{e}cka","Ewa Szczurek"],"url":"https://arxiv.org/abs/2504.18914"}
{"created":"2025-04-29","title":"UnifyFL: Enabling Decentralized Cross-Silo Federated Learning","abstract":"Federated Learning (FL) is a decentralized machine learning (ML) paradigm in which models are trained on private data across several devices called clients and combined at a single node called an aggregator rather than aggregating the data itself. Many organizations employ FL to have better privacy-aware ML-driven decision-making capabilities. However, organizations often operate independently rather than collaborate to enhance their FL capabilities due to the lack of an effective mechanism for collaboration. The challenge lies in balancing trust and resource efficiency. One approach relies on trusting a third-party aggregator to consolidate models from all organizations (multilevel FL), but this requires trusting an entity that may be biased or unreliable. Alternatively, organizations can bypass a third party by sharing their local models directly, which requires significant computational resources for validation. Both approaches reflect a fundamental trade-off between trust and resource constraints, with neither offering an ideal solution. In this work, we develop a trust-based cross-silo FL framework called \\proj, which uses decentralized orchestration and distributed storage. \\proj provides flexibility to the participating organizations and presents synchronous and asynchronous modes to handle stragglers. Our evaluation on a diverse testbed shows that \\proj achieves a performance comparable to the ideal multilevel centralized FL while allowing trust and optimal use of resources.","authors":["Sarang S","Druva Dhakshinamoorthy","Aditya Shiva Sharma","Yuvraj Singh Bhadauria","Siddharth Chaitra Vivek","Arihant Bansal","Arnab K. Paul"],"url":"https://arxiv.org/abs/2504.18916"}
{"created":"2025-04-29","title":"Meta-Learning in Self-Play Regret Minimization","abstract":"Regret minimization is a general approach to online optimization which plays a crucial role in many algorithms for approximating Nash equilibria in two-player zero-sum games. The literature mainly focuses on solving individual games in isolation. However, in practice, players often encounter a distribution of similar but distinct games. For example, when trading correlated assets on the stock market, or when refining the strategy in subgames of a much larger game. Recently, offline meta-learning was used to accelerate one-sided equilibrium finding on such distributions. We build upon this, extending the framework to the more challenging self-play setting, which is the basis for most state-of-the-art equilibrium approximation algorithms for domains at scale. When selecting the strategy, our method uniquely integrates information across all decision states, promoting global communication as opposed to the traditional local regret decomposition. Empirical evaluation on normal-form games and river poker subgames shows our meta-learned algorithms considerably outperform other state-of-the-art regret minimization algorithms.","authors":["David Sychrovsk\\'y","Martin Schmid","Michal \\v{S}ustr","Michael Bowling"],"url":"https://arxiv.org/abs/2504.18917"}
{"created":"2025-04-29","title":"Clinical knowledge in LLMs does not translate to human interactions","abstract":"Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.","authors":["Andrew M. Bean","Rebecca Payne","Guy Parsons","Hannah Rose Kirk","Juan Ciro","Rafael Mosquera","Sara Hincapi\\'e Monsalve","Aruna S. Ekanayaka","Lionel Tarassenko","Luc Rocher","Adam Mahdi"],"url":"https://arxiv.org/abs/2504.18919"}
{"created":"2025-04-29","title":"The Algebra of Patterns (Extended Version)","abstract":"Pattern matching is a popular feature in functional, imperative and object-oriented programming languages. Language designers should therefore invest effort in a good design for pattern matching. Most languages choose a first-match semantics for pattern matching; that is, clauses are tried in the order in which they appear in the program until the first one matches. As a consequence, the order in which the clauses appear cannot be arbitrarily changed, which results in a less declarative programming model. The declarative alternative to this is an order-independent semantics for pattern matching, which is not implemented in most programming languages since it requires more verbose patterns. The reason for this verbosity is that the syntax of patterns is usually not expressive enough to express the complement of a pattern. In this paper, we show a principled way to make order-independent pattern matching practical. Our solution consists of two parts: First, we introduce a boolean algebra of patterns which can express the complement of a pattern. Second, we introduce default clauses to pattern matches. These default clauses capture the essential idea of a fallthrough case without sacrificing the property of order-independence.","authors":["David Binder","Lean Ermantraut"],"url":"https://arxiv.org/abs/2504.18920"}
{"created":"2025-04-29","title":"State Reconstruction Under Malicious Sensor Attacks","abstract":"This paper considers the state reconstruction problem for discrete-time cyber-physical systems when some of the sensors can be arbitrarily corrupted by malicious attacks where the attacked sensors belong to an unknown set. We first prove that the state is $s$-error correctable if the system under consideration is $s$-sparse observable where $s$ denotes the maximum number of attacked sensors. Then, two state reconstruction methods are presented where the first method is based on searching elements with the same value in a set and the second method is developed in terms of searching element satisfying a given condition. In addition, after establishing and analyzing the conditions that the proposed state reconstruction methods are not effective, we address that it is very hard to prevent the state reconstruction when either state reconstruction method proposed in this paper is used. The correctness and effectiveness of the proposed methods are examined via an example of four-dimensional dynamic systems and a real-world example of three-inertia systems.","authors":["Wei Liu"],"url":"https://arxiv.org/abs/2504.18921"}
{"created":"2025-04-29","title":"4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression","abstract":"Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints.Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively.Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding.Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook.By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.","authors":["Zicong Chen","Zhenghao Chen","Wei Jiang","Wei Wang","Lei Liu","Dong Xu"],"url":"https://arxiv.org/abs/2504.18925"}
{"created":"2025-04-29","title":"Revisiting Transformers through the Lens of Low Entropy and Dynamic Sparsity","abstract":"Compression has been a critical lens to understand the success of Transformers. In the past, we have typically taken the target distribution as a criterion to evaluate a model's compression performance. Nevertheless,it often remains challenging to precisely assess how well the model achieves compression and to compare the information content of the learned distribution with that of the target distribution during compression,as the target distribution is typically unknown and entropy computation often incurs exponential cost. In this work, we explore these issues under a controlled experimental setup. We find that Transformers exhibit a unique inductive bias in data compression: beyond approaching the target distribution, they tend to favor learning lower-entropy distributions, with this tendency becoming more pronounced as the model size increases. This preference prevents Transformers from perfectly aligning with the target distribution, instead further compressing its information content. Furthermore, we show that the FFN module plays a critical role in driving this bias. In addition, while models remove informational redundancy from data during compression, they also exhibit redundancy within their parameters, which enables compression and can be characterized through dynamic sparsity. However, the dynamic sparsity patterns in Transformers, particularly in attention and FFN modules, demand further exploration. As for this, we show that larger Transformers show stronger preferences for bypassing attention computations via residual connections and have lower proportion of active neurons. Interestingly, we also find that training instability in larger models strongly correlates with sudden increases in dead neurons. Our work contributes to a deeper understanding of Transformers from the lens of entropy and dynamic sparsity.","authors":["Ruifeng Ren","Yong Liu"],"url":"https://arxiv.org/abs/2504.18929"}
{"created":"2025-04-29","title":"Advanced Longitudinal Control and Collision Avoidance for High-Risk Edge Cases in Autonomous Driving","abstract":"Advanced Driver Assistance Systems (ADAS) and Advanced Driving Systems (ADS) are key to improving road safety, yet most existing implementations focus primarily on the vehicle ahead, neglecting the behavior of following vehicles. This shortfall often leads to chain reaction collisions in high speed, densely spaced traffic particularly when a middle vehicle suddenly brakes and trailing vehicles cannot respond in time. To address this critical gap, we propose a novel longitudinal control and collision avoidance algorithm that integrates adaptive cruising with emergency braking. Leveraging deep reinforcement learning, our method simultaneously accounts for both leading and following vehicles. Through a data preprocessing framework that calibrates real-world sensor data, we enhance the robustness and reliability of the training process, ensuring the learned policy can handle diverse driving conditions. In simulated high risk scenarios (e.g., emergency braking in dense traffic), the algorithm effectively prevents potential pile up collisions, even in situations involving heavy duty vehicles. Furthermore, in typical highway scenarios where three vehicles decelerate, the proposed DRL approach achieves a 99% success rate far surpassing the standard Federal Highway Administration speed concepts guide, which reaches only 36.77% success under the same conditions.","authors":["Dianwei Chen","Yaobang Gong","Xianfeng Yang"],"url":"https://arxiv.org/abs/2504.18931"}
{"created":"2025-04-29","title":"AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression","abstract":"Recent advancements in LLMs enable chatbots to interact with individuals on a range of queries, including sensitive mental health contexts. Despite uncertainties about their effectiveness and reliability, the development of LLMs in these areas is growing, potentially leading to harms. To better identify and mitigate these harms, it is critical to understand how the values of people with lived experiences relate to the harms. In this study, we developed a technology probe, a GPT-4o based chatbot called Zenny, enabling participants to engage with depression self-management scenarios informed by previous research. We used Zenny to interview 17 individuals with lived experiences of depression. Our thematic analysis revealed key values: informational support, emotional support, personalization, privacy, and crisis management. This work explores the relationship between lived experience values, potential harms, and design recommendations for mental health AI chatbots, aiming to enhance self-management support while minimizing risks.","authors":["Dong Whi Yoo","Jiayue Melissa Shi","Violeta J. Rodriguez","Koustuv Saha"],"url":"https://arxiv.org/abs/2504.18932"}
{"created":"2025-04-29","title":"MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction","abstract":"Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens in sentences. While Large Language Models (LLMs) have shown remarkable success in identifying and rectifying potential errors, they often struggle with maintaining consistent output lengths and adapting to domain-specific corrections. Furthermore, existing CSC task impose rigid constraints requiring input and output lengths to be identical, limiting their applicability. In this work, we extend traditional CSC to variable-length correction scenarios, including Chinese Splitting Error Correction (CSEC) and ASR N-best Error Correction. To address domain adaptation and length consistency, we propose MTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection mechanism. Our approach constructs a retrieval database from domain-specific training data and dictionaries, fine-tuning retrievers to optimize performance for error-containing inputs. Additionally, we introduce a multi-source combination strategy with iterative length reflection to ensure output length fidelity. Experiments across diverse domain datasets demonstrate that our method significantly outperforms current approaches in correction quality, particularly in handling domain-specific and variable-length error correction tasks.","authors":["Junhong Liang","Yu Zhou"],"url":"https://arxiv.org/abs/2504.18938"}
{"created":"2025-04-29","title":"Federated Learning-based Semantic Segmentation for Lane and Object Detection in Autonomous Driving","abstract":"Autonomous Vehicles (AVs) require precise lane and object detection to ensure safe navigation. However, centralized deep learning (DL) approaches for semantic segmentation raise privacy and scalability challenges, particularly when handling sensitive data. This research presents a new federated learning (FL) framework that integrates secure deep Convolutional Neural Networks (CNNs) and Differential Privacy (DP) to address these issues. The core contribution of this work involves: (1) developing a new hybrid UNet-ResNet34 architecture for centralized semantic segmentation to achieve high accuracy and tackle privacy concerns due to centralized training, and (2) implementing the privacy-preserving FL model, distributed across AVs to enhance performance through secure CNNs and DP mechanisms. In the proposed FL framework, the methodology distinguishes itself from the existing approach through the following: (a) ensuring data decentralization through FL to uphold user privacy by eliminating the need for centralized data aggregation, (b) integrating DP mechanisms to secure sensitive model updates against potential adversarial inference attacks, and (c) evaluating the frameworks performance and generalizability using RGB and semantic segmentation datasets derived from the CARLA simulator. Experimental results show significant improvements in accuracy, from 81.5% to 88.7% for the RGB dataset and from 79.3% to 86.9% for the SEG dataset over 20 to 70 Communication Rounds (CRs). Global loss was reduced by over 60%, and minor accuracy trade-offs from DP were observed. This study contributes by offering a scalable, privacy-preserving FL framework tailored for AVs, optimizing communication efficiency while balancing performance and data security.","authors":["Gharbi Khamis Alshammari","Ahmad Abubakar","Nada M. O. Sid Ahmed","Naif Khalaf Alshammari"],"url":"https://arxiv.org/abs/2504.18939"}
{"created":"2025-04-29","title":"LawFlow : Collecting and Simulating Lawyers' Thought Processes","abstract":"Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, we propose a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).","authors":["Debarati Das","Khanh Chi Le","Ritik Sachin Parkar","Karin De Langis","Brendan Madson","Chad M. Berryman","Robin M. Willis","Daniel H. Moses","Brett McDonnell","Daniel Schwarcz","Dongyeop Kang"],"url":"https://arxiv.org/abs/2504.18942"}
{"created":"2025-04-29","title":"GPU accelerated program synthesis: Enumerate semantics, not syntax!","abstract":"Program synthesis is an umbrella term for generating programs and logical formulae from specifications. With the remarkable performance improvements that GPUs enable for deep learning, a natural question arose: can we also implement a search-based program synthesiser on GPUs to achieve similar performance improvements? In this article we discuss our insights on this question, based on recent works~. The goal is to build a synthesiser running on GPUs which takes as input positive and negative example traces and returns a logical formula accepting the positive and rejecting the negative traces. With GPU-friendly programming techniques -- using the semantics of formulae to minimise data movement and reduce data-dependent branching -- our synthesiser scales to significantly larger synthesis problems, and operates much faster than the previous CPU-based state-of-the-art. We believe the insights that make our approach GPU-friendly have wide potential for enhancing the performance of other formal methods (FM) workloads.","authors":["Martin Berger","Nathana\\\"el Fijalkow","Mojtaba Valizadeh"],"url":"https://arxiv.org/abs/2504.18943"}
{"created":"2025-04-29","title":"Demonstrating DVS: Dynamic Virtual-Real Simulation Platform for Mobile Robotic Tasks","abstract":"With the development of embodied artificial intelligence, robotic research has increasingly focused on complex tasks. Existing simulation platforms, however, are often limited to idealized environments, simple task scenarios and lack data interoperability. This restricts task decomposition and multi-task learning. Additionally, current simulation platforms face challenges in dynamic pedestrian modeling, scene editability, and synchronization between virtual and real assets. These limitations hinder real world robot deployment and feedback. To address these challenges, we propose DVS (Dynamic Virtual-Real Simulation Platform), a platform for dynamic virtual-real synchronization in mobile robotic tasks. DVS integrates a random pedestrian behavior modeling plugin and large-scale, customizable indoor scenes for generating annotated training datasets. It features an optical motion capture system, synchronizing object poses and coordinates between virtual and real world to support dynamic task benchmarking. Experimental validation shows that DVS supports tasks such as pedestrian trajectory prediction, robot path planning, and robotic arm grasping, with potential for both simulation and real world deployment. In this way, DVS represents more than just a versatile robotic platform; it paves the way for research in human intervention in robot execution tasks and real-time feedback algorithms in virtual-real fusion environments. More information about the simulation platform is available on https://immvlab.github.io/DVS/.","authors":["Zijie Zheng","Zeshun Li","Yunpeng Wang","Qinghongbing Xie","Long Zeng"],"url":"https://arxiv.org/abs/2504.18944"}
{"created":"2025-04-29","title":"Generative AI in Embodied Systems: System-Level Analysis of Performance, Efficiency and Scalability","abstract":"Embodied systems, where generative autonomous agents engage with the physical world through integrated perception, cognition, action, and advanced reasoning powered by large language models (LLMs), hold immense potential for addressing complex, long-horizon, multi-objective tasks in real-world environments. However, deploying these systems remains challenging due to prolonged runtime latency, limited scalability, and heightened sensitivity, leading to significant system inefficiencies.","authors":["Zishen Wan","Jiayi Qian","Yuhang Du","Jason Jabbour","Yilun Du","Yang Katie Zhao","Arijit Raychowdhury","Tushar Krishna","Vijay Janapa Reddi"],"url":"https://arxiv.org/abs/2504.18945"}
{"created":"2025-04-29","title":"Use of Metric Learning for the Recognition of Handwritten Digits, and its Application to Increase the Outreach of Voice-based Communication Platforms","abstract":"Initiation, monitoring, and evaluation of development programmes can involve field-based data collection about project activities. This data collection through digital devices may not always be feasible though, for reasons such as unaffordability of smartphones and tablets by field-based cadre, or shortfalls in their training and capacity building. Paper-based data collection has been argued to be more appropriate in several contexts, with automated digitization of the paper forms through OCR (Optical Character Recognition) and OMR (Optical Mark Recognition) techniques. We contribute with providing a large dataset of handwritten digits, and deep learning based models and methods built using this data, that are effective in real-world environments. We demonstrate the deployment of these tools in the context of a maternal and child health and nutrition awareness project, which uses IVR (Interactive Voice Response) systems to provide awareness information to rural women SHG (Self Help Group) members in north India. Paper forms were used to collect phone numbers of the SHG members at scale, which were digitized using the OCR tools developed by us, and used to push almost 4 million phone calls. The data, model, and code have been released in the open-source domain.","authors":["Devesh Pant","Dibyendu Talukder","Deepak Kumar","Rachit Pandey","Aaditeshwar Seth","Chetan Arora"],"url":"https://arxiv.org/abs/2504.18948"}
{"created":"2025-04-29","title":"Speaker Retrieval in the Wild: Challenges, Effectiveness and Robustness","abstract":"There is a growing abundance of publicly available or company-owned audio/video archives, highlighting the increasing importance of efficient access to desired content and information retrieval from these archives. This paper investigates the challenges, solutions, effectiveness, and robustness of speaker retrieval systems developed \"in the wild\" which involves addressing two primary challenges: extraction of task-relevant labels from limited metadata for system development and evaluation, as well as the unconstrained acoustic conditions encountered in the archive, ranging from quiet studios to adverse noisy environments. While we focus on the publicly-available BBC Rewind archive (spanning 1948 to 1979), our framework addresses the broader issue of speaker retrieval on extensive and possibly aged archives with no control over the content and acoustic conditions. Typically, these archives offer a brief and general file description, mostly inadequate for specific applications like speaker retrieval, and manual annotation of such large-scale archives is unfeasible. We explore various aspects of system development (e.g., speaker diarisation, embedding extraction, query selection) and analyse the challenges, possible solutions, and their functionality. To evaluate the performance, we conduct systematic experiments in both clean setup and against various distortions simulating real-world applications. Our findings demonstrate the effectiveness and robustness of the developed speaker retrieval systems, establishing the versatility and scalability of the proposed framework for a wide range of applications beyond the BBC Rewind corpus.","authors":["Erfan Loweimi","Mengjie Qian","Kate Knill","Mark Gales"],"url":"https://arxiv.org/abs/2504.18950"}
{"created":"2025-04-29","title":"A Quadratic Programming Approach to Flight Envelope Protection Using Control Barrier Functions","abstract":"Ensuring the safe operation of aerospace systems within their prescribed flight envelope is a fundamental requirement for modern flight control systems. Flight envelope protection prevents violations of aerodynamic, structural, and performance constraints, mitigating risks such as stall, excessive loads, and loss of control. Conventional FEP approaches, such as reference clipping via saturation functions and model-based command filtering, impose constraints at the reference input level but often fail to account for closed-loop system dynamics, potentially leading to constraint violations during transients. This paper introduces a new approach to the flight envelope protection problem by employing a quadratic programming-based safety filter using control barrier functions to dynamically enforce flight envelope constraints while preserving control performance. Unlike traditional reference filtering methods, the control barrier function-based safety filter actively ensures strict forward invariance of the safe flight envelope set, integrating seamlessly with existing control architectures. The proposed framework is implemented in a nonlinear missile flight control system and evaluated in a simulated environment. The results demonstrate its ability to prevent constraint violations while minimizing conservatism, offering a robust alternative to existing flight envelope protection methodologies.","authors":["Johannes Autenrieb"],"url":"https://arxiv.org/abs/2504.18951"}
{"created":"2025-04-29","title":"Application of the Brain Drain Optimization Algorithm to the N-Queens Problem","abstract":"This paper introduces the application of the Brain Drain Optimization algorithm -- a swarm-based metaheuristic inspired by the emigration of intellectual elites -- to the N-Queens problem. The N-Queens problem, a classic combinatorial optimization problem, serves as a challenge for applying the BRADO. A designed cost function guides the search, and the configurations are tuned using a TOPSIS-based multicriteria decision making process. BRADO consistently outperforms alternatives in terms of solution quality, achieving fewer threats and better objective function values. To assess BRADO's efficacy, it is benchmarked against several established metaheuristic algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Imperialist Competitive Algorithm (ICA), Iterated Local Search (ILS), and basic Local Search (LS). The study highlights BRADO's potential as a general-purpose solver for combinatorial problems, opening pathways for future applications in other domains of artificial intelligence.","authors":["Sahar Ramezani Jolfaei","Sepehr Khodadadi Hossein Abadi"],"url":"https://arxiv.org/abs/2504.18953"}
{"created":"2025-04-29","title":"Towards Automated Detection of Inline Code Comment Smells","abstract":"Code comments are important in software development because they directly influence software maintainability and overall quality. Bad practices of code comments lead to code comment smells, negatively impacting software maintenance. Recent research has been conducted on classifying inline code comment smells, yet automatically detecting these still remains a challenge. We aim to automatically detect and classify inline code comment smells through machine learning (ML) models and a large language model (LLM) to determine how accurately each smell type can be detected. We enhanced a previously labeled dataset, where comments are labeled according to a determined taxonomy, by augmenting it with additional code segments and their associated comments. GPT 4, a large language model, was used to classify code comment smells on both the original and augmented datasets to evaluate its performance. In parallel, we trained and tested seven different machine learning algorithms on the augmented dataset to compare their classification performance against GPT 4. The performance of models, particularly Random Forest, which achieved an overall accuracy of 69 percent, along with Gradient Boosting and Logistic Regression, each achieving 66 percent and 65 percent, respectively, establishes a solid baseline for future research in this domain. The Random Forest model outperformed all other ML models, by achieving the highest Matthews Correlation Coefficient (MCC) score of 0.44. The augmented dataset improved the overall classification accuracy of the GPT 4 model predictions from 34 percent to 55 percent. This study contributes to software maintainability by exploring the automatic detection and classification of inline code comment smells. We have made our augmented dataset and code artifacts available online, offering a valuable resource for developing automated comment smell detection tools.","authors":["Ipek Oztas","U Boran Torun","Eray T\\\"uz\\\"un"],"url":"https://arxiv.org/abs/2504.18956"}
{"created":"2025-04-29","title":"Deep Reinforcement Learning for MIMO Communication with Low-Resolution ADCs","abstract":"Multiple-input multiple-output (MIMO) wireless systems conventionally use high-resolution analog-to-digital converters (ADCs) at the receiver side to faithfully digitize received signals prior to digital signal processing. However, the power consumption of ADCs increases significantly as the bandwidth is increased, particularly in millimeter wave communications systems. A combination of two mitigating approaches has been considered in the literature: i) to use hybrid beamforming to reduce the number of ADCs, and ii) to use low-resolution ADCs to reduce per ADC power consumption. Lowering the number and resolution of the ADCs naturally reduces the communication rate of the system, leading to a tradeoff between ADC power consumption and communication rate. Prior works have shown that optimizing over the hybrid beamforming matrix and ADC thresholds may reduce the aforementioned rate-loss significantly. A key challenge is the complexity of optimization over all choices of beamforming matrices and threshold vectors. This work proposes a reinforcement learning (RL) architecture to perform the optimization. The proposed approach integrates deep neural network-based mutual information estimators for reward calculation with policy gradient methods for reinforcement learning. The approach is robust to dynamic channel statistics and noisy CSI estimates. It is shown theoretically that greedy RL methods converge to the globally optimal policy. Extensive empirical evaluations are provided demonstrating that the performance of the RL-based approach closely matches exhaustive search optimization across the solution space.","authors":["Marian Temprana Alonso","Dongsheng Luo","Farhad Shirani"],"url":"https://arxiv.org/abs/2504.18957"}
{"created":"2025-04-29","title":"R-Sparse R-CNN: SAR Ship Detection Based on Background-Aware Sparse Learnable Proposals","abstract":"We introduce R-Sparse R-CNN, a novel pipeline for oriented ship detection in Synthetic Aperture Radar (SAR) images that leverages sparse learnable proposals enriched with background contextual information, termed background-aware proposals (BAPs). The adoption of sparse proposals streamlines the pipeline by eliminating the need for proposal generators and post-processing for overlapping predictions. The proposed BAPs enrich object representation by integrating ship and background features, allowing the model to learn their contextual relationships for more accurate distinction of ships in complex environments. To complement BAPs, we propose Dual-Context Pooling (DCP), a novel strategy that jointly extracts ship and background features in a single unified operation. This unified design improves efficiency by eliminating redundant computation inherent in separate pooling. Moreover, by ensuring that ship and background features are pooled from the same feature map level, DCP provides aligned features that improve contextual relationship learning. Finally, as a core component of contextual relationship learning in R-Sparse R-CNN, we design a dedicated transformer-based Interaction Module. This module interacts pooled ship and background features with corresponding proposal features and models their relationships. Experimental results show that R-Sparse R-CNN delivers outstanding accuracy, surpassing state-of-the-art models by margins of up to 12.8% and 11.9% on SSDD and RSDD-SAR inshore datasets, respectively. These results demonstrate the effectiveness and competitiveness of R-Sparse R-CNN as a robust framework for oriented ship detection in SAR imagery. The code is available at: www.github.com/ka-mirul/R-Sparse-R-CNN.","authors":["Kamirul Kamirul","Odysseas Pappas","Alin Achim"],"url":"https://arxiv.org/abs/2504.18959"}
{"created":"2025-04-29","title":"Feature Fusion Revisited: Multimodal CTR Prediction for MMCTR Challenge","abstract":"With the rapid advancement of Multimodal Large Language Models (MLLMs), an increasing number of researchers are exploring their application in recommendation systems. However, the high latency associated with large models presents a significant challenge for such use cases. The EReL@MIR workshop provided a valuable opportunity to experiment with various approaches aimed at improving the efficiency of multimodal representation learning for information retrieval tasks. As part of the competition's requirements, participants were mandated to submit a technical report detailing their methodologies and findings. Our team was honored to receive the award for Task 2 - Winner (Multimodal CTR Prediction). In this technical report, we present our methods and key findings. Additionally, we propose several directions for future work, particularly focusing on how to effectively integrate recommendation signals into multimodal representations. The codebase for our implementation is publicly available at: https://github.com/Lattice-zjj/MMCTR_Code, and the trained model weights can be accessed at: https://huggingface.co/FireFlyCourageous/MMCTR_DIN_MicroLens_1M_x1.","authors":["Junjie Zhou"],"url":"https://arxiv.org/abs/2504.18961"}
{"created":"2025-04-29","title":"Redefining Hybrid Blockchains: A Balanced Architecture","abstract":"Blockchain technology has completely revolutionized the field of decentralized finance with the emergence of a variety of cryptocurrencies and digital assets. However, widespread adoption of this technology by governments and enterprises has been limited by concerns regarding the technology's scalability, governance, and economic sustainability. This paper aims to introduce a novel hybrid blockchain architecture that balances scalability, governance, and decentralization while being economically viable for all parties involved. The new semi-centralized model leverages strategies not prevalent in the field, such as resource and node isolation, containerization, separation of networking and compute layers, use of a Kafka pub-sub network instead of a peer-to-peer network, and stakes-based validator selection to possibly mitigate a variety of issues related to scalability, security, governance, and economic sustainability. Simulations conducted on Kubernetes demonstrate the architecture's ability to achieve over 1000 transactions per second, with consistent performance across scaled deployments, even on a lightweight consumer-grade laptop with resource constraints. The findings highlight the system's scalability, security, and economic viability, offering a robust framework for enterprise and government adoption.","authors":["Syed Ibrahim Omer"],"url":"https://arxiv.org/abs/2504.18966"}
{"created":"2025-04-29","title":"Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)","abstract":"Emotion recognition has the potential to play a pivotal role in enhancing human-computer interaction by enabling systems to accurately interpret and respond to human affect. Yet, capturing emotions in face-to-face contexts remains challenging due to subtle nonverbal cues, variations in personal traits, and the real-time dynamics of genuine interactions. Existing emotion recognition datasets often rely on limited modalities or controlled conditions, thereby missing the richness and variability found in real-world scenarios.","authors":["Meisam J. Sekiavandi","Laurits Dixen","Jostein Fimland","Sree Keerthi Desu","Antonia-Bianca Zserai","Ye Sul Lee","Maria Barrett","Paolo Burre"],"url":"https://arxiv.org/abs/2504.18969"}
{"created":"2025-04-29","title":"Secret Sharing for DNA Probability Vectors","abstract":"Emerging DNA storage technologies use composite DNA letters, where information is represented by a probability vector, leading to higher information density and lower synthesis costs. However, it faces the problem of information leakage in sharing the DNA vessels among untrusted vendors. This paper introduces an asymptotic ramp secret sharing scheme (ARSSS) for secret information storage using composite DNA letters. This innovative scheme, inspired by secret sharing methods over finite fields and enhanced with a modified matrix-vector multiplication operation for probability vectors, achieves asymptotic information-theoretic data security for a large alphabet size. Moreover, this scheme reduces the number of reading operations for DNA samples compared to traditional schemes, and therefore lowers the complexity and the cost of DNA-based secret sharing. We further explore the construction of the scheme, starting with a proof of the existence of a suitable generator, followed by practical examples. Finally, we demonstrate efficient constructions to support large information sizes, which utilize multiple vessels for each secret share rather than a single vessel.","authors":["Wenkai Zhang","Zhiying Wang"],"url":"https://arxiv.org/abs/2504.18970"}
{"created":"2025-04-29","title":"Scientific Open-Source Software Is Less Likely to Become Abandoned Than One Might Think! Lessons from Curating a Catalog of Maintained Scientific Software","abstract":"Scientific software is essential to scientific innovation and in many ways it is distinct from other types of software. Abandoned (or unmaintained), buggy, and hard to use software, a perception often associated with scientific software can hinder scientific progress, yet, in contrast to other types of software, its longevity is poorly understood. Existing data curation efforts are fragmented by science domain and/or are small in scale and lack key attributes. We use large language models to classify public software repositories in World of Code into distinct scientific domains and layers of the software stack, curating a large and diverse collection of over 18,000 scientific software projects. Using this data, we estimate survival models to understand how the domain, infrastructural layer, and other attributes of scientific software affect its longevity. We further obtain a matched sample of non-scientific software repositories and investigate the differences. We find that infrastructural layers, downstream dependencies, mentions of publications, and participants from government are associated with a longer lifespan, while newer projects with participants from academia had shorter lifespan. Against common expectations, scientific projects have a longer lifetime than matched non-scientific open-source software projects. We expect our curated attribute-rich collection to support future research on scientific software and provide insights that may help extend longevity of both scientific and other projects.","authors":["Addi Malviya Thakur","Reed Milewicz","Mahmoud Jahanshahi","Lav\\'inia Paganini","Bogdan Vasilescu","Audris Mockus"],"url":"https://arxiv.org/abs/2504.18971"}
{"created":"2025-04-29","title":"SONNI: Secure Oblivious Neural Network Inference","abstract":"In the standard privacy-preserving Machine learning as-a-service (MLaaS) model, the client encrypts data using homomorphic encryption and uploads it to a server for computation. The result is then sent back to the client for decryption. It has become more and more common for the computation to be outsourced to third-party servers. In this paper we identify a weakness in this protocol that enables a completely undetectable novel model-stealing attack that we call the Silver Platter attack. This attack works even under multikey encryption that prevents a simple collusion attack to steal model parameters. We also propose a mitigation that protects privacy even in the presence of a malicious server and malicious client or model provider (majority dishonest). When compared to a state-of-the-art but small encrypted model with 32k parameters, we preserve privacy with a failure chance of 1.51 x 10^-28 while batching capability is reduced by 0.2%. Our approach uses a novel results-checking protocol that ensures the computation was performed correctly without violating honest clients' data privacy. Even with collusion between the client and the server, they are unable to steal model parameters. Additionally, the model provider cannot learn any client data if maliciously working with the server.","authors":["Luke Sperling","Sandeep S. Kulkarni"],"url":"https://arxiv.org/abs/2504.18974"}
{"created":"2025-04-29","title":"3DPyranet Features Fusion for Spatio-temporal Feature Learning","abstract":"Convolutional neural network (CNN) slides a kernel over the whole image to produce an output map. This kernel scheme reduces the number of parameters with respect to a fully connected neural network (NN). While CNN has proven to be an effective model in recognition of handwritten characters and traffic signal sign boards, etc. recently, its deep variants have proven to be effective in similar as well as more challenging applications like object, scene and action recognition. Deep CNN add more layers and kernels to the classical CNN, increasing the number of parameters, and partly reducing the main advantage of CNN which is less parameters. In this paper, a 3D pyramidal neural network called 3DPyraNet and a discriminative approach for spatio-temporal feature learning based on it, called 3DPyraNet-F, are proposed. 3DPyraNet introduces a new weighting scheme which learns features from both spatial and temporal dimensions analyzing multiple adjacent frames and keeping a biological plausible structure. It keeps the spatial topology of the input image and presents fewer parameters and lower computational and memory costs compared to both fully connected NNs and recent deep CNNs. 3DPyraNet-F extract the features maps of the highest layer of the learned network, fuse them in a single vector, and provide it as input in such a way to a linear-SVM classifier that enhances the recognition of human actions and dynamic scenes from the videos. Encouraging results are reported with 3DPyraNet in real-world environments, especially in the presence of camera induced motion. Further, 3DPyraNet-F clearly outperforms the state-of-the-art on three benchmark datasets and shows comparable result for the fourth.","authors":["Ihsan Ullah","Alfredo Petrosino"],"url":"https://arxiv.org/abs/2504.18977"}
{"created":"2025-04-29","title":"A biconvex method for minimum-time motion planning through sequences of convex sets","abstract":"We consider the problem of designing a smooth trajectory that traverses a sequence of convex sets in minimum time, while satisfying given velocity and acceleration constraints. This problem is naturally formulated as a nonconvex program. To solve it, we propose a biconvex method that quickly produces an initial trajectory and iteratively refines it by solving two convex subproblems in alternation. This method is guaranteed to converge, returns a feasible trajectory even if stopped early, and does not require the selection of any line-search or trust-region parameter. Exhaustive experiments show that our method finds high-quality trajectories in a fraction of the time of state-of-the-art solvers for nonconvex optimization. In addition, it achieves runtimes comparable to industry-standard waypoint-based motion planners, while consistently designing lower-duration trajectories than existing optimization-based planners.","authors":["Tobia Marcucci","Mathew Halm","Will Yang","Dongchan Lee","Andrew D. Marchese"],"url":"https://arxiv.org/abs/2504.18978"}
{"created":"2025-04-29","title":"Beyond Performance: Measuring the Environmental Impact of Analytical Databases","abstract":"The exponential growth of data is making query processing increasingly critical for modern computing infrastructure, yet the environmental impact of database operations remains poorly understood and largely overlooked. This paper presents ATLAS, a comprehensive methodology for measuring and quantifying the environmental footprint of analytical database systems, considering both operational impacts and manufacturing costs of hardware components. Through extensive empirical evaluation of four distinct database architectures (DuckDB, MonetDB, Hyper, and StarRocks), we uncover how fundamental architectural decisions affect environmental efficiency. Our findings reveal that environmental considerations in database operations are multifaceted, encompassing both immediate operational impacts and long-term sustainability implications. We demonstrate that architectural choices can significantly influence both power consumption and environmental sustainability, while deployment location emerges as a critical factor that can amplify or diminish these architectural advantages.","authors":["Michail Bachras","Hans-Arno Jacobsen"],"url":"https://arxiv.org/abs/2504.18980"}
{"created":"2025-04-29","title":"MediAug: Exploring Visual Augmentation in Medical Imaging","abstract":"Data augmentation is essential in medical imaging for improving classification accuracy, lesion detection, and organ segmentation under limited data conditions. However, two significant challenges remain. First, a pronounced domain gap between natural photographs and medical images can distort critical disease features. Second, augmentation studies in medical imaging are fragmented and limited to single tasks or architectures, leaving the benefits of advanced mix-based strategies unclear. To address these challenges, we propose a unified evaluation framework with six mix-based augmentation methods integrated with both convolutional and transformer backbones on brain tumour MRI and eye disease fundus datasets. Our contributions are threefold. (1) We introduce MediAug, a comprehensive and reproducible benchmark for advanced data augmentation in medical imaging. (2) We systematically evaluate MixUp, YOCO, CropMix, CutMix, AugMix, and SnapMix with ResNet-50 and ViT-B backbones. (3) We demonstrate through extensive experiments that MixUp yields the greatest improvement on the brain tumor classification task for ResNet-50 with 79.19% accuracy and SnapMix yields the greatest improvement for ViT-B with 99.44% accuracy, and that YOCO yields the greatest improvement on the eye disease classification task for ResNet-50 with 91.60% accuracy and CutMix yields the greatest improvement for ViT-B with 97.94% accuracy. Code will be available at https://github.com/AIGeeksGroup/MediAug.","authors":["Xuyin Qi","Zeyu Zhang","Canxuan Gang","Hao Zhang","Lei Zhang","Zhiwei Zhang","Yang Zhao"],"url":"https://arxiv.org/abs/2504.18983"}
{"created":"2025-04-29","title":"Tracking the Moving Target: A Framework for Continuous Evaluation of LLM Test Generation in Industry","abstract":"Large Language Models (LLMs) have shown great potential in automating software testing tasks, including test generation. However, their rapid evolution poses a critical challenge for companies implementing DevSecOps - evaluations of their effectiveness quickly become outdated, making it difficult to assess their reliability for production use. While academic research has extensively studied LLM-based test generation, evaluations typically provide point-in-time analyses using academic benchmarks. Such evaluations do not address the practical needs of companies who must continuously assess tool reliability and integration with existing development practices. This work presents a measurement framework for the continuous evaluation of commercial LLM test generators in industrial environments. We demonstrate its effectiveness through a longitudinal study at LKS Next. The framework integrates with industry-standard tools like SonarQube and provides metrics that evaluate both technical adequacy (e.g., test coverage) and practical considerations (e.g., maintainability or expert assessment). Our methodology incorporates strategies for test case selection, prompt engineering, and measurement infrastructure, addressing challenges such as data leakage and reproducibility. Results highlight both the rapid evolution of LLM capabilities and critical factors for successful industrial adoption, offering practical guidance for companies seeking to integrate these technologies into their development pipelines.","authors":["Maider Azanza","Beatriz P\\'erez Lamancha","Eneko Pizarro"],"url":"https://arxiv.org/abs/2504.18985"}
{"created":"2025-04-29","title":"LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings","abstract":"Collaborative research often includes contributors with varied perspectives from diverse linguistic backgrounds. However, English as a Second Language (ESL) researchers often struggle to communicate during meetings in English and comprehend discussions, leading to limited contribution. To investigate these challenges, we surveyed 64 ESL researchers who frequently collaborate in multilingual teams and identified four key design goals around participation, comprehension, documentation, and feedback. Guided by these design goals, we developed LINC, a multimodal Language INdependent Collaboration system with two components: a real-time module for multilingual communication during meetings and a post-meeting dashboard for discussion analysis. We evaluated the system through a two-phased study with six triads of multilingual teams. We found that using LINC, participants benefited from communicating in their preferred language, recalled and reviewed actionable insights, and prepared for upcoming meetings effectively. We discuss external factors that impact multilingual meeting participation beyond language preferences and the implications of multimodal systems in facilitating meetings in hybrid multilingual collaborative settings beyond research.","authors":["Saramsh Gautam","Mahmood Jasim"],"url":"https://arxiv.org/abs/2504.18988"}
{"created":"2025-04-29","title":"REED-VAE: RE-Encode Decode Training for Iterative Image Editing with Diffusion Models","abstract":"While latent diffusion models achieve impressive image editing results, their application to iterative editing of the same image is severely restricted. When trying to apply consecutive edit operations using current models, they accumulate artifacts and noise due to repeated transitions between pixel and latent spaces. Some methods have attempted to address this limitation by performing the entire edit chain within the latent space, sacrificing flexibility by supporting only a limited, predetermined set of diffusion editing operations. We present a RE-encode decode (REED) training scheme for variational autoencoders (VAEs), which promotes image quality preservation even after many iterations. Our work enables multi-method iterative image editing: users can perform a variety of iterative edit operations, with each operation building on the output of the previous one using both diffusion-based operations and conventional editing techniques. We demonstrate the advantage of REED-VAE across a range of image editing scenarios, including text-based and mask-based editing frameworks. In addition, we show how REED-VAE enhances the overall editability of images, increasing the likelihood of successful and precise edit operations. We hope that this work will serve as a benchmark for the newly introduced task of multi-method image editing. Our code and models will be available at https://github.com/galmog/REED-VAE","authors":["Gal Almog","Ariel Shamir","Ohad Fried"],"url":"https://arxiv.org/abs/2504.18989"}
{"created":"2025-04-29","title":"Safety Interventions against Adversarial Patches in an Open-Source Driver Assistance System","abstract":"Drivers are becoming increasingly reliant on advanced driver assistance systems (ADAS) as autonomous driving technology becomes more popular and developed with advanced safety features to enhance road safety. However, the increasing complexity of the ADAS makes autonomous vehicles (AVs) more exposed to attacks and accidental faults. In this paper, we evaluate the resilience of a widely used ADAS against safety-critical attacks that target perception inputs. Various safety mechanisms are simulated to assess their impact on mitigating attacks and enhancing ADAS resilience. Experimental results highlight the importance of timely intervention by human drivers and automated safety mechanisms in preventing accidents in both driving and lateral directions and the need to resolve conflicts among safety interventions to enhance system resilience and reliability.","authors":["Cheng Chen","Grant Xiao","Daehyun Lee","Lishan Yang","Evgenia Smirni","Homa Alemzadeh","Xugui Zhou"],"url":"https://arxiv.org/abs/2504.18990"}
{"created":"2025-04-29","title":"Dynamic Fisher-weighted Model Merging via Bayesian Optimization","abstract":"The fine-tuning of pre-trained language models has resulted in the widespread availability of task-specific models. Model merging offers an efficient way to create multi-task models by combining these fine-tuned models at the parameter level, without the need for training data or joint training on multiple datasets. Existing merging approaches typically involve scaling the parameters model-wise or integrating parameter importance parameter-wise. Both approaches exhibit their own weaknesses, leading to a notable performance gap compared to multi-task fine-tuning. In this paper, we unify these seemingly distinct strategies into a more general merging framework, and introduce Dynamic Fisher-weighted Merging (DF-Merge). Specifically, candidate models are associated with a set of coefficients that linearly scale their fine-tuned parameters. Bayesian optimization is applied to dynamically adjust these coefficients, aiming to maximize overall performance on validation sets. Each iteration of this process integrates parameter importance based on the Fisher information conditioned by the coefficients. Experimental results show that DF-Merge outperforms strong baselines across models of different sizes and a variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises from the unified view of merging and that near-optimal performance is achievable in a few iterations, even with minimal validation data.","authors":["Sanwoo Lee","Jiahao Liu","Qifan Wang","Jingang Wang","Xunliang Cai","Yunfang Wu"],"url":"https://arxiv.org/abs/2504.18992"}
{"created":"2025-04-29","title":"Unveiling and Mitigating Adversarial Vulnerabilities in Iterative Optimizers","abstract":"Machine learning (ML) models are often sensitive to carefully crafted yet seemingly unnoticeable perturbations. Such adversarial examples are considered to be a property of ML models, often associated with their black-box operation and sensitivity to features learned from data. This work examines the adversarial sensitivity of non-learned decision rules, and particularly of iterative optimizers. Our analysis is inspired by the recent developments in deep unfolding, which cast such optimizers as ML models. We show that non-learned iterative optimizers share the sensitivity to adversarial examples of ML models, and that attacking iterative optimizers effectively alters the optimization objective surface in a manner that modifies the minima sought. We then leverage the ability to cast iteration-limited optimizers as ML models to enhance robustness via adversarial training. For a class of proximal gradient optimizers, we rigorously prove how their learning affects adversarial sensitivity. We numerically back our findings, showing the vulnerability of various optimizers, as well as the robustness induced by unfolding and adversarial training.","authors":["Elad Sofer","Tomer Shaked","Caroline Chaux","Nir Shlezinger"],"url":"https://arxiv.org/abs/2504.19000"}
{"created":"2025-04-29","title":"Differentially Private Quasi-Concave Optimization: Bypassing the Lower Bound and Application to Geometric Problems","abstract":"We study the sample complexity of differentially private optimization of quasi-concave functions. For a fixed input domain $\\mathcal{X}$, Cohen et al. (STOC 2023) proved that any generic private optimizer for low sensitive quasi-concave functions must have sample complexity $\\Omega(2^{\\log^*|\\mathcal{X}|})$. We show that the lower bound can be bypassed for a series of ``natural'' problems. We define a new class of \\emph{approximated} quasi-concave functions, and present a generic differentially private optimizer for approximated quasi-concave functions with sample complexity $\\tilde{O}(\\log^*|\\mathcal{X}|)$. As applications, we use our optimizer to privately select a center point of points in $d$ dimensions and \\emph{probably approximately correct} (PAC) learn $d$-dimensional halfspaces. In previous works, Bun et al. (FOCS 2015) proved a lower bound of $\\Omega(\\log^*|\\mathcal{X}|)$ for both problems. Beimel et al. (COLT 2019) and Kaplan et al. (NeurIPS 2020) gave an upper bound of $\\tilde{O}(d^{2.5}\\cdot 2^{\\log^*|\\mathcal{X}|})$ for the two problems, respectively. We improve the dependency of the upper bounds on the cardinality of the domain by presenting a new upper bound of $\\tilde{O}(d^{5.5}\\cdot\\log^*|\\mathcal{X}|)$ for both problems. To the best of our understanding, this is the first work to reduce the sample complexity dependency on $|\\mathcal{X}|$ for these two problems from exponential in $\\log^* |\\mathcal{X}|$ to $\\log^* |\\mathcal{X}|$.","authors":["Kobbi Nissim","Eliad Tsfadia","Chao Yan"],"url":"https://arxiv.org/abs/2504.19001"}
{"created":"2025-04-29","title":"Deep Learning-Based Multi-Modal Fusion for Robust Robot Perception and Navigation","abstract":"This paper introduces a novel deep learning-based multimodal fusion architecture aimed at enhancing the perception capabilities of autonomous navigation robots in complex environments. By utilizing innovative feature extraction modules, adaptive fusion strategies, and time-series modeling mechanisms, the system effectively integrates RGB images and LiDAR data. The key contributions of this work are as follows: a. the design of a lightweight feature extraction network to enhance feature representation; b. the development of an adaptive weighted cross-modal fusion strategy to improve system robustness; and c. the incorporation of time-series information modeling to boost dynamic scene perception accuracy. Experimental results on the KITTI dataset demonstrate that the proposed approach increases navigation and positioning accuracy by 3.5% and 2.2%, respectively, while maintaining real-time performance. This work provides a novel solution for autonomous robot navigation in complex environments.","authors":["Delun Lai","Yeyubei Zhang","Yunchong Liu","Chaojie Li","Huadong Mo"],"url":"https://arxiv.org/abs/2504.19002"}
{"created":"2025-04-29","title":"An SE(3) Noise Model for Range-Azimuth-Elevation Sensors","abstract":"Scan matching is a widely used technique in state estimation. Point-cloud alignment, one of the most popular methods for scan matching, is a weighted least-squares problem in which the weights are determined from the inverse covariance of the measured points. An inaccurate representation of the covariance will affect the weighting of the least-squares problem. For example, if ellipsoidal covariance bounds are used to approximate the curved, \"banana-shaped\" noise characteristics of many scanning sensors, the weighting in the least-squares problem may be overconfident. Additionally, sensor-to-vehicle extrinsic uncertainty and odometry uncertainty during submap formation are two sources of uncertainty that are often overlooked in scan matching applications, also likely contributing to overconfidence on the scan matching estimate. This paper attempts to address these issues by developing a model for range-azimuth-elevation sensors on matrix Lie groups. The model allows for the seamless incorporation of extrinsic and odometry uncertainty. Illustrative results are shown both for a simulated example and for a real point-cloud submap collected with an underwater laser scanner.","authors":["Thomas Hitchcox","James Richard Forbes"],"url":"https://arxiv.org/abs/2504.19009"}
{"created":"2025-04-29","title":"Investigating the Prominence and Severity of Bugs and Glitches Within Games and Their Effects on Player Experience","abstract":"Different errors that occur in video games are often referred to as glitches or bugs. The goal of this exploratory research is to understand how these glitches and bugs within video games affect a players experience. To do this, I reviewed relevant literature and performed observations of these different errors in different games via Twitch livestreams. I then performed thematic analysis with the observation data and generated themes that tie back into to the relevant literature. Most of the current literature focuses on the what and how behind bugs in games, but very little on the implications of these bugs on the overall experience for the players, and what patterns of behavior may emerge because of them.","authors":["Jessica Backus"],"url":"https://arxiv.org/abs/2504.19010"}
{"created":"2025-04-29","title":"\\$PINN -- a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks","abstract":"Physics-Informed Neural Networks (PINNs) are a novel computational approach for solving partial differential equations (PDEs) with noisy and sparse initial and boundary data. Although, efficient quantification of epistemic and aleatoric uncertainties in big multi-scale problems remains challenging. We propose \\$PINN a novel method of computing global uncertainty in PDEs using a Bayesian framework, by combining local Bayesian Physics-Informed Neural Networks (BPINN) with domain decomposition. The solution continuity across subdomains is obtained by imposing the flux continuity across the interface of neighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct a series of computational experiments on PDEs in 1D and 2D spatial domains. Although we have adopted conservative PINNs (cPINNs), the method can be seamlessly extended to other domain decomposition techniques. The results infer that the proposed method recovers the global uncertainty by computing the local uncertainty exactly more efficiently as the uncertainty in each subdomain can be computed concurrently. The robustness of \\$PINN is verified by adding uncorrelated random noise to the training data up to 15% and testing for different domain sizes.","authors":["J\\'ulia Vicens Figueres","Juliette Vanderhaeghen","Federica Bragone","Kateryna Morozovska","Khemraj Shukla"],"url":"https://arxiv.org/abs/2504.19013"}
{"created":"2025-04-29","title":"Towards minimax optimal algorithms for Active Simple Hypothesis Testing","abstract":"We study the Active Simple Hypothesis Testing (ASHT) problem, a simpler variant of the Fixed Budget Best Arm Identification problem. In this work, we provide novel game theoretic formulation of the upper bounds of the ASHT problem. This formulation allows us to leverage tools of differential games and Partial Differential Equations (PDEs) to propose an approximately optimal algorithm that is computationally tractable compared to prior work. However, the optimal algorithm still suffers from a curse of dimensionality and instead we use a novel link to Blackwell Approachability to propose an algorithm that is far more efficient computationally. We show that this new algorithm, although not proven to be optimal, is always better than static algorithms in all instances of ASHT and is numerically observed to attain the optimal exponent in various instances.","authors":["Sushant Vijayan"],"url":"https://arxiv.org/abs/2504.19014"}
{"created":"2025-04-29","title":"Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein Design Principles","abstract":"Advances in artificial intelligence (AI) promise autonomous discovery, yet most systems still resurface knowledge latent in their training data. We present Sparks, a multi-modal multi-agent AI model that executes the entire discovery cycle that includes hypothesis generation, experiment design and iterative refinement to develop generalizable principles and a report without human intervention. Applied to protein science, Sparks uncovered two previously unknown phenomena: (i) a length-dependent mechanical crossover whereby beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond ~80 residues, establishing a new design principle for peptide mechanics; and (ii) a chain-length/secondary-structure stability map revealing unexpectedly robust beta-sheet-rich architectures and a \"frustration zone\" of high variance in mixed alpha/beta folds. These findings emerged from fully self-directed reasoning cycles that combined generative sequence design, high-accuracy structure prediction and physics-aware property models, with paired generation-and-reflection agents enforcing self-correction and reproducibility. The key result is that Sparks can independently conduct rigorous scientific inquiry and identify previously unknown scientific principles.","authors":["Alireza Ghafarollahi","Markus J. Buehler"],"url":"https://arxiv.org/abs/2504.19017"}
{"created":"2025-04-29","title":"Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs","abstract":"The challenge of ensuring Large Language Models (LLMs) align with societal standards is of increasing interest, as these models are still prone to adversarial jailbreaks that bypass their safety mechanisms. Identifying these vulnerabilities is crucial for enhancing the robustness of LLMs against such exploits. We propose Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the robustness of LLM alignment using the Graph of Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly effective jailbreak prompts with fewer queries to the victim model than state-of-the-art attacks, achieving up to five times better jailbreak success rate against robust models like Llama. Notably, GoAT creates high-quality, human-readable prompts without requiring access to the targeted model's parameters, making it a black-box attack. Unlike approaches constrained by tree-based reasoning, GoAT's reasoning is based on a more intricate graph structure. By making simultaneous attack paths aware of each other's progress, this dynamic framework allows a deeper integration and refinement of reasoning paths, significantly enhancing the collaborative exploration of adversarial vulnerabilities in LLMs. At a technical level, GoAT starts with a graph structure and iteratively refines it by combining and improving thoughts, enabling synergy between different thought paths. The code for our implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.","authors":["Mohammad Akbar-Tajari","Mohammad Taher Pilehvar","Mohammad Mahmoody"],"url":"https://arxiv.org/abs/2504.19019"}
{"created":"2025-04-29","title":"Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting","abstract":"Efficient text classification is essential for handling the increasing volume of academic publications. This study explores the use of pre-trained language models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on the Web of Science (WoS-46985) dataset for scientific text classification. To enhance performance, we augment the dataset by executing seven targeted queries in the WoS database, retrieving 1,000 articles per category aligned with WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a hard-voting strategy combines predictions for improved accuracy and confidence. Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly boosts classification accuracy, especially in specialized domains. Domain-specific models like SciBERT and BioBERT consistently outperform general-purpose models such as BERT. These findings underscore the efficacy of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in creating robust and scalable solutions for automated academic text classification.","authors":["Zhyar Rzgar K Rostam","G\\'abor Kert\\'esz"],"url":"https://arxiv.org/abs/2504.19021"}
{"created":"2025-04-29","title":"GLaMoR: Consistency Checking of OWL Ontologies using Graph Language Models","abstract":"Semantic reasoning aims to infer new knowledge from existing knowledge, with OWL ontologies serving as a standardized framework for organizing information. A key challenge in semantic reasoning is verifying ontology consistency. However, state-of-the-art reasoners are computationally expensive, and their efficiency decreases as ontology sizes grow. While classical machine learning models have been explored for consistency checking, they struggle to capture complex relationships within ontologies. Large language models (LLMs) have shown promising results for simple reasoning tasks but perform poorly on structured reasoning. The recently introduced Graph Language Model (GLM) offers a way to simultaneously process graph-structured data and text. This paper proposes GLaMoR (Graph Language Model for Reasoning), a reasoning pipeline that transforms OWL ontologies into graph-structured data and adapts the GLM architecture for consistency checking. We evaluate GLaMoR on ontologies from the NCBO BioPortal repository, converting them into triples suitable for model input. Our results show that the GLM outperforms all baseline models, achieving $95\\%$ accuracy while being 20 times faster than classical reasoners.","authors":["Justin M\\\"ucke","Ansgar Scherp"],"url":"https://arxiv.org/abs/2504.19023"}
{"created":"2025-04-29","title":"KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation","abstract":"We propose a novel k-step return estimation method (called KETCHUP) for Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation tasks. Our idea is to induce a K-step return by using the Bellman Optimality Equation for multiple steps. Theoretical analysis shows that this K-step formulation reduces the variance of the gradient estimates, thus leading to improved RL optimization especially when the student model size is large. Empirical evaluation on three text generation tasks demonstrates that our approach yields superior performance in both standard task metrics and large language model (LLM)-based evaluation. These results suggest that our K-step return induction offers a promising direction for enhancing RL-based KD in LLM research.","authors":["Jiabin Fan","Guoqing Luo","Michael Bowling","Lili Mou"],"url":"https://arxiv.org/abs/2504.19024"}
{"created":"2025-04-29","title":"The Masked Matrix Separation Problem: A First Analysis","abstract":"Given a known matrix that is the sum of a low rank matrix and a masked sparse matrix, we wish to recover both the low rank component and the sparse component. The sparse matrix is masked in the sense that a linear transformation has been applied on its left. We propose a convex optimization problem to recover the low rank and sparse matrices, which generalizes the robust PCA framework. We provide incoherence conditions for the success of the proposed convex optimizaiton problem, adapting to the masked setting. The ``mask'' matrix can be quite general as long as a so-called restricted infinity norm condition is satisfied. Further analysis on the incoherence condition is provided and we conclude with promising numerical experiments.","authors":["Xuemei Chen","Rongrong Wang"],"url":"https://arxiv.org/abs/2504.19025"}
{"created":"2025-04-29","title":"Smooth Approximations of the Rounding Function","abstract":"We propose novel smooth approximations to the classical rounding function, suitable for differentiable optimization and machine learning applications. Our constructions are based on two approaches: (1) localized sigmoid window functions centered at each integer, and (2) normalized weighted sums of sigmoid derivatives representing local densities. The first method approximates the step-like behavior of rounding through differences of shifted sigmoids, while the second method achieves smooth interpolation between integers via density-based weighting. Both methods converge pointwise to the classical rounding function as the sharpness parameter k tends to infinity, and allow controlled trade-offs between smoothness and approximation accuracy. We demonstrate that by restricting the summation to a small set of nearest integers, the computational cost remains low without sacrificing precision. These constructions provide fully differentiable alternatives to hard rounding, which are valuable in contexts where gradient-based methods are essential.","authors":["Stanislav Semenov"],"url":"https://arxiv.org/abs/2504.19026"}
{"created":"2025-04-29","title":"DiCE-Extended: A Robust Approach to Counterfactual Explanations in Machine Learning","abstract":"Explainable artificial intelligence (XAI) has become increasingly important in decision-critical domains such as healthcare, finance, and law. Counterfactual (CF) explanations, a key approach in XAI, provide users with actionable insights by suggesting minimal modifications to input features that lead to different model outcomes. Despite significant advancements, existing CF generation methods often struggle to balance proximity, diversity, and robustness, limiting their real-world applicability. A widely adopted framework, Diverse Counterfactual Explanations (DiCE), emphasizes diversity but lacks robustness, making CF explanations sensitive to perturbations and domain constraints. To address these challenges, we introduce DiCE-Extended, an enhanced CF explanation framework that integrates multi-objective optimization techniques to improve robustness while maintaining interpretability. Our approach introduces a novel robustness metric based on the Dice-Sorensen coefficient, ensuring stability under small input variations. Additionally, we refine CF generation using weighted loss components (lambda_p, lambda_d, lambda_r) to balance proximity, diversity, and robustness. We empirically validate DiCE-Extended on benchmark datasets (COMPAS, Lending Club, German Credit, Adult Income) across multiple ML backends (Scikit-learn, PyTorch, TensorFlow). Results demonstrate improved CF validity, stability, and alignment with decision boundaries compared to standard DiCE-generated explanations. Our findings highlight the potential of DiCE-Extended in generating more reliable and interpretable CFs for high-stakes applications. Future work will explore adaptive optimization techniques and domain-specific constraints to further enhance CF generation in real-world scenarios.","authors":["Volkan Bakir","Polat Goktas","Sureyya Akyuz"],"url":"https://arxiv.org/abs/2504.19027"}
{"created":"2025-04-29","title":"Improving Pretrained YAMNet for Enhanced Speech Command Detection via Transfer Learning","abstract":"This work addresses the need for enhanced accuracy and efficiency in speech command recognition systems, a critical component for improving user interaction in various smart applications. Leveraging the robust pretrained YAMNet model and transfer learning, this study develops a method that significantly improves speech command recognition. We adapt and train a YAMNet deep learning model to effectively detect and interpret speech commands from audio signals. Using the extensively annotated Speech Commands dataset (speech_commands_v0.01), our approach demonstrates the practical application of transfer learning to accurately recognize a predefined set of speech commands. The dataset is meticulously augmented, and features are strategically extracted to boost model performance. As a result, the final model achieved a recognition accuracy of 95.28%, underscoring the impact of advanced machine learning techniques on speech command recognition. This achievement marks substantial progress in audio processing technologies and establishes a new benchmark for future research in the field.","authors":["Sidahmed Lachenani","Hamza Kheddar","Mohamed Ouldzmirli"],"url":"https://arxiv.org/abs/2504.19030"}
{"created":"2025-04-29","title":"VISUALCENT: Visual Human Analysis using Dynamic Centroid Representation","abstract":"We introduce VISUALCENT, a unified human pose and instance segmentation framework to address generalizability and scalability limitations to multi person visual human analysis. VISUALCENT leverages centroid based bottom up keypoint detection paradigm and uses Keypoint Heatmap incorporating Disk Representation and KeyCentroid to identify the optimal keypoint coordinates. For the unified segmentation task, an explicit keypoint is defined as a dynamic centroid called MaskCentroid to swiftly cluster pixels to specific human instance during rapid changes in human body movement or significantly occluded environment. Experimental results on COCO and OCHuman datasets demonstrate VISUALCENTs accuracy and real time performance advantages, outperforming existing methods in mAP scores and execution frame rate per second. The implementation is available on the project page.","authors":["Niaz Ahmad","Youngmoon Lee","Guanghui Wang"],"url":"https://arxiv.org/abs/2504.19032"}
{"created":"2025-04-29","title":"On learning functions over biological sequence space: relating Gaussian process priors, regularization, and gauge fixing","abstract":"Mappings from biological sequences (DNA, RNA, protein) to quantitative measures of sequence functionality play an important role in contemporary biology. We are interested in the related tasks of (i) inferring predictive sequence-to-function maps and (ii) decomposing sequence-function maps to elucidate the contributions of individual subsequences. Because each sequence-function map can be written as a weighted sum over subsequences in multiple ways, meaningfully interpreting these weights requires \"gauge-fixing,\" i.e., defining a unique representation for each map. Recent work has established that most existing gauge-fixed representations arise as the unique solutions to $L_2$-regularized regression in an overparameterized \"weight space\" where the choice of regularizer defines the gauge. Here, we establish the relationship between regularized regression in overparameterized weight space and Gaussian process approaches that operate in \"function space,\" i.e. the space of all real-valued functions on a finite set of sequences. We disentangle how weight space regularizers both impose an implicit prior on the learned function and restrict the optimal weights to a particular gauge. We also show how to construct regularizers that correspond to arbitrary explicit Gaussian process priors combined with a wide variety of gauges. Next, we derive the distribution of gauge-fixed weights implied by the Gaussian process posterior and demonstrate that even for long sequences this distribution can be efficiently computed for product-kernel priors using a kernel trick. Finally, we characterize the implicit function space priors associated with the most common weight space regularizers. Overall, our framework unifies and extends our ability to infer and interpret sequence-function relationships.","authors":["Samantha Petti","Carlos Mart\\'i-G\\'omez","Justin B. Kinney","Juannan Zhou","David M. McCandlish"],"url":"https://arxiv.org/abs/2504.19034"}
{"created":"2025-04-29","title":"Atlantes: A system of GPS transformers for global-scale real-time maritime intelligence","abstract":"Unsustainable exploitation of the oceans exacerbated by global warming is threatening coastal communities worldwide. Accurate and timely monitoring of maritime activity is an essential step to effective governance and to inform future policy. In support of this complex global-scale effort, we built Atlantes, a deep learning based system that provides the first-ever real-time view of vessel behavior at global scale. Atlantes leverages a series of bespoke transformers to distill a high volume, continuous stream of GPS messages emitted by hundreds of thousands of vessels into easily quantifiable behaviors. The combination of low latency and high performance enables operationally relevant decision-making and successful interventions on the high seas where illegal and exploitative activity is too common. Atlantes is already in use by hundreds of organizations worldwide. Here we provide an overview of the model and infrastructure that enables this system to function efficiently and cost-effectively at global-scale and in real-time.","authors":["Henry Herzog","Joshua Hansen","Yawen Zhang","Patrick Beukema"],"url":"https://arxiv.org/abs/2504.19036"}
{"created":"2025-04-29","title":"\"I Would Have Written My Code Differently'': Beginners Struggle to Understand LLM-Generated Code","abstract":"Large language models (LLMs) are being increasingly adopted for programming work. Prior work shows that while LLMs accelerate task completion for professional programmers, beginning programmers struggle to prompt models effectively. However, prompting is just half of the code generation process -- when code is generated, it must be read, evaluated, and integrated (or rejected). How accessible are these tasks for beginning programmers?","authors":["Yangtian Zi","Luisa Li","Arjun Guha","Carolyn Jane Anderson","Molly Q Feldman"],"url":"https://arxiv.org/abs/2504.19037"}
{"created":"2025-04-29","title":"Generative AI Literacy: A Comprehensive Framework for Literacy and Responsible Use","abstract":"After the release of several AI literacy guidelines, the rapid rise and widespread adoption of generative AI, such as ChatGPT, Dall E, and Deepseek, have transformed our lives. Unlike traditional AI algorithms (e.g., convolutional neural networks, semantic networks, classifiers) captured in existing AI literacy frameworks, generative AI exhibits distinct and more nuanced characteristics. However, a lack of robust generative AI literacy is hindering individuals ability to evaluate critically and use these models effectively and responsibly. To address this gap, we propose a set of guidelines with 12 items for generative AI literacy, organized into four key aspects: (1) Guidelines for Generative AI Tool Selection and Prompting, (2) Guidelines for Understanding Interaction with Generative AI, (3) Guidelines for Understanding Interaction with Generative AI, and (4) Guidelines for High Level Understanding of Generative AI. These guidelines aim to support schools, companies, educators, and organizations in developing frameworks that empower their members, such as students, employees, and stakeholders, to use generative AI in an efficient, ethical, and informed way.","authors":["Chengzhi Zhang","Brian Magerko"],"url":"https://arxiv.org/abs/2504.19038"}
{"created":"2025-04-29","title":"Cubing for Tuning","abstract":"We are exploring the problem of building an automated reasoning procedure that adaptively tunes the high-level solving strategy for a given problem. There are two main distinctive characteristics of our approach: tuning is performed solely online, unlike the common use of tuning as an offline process; and tuning data comes exclusively from the given instance, so we do not rely on the availability of similar benchmarks and can work with unique challenging instances. Our approach builds on top of the divide-and-conquer paradigm that naturally serves partitioned sub-problems for an automated tuning algorithm to obtain a good solving strategy. We demonstrate performance improvement on two classes of important problems--SAT-solving and neural network verification\\,--\\,and show that our method can learn unconventional solving strategies in some cases.","authors":["Haoze Wu","Clark Barrett","Nina Narodytska"],"url":"https://arxiv.org/abs/2504.19039"}
{"created":"2025-04-29","title":"Improved Molecular Generation through Attribute-Driven Integrative Embeddings and GAN Selectivity","abstract":"The growing demand for molecules with tailored properties in fields such as drug discovery and chemical engineering has driven advancements in computational methods for molecular design. Machine learning-based approaches for de-novo molecular generation have recently garnered significant attention. This paper introduces a transformer-based vector embedding generator combined with a modified Generative Adversarial Network (GAN) to generate molecules with desired properties. The embedding generator utilizes a novel molecular descriptor, integrating Morgan fingerprints with global molecular attributes, enabling the transformer to capture local functional groups and broader molecular characteristics. Modifying the GAN generator loss function ensures the generation of molecules with specific desired properties. The transformer achieves a reconversion accuracy of 94% while translating molecular descriptors back to SMILES strings, validating the utility of the proposed embeddings for generative tasks. The approach is validated by generating novel odorant molecules using a labeled dataset of odorant and non-odorant compounds. With the modified range-loss function, the GAN exclusively generates odorant molecules. This work underscores the potential of combining novel vector embeddings with transformers and modified GAN architectures to accelerate the discovery of tailored molecules, offering a robust tool for diverse molecular design applications.","authors":["Nandan Joshi","Erhan Guven"],"url":"https://arxiv.org/abs/2504.19040"}
{"created":"2025-04-29","title":"Calibrating Translation Decoding with Quality Estimation on LLMs","abstract":"Neural machine translation (NMT) systems typically employ maximum a posteriori (MAP) decoding to select the highest-scoring translation from the distribution mass. However, recent evidence highlights the inadequacy of MAP decoding, often resulting in low-quality or even pathological hypotheses -- the decoding objective is not aligned with real-world translation quality. This paper proposes calibrating hypothesis likelihoods with translation quality from a distribution view by directly optimizing their Pearson correlation -- thereby enhancing the effectiveness of translation decoding. With our method, translation on large language models (LLMs) improves substantially after limited training (2K instances per direction). This improvement is orthogonal to those achieved through supervised fine-tuning, leading to substantial gains across a broad range of metrics and human evaluations -- even when applied to top-performing translation-specialized LLMs fine-tuned on high-quality translation data, such as Tower, or when compared to recent preference optimization methods, like CPO. Moreover, the calibrated translation likelihood can directly serve as a strong proxy for translation quality, closely approximating or even surpassing some state-of-the-art translation quality estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates that calibration enhances the effectiveness of MAP decoding, thereby enabling greater efficiency in real-world deployment. The resulting state-of-the-art translation model, which covers 10 languages, along with the accompanying code and human evaluation data, has been released to the community: https://github.com/moore3930/calibrating-llm-mt.","authors":["Di Wu","Yibin Lei","Christof Monz"],"url":"https://arxiv.org/abs/2504.19044"}
{"created":"2025-04-29","title":"GPU Acceleration of Monte Carlo Tallies on Unstructured Meshes in OpenMC with PUMI-Tally","abstract":"Unstructured mesh tallies are a bottleneck in Monte Carlo neutral particle transport simulations of fusion reactors. This paper introduces the PUMI-Tally library that takes advantage of mesh adjacency information to accelerate these tallies on CPUs and GPUs. For a fixed source simulation using track-length tallies, we achieved a speed-up of 19.7X on an NVIDIA A100, and 9.2X using OpenMP on 128 threads of two AMD EPYC 7763 CPUs on NERSC Perlmutter. On the Empire AI alpha system, we achieved a speed-up of 20X using an NVIDIA H100 and 96 threads of an Intel Xenon 8568Y+. Our method showed better scaling with number of particles and number of elements. Additionally, we observed a 199X reduction in the number of allocations during initialization and the first three iterations, with a similar overall memory consumption. And, our hybrid CPU/GPU method demonstrated a 6.69X improvement in the energy consumption over the current approach.","authors":["Fuad Hasan","Cameron W. Smith","Mark S. Shephard","R. Michael Churchill","George J. Wilkie","Paul K. Romano","Patrick C. Shriwise","Jacob S. Merson"],"url":"https://arxiv.org/abs/2504.19048"}
{"created":"2025-04-29","title":"Efficient Control Allocation and 3D Trajectory Tracking of a Highly Manoeuvrable Under-actuated Bio-inspired AUV","abstract":"Fin actuators can be used for for both thrust generation and vectoring. Therefore, fin-driven autonomous underwater vehicles (AUVs) can achieve high maneuverability with a smaller number of actuators, but their control is challenging. This study proposes an analytic control allocation method for underactuated Autonomous Underwater Vehicles (AUVs). By integrating an adaptive hybrid feedback controller, we enable an AUV with 4 actuators to move in 6 degrees of freedom (DOF) in simulation and up to 5-DOF in real-world experiments. The proposed method outperformed state-of-the-art control allocation techniques in 6-DOF trajectory tracking simulations, exhibiting centimeter-scale accuracy and higher energy and computational efficiency. Real-world pool experiments confirmed the method's robustness and efficacy in tracking complex 3D trajectories, with significant computational efficiency gains 0.007 (ms) vs. 22.28 (ms). Our method offers a balance between performance, energy efficiency, and computational efficiency, showcasing a potential avenue for more effective tracking of a large number of DOF for under-actuated underwater robots.","authors":["Walid Remmas","Christian Meurer","Yuya Hamamatsu","Ahmed Chemori","Maarja Kruusmaa"],"url":"https://arxiv.org/abs/2504.19049"}
{"created":"2025-04-29","title":"Min-CSPs on Complete Instances II: Polylogarithmic Approximation for Min-NAE-3-SAT","abstract":"This paper studies complete $k$-Constraint Satisfaction Problems (CSPs), where an $n$-variable instance has exactly one nontrivial constraint for each subset of $k$ variables, i.e., it has $\\binom{n}{k}$ constraints. A recent work started a systematic study of complete $k$-CSPs [Anand, Lee, Sharma, SODA'25], and showed a quasi-polynomial time algorithm that decides if there is an assignment satisfying all the constraints of any complete Boolean-alphabet $k$-CSP, algorithmically separating complete instances from dense instances.","authors":["Aditya Anand","Euiwoong Lee","Davide Mazzali","Amatya Sharma"],"url":"https://arxiv.org/abs/2504.19051"}
{"created":"2025-04-29","title":"Entrywise Approximate Matrix Inversion","abstract":"We study the bit complexity of inverting diagonally dominant matrices, which are associated with random walk quantities such as hitting times and escape probabilities. Such quantities can be exponentially small, even on undirected unit-weighted graphs. However, their nonnegativity suggests that they can be approximated entrywise, leading to a stronger notion of approximation than vector norm-based error.","authors":["Mehrdad Ghadiri","Junzhao Yang"],"url":"https://arxiv.org/abs/2504.19054"}
{"created":"2025-04-29","title":"BinPool: A Dataset of Vulnerabilities for Binary Security Analysis","abstract":"The development of machine learning techniques for discovering software vulnerabilities relies fundamentally on the availability of appropriate datasets. The ideal dataset consists of a large and diverse collection of real-world vulnerabilities, paired so as to contain both vulnerable and patched versions of each program. Naturally, collecting such datasets is a laborious and time-consuming task. Within the specific domain of vulnerability discovery in binary code, previous datasets are either publicly unavailable, lack semantic diversity, involve artificially introduced vulnerabilities, or were collected using static analyzers, thereby themselves containing incorrectly labeled example programs.","authors":["Sima Arasteh","Georgios Nikitopoulos","Wei-Cheng Wu","Nicolaas Weideman","Aaron Portnoy","Mukund Raghothaman","Christophe Hauser"],"url":"https://arxiv.org/abs/2504.19055"}
{"created":"2025-04-29","title":"Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions","abstract":"Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.","authors":["Mohammad Mahdi Abootorabi","Omid Ghahroodi","Pardis Sadat Zahraei","Hossein Behzadasl","Alireza Mirrokni","Mobina Salimipanah","Arash Rasouli","Bahar Behzadipour","Sara Azarnoush","Benyamin Maleki","Erfan Sadraiye","Kiarash Kiani Feriz","Mahdi Teymouri Nahad","Ali Moghadasi","Abolfazl Eshagh Abianeh","Nizi Nazar","Hamid R. Rabiee","Mahdieh Soleymani Baghshah","Meisam Ahmadi","Ehsaneddin Asgari"],"url":"https://arxiv.org/abs/2504.19056"}
{"created":"2025-04-29","title":"Scaling Data Center TCP to Terabits with Laminar","abstract":"Laminar is the first TCP stack designed for the reconfigurable match-action table (RMT) architecture, widely used in high-speed programmable switches and SmartNICs. Laminar reimagines TCP processing as a pipeline of simple match-action operations, enabling line-rate performance with low latency and minimal energy consumption, while maintaining compatibility with standard TCP and POSIX sockets. Leveraging novel techniques like optimistic concurrency, pseudo segment updates, and bump-in-the-wire processing, Laminar handles the transport logic, including retransmission, reassembly, flow, and congestion control, entirely within the RMT pipeline.","authors":["Rajath Shashidhara","Antoine Kaufmann","Simon Peter"],"url":"https://arxiv.org/abs/2504.19058"}
{"created":"2025-04-29","title":"Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models","abstract":"Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, such as reasons for hospital admission, significant in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive numerical simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization.","authors":["Anindya Bijoy Das","Shibbir Ahmed","Shahnewaz Karim Sakib"],"url":"https://arxiv.org/abs/2504.19061"}
{"created":"2025-04-29","title":"Security Vulnerabilities in Quantum Cloud Systems: A Survey on Emerging Threats","abstract":"Quantum computing is becoming increasingly widespread due to the potential and capabilities to solve complex problems beyond the scope of classical computers. As Quantum Cloud services are adopted by businesses and research groups, they allow for greater progress and application in many fields. However, the inherent vulnerabilities of these environments pose significant security concerns. This survey delivers a comprehensive analysis of the security challenges that emerged in quantum cloud systems, with a distinct focus on multi-tenant vulnerabilities and the classical-quantum interface. Key threats such as crosstalk attacks, quantum-specific side-channel vulnerabilities, and insider threats are all examined, as well as their effects on the confidentiality, integrity, and availability of quantum circuits. The design and implementation of various quantum architectures from quantum cloud providers are also discussed. In addition, this paper delves into emerging quantum security solutions and best practices to mitigate these risks. This survey offers insights into current research gaps and proposes future directions for secure and resilient quantum cloud infrastructures.","authors":["Justin Coupel","Tasnuva Farheen"],"url":"https://arxiv.org/abs/2504.19064"}
{"created":"2025-04-29","title":"ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics","abstract":"Accurate assessments of extreme weather events are vital for research and policy, yet localized and granular data remain scarce in many parts of the world. This data gap limits our ability to analyze potential outcomes and implications of extreme weather events, hindering effective decision-making. Large Language Models (LLMs) can process vast amounts of unstructured text data, extract meaningful insights, and generate detailed assessments by synthesizing information from multiple sources. Furthermore, LLMs can seamlessly transfer their general language understanding to smaller models, enabling these models to retain key knowledge while being fine-tuned for specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware Alignment (EWRA), a method that enhances small language models (SLMs) by incorporating structured reasoning paths derived from LLMs, and ExtremeWeatherNews, a large dataset of extreme weather event-related news articles. EWRA and ExtremeWeatherNews together form the overall framework, ClimaEmpact, that focuses on addressing three critical extreme-weather tasks: categorization of tangible vulnerabilities/impacts, topic labeling, and emotion analysis. By aligning SLMs with advanced reasoning strategies on ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and domain-specific responses for extreme weather analytics. Our results show that the approach proposed guides SLMs to output domain-aligned responses, surpassing the performance of task-specific models and offering enhanced real-world applicability for extreme weather analytics.","authors":["Deeksha Varshney","Keane Ong","Rui Mao","Erik Cambria","Gianmarco Mengaldo"],"url":"https://arxiv.org/abs/2504.19066"}
{"created":"2025-04-29","title":"Performance Analysis of OpenVPN on a Consumer Grade Router","abstract":"Virtual Private Networks (VPNs) offer an alternative solution using Internet Protocol (IP) tunnels to create secure, encrypted communication between geographically distant networks using a common shared medium such as the Internet. They use tunneling to establish end-to-end connectivity. OpenVPN is a cross-platform, secure, highly configurable VPN solution. Security in OpenVPN is handled by the OpenSSL cryptographic library which provides strong security over a Secure Socket Layer (SSL) using standard algorithms such as Advanced Encryption Standard (AES), Blowfish, or Triple DES (3DES). The Linksys WRT54GL router is a consumer-grade router made by Linksys, a division of Cisco Systems, capable of running under Linux. The Linux-based DD-WRT open-source router firmware can run OpenVPN on the Linksys WRT54GL router. For this case study, the performance of OpenVPN is measured and analyzed using a $2^{k-p}$ fractional factorial design for 5 minus 1 factors where $k=5$ and $p=1$. The results show that the throughput is mainly limited by the encryption cipher used, and that the round-trip time (RTT) is mostly dependent on the transport protocol selected.","authors":["Michael J. Hall (Washington University in St. Louis)"],"url":"https://arxiv.org/abs/2504.19069"}
{"created":"2025-04-29","title":"Sample-Efficient Language Model for Hinglish Conversational AI","abstract":"This paper presents our process for developing a sample-efficient language model for a conversational Hinglish chatbot. Hinglish, a code-mixed language that combines Hindi and English, presents a unique computational challenge due to inconsistent spelling, lack of standardization, and limited quality of conversational data. This work evaluates multiple pre-trained cross-lingual language models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning techniques to improve performance on Hinglish conversational tasks. The proposed approach integrates synthetically generated dialogues with insights from existing Hinglish datasets to address data scarcity. Experimental results demonstrate that models with fewer parameters, when appropriately fine-tuned on high-quality code-mixed data, can achieve competitive performance for Hinglish conversation generation while maintaining computational efficiency.","authors":["Sakshi Singh","Abhinav Prakash","Aakriti Shah","Chaitanya Sachdeva","Sanjana Dumpala"],"url":"https://arxiv.org/abs/2504.19070"}
{"created":"2025-04-29","title":"Geometric Gait Optimization for Kinodynamic Systems Using a Lie Group Integrator","abstract":"This paper presents a gait optimization and motion planning framework for a class of locomoting systems with mixed kinematic and dynamic properties. Using Lagrangian reduction and differential geometry, we derive a general dynamic model that incorporates second-order dynamics and nonholonomic constraints, applicable to kinodynamic systems such as wheeled robots with nonholonomic constraints as well as swimming robots with nonisotropic fluid-added inertia and hydrodynamic drag. Building on Lie group integrators and group symmetries, we develop a variational gait optimization method for kinodynamic systems. By integrating multiple gaits and their transitions, we construct comprehensive motion plans that enable a wide range of motions for these systems. We evaluate our framework on three representative examples: roller racer, snakeboard, and swimmer. Simulation and hardware experiments demonstrate diverse motions, including acceleration, steady-state maintenance, gait transitions, and turning. The results highlight the effectiveness of the proposed method and its potential for generalization to other biological and robotic locomoting systems.","authors":["Yanhao Yang","Ross L. Hatton"],"url":"https://arxiv.org/abs/2504.19072"}
{"created":"2025-04-29","title":"Dual-Branch Residual Network for Cross-Domain Few-Shot Hyperspectral Image Classification with Refined Prototype","abstract":"Convolutional neural networks (CNNs) are effective for hyperspectral image (HSI) classification, but their 3D convolutional structures introduce high computational costs and limited generalization in few-shot scenarios. Domain shifts caused by sensor differences and environmental variations further hinder cross-dataset adaptability. Metric-based few-shot learning (FSL) prototype networks mitigate this problem, yet their performance is sensitive to prototype quality, especially with limited samples. To overcome these challenges, a dual-branch residual network that integrates spatial and spectral features via parallel branches is proposed in this letter. Additionally, more robust refined prototypes are obtained through a regulation term. Furthermore, a kernel probability matching strategy aligns source and target domain features, alleviating domain shift. Experiments on four publicly available HSI datasets illustrate that the proposal achieves superior performance compared to other methods.","authors":["Anyong Qin","Chaoqi Yuan","Qiang Li","Feng Yang","Tiecheng Song","Chenqiang Gao"],"url":"https://arxiv.org/abs/2504.19074"}
{"created":"2025-04-29","title":"HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's Disease","abstract":"Accurate diagnosis of Alzheimer's disease (AD) requires effectively integrating multimodal data and clinical expertise. However, existing methods often struggle to fully utilize multimodal information and lack structured mechanisms to incorporate dynamic domain knowledge. To address these limitations, we propose HoloDx, a knowledge- and data-driven framework that enhances AD diagnosis by aligning domain knowledge with multimodal clinical data. HoloDx incorporates a knowledge injection module with a knowledge-aware gated cross-attention, allowing the model to dynamically integrate domain-specific insights from both large language models (LLMs) and clinical expertise. Also, a memory injection module with a designed prototypical memory attention enables the model to retain and retrieve subject-specific information, ensuring consistency in decision-making. By jointly leveraging these mechanisms, HoloDx enhances interpretability, improves robustness, and effectively aligns prior knowledge with current subject data. Evaluations on five AD datasets demonstrate that HoloDx outperforms state-of-the-art methods, achieving superior diagnostic accuracy and strong generalization across diverse cohorts. The source code will be released upon publication acceptance.","authors":["Qiuhui Chen","Jintao Wang","Gang Wang","Yi Hong"],"url":"https://arxiv.org/abs/2504.19075"}
{"created":"2025-04-29","title":"LLM-Evaluation Tropes: Perspectives on the Validity of LLM-Evaluations","abstract":"Large Language Models (LLMs) are increasingly used to evaluate information retrieval (IR) systems, generating relevance judgments traditionally made by human assessors. Recent empirical studies suggest that LLM-based evaluations often align with human judgments, leading some to suggest that human judges may no longer be necessary, while others highlight concerns about judgment reliability, validity, and long-term impact. As IR systems begin incorporating LLM-generated signals, evaluation outcomes risk becoming self-reinforcing, potentially leading to misleading conclusions.","authors":["Laura Dietz","Oleg Zendel","Peter Bailey","Charles Clarke","Ellese Cotterill","Jeff Dalton","Faegheh Hasibi","Mark Sanderson","Nick Craswell"],"url":"https://arxiv.org/abs/2504.19076"}
{"created":"2025-04-29","title":"Learning to Drive from a World Model","abstract":"Most self-driving systems rely on hand-coded perception outputs and engineered driving rules. Learning directly from human driving data with an end-to-end method can allow for a training architecture that is simpler and scales well with compute and data.","authors":["Mitchell Goff","Greg Hogan","George Hotz","Armand du Parc Locmaria","Kacper Raczy","Harald Sch\\\"afer","Adeeb Shihadeh","Weixing Zhang","Yassine Yousfi"],"url":"https://arxiv.org/abs/2504.19077"}
{"created":"2025-04-29","title":"MIA-Mind: A Multidimensional Interactive Attention Mechanism Based on MindSpore","abstract":"Attention mechanisms have significantly advanced deep learning by enhancing feature representation through selective focus. However, existing approaches often independently model channel importance and spatial saliency, overlooking their inherent interdependence and limiting their effectiveness. To address this limitation, we propose MIA-Mind, a lightweight and modular Multidimensional Interactive Attention Mechanism, built upon the MindSpore framework. MIA-Mind jointly models spatial and channel features through a unified cross-attentive fusion strategy, enabling fine-grained feature recalibration with minimal computational overhead. Extensive experiments are conducted on three representative datasets: on CIFAR-10, MIA-Mind achieves an accuracy of 82.9\\%; on ISBI2012, it achieves an accuracy of 78.7\\%; and on CIC-IDS2017, it achieves an accuracy of 91.9\\%. These results validate the versatility, lightweight design, and generalization ability of MIA-Mind across heterogeneous tasks. Future work will explore the extension of MIA-Mind to large-scale datasets, the development of ada,ptive attention fusion strategies, and distributed deployment to further enhance scalability and robustness.","authors":["Zhenkai Qin","Jiaquan Liang","Qiao Fang"],"url":"https://arxiv.org/abs/2504.19080"}
{"created":"2025-04-29","title":"Score-Debiased Kernel Density Estimation","abstract":"We propose a novel method for density estimation that leverages an estimated score function to debias kernel density estimation (SD-KDE). In our approach, each data point is adjusted by taking a single step along the score function with a specific choice of step size, followed by standard KDE with a modified bandwidth. The step size and modified bandwidth are chosen to remove the leading order bias in the KDE. Our experiments on synthetic tasks in 1D, 2D and on MNIST, demonstrate that our proposed SD-KDE method significantly reduces the mean integrated squared error compared to the standard Silverman KDE, even with noisy estimates in the score function. These results underscore the potential of integrating score-based corrections into nonparametric density estimation.","authors":["Elliot L. Epstein","Rajat Dwaraknath","Thanawat Sornwanee","John Winnicki","Jerry Weihong Liu"],"url":"https://arxiv.org/abs/2504.19084"}
{"created":"2025-04-29","title":"Toward Inclusive Low-Code Development: Detecting Accessibility Issues in User Reviews","abstract":"Low-code applications are gaining popularity across various fields, enabling non-developers to participate in the software development process. However, due to the strong reliance on graphical user interfaces, they may unintentionally exclude users with visual impairments, such as color blindness and low vision. This paper investigates the accessibility issues users report when using low-code applications. We construct a comprehensive dataset of low-code application reviews, consisting of accessibility-related reviews and non-accessibility-related reviews. We then design and implement a complex model to identify whether a review contains an accessibility-related issue, combining two state-of-the-art Transformers-based models and a traditional keyword-based system. Our proposed hybrid model achieves an accuracy and F1-score of 78% in detecting accessibility-related issues.","authors":["Mohammadali Mohammadkhani","Sara Zahedi Movahed","Hourieh Khalajzadeh","Mojtaba Shahin","Khuong Tran Hoang"],"url":"https://arxiv.org/abs/2504.19085"}
{"created":"2025-04-29","title":"Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction","abstract":"Single-Domain Generalized Object Detection~(S-DGOD) aims to train an object detector on a single source domain while generalizing well to diverse unseen target domains, making it suitable for multimedia applications that involve various domain shifts, such as intelligent video surveillance and VR/AR technologies. With the success of large-scale Vision-Language Models, recent S-DGOD approaches exploit pre-trained vision-language knowledge to guide invariant feature learning across visual domains. However, the utilized knowledge remains at a coarse-grained level~(e.g., the textual description of adverse weather paired with the image) and serves as an implicit regularization for guidance, struggling to learn accurate region- and object-level features in varying domains. In this work, we propose a new cross-modal feature learning method, which can capture generalized and discriminative regional features for S-DGOD tasks. The core of our method is the mechanism of Cross-modal and Region-aware Feature Interaction, which simultaneously learns both inter-modal and intra-modal regional invariance through dynamic interactions between fine-grained textual and visual features. Moreover, we design a simple but effective strategy called Cross-domain Proposal Refining and Mixing, which aligns the position of region proposals across multiple domains and diversifies them, enhancing the localization ability of detectors in unseen scenarios. Our method achieves new state-of-the-art results on S-DGOD benchmark datasets, with improvements of +8.8\\%~mPC on Cityscapes-C and +7.9\\%~mPC on DWD over baselines, demonstrating its efficacy.","authors":["Xiaoran Xu","Jiangang Yang","Wenyue Chong","Wenhui Shi","Shichu Sun","Jing Xing","Jian Liu"],"url":"https://arxiv.org/abs/2504.19086"}
{"created":"2025-04-29","title":"CipherBank: Exploring the Boundary of LLM Reasoning Capabilities through Cryptography Challenges","abstract":"Large language models (LLMs) have demonstrated remarkable capabilities, especially the recent advancements in reasoning, such as o1 and o3, pushing the boundaries of AI. Despite these impressive achievements in mathematics and coding, the reasoning abilities of LLMs in domains requiring cryptographic expertise remain underexplored. In this paper, we introduce CipherBank, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs in cryptographic decryption tasks. CipherBank comprises 2,358 meticulously crafted problems, covering 262 unique plaintexts across 5 domains and 14 subdomains, with a focus on privacy-sensitive and real-world scenarios that necessitate encryption. From a cryptographic perspective, CipherBank incorporates 3 major categories of encryption methods, spanning 9 distinct algorithms, ranging from classical ciphers to custom cryptographic techniques. We evaluate state-of-the-art LLMs on CipherBank, e.g., GPT-4o, DeepSeek-V3, and cutting-edge reasoning-focused models such as o1 and DeepSeek-R1. Our results reveal significant gaps in reasoning abilities not only between general-purpose chat LLMs and reasoning-focused LLMs but also in the performance of current reasoning-focused models when applied to classical cryptographic decryption tasks, highlighting the challenges these models face in understanding and manipulating encrypted data. Through detailed analysis and error investigations, we provide several key observations that shed light on the limitations and potential improvement areas for LLMs in cryptographic reasoning. These findings underscore the need for continuous advancements in LLM reasoning capabilities.","authors":["Yu Li","Qizhi Pei","Mengyuan Sun","Honglin Lin","Chenlin Ming","Xin Gao","Jiang Wu","Conghui He","Lijun Wu"],"url":"https://arxiv.org/abs/2504.19093"}
{"created":"2025-04-29","title":"Efficient Reasoning for LLMs through Speculative Chain-of-Thought","abstract":"Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have recently attracted widespread attention due to their impressive task-solving abilities. However, the enormous model size and the generation of lengthy thought chains introduce significant reasoning costs and response latency. Existing methods for efficient reasoning mainly focus on reducing the number of model parameters or shortening the chain-of-thought length. In this paper, we introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency from another perspective by accelerated average reasoning speed through large and small model collaboration. SCoT conducts thought-level drafting using a lightweight draft model. Then it selects the best CoT draft and corrects the error cases with the target model. The proposed thinking behavior alignment improves the efficiency of drafting and the draft selection strategy maintains the prediction accuracy for complex problems. Experimental results on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces reasoning latency by 48\\%$\\sim$66\\% for Deepseek-R1-Distill-Qwen-32B while achieving near-target-model-level performance. Our code is available at https://github.com/Jikai0Wang/Speculative_CoT.","authors":["Jikai Wang","Juntao Li","Lijun Wu","Min Zhang"],"url":"https://arxiv.org/abs/2504.19095"}
{"created":"2025-04-29","title":"VeriDebug: A Unified LLM for Verilog Debugging via Contrastive Embedding and Guided Correction","abstract":"Large Language Models (LLMs) have demonstrated remarkable potential in debugging for various programming languages. However, the application of LLMs to Verilog debugging remains insufficiently explored. Here, we present VeriDebug, an approach that integrates contrastive representation and guided correction capabilities for automated Verilog debugging. Unlike existing methods, VeriDebug employs an embedding-based technique to accurately retrieve internal information, followed by bug-fixing. VeriDebug unifies Verilog bug detection and correction through a shared parameter space. By simultaneously learning bug patterns and fixes, it streamlines debugging via contrastive embedding and guided correction. Empirical results show the efficacy of VeriDebug in enhancing Verilog debugging. Our VeriDebugLoc, Type model achieves 64.7 accuracy in bug fixing (Acc1), a significant improvement from the existing open-source SOTAs 11.3. This performance not only outperforms open-source alternatives but also exceeds larger closed-source models like GPT-3.5-turbo (36.6), offering a more accurate alternative to conventional debugging methods.","authors":["Ning Wang","Bingkun Yao","Jie Zhou","Yuchen Hu","Xi Wang","Nan Guan","Zhe Jiang"],"url":"https://arxiv.org/abs/2504.19099"}
{"created":"2025-04-29","title":"Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation","abstract":"Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution for enhancing the accuracy and credibility of Large Language Models (LLMs), particularly in Question & Answer tasks. This is achieved by incorporating proprietary and private data from integrated databases. However, private RAG systems face significant challenges due to the scarcity of private domain data and critical data privacy issues. These obstacles impede the deployment of private RAG systems, as developing privacy-preserving RAG systems requires a delicate balance between data security and data availability. To address these challenges, we regard federated learning (FL) as a highly promising technology for privacy-preserving RAG services. We propose a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG). This framework facilitates collaborative training of client-side RAG retrieval models. The parameters of these models are aggregated and distributed on a central-server, ensuring data privacy without direct sharing of raw data. In FedE4RAG, knowledge distillation is employed for communication between the server and client models. This technique improves the generalization of local RAG retrievers during the federated learning process. Additionally, we apply homomorphic encryption within federated learning to safeguard model parameters and mitigate concerns related to data leakage. Extensive experiments conducted on the real-world dataset have validated the effectiveness of FedE4RAG. The results demonstrate that our proposed framework can markedly enhance the performance of private RAG systems while maintaining robust data privacy protection.","authors":["Qianren Mao","Qili Zhang","Hanwen Hao","Zhentao Han","Runhua Xu","Weifeng Jiang","Qi Hu","Zhijun Chen","Tyler Zhou","Bo Li","Yangqiu Song","Jin Dong","Jianxin Li","Philip S. Yu"],"url":"https://arxiv.org/abs/2504.19101"}
{"created":"2025-04-29","title":"Harmonizing Generalization and Personalization in Ring-topology Decentralized Federated Learning","abstract":"We introduce Ring-topology Decentralized Federated Learning (RDFL) for distributed model training, aiming to avoid the inherent risks of centralized failure in server-based FL. However, RDFL faces the challenge of low information-sharing efficiency due to the point-to-point communication manner when handling inherent data heterogeneity. Existing studies to mitigate data heterogeneity focus on personalized optimization of models, ignoring that the lack of shared information constraints can lead to large differences among models, weakening the benefits of collaborative learning. To tackle these challenges, we propose a Divide-and-conquer RDFL framework (DRDFL) that uses a feature generation model to extract personalized information and invariant shared knowledge from the underlying data distribution, ensuring both effective personalization and strong generalization. Specifically, we design a \\textit{PersonaNet} module that encourages class-specific feature representations to follow a Gaussian mixture distribution, facilitating the learning of discriminative latent representations tailored to local data distributions. Meanwhile, the \\textit{Learngene} module is introduced to encapsulate shared knowledge through an adversarial classifier to align latent representations and extract globally invariant information. Extensive experiments demonstrate that DRDFL outperforms state-of-the-art methods in various data heterogeneity settings.","authors":["Shunxin Guo","Jiaqi Lv","Xin Geng"],"url":"https://arxiv.org/abs/2504.19103"}
{"created":"2025-04-29","title":"MISO: Multiresolution Submap Optimization for Efficient Globally Consistent Neural Implicit Reconstruction","abstract":"Neural implicit representations have had a significant impact on simultaneous localization and mapping (SLAM) by enabling robots to build continuous, differentiable, and high-fidelity 3D maps from sensor data. However, as the scale and complexity of the environment increase, neural SLAM approaches face renewed challenges in the back-end optimization process to keep up with runtime requirements and maintain global consistency. We introduce MISO, a hierarchical optimization approach that leverages multiresolution submaps to achieve efficient and scalable neural implicit reconstruction. For local SLAM within each submap, we develop a hierarchical optimization scheme with learned initialization that substantially reduces the time needed to optimize the implicit submap features. To correct estimation drift globally, we develop a hierarchical method to align and fuse the multiresolution submaps, leading to substantial acceleration by avoiding the need to decode the full scene geometry. MISO significantly improves computational efficiency and estimation accuracy of neural signed distance function (SDF) SLAM on large-scale real-world benchmarks.","authors":["Yulun Tian","Hanwen Cao","Sunghwan Kim","Nikolay Atanasov"],"url":"https://arxiv.org/abs/2504.19104"}
{"created":"2025-04-29","title":"Blended PC Peer Review Model: Process and Reflection","abstract":"The academic peer review system is under increasing pressure due to a growing volume of submissions and a limited pool of available reviewers, resulting in delayed decisions and an uneven distribution of reviewing responsibilities. To address this challenge, the International Conference on Mining Software Repositories (MSR) 2025 introduced a Blended Program Committee (PC) peer review model for its Technical Track. Building upon the community's earlier experience with the Shadow PC (2021 and 2022) and Junior PC (2023 and 2024), the new model pairs up one Junior PC member with two regular PC members as part of the core review team of a given paper, instead of adding them as an extra reviewer. This paper presents the rationale, implementation, and reflections on the model, including insights from a post-review author survey evaluating the quality and usefulness of reviews. Our findings highlight the potential of Junior PCs to alleviate reviewer shortages, foster inclusivity, and sustain a high-quality peer review process. We offer lessons learned and recommendations to guide future adoption and refinement of the model.","authors":["Chakkrit Tantithamthavorn","Nicole Novielli","Ayushi Rastogi","Olga Baysal","Bram Adams"],"url":"https://arxiv.org/abs/2504.19105"}
{"created":"2025-04-29","title":"A Multi-Language Perspective on the Robustness of LLM Code Generation","abstract":"Large language models have gained significant traction and popularity in recent times, extending their usage to code-generation tasks. While this field has garnered considerable attention, the exploration of testing and evaluating the robustness of code generation models remains an ongoing endeavor. Previous studies have primarily focused on code generation models specifically for the Python language, overlooking other widely used programming languages. In this research, we conduct a comprehensive comparative analysis to assess the robustness performance of several prominent code generation models. Furthermore, we investigate how their performance varies across different programming languages. To accomplish this, we introduce perturbations in four key areas of the prompt: DocString, function name, syntax, and format. We have compiled and released a dedicated dataset for this purpose. This work presents our experimental findings, shedding light on the performance of code generation models in various scenarios.","authors":["Fazle Rabbi","Zushuo Ding","Jinqiu Yang"],"url":"https://arxiv.org/abs/2504.19108"}
{"created":"2025-04-29","title":"APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries","abstract":"Recent progress in large language models (LLMs) has shown promise in formal theorem proving, yet existing benchmarks remain limited to isolated, static proof tasks, failing to capture the iterative, engineering-intensive workflows of real-world formal mathematics libraries. Motivated by analogous advances in software engineering, we introduce the paradigm of Automated Proof Engineering (APE), which aims to automate proof engineering tasks such as feature addition, proof refactoring, and bug fixing using LLMs. To facilitate research in this direction, we present APE-Bench I, the first realistic benchmark built from real-world commit histories of Mathlib4, featuring diverse file-level tasks described in natural language and verified via a hybrid approach combining the Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable parallel verification infrastructure optimized for proof checking across multiple versions of Mathlib. Empirical results on state-of-the-art LLMs demonstrate strong performance on localized edits but substantial degradation on handling complex proof engineering. This work lays the foundation for developing agentic workflows in proof engineering, with future benchmarks targeting multi-file coordination, project-scale verification, and autonomous agents capable of planning, editing, and repairing formal libraries.","authors":["Huajian Xin","Luming Li","Xiaoran Jin","Jacques Fleuriot","Wenda Li"],"url":"https://arxiv.org/abs/2504.19110"}
{"created":"2025-04-29","title":"Vessel Length Estimation from Magnetic Wake Signature: A Physics-Informed Residual Neural Network Approach","abstract":"Marine remote sensing enhances maritime surveillance, environmental monitoring, and naval operations. Vessel length estimation, a key component of this technology, supports effective maritime surveillance by empowering features such as vessel classification. Departing from traditional methods relying on two-dimensional hydrodynamic wakes or computationally intensive satellite imagery, this paper introduces an innovative approach for vessel length estimation that leverages the subtle magnetic wake signatures of vessels, captured through a low-complexity one-dimensional profile from a single airborne magnetic sensor scan. The proposed method centers around our characterized nonlinear integral equations that connect the magnetic wake to the vessel length within a realistic finite-depth marine environment. To solve the derived equations, we initially leverage a deep residual neural network (DRNN). The proposed DRNN-based solution framework is shown to be unable to exactly learn the intricate relationships between parameters when constrained by a limited training-dataset. To overcome this issue, we introduce an innovative approach leveraging a physics-informed residual neural network (PIRNN). This model integrates physical formulations directly into the loss function, leading to improved performance in terms of both accuracy and convergence speed. Considering a sensor scan angle of less than $15^\\circ$, which maintains a reasonable margin below Kelvin's limit angle of $19.5^\\circ$, we explore the impact of various parameters on the accuracy of the vessel","authors":["Mohammad Amir Fallah","Mehdi Monemi","Matti Latva-aho"],"url":"https://arxiv.org/abs/2504.19112"}
{"created":"2025-04-29","title":"Towards Latency-Aware 3D Streaming Perception for Autonomous Driving","abstract":"Although existing 3D perception algorithms have demonstrated significant improvements in performance, their deployment on edge devices continues to encounter critical challenges due to substantial runtime latency. We propose a new benchmark tailored for online evaluation by considering runtime latency. Based on the benchmark, we build a Latency-Aware 3D Streaming Perception (LASP) framework that addresses the latency issue through two primary components: 1) latency-aware history integration, which extends query propagation into a continuous process, ensuring the integration of historical feature regardless of varying latency; 2) latency-aware predictive detection, a module that compensates the detection results with the predicted trajectory and the posterior accessed latency. By incorporating the latency-aware mechanism, our method shows generalization across various latency levels, achieving an online performance that closely aligns with 80\\% of its offline evaluation on the Jetson AGX Orin without any acceleration techniques.","authors":["Jiaqi Peng","Tai Wang","Jiangmiao Pang","Yuan Shen"],"url":"https://arxiv.org/abs/2504.19115"}
{"created":"2025-04-29","title":"Analysis and Elimination of Numerical Pressure Dependency in Coupled Stokes-Darcy Problem","abstract":"This paper presents a pressure-robust mixed finite element method (FEM) for the coupled Stokes-Darcy system. We revisits the rigorous theoretical framework of Layton et al. [2002], where velocity and pressure errors are coupled, masking pressure's influence on velocity accuracy. To investigate the pressure dependency, we introduce a auxiliary velocity projection that preserves discrete divergence and interface continuity constraints. By analyzing the difference between the discrete and projected velocities, we rigorously prove that classical FEM incurs pressure-dependent consistency errors due to inexact divergence enforcement and approximate interface conditions. To eliminate these errors, we design a pressure-robust method using divergence-free reconstruction operator, which enforce exact divergence constraint and interface continuity. Numerical examples confirm the theory: under high-pressure or low-viscosity, the proposed method reduces velocity errors by orders of magnitude compared to classical method.","authors":["Jiachuan Zhang"],"url":"https://arxiv.org/abs/2504.19116"}
{"created":"2025-04-29","title":"Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility","abstract":"The goal of the current study is to introduce a triadic human-AI collaboration framework for the automated vehicle domain. Previous classifications (e.g., SAE Levels of Automation) focus on defining automation levels based on who controls the vehicle. However, it remains unclear how human users and AI should collaborate in real-time, especially in dynamic driving contexts, where roles can shift frequently. To fill the gap, this study proposes a triadic human-AI collaboration framework with three AI roles (i.e., Advisor, Co-Pilot, and Guardian) that dynamically adapt to human needs. Overall, the study lays a foundation for developing adaptive, role-based human-AI collaboration strategies in automated vehicles.","authors":["Gaojian Huang","Yantong Jin","Wei-Hsiang Lo"],"url":"https://arxiv.org/abs/2504.19120"}
{"created":"2025-04-29","title":"Fast and memory-efficient BWT construction of repetitive texts using Lyndon grammars","abstract":"The Burrows-Wheeler Transform (BWT) serves as the basis for many important sequence indexes. On very large datasets (e.g. genomic databases), classical BWT construction algorithms are often infeasible because they usually need to have the entire dataset in main memory. Fortunately, such large datasets are often highly repetitive. It can thus be beneficial to compute the BWT from a compressed representation. We propose an algorithm for computing the BWT via the Lyndon straight-line program, a grammar based on the standard factorization of Lyndon words. Our algorithm can also be used to compute the extended BWT (eBWT) of a multiset of sequences. We empirically evaluate our implementation and find that we can compute the BWT and eBWT of very large datasets faster and/or with less memory than competing methods.","authors":["Jannik Olbrich"],"url":"https://arxiv.org/abs/2504.19123"}
{"created":"2025-04-29","title":"Blind Source Separation Based on Sparsity","abstract":"Blind source separation (BSS) is a key technique in array processing and data analysis, aiming to recover unknown sources from observed mixtures without knowledge of the mixing matrix. Classical independent component analysis (ICA) methods rely on the assumption that sources are mutually independent. To address limitations of ICA, sparsity-based methods have been introduced, which decompose source signals sparsely in a predefined dictionary. Morphological Component Analysis (MCA), based on sparse representation theory, assumes that a signal is a linear combination of components with distinct geometries, each sparsely representable in one dictionary and not in others. This approach has recently been applied to BSS with promising results.","authors":["Zhongxuan Li"],"url":"https://arxiv.org/abs/2504.19124"}
{"created":"2025-04-29","title":"DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning","abstract":"There has long been a belief that high-level semantics learning can benefit various downstream computer vision tasks. However, in the low-light image enhancement (LLIE) community, existing methods learn a brutal mapping between low-light and normal-light domains without considering the semantic information of different regions, especially in those extremely dark regions that suffer from severe information loss. To address this issue, we propose a new deep semantic prior-guided framework (DeepSPG) based on Retinex image decomposition for LLIE to explore informative semantic knowledge via a pre-trained semantic segmentation model and multimodal learning. Notably, we incorporate both image-level semantic prior and text-level semantic prior and thus formulate a multimodal learning framework with combinatorial deep semantic prior guidance for LLIE. Specifically, we incorporate semantic knowledge to guide the enhancement process via three designs: an image-level semantic prior guidance by leveraging hierarchical semantic features from a pre-trained semantic segmentation model; a text-level semantic prior guidance by integrating natural language semantic constraints via a pre-trained vision-language model; a multi-scale semantic-aware structure that facilitates effective semantic feature incorporation. Eventually, our proposed DeepSPG demonstrates superior performance compared to state-of-the-art methods across five benchmark datasets. The implementation details and code are publicly available at https://github.com/Wenyuzhy/DeepSPG.","authors":["Jialang Lu","Huayu Zhao","Huiyu Zhai","Xingxing Yang","Shini Han"],"url":"https://arxiv.org/abs/2504.19127"}
{"created":"2025-04-29","title":"Detecting speculative data flow vulnerabilities using weakest precondition reasoning","abstract":"Speculative execution is a hardware optimisation technique where a processor, while waiting on the completion of a computation required for an instruction, continues to execute later instructions based on a predicted value of the pending computation. It came to the forefront of security research in 2018 with the disclosure of two related attacks, Spectre and Meltdown. Since then many similar attacks have been identified. While there has been much research on using formal methods to detect speculative execution vulnerabilities based on predicted control flow, there has been significantly less on vulnerabilities based on predicted data flow. In this paper, we introduce an approach for detecting the data flow vulnerabilities, Spectre-STL and Spectre-PSF, using weakest precondition reasoning. We validate our approach on a suite of litmus tests used to validate related approaches in the literature.","authors":["Graeme Smith"],"url":"https://arxiv.org/abs/2504.19128"}
{"created":"2025-04-29","title":"Automatic Goal Clone Detection in Rocq","abstract":"Proof engineering in Rocq is a labor-intensive process, and as proof developments grow in size, redundancy and maintainability become challenges. One such redundancy is goal cloning, i.e., proving {\\alpha}-equivalent goals multiple times, leading to wasted effort and bloated proof scripts. In this paper, we introduce clone-finder, a novel technique for detecting goal clones in Rocq proofs. By leveraging the formal notion of {\\alpha}-equivalence for Gallina terms, clone-finder systematically identifies duplicated proof goals across large Rocq codebases. We evaluate clone-finder on 40 real-world Rocq projects from the CoqGym dataset. Our results reveal that each project contains an average of 27.73 instances of goal clone. We observed that the clones can be categorized as either exact goal duplication, generalization, or {\\alpha}-equivalent goals with different proofs, each signifying varying levels duplicate effort. Our findings highlight significant untapped potential for proof reuse in Rocq-based formal verification projects, paving the way for future improvements in automated proof engineering.","authors":["Ali Ghanbari"],"url":"https://arxiv.org/abs/2504.19129"}
{"created":"2025-04-29","title":"Making Physical Objects with Generative AI and Robotic Assembly: Considering Fabrication Constraints, Sustainability, Time, Functionality, and Accessibility","abstract":"3D generative AI enables rapid and accessible creation of 3D models from text or image inputs. However, translating these outputs into physical objects remains a challenge due to the constraints in the physical world. Recent studies have focused on improving the capabilities of 3D generative AI to produce fabricable outputs, with 3D printing as the main fabrication method. However, this workshop paper calls for a broader perspective by considering how fabrication methods align with the capabilities of 3D generative AI. As a case study, we present a novel system using discrete robotic assembly and 3D generative AI to make physical objects. Through this work, we identified five key aspects to consider in a physical making process based on the capabilities of 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can generate a wide range of 3D designs, requiring fabrication methods that can adapt to the variability of generative AI outputs. 2) Time: While generative AI can generate 3D models in seconds, fabricating physical objects can take hours or even days. Faster production could enable a closer iterative design loop between humans and AI in the making process. 3) Sustainability: Although text-to-3D models can generate thousands of models in the digital world, extending this capability to the real world would be resource-intensive, unsustainable and irresponsible. 4) Functionality: Unlike digital outputs from 3D generative AI models, the fabrication method plays a crucial role in the usability of physical objects. 5) Accessibility: While generative AI simplifies 3D model creation, the need for fabrication equipment can limit participation, making AI-assisted creation less inclusive. These five key aspects provide a framework for assessing how well a physical making process aligns with the capabilities of 3D generative AI and values in the world.","authors":["Alexander Htet Kyaw","Se Hwan Jeon","Miana Smith","Neil Gershenfeld"],"url":"https://arxiv.org/abs/2504.19131"}
{"created":"2025-04-29","title":"PAD: Phase-Amplitude Decoupling Fusion for Multi-Modal Land Cover Classification","abstract":"The fusion of Synthetic Aperture Radar (SAR) and RGB imagery for land cover classification remains challenging due to modality heterogeneity and the underutilization of spectral complementarity. Existing methods often fail to decouple shared structural features from modality-specific radiometric attributes, leading to feature conflicts and information loss. To address this issue, we propose Phase-Amplitude Decoupling (PAD), a frequency-aware framework that separates phase (modality-shared) and amplitude (modality-specific) components in the Fourier domain. Specifically, PAD consists of two key components: 1) Phase Spectrum Correction (PSC), which aligns cross-modal phase features through convolution-guided scaling to enhance geometric consistency, and 2) Amplitude Spectrum Fusion (ASF), which dynamically integrates high-frequency details and low-frequency structures using frequency-adaptive multilayer perceptrons. This approach leverages SAR's sensitivity to morphological features and RGB's spectral richness. Extensive experiments on WHU-OPT-SAR and DDHR-SK datasets demonstrate state-of-the-art performance. Our work establishes a new paradigm for physics-aware multi-modal fusion in remote sensing. The code will be available at https://github.com/RanFeng2/PAD.","authors":["Huiling Zheng","Xian Zhong","Bin Liu","Yi Xiao","Bihan Wen","Xiaofeng Li"],"url":"https://arxiv.org/abs/2504.19136"}
{"created":"2025-04-29","title":"Validation Framework for E-Contract and Smart Contract","abstract":"We propose and develop a framework for validating smart contracts derived from e-contracts. The goal is to ensure the generated smart contracts fulfil all the conditions outlined in their corresponding e-contracts. By confirming alignment between the smart contracts and their original agreements, this approach enhances trust and reliability in automated contract execution. The proposed framework will systematically compare and validate the terms and clauses of the e-contracts with the logic of the smart contracts. This validation confirms that the agreement is accurately translated into executable code. Automated verification identifies issues between the e-contracts and their smart contract counterparts. This proposed work will solve the problems of gap between legal language and code execution, this framework ensures seamless integration of smart contracts into the existing legal framework.","authors":["Sangharatna Godboley (NITMiner Technologies","Department of Computer Science","Engineering National Institute of Technology Warangal","Warangal","Telangana","India)","P. Radha Krishna (NITMiner Technologies","Department of Computer Science","Engineering National Institute of Technology Warangal","Warangal","Telangana","India)","Sunkara Sri Harika (NITMiner Technologies","Department of Computer Science","Engineering National Institute of Technology Warangal","Warangal","Telangana","India)","Pooja Varnam (NITMiner Technologies","Department of Computer Science","Engineering National Institute of Technology Warangal","Warangal","Telangana","India)"],"url":"https://arxiv.org/abs/2504.19137"}
{"created":"2025-04-29","title":"Fast and Robust: Task Sampling with Posterior and Diversity Synergies for Adaptive Decision-Makers in Randomized Environments","abstract":"Task robust adaptation is a long-standing pursuit in sequential decision-making. Some risk-averse strategies, e.g., the conditional value-at-risk principle, are incorporated in domain randomization or meta reinforcement learning to prioritize difficult tasks in optimization, which demand costly intensive evaluations. The efficiency issue prompts the development of robust active task sampling to train adaptive policies, where risk-predictive models are used to surrogate policy evaluation. This work characterizes the optimization pipeline of robust active task sampling as a Markov decision process, posits theoretical and practical insights, and constitutes robustness concepts in risk-averse scenarios. Importantly, we propose an easy-to-implement method, referred to as Posterior and Diversity Synergized Task Sampling (PDTS), to accommodate fast and robust sequential decision-making. Extensive experiments show that PDTS unlocks the potential of robust active task sampling, significantly improves the zero-shot and few-shot adaptation robustness in challenging tasks, and even accelerates the learning process under certain scenarios. Our project website is at https://thu-rllab.github.io/PDTS_project_page.","authors":["Yun Qu (Cheems)","Qi (Cheems)","Wang","Yixiu Mao","Yiqin Lv","Xiangyang Ji"],"url":"https://arxiv.org/abs/2504.19139"}
{"created":"2025-04-29","title":"Provable algorithms for multi-reference alignment over $\\SO(2)$","abstract":"The multi-reference alignment (MRA) problem involves reconstructing a signal from multiple noisy observations, each transformed by a random group element. In this paper, we focus on the group \\(\\mathrm{SO}(2)\\) of in-plane rotations and propose two computationally efficient algorithms with theoretical guarantees for accurate signal recovery under a non-uniform distribution over the group. The first algorithm exploits the spectral properties of the second moment of the data, while the second utilizes the frequency matching principle. Both algorithms achieve the optimal estimation rate in high-noise regimes, marking a significant advancement in the development of computationally efficient and statistically optimal methods for estimation problems over groups.","authors":["Gil Drozatz","Tamir Bendory","Nir Sharon"],"url":"https://arxiv.org/abs/2504.19140"}
{"created":"2025-04-29","title":"Reliable Thermal Monitoring of Electric Machines through Machine Learning","abstract":"The electrification of powertrains is rising as the objective for a more viable future is intensified. To ensure continuous and reliable operation without undesirable malfunctions, it is essential to monitor the internal temperatures of machines and keep them within safe operating limits. Conventional modeling methods can be complex and usually require expert knowledge. With the amount of data collected these days, it is possible to use information models to assess thermal behaviors. This paper investigates artificial intelligence techniques for monitoring the cooling efficiency of induction machines. Experimental data was collected under specific operating conditions, and three machine-learning models have been developed. The optimal configuration for each approach was determined through rigorous hyperparameter searches, and the models were evaluated using a variety of metrics. The three solutions performed well in monitoring the condition of the machine even under transient operation, highlighting the potential of data-driven methods in improving the thermal management.","authors":["Panagiotis Kakosimos"],"url":"https://arxiv.org/abs/2504.19141"}
{"created":"2025-04-29","title":"BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning","abstract":"Most large enterprises build predefined data pipelines and execute them periodically to process operational data using SQL queries for various tasks. A key issue in minimizing the overall makespan of these pipelines is the efficient scheduling of concurrent queries within the pipelines. Existing tools mainly rely on simple heuristic rules due to the difficulty of expressing the complex features and mutual influences of queries. The latest reinforcement learning (RL) based methods have the potential to capture these patterns from feedback, but it is non-trivial to apply them directly due to the large scheduling space, high sampling cost, and poor sample utilization.","authors":["Chenhao Xu","Chunyu Chen","Jinglin Peng","Jiannan Wang","Jun Gao"],"url":"https://arxiv.org/abs/2504.19142"}
{"created":"2025-04-29","title":"ChiseLLM: Unleashing the Power of Reasoning LLMs for Chisel Agile Hardware Development","abstract":"The growing demand for Domain-Specific Architecture (DSA) has driven the development of Agile Hardware Development Methodology (AHDM). Hardware Construction Language (HCL) like Chisel offers high-level abstraction features, making it an ideal language for HCL-Based AHDM. While Large Language Models (LLMs) excel in code generation tasks, they still face challenges with Chisel generation, particularly regarding syntax correctness and design variability. Recent reasoning models have significantly enhanced code generation capabilities through test-time scaling techniques. However, we found that reasoning models without domain adaptation cannot bring substantial benefits to Chisel code generation tasks. This paper presents ChiseLLM, a solution comprising data processing and transformation, prompt-guided reasoning trace synthesis, and domain-adapted model training. We constructed high-quality datasets from public RTL code resources and guided the model to adopt structured thinking patterns through prompt enhancement methods. Experiments demonstrate that our ChiseLLM-7B and ChiseLLM-32B models improved syntax correctness by 18.85% and 26.32% respectively over base models, while increasing variability design ability by 47.58% compared to baseline reasoning models. Our datasets and models are publicly available, providing high-performance, cost-effective models for HCL-Based AHDM, and offering an effective baseline for future research. Github repository: https://github.com/observerw/ChiseLLM","authors":["Bowei Wang","Jiaran Gao","Yelai Feng","Renzhi Chen","Shanshan Li","Lei Wang"],"url":"https://arxiv.org/abs/2504.19144"}
{"created":"2025-04-29","title":"Muyan-TTS: A Trainable Text-to-Speech Model Optimized for Podcast Scenarios with a $50K Budget","abstract":"Recent advancements in text-to-speech (TTS) models have been driven by the integration of large language models (LLMs), enhancing semantic comprehension and improving speech naturalness. However, existing LLM-based TTS models often lack open-source training code and efficient inference acceleration frameworks, limiting their accessibility and adaptability. Additionally, there is no publicly available TTS model specifically optimized for podcast scenarios, which are in high demand for voice interaction applications. To address these limitations, we introduce Muyan-TTS, an open-source trainable TTS model designed for podcast applications within a $50,000 budget. Our model is pre-trained on over 100,000 hours of podcast audio data, enabling zero-shot TTS synthesis with high-quality voice generation. Furthermore, Muyan-TTS supports speaker adaptation with dozens of minutes of target speech, making it highly customizable for individual voices. In addition to open-sourcing the model, we provide a comprehensive data collection and processing pipeline, a full training procedure, and an optimized inference framework that accelerates LLM-based TTS synthesis. Our code and models are available at https://github.com/MYZY-AI/Muyan-TTS.","authors":["Xin Li","Kaikai Jia","Hao Sun","Jun Dai","Ziyang Jiang"],"url":"https://arxiv.org/abs/2504.19146"}
{"created":"2025-04-29","title":"A Dynamic Fuzzy Rule and Attribute Management Framework for Fuzzy Inference Systems in High-Dimensional Data","abstract":"This paper presents an Adaptive Dynamic Attribute and Rule (ADAR) framework designed to address the challenges posed by high-dimensional data in neuro-fuzzy inference systems. By integrating dual weighting mechanisms-assigning adaptive importance to both attributes and rules-together with automated growth and pruning strategies, ADAR adaptively streamlines complex fuzzy models without sacrificing performance or interpretability. Experimental evaluations on four diverse datasets - Auto MPG (7 variables), Beijing PM2.5 (10 variables), Boston Housing (13 variables), and Appliances Energy Consumption (27 variables) show that ADAR-based models achieve consistently lower Root Mean Square Error (RMSE) compared to state-of-the-art baselines. On the Beijing PM2.5 dataset, for instance, ADAR-SOFENN attained an RMSE of 56.87 with nine rules, surpassing traditional ANFIS [12] and SOFENN [16] models. Similarly, on the high-dimensional Appliances Energy dataset, ADAR-ANFIS reached an RMSE of 83.25 with nine rules, outperforming established fuzzy logic approaches and interpretability-focused methods such as APLR. Ablation studies further reveal that combining rule-level and attribute-level weight assignment significantly reduces model overlap while preserving essential features, thereby enhancing explainability. These results highlight ADAR's effectiveness in dynamically balancing rule complexity and feature importance, paving the way for scalable, high-accuracy, and transparent neuro-fuzzy systems applicable to a range of real-world scenarios.","authors":["Ke Liu","Jing Ma","Edmund M-K Lai"],"url":"https://arxiv.org/abs/2504.19148"}
{"created":"2025-04-29","title":"The Trichotomy of Regular Property Testing","abstract":"Property testing is concerned with the design of algorithms making a sublinear number of queries to distinguish whether the input satisfies a given property or is far from having this property. A seminal paper of Alon, Krivelevich, Newman, and Szegedy in 2001 introduced property testing of formal languages: the goal is to determine whether an input word belongs to a given language, or is far from any word in that language. They constructed the first property testing algorithm for the class of all regular languages. This opened a line of work with improved complexity results and applications to streaming algorithms. In this work, we show a trichotomy result: the class of regular languages can be divided into three classes, each associated with an optimal query complexity. Our analysis yields effective characterizations for all three classes using so-called minimal blocking sequences, reasoning directly and combinatorially on automata.","authors":["Gabriel Bathie","Nathana\\\"el Fijalkow","Corto Mascle"],"url":"https://arxiv.org/abs/2504.19152"}
{"created":"2025-04-29","title":"Comparative Analysis of AI-Driven Security Approaches in DevSecOps: Challenges, Solutions, and Future Directions","abstract":"The integration of security within DevOps, known as DevSecOps, has gained traction in modern software development to address security vulnerabilities while maintaining agility. Artificial Intelligence (AI) and Machine Learning (ML) have been increasingly leveraged to enhance security automation, threat detection, and compliance enforcement. However, existing studies primarily focus on individual aspects of AI-driven security in DevSecOps, lacking a structured comparison of methodologies. This study conducts a systematic literature review (SLR) to analyze and compare AI-driven security solutions in DevSecOps, evaluating their technical capabilities, implementation challenges, and operational impacts. The findings reveal gaps in empirical validation, scalability, and integration of AI in security automation. The study highlights best practices, identifies research gaps, and proposes future directions for optimizing AI-based security frameworks in DevSecOps.","authors":["Farid Binbeshr","Muhammad Imam"],"url":"https://arxiv.org/abs/2504.19154"}
{"created":"2025-04-29","title":"Parameter estimation for multivariate exponential sums via iterative rational approximation","abstract":"We present two new methods for multivariate exponential analysis. In [7], we developed a new algorithm for reconstruction of univariate exponential sums by exploiting the rational structure of their Fourier coefficients and reconstructing this rational structure with the AAA (adaptive Antoulas-Anderson) method for rational approximation [15]. In this paper, we extend these ideas to the multivariate setting. Similarly as in univariate case, the Fourier coefficients of multivariate exponential sums have a rational structure and the multivariate exponential recovery problem can be reformulated as multivariate rational interpolation problem. We develop two approaches to solve this special multivariate rational interpolation problem by reducing it to the several univariate ones, which are then solved again via the univariate AAA method. Our first approach is based on using indices of the Fourier coefficients chosen from some sparse grid, which ensures efficient reconstruction using a respectively small amount of input data. The second approach is based on using the full grid of indices of the Fourier coefficients and relies on the idea of recursive dimension reduction. We demonstrate performance of our methods with several numerical examples.","authors":["Nadiia Derevianko","Lennart Aljoscha H\\\"ubner"],"url":"https://arxiv.org/abs/2504.19157"}
{"created":"2025-04-29","title":"SnuggleSense: Empowering Online Harm Survivors Through a Structured Sensemaking Process","abstract":"Online interpersonal harm, such as cyberbullying and sexual harassment, remains a pervasive issue on social media platforms. Traditional approaches, primarily content moderation, often overlook survivors' needs and agency. We introduce SnuggleSense, a system that empowers survivors through structured sensemaking. Inspired by restorative justice practices, SnuggleSense guides survivors through reflective questions, offers personalized recommendations from similar survivors, and visualizes plans using interactive sticky notes. A controlled experiment demonstrates that SnuggleSense significantly enhances sensemaking compared to an unstructured process of making sense of the harm. We argue that SnuggleSense fosters community awareness, cultivates a supportive survivor network, and promotes a restorative justice-oriented approach toward restoration and healing. We also discuss design insights, such as tailoring informational support and providing guidance while preserving survivors' agency.","authors":["Sijia Xiao","Haodi Zou","Amy Mathews","Jingshu Rui","Coye Cheshire","Niloufar Salehi"],"url":"https://arxiv.org/abs/2504.19158"}
{"created":"2025-04-29","title":"RadioFormer: A Multiple-Granularity Radio Map Estimation Transformer with 1\\textpertenthousand Spatial Sampling","abstract":"The task of radio map estimation aims to generate a dense representation of electromagnetic spectrum quantities, such as the received signal strength at each grid point within a geographic region, based on measurements from a subset of spatially distributed nodes (represented as pixels). Recently, deep vision models such as the U-Net have been adapted to radio map estimation, whose effectiveness can be guaranteed with sufficient spatial observations (typically 0.01% to 1% of pixels) in each map, to model local dependency of observed signal power. However, such a setting of sufficient measurements can be less practical in real-world scenarios, where extreme sparsity in spatial sampling can be widely encountered. To address this challenge, we propose RadioFormer, a novel multiple-granularity transformer designed to handle the constraints posed by spatial sparse observations. Our RadioFormer, through a dual-stream self-attention (DSA) module, can respectively discover the correlation of pixel-wise observed signal power and also learn patch-wise buildings' geometries in a style of multiple granularities, which are integrated into multi-scale representations of radio maps by a cross stream cross-attention (CCA) module. Extensive experiments on the public RadioMapSeer dataset demonstrate that RadioFormer outperforms state-of-the-art methods in radio map estimation while maintaining the lowest computational cost. Furthermore, the proposed approach exhibits exceptional generalization capabilities and robust zero-shot performance, underscoring its potential to advance radio map estimation in a more practical setting with very limited observation nodes.","authors":["Zheng Fang","Kangjun Liu","Ke Chen","Qingyu Liu","Jianguo Zhang","Lingyang Song","Yaowei Wang"],"url":"https://arxiv.org/abs/2504.19161"}
{"created":"2025-04-29","title":"SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning","abstract":"Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a \"sneaky generator\" that deliberately produces erroneous steps designed to be difficult to detect, and a \"critic\" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.","authors":["Jiaqi Chen","Bang Zhang","Ruotian Ma","Peisong Wang","Xiaodan Liang","Zhaopeng Tu","Xiaolong Li","Kwan-Yee K. Wong"],"url":"https://arxiv.org/abs/2504.19162"}
{"created":"2025-04-29","title":"Bernstein Bounds for Caustics","abstract":"Systematically simulating specular light transport requires an exhaustive search for primitive tuples containing admissible paths. Given the extreme inefficiency of enumerating all combinations, we propose to significantly reduce the search domain by sampling such tuples. The challenge is to design proper sampling probabilities that keep the noise level controllable. Our key insight is that by bounding the range of irradiance contributed by each primitive tuple at a given position, we can sample a subset of primitive tuples with potentially high contributions. Although low-contribution tuples are assigned a negligible probability, the overall variance remains low. Therefore, we derive vertex position and irradiance bounds for each primitive tuple, introducing a bounding property of rational functions on the Bernstein basis. When formulating position and irradiance expressions into rational functions, we handle non-rational components through remainder variables to maintain validity. Finally, we carefully design the sampling probabilities by optimizing the upper bound of the variance, expressed only using the position and irradiance bound. The proposed primitive sampling is intrinsically unbiased. It can be seamlessly combined with various unbiased and biased root-finding techniques within a local primitive domain. Extensive evaluations show that our method enables fast and reliable rendering of complex caustic effects.","authors":["Zhimin Fan","Chen Wang","Yiming Wang","Boxuan Li","Yuxuan Guo","Ling-Qi Yan","Yanwen Guo","Jie Guo"],"url":"https://arxiv.org/abs/2504.19163"}
{"created":"2025-04-29","title":"IM-Portrait: Learning 3D-aware Video Diffusion for PhotorealisticTalking Heads from Monocular Videos","abstract":"We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data.","authors":["Yuan Li","Ziqian Bai","Feitong Tan","Zhaopeng Cui","Sean Fanello","Yinda Zhang"],"url":"https://arxiv.org/abs/2504.19165"}
{"created":"2025-04-29","title":"SA-MIMO: Scalable Quantum-Based Wireless Communications","abstract":"Rydberg atomic receivers offer a quantum-native alternative to conventional RF front-ends by directly detecting electromagnetic fields via highly excited atomic states. While their quantum-limited sensitivity and hardware simplicity make them promising for future wireless systems, extending their use to scalable multi-antenna and multi-carrier configurations, termed Scalable Atomic-MIMO (SA-MIMO), remains largely unexplored. This paper introduces a novel RF transmitter-atomic receiver architecture that addresses this gap. The core idea lies in a novel modulation technique called Phase-Rotated Symbol Spreading (PRSS), which transforms the nonlinear phase retrieval problem inherent to atomic detection into a tractable linear demultiplexing task. PRSS enables efficient signal processing and supports scalable MUX/DeMUX operations in both atomic MIMO and atomic OFDM systems. Simulation results show that the proposed system achieves up to 2.5 dB gain under optimal maximum-likelihood detection and over 10 dB under suboptimal detection in MIMO settings. These results establish PRSS assisted SA-MIMO as a promising architecture for realizing high-sensitivity, interference-resilient atomic wireless communication.","authors":["Jiuyu Liu","Yi Ma","Rahim Tafazolli"],"url":"https://arxiv.org/abs/2504.19170"}
{"created":"2025-04-29","title":"GPU-Accelerated Parallel Selected Inversion for Structured Matrices Using sTiles","abstract":"Selected inversion is essential for applications such as Bayesian inference, electronic structure calculations, and inverse covariance estimation, where computing only specific elements of large sparse matrix inverses significantly reduces computational and memory overhead. We present an efficient implementation of a two-phase parallel algorithm for computing selected elements of the inverse of a sparse symmetric matrix A, which can be expressed as A = LL^T through sparse Cholesky factorization. Our approach leverages a tile-based structure, focusing on selected dense tiles to optimize computational efficiency and parallelism. While the focus is on arrowhead matrices, the method can be extended to handle general structured matrices. Performance evaluations on a dual-socket 26-core Intel Xeon CPU server demonstrate that sTiles outperforms state-of-the-art direct solvers such as Panua-PARDISO, achieving up to 13X speedup on large-scale structured matrices. Additionally, our GPU implementation using an NVIDIA A100 GPU demonstrates substantial acceleration over its CPU counterpart, achieving up to 5X speedup for large, high-bandwidth matrices with high computational intensity. These results underscore the robustness and versatility of sTiles, validating its effectiveness across various densities and problem configurations.","authors":["Esmail Abdul Fattah","Hatem Ltaief","Havard Rue","David Keyes"],"url":"https://arxiv.org/abs/2504.19171"}
{"created":"2025-04-29","title":"CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation","abstract":"We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified Continuous Latent Representation. Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation.","authors":["Xueqi Ma","Yilin Liu","Tianlong Gao","Qirui Huang","Hui Huang"],"url":"https://arxiv.org/abs/2504.19174"}
{"created":"2025-04-29","title":"Newton-Puiseux Analysis for Interpretability and Calibration of Complex-Valued Neural Networks","abstract":"Complex-valued neural networks (CVNNs) excel where phase matters, yet their multi-sheeted decision surfaces defy standard explainability and calibration tools. We propose a \\emph{Newton-Puiseux} framework that fits a local polynomial surrogate to a high-uncertainty input and analytically decomposes this surrogate into fractional-power series. The resulting Puiseux expansions, dominant Puiseux coefficients, and phase-aligned curvature descriptors deliver closed-form estimates of robustness and over-confidence that gradient - or perturbation-based methods (saliency, LIME, SHAP) cannot provide. On a controlled $\\mathbb{C}^2$ helix the surrogate attains RMSE $< 0.09$ while recovering the number of decision sheets; quartic coefficients predict adversarial flip radii within $10^{-3}$. On the real-world MIT-BIH arrhythmia corpus, Puiseux-guided, phase-aware temperature scaling lowers expected calibration error from 0.087 to 0.034, contributing to the advancement of CVNNs. Full code, pre-trained weights, and scripts are at https://github.com/piotrmgs/puiseux-cvnn.","authors":["Piotr Migus"],"url":"https://arxiv.org/abs/2504.19176"}
{"created":"2025-04-29","title":"Relative Contrastive Learning for Sequential Recommendation with Similarity-based Positive Pair Selection","abstract":"Contrastive Learning (CL) enhances the training of sequential recommendation (SR) models through informative self-supervision signals. Existing methods often rely on data augmentation strategies to create positive samples and promote representation invariance. Some strategies such as item reordering and item substitution may inadvertently alter user intent. Supervised Contrastive Learning (SCL) based methods find an alternative to augmentation-based CL methods by selecting same-target sequences (interaction sequences with the same target item) to form positive samples. However, SCL-based methods suffer from the scarcity of same-target sequences and consequently lack enough signals for contrastive learning. In this work, we propose to use similar sequences (with different target items) as additional positive samples and introduce a Relative Contrastive Learning (RCL) framework for sequential recommendation. RCL comprises a dual-tiered positive sample selection module and a relative contrastive learning module. The former module selects same-target sequences as strong positive samples and selects similar sequences as weak positive samples. The latter module employs a weighted relative contrastive loss, ensuring that each sequence is represented closer to its strong positive samples than its weak positive samples. We apply RCL on two mainstream deep learning-based SR models, and our empirical results reveal that RCL can achieve 4.88% improvement averagely than the state-of-the-art SR methods on five public datasets and one private dataset.","authors":["Zhikai Wang","Yanyan Shen","Zexi Zhang","Li He","Yichun Li","Hao Gu","Yinghua Zhang"],"url":"https://arxiv.org/abs/2504.19178"}
{"created":"2025-04-29","title":"A Design Framework for operationalizing Trustworthy Artificial Intelligence in Healthcare: Requirements, Tradeoffs and Challenges for its Clinical Adoption","abstract":"Artificial Intelligence (AI) holds great promise for transforming healthcare, particularly in disease diagnosis, prognosis, and patient care. The increasing availability of digital medical data, such as images, omics, biosignals, and electronic health records, combined with advances in computing, has enabled AI models to approach expert-level performance. However, widespread clinical adoption remains limited, primarily due to challenges beyond technical performance, including ethical concerns, regulatory barriers, and lack of trust. To address these issues, AI systems must align with the principles of Trustworthy AI (TAI), which emphasize human agency and oversight, algorithmic robustness, privacy and data governance, transparency, bias and discrimination avoidance, and accountability. Yet, the complexity of healthcare processes (e.g., screening, diagnosis, prognosis, and treatment) and the diversity of stakeholders (clinicians, patients, providers, regulators) complicate the integration of TAI principles. To bridge the gap between TAI theory and practical implementation, this paper proposes a design framework to support developers in embedding TAI principles into medical AI systems. Thus, for each stakeholder identified across various healthcare processes, we propose a disease-agnostic collection of requirements that medical AI systems should incorporate to adhere to the principles of TAI. Additionally, we examine the challenges and tradeoffs that may arise when applying these principles in practice. To ground the discussion, we focus on cardiovascular diseases, a field marked by both high prevalence and active AI innovation, and demonstrate how TAI principles have been applied and where key obstacles persist.","authors":["Pedro A. Moreno-S\\'anchez","Javier Del Ser","Mark van Gils","Jussi Hernesniemi"],"url":"https://arxiv.org/abs/2504.19179"}
{"created":"2025-04-29","title":"Critical Considerations on Effort-aware Software Defect Prediction Metrics","abstract":"Background. Effort-aware metrics (EAMs) are widely used to evaluate the effectiveness of software defect prediction models, while accounting for the effort needed to analyze the software modules that are estimated defective. The usual underlying assumption is that this effort is proportional to the modules' size measured in LOC. However, the research on module analysis (including code understanding, inspection, testing, etc.) suggests that module analysis effort may be better correlated to code attributes other than size.","authors":["Luigi Lavazza (Universit\\`a degli Studi dell'Insubria)","Gabriele Rotoloni (Universit\\`a degli Studi dell'Insubria)","Sandro Morasca (Universit\\`a degli Studi dell'Insubria)"],"url":"https://arxiv.org/abs/2504.19181"}
{"created":"2025-04-29","title":"Segmenting Objectiveness and Task-awareness Unknown Region for Autonomous Driving","abstract":"With the emergence of transformer-based architectures and large language models (LLMs), the accuracy of road scene perception has substantially advanced. Nonetheless, current road scene segmentation approaches are predominantly trained on closed-set data, resulting in insufficient detection capabilities for out-of-distribution (OOD) objects. To overcome this limitation, road anomaly detection methods have been proposed. However, existing methods primarily depend on image inpainting and OOD distribution detection techniques, facing two critical issues: (1) inadequate consideration of the objectiveness attributes of anomalous regions, causing incomplete segmentation when anomalous objects share similarities with known classes, and (2) insufficient attention to environmental constraints, leading to the detection of anomalies irrelevant to autonomous driving tasks. In this paper, we propose a novel framework termed Segmenting Objectiveness and Task-Awareness (SOTA) for autonomous driving scenes. Specifically, SOTA enhances the segmentation of objectiveness through a Semantic Fusion Block (SFB) and filters anomalies irrelevant to road navigation tasks using a Scene-understanding Guided Prompt-Context Adaptor (SG-PCA). Extensive empirical evaluations on multiple benchmark datasets, including Fishyscapes Lost and Found, Segment-Me-If-You-Can, and RoadAnomaly, demonstrate that the proposed SOTA consistently improves OOD detection performance across diverse detectors, achieving robust and accurate segmentation outcomes.","authors":["Mi Zheng","Guanglei Yang","Zitong Huang","Zhenhua Guo","Kevin Han","Wangmeng Zuo"],"url":"https://arxiv.org/abs/2504.19183"}
{"created":"2025-04-29","title":"LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition","abstract":"In autonomous driving, place recognition is critical for global localization in GPS-denied environments. LiDAR and radar-based place recognition methods have garnered increasing attention, as LiDAR provides precise ranging, whereas radar excels in adverse weather resilience. However, effectively leveraging LiDAR-radar fusion for place recognition remains challenging. The noisy and sparse nature of radar data limits its potential to further improve recognition accuracy. In addition, heterogeneous radar configurations complicate the development of unified cross-modality fusion frameworks. In this paper, we propose LRFusionPR, which improves recognition accuracy and robustness by fusing LiDAR with either single-chip or scanning radar. Technically, a dual-branch network is proposed to fuse different modalities within the unified polar coordinate bird's eye view (BEV) representation. In the fusion branch, cross-attention is utilized to perform cross-modality feature interactions. The knowledge from the fusion branch is simultaneously transferred to the distillation branch, which takes radar as its only input to further improve the robustness. Ultimately, the descriptors from both branches are concatenated, producing the multimodal global descriptor for place retrieval. Extensive evaluations on multiple datasets demonstrate that our LRFusionPR achieves accurate place recognition, while maintaining robustness under varying weather conditions. Our open-source code will be released at https://github.com/QiZS-BIT/LRFusionPR.","authors":["Zhangshuo Qi","Luqi Cheng","Zijie Zhou","Guangming Xiong"],"url":"https://arxiv.org/abs/2504.19186"}
{"created":"2025-04-29","title":"Hierarchical Attention Generates Better Proofs","abstract":"Large language models (LLMs) have shown promise in formal theorem proving, but their token-level processing often fails to capture the inherent hierarchical nature of mathematical proofs. We introduce \\textbf{Hierarchical Attention}, a regularization method that aligns LLMs' attention mechanisms with mathematical reasoning structures. Our approach establishes a five-level hierarchy from foundational elements to high-level concepts, ensuring structured information flow in proof generation. Experiments demonstrate that our method improves proof success rates by 2.05\\% on miniF2F and 1.69\\% on ProofNet while reducing proof complexity by 23.81\\% and 16.50\\% respectively. The code is available at https://github.com/Car-pe/HAGBP.","authors":["Jianlong Chen","Chao Li","Yang Yuan","Andrew C Yao"],"url":"https://arxiv.org/abs/2504.19188"}
{"created":"2025-04-29","title":"Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation","abstract":"Storyboarding is widely used for creating 3D animations. Animators use the 2D sketches in storyboards as references to craft the desired 3D animations through a trial-and-error process. The traditional approach requires exceptional expertise and is both labor-intensive and time-consuming. Consequently, there is a high demand for automated methods that can directly translate 2D storyboard sketches into 3D animations. This task is under-explored to date and inspired by the significant advancements of motion diffusion models, we propose to address it from the perspective of conditional motion synthesis. We thus present Sketch2Anim, composed of two key modules for sketch constraint understanding and motion generation. Specifically, due to the large domain gap between the 2D sketch and 3D motion, instead of directly conditioning on 2D inputs, we design a 3D conditional motion generator that simultaneously leverages 3D keyposes, joint trajectories, and action words, to achieve precise and fine-grained motion control. Then, we invent a neural mapper dedicated to aligning user-provided 2D sketches with their corresponding 3D keyposes and trajectories in a shared embedding space, enabling, for the first time, direct 2D control of motion generation. Our approach successfully transfers storyboards into high-quality 3D motions and inherently supports direct 3D animation editing, thanks to the flexibility of our multi-conditional motion generator. Comprehensive experiments and evaluations, and a user perceptual study demonstrate the effectiveness of our approach.","authors":["Lei Zhong","Chuan Guo","Yiming Xie","Jiawei Wang","Changjian Li"],"url":"https://arxiv.org/abs/2504.19189"}
{"created":"2025-04-29","title":"WuNeng: Hybrid State with Attention","abstract":"The WuNeng architecture introduces a novel approach to enhancing the expressivity and power of large language models by integrating recurrent neural network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing heightened contextual coherence over reducing KV cache size. Building upon the hybrid-head concept from Hymba, WuNeng augments standard multi-head attention with additional RWKV-7 state-driven heads, rather than replacing existing heads, to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among standard, state-driven, and newly introduced middle heads, leveraging concatenation, additive modulation, and gated fusion for robust information integration. Furthermore, a multi-token state processing mechanism harnesses the continuous RWKV-7 state to capture intricate, sequence-wide dependencies, significantly boosting expressivity. Remarkably, these enhancements are achieved with minimal additional parameters, ensuring efficiency while empowering the model to excel in complex reasoning and sequence generation tasks. WuNeng sets a new standard for balancing expressivity and computational efficiency in modern neural architectures.","authors":["Liu Xiao","Li Zhiyuan","Lin Yueyu"],"url":"https://arxiv.org/abs/2504.19191"}
{"created":"2025-04-29","title":"Strong and weak convergence orders of numerical methods for SDEs driven by time-changed L\\'{e}vy noise","abstract":"This work investigates the strong and weak convergence orders of numerical methods for SDEs driven by time-changed L\\'{e}vy noise under the globally Lipschitz conditions. Based on the duality theorem, we prove that the numerical approximation generated by the stochastic $\\theta$ method with $\\theta \\in [0,1]$ and the simulation of inverse subordinator converges strongly with order $1/2$. Moreover, the numerical approximation combined with the Euler--Maruyama method and the estimate of inverse subordinator is shown to have the weak convergence order $1$ by means of the Kolmogorov backward partial integro differential equations. These theoretical results are finally confirmed by some numerical experiments.","authors":["Ziheng Chen","Jiao Liu","Anxin Wu"],"url":"https://arxiv.org/abs/2504.19192"}
{"created":"2025-04-29","title":"Trajectory Planning with Model Predictive Control for Obstacle Avoidance Considering Prediction Uncertainty","abstract":"This paper introduces a novel trajectory planner for autonomous robots, specifically designed to enhance navigation by incorporating dynamic obstacle avoidance within the Robot Operating System 2 (ROS2) and Navigation 2 (Nav2) framework. The proposed method utilizes Model Predictive Control (MPC) with a focus on handling the uncertainties associated with the movement prediction of dynamic obstacles. Unlike existing Nav2 trajectory planners which primarily deal with static obstacles or react to the current position of dynamic obstacles, this planner predicts future obstacle positions using a stochastic Vector Auto-Regressive Model (VAR). The obstacles' future positions are represented by probability distributions, and collision avoidance is achieved through constraints based on the Mahalanobis distance, ensuring the robot avoids regions where obstacles are likely to be. This approach considers the robot's kinodynamic constraints, enabling it to track a reference path while adapting to real-time changes in the environment. The paper details the implementation, including obstacle prediction, tracking, and the construction of feasible sets for MPC. Simulation results in a Gazebo environment demonstrate the effectiveness of this method in scenarios where robots must navigate around each other, showing improved collision avoidance capabilities.","authors":["Eric Sch\\\"oneberg","Michael Schr\\\"oder","Daniel G\\\"orges","Hans D. Schotten"],"url":"https://arxiv.org/abs/2504.19193"}
{"created":"2025-04-29","title":"VTire: A Bimodal Visuotactile Tire with High-Resolution Sensing Capability","abstract":"Developing smart tires with high sensing capability is significant for improving the moving stability and environmental adaptability of wheeled robots and vehicles. However, due to the classical manufacturing design, it is always challenging for tires to infer external information precisely. To this end, this paper introduces a bimodal sensing tire, which can simultaneously capture tactile and visual data. By leveraging the emerging visuotactile techniques, the proposed smart tire can realize various functions, including terrain recognition, ground crack detection, load sensing, and tire damage detection. Besides, we optimize the material and structure of the tire to ensure its outstanding elasticity, toughness, hardness, and transparency. In terms of algorithms, a transformer-based multimodal classification algorithm, a load detection method based on finite element analysis, and a contact segmentation algorithm have been developed. Furthermore, we construct an intelligent mobile platform to validate the system's effectiveness and develop visual and tactile datasets in complex terrains. The experimental results show that our multimodal terrain sensing algorithm can achieve a classification accuracy of 99.2\\%, a tire damage detection accuracy of 97\\%, a 98\\% success rate in object search, and the ability to withstand tire loading weights exceeding 35 kg. In addition, we open-source our algorithms, hardware, and datasets at https://sites.google.com/view/vtire.","authors":["Shoujie Li","Jianle Xu","Tong Wu","Yang Yang","Yanbo Chen","Xueqian Wang","Wenbo Ding","Xiao-Ping Zhang"],"url":"https://arxiv.org/abs/2504.19194"}
{"created":"2025-04-29","title":"NANO-SLAM : Natural Gradient Gaussian Approximation for Vehicle SLAM","abstract":"Accurate localization is a challenging task for autonomous vehicles, particularly in GPS-denied environments such as urban canyons and tunnels. In these scenarios, simultaneous localization and mapping (SLAM) offers a more robust alternative to GPS-based positioning, enabling vehicles to determine their position using onboard sensors and surrounding environment's landmarks. Among various vehicle SLAM approaches, Rao-Blackwellized particle filter (RBPF) stands out as one of the most widely adopted methods due to its efficient solution with logarithmic complexity relative to the map size. RBPF approximates the posterior distribution of the vehicle pose using a set of Monte Carlo particles through two main steps: sampling and importance weighting. The key to effective sampling lies in solving a distribution that closely approximates the posterior, known as the sampling distribution, to accelerate convergence. Existing methods typically derive this distribution via linearization, which introduces significant approximation errors due to the inherent nonlinearity of the system. To address this limitation, we propose a novel vehicle SLAM method called \\textit{N}atural Gr\\textit{a}dient Gaussia\\textit{n} Appr\\textit{o}ximation (NANO)-SLAM, which avoids linearization errors by modeling the sampling distribution as the solution to an optimization problem over Gaussian parameters and solving it using natural gradient descent. This approach improves the accuracy of the sampling distribution and consequently enhances localization performance. Experimental results on the long-distance Sydney Victoria Park vehicle SLAM dataset show that NANO-SLAM achieves over 50\\% improvement in localization accuracy compared to the most widely used vehicle SLAM algorithms, with minimal additional computational cost.","authors":["Tianyi Zhang","Wenhan Cao","Chang Liu","Feihong Zhang","Wei Wu","Shengbo Eben Li"],"url":"https://arxiv.org/abs/2504.19195"}
{"created":"2025-04-29","title":"Generative Adversarial Network based Voice Conversion: Techniques, Challenges, and Recent Advancements","abstract":"Voice conversion (VC) stands as a crucial research area in speech synthesis, enabling the transformation of a speaker's vocal characteristics to resemble another while preserving the linguistic content. This technology has broad applications, including automated movie dubbing, speech-to-singing conversion, and assistive devices for pathological speech rehabilitation. With the increasing demand for high-quality and natural-sounding synthetic voices, researchers have developed a wide range of VC techniques. Among these, generative adversarial network (GAN)-based approaches have drawn considerable attention for their powerful feature-mapping capabilities and potential to produce highly realistic speech. Despite notable advancements, challenges such as ensuring training stability, maintaining linguistic consistency, and achieving perceptual naturalness continue to hinder progress in GAN-based VC systems. This systematic review presents a comprehensive analysis of the voice conversion landscape, highlighting key techniques, key challenges, and the transformative impact of GANs in the field. The survey categorizes existing methods, examines technical obstacles, and critically evaluates recent developments in GAN-based VC. By consolidating and synthesizing research findings scattered across the literature, this review provides a structured understanding of the strengths and limitations of different approaches. The significance of this survey lies in its ability to guide future research by identifying existing gaps, proposing potential directions, and offering insights for building more robust and efficient VC systems. Overall, this work serves as an essential resource for researchers, developers, and practitioners aiming to advance the state-of-the-art (SOTA) in voice conversion technology.","authors":["Sandipan Dhar","Nanda Dulal Jana","Swagatam Das"],"url":"https://arxiv.org/abs/2504.19197"}
{"created":"2025-04-29","title":"Adaptive Dual-domain Learning for Underwater Image Enhancement","abstract":"Recently, learning-based Underwater Image Enhancement (UIE) methods have demonstrated promising performance. However, existing learning-based methods still face two challenges. 1) They rarely consider the inconsistent degradation levels in different spatial regions and spectral bands simultaneously. 2) They treat all regions equally, ignoring that the regions with high-frequency details are more difficult to reconstruct. To address these challenges, we propose a novel UIE method based on spatial-spectral dual-domain adaptive learning, termed SS-UIE. Specifically, we first introduce a spatial-wise Multi-scale Cycle Selective Scan (MCSS) module and a Spectral-Wise Self-Attention (SWSA) module, both with linear complexity, and combine them in parallel to form a basic Spatial-Spectral block (SS-block). Benefiting from the global receptive field of MCSS and SWSA, SS-block can effectively model the degradation levels of different spatial regions and spectral bands, thereby enabling degradation level-based dual-domain adaptive UIE. By stacking multiple SS-blocks, we build our SS-UIE network. Additionally, a Frequency-Wise Loss (FWL) is introduced to narrow the frequency-wise discrepancy and reinforce the model's attention on the regions with high-frequency details. Extensive experiments validate that the SS-UIE technique outperforms state-of-the-art UIE methods while requiring cheaper computational and memory costs.","authors":["Lingtao Peng","Liheng Bian"],"url":"https://arxiv.org/abs/2504.19198"}
{"created":"2025-04-29","title":"HetGL2R: Learning to Rank Critical Road Segments via Attributed Heterogeneous Graph Random Walks","abstract":"Accurately identifying critical nodes with high spatial influence in road networks is essential for enhancing the efficiency of traffic management and urban planning. However, existing node importance ranking methods mainly rely on structural features and topological information, often overlooking critical factors such as origin-destination (OD) demand and route information. This limitation leaves considerable room for improvement in ranking accuracy. To address this issue, we propose HetGL2R, an attributed heterogeneous graph learning approach for ranking node importance in road networks. This method introduces a tripartite graph (trip graph) to model the structure of the road network, integrating OD demand, route choice, and various structural features of road segments. Based on the trip graph, we design an embedding method to learn node representations that reflect the spatial influence of road segments. The method consists of a heterogeneous random walk sampling algorithm (HetGWalk) and a Transformer encoder. HetGWalk constructs multiple attribute-guided graphs based on the trip graph to enrich the diversity of semantic associations between nodes. It then applies a joint random walk mechanism to convert both topological structures and node attributes into sequences, enabling the encoder to capture spatial dependencies more effectively among road segments. Finally, a listwise ranking strategy is employed to evaluate node importance. To validate the performance of our method, we construct two synthetic datasets using SUMO based on simulated road networks. Experimental results demonstrate that HetGL2R significantly outperforms baselines in incorporating OD demand and route choice information, achieving more accurate and robust node ranking. Furthermore, we conduct a case study using real-world taxi trajectory data from Beijing, further verifying the practicality of the proposed method.","authors":["Ming Xu","Jinrong Xiang","Zilong Xie","Xiangfu Meng"],"url":"https://arxiv.org/abs/2504.19199"}
{"created":"2025-04-29","title":"The Satisfiability and Validity Problems for Probabilistic Computational Tree Logic are Highly Undecidable","abstract":"The Probabilistic Computational Tree Logic (PCTL) is the main specification formalism for discrete probabilistic systems modeled by Markov chains. Despite serious research attempts, the decidability of PCTL satisfiability and validity problems remained unresolved for 30 years. We show that both problems are highly undecidable, i.e., beyond the arithmetical hierarchy. Consequently, there is no sound and complete deductive system for PCTL.","authors":["Miroslav Chodil","Anton\\'in Ku\\v{c}era"],"url":"https://arxiv.org/abs/2504.19207"}
{"created":"2025-04-29","title":"Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora","abstract":"We measure the effects of several implementation choices for the Dynamic Embedded Topic Model, as applied to five distinct diachronic corpora, with the goal of isolating important decisions for its use and further development. We identify priorities that will maximize utility in applied scholarship, including the practical scalability of vocabulary size to best exploit the strengths of embedded representations, and more flexible modeling of intervals to accommodate the uneven temporal distributions of historical writing. Of similar importance, we find performance is not significantly or consistently affected by several aspects that otherwise limit the model's application or might consume the resources of a grid search.","authors":["Elisabeth Fittschen","Bella Xia","Leib Celnik","Paul Dilley","Tom Lippincott"],"url":"https://arxiv.org/abs/2504.19209"}
{"created":"2025-04-29","title":"FlexPara: Flexible Neural Surface Parameterization","abstract":"Surface parameterization is a fundamental geometry processing task, laying the foundations for the visual presentation of 3D assets and numerous downstream shape analysis scenarios. Conventional parameterization approaches demand high-quality mesh triangulation and are restricted to certain simple topologies unless additional surface cutting and decomposition are provided. In practice, the optimal configurations (e.g., type of parameterization domains, distribution of cutting seams, number of mapping charts) may vary drastically with different surface structures and task characteristics, thus requiring more flexible and controllable processing pipelines. To this end, this paper introduces FlexPara, an unsupervised neural optimization framework to achieve both global and multi-chart surface parameterizations by establishing point-wise mappings between 3D surface points and adaptively-deformed 2D UV coordinates. We ingeniously design and combine a series of geometrically-interpretable sub-networks, with specific functionalities of cutting, deforming, unwrapping, and wrapping, to construct a bi-directional cycle mapping framework for global parameterization without the need for manually specified cutting seams. Furthermore, we construct a multi-chart parameterization framework with adaptively-learned chart assignment. Extensive experiments demonstrate the universality, superiority, and inspiring potential of our neural surface parameterization paradigm. The code will be publicly available at https://github.com/AidenZhao/FlexPara","authors":["Yuming Zhao","Qijian Zhang","Junhui Hou","Jiazhi Xia","Wenping Wang","Ying He"],"url":"https://arxiv.org/abs/2504.19210"}
{"created":"2025-04-29","title":"CapsFake: A Multimodal Capsule Network for Detecting Instruction-Guided Deepfakes","abstract":"The rapid evolution of deepfake technology, particularly in instruction-guided image editing, threatens the integrity of digital images by enabling subtle, context-aware manipulations. Generated conditionally from real images and textual prompts, these edits are often imperceptible to both humans and existing detection systems, revealing significant limitations in current defenses. We propose a novel multimodal capsule network, CapsFake, designed to detect such deepfake image edits by integrating low-level capsules from visual, textual, and frequency-domain modalities. High-level capsules, predicted through a competitive routing mechanism, dynamically aggregate local features to identify manipulated regions with precision. Evaluated on diverse datasets, including MagicBrush, Unsplash Edits, Open Images Edits, and Multi-turn Edits, CapsFake outperforms state-of-the-art methods by up to 20% in detection accuracy. Ablation studies validate its robustness, achieving detection rates above 94% under natural perturbations and 96% against adversarial attacks, with excellent generalization to unseen editing scenarios. This approach establishes a powerful framework for countering sophisticated image manipulations.","authors":["Tuan Nguyen","Naseem Khan","Issa Khalil"],"url":"https://arxiv.org/abs/2504.19212"}
{"created":"2025-04-29","title":"On the Prevalence and Usage of Commit Signing on GitHub: A Longitudinal and Cross-Domain Study","abstract":"GitHub is one of the most widely used public code development platform. However, the code hosted publicly on the platform is vulnerable to commit spoofing that allows an adversary to introduce malicious code or commits into the repository by spoofing the commit metadata to indicate that the code was added by a legitimate user. The only defense that GitHub employs is the process of commit signing, which indicates whether a commit is from a valid source or not based on the keys registered by the users.","authors":["Anupam Sharma","Sreyashi Karmakar","Gayatri Priyadarsini Kancherla","Abhishek Bichhawat"],"url":"https://arxiv.org/abs/2504.19215"}
{"created":"2025-04-29","title":"AlphaFuse: Learn ID Embeddings for Sequential Recommendation in Null Space of Language Embeddings","abstract":"Recent advancements in sequential recommendation have underscored the potential of Large Language Models (LLMs) for enhancing item embeddings. However, existing approaches face three key limitations: 1) the degradation of the semantic space when high-dimensional language embeddings are mapped to lower-dimensional ID embeddings, 2) the underutilization of language embeddings, and 3) the reliance on additional trainable parameters, such as an adapter, to bridge the gap between the semantic and behavior spaces.In this paper, we introduce AlphaFuse, a simple but effective language-guided learning strategy that addresses these challenges by learning ID embeddings within the null space of language embeddings. Specifically, we decompose the semantic space of language embeddings via Singular Value Decomposition (SVD), distinguishing it into a semantic-rich row space and a semantic-sparse null space. Collaborative signals are then injected into the null space, while preserving the rich semantics of the row space. AlphaFuse prevents degradation of the semantic space, integrates the retained language embeddings into the final item embeddings, and eliminates the need for auxiliary trainable modules, enabling seamless adaptation to any sequential recommendation framework. We validate the effectiveness and flexibility of AlphaFuse through extensive experiments on three benchmark datasets, including cold-start user and long-tail settings, showcasing significant improvements in both discriminative and diffusion-based generative sequential recommenders. Our codes and datasets are available at https://github.com/Hugo-Chinn/AlphaFuse.","authors":["Guoqing Hu","An Zhang","Shuo Liu","Zhibo Cai","Xun Yang","Xiang Wang"],"url":"https://arxiv.org/abs/2504.19218"}
{"created":"2025-04-29","title":"Evaluating Organization Security: User Stories of European Union NIS2 Directive","abstract":"The NIS2 directive requires EU Member States to ensure a consistently high level of cybersecurity by setting risk-management measures for essential and important entities. Evaluations are necessary to assess whether the required security level is met. This involves understanding the needs and goals of different personas defined by NIS2, who benefit from evaluation results. In this paper, we consider how NIS2 user stories support the evaluation of the level of information security in organizations. Using requirements elicitation principles, we extracted the legal requirements from NIS2 from our narrowed scope, identified six key personas and their goals, formulated user stories based on the gathered information, and validated the usability and relevance of the user stories with security evaluation instruments or methods we found from the literature. The defined user stories help to adjust existing instruments and methods of assessing the security level to comply with NIS2. On the other hand, user stories enable us to see the patterns related to security evaluation when developing new NIS2-compliant security evaluation methods to optimize the administrative burden of entities.","authors":["Mari Seeba","Magnus Valgre","Raimundas Matulevi\\v{c}ius"],"url":"https://arxiv.org/abs/2504.19222"}
{"created":"2025-04-29","title":"CARL: Camera-Agnostic Representation Learning for Spectral Image Analysis","abstract":"Spectral imaging offers promising applications across diverse domains, including medicine and urban scene understanding, and is already established as a critical modality in remote sensing. However, variability in channel dimensionality and captured wavelengths among spectral cameras impede the development of AI-driven methodologies, leading to camera-specific models with limited generalizability and inadequate cross-camera applicability. To address this bottleneck, we introduce $\\textbf{CARL}$, a model for $\\textbf{C}$amera-$\\textbf{A}$gnostic $\\textbf{R}$epresentation $\\textbf{L}$earning across RGB, multispectral, and hyperspectral imaging modalities. To enable the conversion of a spectral image with any channel dimensionality to a camera-agnostic embedding, we introduce wavelength positional encoding and a self-attention-cross-attention mechanism to compress spectral information into learned query representations. Spectral-spatial pre-training is achieved with a novel spectral self-supervised JEPA-inspired strategy tailored to CARL. Large-scale experiments across the domains of medical imaging, autonomous driving, and satellite imaging demonstrate our model's unique robustness to spectral heterogeneity, outperforming on datasets with simulated and real-world cross-camera spectral variations. The scalability and versatility of the proposed approach position our model as a backbone for future spectral foundation models.","authors":["Alexander Baumann","Leonardo Ayala","Silvia Seidlitz","Jan Sellner","Alexander Studier-Fischer","Berkin \\\"Ozdemir","Lena Maier-Hein","Slobodan Ilic"],"url":"https://arxiv.org/abs/2504.19223"}
{"created":"2025-04-29","title":"Representing and querying data tensors in RDF and SPARQL","abstract":"Embedding tensors in databases has recently gained in significance, due to the rapid proliferation of machine learning methods (including LLMs) which produce embeddings in the form of tensors. To support emerging use cases hybridizing machine learning with knowledge graphs, a robust and efficient tensor representation scheme is needed. We introduce a novel approach for representing data tensors as literals in RDF, along with an extension of SPARQL implementing specialized functionalities for handling such literals. The extension includes 36 SPARQL functions and four aggregates. To support this approach, we provide a thoroughly tested, open-source implementation based on Apache Jena, along with an exemplary knowledge graph and query set.","authors":["Piotr Marciniak","Piotr Sowinski","Maria Ganzha"],"url":"https://arxiv.org/abs/2504.19224"}
{"created":"2025-04-29","title":"Providing Information About Implemented Algorithms Improves Program Comprehension: A Controlled Experiment","abstract":"Context: Various approaches aim to support program comprehension by automatically detecting algorithms in source code. However, no empirical evaluations of their helpfulness have been performed. Objective: To empirically evaluate how algorithm labels - which include the algorithm's name and additional information - impact program comprehension in terms of correctness and time. Method: We conducted a controlled experiment with 56 participants, where the experimental group received code with labeled algorithms. The groups completed exercises designed to measure program comprehension as well as a post-questionnaire on label helpfulness, use cases for algorithm recognition, and reasons for self-implementation of algorithms in practice. Results: Annotating source code with algorithm labels significantly improves program comprehension (p=0.040), with a median improvement of 6 points (~23%), but does not affect completion times (p=0.991). Qualitative analysis revealed that a majority of participants perceived the labels as helpful, especially for recognizing the codes intent. Participants also proposed use cases such as error detection, optimization, and library replacement. Reasons for self-implementing algorithms included library inadequacies, performance needs and avoiding dependencies or licensing costs. Conclusion: This study shows that algorithm labels improve program comprehension, especially for developers with medium programming experience. Our qualitative analysis also sheds light on how participants benefit from the labels, further use cases for algorithm recognition and motivations behind self-implementing algorithms.","authors":["Denis Neum\\\"uller","Alexander Raschke","Matthias Tichy"],"url":"https://arxiv.org/abs/2504.19225"}
{"created":"2025-04-29","title":"Unsupervised 2D-3D lifting of non-rigid objects using local constraints","abstract":"For non-rigid objects, predicting the 3D shape from 2D keypoint observations is ill-posed due to occlusions, and the need to disentangle changes in viewpoint and changes in shape. This challenge has often been addressed by embedding low-rank constraints into specialized models. These models can be hard to train, as they depend on finding a canonical way of aligning observations, before they can learn detailed geometry. These constraints have limited the reconstruction quality. We show that generic, high capacity models, trained with an unsupervised loss, allow for more accurate predicted shapes. In particular, applying low-rank constraints to localized subsets of the full shape allows the high capacity to be suitably constrained. We reduce the state-of-the-art reconstruction error on the S-Up3D dataset by over 70%.","authors":["Shalini Maiti","Lourdes Agapito","Benjamin Graham"],"url":"https://arxiv.org/abs/2504.19227"}
{"created":"2025-04-29","title":"Robotic Trail Maker Platform for Rehabilitation in Neurological Conditions: Clinical Use Cases","abstract":"Patients with neurological conditions require rehabilitation to restore their motor, visual, and cognitive abilities. To meet the shortage of therapists and reduce their workload, a robotic rehabilitation platform involving the clinical trail making test is proposed. Therapists can create custom trails for each patient and the patient can trace the trails using a robotic device. The platform can track the performance of the patient and use these data to provide dynamic assistance through the robot to the patient interface. Therefore, the proposed platform not only functions as an evaluation platform, but also trains the patient in recovery. The developed platform has been validated at a rehabilitation center, with therapists and patients operating the device. It was found that patients performed poorly while using the platform compared to healthy subjects and that the assistance provided also improved performance amongst patients. Statistical analysis demonstrated that the speed of the patients was significantly enhanced with the robotic assistance. Further, neural networks are trained to classify between patients and healthy subjects and to forecast their movements using the data collected.","authors":["Srikar Annamraju","Harris Nisar","Dayu Xia","Shankar A. Deka","Anne Horowitz","Nadica Miljkovi\\'c","Du\\v{s}an M. Stipanovi\\'c"],"url":"https://arxiv.org/abs/2504.19230"}
{"created":"2025-04-29","title":"Adaptra: Straggler-Resilient Hybrid-Parallel Training with Pipeline Adaptation","abstract":"Training large Deep Neural Network (DNN) models at scale often encounters straggler issues, mostly in communications due to network congestion, RNIC/switch defects, or topological asymmetry. Under advanced pipeline parallelism, even minor communication delays can induce significant training slowdowns. This occurs because (1) slow communication disrupts the pipeline schedule, creating cascading \"bubbles\" in a domino effect, and (2) current GPU kernel scheduling is susceptible to head-of-line blocking, where slow communication blocks subsequent computations, further adding to these bubbles. To address these challenges, we present ADAPTRA, a straggler-resilient training system with two key optimizations. First, it optimally adapts the pipeline schedule in the presence of stragglers to absorb communication delays without inducing cascading bubbles, using a simple yet effective algorithm guided by an analytical model. Second, upon detecting slow communication, ADAPTRA offloads communication operations from GPU to host memory and utilizes CPU-side RDMA for data transfer. This eliminates head-of-line blocking as subsequent computation kernels can be scheduled immediately on GPUs. Together, these optimizations effectively reduce pipeline stalls in the presence of communication stragglers, improving the training iteration time by 1.2-3.5x in our experiments under various settings.","authors":["Tianyuan Wu","Lunxi Cao","Hanfeng Lu","Xiaoxiao Jiang","Yinghao Yu","Siran Yang","Guodong Yang","Jiamang Wang","Lin Qu","Liping Zhang","Wei Wang"],"url":"https://arxiv.org/abs/2504.19232"}
{"created":"2025-04-29","title":"Deep Reinforcement Learning for Automated Web GUI Testing","abstract":"Automated GUI testing of web applications has always been considered a challenging task considering their large state space and complex interaction logic. Deep Reinforcement Learning (DRL) is a recent extension of Reinforcement Learning (RL), which takes advantage of the powerful learning capabilities of neural networks, making it suitable for complex exploration space. In this paper, leveraging the capability of deep reinforcement learning, we propose WebRLED, an effective approach for automated GUI testing of complex web applications. WebRLED has the following characteristics: (1) a grid-based action value learning technique, which can improve the efficiency of state space exploration; (2) a novel action discriminator which can be trained during the exploration to identify more actions; (3) an adaptive, curiosity-driven reward model, which considers the novelty of an explored state within an episode and global history, and can guide exploration continuously. We conduct a comprehensive evaluation of WebRLED on 12 open-source web applications and a field study of the top 50 most popular web applications in the world. The experimental results show that WebRLED achieves higher code/state coverage and failure detection rate compared to existing state-of-the-art (SOTA) techniques. Furthermore, WebRLED finds 695 unique failures in 50 real-world applications.","authors":["Zhiyu Gu","Chenxu Liu","Guoquan Wu","Yifei Zhang","ChenXi Yang","Zheheng Liang","Wei Chen","Jun Wei"],"url":"https://arxiv.org/abs/2504.19237"}
{"created":"2025-04-29","title":"Algorithmic Detection of Jacobi Stability for Systems of Second Order Differential Equations","abstract":"This paper introduces an algorithmic approach to the analysis of Jacobi stability of systems of second order ordinary differential equations (ODEs) via the Kosambi--Cartan--Chern (KCC) theory. We develop an efficient symbolic program using Maple for computing the second KCC invariant for systems of second order ODEs in arbitrary dimension. The program allows us to systematically analyze Jacobi stability of a system of second order ODEs by means of real solving and solution classification using symbolic computation. The effectiveness of the proposed approach is illustrated by a model of wound strings, a two-dimensional airfoil model with cubic nonlinearity in supersonic flow and a 3-DOF tractor seat-operator model. The computational results on Jacobi stability of these models are further verified by numerical simulations. Moreover, our algorithmic approach allows us to detect hand-guided computation errors in published papers.","authors":["Christian G. B\\\"ohmer","Bo Huang","Dongming Wang","Xinyu Wang"],"url":"https://arxiv.org/abs/2504.19243"}
{"created":"2025-04-29","title":"Semantic-Aligned Learning with Collaborative Refinement for Unsupervised VI-ReID","abstract":"Unsupervised visible-infrared person re-identification (USL-VI-ReID) seeks to match pedestrian images of the same individual across different modalities without human annotations for model learning. Previous methods unify pseudo-labels of cross-modality images through label association algorithms and then design contrastive learning framework for global feature learning. However, these methods overlook the cross-modality variations in feature representation and pseudo-label distributions brought by fine-grained patterns. This insight results in insufficient modality-shared learning when only global features are optimized. To address this issue, we propose a Semantic-Aligned Learning with Collaborative Refinement (SALCR) framework, which builds up optimization objective for specific fine-grained patterns emphasized by each modality, thereby achieving complementary alignment between the label distributions of different modalities. Specifically, we first introduce a Dual Association with Global Learning (DAGI) module to unify the pseudo-labels of cross-modality instances in a bi-directional manner. Afterward, a Fine-Grained Semantic-Aligned Learning (FGSAL) module is carried out to explore part-level semantic-aligned patterns emphasized by each modality from cross-modality instances. Optimization objective is then formulated based on the semantic-aligned features and their corresponding label space. To alleviate the side-effects arising from noisy pseudo-labels, we propose a Global-Part Collaborative Refinement (GPCR) module to mine reliable positive sample sets for the global and part features dynamically and optimize the inter-instance relationships. Extensive experiments demonstrate the effectiveness of the proposed method, which achieves superior performances to state-of-the-art methods. Our code is available at \\href{https://github.com/FranklinLingfeng/code-for-SALCR}.","authors":["De Cheng","Lingfeng He","Nannan Wang","Dingwen Zhang","Xinbo Gao"],"url":"https://arxiv.org/abs/2504.19244"}
{"created":"2025-04-29","title":"Efficient COLREGs-Compliant Collision Avoidance using Turning Circle-based Control Barrier Function","abstract":"This paper proposes a computationally efficient collision avoidance algorithm using turning circle-based control barrier functions (CBFs) that comply with international regulations for preventing collisions at sea (COLREGs). Conventional CBFs often lack explicit consideration of turning capabilities and avoidance direction, which are key elements in developing a COLREGs-compliant collision avoidance algorithm. To overcome these limitations, we introduce two CBFs derived from left and right turning circles. These functions establish safety conditions based on the proximity between the traffic ships and the centers of the turning circles, effectively determining both avoidance directions and turning capabilities. The proposed method formulates a quadratic programming problem with the CBFs as constraints, ensuring safe navigation without relying on computationally intensive trajectory optimization. This approach significantly reduces computational effort while maintaining performance comparable to model predictive control-based methods. Simulation results validate the effectiveness of the proposed algorithm in enabling COLREGs-compliant, safe navigation, demonstrating its potential for reliable and efficient operation in complex maritime environments.","authors":["Changyu Lee","Jinwook Park","Jinwhan Kim"],"url":"https://arxiv.org/abs/2504.19247"}
{"created":"2025-04-29","title":"ODExAI: A Comprehensive Object Detection Explainable AI Evaluation","abstract":"Explainable Artificial Intelligence (XAI) techniques for interpreting object detection models remain in an early stage, with no established standards for systematic evaluation. This absence of consensus hinders both the comparative analysis of methods and the informed selection of suitable approaches. To address this gap, we introduce the Object Detection Explainable AI Evaluation (ODExAI), a comprehensive framework designed to assess XAI methods in object detection based on three core dimensions: localization accuracy, faithfulness to model behavior, and computational complexity. We benchmark a set of XAI methods across two widely used object detectors (YOLOX and Faster R-CNN) and standard datasets (MS-COCO and PASCAL VOC). Empirical results demonstrate that region-based methods (e.g., D-CLOSE) achieve strong localization (PG = 88.49%) and high model faithfulness (OA = 0.863), though with substantial computational overhead (Time = 71.42s). On the other hand, CAM-based methods (e.g., G-CAME) achieve superior localization (PG = 96.13%) and significantly lower runtime (Time = 0.54s), but at the expense of reduced faithfulness (OA = 0.549). These findings demonstrate critical trade-offs among existing XAI approaches and reinforce the need for task-specific evaluation when deploying them in object detection pipelines. Our implementation and evaluation benchmarks are publicly available at: https://github.com/Analytics-Everywhere-Lab/odexai.","authors":["Loc Phuc Truong Nguyen","Hung Truong Thanh Nguyen","Hung Cao"],"url":"https://arxiv.org/abs/2504.19249"}
{"created":"2025-04-29","title":"Hybridizable Discontinuous Galerkin Methods for Coupled Poro-Viscoelastic and Thermo-Viscoelastic Systems","abstract":"This article presents a unified mathematical framework for modeling coupled poro-viscoelastic and thermo-viscoelastic phenomena, formulated as a system of first-order in time partial differential equations. The model describes the evolution of solid velocity, elastic and viscous stress tensors, and additional fields related to either fluid pressure or temperature, depending on the physical context. We develop a hybridizable discontinuous Galerkin method for the numerical approximation of this coupled system, providing a high-order, stable discretization that efficiently handles the multiphysics nature of the problem. We establish stability analysis and derive optimal $hp$-error estimates for the semi-discrete formulation. The theoretical convergence rates are validated through comprehensive numerical experiments, demonstrating the method's accuracy and robustness across various test cases, including wave propagation in heterogeneous media with mixed viscoelastic properties.","authors":["Salim Meddahi"],"url":"https://arxiv.org/abs/2504.19250"}
{"created":"2025-04-29","title":"Cost-based Selection of Provenance Sketches for Data Skipping","abstract":"Provenance sketches, light-weight indexes that record what data is needed (is relevant) for answering a query, can significantly improve performance of important classes of queries (e.g., HAVING and top-k queries). Given a horizontal partition of a table, a provenance sketch for a query Q records which fragments contain provenance. Once a provenance sketch has been captured for a query, it can be used to speed-up subsequent queries by skipping data that does not belong to a sketch. The size and, thus, also the effectiveness of a provenance sketch is often quite sensitive to the choice of attribute(s) we are partitioning on. In this work, we develop sample-based estimation techniques for the size of provenance sketches akin to a specialized form of approximate query processing. This technique enables the online selection of provenance sketches by estimating the size of sketches for a set of candidate attributes and then creating the sketch that is estimated to yield the largest benefit. We demonstrate experimentally that our estimation is accurate enough to select optimal or near optimal provenance sketches in most cases which in turn leads to a runtime improvement of up to %60 compared to other strategies for selecting provenance sketches.","authors":["Ziyu Liu","Boris Glavic"],"url":"https://arxiv.org/abs/2504.19252"}
{"created":"2025-04-29","title":"Quantitative evaluation of brain-inspired vision sensors in high-speed robotic perception","abstract":"Perception systems in robotics encounter significant challenges in high-speed and dynamic conditions when relying on traditional cameras, where motion blur can compromise spatial feature integrity and task performance. Brain-inspired vision sensors (BVS) have recently gained attention as an alternative, offering high temporal resolution with reduced bandwidth and power requirements. Here, we present the first quantitative evaluation framework for two representative classes of BVSs in variable-speed robotic sensing, including event-based vision sensors (EVS) that detect asynchronous temporal contrasts, and the primitive-based sensor Tianmouc that employs a complementary mechanism to encode both spatiotemporal changes and intensity. A unified testing protocol is established, including crosssensor calibrations, standardized testing platforms, and quality metrics to address differences in data modality. From an imaging standpoint, we evaluate the effects of sensor non-idealities, such as motion-induced distortion, on the capture of structural information. For functional benchmarking, we examine task performance in corner detection and motion estimation under different rotational speeds. Results indicate that EVS performs well in highspeed, sparse scenarios and in modestly fast, complex scenes, but exhibits performance limitations in high-speed, cluttered settings due to pixel-level bandwidth variations and event rate saturation. In comparison, Tianmouc demonstrates consistent performance across sparse and complex scenarios at various speeds, supported by its global, precise, high-speed spatiotemporal gradient samplings. These findings offer valuable insights into the applicationdependent suitability of BVS technologies and support further advancement in this area.","authors":["Taoyi Wang","Lijian Wang","Yihan Lin","Mingtao Ou","Yuguo Chen","Xinglong Ji","Rong Zhao"],"url":"https://arxiv.org/abs/2504.19253"}
{"created":"2025-04-29","title":"Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers","abstract":"Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we propose a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.","authors":["Dylan Bouchard","Mohit Singh Chauhan"],"url":"https://arxiv.org/abs/2504.19254"}
{"created":"2025-04-29","title":"The Convergent Ethics of AI? Analyzing Moral Foundation Priorities in Large Language Models with a Multi-Framework Approach","abstract":"As large language models (LLMs) are increasingly deployed in consequential decision-making contexts, systematically assessing their ethical reasoning capabilities becomes a critical imperative. This paper introduces the Priorities in Reasoning and Intrinsic Moral Evaluation (PRIME) framework--a comprehensive methodology for analyzing moral priorities across foundational ethical dimensions including consequentialist-deontological reasoning, moral foundations theory, and Kohlberg's developmental stages. We apply this framework to six leading LLMs through a dual-protocol approach combining direct questioning and response analysis to established ethical dilemmas. Our analysis reveals striking patterns of convergence: all evaluated models demonstrate strong prioritization of care/harm and fairness/cheating foundations while consistently underweighting authority, loyalty, and sanctity dimensions. Through detailed examination of confidence metrics, response reluctance patterns, and reasoning consistency, we establish that contemporary LLMs (1) produce decisive ethical judgments, (2) demonstrate notable cross-model alignment in moral decision-making, and (3) generally correspond with empirically established human moral preferences. This research contributes a scalable, extensible methodology for ethical benchmarking while highlighting both the promising capabilities and systematic limitations in current AI moral reasoning architectures--insights critical for responsible development as these systems assume increasingly significant societal roles.","authors":["Chad Coleman","W. Russell Neuman","Ali Dasdan","Safinah Ali","Manan Shah"],"url":"https://arxiv.org/abs/2504.19255"}
{"created":"2025-04-29","title":"LM-MCVT: A Lightweight Multi-modal Multi-view Convolutional-Vision Transformer Approach for 3D Object Recognition","abstract":"In human-centered environments such as restaurants, homes, and warehouses, robots often face challenges in accurately recognizing 3D objects. These challenges stem from the complexity and variability of these environments, including diverse object shapes. In this paper, we propose a novel Lightweight Multi-modal Multi-view Convolutional-Vision Transformer network (LM-MCVT) to enhance 3D object recognition in robotic applications. Our approach leverages the Globally Entropy-based Embeddings Fusion (GEEF) method to integrate multi-views efficiently. The LM-MCVT architecture incorporates pre- and mid-level convolutional encoders and local and global transformers to enhance feature extraction and recognition accuracy. We evaluate our method on the synthetic ModelNet40 dataset and achieve a recognition accuracy of 95.6% using a four-view setup, surpassing existing state-of-the-art methods. To further validate its effectiveness, we conduct 5-fold cross-validation on the real-world OmniObject3D dataset using the same configuration. Results consistently show superior performance, demonstrating the method's robustness in 3D object recognition across synthetic and real-world 3D data.","authors":["Songsong Xiong","Hamidreza Kasaei"],"url":"https://arxiv.org/abs/2504.19256"}
{"created":"2025-04-29","title":"OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion","abstract":"LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel network for LiDAR place recognition that leverages OpenStreetMap as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components: a cross-modal visibility mask that identifies maximal observable regions from both modalities to guide feature learning, and an adaptive radial fusion module that dynamically consolidates multiscale radial features into discriminative global descriptors. Extensive experiments on the augmented KITTI and KITTI-360 datasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold for top-1 retrieved matches while operating at 12x faster inference speeds compared to state-of-the-art approaches. Code and datasets are publicly available at: https://github.com/WHU-USI3DV/OPAL .","authors":["Shuhao Kang","Martin Y. Liao","Yan Xia","Olaf Wysocki","Boris Jutzi","Daniel Cremers"],"url":"https://arxiv.org/abs/2504.19258"}
{"created":"2025-04-29","title":"Convergence Properties of Natural Gradient Descent for Minimizing KL Divergence","abstract":"The Kullback-Leibler (KL) divergence plays a central role in probabilistic machine learning, where it commonly serves as the canonical loss function. Optimization in such settings is often performed over the probability simplex, where the choice of parameterization significantly impacts convergence. In this work, we study the problem of minimizing the KL divergence and analyze the behavior of gradient-based optimization algorithms under two dual coordinate systems within the framework of information geometry$-$ the exponential family ($\\theta$ coordinates) and the mixture family ($\\eta$ coordinates). We compare Euclidean gradient descent (GD) in these coordinates with the coordinate-invariant natural gradient descent (NGD), where the natural gradient is a Riemannian gradient that incorporates the intrinsic geometry of the parameter space. In continuous time, we prove that the convergence rates of GD in the $\\theta$ and $\\eta$ coordinates provide lower and upper bounds, respectively, on the convergence rate of NGD. Moreover, under affine reparameterizations of the dual coordinates, the convergence rates of GD in $\\eta$ and $\\theta$ coordinates can be scaled to $2c$ and $\\frac{2}{c}$, respectively, for any $c>0$, while NGD maintains a fixed convergence rate of $2$, remaining invariant to such transformations and sandwiched between them. Although this suggests that NGD may not exhibit uniformly superior convergence in continuous time, we demonstrate that its advantages become pronounced in discrete time, where it achieves faster convergence and greater robustness to noise, outperforming GD. Our analysis hinges on bounding the spectrum and condition number of the Hessian of the KL divergence at the optimum, which coincides with the Fisher information matrix.","authors":["Adwait Datar","Nihat Ay"],"url":"https://arxiv.org/abs/2504.19259"}
{"created":"2025-04-29","title":"Rendering Anywhere You See: Renderability Field-guided Gaussian Splatting","abstract":"Scene view synthesis, which generates novel views from limited perspectives, is increasingly vital for applications like virtual reality, augmented reality, and robotics. Unlike object-based tasks, such as generating 360{\\deg} views of a car, scene view synthesis handles entire environments where non-uniform observations pose unique challenges for stable rendering quality. To address this issue, we propose a novel approach: renderability field-guided gaussian splatting (RF-GS). This method quantifies input inhomogeneity through a renderability field, guiding pseudo-view sampling to enhanced visual consistency. To ensure the quality of wide-baseline pseudo-views, we train an image restoration model to map point projections to visible-light styles. Additionally, our validated hybrid data optimization strategy effectively fuses information of pseudo-view angles and source view textures. Comparative experiments on simulated and real-world data show that our method outperforms existing approaches in rendering stability.","authors":["Xiaofeng Jin","Yan Fang","Matteo Frosi","Jianfei Ge","Jiangjian Xiao","Matteo Matteucci"],"url":"https://arxiv.org/abs/2504.19261"}
{"created":"2025-04-29","title":"Navigating AI Policy Landscapes: Insights into Human Rights Considerations Across IEEE Regions","abstract":"This paper explores the integration of human rights considerations into AI regulatory frameworks across different IEEE regions - specifically the United States (Region 1-6), Europe (Region 8), China (part of Region 10), and Singapore (part of Region 10). While all acknowledge the transformative potential of AI and the necessity of ethical guidelines, their regulatory approaches significantly differ. Europe exhibits a rigorous framework with stringent protections for individual rights, while the U.S. promotes innovation with less restrictive regulations. China emphasizes state control and societal order in its AI strategies. In contrast, Singapore's advisory framework encourages self-regulation and aligns closely with international norms. This comparative analysis underlines the need for ongoing global dialogue to harmonize AI regulations that safeguard human rights while promoting technological advancement, reflecting the diverse perspectives and priorities of each region.","authors":["Angel Mary John","Jerrin Thomas Panachakel","Anusha S. P"],"url":"https://arxiv.org/abs/2504.19264"}
{"created":"2025-04-29","title":"OpenFusion++: An Open-vocabulary Real-time Scene Understanding System","abstract":"Real-time open-vocabulary scene understanding is essential for efficient 3D perception in applications such as vision-language navigation, embodied intelligence, and augmented reality. However, existing methods suffer from imprecise instance segmentation, static semantic updates, and limited handling of complex queries. To address these issues, we present OpenFusion++, a TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach refines 3D point clouds by fusing confidence maps from foundational models, dynamically updates global semantic labels via an adaptive cache based on instance area, and employs a dual-path encoding framework that integrates object attributes with environmental context for precise query responses. Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate that OpenFusion++ significantly outperforms the baseline in both semantic accuracy and query responsiveness.","authors":["Xiaofeng Jin","Matteo Frosi","Matteo Matteucci"],"url":"https://arxiv.org/abs/2504.19266"}
{"created":"2025-04-29","title":"VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?","abstract":"Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.","authors":["Mohamed Gado","Towhid Taliee","Muhammad Memon","Dmitry Ignatov","Radu Timofte"],"url":"https://arxiv.org/abs/2504.19267"}
{"created":"2025-04-29","title":"VI3NR: Variance Informed Initialization for Implicit Neural Representations","abstract":"Implicit Neural Representations (INRs) are a versatile and powerful tool for encoding various forms of data, including images, videos, sound, and 3D shapes. A critical factor in the success of INRs is the initialization of the network, which can significantly impact the convergence and accuracy of the learned model. Unfortunately, commonly used neural network initializations are not widely applicable for many activation functions, especially those used by INRs. In this paper, we improve upon previous initialization methods by deriving an initialization that has stable variance across layers, and applies to any activation function. We show that this generalizes many previous initialization methods, and has even better stability for well studied activations. We also show that our initialization leads to improved results with INR activation functions in multiple signal modalities. Our approach is particularly effective for Gaussian INRs, where we demonstrate that the theory of our initialization matches with task performance in multiple experiments, allowing us to achieve improvements in image, audio, and 3D surface reconstruction.","authors":["Chamin Hewa Koneputugodage","Yizhak Ben-Shabat","Sameera Ramasinghe","Stephen Gould"],"url":"https://arxiv.org/abs/2504.19270"}
{"created":"2025-04-29","title":"Leveraging Multi-Modal Saliency and Fusion for Gaze Target Detection","abstract":"Gaze target detection (GTD) is the task of predicting where a person in an image is looking. This is a challenging task, as it requires the ability to understand the relationship between the person's head, body, and eyes, as well as the surrounding environment. In this paper, we propose a novel method for GTD that fuses multiple pieces of information extracted from an image. First, we project the 2D image into a 3D representation using monocular depth estimation. We then extract a depth-infused saliency module map, which highlights the most salient (\\textit{attention-grabbing}) regions in image for the subject in consideration. We also extract face and depth modalities from the image, and finally fuse all the extracted modalities to identify the gaze target. We quantitatively evaluated our method, including the ablation analysis on three publicly available datasets, namely VideoAttentionTarget, GazeFollow and GOO-Real, and showed that it outperforms other state-of-the-art methods. This suggests that our method is a promising new approach for GTD.","authors":["Athul M. Mathew","Arshad Ali Khan","Thariq Khalid","Faroq AL-Tam","Riad Souissi"],"url":"https://arxiv.org/abs/2504.19271"}
{"created":"2025-04-29","title":"TeleSparse: Practical Privacy-Preserving Verification of Deep Neural Networks","abstract":"Verification of the integrity of deep learning inference is crucial for understanding whether a model is being applied correctly. However, such verification typically requires access to model weights and (potentially sensitive or private) training data. So-called Zero-knowledge Succinct Non-Interactive Arguments of Knowledge (ZK-SNARKs) would appear to provide the capability to verify model inference without access to such sensitive data. However, applying ZK-SNARKs to modern neural networks, such as transformers and large vision models, introduces significant computational overhead.","authors":["Mohammad M Maheri","Hamed Haddadi","Alex Davidson"],"url":"https://arxiv.org/abs/2504.19274"}
{"created":"2025-04-29","title":"Balancing Creativity and Automation: The Influence of AI on Modern Film Production and Dissemination","abstract":"The integration of Artificial Intelligence(AI) into film production has revolutionized efficiency and creativity, yet it simultaneously raises critical ethical and practical challenges. This study explores the dual impact of AI on modern cinema through three objectives: defining the optimal human-AI relationship, balancing creativity with automation, and developing ethical guidelines. By employing a mixed-method approach combining theoretical frameworks (auteur theory, human-technology relations) and case studies (The Safe Zone, Fast & Furious 7, The Brutalist), the research reveals that positioning AI as an \"embodiment tool\" rather than an independent \"alterity partner\" preserves human authorship and artistic integrity. Key findings highlight the risks of surveillance capitalism in AI-driven markets and the ethical dilemmas of deepfake technology. The study concludes with actionable recommendations, including international regulatory frameworks and a Human Control Index (HCI) to quantify AI involvement. These insights aim to guide filmmakers, policymakers, and scholars in navigating the evolving AI-cinema landscape while safeguarding cultural diversity and ethical standards.","authors":["Yiren Xu"],"url":"https://arxiv.org/abs/2504.19275"}
{"created":"2025-04-29","title":"Anyprefer: An Agentic Framework for Preference Data Synthesis","abstract":"High-quality preference data is essential for aligning foundation models with human values through preference learning. However, manual annotation of such data is often time-consuming and costly. Recent methods often adopt a self-rewarding approach, where the target model generates and annotates its own preference data, but this can lead to inaccuracies since the reward model shares weights with the target model, thereby amplifying inherent biases. To address these issues, we propose Anyprefer, a framework designed to synthesize high-quality preference data for aligning the target model. Anyprefer frames the data synthesis process as a cooperative two-player Markov Game, where the target model and the judge model collaborate together. Here, a series of external tools are introduced to assist the judge model in accurately rewarding the target model's responses, mitigating biases in the rewarding process. In addition, a feedback mechanism is introduced to optimize prompts for both models, enhancing collaboration and improving data quality. The synthesized data is compiled into a new preference dataset, Anyprefer-V1, consisting of 58K high-quality preference pairs. Extensive experiments show that Anyprefer significantly improves model alignment performance across four main applications, covering 21 datasets, achieving average improvements of 18.55% in five natural language generation datasets, 3.66% in nine vision-language understanding datasets, 30.05% in three medical image analysis datasets, and 16.00% in four visuo-motor control tasks.","authors":["Yiyang Zhou","Zhaoyang Wang","Tianle Wang","Shangyu Xing","Peng Xia","Bo Li","Kaiyuan Zheng","Zijian Zhang","Zhaorun Chen","Wenhao Zheng","Xuchao Zhang","Chetan Bansal","Weitong Zhang","Ying Wei","Mohit Bansal","Huaxiu Yao"],"url":"https://arxiv.org/abs/2504.19276"}
{"created":"2025-04-29","title":"Small Models, Big Tasks: An Exploratory Empirical Study on Small Language Models for Function Calling","abstract":"Function calling is a complex task with widespread applications in domains such as information retrieval, software engineering and automation. For example, a query to book the shortest flight from New York to London on January 15 requires identifying the correct parameters to generate accurate function calls. Large Language Models (LLMs) can automate this process but are computationally expensive and impractical in resource-constrained settings. In contrast, Small Language Models (SLMs) can operate efficiently, offering faster response times, and lower computational demands, making them potential candidates for function calling on edge devices. In this exploratory empirical study, we evaluate the efficacy of SLMs in generating function calls across diverse domains using zero-shot, few-shot, and fine-tuning approaches, both with and without prompt injection, while also providing the finetuned models to facilitate future applications. Furthermore, we analyze the model responses across a range of metrics, capturing various aspects of function call generation. Additionally, we perform experiments on an edge device to evaluate their performance in terms of latency and memory usage, providing useful insights into their practical applicability. Our findings show that while SLMs improve from zero-shot to few-shot and perform best with fine-tuning, they struggle significantly with adhering to the given output format. Prompt injection experiments further indicate that the models are generally robust and exhibit only a slight decline in performance. While SLMs demonstrate potential for the function call generation task, our results also highlight areas that need further refinement for real-time functioning.","authors":["Ishan Kavathekar","Raghav Donakanti","Ponnurangam Kumaraguru","Karthik Vaidhyanathan"],"url":"https://arxiv.org/abs/2504.19277"}
{"created":"2025-04-29","title":"Optimal Hyperspectral Undersampling Strategy for Satellite Imaging","abstract":"Hyperspectral image (HSI) classification presents significant challenges due to the high dimensionality, spectral redundancy, and limited labeled data typically available in real-world applications. To address these issues and optimize classification performance, we propose a novel band selection strategy known as Iterative Wavelet-based Gradient Sampling (IWGS). This method incrementally selects the most informative spectral bands by analyzing gradients within the wavelet-transformed domain, enabling efficient and targeted dimensionality reduction. Unlike traditional selection methods, IWGS leverages the multi-resolution properties of wavelets to better capture subtle spectral variations relevant for classification. The iterative nature of the approach ensures that redundant or noisy bands are systematically excluded while maximizing the retention of discriminative features. We conduct comprehensive experiments on two widely-used benchmark HSI datasets: Houston 2013 and Indian Pines. Results demonstrate that IWGS consistently outperforms state-of-the-art band selection and classification techniques in terms of both accuracy and computational efficiency. These improvements make our method especially suitable for deployment in edge devices or other resource-constrained environments, where memory and processing power are limited. In particular, IWGS achieved an overall accuracy up to 97.8% on Indian Pines for selected classes, confirming its effectiveness and generalizability across different HSI scenarios.","authors":["Vita V. Vlasova","Vladimir G. Kuzmin","Maria S. Varetsa","Natalia A. Ibragimova","Oleg Y. Rogov","Elena V. Lyapuntsova"],"url":"https://arxiv.org/abs/2504.19279"}
{"created":"2025-04-29","title":"Trigonometric Interpolation Based Optimization for Second Order Non-Linear ODE with Mixed Boundary Conditions","abstract":"In this paper, we propose a trigonometric-interpolation approach for solutions of second order nonlinear ODEs with mixed boundary conditions. The method interpolates secondary derivative $y''$ of a target solution $y$ by a trigonometric polynomial. The solution is identified through an optimization process to capture the dynamics of $y,y',y''$ characterized by the underlying differential equation. The gradient function of the optimization can be carried out by Fast Fourier Transformation and high-degree accuracy can be achieved effectively by increasing interpolation grid points. In case that solution of ODE system is not unique, the algorithm has flexibility to approach to a desired solution that meets certain requirements such as being positive. Numerical tests have been conducted under various boundary conditions with expected performance.","authors":["Xiaorong Zou"],"url":"https://arxiv.org/abs/2504.19280"}
{"created":"2025-04-29","title":"Efficient Serverless Cold Start: Reducing Library Loading Overhead by Profile-guided Optimization","abstract":"Serverless computing abstracts away server management, enabling automatic scaling, efficient resource utilization, and cost-effective pricing models. However, despite these advantages, it faces the significant challenge of cold-start latency, adversely impacting end-to-end performance. Our study shows that many serverless functions initialize libraries that are rarely or never used under typical workloads, thus introducing unnecessary overhead. Although existing static analysis techniques can identify unreachable libraries, they fail to address workload-dependent inefficiencies, resulting in limited performance improvements. To overcome these limitations, we present SLIMSTART, a profile-guided optimization tool designed to identify and mitigate inefficient library usage patterns in serverless applications. By leveraging statistical sampling and call-path profiling, SLIMSTART collects runtime library usage data, generates detailed optimization reports, and applies automated code transformations to reduce cold-start overhead. Furthermore, SLIMSTART integrates seamlessly into CI/CD pipelines, enabling adaptive monitoring and continuous optimizations tailored to evolving workloads. Through extensive evaluation across three benchmark suites and four real-world serverless applications, SLIMSTART achieves up to a 2.30X speedup in initialization latency, a 2.26X improvement in end-to-end latency, and a 1.51X reduction in memory usage, demonstrating its effectiveness in addressing cold-start inefficiencies and optimizing resource utilization.","authors":["Syed Salauddin Mohammad Tariq","Ali Al Zein","Soumya Sripad Vaidya","Arati Khanolkar","Zheng Song","Probir Roy"],"url":"https://arxiv.org/abs/2504.19283"}
{"created":"2025-04-29","title":"Ethical Challenges of Using Artificial Intelligence in Judiciary","abstract":"Artificial intelligence (AI) has emerged as a ubiquitous concept in numerous domains, including the legal system. AI has the potential to revolutionize the functioning of the judiciary and the dispensation of justice. Incorporating AI into the legal system offers the prospect of enhancing decision-making for judges, lawyers, and legal professionals, while concurrently providing the public with more streamlined, efficient, and cost-effective services. The integration of AI into the legal landscape offers manifold benefits, encompassing tasks such as document review, legal research, contract analysis, case prediction, and decision-making. By automating laborious and error-prone procedures, AI has the capacity to alleviate the burden associated with these arduous tasks. Consequently, courts around the world have begun embracing AI technology as a means to enhance the administration of justice. However, alongside its potential advantages, the use of AI in the judiciary poses a range of ethical challenges. These ethical quandaries must be duly addressed to ensure the responsible and equitable deployment of AI systems. This article delineates the principal ethical challenges entailed in employing AI within the judiciary and provides recommendations to effectively address these issues.","authors":["Angel Mary John","Aiswarya M. U.","Jerrin Thomas Panachakel"],"url":"https://arxiv.org/abs/2504.19284"}
{"created":"2025-04-29","title":"Sojourner under Sabotage: A Serious Testing and Debugging Game","abstract":"Teaching software testing and debugging is a critical yet challenging task in computer science education, often hindered by low student engagement and the perceived monotony of these activities. Sojourner under Sabotage, a browser-based serious game, reimagines this learning experience by blending education with an immersive and interactive storyline. Players take on the role of a spaceship crew member, using unit testing and debugging techniques to identify and repair sabotaged components across seven progressively challenging levels. A study with 79 students demonstrates that the game is a powerful tool for enhancing motivation, engagement, and skill development. These findings underscore the transformative potential of serious games in making essential software engineering practices accessible and enjoyable.","authors":["Philipp Straubinger","Tim Greller","Gordon Fraser"],"url":"https://arxiv.org/abs/2504.19287"}
{"created":"2025-04-29","title":"Generalized Score Matching: Bridging $f$-Divergence and Statistical Estimation Under Correlated Noise","abstract":"Relative Fisher information, also known as score matching, is a recently introduced learning method for parameter estimation. Fundamental relations between relative entropy and score matching have been established in the literature for scalar and isotropic Gaussian channels. This paper demonstrates that such relations hold for a much larger class of observation models. We introduce the vector channel where the perturbation is non-isotropic Gaussian noise. For such channels, we derive new representations that connect the $f$-divergence between two distributions to the estimation loss induced by mismatch at the decoder. This approach not only unifies but also greatly extends existing results from both the isotropic Gaussian and classical relative entropy frameworks. Building on this generalization, we extend De Bruijn's identity to mismatched non-isotropic Gaussian models and demonstrate that the connections to generative models naturally follow as a consequence application of this new result.","authors":["Yirong Shen","Lu Gan","Cong Ling"],"url":"https://arxiv.org/abs/2504.19288"}
{"created":"2025-04-29","title":"Marine Snow Removal Using Internally Generated Pseudo Ground Truth","abstract":"Underwater videos often suffer from degraded quality due to light absorption, scattering, and various noise sources. Among these, marine snow, which is suspended organic particles appearing as bright spots or noise, significantly impacts machine vision tasks, particularly those involving feature matching. Existing methods for removing marine snow are ineffective due to the lack of paired training data. To address this challenge, this paper proposes a novel enhancement framework that introduces a new approach for generating paired datasets from raw underwater videos. The resulting dataset consists of paired images of generated snowy and snow, free underwater videos, enabling supervised training for video enhancement. We describe the dataset creation process, highlight its key characteristics, and demonstrate its effectiveness in enhancing underwater image restoration in the absence of ground truth.","authors":["Alexandra Malyugina","Guoxi Huang","Eduardo Ruiz","Benjamin Leslie","Nantheera Anantrasirichai"],"url":"https://arxiv.org/abs/2504.19289"}
{"created":"2025-04-29","title":"Teaching Software Testing and Debugging with the Serious Game Sojourner under Sabotage","abstract":"Software testing and debugging are often seen as tedious, making them challenging to teach effectively. We present Sojourner under Sabotage, a browser-based serious game that enhances learning through interactive, narrative-driven challenges. Players act as spaceship crew members, using unit tests and debugging techniques to fix sabotaged components. Sojourner under Sabotage provides hands-on experience with the real-world testing framework JUnit, improving student engagement, test coverage, and debugging skills.","authors":["Philipp Straubinger","Tim Greller","Gordon Fraser"],"url":"https://arxiv.org/abs/2504.19291"}
{"created":"2025-04-29","title":"Gamifying Testing in IntelliJ: A Replicability Study","abstract":"Gamification is an emerging technique to enhance motivation and performance in traditionally unengaging tasks like software testing. Previous studies have indicated that gamified systems have the potential to improve software testing processes by providing testers with achievements and feedback. However, further evidence of these benefits across different environments, programming languages, and participant groups is required. This paper aims to replicate and validate the effects of IntelliGame, a gamification plugin for IntelliJ IDEA to engage developers in writing and executing tests. The objective is to generalize the benefits observed in earlier studies to new contexts, i.e., the TypeScript programming language and a larger participant pool. The replicability study consists of a controlled experiment with 174 participants, divided into two groups: one using IntelliGame and one with no gamification plugin. The study employed a two-group experimental design to compare testing behavior, coverage, mutation scores, and participant feedback between the groups. Data was collected through test metrics and participant surveys, and statistical analysis was performed to determine the statistical significance. Participants using IntelliGame showed higher engagement and productivity in testing practices than the control group, evidenced by the creation of more tests, increased frequency of executions, and enhanced utilization of testing tools. This ultimately led to better code implementations, highlighting the effectiveness of gamification in improving functional outcomes and motivating users in their testing endeavors. The replication study confirms that gamification, through IntelliGame, positively impacts software testing behavior and developer engagement in coding tasks.","authors":["Philipp Straubinger","Tommaso Fulcini","Giacomo Garaccione","Luca Ardito","Gordon Fraser"],"url":"https://arxiv.org/abs/2504.19294"}
{"created":"2025-04-29","title":"FusionNet: Multi-model Linear Fusion Framework for Low-light Image Enhancement","abstract":"The advent of Deep Neural Networks (DNNs) has driven remarkable progress in low-light image enhancement (LLIE), with diverse architectures (e.g., CNNs and Transformers) and color spaces (e.g., sRGB, HSV, HVI) yielding impressive results. Recent efforts have sought to leverage the complementary strengths of these paradigms, offering promising solutions to enhance performance across varying degradation scenarios. However, existing fusion strategies are hindered by challenges such as parameter explosion, optimization instability, and feature misalignment, limiting further improvements. To overcome these issues, we introduce FusionNet, a novel multi-model linear fusion framework that operates in parallel to effectively capture global and local features across diverse color spaces. By incorporating a linear fusion strategy underpinned by Hilbert space theoretical guarantees, FusionNet mitigates network collapse and reduces excessive training costs. Our method achieved 1st place in the CVPR2025 NTIRE Low Light Enhancement Challenge. Extensive experiments conducted on synthetic and real-world benchmark datasets demonstrate that the proposed method significantly outperforms state-of-the-art methods in terms of both quantitative and qualitative results, delivering robust enhancement under diverse low-light conditions.","authors":["Kangbiao Shi","Yixu Feng","Tao Hu","Yu Cao","Peng Wu","Yijin Liang","Yanning Zhang","Qingsen Yan"],"url":"https://arxiv.org/abs/2504.19295"}
{"created":"2025-04-29","title":"AndroidGen: Building an Android Language Agent under Data Scarcity","abstract":"Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.","authors":["Hanyu Lai","Junjie Gao","Xiao Liu","Yifan Xu","Shudan Zhang","Yuxiao Dong","Jie Tang"],"url":"https://arxiv.org/abs/2504.19298"}
{"created":"2025-04-29","title":"Myocardial Region-guided Feature Aggregation Net for Automatic Coronary artery Segmentation and Stenosis Assessment using Coronary Computed Tomography Angiography","abstract":"Coronary artery disease (CAD) remains a leading cause of mortality worldwide, requiring accurate segmentation and stenosis detection using Coronary Computed Tomography angiography (CCTA). Existing methods struggle with challenges such as low contrast, morphological variability and small vessel segmentation. To address these limitations, we propose the Myocardial Region-guided Feature Aggregation Net, a novel U-shaped dual-encoder architecture that integrates anatomical prior knowledge to enhance robustness in coronary artery segmentation. Our framework incorporates three key innovations: (1) a Myocardial Region-guided Module that directs attention to coronary regions via myocardial contour expansion and multi-scale feature fusion, (2) a Residual Feature Extraction Encoding Module that combines parallel spatial channel attention with residual blocks to enhance local-global feature discrimination, and (3) a Multi-scale Feature Fusion Module for adaptive aggregation of hierarchical vascular features. Additionally, Monte Carlo dropout f quantifies prediction uncertainty, supporting clinical interpretability. For stenosis detection, a morphology-based centerline extraction algorithm separates the vascular tree into anatomical branches, enabling cross-sectional area quantification and stenosis grading. The superiority of MGFA-Net was demonstrated by achieving an Dice score of 85.04%, an accuracy of 84.24%, an HD95 of 6.1294 mm, and an improvement of 5.46% in true positive rate for stenosis detection compared to3D U-Net. The integrated segmentation-to-stenosis pipeline provides automated, clinically interpretable CAD assessment, bridging deep learning with anatomical prior knowledge for precision medicine. Our code is publicly available at http://github.com/chenzhao2023/MGFA_CCTA","authors":["Ni Yao","Xiangyu Liu","Danyang Sun","Chuang Han","Yanting Li","Jiaofen Nan","Chengyang Li","Fubao Zhu","Weihua Zhou","Chen Zhao"],"url":"https://arxiv.org/abs/2504.19300"}
{"created":"2025-04-29","title":"(Almost-)Optimal FPT Algorithm and Kernel for $T$-Cycle on Planar Graphs","abstract":"Research of cycles through specific vertices is a central topic in graph theory. In this context, we focus on a well-studied computational problem, \\textsc{$T$-Cycle}: given an undirected $n$-vertex graph $G$ and a set of $k$ vertices $T\\subseteq V(G)$ termed \\textit{terminals}, the objective is to determine whether $G$ contains a simple cycle $C$ through all the terminals. Our contribution is twofold: (i) We provide a $2^{O(\\sqrt{k}\\log k)}\\cdot n$-time fixed-parameter deterministic algorithm for \\textsc{$T$-Cycle} on planar graphs; (ii) We provide a $k^{O(1)}\\cdot n$-time deterministic kernelization algorithm for \\textsc{$T$-Cycle} on planar graphs where the produced instance is of size $k\\log^{O(1)}k$.","authors":["Harmender Gahlawat","Abhishek Rathod","Meirav Zehavi"],"url":"https://arxiv.org/abs/2504.19301"}
{"created":"2025-04-29","title":"Efficient approximations of matrix multiplication using truncated decompositions","abstract":"We exploit the truncated singular value decomposition and the recently proposed circulant decomposition for an efficient first-order approximation of the multiplication of large dense matrices. A decomposition of each matrix into a sum of a sparse matrix with relatively few dominant entries and a dense residue can also use the above approach, and we present methods for multiplication using a Fourier decomposition and a cycle decomposition-based sparsifications. The proposed methods scale as $\\mathcal{O}(n^2 \\log n)$ in arithmetic operations for $n \\times n$ matrices for usable tolerances in relative error $\\sim$ 1\\%. Note that different decompositions for the two matrices $A$ and $B$ in the product $AB$ are also possible in this approach, using a priori evaluations for suitability, to improve further on the error tolerances demonstrated here.","authors":["Suvendu Kar","Hariprasad M.","Sai Gowri J. N.","Murugesan Venkatapathi"],"url":"https://arxiv.org/abs/2504.19308"}
{"created":"2025-04-29","title":"BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese","abstract":"As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.","authors":["Peilin Zhou","Bruce Leon","Xiang Ying","Can Zhang","Yifan Shao","Qichen Ye","Dading Chong","Zhiling Jin","Chenxuan Xie","Meng Cao","Yuxin Gu","Sixin Hong","Jing Ren","Jian Chen","Chao Liu","Yining Hua"],"url":"https://arxiv.org/abs/2504.19314"}
{"created":"2025-04-29","title":"Unscented Particle Filter for Visual-inertial Navigation using IMU and Landmark Measurements","abstract":"This paper introduces a geometric Quaternion-based Unscented Particle Filter for Visual-Inertial Navigation (QUPF-VIN) specifically designed for a vehicle operating with six degrees of freedom (6 DoF). The proposed QUPF-VIN technique is quaternion-based capturing the inherently nonlinear nature of true navigation kinematics. The filter fuses data from a low-cost inertial measurement unit (IMU) and landmark observations obtained via a vision sensor. The QUPF-VIN is implemented in discrete form to ensure seamless integration with onboard inertial sensing systems. Designed for robustness in GPS-denied environments, the proposed method has been validated through experiments with real-world dataset involving an unmanned aerial vehicle (UAV) equipped with a 6-axis IMU and a stereo camera, operating with 6 DoF. The numerical results demonstrate that the QUPF-VIN provides superior tracking accuracy compared to ground truth data. Additionally, a comparative analysis with a standard Kalman filter-based navigation technique further highlights the enhanced performance of the QUPF-VIN.","authors":["Khashayar Ghanizadegan","Hashim A. Hashim"],"url":"https://arxiv.org/abs/2504.19318"}
{"created":"2025-04-29","title":"Logic-Based Artificial Intelligence Algorithms Supporting Categorical Semantics","abstract":"This paper seeks to apply categorical logic to the design of artificial intelligent agents that reason symbolically about objects more richly structured than sets. Using Johnstone's sequent calculus of terms- and formulae-in-context, we develop forward chaining and normal form algorithms for reasoning about objects in cartesian categories with the rules for Horn logic. We also adapt first-order unification to support multi-sorted theories, contexts, and fragments of first-order logic. The significance of these reformulations rests in the fact that they can be applied to reasoning about objects in semantic categories that do not support classical logic or even all its connectives.","authors":["Ralph Wojtowicz"],"url":"https://arxiv.org/abs/2504.19320"}
{"created":"2025-04-29","title":"Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation","abstract":"Ensuring safe navigation in complex environments requires accurate real-time traversability assessment and understanding of environmental interactions relative to the robot`s capabilities. Traditional methods, which assume simplified dynamics, often require designing and tuning cost functions to safely guide paths or actions toward the goal. This process is tedious, environment-dependent, and not generalizable.To overcome these issues, we propose a novel learned perceptive Forward Dynamics Model (FDM) that predicts the robot`s future state conditioned on the surrounding geometry and history of proprioceptive measurements, proposing a more scalable, safer, and heuristic-free solution. The FDM is trained on multiple years of simulated navigation experience, including high-risk maneuvers, and real-world interactions to incorporate the full system dynamics beyond rigid body simulation. We integrate our perceptive FDM into a zero-shot Model Predictive Path Integral (MPPI) planning framework, leveraging the learned mapping between actions, future states, and failure probability. This allows for optimizing a simplified cost function, eliminating the need for extensive cost-tuning to ensure safety. On the legged robot ANYmal, the proposed perceptive FDM improves the position estimation by on average 41% over competitive baselines, which translates into a 27% higher navigation success rate in rough simulation environments. Moreover, we demonstrate effective sim-to-real transfer and showcase the benefit of training on synthetic and real data. Code and models are made publicly available under https://github.com/leggedrobotics/fdm.","authors":["Pascal Roth","Jonas Frey","Cesar Cadena","Marco Hutter"],"url":"https://arxiv.org/abs/2504.19322"}
{"created":"2025-04-29","title":"NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI","abstract":"Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural networks with symbolic reasoning to enhance the transparency, reasoning capabilities, and data efficiency of AI systems. Recent NSAI systems have gained traction due to their exceptional performance in reasoning tasks and human-AI collaborative scenarios. Despite these algorithmic advancements, executing NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains challenging, due to their heterogeneous computing kernels, high memory intensity, and unique memory access patterns. Moreover, current NSAI algorithms exhibit significant variation in operation types and scales, making them incompatible with existing ML accelerators. These challenges highlight the need for a versatile and flexible acceleration framework tailored to NSAI workloads. In this paper, we propose NSFlow, an FPGA-based acceleration framework designed to achieve high efficiency, scalability, and versatility across NSAI systems. NSFlow features a design architecture generator that identifies workload data dependencies and creates optimized dataflow architectures, as well as a reconfigurable array with flexible compute units, re-organizable memory, and mixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves 31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like systolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates enhanced scalability, with only 4x runtime increase when symbolic workloads scale by 150x. To the best of our knowledge, NSFlow is the first framework to enable real-time generalizable NSAI algorithms acceleration, demonstrating a promising solution for next-generation cognitive systems.","authors":["Hanchen Yang","Zishen Wan","Ritik Raj","Joongun Park","Ziwei Li","Ananda Samajdar","Arijit Raychowdhury","Tushar Krishna"],"url":"https://arxiv.org/abs/2504.19323"}
{"created":"2025-04-29","title":"Platonic Grounding for Efficient Multimodal Language Models","abstract":"The hyperscaling of data and parameter count in Transformer-based models is yielding diminishing performance improvement, especially when weighed against training costs. Such plateauing indicates the importance of methods for more efficient finetuning and inference, while retaining similar performance. This is especially relevant for multimodal learning paradigms, where inference costs of processing multimodal tokens can determine the model's practical viability. At the same time, research on representations and mechanistic interpretability has improved our understanding of the inner workings of Transformer-based models; one such line of work reveals an implicit alignment in the deeper layers of pretrained models, across modalities. Taking inspiration from this, we motivate and propose a simple modification to existing multimodal frameworks that rely on aligning pretrained models. We demonstrate that our approach maintains and, in some cases, even improves performance of baseline methods while achieving significant gains in both training and inference-time compute. Our work also has implications for combining pretrained models into larger systems efficiently.","authors":["Moulik Choraria","Xinbo Wu","Akhil Bhimaraju","Nitesh Sekhar","Yue Wu","Xu Zhang","Prateek Singhal","Lav R. Varshney"],"url":"https://arxiv.org/abs/2504.19327"}
{"created":"2025-04-29","title":"Scalable Substructure Discovery Algorithm For Homogeneous Multilayer Networks","abstract":"Graph mining analyzes real-world graphs to find core substructures (connected subgraphs) in applications modeled as graphs. Substructure discovery is a process that involves identifying meaningful patterns, structures, or components within a large data set. These substructures can be of various types, such as frequent patterns, motifs, or other relevant features within the data.","authors":["Arshdeep Singh","Abhishek Santra","Sharma Chakravarthy"],"url":"https://arxiv.org/abs/2504.19328"}
{"created":"2025-04-29","title":"Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing","abstract":"The trend towards large language models (LLMs) for guardrailing against undesired behaviors is increasing and has shown promise for censoring user inputs. However, increased latency, memory consumption, hosting expenses and non-structured outputs can make their use prohibitive.","authors":["James O' Neill","Santhosh Subramanian","Eric Lin","Vaikkunth Mugunthan"],"url":"https://arxiv.org/abs/2504.19333"}
{"created":"2025-04-29","title":"Enhancing seeding efficiency using a computer vision system to monitor furrow quality in real-time","abstract":"Effective seed sowing in precision agriculture is hindered by challenges such as residue accumulation, low soil temperatures, and hair pinning (crop residue pushed in the trench by furrow opener), which obstruct optimal trench formation. Row cleaners are employed to mitigate these issues, but there is a lack of quantitative methods to assess trench cleanliness. In this study, a novel computer vision-based method was developed to evaluate row cleaner performance. Multiple air seeders were equipped with a video acquisition system to capture trench conditions after row cleaner operation, enabling an effective comparison of the performance of each row cleaner. The captured data were used to develop a segmentation model that analyzed key elements such as soil, straw, and machinery. Using the results from the segmentation model, an objective method was developed to quantify row cleaner performance. The results demonstrated the potential of this method to improve row cleaner selection and enhance seeding efficiency in precision agriculture.","authors":["Sidharth Rai","Aryan Dalal","Riley Slichter","Ajay Sharda"],"url":"https://arxiv.org/abs/2504.19334"}
{"created":"2025-04-29","title":"Exploring the Impact of Integrating UI Testing in CI/CD Workflows on GitHub","abstract":"Background: User interface (UI) testing, which is used to verify the behavior of interactive elements in applications, plays an important role in software development and quality assurance. However, little is known about the adoption of UI testing frameworks in continuous integration and continuous delivery (CI/CD) workflows and their impact on open-source software development processes. Objective: We aim to investigate the current usage of popular UI testing frameworks-Selenium, Playwright and Cypress-in CI/CD pipelines among GitHub repositories. Our goal is to understand how UI testing tools are used in CI/CD processes and assess their potential impacts on open-source development activity and CI/CD workflows. Method: We propose an empirical study to examine GitHub repositories that incorporate UI testing in CI/CD workflows. Our exploratory evaluation will collect repositories that implement UI testing frameworks in configuration files for GitHub Actions workflows to inspect UI testing-related and non-UI testing-related workflows. Moreover, we further plan to collect metrics related to repository development activity and GitHub Actions workflows to conduct comparative and time series analyses exploring whether UI testing integration and usage within CI/CD processes has an impact on open-source development.","authors":["Xiaoxiao Gan","Chris Brown"],"url":"https://arxiv.org/abs/2504.19335"}
{"created":"2025-04-29","title":"Explanatory Summarization with Discourse-Driven Planning","abstract":"Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination.","authors":["Dongqi Liu","Xi Yu","Vera Demberg","Mirella Lapata"],"url":"https://arxiv.org/abs/2504.19339"}
{"created":"2025-04-29","title":"PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies","abstract":"Achieving robust dexterous manipulation in unstructured domestic environments remains a significant challenge in robotics. Even with state-of-the-art robot learning methods, haptic-oblivious control strategies (i.e. those relying only on external vision and/or proprioception) often fall short due to occlusions, visual complexities, and the need for precise contact interaction control. To address these limitations, we introduce PolyTouch, a novel robot finger that integrates camera-based tactile sensing, acoustic sensing, and peripheral visual sensing into a single design that is compact and durable. PolyTouch provides high-resolution tactile feedback across multiple temporal scales, which is essential for efficiently learning complex manipulation tasks. Experiments demonstrate an at least 20-fold increase in lifespan over commercial tactile sensors, with a design that is both easy to manufacture and scalable. We then use this multi-modal tactile feedback along with visuo-proprioceptive observations to synthesize a tactile-diffusion policy from human demonstrations; the resulting contact-aware control policy significantly outperforms haptic-oblivious policies in multiple contact-aware manipulation policies. This paper highlights how effectively integrating multi-modal contact sensing can hasten the development of effective contact-aware manipulation policies, paving the way for more reliable and versatile domestic robots. More information can be found at https://polytouch.alanz.info/","authors":["Jialiang Zhao","Naveen Kuppuswamy","Siyuan Feng","Benjamin Burchfiel","Edward Adelson"],"url":"https://arxiv.org/abs/2504.19341"}
{"created":"2025-04-29","title":"Beyond Physical Reach: Comparing Head- and Cane-Mounted Cameras for Last-Mile Navigation by Blind Users","abstract":"Blind individuals face persistent challenges in last-mile navigation, including locating entrances, identifying obstacles, and navigating complex or cluttered spaces. Although wearable cameras are increasingly used in assistive systems, there has been no systematic, vantage-focused comparison to guide their design. This paper addresses that gap through a two-part investigation. First, we surveyed ten experienced blind cane users, uncovering navigation strategies, pain points, and technology preferences. Participants stressed the importance of multi-sensory integration, destination-focused travel, and assistive tools that complement (rather than replace) the cane's tactile utility. Second, we conducted controlled data collection with a blind participant navigating five real-world environments using synchronized head- and cane-mounted cameras, isolating vantage placement as the primary variable. To assess how each vantage supports spatial perception, we evaluated SLAM performance (for localization and mapping) and NeRF-based 3D reconstruction (for downstream scene understanding). Head-mounted sensors delivered superior localization accuracy, while cane-mounted views offered broader ground-level coverage and richer environmental reconstructions. A combined (head+cane) configuration consistently outperformed both. These results highlight the complementary strengths of different sensor placements and offer actionable guidance for developing hybrid navigation aids that are perceptive, robust, and user-aligned.","authors":["Apurv Varshney","Lucas Nadolskis","Tobias H\\\"ollerer","Michael Beyeler"],"url":"https://arxiv.org/abs/2504.19345"}
{"created":"2025-04-29","title":"Improving Small Drone Detection Through Multi-Scale Processing and Data Augmentation","abstract":"Detecting small drones, often indistinguishable from birds, is crucial for modern surveillance. This work introduces a drone detection methodology built upon the medium-sized YOLOv11 object detection model. To enhance its performance on small targets, we implemented a multi-scale approach in which the input image is processed both as a whole and in segmented parts, with subsequent prediction aggregation. We also utilized a copy-paste data augmentation technique to enrich the training dataset with diverse drone and bird examples. Finally, we implemented a post-processing technique that leverages frame-to-frame consistency to mitigate missed detections. The proposed approach attained a top-3 ranking in the 8th WOSDETC Drone-vsBird Detection Grand Challenge, held at the 2025 International Joint Conference on Neural Networks (IJCNN), showcasing its capability to detect drones in complex environments effectively.","authors":["Rayson Laroca","Marcelo dos Santos","David Menotti"],"url":"https://arxiv.org/abs/2504.19347"}
{"created":"2025-04-29","title":"Optimal Static Fully Indexable Dictionaries","abstract":"Fully indexable dictionaries (FID) store sets of integer keys while supporting rank/select queries. They serve as basic building blocks in many succinct data structures. Despite the great importance of FIDs, no known FID is succinct with efficient query time when the universe size $U$ is a large polynomial in the number of keys $n$, which is the conventional parameter regime for dictionary problems. In this paper, we design an FID that uses $\\log \\binom{U}{n} + \\frac{n}{(\\log U / t)^{\\Omega(t)}}$ bits of space, and answers rank/select queries in $O(t + \\log \\log n)$ time in the worst case, for any parameter $1 \\le t \\le \\log n / \\log \\log n$, provided $U = n^{1 + \\Theta(1)}$. This time-space trade-off matches known lower bounds for FIDs [P\\v{a}tra\\c{s}cu & Thorup STOC 2006; P\\v{a}tra\\c{s}cu & Viola SODA 2010] when $t \\le \\log^{0.99} n$.","authors":["Jingxun Liang","Renfei Zhou"],"url":"https://arxiv.org/abs/2504.19350"}
{"created":"2025-04-29","title":"Flow Along the K-Amplitude for Generative Modeling","abstract":"In this work, we propose a novel generative learning paradigm, K-Flow, an algorithm that flows along the $K$-amplitude. Here, $k$ is a scaling parameter that organizes frequency bands (or projected coefficients), and amplitude describes the norm of such projected coefficients. By incorporating the $K$-amplitude decomposition, K-Flow enables flow matching across the scaling parameter as time. We discuss three venues and six properties of K-Flow, from theoretical foundations, energy and temporal dynamics, and practical applications, respectively. Specifically, from the practical usage perspective, K-Flow allows steerable generation by controlling the information at different scales. To demonstrate the effectiveness of K-Flow, we conduct experiments on unconditional image generation, class-conditional image generation, and molecule assembly generation. Additionally, we conduct three ablation studies to demonstrate how K-Flow steers scaling parameter to effectively control the resolution of image generation.","authors":["Weitao Du","Shuning Chang","Jiasheng Tang","Yu Rong","Fan Wang","Shengchao Liu"],"url":"https://arxiv.org/abs/2504.19353"}
{"created":"2025-04-29","title":"Neurosymbolic Association Rule Mining from Tabular Data","abstract":"Association Rule Mining (ARM) is the task of mining patterns among data features in the form of logical rules, with applications across a myriad of domains. However, high-dimensional datasets often result in an excessive number of rules, increasing execution time and negatively impacting downstream task performance. Managing this rule explosion remains a central challenge in ARM research. To address this, we introduce Aerial+, a novel neurosymbolic ARM method. Aerial+ leverages an under-complete autoencoder to create a neural representation of the data, capturing associations between features. It extracts rules from this neural representation by exploiting the model's reconstruction mechanism. Extensive evaluations on five datasets against seven baselines demonstrate that Aerial+ achieves state-of-the-art results by learning more concise, high-quality rule sets with full data coverage. When integrated into rule-based interpretable machine learning models, Aerial+ significantly reduces execution time while maintaining or improving accuracy.","authors":["Erkan Karabulut","Paul Groth","Victoria Degeler"],"url":"https://arxiv.org/abs/2504.19354"}
{"created":"2025-04-29","title":"MERA: Multimodal and Multiscale Self-Explanatory Model with Considerably Reduced Annotation for Lung Nodule Diagnosis","abstract":"Lung cancer, a leading cause of cancer-related deaths globally, emphasises the importance of early detection for better patient outcomes. Pulmonary nodules, often early indicators of lung cancer, necessitate accurate, timely diagnosis. Despite Explainable Artificial Intelligence (XAI) advances, many existing systems struggle providing clear, comprehensive explanations, especially with limited labelled data. This study introduces MERA, a Multimodal and Multiscale self-Explanatory model designed for lung nodule diagnosis with considerably Reduced Annotation requirements. MERA integrates unsupervised and weakly supervised learning strategies (self-supervised learning techniques and Vision Transformer architecture for unsupervised feature extraction) and a hierarchical prediction mechanism leveraging sparse annotations via semi-supervised active learning in the learned latent space. MERA explains its decisions on multiple levels: model-level global explanations via semantic latent space clustering, instance-level case-based explanations showing similar instances, local visual explanations via attention maps, and concept explanations using critical nodule attributes. Evaluations on the public LIDC dataset show MERA's superior diagnostic accuracy and self-explainability. With only 1% annotated samples, MERA achieves diagnostic accuracy comparable to or exceeding state-of-the-art methods requiring full annotation. The model's inherent design delivers comprehensive, robust, multilevel explanations aligned closely with clinical practice, enhancing trustworthiness and transparency. Demonstrated viability of unsupervised and weakly supervised learning lowers the barrier to deploying diagnostic AI in broader medical domains. Our complete code is open-source available: https://github.com/diku-dk/credanno.","authors":["Jiahao Lu","Chong Yin","Silvia Ingala","Kenny Erleben","Michael Bachmann Nielsen","Sune Darkner"],"url":"https://arxiv.org/abs/2504.19357"}
{"created":"2025-04-29","title":"A filtered finite difference method for a highly oscillatory nonlinear Klein--Gordon equation","abstract":"We consider a nonlinear Klein--Gordon equation in the nonrelativistic limit regime with highly oscillatory initial data in the form of a modulated plane wave. In this regime, the solution exhibits rapid oscillations in both time and space, posing challenges for numerical approximation. We propose a filtered finite difference method that achieves second-order accuracy with time steps and mesh sizes that are not restricted in magnitude by the small parameter. Moreover, the method is uniformly convergent in the range from arbitrarily small to moderately bounded scaling parameters. Numerical experiments illustrate the theoretical results.","authors":["Yanyan Shi","Christian Lubich"],"url":"https://arxiv.org/abs/2504.19359"}
{"created":"2025-04-29","title":"Sequence Reconstruction for Sticky Insertion/Deletion Channels","abstract":"The sequence reconstruction problem for insertion/deletion channels has attracted significant attention owing to their applications recently in some emerging data storage systems, such as racetrack memories, DNA-based data storage. Our goal is to investigate the reconstruction problem for sticky-insdel channels where both sticky-insertions and sticky-deletions occur. If there are only sticky-insertion errors, the reconstruction problem for sticky-insertion channel is a special case of the reconstruction problem for tandem-duplication channel which has been well-studied. In this work, we consider the $(t, s)$-sticky-insdel channel where there are at most $t$ sticky-insertion errors and $s$ sticky-deletion errors when we transmit a message through the channel. For the reconstruction problem, we are interested in the minimum number of distinct outputs from these channels that are needed to uniquely recover the transmitted vector. We first provide a recursive formula to determine the minimum number of distinct outputs required. Next, we provide an efficient algorithm to reconstruct the transmitted vector from erroneous sequences.","authors":["Van Long Phuoc Pham","Yeow Meng Chee","Kui Cai","Van Khu Vu"],"url":"https://arxiv.org/abs/2504.19363"}
{"created":"2025-04-29","title":"AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration","abstract":"Graphics Processing Units (GPUs) have become essential for computationally intensive applications. However, emerging workloads such as recommender systems, graph analytics, and data analytics often involve processing data exceeding GPU on-chip memory capacity. To mitigate this issue, existing solutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them, the GPU-centric approach lets GPU threads directly initiate NVMe requests, eliminating CPU intervention overhead over traditional methods. However, the SOTA GPU-centric approach adopts a synchronous IO model, and threads must tolerate the long latency in communication before starting any tasks.","authors":["Zhuoping Yang","Jinming Zhuang","Xingzhen Chen","Alex K. Jones","Peipei Zhou"],"url":"https://arxiv.org/abs/2504.19365"}
{"created":"2025-04-29","title":"Mitigating Bias in Facial Recognition Systems: Centroid Fairness Loss Optimization","abstract":"The urging societal demand for fair AI systems has put pressure on the research community to develop predictive models that are not only globally accurate but also meet new fairness criteria, reflecting the lack of disparate mistreatment with respect to sensitive attributes ($\\textit{e.g.}$ gender, ethnicity, age). In particular, the variability of the errors made by certain Facial Recognition (FR) systems across specific segments of the population compromises the deployment of the latter, and was judged unacceptable by regulatory authorities. Designing fair FR systems is a very challenging problem, mainly due to the complex and functional nature of the performance measure used in this domain ($\\textit{i.e.}$ ROC curves) and because of the huge heterogeneity of the face image datasets usually available for training. In this paper, we propose a novel post-processing approach to improve the fairness of pre-trained FR models by optimizing a regression loss which acts on centroid-based scores. Beyond the computational advantages of the method, we present numerical experiments providing strong empirical evidence of the gain in fairness and of the ability to preserve global accuracy.","authors":["Jean-R\\'emy Conti","St\\'ephan Cl\\'emen\\c{c}on"],"url":"https://arxiv.org/abs/2504.19370"}
{"created":"2025-04-29","title":"Doxing via the Lens: Revealing Privacy Leakage in Image Geolocation for Agentic Multi-Modal Large Reasoning Model","abstract":"The increasing capabilities of agentic multi-modal large reasoning models, such as ChatGPT o3, have raised critical concerns regarding privacy leakage through inadvertent image geolocation. In this paper, we conduct the first systematic and controlled study on the potential privacy risks associated with visual reasoning abilities of ChatGPT o3. We manually collect and construct a dataset comprising 50 real-world images that feature individuals alongside privacy-relevant environmental elements, capturing realistic and sensitive scenarios for analysis. Our experimental evaluation reveals that ChatGPT o3 can predict user locations with high precision, achieving street-level accuracy (within one mile) in 60% of cases. Through analysis, we identify key visual cues, including street layout and front yard design, that significantly contribute to the model inference success. Additionally, targeted occlusion experiments demonstrate that masking critical features effectively mitigates geolocation accuracy, providing insights into potential defense mechanisms. Our findings highlight an urgent need for privacy-aware development for agentic multi-modal large reasoning models, particularly in applications involving private imagery.","authors":["Weidi Luo","Qiming Zhang","Tianyu Lu","Xiaogeng Liu","Yue Zhao","Zhen Xiang","Chaowei Xiao"],"url":"https://arxiv.org/abs/2504.19373"}
{"created":"2025-04-29","title":"Rethinking Label-specific Features for Label Distribution Learning","abstract":"Label distribution learning (LDL) is an emerging learning paradigm designed to capture the relative importance of labels for each instance. Label-specific features (LSFs), constructed by LIFT, have proven effective for learning tasks with label ambiguity by leveraging clustering-based prototypes for each label to re-characterize instances. However, directly introducing LIFT into LDL tasks can be suboptimal, as the prototypes it collects primarily reflect intra-cluster relationships while neglecting interactions among distinct clusters. Additionally, constructing LSFs using multi-perspective information, rather than relying solely on Euclidean distance, provides a more robust and comprehensive representation of instances, mitigating noise and bias that may arise from a single distance perspective. To address these limitations, we introduce Structural Anchor Points (SAPs) to capture inter-cluster interactions. This leads to a novel LSFs construction strategy, LIFT-SAP, which enhances LIFT by integrating both distance and direction information of each instance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label Distribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP), which unifies multiple label description degrees predicted from different LSF spaces into a cohesive label distribution. Extensive experiments on 15 real-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as well as the superiority of LDL-LIFT-SAP compared to seven other well-established algorithms.","authors":["Suping Xu","Chuyi Dai","Lin Shang","Changbin Shao","Xibei Yang","Witold Pedrycz"],"url":"https://arxiv.org/abs/2504.19374"}
{"created":"2025-04-29","title":"$O(1/k)$ Finite-Time Bound for Non-Linear Two-Time-Scale Stochastic Approximation","abstract":"Two-time-scale stochastic approximation is an algorithm with coupled iterations which has found broad applications in reinforcement learning, optimization and game control. While several prior works have obtained a mean square error bound of $O(1/k)$ for linear two-time-scale iterations, the best known bound in the non-linear contractive setting has been $O(1/k^{2/3})$. In this work, we obtain an improved bound of $O(1/k)$ for non-linear two-time-scale stochastic approximation. Our result applies to algorithms such as gradient descent-ascent and two-time-scale Lagrangian optimization. The key step in our analysis involves rewriting the original iteration in terms of an averaged noise sequence which decays sufficiently fast. Additionally, we use an induction-based approach to show that the iterates are bounded in expectation.","authors":["Siddharth Chandak"],"url":"https://arxiv.org/abs/2504.19375"}
{"created":"2025-04-29","title":"Second-Order Compatible-Strain Mixed Finite Elements for 2D Compressible Nonlinear Elasticity","abstract":"In recent years, a new class of mixed finite elements -- compatible-strain mixed finite elements (CSMFEs) -- has emerged that uses the differential complex of nonlinear elasticity. Their excellent performance in benchmark problems, such as numerical stability for modeling large deformations in near-incompressible solids, makes them a promising choice for solving engineering problems. Explicit forms exist for various shape functions of first-order CSMFEs. In contrast, existing second-order CSMFEs evaluate shape functions using numerical integration. In this paper, we formulate second-order CSMFEs with explicit shape functions for the displacement gradient and stress tensor. Concepts of vector calculus that stem from exterior calculus are presented and used to provide efficient forms for shape functions in the natural coordinate system. Covariant and contravariant Piola transformations are then applied to transform the shape functions to the physical space. Mid-nodes and pseudo-nodes are used to enforce the continuity constraints for the displacement gradient and stress tensor over the boundaries of elements. The formulation of the proposed second-order CSMFEs and technical aspects regarding their implementation are discussed in detail. Several benchmark problems are solved to compare the performance of CSMFEs with first-order CSMFEs and other second-order elements that rely on numerical integration. It is shown that the proposed CSMFEs are numerically stable for modeling near-incompressible solids in the finite strain regime.","authors":["Mohsen Jahanshahi","Damiano Pasini","Arash Yavari"],"url":"https://arxiv.org/abs/2504.19376"}
{"created":"2025-04-29","title":"HyperController: A Hyperparameter Controller for Fast and Stable Training of Reinforcement Learning Neural Networks","abstract":"We introduce Hyperparameter Controller (HyperController), a computationally efficient algorithm for hyperparameter optimization during training of reinforcement learning neural networks. HyperController optimizes hyperparameters quickly while also maintaining improvement of the reinforcement learning neural network, resulting in faster training and deployment. It achieves this by modeling the hyperparameter optimization problem as an unknown Linear Gaussian Dynamical System, which is a system with a state that linearly changes. It then learns an efficient representation of the hyperparameter objective function using the Kalman filter, which is the optimal one-step predictor for a Linear Gaussian Dynamical System. To demonstrate the performance of HyperController, it is applied as a hyperparameter optimizer during training of reinforcement learning neural networks on a variety of OpenAI Gymnasium environments. In four out of the five Gymnasium environments, HyperController achieves highest median reward during evaluation compared to other algorithms. The results exhibit the potential of HyperController for efficient and stable training of reinforcement learning neural networks.","authors":["Jonathan Gornet","Yiannis Kantaros","Bruno Sinopoli"],"url":"https://arxiv.org/abs/2504.19382"}
{"created":"2025-04-29","title":"From Inductive to Deductive: LLMs-Based Qualitative Data Analysis in Requirements Engineering","abstract":"Requirements Engineering (RE) is essential for developing complex and regulated software projects. Given the challenges in transforming stakeholder inputs into consistent software designs, Qualitative Data Analysis (QDA) provides a systematic approach to handling free-form data. However, traditional QDA methods are time-consuming and heavily reliant on manual effort. In this paper, we explore the use of Large Language Models (LLMs), including GPT-4, Mistral, and LLaMA-2, to improve QDA tasks in RE. Our study evaluates LLMs' performance in inductive (zero-shot) and deductive (one-shot, few-shot) annotation tasks, revealing that GPT-4 achieves substantial agreement with human analysts in deductive settings, with Cohen's Kappa scores exceeding 0.7, while zero-shot performance remains limited. Detailed, context-rich prompts significantly improve annotation accuracy and consistency, particularly in deductive scenarios, and GPT-4 demonstrates high reliability across repeated runs. These findings highlight the potential of LLMs to support QDA in RE by reducing manual effort while maintaining annotation quality. The structured labels automatically provide traceability of requirements and can be directly utilized as classes in domain models, facilitating systematic software design.","authors":["Syed Tauhid Ullah Shah","Mohamad Hussein","Ann Barcomb","Mohammad Moshirpour"],"url":"https://arxiv.org/abs/2504.19384"}
{"created":"2025-04-29","title":"Hardness of Finding Kings and Strong Kings","abstract":"A king in a directed graph is a vertex $v$ such that every other vertex is reachable from $v$ via a path of length at most $2$. It is well known that every tournament (a complete graph where each edge has a direction) has at least one king. Our contributions in this work are:","authors":["Ziad Ismaili Alaoui","Nikhil S. Mande"],"url":"https://arxiv.org/abs/2504.19386"}
{"created":"2025-04-29","title":"HumMorph: Generalized Dynamic Human Neural Fields from Few Views","abstract":"We introduce HumMorph, a novel generalized approach to free-viewpoint rendering of dynamic human bodies with explicit pose control. HumMorph renders a human actor in any specified pose given a few observed views (starting from just one) in arbitrary poses. Our method enables fast inference as it relies only on feed-forward passes through the model. We first construct a coarse representation of the actor in the canonical T-pose, which combines visual features from individual partial observations and fills missing information using learned prior knowledge. The coarse representation is complemented by fine-grained pixel-aligned features extracted directly from the observed views, which provide high-resolution appearance information. We show that HumMorph is competitive with the state-of-the-art when only a single input view is available, however, we achieve results with significantly better visual quality given just 2 monocular observations. Moreover, previous generalized methods assume access to accurate body shape and pose parameters obtained using synchronized multi-camera setups. In contrast, we consider a more practical scenario where these body parameters are noisily estimated directly from the observed views. Our experimental results demonstrate that our architecture is more robust to errors in the noisy parameters and clearly outperforms the state of the art in this setting.","authors":["Jakub Zadro\\.zny","Hakan Bilen"],"url":"https://arxiv.org/abs/2504.19390"}
{"created":"2025-04-29","title":"Bi-directional Model Cascading with Proxy Confidence","abstract":"Model Cascading, recently applied successfully to LLMs, is a simple but powerful technique that improves the efficiency of inference by selectively applying models of varying sizes. Models are used in sequence from smallest to largest, only deferring samples to large, costly models when smaller models are not sufficiently confident. Existing approaches to deferral use only limited small model confidence estimates because of the inaccessibility of the large model, although large model confidence is known to be important. We therefore propose a bi-directional approach to deferral that considers the confidence of small and large models in the cascade simultaneously through the use of a proxy for the large model. This requires a richer representation of model confidence to enable comparative calibration: we use an analysis of hidden states to improve post-invocation confidence of the small model, which in itself improves cascading results over prior approaches. We then combine this with a tiny proxy model to estimate pre-invocation confidence of the large model. We examine the proposed cascading system over challenging, multiple-choice datasets, finding improvements over standard cascading baselines reflected in reductions in deferrals to more costly models.","authors":["David Warren","Mark Dras"],"url":"https://arxiv.org/abs/2504.19391"}
{"created":"2025-04-29","title":"LLMs for Engineering: Teaching Models to Design High Powered Rockets","abstract":"Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.","authors":["Toby Simonds"],"url":"https://arxiv.org/abs/2504.19394"}
{"created":"2025-04-29","title":"ICL CIPHERS: Quantifying \"Learning'' in In-Context Learning via Substitution Ciphers","abstract":"Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning'' from demonstrations). However, disentangling these the two modes remains a challenging goal. We introduce ICL CIPHERS, a class of task reformulations based on substitution ciphers borrowed from classic cryptography. In this approach, a subset of tokens in the in-context inputs are substituted with other (irrelevant) tokens, rendering English sentences less comprehensible to human eye. However, by design, there is a latent, fixed pattern to this substitution, making it reversible. This bijective (reversible) cipher ensures that the task remains a well-defined task in some abstract sense, despite the transformations. It is a curious question if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires deciphering the latent cipher. We show that LLMs are better at solving ICL CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify ``learning'' in ICL. While this gap is small, it is consistent across the board on four datasets and six models. Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.","authors":["Zhouxiang Fang","Aayush Mishra","Muhan Gao","Anqi Liu","Daniel Khashabi"],"url":"https://arxiv.org/abs/2504.19395"}
{"created":"2025-04-29","title":"Observational Learning with a Budget","abstract":"We consider a model of Bayesian observational learning in which a sequence of agents receives a private signal about an underlying binary state of the world. Each agent makes a decision based on its own signal and its observations of previous agents. A central planner seeks to improve the accuracy of these signals by allocating a limited budget to enhance signal quality across agents. We formulate and analyze the budget allocation problem and propose two optimal allocation strategies. At least one of these strategies is shown to maximize the probability of achieving a correct information cascade.","authors":["Shuo Wu","Pawan Poojary","Randall Berry"],"url":"https://arxiv.org/abs/2504.19396"}
{"created":"2025-04-29","title":"Symmetric Policy Design for Multi-Agent Dispatch Coordination in Supply Chains","abstract":"We study a decentralized dispatch coordination problem in a multi-agent supply chain setting with shared logistics capacity. We propose symmetric (identical) dispatch strategies for all agents, enabling efficient coordination without centralized control. Using a common information approach, we derive a dynamic programming solution that computes optimal symmetric dispatch strategies by transforming the multi-agent problem into a tractable dynamic program on the agents common information state. Simulation results demonstrate that our method significantly reduces coordination cost compared to baseline heuristics, including belief-based strategies and an always-dispatch policy. These findings highlight the benefits of combining symmetric strategy design with a common information-based dynamic programming framework for improving multi-agent coordination performance.","authors":["Sagar Sudhakara"],"url":"https://arxiv.org/abs/2504.19397"}
{"created":"2025-04-29","title":"Dynamic Arthroscopic Navigation System for Anterior Cruciate Ligament Reconstruction Based on Multi-level Memory Architecture","abstract":"This paper presents a dynamic arthroscopic navigation system based on multi-level memory architecture for anterior cruciate ligament (ACL) reconstruction surgery. The system extends our previously proposed markerless navigation method from static image matching to dynamic video sequence tracking. By integrating the Atkinson-Shiffrin memory model's three-level architecture (sensory memory, working memory, and long-term memory), our system maintains continuous tracking of the femoral condyle throughout the surgical procedure, providing stable navigation support even in complex situations involving viewpoint changes, instrument occlusion, and tissue deformation. Unlike existing methods, our system operates in real-time on standard arthroscopic equipment without requiring additional tracking hardware, achieving 25.3 FPS with a latency of only 39.5 ms, representing a 3.5-fold improvement over our previous static system. For extended sequences (1000 frames), the dynamic system maintained an error of 5.3 plus-minus 1.5 pixels, compared to the static system's 12.6 plus-minus 3.7 pixels - an improvement of approximately 45 percent. For medium-length sequences (500 frames) and short sequences (100 frames), the system achieved approximately 35 percent and 19 percent accuracy improvements, respectively. Experimental results demonstrate the system overcomes limitations of traditional static matching methods, providing new technical support for improving surgical precision in ACL reconstruction.","authors":["Shuo Wang","Weili Shi","Shuai Yang","Jiahao Cui","Qinwei Guo"],"url":"https://arxiv.org/abs/2504.19398"}
{"created":"2025-04-29","title":"Follow Everything: A Leader-Following and Obstacle Avoidance Framework with Goal-Aware Adaptation","abstract":"Robust and flexible leader-following is a critical capability for robots to integrate into human society. While existing methods struggle to generalize to leaders of arbitrary form and often fail when the leader temporarily leaves the robot's field of view, this work introduces a unified framework addressing both challenges. First, traditional detection models are replaced with a segmentation model, allowing the leader to be anything. To enhance recognition robustness, a distance frame buffer is implemented that stores leader embeddings at multiple distances, accounting for the unique characteristics of leader-following tasks. Second, a goal-aware adaptation mechanism is designed to govern robot planning states based on the leader's visibility and motion, complemented by a graph-based planner that generates candidate trajectories for each state, ensuring efficient following with obstacle avoidance. Simulations and real-world experiments with a legged robot follower and various leaders (human, ground robot, UAV, legged robot, stop sign) in both indoor and outdoor environments show competitive improvements in follow success rate, reduced visual loss duration, lower collision rate, and decreased leader-follower distance.","authors":["Qianyi Zhang","Shijian Ma","Boyi Liu","Jingtai Liu","Jianhao Jiao","Dimitrios Kanoulas"],"url":"https://arxiv.org/abs/2504.19399"}
{"created":"2025-04-29","title":"Boosting 3D Liver Shape Datasets with Diffusion Models and Implicit Neural Representations","abstract":"While the availability of open 3D medical shape datasets is increasing, offering substantial benefits to the research community, we have found that many of these datasets are, unfortunately, disorganized and contain artifacts. These issues limit the development and training of robust models, particularly for accurate 3D reconstruction tasks. In this paper, we examine the current state of available 3D liver shape datasets and propose a solution using diffusion models combined with implicit neural representations (INRs) to augment and expand existing datasets. Our approach utilizes the generative capabilities of diffusion models to create realistic, diverse 3D liver shapes, capturing a wide range of anatomical variations and addressing the problem of data scarcity. Experimental results indicate that our method enhances dataset diversity, providing a scalable solution to improve the accuracy and reliability of 3D liver reconstruction and generation in medical applications. Finally, we suggest that diffusion models can also be applied to other downstream tasks in 3D medical imaging.","authors":["Khoa Tuan Nguyen","Francesca Tozzi","Wouter Willaert","Joris Vankerschaver","Nikdokht Rashidian","Wesley De Neve"],"url":"https://arxiv.org/abs/2504.19402"}
{"created":"2025-04-29","title":"Context Selection and Rewriting for Video-based EducationalQuestion Generation","abstract":"Educational question generation (EQG) is a crucial component of intelligent educational systems, significantly aiding self-assessment, active learning, and personalized education. While EQG systems have emerged, existing datasets typically rely on predefined, carefully edited texts, failing to represent real-world classroom content, including lecture speech with a set of complementary slides. To bridge this gap, we collect a dataset of educational questions based on lectures from real-world classrooms. On this realistic dataset, we find that current methods for EQG struggle with accurately generating questions from educational videos, particularly in aligning with specific timestamps and target answers. Common challenges include selecting informative contexts from extensive transcripts and ensuring generated questions meaningfully incorporate the target answer. To address the challenges, we introduce a novel framework utilizing large language models for dynamically selecting and rewriting contexts based on target timestamps and answers. First, our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity. Then, we integrate the contexts selected from both modalities and rewrite them into answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer. This approach significantly improves the quality and relevance of the generated questions. Our dataset and code are released in https://github.com/mengxiayu/COSER.","authors":["Mengxia Yu","Bang Nguyen","Olivia Zino","Meng Jiang"],"url":"https://arxiv.org/abs/2504.19406"}
{"created":"2025-04-29","title":"UNet with Axial Transformer : A Neural Weather Model for Precipitation Nowcasting","abstract":"Making accurate weather predictions can be particularly challenging for localized storms or events that evolve on hourly timescales, such as thunderstorms. Hence, our goal for the project was to model Weather Nowcasting for making highly localized and accurate predictions that apply to the immediate future replacing the current numerical weather models and data assimilation systems with Deep Learning approaches. A significant advantage of machine learning is that inference is computationally cheap given an already-trained model, allowing forecasts that are nearly instantaneous and in the native high resolution of the input data. In this work we developed a novel method that employs Transformer-based machine learning models to forecast precipitation. This approach works by leveraging axial attention mechanisms to learn complex patterns and dynamics from time series frames. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings data. This paper represents an initial research on the dataset used in the domain of next frame prediciton, and hence, we demonstrate state-of-the-art results in terms of metrices (PSNR = 47.67, SSIM = 0.9943) used for the given dataset using UNet with Axial Transformer.","authors":["Maitreya Sonawane","Sumit Mamtani"],"url":"https://arxiv.org/abs/2504.19408"}
{"created":"2025-04-29","title":"GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field","abstract":"Semantic-aware 3D scene reconstruction is essential for autonomous robots to perform complex interactions. Semantic SLAM, an online approach, integrates pose tracking, geometric reconstruction, and semantic mapping into a unified framework, shows significant potential. However, existing systems, which rely on 2D ground truth priors for supervision, are often limited by the sparsity and noise of these signals in real-world environments. To address this challenge, we propose GSFF-SLAM, a novel dense semantic SLAM system based on 3D Gaussian Splatting that leverages feature fields to achieve joint rendering of appearance, geometry, and N-dimensional semantic features. By independently optimizing feature gradients, our method supports semantic reconstruction using various forms of 2D priors, particularly sparse and noisy signals. Experimental results demonstrate that our approach outperforms previous methods in both tracking accuracy and photorealistic rendering quality. When utilizing 2D ground truth priors, GSFF-SLAM achieves state-of-the-art semantic segmentation performance with 95.03\\% mIoU, while achieving up to 2.9$\\times$ speedup with only marginal performance degradation.","authors":["Zuxing Lu","Xin Yuan","Shaowen Yang","Jingyu Liu","Jiawei Wang","Changyin Sun"],"url":"https://arxiv.org/abs/2504.19409"}
{"created":"2025-04-29","title":"Fast convolution solver based on far-field smooth approximation","abstract":"The convolution potential arises in a wide variety of application areas, and its efficient and accurate evaluation encounters three challenges: singularity, nonlocality and anisotropy. We introduce a fast algorithm based on a far-field smooth approximation of the kernel, where the bounded domain Fourier transform, one of the most essential difficulties, is well approximated by the whole space Fourier transform which usually admits explicit formula. The convolution is split into a regular and singular integral, and they are well resolved by trapezoidal rule and Fourier spectral method respectively. The scheme is simplified to a discrete convolution and is implemented efficiently with Fast Fourier Transform (FFT). Importantly, the tensor generation procedure is quite simple, highly efficient and independent of the anisotropy strength. It is easy to implement and achieves spectral accuracy with nearly optimal efficiency and minimum memory requirement. Rigorous error estimates and extensive numerical investigations, together with a comprehensive comparison, showcase its superiorities for different kernels.","authors":["Xin Liu","Yong Zhang"],"url":"https://arxiv.org/abs/2504.19410"}
{"created":"2025-04-29","title":"A Comparison-Relationship-Surrogate Evolutionary Algorithm for Multi-Objective Optimization","abstract":"Evolutionary algorithms often struggle to find high-quality solutions to multi-objective optimization problems on a limited budget of function evaluations (here, a few hundred). A promising direction to improve the efficiency of these methods is to augment the objective functions with a data-driven surrogate model. These ``surrogate-assisted'' optimization algorithms can achieve better solutions than conventional algorithms for the same number of function evaluations on a wide variety of test problems. In this work, we continue to explore the area of surrogate-assisted multi-objective optimization by introducing and testing an algorithm driven by a new type of surrogate model: a comparison-relationship-surrogate model. This model predicts the truth values of the comparison operator evaluated on the objective functions for two candidate solutions. These predictions can be used to infer the domination relationships that power the non-dominated sorting mechanism used by many multi-objective genetic algorithms to select fit individuals. Several numerical experiments are performed on this algorithm using well-known test suites plus a real-world problem from the field of accelerator physics. Statistical analysis of the results demonstrates that the new algorithm can, on average, achieve better-converged solutions to many medium-scale, biobjective problems than existing state-of-the-art methods for a limited budget of function evaluations.","authors":["Christopher M. Pierce","Young-Kee Kim","Ivan Bazarov"],"url":"https://arxiv.org/abs/2504.19411"}
{"created":"2025-04-29","title":"Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory","abstract":"Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.","authors":["Prateek Chhikara","Dev Khant","Saket Aryan","Taranjeet Singh","Deshraj Yadav"],"url":"https://arxiv.org/abs/2504.19413"}
{"created":"2025-04-29","title":"GMAR: Gradient-Driven Multi-Head Attention Rollout for Vision Transformer Interpretability","abstract":"The Vision Transformer (ViT) has made significant advancements in computer vision, utilizing self-attention mechanisms to achieve state-of-the-art performance across various tasks, including image classification, object detection, and segmentation. Its architectural flexibility and capabilities have made it a preferred choice among researchers and practitioners. However, the intricate multi-head attention mechanism of ViT presents significant challenges to interpretability, as the underlying prediction process remains opaque. A critical limitation arises from an observation commonly noted in transformer architectures: \"Not all attention heads are equally meaningful.\" Overlooking the relative importance of specific heads highlights the limitations of existing interpretability methods. To address these challenges, we introduce Gradient-Driven Multi-Head Attention Rollout (GMAR), a novel method that quantifies the importance of each attention head using gradient-based scores. These scores are normalized to derive a weighted aggregate attention score, effectively capturing the relative contributions of individual heads. GMAR clarifies the role of each head in the prediction process, enabling more precise interpretability at the head level. Experimental results demonstrate that GMAR consistently outperforms traditional attention rollout techniques. This work provides a practical contribution to transformer-based architectures, establishing a robust framework for enhancing the interpretability of Vision Transformer models.","authors":["Sehyeong Jo","Gangjae Jang","Haesol Park"],"url":"https://arxiv.org/abs/2504.19414"}
{"created":"2025-04-29","title":"A Real-Time Event-Based Normal Flow Estimator","abstract":"This paper presents a real-time, asynchronous, event-based normal flow estimator. It follows the same algorithm as Learning Normal Flow Directly From Event Neighborhoods, but with a more optimized implementation. The original method treats event slices as 3D point clouds, encodes each event's local geometry into a fixed-length vector, and uses a multi-layer perceptron to predict normal flow. It constructs representations by multiplying an adjacency matrix with a feature matrix, resulting in quadratic time complexity with respect to the number of events. In contrast, we leverage the fact that event coordinates are integers and reformulate the representation step as a pooling operation. This achieves the same effect as the adjacency matrix but with much lower computational cost. As a result, our method supports real-time normal flow prediction on event cameras. Our estimator uses 1 GB of CUDA memory and runs at 4 million normal flows per second on an RTX 3070, or 6 million per second on an RTX A5000. We release the CUDA implementation along with a Python interface at https://github.com/dhyuan99/VecKM_flow_cpp.","authors":["Dehao Yuan","Cornelia Ferm\\\"uller"],"url":"https://arxiv.org/abs/2504.19417"}
{"created":"2025-04-29","title":"ChipletQuake: On-die Digital Impedance Sensing for Chiplet and Interposer Verification","abstract":"The increasing complexity and cost of manufacturing monolithic chips have driven the semiconductor industry toward chiplet-based designs, where smaller and modular chiplets are integrated onto a single interposer. While chiplet architectures offer significant advantages, such as improved yields, design flexibility, and cost efficiency, they introduce new security challenges in the horizontal hardware manufacturing supply chain. These challenges include risks of hardware Trojans, cross-die side-channel and fault injection attacks, probing of chiplet interfaces, and intellectual property theft. To address these concerns, this paper presents \\textit{ChipletQuake}, a novel on-chiplet framework for verifying the physical security and integrity of adjacent chiplets during the post-silicon stage. By sensing the impedance of the power delivery network (PDN) of the system, \\textit{ChipletQuake} detects tamper events in the interposer and neighboring chiplets without requiring any direct signal interface or additional hardware components. Fully compatible with the digital resources of FPGA-based chiplets, this framework demonstrates the ability to identify the insertion of passive and subtle malicious circuits, providing an effective solution to enhance the security of chiplet-based systems. To validate our claims, we showcase how our framework detects Hardware Trojan and interposer tampering.","authors":["Saleh Khalaj Monfared","Maryam Saadat Safa","Shahin Tajik"],"url":"https://arxiv.org/abs/2504.19418"}
{"created":"2025-04-29","title":"Graph-based Semi-supervised and Unsupervised Methods for Local Clustering","abstract":"Local clustering aims to identify specific substructures within a large graph without requiring full knowledge of the entire graph. These substructures are typically small compared to the overall graph, enabling the problem to be approached by finding a sparse solution to a linear system associated with the graph Laplacian. In this work, we first propose a method for identifying specific local clusters when very few labeled data is given, which we term semi-supervised local clustering. We then extend this approach to the unsupervised setting when no prior information on labels is available. The proposed methods involve randomly sampling the graph, applying diffusion through local cluster extraction, then examining the overlap among the results to find each cluster. We establish the co-membership conditions for any pair of nodes and rigorously prove the correctness of our methods. Additionally, we conduct extensive experiments to demonstrate that the proposed methods achieve state-of-the-arts results in the low-label rates regime.","authors":["Zhaiming Shen","Sung Ha Kang"],"url":"https://arxiv.org/abs/2504.19419"}
{"created":"2025-04-29","title":"Quantitative estimates for a nonlinear inverse source problem in a coupled diffusion equations with uncertain measurements","abstract":"This work considers a nonlinear inverse source problem in a coupled diffusion equation from the terminal observation. Theoretically, under some conditions on problem data, we build the uniqueness theorem for this inverse problem and show two Lipschitz-type stability results in $L^2$ and $(H^1(\\cdot))^*$ norms, respectively. However, in practice, we could only observe the measurements at discrete sensors, which contain the noise. Hence, this work further investigates the recovery of the unknown source from the discrete noisy measurements. We propose a stable inversion scheme and provide probabilistic convergence estimates between the reconstructions and exact solution in two cases: convergence respect to expectation and convergence with an exponential tail. We provide several numerical experiments to illustrate and complement our theoretical analysis.","authors":["Chunlong Sun","Wenlong Zhang","Zhidong Zhang"],"url":"https://arxiv.org/abs/2504.19421"}
{"created":"2025-04-29","title":"MER 2025: When Affective Computing Meets Large Language Models","abstract":"MER2025 is the third year of our MER series of challenges, aiming to bring together researchers in the affective computing community to explore emerging trends and future directions in the field. Previously, MER2023 focused on multi-label learning, noise robustness, and semi-supervised learning, while MER2024 introduced a new track dedicated to open-vocabulary emotion recognition. This year, MER2025 centers on the theme \"When Affective Computing Meets Large Language Models (LLMs)\".We aim to shift the paradigm from traditional categorical frameworks reliant on predefined emotion taxonomies to LLM-driven generative methods, offering innovative solutions for more accurate and reliable emotion understanding. The challenge features four tracks: MER-SEMI focuses on fixed categorical emotion recognition enhanced by semi-supervised learning; MER-FG explores fine-grained emotions, expanding recognition from basic to nuanced emotional states; MER-DES incorporates multimodal cues (beyond emotion words) into predictions to enhance model interpretability; MER-PR investigates whether emotion prediction results can improve personality recognition performance. For the first three tracks, baseline code is available at MERTools, and datasets can be accessed via Hugging Face. For the last track, the dataset and baseline code are available on GitHub.","authors":["Zheng Lian","Rui Liu","Kele Xu","Bin Liu","Xuefei Liu","Yazhou Zhang","Xin Liu","Yong Li","Zebang Cheng","Haolin Zuo","Ziyang Ma","Xiaojiang Peng","Xie Chen","Ya Li","Erik Cambria","Guoying Zhao","Bj\\\"orn W. Schuller","Jianhua Tao"],"url":"https://arxiv.org/abs/2504.19423"}
{"created":"2025-04-29","title":"EarthMapper: Visual Autoregressive Models for Controllable Bidirectional Satellite-Map Translation","abstract":"Satellite imagery and maps, as two fundamental data modalities in remote sensing, offer direct observations of the Earth's surface and human-interpretable geographic abstractions, respectively. The task of bidirectional translation between satellite images and maps (BSMT) holds significant potential for applications in urban planning and disaster response. However, this task presents two major challenges: first, the absence of precise pixel-wise alignment between the two modalities substantially complicates the translation process; second, it requires achieving both high-level abstraction of geographic features and high-quality visual synthesis, which further elevates the technical complexity. To address these limitations, we introduce EarthMapper, a novel autoregressive framework for controllable bidirectional satellite-map translation. EarthMapper employs geographic coordinate embeddings to anchor generation, ensuring region-specific adaptability, and leverages multi-scale feature alignment within a geo-conditioned joint scale autoregression (GJSA) process to unify bidirectional translation in a single training cycle. A semantic infusion (SI) mechanism is introduced to enhance feature-level consistency, while a key point adaptive guidance (KPAG) mechanism is proposed to dynamically balance diversity and precision during inference. We further contribute CNSatMap, a large-scale dataset comprising 302,132 precisely aligned satellite-map pairs across 38 Chinese cities, enabling robust benchmarking. Extensive experiments on CNSatMap and the New York dataset demonstrate EarthMapper's superior performance, achieving significant improvements in visual realism, semantic consistency, and structural fidelity over state-of-the-art methods. Additionally, EarthMapper excels in zero-shot tasks like in-painting, out-painting and coordinate-conditional generation, underscoring its versatility.","authors":["Zhe Dong","Yuzhe Sun","Tianzhu Liu","Wangmeng Zuo","Yanfeng Gu"],"url":"https://arxiv.org/abs/2504.19432"}
{"created":"2025-04-29","title":"GTSD: Generative Text Steganography Based on Diffusion Model","abstract":"With the rapid development of deep learning, existing generative text steganography methods based on autoregressive models have achieved success. However, these autoregressive steganography approaches have certain limitations. Firstly, existing methods require encoding candidate words according to their output probability and generating each stego word one by one, which makes the generation process time-consuming. Secondly, encoding and selecting candidate words changes the sampling probabilities, resulting in poor imperceptibility of the stego text. Thirdly, existing methods have low robustness and cannot resist replacement attacks. To address these issues, we propose a generative text steganography method based on a diffusion model (GTSD), which improves generative speed, robustness, and imperceptibility while maintaining security. To be specific, a novel steganography scheme based on diffusion model is proposed to embed secret information through prompt mapping and batch mapping. The prompt mapping maps secret information into a conditional prompt to guide the pre-trained diffusion model generating batches of candidate sentences. The batch mapping selects stego text based on secret information from batches of candidate sentences. Extensive experiments show that the GTSD outperforms the SOTA method in terms of generative speed, robustness, and imperceptibility while maintaining comparable anti-steganalysis performance. Moreover, we verify that the GTSD has strong potential: embedding capacity is positively correlated with prompt capacity and model batch sizes while maintaining security.","authors":["Zhengxian Wu","Juan Wen","Yiming Xue","Ziwei Zhang","Yinghan Zhou"],"url":"https://arxiv.org/abs/2504.19433"}
{"created":"2025-04-29","title":"Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models","abstract":"This paper focuses on the dynamic optimization of the Retrieval-Augmented Generation (RAG) architecture. It proposes a state-aware dynamic knowledge retrieval mechanism to enhance semantic understanding and knowledge scheduling efficiency in large language models for open-domain question answering and complex generation tasks. The method introduces a multi-level perceptive retrieval vector construction strategy and a differentiable document matching path. These components enable end-to-end joint training and collaborative optimization of the retrieval and generation modules. This effectively addresses the limitations of static RAG structures in context adaptation and knowledge access. Experiments are conducted on the Natural Questions dataset. The proposed structure is thoroughly evaluated across different large models, including GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments from multiple perspectives confirm the significant improvements in BLEU and ROUGE-L scores. The approach also demonstrates stronger robustness and generation consistency in tasks involving semantic ambiguity and multi-document fusion. These results highlight its broad application potential and practical value in building high-quality language generation systems.","authors":["Jacky He","Guiran Liu","Binrong Zhu","Hanlu Zhang","Hongye Zheng","Xiaokai Wang"],"url":"https://arxiv.org/abs/2504.19436"}
{"created":"2025-04-29","title":"JailbreaksOverTime: Detecting Jailbreak Attacks Under Distribution Shift","abstract":"Safety and security remain critical concerns in AI deployment. Despite safety training through reinforcement learning with human feedback (RLHF) [ 32], language models remain vulnerable to jailbreak attacks that bypass safety guardrails. Universal jailbreaks - prefixes that can circumvent alignment for any payload - are particularly concerning. We show empirically that jailbreak detection systems face distribution shift, with detectors trained at one point in time performing poorly against newer exploits. To study this problem, we release JailbreaksOverTime, a comprehensive dataset of timestamped real user interactions containing both benign requests and jailbreak attempts collected over 10 months. We propose a two-pronged method for defenders to detect new jailbreaks and continuously update their detectors. First, we show how to use continuous learning to detect jailbreaks and adapt rapidly to new emerging jailbreaks. While detectors trained at a single point in time eventually fail due to drift, we find that universal jailbreaks evolve slowly enough for self-training to be effective. Retraining our detection model weekly using its own labels - with no new human labels - reduces the false negative rate from 4% to 0.3% at a false positive rate of 0.1%. Second, we introduce an unsupervised active monitoring approach to identify novel jailbreaks. Rather than classifying inputs directly, we recognize jailbreaks by their behavior, specifically, their ability to trigger models to respond to known-harmful prompts. This approach has a higher false negative rate (4.1%) than supervised methods, but it successfully identified some out-of-distribution attacks that were missed by the continuous learning approach.","authors":["Julien Piet","Xiao Huang","Dennis Jacob","Annabella Chow","Maha Alrashed","Geng Zhao","Zhanhao Hu","Chawin Sitawarin","Basel Alomair","David Wagner"],"url":"https://arxiv.org/abs/2504.19440"}
{"created":"2025-04-29","title":"Age of Information Analysis for NOMA-Assisted Grant-Free Transmissions with Randomly Arrived Packets","abstract":"This paper investigates the application of non-orthogonal multiple access (NOMA) to grant-free transmissions to reduce the age of information (AoI) in uplink status update systems, where multiple sources upload their {status updates} to {a common} receiver. Unlike existing studies which {adopted} the idealized generate-at-will (GAW) model, {i.e., a status} update data can be generated and transmitted at any time, this paper utilizes a more practical model {to characterize} the inherent randomness of the generation of the status updating data packets. A rigorous analytical framework is established to precisely evaluate the average AoI achieved by the NOMA-assisted grant-free schemes for both {the} cases with and without retransmission. The impact of the choice of the probability {of transmission} on the average AoI is investigated. Extensive simulation results are provided to validate the accuracy of the developed analysis. It is shown that NOMA-assisted schemes are more superior in reducing AoI{, compared} to orthogonal multiple access (OMA) based schemes. In addition, compared to schemes without retransmission, the AoI performance {of} the schemes with retransmission can {be improved} significantly when the status update generation rate is low or the user density is relatively high.","authors":["Yanshi Sun","Yanglin Ye","Caihong Kai","Zhiguo Ding","Bin Chen"],"url":"https://arxiv.org/abs/2504.19441"}
{"created":"2025-04-29","title":"Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler","abstract":"In this report, we propose Triton-distributed, an extension of existing Triton compiler, to overcome the programming challenges in distributed AI systems. Triton-distributed is the first compiler that supports native overlapping optimizations for distributed AI workloads, providing a good coverage of existing optimizations from different frameworks. First, we integrate communication primitives compliant with the OpenSHMEM standard into the compiler. This enables programmers to utilize these primitives with a higher-level Python programming model. Second, we illustrate how to achieve complex joint optimization of computation, memory access, and communication with the assistance of the compiler. In particular, we show how to use overlapping techniques to hide latency and present our compiler-based programming methods in both single-node and multi-node scenarios. Finally, we showcase the performance of the code generated by our compiler. In a test environment with up to 64 devices, our compiler can fully utilize heterogeneous communication and computation resources to provide effective overlapping and high performance. In many cases, the performance of the generated code can even outperform hand-optimized code. Moreover, the development difficulty and the time cost for development using our compiler are far less than those of low-level programming such as CUDA/C++, which clearly demonstrates significant productivity advantages.","authors":["Size Zheng","Wenlei Bao","Qi Hou","Xuegui Zheng","Jin Fang","Chenhui Huang","Tianqi Li","Haojie Duanmu","Renze Chen","Ruifan Xu","Yifan Guo","Ningxin Zheng","Ziheng Jiang","Xinyi Di","Dongyang Wang","Jianxi Ye","Haibin Lin","Li-Wen Chang","Liqiang Lu","Yun Liang","Jidong Zhai","Xin Liu"],"url":"https://arxiv.org/abs/2504.19442"}
{"created":"2025-04-29","title":"CLIP-KOA: Enhancing Knee Osteoarthritis Diagnosis with Multi-Modal Learning and Symmetry-Aware Loss Functions","abstract":"Knee osteoarthritis (KOA) is a universal chronic musculoskeletal disorders worldwide, making early diagnosis crucial. Currently, the Kellgren and Lawrence (KL) grading system is widely used to assess KOA severity. However, its high inter-observer variability and subjectivity hinder diagnostic consistency. To address these limitations, automated diagnostic techniques using deep learning have been actively explored in recent years. In this study, we propose a CLIP-based framework (CLIP-KOA) to enhance the consistency and reliability of KOA grade prediction. To achieve this, we introduce a learning approach that integrates image and text information and incorporate Symmetry Loss and Consistency Loss to ensure prediction consistency between the original and flipped images. CLIP-KOA achieves state-of-the-art accuracy of 71.86\\% on KOA severity prediction task, and ablation studies show that CLIP-KOA has 2.36\\% improvement in accuracy over the standard CLIP model due to our contribution. This study shows a novel direction for data-driven medical prediction not only to improve reliability of fine-grained diagnosis and but also to explore multimodal methods for medical image analysis. Our code is available at https://github.com/anonymized-link.","authors":["Yejin Jeong","Donghun Lee"],"url":"https://arxiv.org/abs/2504.19443"}
{"created":"2025-04-29","title":"Large Language Models are Qualified Benchmark Builders: Rebuilding Pre-Training Datasets for Advancing Code Intelligence Tasks","abstract":"Pre-trained code models rely heavily on high-quality pre-training data, particularly human-written reference comments that bridge code and natural language. However, these comments often become outdated as software evolves, degrading model performance. Large language models (LLMs) excel at generating high-quality code comments. We investigate whether replacing human-written comments with LLM-generated ones improves pre-training datasets. Since standard metrics cannot assess reference comment quality, we propose two novel reference-free evaluation tasks: code-comment inconsistency detection and semantic code search. Results show that LLM-generated comments are more semantically consistent with code than human-written ones, as confirmed by manual evaluation. Leveraging this finding, we rebuild the CodeSearchNet dataset with LLM-generated comments and re-pre-train CodeT5. Evaluations demonstrate that models trained on LLM-enhanced data outperform those using original human comments in code summarization, generation, and translation tasks. This work validates rebuilding pre-training datasets with LLMs to advance code intelligence, challenging the traditional reliance on human reference comments.","authors":["Kang Yang","Xinjun Mao","Shangwen Wang","Yanlin Wang","Tanghaoran Zhang","Bo Lin","Yihao Qin","Zhang Zhang","Yao Lu","Kamal Al-Sabahi"],"url":"https://arxiv.org/abs/2504.19444"}
{"created":"2025-04-29","title":"Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks","abstract":"Large Language Models (LLMs) are increasingly used in tasks such as psychological text analysis and decision-making in automated workflows. However, their reliability remains a concern due to potential biases inherited from their training process. In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments. In a value statement judgments task and a text sentiment analysis task, we prompted LLMs to simulate human responses and tested both formats across several models, including both open-source and commercial models. Our findings revealed a consistent negative bias: LLMs were more likely to deliver \"negative\" judgments in binary formats compared to continuous ones. Control experiments further revealed that this pattern holds across both tasks. Our results highlight the importance of considering response format when applying LLMs to decision tasks, as small changes in task design can introduce systematic biases.","authors":["Yi-Long Lu","Chunhui Zhang","Wei Wang"],"url":"https://arxiv.org/abs/2504.19445"}
{"created":"2025-04-29","title":"Learning High-dimensional Gaussians from Censored Data","abstract":"We provide efficient algorithms for the problem of distribution learning from high-dimensional Gaussian data where in each sample, some of the variable values are missing. We suppose that the variables are missing not at random (MNAR). The missingness model, denoted by $S(y)$, is the function that maps any point $y$ in $R^d$ to the subsets of its coordinates that are seen. In this work, we assume that it is known. We study the following two settings:","authors":["Arnab Bhattacharyya","Constantinos Daskalakis","Themis Gouleakis","Yuhao Wang"],"url":"https://arxiv.org/abs/2504.19446"}
{"created":"2025-04-29","title":"An End-to-End Framework for Optimizing Foot Trajectory and Force in Dry Adhesion Legged Wall-Climbing Robots","abstract":"Foot trajectory planning for dry adhesion legged climbing robots presents challenges, as the phases of foot detachment, swing, and adhesion significantly influence the adhesion and detachment forces essential for stable climbing. To tackle this, an end-to-end foot trajectory and force optimization framework (FTFOF) is proposed, which optimizes foot adhesion and detachment forces through trajectory adjustments. This framework accepts general foot trajectory constraints and user-defined parameters as input, ultimately producing an optimal single foot trajectory. It integrates three-segment $C^2$ continuous Bezier curves, tailored to various foot structures, enabling the generation of effective climbing trajectories. A dilate-based GRU predictive model establishes the relationship between foot trajectories and the corresponding foot forces. Multi-objective optimization algorithms, combined with a redundancy hierarchical strategy, identify the most suitable foot trajectory for specific tasks, thereby ensuring optimal performance across detachment force, adhesion force and vibration amplitude. Experimental validation on the quadruped climbing robot MST-M3F showed that, compared to commonly used trajectories in existing legged climbing robots, the proposed framework achieved reductions in maximum detachment force by 28 \\%, vibration amplitude by 82 \\%, which ensures the stable climbing of dry adhesion legged climbing robots.","authors":["Jichun Xiao","Jiawei Nie","Lina Hao","Zhi Li"],"url":"https://arxiv.org/abs/2504.19448"}
{"created":"2025-04-29","title":"R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference","abstract":"Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels.","authors":["Zhenyu Zhang","Zechun Liu","Yuandong Tian","Harshit Khaitan","Zhangyang Wang","Steven Li"],"url":"https://arxiv.org/abs/2504.19449"}
{"created":"2025-04-29","title":"Geometry-Informed Neural Operator Transformer","abstract":"Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions for arbitrary geometries. GINOT encodes the surface points cloud of a geometry using a sampling and grouping mechanism combined with an attention mechanism, ensuring invariance to point order and padding while maintaining robustness to variations in point density. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries.","authors":["Qibang Liu","Vincient Zhong","Hadi Meidani","Diab Abueidda","Seid Koric","Philippe Geubelle"],"url":"https://arxiv.org/abs/2504.19452"}
{"created":"2025-04-29","title":"Provably Secure Public-Key Steganography Based on Admissible Encoding","abstract":"The technique of hiding secret messages within seemingly harmless covertext to evade examination by censors with rigorous security proofs is known as provably secure steganography (PSS). PSS evolves from symmetric key steganography to public-key steganography, functioning without the requirement of a pre-shared key and enabling the extension to multi-party covert communication and identity verification mechanisms. Recently, a public-key steganography method based on elliptic curves was proposed, which uses point compression to eliminate the algebraic structure of curve points. However, this method has strict requirements on the curve parameters and is only available on half of the points. To overcome these limitations, this paper proposes a more general elliptic curve public key steganography method based on admissible encoding. By applying the tensor square function to the known well-distributed encoding, we construct admissible encoding, which can create the pseudo-random public-key encryption function. The theoretical analysis and experimental results show that the proposed provable secure public-key steganography method can be deployed on all types of curves and utilize all points on the curve.","authors":["Xin Zhang","Kejiang Chen","Na Zhao","Weiming Zhang","Nenghai Yu"],"url":"https://arxiv.org/abs/2504.19454"}
{"created":"2025-04-29","title":"Masked Language Prompting for Generative Data Augmentation in Few-shot Fashion Style Recognition","abstract":"Constructing dataset for fashion style recognition is challenging due to the inherent subjectivity and ambiguity of style concepts. Recent advances in text-to-image models have facilitated generative data augmentation by synthesizing images from labeled data, yet existing methods based solely on class names or reference captions often fail to balance visual diversity and style consistency. In this work, we propose \\textbf{Masked Language Prompting (MLP)}, a novel prompting strategy that masks selected words in a reference caption and leverages large language models to generate diverse yet semantically coherent completions. This approach preserves the structural semantics of the original caption while introducing attribute-level variations aligned with the intended style, enabling style-consistent and diverse image generation without fine-tuning. Experimental results on the FashionStyle14 dataset demonstrate that our MLP-based augmentation consistently outperforms class-name and caption-based baselines, validating its effectiveness for fashion style recognition under limited supervision.","authors":["Yuki Hirakawa","Ryotaro Shimizu"],"url":"https://arxiv.org/abs/2504.19455"}
{"created":"2025-04-29","title":"FCGHunter: Towards Evaluating Robustness of Graph-Based Android Malware Detection","abstract":"Graph-based detection methods leveraging Function Call Graphs (FCGs) have shown promise for Android malware detection (AMD) due to their semantic insights. However, the deployment of malware detectors in dynamic and hostile environments raises significant concerns about their robustness. While recent approaches evaluate the robustness of FCG-based detectors using adversarial attacks, their effectiveness is constrained by the vast perturbation space, particularly across diverse models and features.","authors":["Shiwen Song","Xiaofei Xie","Ruitao Feng","Qi Guo","Sen Chen"],"url":"https://arxiv.org/abs/2504.19456"}
{"created":"2025-04-29","title":"Towards Long Context Hallucination Detection","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, they are prone to contextual hallucination, generating information that is either unsubstantiated or contradictory to the given context. Although many studies have investigated contextual hallucinations in LLMs, addressing them in long-context inputs remains an open problem. In this work, we take an initial step toward solving this problem by constructing a dataset specifically designed for long-context hallucination detection. Furthermore, we propose a novel architecture that enables pre-trained encoder models, such as BERT, to process long contexts and effectively detect contextual hallucinations through a decomposition and aggregation mechanism. Our experimental results show that the proposed architecture significantly outperforms previous models of similar size as well as LLM-based models across various metrics, while providing substantially faster inference.","authors":["Siyi Liu","Kishaloy Halder","Zheng Qi","Wei Xiao","Nikolaos Pappas","Phu Mon Htut","Neha Anna John","Yassine Benajiba","Dan Roth"],"url":"https://arxiv.org/abs/2504.19457"}
{"created":"2025-04-29","title":"Mitigating Modality Bias in Multi-modal Entity Alignment from a Causal Perspective","abstract":"Multi-Modal Entity Alignment (MMEA) aims to retrieve equivalent entities from different Multi-Modal Knowledge Graphs (MMKGs), a critical information retrieval task. Existing studies have explored various fusion paradigms and consistency constraints to improve the alignment of equivalent entities, while overlooking that the visual modality may not always contribute positively. Empirically, entities with low-similarity images usually generate unsatisfactory performance, highlighting the limitation of overly relying on visual features. We believe the model can be biased toward the visual modality, leading to a shortcut image-matching task. To address this, we propose a counterfactual debiasing framework for MMEA, termed CDMEA, which investigates visual modality bias from a causal perspective. Our approach aims to leverage both visual and graph modalities to enhance MMEA while suppressing the direct causal effect of the visual modality on model predictions. By estimating the Total Effect (TE) of both modalities and excluding the Natural Direct Effect (NDE) of the visual modality, we ensure that the model predicts based on the Total Indirect Effect (TIE), effectively utilizing both modalities and reducing visual modality bias. Extensive experiments on 9 benchmark datasets show that CDMEA outperforms 14 state-of-the-art methods, especially in low-similarity, high-noise, and low-resource data scenarios.","authors":["Taoyu Su","Jiawei Sheng","Duohe Ma","Xiaodong Li","Juwei Yue","Mengxiao Song","Yingkai Tang","Tingwen Liu"],"url":"https://arxiv.org/abs/2504.19458"}
{"created":"2025-04-29","title":"Do Automatic Comment Generation Techniques Fall Short? Exploring the Influence of Method Dependencies on Code Understanding","abstract":"Method-level comments are critical for improving code comprehension and supporting software maintenance. With advancements in large language models (LLMs), automated comment generation has become a major research focus. However, existing approaches often overlook method dependencies, where one method relies on or calls others, affecting comment quality and code understandability. This study investigates the prevalence and impact of dependent methods in software projects and introduces a dependency-aware approach for method-level comment generation. Analyzing a dataset of 10 popular Java GitHub projects, we found that dependent methods account for 69.25% of all methods and exhibit higher engagement and change proneness compared to independent methods. Across 448K dependent and 199K independent methods, we observed that state-of-the-art fine-tuned models (e.g., CodeT5+, CodeBERT) struggle to generate comprehensive comments for dependent methods, a trend also reflected in LLM-based approaches like ASAP. To address this, we propose HelpCOM, a novel dependency-aware technique that incorporates helper method information to improve comment clarity, comprehensiveness, and relevance. Experiments show that HelpCOM outperforms baseline methods by 5.6% to 50.4% across syntactic (e.g., BLEU), semantic (e.g., SentenceBERT), and LLM-based evaluation metrics. A survey of 156 software practitioners further confirms that HelpCOM significantly improves the comprehensibility of code involving dependent methods, highlighting its potential to enhance documentation, maintainability, and developer productivity in large-scale systems.","authors":["Md Mustakim Billah","Md Shamimur Rahman","Banani Roy"],"url":"https://arxiv.org/abs/2504.19459"}
{"created":"2025-04-29","title":"A Real-Time Gesture-Based Control Framework","abstract":"We introduce a real-time, human-in-the-loop gesture control framework that can dynamically adapt audio and music based on human movement by analyzing live video input. By creating a responsive connection between visual and auditory stimuli, this system enables dancers and performers to not only respond to music but also influence it through their movements. Designed for live performances, interactive installations, and personal use, it offers an immersive experience where users can shape the music in real time.","authors":["Mahya Khazaei","Ali Bahrani","George Tzanetakis"],"url":"https://arxiv.org/abs/2504.19460"}
{"created":"2025-04-29","title":"The Role of Generative AI in Strengthening Secure Software Coding Practices: A Systematic Perspective","abstract":"As software security threats continue to evolve, the demand for innovative ways of securing coding has tremendously grown. The integration of Generative AI (GenAI) into software development holds significant potential for improving secure coding practices. This paper aims at systematically studying the impact of GenAI in enhancing secure coding practices from improving software security, setting forth its potential benefits, challenges, and implications. To outline the contribution of AI driven code generation tools, we analyze via a structured review of recent literature, application to the industry, and empirical studies on how these tools help to mitigate security risks, comply with the secure coding standards, and make software development efficient. We hope that our findings will benefit researchers, software engineers and cybersecurity professionals alike in integrating GenAI into a secure development workflow without losing the advantages GenAI provides. Finally, the state of the art advances and future directions of AI assisted in secure software engineering discussed in this study can contribute to the ongoing discourse on AI assisted in secure software engineering.","authors":["Hathal S. Alwageed","Rafiq Ahmad Khan"],"url":"https://arxiv.org/abs/2504.19461"}
{"created":"2025-04-29","title":"Bearing-Only Tracking and Circumnavigation of a Fast Time-Varied Velocity Target Utilising an LSTM","abstract":"Bearing-only tracking, localisation, and circumnavigation is a problem in which a single or a group of agents attempts to track a target while circumnavigating it at a fixed distance using only bearing measurements. While previous studies have addressed scenarios involving stationary targets or those moving with an unknown constant velocity, the challenge of accurately tracking a target moving with a time-varying velocity remains open. This paper presents an approach utilising a Long Short-Term Memory (LSTM) based estimator for predicting the target's position and velocity. We also introduce a corresponding control strategy. When evaluated against previously proposed estimation and circumnavigation approaches, our approach demonstrates significantly lower control and estimation errors across various time-varying velocity scenarios. Additionally, we illustrate the effectiveness of the proposed method in tracking targets with a double integrator nonholonomic system dynamics that mimic real-world systems.","authors":["Mitchell Torok","Mohammad Deghat","Yang Song"],"url":"https://arxiv.org/abs/2504.19463"}
{"created":"2025-04-29","title":"BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text","abstract":"Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding.","authors":["Jiageng Wu","Bowen Gu","Ren Zhou","Kevin Xie","Doug Snyder","Yixing Jiang","Valentina Carducci","Richard Wyss","Rishi J Desai","Emily Alsentzer","Leo Anthony Celi","Adam Rodman","Sebastian Schneeweiss","Jonathan H. Chen","Santiago Romero-Brufau","Kueiyu Joshua Lin","Jie Yang"],"url":"https://arxiv.org/abs/2504.19467"}
{"created":"2025-04-29","title":"Conflicts in Texts: Data, Implications and Challenges","abstract":"As NLP models become increasingly integrated into real-world applications, it becomes clear that there is a need to address the fact that models often rely on and generate conflicting information. Conflicts could reflect the complexity of situations, changes that need to be explained and dealt with, difficulties in data annotation, and mistakes in generated outputs. In all cases, disregarding the conflicts in data could result in undesired behaviors of models and undermine NLP models' reliability and trustworthiness. This survey categorizes these conflicts into three key areas: (1) natural texts on the web, where factual inconsistencies, subjective biases, and multiple perspectives introduce contradictions; (2) human-annotated data, where annotator disagreements, mistakes, and societal biases impact model training; and (3) model interactions, where hallucinations and knowledge conflicts emerge during deployment. While prior work has addressed some of these conflicts in isolation, we unify them under the broader concept of conflicting information, analyze their implications, and discuss mitigation strategies. We highlight key challenges and future directions for developing conflict-aware NLP systems that can reason over and reconcile conflicting information more effectively.","authors":["Siyi Liu","Dan Roth"],"url":"https://arxiv.org/abs/2504.19472"}
{"created":"2025-04-29","title":"Stability Enhancement in Reinforcement Learning via Adaptive Control Lyapunov Function","abstract":"Reinforcement Learning (RL) has shown promise in control tasks but faces significant challenges in real-world applications, primarily due to the absence of safety guarantees during the learning process. Existing methods often struggle with ensuring safe exploration, leading to potential system failures and restricting applications primarily to simulated environments. Traditional approaches such as reward shaping and constrained policy optimization can fail to guarantee safety during initial learning stages, while model-based methods using Control Lyapunov Functions (CLFs) or Control Barrier Functions (CBFs) may hinder efficient exploration and performance. To address these limitations, this paper introduces Soft Actor-Critic with Control Lyapunov Function (SAC-CLF), a framework that enhances stability and safety through three key innovations: (1) a task-specific CLF design method for safe and optimal performance; (2) dynamic adjustment of constraints to maintain robustness under unmodeled dynamics; and (3) improved control input smoothness while ensuring safety. Experimental results on a classical nonlinear system and satellite attitude control demonstrate the effectiveness of SAC-CLF in overcoming the shortcomings of existing methods.","authors":["Donghe Chen","Han Wang","Lin Cheng","Shengping Gong"],"url":"https://arxiv.org/abs/2504.19473"}
{"created":"2025-04-29","title":"Prisma: An Open Source Toolkit for Mechanistic Interpretability in Vision and Video","abstract":"Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.","authors":["Sonia Joseph","Praneet Suresh","Lorenz Hufe","Edward Stevinson","Robert Graham","Yash Vadi","Danilo Bzdok","Sebastian Lapuschkin","Lee Sharkey","Blake Aaron Richards"],"url":"https://arxiv.org/abs/2504.19475"}
{"created":"2025-04-29","title":"CasaGPT: Cuboid Arrangement and Scene Assembly for Interior Design","abstract":"We present a novel approach for indoor scene synthesis, which learns to arrange decomposed cuboid primitives to represent 3D objects within a scene. Unlike conventional methods that use bounding boxes to determine the placement and scale of 3D objects, our approach leverages cuboids as a straightforward yet highly effective alternative for modeling objects. This allows for compact scene generation while minimizing object intersections. Our approach, coined CasaGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive model to sequentially arrange cuboids, producing physically plausible scenes. By applying rejection sampling during the fine-tuning stage to filter out scenes with object collisions, our model further reduces intersections and enhances scene quality. Additionally, we introduce a refined dataset, 3DFRONT-NC, which eliminates significant noise presented in the original dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our dataset demonstrate that our approach consistently outperforms the state-of-the-art methods, enhancing the realism of generated scenes, and providing a promising direction for 3D scene synthesis.","authors":["Weitao Feng","Hang Zhou","Jing Liao","Li Cheng","Wenbo Zhou"],"url":"https://arxiv.org/abs/2504.19478"}
{"created":"2025-04-29","title":"An Automated Reinforcement Learning Reward Design Framework with Large Language Model for Cooperative Platoon Coordination","abstract":"Reinforcement Learning (RL) has demonstrated excellent decision-making potential in platoon coordination problems. However, due to the variability of coordination goals, the complexity of the decision problem, and the time-consumption of trial-and-error in manual design, finding a well performance reward function to guide RL training to solve complex platoon coordination problems remains challenging. In this paper, we formally define the Platoon Coordination Reward Design Problem (PCRDP), extending the RL-based cooperative platoon coordination problem to incorporate automated reward function generation. To address PCRDP, we propose a Large Language Model (LLM)-based Platoon coordination Reward Design (PCRD) framework, which systematically automates reward function discovery through LLM-driven initialization and iterative optimization. In this method, LLM first initializes reward functions based on environment code and task requirements with an Analysis and Initial Reward (AIR) module, and then iteratively optimizes them based on training feedback with an evolutionary module. The AIR module guides LLM to deepen their understanding of code and tasks through a chain of thought, effectively mitigating hallucination risks in code generation. The evolutionary module fine-tunes and reconstructs the reward function, achieving a balance between exploration diversity and convergence stability for training. To validate our approach, we establish six challenging coordination scenarios with varying complexity levels within the Yangtze River Delta transportation network simulation. Comparative experimental results demonstrate that RL agents utilizing PCRD-generated reward functions consistently outperform human-engineered reward functions, achieving an average of 10\\% higher performance metrics in all scenarios.","authors":["Dixiao Wei","Peng Yi","Jinlong Lei","Yiguang Hong","Yuchuan Du"],"url":"https://arxiv.org/abs/2504.19480"}
{"created":"2025-04-29","title":"Preasymptotic error estimates of higher-order EEM for the time-harmonic Maxwell equations with large wave number","abstract":"The time-harmonic Maxwell equations with impedance boundary condition and large wave number are discretized using the second-type N\\'{e}d\\'{e}lec's edge element method (EEM). Preasymptotic error bounds are derived, showing that, under the mesh condition $\\kappa^{2p+1}h^{2p}$ being sufficiently small, the error of the EEM of order $p$ in the energy norm is bounded by $\\mathcal{O}\\big(\\kappa^{p}h^p + \\kappa^{2p+1}h^{2p}\\big)$, while the error in the $\\kappa$-scaled $\\boldsymbol{L}^2$ norm is bounded by $\\mathcal{O}\\big((\\kappa h)^{p+1} + \\kappa^{2p+1} h^{2p}\\big)$. Here, $\\kappa$ is the wave number and $h$ is the mesh size. Numerical tests are provided to illustrate our theoretical results.","authors":["Shuaishuai Lu","Haijun Wu"],"url":"https://arxiv.org/abs/2504.19481"}
{"created":"2025-04-29","title":"Dynamic r-index: An Updatable Self-Index for Highly Repetitive Strings","abstract":"A self-index is a compressed data structure that supports locate queries-reporting all positions where a given pattern occurs in a string. While many self-indexes have been proposed, developing dynamically updatable ones supporting string insertions and deletions remains a challenge. The r-index (Gagie et al., SODA'18) is a representative static self-index based on the run-length Burrows-Wheeler transform (RLBWT), designed for highly repetitive strings - those with many repeated substrings. We present the dynamic r-index, an extension of the r-index that supports locate queries in $\\mathcal{O}((m + \\occ) \\log n)$ time using $\\mathcal{O}(r)$ words, where $n$ is the length of the string $T$, $m$ is the pattern length, $\\occ$ is the number of occurrences, and $r$ is the number of runs in the RLBWT of $T$. It supports string insertions and deletions in $\\mathcal{O}((m + L_{\\max}) \\log n)$ time, where $L_{\\max}$ is the maximum value in the LCP array of $T$. The average running time is $\\mathcal{O}((m + L_{\\avg}) \\log n)$, where $L_{\\avg}$ is the average LCP value. We experimentally evaluated the dynamic r-index on various highly repetitive strings and demonstrated its practicality.","authors":["Takaaki Nishimoto","Yasuo Tabei"],"url":"https://arxiv.org/abs/2504.19482"}
{"created":"2025-04-29","title":"Improving Reasoning Performance in Large Language Models via Representation Engineering","abstract":"Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. Whether reasoning in LLMs should be understood to be inherently different is, however, widely debated. We propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task. We publish the code for deriving control vectors and analyzing model representations. The method allows us to improve performance on reasoning benchmarks and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on an inductive, a deductive and mathematical reasoning task. We show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the model's typical state when correctly solving a task. Our results suggest that reasoning performance can be modulated in the same manner as other information-processing tasks performed by LLMs and demonstrate that we are capable of improving performance on specific tasks via a simple intervention on the residual stream with no additional training.","authors":["Bertram H{\\o}jer","Oliver Jarvis","Stefan Heinrich"],"url":"https://arxiv.org/abs/2504.19483"}
{"created":"2025-04-29","title":"Topological derivative for a fast identification of short, linear perfectly conducting cracks with inaccurate background information","abstract":"In this study, we consider a topological derivative-based imaging technique for the fast identification of short, linear perfectly conducting cracks completely embedded in a two-dimensional homogeneous domain with smooth boundary. Unlike conventional approaches, we assume that the background permittivity and permeability are unknown due to their dependence on frequency and temperature, and we propose a normalized imaging function to localize cracks. Despite inaccuracies in background parameters, application of the proposed imaging function enables to recognize the existence of crack but it is still impossible to identify accurate crack locations. Furthermore, the shift in crack localization of imaging results is significantly influenced by the applied background parameters. In order to theoretically explain this phenomenon, we show that the imaging function can be expressed in terms of the zero-order Bessel function of the first kind, the crack lengths, and the applied inaccurate background wavenumber corresponding to the applied inaccurate background permittivity and permeability. Various numerical simulations results with synthetic data polluted by random noise validate the theoretical results.","authors":["Won-Kwang Park"],"url":"https://arxiv.org/abs/2504.19485"}
{"created":"2025-04-29","title":"The Cost of Performance: Breaking ThreadX with Kernel Object Masquerading Attacks","abstract":"Microcontroller-based IoT devices often use embedded real-time operating systems (RTOSs). Vulnerabilities in these embedded RTOSs can lead to compromises of those IoT devices. Despite the significance of security protections, the absence of standardized security guidelines results in various levels of security risk across RTOS implementations. Our initial analysis reveals that popular RTOSs such as FreeRTOS lack essential security protections. While Zephyr OS and ThreadX are designed and implemented with essential security protections, our closer examination uncovers significant differences in their implementations of system call parameter sanitization. We identify a performance optimization practice in ThreadX that introduces security vulnerabilities, allowing for the circumvention of parameter sanitization processes. Leveraging this insight, we introduce a novel attack named the Kernel Object Masquerading (KOM) Attack (as the attacker needs to manipulate one or multiple kernel objects through carefully selected system calls to launch the attack), demonstrating how attackers can exploit these vulnerabilities to access sensitive fields within kernel objects, potentially leading to unauthorized data manipulation, privilege escalation, or system compromise. We introduce an automated approach involving under-constrained symbolic execution to identify the KOM attacks and to understand the implications. Experimental results demonstrate the feasibility of KOM attacks on ThreadX-powered platforms. We reported our findings to the vendors, who recognized the vulnerabilities, with Amazon and Microsoft acknowledging our contribution on their websites.","authors":["Xinhui Shao","Zhen Ling","Yue Zhang","Huaiyu Yan","Yumeng Wei","Lan Luo","Zixia Liu","Junzhou Luo","Xinwen Fu"],"url":"https://arxiv.org/abs/2504.19486"}
{"created":"2025-04-29","title":"Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies","abstract":"The evolution of cooperation has been extensively studied using abstract mathematical models and simulations. Recent advances in Large Language Models (LLM) and the rise of LLM agents have demonstrated their ability to perform social reasoning, thus providing an opportunity to test the emergence of norms in more realistic agent-based simulations with human-like reasoning using natural language. In this research, we investigate whether the cooperation dynamics presented in Boyd and Richerson's model persist in a more realistic simulation of the diner's dilemma using LLM agents compared to the abstract mathematical nature in the work of Boyd and Richerson. Our findings indicate that agents follow the strategies defined in the Boyd and Richerson model, and explicit punishment mechanisms drive norm emergence, reinforcing cooperative behaviour even when the agent strategy configuration varies. Our results suggest that LLM-based Multi-Agent System simulations, in fact, can replicate the evolution of cooperation predicted by the traditional mathematical models. Moreover, our simulations extend beyond the mathematical models by integrating natural language-driven reasoning and a pairwise imitation method for strategy adoption, making them a more realistic testbed for cooperative behaviour in MASs.","authors":["Kavindu Warnakulasuriya","Prabhash Dissanayake","Navindu De Silva","Stephen Cranefield","Bastin Tony Roy Savarimuthu","Surangika Ranathunga","Nisansa de Silva"],"url":"https://arxiv.org/abs/2504.19487"}
{"created":"2025-04-29","title":"How Cohesive Are Community Search Results on Online Social Networks?: An Experimental Evaluation","abstract":"Recently, numerous community search methods for large graphs have been proposed, at the core of which is defining and measuring cohesion. This paper experimentally evaluates the effectiveness of these community search algorithms w.r.t. cohesiveness in the context of online social networks. Social communities are formed and developed under the influence of group cohesion theory, which has been extensively studied in social psychology. However, current generic methods typically measure cohesiveness using structural or attribute-based approaches and overlook domain-specific concepts such as group cohesion. We introduce five novel psychology-informed cohesiveness measures, based on the concept of group cohesion from social psychology, and propose a novel framework called CHASE for evaluating eight representative CS algorithms w.r.t.these measures on online social networks. Our analysis reveals that there is no clear correlation between structural and psychological cohesiveness, and no algorithm effectively identifies psychologically cohesive communities in online social networks. This study provides new insights that could guide the development of future community search methods.","authors":["Yining Zhao","Sourav S Bhowmick","Nastassja L. Fischer","SH Annabel Chen"],"url":"https://arxiv.org/abs/2504.19489"}
{"created":"2025-04-29","title":"Adjusted Objects: An Efficient and Principled Approach to Scalable Programming (Extended Version)","abstract":"Parallel programs require software support to coordinate access to shared data. For this purpose, modern programming languages provide strongly-consistent shared objects. To account for their many usages, these objects offer a large API.However, in practice, each program calls only a tiny fraction of the interface. Leveraging such an observation, we propose to tailor a shared object for a specific usage. We call this principle adjusted objects. Adjusted objects already exist in the wild. This paper provides their first systematic study. We explain how everyday programmers already adjust common shared objects (such as queues, maps, and counters) for better performance. We present the formal foundations of adjusted objects using a new tool to characterize scalability, the indistinguishability graph. Leveraging this study, we introduce a library named DEGO to inject adjusted objects in a Java program. In micro-benchmarks, objects from the DEGO library improve the performance of standard JDK shared objects by up to two orders of magnitude. We also evaluate DEGO with a Retwis-like benchmark modeled after a social network application. On a modern server-class machine, DEGO boosts by up to 1.7x the performance of the benchmark.","authors":["Boubacar Kane","Pierre Sutra"],"url":"https://arxiv.org/abs/2504.19495"}
{"created":"2025-04-29","title":"DISCO: learning to DISCover an evolution Operator for multi-physics-agnostic prediction","abstract":"We address the problem of predicting the next state of a dynamical system governed by unknown temporal partial differential equations (PDEs) using only a short trajectory. While standard transformers provide a natural black-box solution to this task, the presence of a well-structured evolution operator in the data suggests a more tailored and efficient approach. Specifically, when the PDE is fully known, classical numerical solvers can evolve the state accurately with only a few parameters. Building on this observation, we introduce DISCO, a model that uses a large hypernetwork to process a short trajectory and generate the parameters of a much smaller operator network, which then predicts the next state through time integration. Our framework decouples dynamics estimation (i.e., DISCovering an evolution operator from a short trajectory) from state prediction (i.e., evolving this operator). Experiments show that pretraining our model on diverse physics datasets achieves state-of-the-art performance while requiring significantly fewer epochs. Moreover, it generalizes well and remains competitive when fine-tuned on downstream tasks.","authors":["Rudy Morel","Jiequn Han","Edouard Oyallon"],"url":"https://arxiv.org/abs/2504.19496"}
{"created":"2025-04-29","title":"Negative Imaginary Neural ODEs: Learning to Control Mechanical Systems with Stability Guarantees","abstract":"We propose a neural control method to provide guaranteed stabilization for mechanical systems using a novel negative imaginary neural ordinary differential equation (NINODE) controller. Specifically, we employ neural networks with desired properties as state-space function matrices within a Hamiltonian framework to ensure the system possesses the NI property. This NINODE system can serve as a controller that asymptotically stabilizes an NI plant under certain conditions. For mechanical plants with colocated force actuators and position sensors, we demonstrate that all the conditions required for stability can be translated into regularity constraints on the neural networks used in the controller. We illustrate the utility, effectiveness, and stability guarantees of the NINODE controller through an example involving a nonlinear mass-spring system.","authors":["Kanghong Shi","Ruigang Wang","Ian R. Manchester"],"url":"https://arxiv.org/abs/2504.19497"}
{"created":"2025-04-29","title":"Motion Generation for Food Topping Challenge 2024: Serving Salmon Roe Bowl and Picking Fried Chicken","abstract":"Although robots have been introduced in many industries, food production robots are yet to be widely employed because the food industry requires not only delicate movements to handle food but also complex movements that adapt to the environment. Force control is important for handling delicate objects such as food. In addition, achieving complex movements is possible by making robot motions based on human teachings. Four-channel bilateral control is proposed, which enables the simultaneous teaching of position and force information. Moreover, methods have been developed to reproduce motions obtained through human teachings and generate adaptive motions using learning. We demonstrated the effectiveness of these methods for food handling tasks in the Food Topping Challenge at the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024). For the task of serving salmon roe on rice, we achieved the best performance because of the high reproducibility and quick motion of the proposed method. Further, for the task of picking fried chicken, we successfully picked the most pieces of fried chicken among all participating teams. This paper describes the implementation and performance of these methods.","authors":["Koki Inami","Masashi Konosu","Koki Yamane","Nozomu Masuya","Yunhan Li","Yu-Han Shu","Hiroshi Sato","Shinnosuke Homma","Sho Sakaino"],"url":"https://arxiv.org/abs/2504.19498"}
{"created":"2025-04-29","title":"Graph Reinforcement Learning for QoS-Aware Load Balancing in Open Radio Access Networks","abstract":"Next-generation wireless cellular networks are expected to provide unparalleled Quality-of-Service (QoS) for emerging wireless applications, necessitating strict performance guarantees, e.g., in terms of link-level data rates. A critical challenge in meeting these QoS requirements is the prevention of cell congestion, which involves balancing the load to ensure sufficient radio resources are available for each cell to serve its designated User Equipments (UEs). In this work, a novel QoS-aware Load Balancing (LB) approach is developed to optimize the performance of Guaranteed Bit Rate (GBR) and Best Effort (BE) traffic in a multi-band Open Radio Access Network (O-RAN) under QoS and resource constraints. The proposed solution builds on Graph Reinforcement Learning (GRL), a powerful framework at the intersection of Graph Neural Network (GNN) and RL. The QoS-aware LB is modeled as a Markov Decision Process, with states represented as graphs. QoS consideration are integrated into both state representations and reward signal design. The LB agent is then trained using an off-policy dueling Deep Q Network (DQN) that leverages a GNN-based architecture. This design ensures the LB policy is invariant to the ordering of nodes (UE or cell), flexible in handling various network sizes, and capable of accounting for spatial node dependencies in LB decisions. Performance of the GRL-based solution is compared with two baseline methods. Results show substantial performance gains, including a $53\\%$ reduction in QoS violations and a fourfold increase in the 5th percentile rate for BE traffic.","authors":["Omid Semiari","Hosein Nikopour","Shilpa Talwar"],"url":"https://arxiv.org/abs/2504.19499"}
{"created":"2025-04-29","title":"Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding","abstract":"Open-vocabulary 3D scene understanding is pivotal for enhancing physical intelligence, as it enables embodied agents to interpret and interact dynamically within real-world environments. This paper introduces MPEC, a novel Masked Point-Entity Contrastive learning method for open-vocabulary 3D semantic segmentation that leverages both 3D entity-language alignment and point-entity consistency across different point cloud views to foster entity-specific feature representations. Our method improves semantic discrimination and enhances the differentiation of unique instances, achieving state-of-the-art results on ScanNet for open-vocabulary 3D semantic segmentation and demonstrating superior zero-shot scene understanding capabilities. Extensive fine-tuning experiments on 8 datasets, spanning from low-level perception to high-level reasoning tasks, showcase the potential of learned 3D features, driving consistent performance gains across varied 3D scene understanding tasks. Project website: https://mpec-3d.github.io/","authors":["Yan Wang","Baoxiong Jia","Ziyu Zhu","Siyuan Huang"],"url":"https://arxiv.org/abs/2504.19500"}
{"created":"2025-04-29","title":"Simultaneous Pick and Place Detection by Combining SE(3) Diffusion Models with Differential Kinematics","abstract":"Grasp detection methods typically target the detection of a set of free-floating hand poses that can grasp the object. However, not all of the detected grasp poses are executable due to physical constraints. Even though it is straightforward to filter invalid grasp poses in the post-process, such a two-staged approach is computationally inefficient, especially when the constraint is hard. In this work, we propose an approach to take the following two constraints into account during the grasp detection stage, namely, (i) the picked object must be able to be placed with a predefined configuration without in-hand manipulation (ii) it must be reachable by the robot under the joint limit and collision-avoidance constraints for both pick and place cases. Our key idea is to train an SE(3) grasp diffusion network to estimate the noise in the form of spatial velocity, and constrain the denoising process by a multi-target differential inverse kinematics with an inequality constraint, so that the states are guaranteed to be reachable and placement can be performed without collision. In addition to an improved success ratio, we experimentally confirmed that our approach is more efficient and consistent in computation time compared to a naive two-stage approach.","authors":["Tianyi Ko","Takuya Ikeda","Koichi Nishiwaki"],"url":"https://arxiv.org/abs/2504.19502"}
{"created":"2025-04-29","title":"SynergyAmodal: Deocclude Anything with Text Control","abstract":"Image deocclusion (or amodal completion) aims to recover the invisible regions (\\ie, shape and appearance) of occluded instances in images. Despite recent advances, the scarcity of high-quality data that balances diversity, plausibility, and fidelity remains a major obstacle. To address this challenge, we identify three critical elements: leveraging in-the-wild image data for diversity, incorporating human expertise for plausibility, and utilizing generative priors for fidelity. We propose SynergyAmodal, a novel framework for co-synthesizing in-the-wild amodal datasets with comprehensive shape and appearance annotations, which integrates these elements through a tripartite data-human-model collaboration. First, we design an occlusion-grounded self-supervised learning algorithm to harness the diversity of in-the-wild image data, fine-tuning an inpainting diffusion model into a partial completion diffusion model. Second, we establish a co-synthesis pipeline to iteratively filter, refine, select, and annotate the initial deocclusion results of the partial completion diffusion model, ensuring plausibility and fidelity through human expert guidance and prior model constraints. This pipeline generates a high-quality paired amodal dataset with extensive category and scale diversity, comprising approximately 16K pairs. Finally, we train a full completion diffusion model on the synthesized dataset, incorporating text prompts as conditioning signals. Extensive experiments demonstrate the effectiveness of our framework in achieving zero-shot generalization and textual controllability. Our code, dataset, and models will be made publicly available at https://github.com/imlixinyang/SynergyAmodal.","authors":["Xinyang Li","Chengjie Yi","Jiawei Lai","Mingbao Lin","Yansong Qu","Shengchuan Zhang","Liujuan Cao"],"url":"https://arxiv.org/abs/2504.19506"}
{"created":"2025-04-29","title":"\\textit{From Freshness to Effectiveness}: Goal-Oriented Sampling for Remote Decision Making","abstract":"Data freshness, measured by Age of Information (AoI), is highly relevant in networked applications such as Vehicle to Everything (V2X), smart health systems, and Industrial Internet of Things (IIoT). Yet, freshness alone does not equate to informativeness. In decision-critical settings, some stale data may prove more valuable than fresh updates. To explore this nuance, we move beyond AoI-centric policies and investigate how data staleness impacts decision-making under data-staleness-induced uncertainty. We pose a central question: What is the value of information, when freshness fades, and only its power to shape remote decisions remains? To capture this endured value, we propose AR-MDP, an Age-aware Remote Markov Decision Process framework, which co-designs optimal sampling and remote decision-making under a sampling frequency constraint and random delay. To efficiently solve this problem, we design a new two-stage hierarchical algorithm namely Quick Bellman-Linear-Program (QuickBLP), where the first stage involves solving the Dinkelbach root of a Bellman variant and the second stage involves solving a streamlined linear program (LP). For the tricky first stage, we propose a new One-layer Primal-Dinkelbach Synchronous Iteration (OnePDSI) method, which overcomes the re-convergence and non-expansive divergence present in existing per-sample multi-layer algorithms. Through rigorous convergence analysis of our proposed algorithms, we establish that the worst-case optimality gap in OnePDSI exhibits exponential decay with respect to iteration $K$ at a rate of $\\mathcal{O}(\\frac{1}{R^K})$. Through sensitivity analysis, we derive a threshold for the sampling frequency, beyond which additional sampling does not yield further gains in decision-making. Simulation results validate our analyses.","authors":["Aimin Li","Shaohua Wu","Gary C. F. Lee","Sumei Sun"],"url":"https://arxiv.org/abs/2504.19507"}
{"created":"2025-04-29","title":"FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding","abstract":"Figure skating, known as the \"Art on Ice,\" is among the most artistic sports, challenging to understand due to its blend of technical elements (like jumps and spins) and overall artistic expression. Existing figure skating datasets mainly focus on single tasks, such as action recognition or scoring, lacking comprehensive annotations for both technical and artistic evaluation. Current sports research is largely centered on ball games, with limited relevance to artistic sports like figure skating. To address this, we introduce FSAnno, a large-scale dataset advancing artistic sports understanding through figure skating. FSAnno includes an open-access training and test dataset, alongside a benchmark dataset, FSBench, for fair model evaluation. FSBench consists of FSBench-Text, with multiple-choice questions and explanations, and FSBench-Motion, containing multimodal data and Question and Answer (QA) pairs, supporting tasks from technical analysis to performance commentary. Initial tests on FSBench reveal significant limitations in existing models' understanding of artistic sports. We hope FSBench will become a key tool for evaluating and enhancing model comprehension of figure skating.","authors":["Rong Gao","Xin Liu","Zhuozhao Hu","Bohao Xing","Baiqiang Xia","Zitong Yu","Heikki K\\\"alvi\\\"ainen"],"url":"https://arxiv.org/abs/2504.19514"}
{"created":"2025-04-29","title":"Bullet: Boosting GPU Utilization for LLM Serving via Dynamic Spatial-Temporal Orchestration","abstract":"Modern LLM serving systems confront inefficient GPU utilization due to the fundamental mismatch between compute-intensive prefill and memory-bound decode phases. While current practices attempt to address this by organizing these phases into hybrid batches, such solutions create an inefficient tradeoff that sacrifices either throughput or latency, leaving substantial GPU resources underutilized. We identify two key root causes: 1) the prefill phase suffers from suboptimal compute utilization due to wave quantization and attention bottlenecks. 2) hybrid batches disproportionately prioritize latency over throughput, resulting in wasted compute and memory bandwidth. To mitigate the issues, we present Bullet, a novel spatial-temporal orchestration system that eliminates these inefficiencies through precise phase coordination. Bullet enables concurrent execution of prefill and decode phases, while dynamically provisioning GPU resources using real-time performance modeling. By integrating SLO-aware scheduling and adaptive resource allocation, Bullet maximizes utilization without compromising latency targets. Experimental evaluations on real-world workloads demonstrate that Bullet delivers 1.26x average throughput gains (up to 1.55x) over state-of-the-arts, while consistently meeting latency constraints.","authors":["Zejia Lin","Hongxin Xu","Guanyi Chen","Xianwei Zhang","Yutong Lu"],"url":"https://arxiv.org/abs/2504.19516"}
{"created":"2025-04-29","title":"Discrete-time Two-Layered Forgetting RLS Identification under Finite Excitation","abstract":"In recent years, adaptive identification methods that can achieve the true value convergence of parameters without requiring persistent excitation (PE) have been widely studied, and concurrent learning has been intensively studied. However, the parameter convergence rate is limited for the gradient-based method owing to small parameter update gain, and even the introduction of forgetting factors does not work sufficiently. To address this problem, this study proposes a novel discrete-time recursive least squares method under finite excitation (FE) conditions using two forgetting factors (inner and outer) and an augmented regressor matrix comprising a sum of regressor vectors. The proposed method ensures the PE condition of the augmented regressor matrix under FE conditions of the regressor vector and allows the properly design of the forgetting factor without estimator windup and/or destabilization of the system. Numerical simulations demonstrate its effectiveness by comparing it with several conventional methods.","authors":["Satoshi Tsuruhara","Kazuhisa Ito"],"url":"https://arxiv.org/abs/2504.19518"}
{"created":"2025-04-29","title":"FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation","abstract":"Generative models have achieved remarkable success across various applications, driving the demand for multi-GPU computing. Inter-GPU communication becomes a bottleneck in multi-GPU computing systems, particularly on consumer-grade GPUs. By exploiting concurrent hardware execution, overlapping computation and communication latency is an effective technique for mitigating the communication overhead. We identify that an efficient and adaptable overlapping design should satisfy (1) tile-wise overlapping to maximize the overlapping opportunity, (2) interference-free computation to maintain the original computational performance, and (3) communication agnosticism to reduce the development burden against varying communication primitives. Nevertheless, current designs fail to simultaneously optimize for all of those features.","authors":["Ke Hong","Xiuhong Li","Minxu Liu","Qiuli Mao","Tianqi Wu","Zixiao Huang","Lufang Chen","Zhong Wang","Yichong Zhang","Zhenhua Zhu","Guohao Dai","Yu Wang"],"url":"https://arxiv.org/abs/2504.19519"}
{"created":"2025-04-29","title":"Security Steerability is All You Need","abstract":"The adoption of Generative AI (GenAI) in various applications inevitably comes with expanding the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate these security threats in GenAI by developing metrics and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. manipulating the LLM to generate forbidden content), there is significantly less discussion on application-level security and how to mitigate it.","authors":["Itay Hazan","Idan Habler","Ron Bitton","Itsik Mantin"],"url":"https://arxiv.org/abs/2504.19521"}
{"created":"2025-04-29","title":"LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning","abstract":"Industrial Anomaly Detection (IAD) is critical for ensuring product quality by identifying defects. Traditional methods such as feature embedding and reconstruction-based approaches require large datasets and struggle with scalability. Existing vision-language models (VLMs) and Multimodal Large Language Models (MLLMs) address some limitations but rely on mask annotations, leading to high implementation costs and false positives. Additionally, industrial datasets like MVTec-AD and VisA suffer from severe class imbalance, with defect samples constituting only 23.8% and 11.1% of total data respectively. To address these challenges, we propose a reward function that dynamically prioritizes rare defect patterns during training to handle class imbalance. We also introduce a mask-free reasoning framework using Chain of Thought (CoT) and Group Relative Policy Optimization (GRPO) mechanisms, enabling anomaly detection directly from raw images without annotated masks. This approach generates interpretable step-by-step explanations for defect localization. Our method achieves state-of-the-art performance, outperforming prior approaches by 36% in accuracy on MVTec-AD and 16% on VisA. By eliminating mask dependency and reducing costs while providing explainable outputs, this work advances industrial anomaly detection and supports scalable quality control in manufacturing. Code to reproduce the experiment is available at https://github.com/LilaKen/LR-IAD.","authors":["Peijian Zeng","Feiyan Pang","Zhanbo Wang","Aimin Yang"],"url":"https://arxiv.org/abs/2504.19524"}
{"created":"2025-04-29","title":"Identification and Estimation of Long-Term Treatment Effects with Monotone Missing","abstract":"Estimating long-term treatment effects has a wide range of applications in various domains. A key feature in this context is that collecting long-term outcomes typically involves a multi-stage process and is subject to monotone missing, where individuals missing at an earlier stage remain missing at subsequent stages. Despite its prevalence, monotone missing has been rarely explored in previous studies on estimating long-term treatment effects. In this paper, we address this gap by introducing the sequential missingness assumption for identification. We propose three novel estimation methods, including inverse probability weighting, sequential regression imputation, and sequential marginal structural model (SeqMSM). Considering that the SeqMSM method may suffer from high variance due to severe data sparsity caused by monotone missing, we further propose a novel balancing-enhanced approach, BalanceNet, to improve the stability and accuracy of the estimation methods. Extensive experiments on two widely used benchmark datasets demonstrate the effectiveness of our proposed methods.","authors":["Qinwei Yang","Ruocheng Guo","Shasha Han","Peng Wu"],"url":"https://arxiv.org/abs/2504.19527"}
{"created":"2025-04-29","title":"Adversarial Shallow Watermarking","abstract":"Recent advances in digital watermarking make use of deep neural networks for message embedding and extraction. They typically follow the ``encoder-noise layer-decoder''-based architecture. By deliberately establishing a differentiable noise layer to simulate the distortion of the watermarked signal, they jointly train the deep encoder and decoder to fit the noise layer to guarantee robustness. As a result, they are usually weak against unknown distortions that are not used in their training pipeline. In this paper, we propose a novel watermarking framework to resist unknown distortions, namely Adversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder that is randomly parameterized and designed to be insensitive to distortions for watermarking extraction. During the watermark embedding, ASW freezes the shallow decoder and adversarially optimizes a host image until its updated version (i.e., the watermarked image) stably triggers the shallow decoder to output the watermark message. During the watermark extraction, it accurately recovers the message from the watermarked image by leveraging the insensitive nature of the shallow decoder against arbitrary distortions. Our ASW is training-free, encoder-free, and noise layer-free. Experiments indicate that the watermarked images created by ASW have strong robustness against various unknown distortions. Compared to the existing ``encoder-noise layer-decoder'' approaches, ASW achieves comparable results on known distortions and better robustness on unknown distortions.","authors":["Guobiao Li","Lei Tan","Yuliang Xue","Gaozhi Liu","Zhenxing Qian","Sheng Li","Xinpeng Zhang"],"url":"https://arxiv.org/abs/2504.19529"}
{"created":"2025-04-29","title":"Euclidean Distance Matrix Completion via Asymmetric Projected Gradient Descent","abstract":"This paper proposes and analyzes a gradient-type algorithm based on Burer-Monteiro factorization, called the Asymmetric Projected Gradient Descent (APGD), for reconstructing the point set configuration from partial Euclidean distance measurements, known as the Euclidean Distance Matrix Completion (EDMC) problem. By paralleling the incoherence matrix completion framework, we show for the first time that global convergence guarantee with exact recovery of this routine can be established given $\\mathcal{O}(\\mu^2 r^3 \\kappa^2 n \\log n)$ Bernoulli random observations without any sample splitting. Unlike leveraging the tangent space Restricted Isometry Property (RIP) and local curvature of the low-rank embedding manifold in some very recent works, our proof provides new upper bounds to replace the random graph lemma under EDMC setting. The APGD works surprisingly well and numerical experiments demonstrate exact linear convergence behavior in rich-sample regions yet deteriorates fast when compared with the performance obtained by optimizing the s-stress function, i.e., the standard but unexplained non-convex approach for EDMC, if the sample size is limited. While virtually matching our theoretical prediction, this unusual phenomenon might indicate that: (i) the power of implicit regularization is weakened when specified in the APGD case; (ii) the stabilization of such new gradient direction requires substantially more samples than the information-theoretic limit would suggest.","authors":["Yicheng Li","Xinghua Sun"],"url":"https://arxiv.org/abs/2504.19530"}
{"created":"2025-04-29","title":"Systematic Hardware Integration Testing for Smart Video-based Medical Device Prototypes","abstract":"This paper presents a hardware-in-the-loop (HIL) verification system for intelligent, camera-based in-body medical devices. A case study of a Video Capsule Endoscopy (VCE) prototype is used to illustrate the system's functionality. The field-programmable gate array (FPGA)-based approach simulates the capsule's traversal through the strointestinal (GI) tract by injecting on-demand pre-recorded images from VCE studies. It is demonstrated that the HIL configuration is capable of meeting the real-time requirements of the prototypes and automatically identifying errors. The integration of machine learning (ML) hardware accelerators within medical devices can be facilitated by utilising this configuration, as it enables the verification of its functionality prior to the initiation of clinical testing.","authors":["Oliver Bause","Julia Werner","Oliver Bringmann"],"url":"https://arxiv.org/abs/2504.19533"}
{"created":"2025-04-29","title":"TeleScope: A Longitudinal Dataset for Investigating Online Discourse and Information Interaction on Telegram","abstract":"Telegram is a globally popular instant messaging platform known for its strong emphasis on security, privacy, and unique social networking features. It has recently emerged as the host for various cross-domain analysis and research works, such as social media influence, propaganda studies, and extremism. This paper introduces TeleScope, an extensive dataset suite that, to our knowledge, is the largest of its kind. It comprises metadata for about 500K Telegram channels and downloaded message metadata for about 71K public channels, accounting for around 120M crawled messages. We also release channel connections and user interaction data built using Telegram's message-forwarding feature to study multiple use cases, such as information spread and message forwarding patterns. In addition, we provide data enrichments, such as language detection, active message posting periods for each channel, and Telegram entities extracted from messages, that enable online discourse analysis beyond what is possible with the original data alone. The dataset is designed for diverse applications, independent of specific research objectives, and sufficiently versatile to facilitate the replication of social media studies comparable to those conducted on platforms like X (formerly Twitter)","authors":["Susmita Gangopadhyay","Danilo Dessi","Dimitar Dimitrov","Stefan Dietze"],"url":"https://arxiv.org/abs/2504.19536"}
{"created":"2025-04-29","title":"Universally Wheeler Languages","abstract":"The notion of Wheeler languages is rooted in the Burrows-Wheeler transform (BWT), one of the most central concepts in data compression and indexing. The BWT has been generalized to finite automata, the so-called Wheeler automata, by Gagie et al. [Theor. Comput. Sci. 2017]. Wheeler languages have subsequently been defined as the class of regular languages for which there exists a Wheeler automaton accepting them. Besides their advantages in data indexing, these Wheelerlanguages also satisfy many interesting properties from a language theoretic point of view [Alanko et al., Inf. Comput. 2021]. A characteristic yet unsatisfying feature of Wheeler languages however is that their definition depends on a fixed order of the alphabet. In this paper we introduce the Universally Wheeler languages UW, i.e., the regular languages that are Wheeler with respect to all orders of a given alphabet. Our first main contribution is to relate UW to some very well known regular language classes. We first show that the Striclty Locally Testable languages are strictly included in UW. After noticing that UW is not closed under taking the complement, we prove that the class of languages for which both the language and its complement are in UW exactly coincides with those languages that are Definite or Reverse Definite. Secondly, we prove that deciding if a regular language given by a DFA is in UW can be done in quadratic time. We also show that this is optimal unless the Strong Exponential Time Hypothesis (SETH) fails.","authors":["Ruben Becker","Giuseppa Castiglione","Giovanna D'Agostino","Alberto Policriti","Nicola Prezza","Antonio Restivo","Brian Riccardi"],"url":"https://arxiv.org/abs/2504.19537"}
{"created":"2025-04-29","title":"Towards Faster and More Compact Foundation Models for Molecular Property Prediction","abstract":"Advancements in machine learning for molecular property prediction have improved accuracy but at the expense of higher computational cost and longer training times. Recently, the Joint Multi-domain Pre-training (JMP) foundation model has demonstrated strong performance across various downstream tasks with reduced training time over previous models. Despite JMP's advantages, fine-tuning it on molecular datasets ranging from small-scale to large-scale requires considerable time and computational resources. In this work, we investigate strategies to enhance efficiency by reducing model size while preserving performance. To better understand the model's efficiency, we analyze the layer contributions of JMP and find that later interaction blocks provide diminishing returns, suggesting an opportunity for model compression. We explore block reduction strategies by pruning the pre-trained model and evaluating its impact on efficiency and accuracy during fine-tuning. Our analysis reveals that removing two interaction blocks results in a minimal performance drop, reducing the model size by 32% while increasing inference throughput by 1.3x. These results suggest that JMP-L is over-parameterized and that a smaller, more efficient variant can achieve comparable performance with lower computational cost. Our study provides insights for developing lighter, faster, and more scalable foundation models for molecular and materials discovery. The code is publicly available at: https://github.com/Yasir-Ghunaim/efficient-jmp.","authors":["Yasir Ghunaim","Andr\\'es Villa","Gergo Ignacz","Gyorgy Szekely","Motasem Alfarra","Bernard Ghanem"],"url":"https://arxiv.org/abs/2504.19538"}
{"created":"2025-04-29","title":"Event-triggered Robust Model Predictive Control under Hard Computation Resource Constraints","abstract":"Model predictive control (MPC) is capable of controlling nonlinear systems with guaranteed constraint satisfaction and stability. However, MPC requires solving optimization problems online periodically, which often exceeds the local system's computational capabilities. A potential solution is to leverage external processing, such as a central industrial server. Yet, this central computer typically serves multiple systems simultaneously, leading to significant hardware demands due to the need to solve numerous optimization problems concurrently. In this work, we tackle this challenge by developing an event-triggered model predictive control (ET-MPC) that provably stabilizes multiple nonlinear systems under disturbances while solving only optimization problems for a fixed-size subset at any given time. Unlike existing ET-MPC methods, which primarily reduce average computational load yet still require hardware capable of handling all systems simultaneously, our approach reduces the worst-case computational load. This significantly lowers central server hardware requirements by diminishing peak computational demands. We achieve our improvements by leveraging recent advancements in distributed event-triggered linear control and integrating them with a robust MPC that employs constraint tightening.","authors":["Alexander Gr\\\"afe","Sebastian Trimpe"],"url":"https://arxiv.org/abs/2504.19540"}
{"created":"2025-04-29","title":"On Weight Enumeration and Structure Characterization of Polar Codes via Group Actions","abstract":"In this article, we provide a complete characterization of codewords in polar codes with weights less than twice the minimum distance, using the group action of the lower triangular affine (LTA) group. We derive a closed-form formula for the enumeration of such codewords. Furthermore, we introduce an enhanced partial order based on weight contributions, offering refined tools for code design. Our results extend previous work on Type II codewords to a full description of Type I codewords and offer new insights into the algebraic structure underlying decreasing monomial codes, including polar and Reed-Muller codes.","authors":["Vlad-Florin Dragoi","Mohammad Rowshan"],"url":"https://arxiv.org/abs/2504.19544"}
{"created":"2025-04-29","title":"Point2Quad: Generating Quad Meshes from Point Clouds via Face Prediction","abstract":"Quad meshes are essential in geometric modeling and computational mechanics. Although learning-based methods for triangle mesh demonstrate considerable advancements, quad mesh generation remains less explored due to the challenge of ensuring coplanarity, convexity, and quad-only meshes. In this paper, we present Point2Quad, the first learning-based method for quad-only mesh generation from point clouds. The key idea is learning to identify quad mesh with fused pointwise and facewise features. Specifically, Point2Quad begins with a k-NN-based candidate generation considering the coplanarity and squareness. Then, two encoders are followed to extract geometric and topological features that address the challenge of quad-related constraints, especially by combining in-depth quadrilaterals-specific characteristics. Subsequently, the extracted features are fused to train the classifier with a designed compound loss. The final results are derived after the refinement by a quad-specific post-processing. Extensive experiments on both clear and noise data demonstrate the effectiveness and superiority of Point2Quad, compared to baseline methods under comprehensive metrics.","authors":["Zezeng Li","Zhihui Qi","Weimin Wang","Ziliang Wang","Junyi Duan","Na Lei"],"url":"https://arxiv.org/abs/2504.19545"}
{"created":"2025-04-29","title":"Crowd Detection Using Very-Fine-Resolution Satellite Imagery","abstract":"Accurate crowd detection (CD) is critical for public safety and historical pattern analysis, yet existing methods relying on ground and aerial imagery suffer from limited spatio-temporal coverage. The development of very-fine-resolution (VFR) satellite sensor imagery (e.g., ~0.3 m spatial resolution) provides unprecedented opportunities for large-scale crowd activity analysis, but it has never been considered for this task. To address this gap, we proposed CrowdSat-Net, a novel point-based convolutional neural network, which features two innovative components: Dual-Context Progressive Attention Network (DCPAN) to improve feature representation of individuals by aggregating scene context and local individual characteristics, and High-Frequency Guided Deformable Upsampler (HFGDU) that recovers high-frequency information during upsampling through frequency-domain guided deformable convolutions. To validate the effectiveness of CrowdSat-Net, we developed CrowdSat, the first VFR satellite imagery dataset designed specifically for CD tasks, comprising over 120k manually labeled individuals from multi-source satellite platforms (Beijing-3N, Jilin-1 Gaofen-04A and Google Earth) across China. In the experiments, CrowdSat-Net was compared with five state-of-the-art point-based CD methods (originally designed for ground or aerial imagery) using CrowdSat and achieved the largest F1-score of 66.12% and Precision of 73.23%, surpassing the second-best method by 1.71% and 2.42%, respectively. Moreover, extensive ablation experiments validated the importance of the DCPAN and HFGDU modules. Furthermore, cross-regional evaluation further demonstrated the spatial generalizability of CrowdSat-Net. This research advances CD capability by providing both a newly developed network architecture for CD and a pioneering benchmark dataset to facilitate future CD development.","authors":["Tong Xiao","Qunming Wang","Ping Lu","Tenghai Huang","Xiaohua Tong","Peter M. Atkinson"],"url":"https://arxiv.org/abs/2504.19546"}
{"created":"2025-04-29","title":"Space-Efficient Depth-First Search via Augmented Succinct Graph Encodings","abstract":"We call a graph $G$ separable if a balanced separator can be computed for $G$ of size $O(n^c)$ with $c<1$. Many real-world graphs are separable such as graphs of bounded genus, graphs of constant treewidth, and graphs excluding a fixed minor $H$. In particular, the well-known planar graphs are separable. We present a succinct encoding of separable graphs $G$ such that any number of depth-first searches DFS can be performed, from any given start vertex, each in $o(n)$ time with $o(n)$ additional bits. After the execution of a DFS, the succinct encoding of $G$ is augmented such that the DFS tree is encoded inside the encoding. Afterward, the encoding provides common DFS-related queries in constant time. These queries include queries such as lowest-common ancestor of two given vertices in the DFS tree or queries that output the lowpoint of a given vertex in the DFS tree. Furthermore, for planar graphs, we show that the succinct encoding can be computed in $O(n)$ bits and expected linear time, and a compact variant can be constructed in $O(n)$ time and bits.","authors":["Michael Elberfeld","Frank Kammer","Johannes Meintrup"],"url":"https://arxiv.org/abs/2504.19547"}
{"created":"2025-04-29","title":"DEEMO: De-identity Multimodal Emotion Recognition and Reasoning","abstract":"Emotion understanding is a critical yet challenging task. Most existing approaches rely heavily on identity-sensitive information, such as facial expressions and speech, which raises concerns about personal privacy. To address this, we introduce the De-identity Multimodal Emotion Recognition and Reasoning (DEEMO), a novel task designed to enable emotion understanding using de-identified video and audio inputs. The DEEMO dataset consists of two subsets: DEEMO-NFBL, which includes rich annotations of Non-Facial Body Language (NFBL), and DEEMO-MER, an instruction dataset for Multimodal Emotion Recognition and Reasoning using identity-free cues. This design supports emotion understanding without compromising identity privacy. In addition, we propose DEEMO-LLaMA, a Multimodal Large Language Model (MLLM) that integrates de-identified audio, video, and textual information to enhance both emotion recognition and reasoning. Extensive experiments show that DEEMO-LLaMA achieves state-of-the-art performance on both tasks, outperforming existing MLLMs by a significant margin, achieving 74.49% accuracy and 74.45% F1-score in de-identity emotion recognition, and 6.20 clue overlap and 7.66 label overlap in de-identity emotion reasoning. Our work contributes to ethical AI by advancing privacy-preserving emotion understanding and promoting responsible affective computing.","authors":["Deng Li","Bohao Xing","Xin Liu","Baiqiang Xia","Bihan Wen","Heikki K\\\"alvi\\\"ainen"],"url":"https://arxiv.org/abs/2504.19549"}
{"created":"2025-04-29","title":"BinCoFer: Three-Stage Purification for Effective C/C++ Binary Third-Party Library Detection","abstract":"Third-party libraries (TPL) are becoming increasingly popular to achieve efficient and concise software development. However, unregulated use of TPL will introduce legal and security issues in software development. Consequently, some studies have attempted to detect the reuse of TPLs in target programs by constructing a feature repository. Most of the works require access to the source code of TPLs, while the others suffer from redundancy in the repository, low detection efficiency, and difficulties in detecting partially referenced third-party libraries. Therefore, we introduce BinCoFer, a tool designed for detecting TPLs reused in binary programs. We leverage the work of binary code similarity detection(BCSD) to extract binary-format TPL features, making it suitable for scenarios where the source code of TPLs is inaccessible. BinCoFer employs a novel three-stage purification strategy to mitigate feature repository redundancy by highlighting core functions and extracting function-level features, making it applicable to scenarios of partial reuse of TPLs. We have observed that directly using similarity threshold to determine the reuse between two binary functions is inaccurate, a problem that previous work has not addressed. Thus we design a method that uses weight to aggregate the similarity between functions in the target binary and core functions to ultimately judge the reuse situation with high frequency. To examine the ability of BinCoFer, we compiled a dataset on ArchLinux and conduct comparative experiments on it with other four most related works (i.e., ModX, B2SFinder, LibAM and BinaryAI)...","authors":["Yayi Zou","Yixiang Zhang","Guanghao Zhao","Yueming Wu","Shuhao Shen","Cai Fu"],"url":"https://arxiv.org/abs/2504.19551"}
{"created":"2025-04-29","title":"Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment","abstract":"Given the subtle human-like effects of large language models on linguistic patterns, this study examines shifts in language over time to detect the impact of AI-mediated communication (AI- MC) on social media. We compare a replicated dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the same period in 2024, all of which mention Donald Trump during election periods. Using a combination of Flesch-Kincaid readability and polarity scores, we analyze changes in text complexity and sentiment. Our findings reveal a significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more positive expressions (28.6% to 45.9%). These findings suggest not only an increasing presence of AI in social media communication but also its impact on language and emotional expression patterns.","authors":["Kristen Sussman","Daniel Carter"],"url":"https://arxiv.org/abs/2504.19556"}
{"created":"2025-04-29","title":"CE-NPBG: Connectivity Enhanced Neural Point-Based Graphics for Novel View Synthesis in Autonomous Driving Scenes","abstract":"Current point-based approaches encounter limitations in scalability and rendering quality when using large 3D point cloud maps because using them directly for novel view synthesis (NVS) leads to degraded visualizations. We identify the primary issue behind these low-quality renderings as a visibility mismatch between geometry and appearance, stemming from using these two modalities together. To address this problem, we present CE-NPBG, a new approach for novel view synthesis (NVS) in large-scale autonomous driving scenes. Our method is a neural point-based technique that leverages two modalities: posed images (cameras) and synchronized raw 3D point clouds (LiDAR). We first employ a connectivity relationship graph between appearance and geometry, which retrieves points from a large 3D point cloud map observed from the current camera perspective and uses them for rendering. By leveraging this connectivity, our method significantly improves rendering quality and enhances run-time and scalability by using only a small subset of points from the large 3D point cloud map. Our approach associates neural descriptors with the points and uses them to synthesize views. To enhance the encoding of these descriptors and elevate rendering quality, we propose a joint adversarial and point rasterization training. During training, we pair an image-synthesizer network with a multi-resolution discriminator. At inference, we decouple them and use the image-synthesizer to generate novel views. We also integrate our proposal into the recent 3D Gaussian Splatting work to highlight its benefits for improved rendering and scalability.","authors":["Mohammad Altillawi","Fengyi Shen","Liudi Yang","Sai Manoj Prakhya","Ziyuan Liu"],"url":"https://arxiv.org/abs/2504.19557"}
{"created":"2025-04-29","title":"Quantifying Memory Utilization with Effective State-Size","abstract":"The need to develop a general framework for architecture analysis is becoming increasingly important, given the expanding design space of sequence models. To this end, we draw insights from classical signal processing and control theory, to develop a quantitative measure of \\textit{memory utilization}: the internal mechanisms through which a model stores past information to produce future outputs. This metric, which we call \\textbf{\\textit{effective state-size}} (ESS), is tailored to the fundamental class of systems with \\textit{input-invariant} and \\textit{input-varying linear operators}, encompassing a variety of computational units such as variants of attention, convolutions, and recurrences. Unlike prior work on memory utilization, which either relies on raw operator visualizations (e.g. attention maps), or simply the total \\textit{memory capacity} (i.e. cache size) of a model, our metrics provide highly interpretable and actionable measurements. In particular, we show how ESS can be leveraged to improve initialization strategies, inform novel regularizers and advance the performance-efficiency frontier through model distillation. Furthermore, we demonstrate that the effect of context delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural differences in how large language models utilize their available memory to recall information. Overall, we find that ESS provides valuable insights into the dynamics that dictate memory utilization, enabling the design of more efficient and effective sequence models.","authors":["Rom N. Parnichkun","Neehal Tumma","Armin W. Thomas","Alessandro Moro","Qi An","Taiji Suzuki","Atsushi Yamashita","Michael Poli","Stefano Massaroli"],"url":"https://arxiv.org/abs/2504.19561"}
{"created":"2025-04-29","title":"m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training","abstract":"The rapid progress of large language models (LLMs) in biomedical research has underscored the limitations of existing open-source annotated scientific corpora, which are often insufficient in quantity and quality. Addressing the challenge posed by the complex hierarchy of biomedical knowledge, we propose a knowledge-driven, multi-agent framework for scientific corpus distillation tailored for LLM training in the biomedical domain. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. These agents collectively generate and refine domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.","authors":["Meng Xiao","Xunxin Cai","Chengrui Wang","Yuanchun Zhou"],"url":"https://arxiv.org/abs/2504.19565"}
{"created":"2025-04-29","title":"Metadata-private Messaging without Coordination","abstract":"For those seeking end-to-end private communication free from pervasive metadata tracking and censorship, the Tor network has been the de-facto choice in practice, despite its susceptibility to traffic analysis attacks. Recently, numerous metadata-private messaging proposals have emerged with the aim to surpass Tor in the messaging context by obscuring the relationships between any two messaging buddies, even against global and active attackers. However, most of these systems face an undesirable usability constraint: they require a metadata-private \"dialing\" phase to establish mutual agreement and timing or round coordination before initiating any regular chats among users. This phase is not only resource-intensive but also inflexible, limiting users' ability to manage multiple concurrent conversations seamlessly. For stringent privacy requirement, the often-enforced traffic uniformity further exacerbated the limitations of this roadblock.","authors":["Peipei Jiang","Yihao Wu","Lei Xu","Wentao Dong","Peiyuan Chen","Yulong Ming","Cong Wang","Xiaohua Jia","Qian Wang"],"url":"https://arxiv.org/abs/2504.19566"}
{"created":"2025-04-29","title":"GenPTW: In-Generation Image Watermarking for Provenance Tracing and Tamper Localization","abstract":"The rapid development of generative image models has brought tremendous opportunities to AI-generated content (AIGC) creation, while also introducing critical challenges in ensuring content authenticity and copyright ownership. Existing image watermarking methods, though partially effective, often rely on post-processing or reference images, and struggle to balance fidelity, robustness, and tamper localization. To address these limitations, we propose GenPTW, an In-Generation image watermarking framework for latent diffusion models (LDMs), which integrates Provenance Tracing and Tamper Localization into a unified Watermark-based design. It embeds structured watermark signals during the image generation phase, enabling unified provenance tracing and tamper localization. For extraction, we construct a frequency-coordinated decoder to improve robustness and localization precision in complex editing scenarios. Additionally, a distortion layer that simulates AIGC editing is introduced to enhance robustness. Extensive experiments demonstrate that GenPTW outperforms existing methods in image fidelity, watermark extraction accuracy, and tamper localization performance, offering an efficient and practical solution for trustworthy AIGC image generation.","authors":["Zhenliang Gan","Chunya Liu","Yichao Tang","Binghao Wang","Weiqiang Wang","Xinpeng Zhang"],"url":"https://arxiv.org/abs/2504.19567"}
{"created":"2025-04-29","title":"Video-Based Detection and Analysis of Errors in Robotic Surgical Training","abstract":"Robot-assisted minimally invasive surgeries offer many advantages but require complex motor tasks that take surgeons years to master. There is currently a lack of knowledge on how surgeons acquire these robotic surgical skills. To help bridge this gap, we previously followed surgical residents learning complex surgical training dry-lab tasks on a surgical robot over six months. Errors are an important measure for self-training and for skill evaluation, but unlike in virtual simulations, in dry-lab training, errors are difficult to monitor automatically. Here, we analyzed the errors in the ring tower transfer task, in which surgical residents moved a ring along a curved wire as quickly and accurately as possible. We developed an image-processing algorithm to detect collision errors and achieved detection accuracy of ~95%. Using the detected errors and task completion time, we found that the surgical residents decreased their completion time and number of errors over the six months. This analysis provides a framework for detecting collision errors in similar surgical training tasks and sheds light on the learning process of the surgical residents.","authors":["Hanna Kossowsky Lev","Yarden Sharon","Alex Geftler","Ilana Nisky"],"url":"https://arxiv.org/abs/2504.19571"}
{"created":"2025-04-29","title":"Category-Level and Open-Set Object Pose Estimation for Robotics","abstract":"Object pose estimation enables a variety of tasks in computer vision and robotics, including scene understanding and robotic grasping. The complexity of a pose estimation task depends on the unknown variables related to the target object. While instance-level methods already excel for opaque and Lambertian objects, category-level and open-set methods, where texture, shape, and size are partially or entirely unknown, still struggle with these basic material properties. Since texture is unknown in these scenarios, it cannot be used for disambiguating object symmetries, another core challenge of 6D object pose estimation. The complexity of estimating 6D poses with such a manifold of unknowns led to various datasets, accuracy metrics, and algorithmic solutions. This paper compares datasets, accuracy metrics, and algorithms for solving 6D pose estimation on the category-level. Based on this comparison, we analyze how to bridge category-level and open-set object pose estimation to reach generalization and provide actionable recommendations.","authors":["Peter H\\\"onig","Matthias Hirschmanner","Markus Vincze"],"url":"https://arxiv.org/abs/2504.19572"}
{"created":"2025-04-29","title":"DG-DETR: Toward Domain Generalized Detection Transformer","abstract":"End-to-end Transformer-based detectors (DETRs) have demonstrated strong detection performance. However, domain generalization (DG) research has primarily focused on convolutional neural network (CNN)-based detectors, while paying little attention to enhancing the robustness of DETRs. In this letter, we introduce a Domain Generalized DEtection TRansformer (DG-DETR), a simple, effective, and plug-and-play method that improves out-of-distribution (OOD) robustness for DETRs. Specifically, we propose a novel domain-agnostic query selection strategy that removes domain-induced biases from object queries via orthogonal projection onto the instance-specific style space. Additionally, we leverage a wavelet decomposition to disentangle features into domain-invariant and domain-specific components, enabling synthesis of diverse latent styles while preserving the semantic features of objects. Experimental results validate the effectiveness of DG-DETR. Our code is available at https://github.com/sminhwang/DG-DETR.","authors":["Seongmin Hwang","Daeyoung Han","Moongu Jeon"],"url":"https://arxiv.org/abs/2504.19574"}
{"created":"2025-04-29","title":"Positive Almost-Sure Termination of Polynomial Random Walks","abstract":"The number of steps until termination of a probabilistic program is a random variable. Probabilistic program termination therefore requires qualitative analysis via almost-sure termination (AST), while also providing quantitative answers via positive almost-sure termination (PAST) on the expected number of steps until termination. While every program which is PAST is AST, the converse is not true. The symmetric random walk with constant step size is a prominent example of a program that is AST but not PAST.","authors":["Lorenz Winkler","Laura Kov\\'acs"],"url":"https://arxiv.org/abs/2504.19575"}
{"created":"2025-04-29","title":"Smart Placement, Faster Robots -- A Comparison of Algorithms for Robot Base-Pose Optimization","abstract":"Robotic automation is a key technology that increases the efficiency and flexibility of manufacturing processes. However, one of the challenges in deploying robots in novel environments is finding the optimal base pose for the robot, which affects its reachability and deployment cost. Yet, the existing research for automatically optimizing the base pose of robots has not been compared. We address this problem by optimizing the base pose of industrial robots with Bayesian optimization, exhaustive search, genetic algorithms, and stochastic gradient descent and find that all algorithms can reduce the cycle time for various evaluated tasks in synthetic and real-world environments. Stochastic gradient descent shows superior performance with regard to success rate solving over 90% of our real-world tasks, while genetic algorithms show the lowest final costs. All benchmarks and implemented methods are available as baselines against which novel approaches can be compared.","authors":["Matthias Mayer","Matthias Althoff"],"url":"https://arxiv.org/abs/2504.19577"}
{"created":"2025-04-29","title":"ARTEMIS: Autoregressive End-to-End Trajectory Planning with Mixture of Experts for Autonomous Driving","abstract":"This paper presents ARTEMIS, an end-to-end autonomous driving framework that combines autoregressive trajectory planning with Mixture-of-Experts (MoE). Traditional modular methods suffer from error propagation, while existing end-to-end models typically employ static one-shot inference paradigms that inadequately capture the dynamic changes of the environment. ARTEMIS takes a different method by generating trajectory waypoints sequentially, preserves critical temporal dependencies while dynamically routing scene-specific queries to specialized expert networks. It effectively relieves trajectory quality degradation issues encountered when guidance information is ambiguous, and overcomes the inherent representational limitations of singular network architectures when processing diverse driving scenarios. Additionally, we use a lightweight batch reallocation strategy that significantly improves the training speed of the Mixture-of-Experts model. Through experiments on the NAVSIM dataset, ARTEMIS exhibits superior competitive performance, achieving 87.0 PDMS and 83.1 EPDMS with ResNet-34 backbone, demonstrates state-of-the-art performance on multiple metrics.","authors":["Renju Feng","Ning Xi","Duanfeng Chu","Rukang Wang","Zejian Deng","Anzheng Wang","Liping Lu","Jinxiang Wang","Yanjun Huang"],"url":"https://arxiv.org/abs/2504.19580"}
{"created":"2025-04-29","title":"SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity","abstract":"Driven by the increasing demand for accurate and efficient representation of 3D data in various domains, point cloud sampling has emerged as a pivotal research topic in 3D computer vision. Recently, learning-to-sample methods have garnered growing interest from the community, particularly for their ability to be jointly trained with downstream tasks. However, previous learning-based sampling methods either lead to unrecognizable sampling patterns by generating a new point cloud or biased sampled results by focusing excessively on sharp edge details. Moreover, they all overlook the natural variations in point distribution across different shapes, applying a similar sampling strategy to all point clouds. In this paper, we propose a Sparse Attention Map and Bin-based Learning method (termed SAMBLE) to learn shape-specific sampling strategies for point cloud shapes. SAMBLE effectively achieves an improved balance between sampling edge points for local details and preserving uniformity in the global shape, resulting in superior performance across multiple common point cloud downstream tasks, even in scenarios with few-point sampling.","authors":["Chengzhi Wu","Yuxin Wan","Hao Fu","Julius Pfrommer","Zeyun Zhong","Junwei Zheng","Jiaming Zhang","J\\\"urgen Beyerer"],"url":"https://arxiv.org/abs/2504.19581"}
{"created":"2025-04-29","title":"Graph-Based Spectral Decomposition for Parameter Coordination in Language Model Fine-Tuning","abstract":"This paper proposes a parameter collaborative optimization algorithm for large language models, enhanced with graph spectral analysis. The goal is to improve both fine-tuning efficiency and structural awareness during training. In the proposed method, the parameters of a pre-trained language model are treated as nodes in a graph. A weighted graph is constructed, and Laplacian spectral decomposition is applied to enable frequency-domain modeling and structural representation of the parameter space. Based on this structure, a joint loss function is designed. It combines the task loss with a spectral regularization term to facilitate collaborative updates among parameters. In addition, a spectral filtering mechanism is introduced during the optimization phase. This mechanism adjusts gradients in a structure-aware manner, enhancing the model's training stability and convergence behavior. The method is evaluated on multiple tasks, including traditional fine-tuning comparisons, few-shot generalization tests, and convergence speed analysis. In all settings, the proposed approach demonstrates superior performance. The experimental results confirm that the spectral collaborative optimization framework effectively reduces parameter perturbations and improves fine-tuning quality while preserving overall model performance. This work contributes significantly to the field of artificial intelligence by advancing parameter-efficient training methodologies for large-scale models, reinforcing the importance of structural signal processing in deep learning optimization, and offering a robust, generalizable framework for enhancing language model adaptability and performance.","authors":["Hanlu Zhang","Yumeng Ma","Shuo Wang","Guiran Liu","Binrong Zhu"],"url":"https://arxiv.org/abs/2504.19583"}
{"created":"2025-04-29","title":"ShowMak3r: Compositional TV Show Reconstruction","abstract":"Reconstructing dynamic radiance fields from video clips is challenging, especially when entertainment videos like TV shows are given. Many challenges make the reconstruction difficult due to (1) actors occluding with each other and having diverse facial expressions, (2) cluttered stages, and (3) small baseline views or sudden shot changes. To address these issues, we present ShowMak3r, a comprehensive reconstruction pipeline that allows the editing of scenes like how video clips are made in a production control room. In ShowMak3r, a 3DLocator module locates recovered actors on the stage using depth prior and estimates unseen human poses via interpolation. The proposed ShotMatcher module then tracks the actors under shot changes. Furthermore, ShowMak3r introduces a face-fitting network that dynamically recovers the actors' expressions. Experiments on Sitcoms3D dataset show that our pipeline can reassemble TV show scenes with new cameras at different timestamps. We also demonstrate that ShowMak3r enables interesting applications such as synthetic shot-making, actor relocation, insertion, deletion, and pose manipulation. Project page : https://nstar1125.github.io/showmak3r","authors":["Sangmin Kim","Seunguk Do","Jaesik Park"],"url":"https://arxiv.org/abs/2504.19584"}
{"created":"2025-04-29","title":"Magnifier: A Multi-grained Neural Network-based Architecture for Burned Area Delineation","abstract":"In crisis management and remote sensing, image segmentation plays a crucial role, enabling tasks like disaster response and emergency planning by analyzing visual data. Neural networks are able to analyze satellite acquisitions and determine which areas were affected by a catastrophic event. The problem in their development in this context is the data scarcity and the lack of extensive benchmark datasets, limiting the capabilities of training large neural network models. In this paper, we propose a novel methodology, namely Magnifier, to improve segmentation performance with limited data availability. The Magnifier methodology is applicable to any existing encoder-decoder architecture, as it extends a model by merging information at different contextual levels through a dual-encoder approach: a local and global encoder. Magnifier analyzes the input data twice using the dual-encoder approach. In particular, the local and global encoders extract information from the same input at different granularities. This allows Magnifier to extract more information than the other approaches given the same set of input images. Magnifier improves the quality of the results of +2.65% on average IoU while leading to a restrained increase in terms of the number of trainable parameters compared to the original model. We evaluated our proposed approach with state-of-the-art burned area segmentation models, demonstrating, on average, comparable or better performances in less than half of the GFLOPs.","authors":["Daniele Rege Cambrin","Luca Colomba","Paolo Garza"],"url":"https://arxiv.org/abs/2504.19589"}
{"created":"2025-04-29","title":"Arabic Metaphor Sentiment Classification Using Semantic Information","abstract":"In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1] using newly designed automatic tools for sentiment classification for AMC based on semantic tags. The tool incorporates semantic emotional tags for sentiment classification. I evaluate the tool using standard methods, which are F-score, recall, and precision. The method is to show the impact of Arabic online metaphors on sentiment through the newly designed tools. To the best of our knowledge, this is the first approach to conduct sentiment classification for Arabic metaphors using semantic tags to find the impact of the metaphor.","authors":["Israa Alsiyat"],"url":"https://arxiv.org/abs/2504.19590"}
{"created":"2025-04-29","title":"Neural network task specialization via domain constraining","abstract":"This paper introduces a concept of neural network specialization via task-specific domain constraining, aimed at enhancing network performance on data subspace in which the network operates. The study presents experiments on training specialists for image classification and object detection tasks. The results demonstrate that specialization can enhance a generalist's accuracy even without additional data or changing training regimes: solely by constraining class label space in which the network performs. Theoretical and experimental analyses indicate that effective specialization requires modifying traditional fine-tuning methods and constraining data space to semantically coherent subsets. The specialist extraction phase before tuning the network is proposed for maximal performance gains. We also provide analysis of the evolution of the feature space during specialization. This study paves way to future research for developing more advanced dynamically configurable image analysis systems, where computations depend on the specific input. Additionally, the proposed methods can help improve system performance in scenarios where certain data domains should be excluded from consideration of the generalist network.","authors":["Roman Malashin","Daniil Ilyukhin"],"url":"https://arxiv.org/abs/2504.19592"}
{"created":"2025-04-29","title":"A Time-dependent Risk-aware distributed Multi-Agent Path Finder based on A*","abstract":"Multi-Agent Path-Finding (MAPF) focuses on the collaborative planning of paths for multiple agents within shared spaces, aiming for collision-free navigation. Conventional planning methods often overlook the presence of other agents, which can result in conflicts. In response, this article introduces the A$^*_+$T algorithm, a distributed approach that improves coordination among agents by anticipating their positions based on their movement speeds. The algorithm also considers dynamic obstacles, assessing potential collisions with respect to observed speeds and trajectories, thereby facilitating collision-free path planning in environments populated by other agents and moving objects. It incorporates a risk layer surrounding both dynamic and static entities, enhancing its utility in real-world applications. Each agent functions autonomously while being mindful of the paths chosen by others, effectively addressing the complexities inherent in multi-agent situations. The performance of A$^*_+$T has been rigorously tested in the Gazebo simulation environment and benchmarked against established approaches such as CBS, ECBS, and SIPP. Furthermore, the algorithm has shown competence in single-agent experiments, with results demonstrating its effectiveness in managing dynamic obstacles and affirming its practical relevance across various scenarios.","authors":["S Nordstr\\\"om","Y Bai","B Lindqvist","G Nikolakopoulos"],"url":"https://arxiv.org/abs/2504.19593"}
{"created":"2025-04-29","title":"Mapping the Italian Telegram Ecosystem","abstract":"Telegram has become a major space for political discourse and alternative media. However, its lack of moderation allows misinformation, extremism, and toxicity to spread. While prior research focused on these particular phenomena or topics, these have mostly been examined separately, and a broader understanding of the Telegram ecosystem is still missing. In this work, we fill this gap by conducting a large-scale analysis of the Italian Telegram sphere, leveraging a dataset of 186 million messages from 13,151 chats collected in 2023. Using network analysis, Large Language Models, and toxicity detection tools, we examine how different thematic communities form, align ideologically, and engage in harmful discourse within the Italian cultural context. Results show strong thematic and ideological homophily. We also identify mixed ideological communities where far-left and far-right rhetoric coexist on particular geopolitical issues. Beyond political analysis, we find that toxicity, rather than being isolated in a few extreme chats, appears widely normalized within highly toxic communities. Moreover, we find that Italian discourse primarily targets Black people, Jews, and gay individuals independently of the topic. Finally, we uncover common trend of intra-national hostility, where Italians often attack other Italians, reflecting regional and intra-regional cultural conflicts that can be traced back to old historical divisions. This study provides the first large-scale mapping of the Italian Telegram ecosystem, offering insights into ideological interactions, toxicity, and identity-targets of hate and contributing to research on online toxicity across different cultural and linguistic contexts on Telegram.","authors":["Lorenzo Alvisi","Serena Tardelli","Maurizio Tesconi"],"url":"https://arxiv.org/abs/2504.19594"}
{"created":"2025-04-29","title":"WILD: a new in-the-Wild Image Linkage Dataset for synthetic image attribution","abstract":"Synthetic image source attribution is an open challenge, with an increasing number of image generators being released yearly. The complexity and the sheer number of available generative techniques, as well as the scarcity of high-quality open source datasets of diverse nature for this task, make training and benchmarking synthetic image source attribution models very challenging. WILD is a new in-the-Wild Image Linkage Dataset designed to provide a powerful training and benchmarking tool for synthetic image attribution models. The dataset is built out of a closed set of 10 popular commercial generators, which constitutes the training base of attribution models, and an open set of 10 additional generators, simulating a real-world in-the-wild scenario. Each generator is represented by 1,000 images, for a total of 10,000 images in the closed set and 10,000 images in the open set. Half of the images are post-processed with a wide range of operators. WILD allows benchmarking attribution models in a wide range of tasks, including closed and open set identification and verification, and robust attribution with respect to post-processing and adversarial attacks. Models trained on WILD are expected to benefit from the challenging scenario represented by the dataset itself. Moreover, an assessment of seven baseline methodologies on closed and open set attribution is presented, including robustness tests with respect to post-processing.","authors":["Pietro Bongini","Sara Mandelli","Andrea Montibeller","Mirko Casu","Orazio Pontorno","Claudio Ragaglia","Luca Zanchetta","Mattia Aquilina","Taiba Majid Wani","Luca Guarnera","Benedetta Tondi","Paolo Bestagini","Irene Amerini","Francesco Denatale","Sebastiano Battiato","Mauro Barni"],"url":"https://arxiv.org/abs/2504.19595"}
{"created":"2025-04-29","title":"Lightweight Adapter Learning for More Generalized Remote Sensing Change Detection","abstract":"Deep learning methods have shown promising performances in remote sensing image change detection (CD). However, existing methods usually train a dataset-specific deep network for each dataset. Due to the significant differences in the data distribution and labeling between various datasets, the trained dataset-specific deep network has poor generalization performances on other datasets. To solve this problem, this paper proposes a change adapter network (CANet) for a more universal and generalized CD. CANet contains dataset-shared and dataset-specific learning modules. The former explores the discriminative features of images, and the latter designs a lightweight adapter model, to deal with the characteristics of different datasets in data distribution and labeling. The lightweight adapter can quickly generalize the deep network for new CD tasks with a small computation cost. Specifically, this paper proposes an interesting change region mask (ICM) in the adapter, which can adaptively focus on interested change objects and decrease the influence of labeling differences in various datasets. Moreover, CANet adopts a unique batch normalization layer for each dataset to deal with data distribution differences. Compared with existing deep learning methods, CANet can achieve satisfactory CD performances on various datasets simultaneously. Experimental results on several public datasets have verified the effectiveness and advantages of the proposed CANet on CD. CANet has a stronger generalization ability, smaller training costs (merely updating 4.1%-7.7% parameters), and better performances under limited training datasets than other deep learning methods, which also can be flexibly inserted with existing deep models.","authors":["Dou Quan","Rufan Zhou","Shuang Wang","Ning Huyan","Dong Zhao","Yunan Li","Licheng Jiao"],"url":"https://arxiv.org/abs/2504.19598"}
{"created":"2025-04-29","title":"GVPO: Group Variance Policy Optimization for Large Language Model Post-Training","abstract":"Post-training plays a crucial role in refining and aligning large language models to meet specific tasks and human preferences. While recent advancements in post-training techniques, such as Group Relative Policy Optimization (GRPO), leverage increased sampling with relative reward scoring to achieve superior performance, these methods often suffer from training instability that limits their practical adoption. To address this challenge, we present Group Variance Policy Optimization (GVPO). GVPO incorporates the analytical solution to KL-constrained reward maximization directly into its gradient weights, ensuring alignment with the optimal policy. The method provides intuitive physical interpretations: its gradient mirrors the mean squared error between the central distance of implicit rewards and that of actual rewards. GVPO offers two key advantages: (1) it guarantees a unique optimal solution, exactly the KL-constrained reward maximization objective, (2) it supports flexible sampling distributions that avoids on-policy and importance sampling limitations. By unifying theoretical guarantees with practical adaptability, GVPO establishes a new paradigm for reliable and versatile LLM post-training.","authors":["Kaichen Zhang","Yuzhong Hong","Junwei Bao","Hongfei Jiang","Yang Song","Dingqian Hong","Hui Xiong"],"url":"https://arxiv.org/abs/2504.19599"}
{"created":"2025-04-29","title":"Image Generation Method Based on Heat Diffusion Models","abstract":"Denoising Diffusion Probabilistic Models (DDPMs) achieve high-quality image generation without adversarial training, but they process images as a whole. Since adjacent pixels are highly likely to belong to the same object, we propose the Heat Diffusion Model (HDM) to further preserve image details and generate more realistic images. HDM is a model that incorporates pixel-level operations while maintaining the same training process as DDPM. In HDM, the discrete form of the two-dimensional heat equation is integrated into the diffusion and generation formulas of DDPM, enabling the model to compute relationships between neighboring pixels during image processing. Our experiments demonstrate that HDM can generate higher-quality samples compared to models such as DDPM, Consistency Diffusion Models (CDM), Latent Diffusion Models (LDM), and Vector Quantized Generative Adversarial Networks (VQGAN).","authors":["Pengfei Zhang","Shouqing Jia"],"url":"https://arxiv.org/abs/2504.19600"}
{"created":"2025-04-29","title":"Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching for Small Buffer or Small Rate","abstract":"We consider the secure coded caching problem proposed by Ravindrakumar et. al where no user can obtain information about files other than the one requested. We first propose three new schemes for the three cases of cache size $M=1$, $N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files and $K$ users, and the general case for arbitrary $N$ files and $K$ users, respectively. Then we derive converse results by characterizing new properties of secure coded caching schemes. As a result, we characterize the two end-points of the optimal memory-rate tradeoff curve for arbitrary number of users and files. Furthermore, for the case of $N=2$ files and arbitrary number of users, we also characterize a segment of the optimal memory-rate tradeoff curve, where the cache size is relatively small.","authors":["Han Fang","Nan Liu","Wei Kang"],"url":"https://arxiv.org/abs/2504.19601"}
{"created":"2025-04-29","title":"Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation","abstract":"Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining accuracy. Enhanced ERA can be tuned to adapt to non-IID data variations, ensuring robust aggregation and performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.","authors":["Kitsuya Azuma","Takayuki Nishio","Yuichi Kitagawa","Wakako Nakano","Takahito Tanimura"],"url":"https://arxiv.org/abs/2504.19602"}
{"created":"2025-04-29","title":"Coreference Resolution for Vietnamese Narrative Texts","abstract":"Coreference resolution is a vital task in natural language processing (NLP) that involves identifying and linking different expressions in a text that refer to the same entity. This task is particularly challenging for Vietnamese, a low-resource language with limited annotated datasets. To address these challenges, we developed a comprehensive annotated dataset using narrative texts from VnExpress, a widely-read Vietnamese online news platform. We established detailed guidelines for annotating entities, focusing on ensuring consistency and accuracy. Additionally, we evaluated the performance of large language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset. Our results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in terms of both accuracy and response consistency, making it a more reliable tool for coreference resolution in Vietnamese.","authors":["Hieu-Dai Tran","Duc-Vu Nguyen","Ngan Luu-Thuy Nguyen"],"url":"https://arxiv.org/abs/2504.19606"}
{"created":"2025-04-29","title":"Adaptive Locomotion on Mud through Proprioceptive Sensing of Substrate Properties","abstract":"Muddy terrains present significant challenges for terrestrial robots, as subtle changes in composition and water content can lead to large variations in substrate strength and force responses, causing the robot to slip or get stuck. This paper presents a method to estimate mud properties using proprioceptive sensing, enabling a flipper-driven robot to adapt its locomotion through muddy substrates of varying strength. First, we characterize mud reaction forces through actuator current and position signals from a statically mounted robotic flipper. We use the measured force to determine key coefficients that characterize intrinsic mud properties. The proprioceptively estimated coefficients match closely with measurements from a lab-grade load cell, validating the effectiveness of the proposed method. Next, we extend the method to a locomoting robot to estimate mud properties online as it crawls across different mud mixtures. Experimental data reveal that mud reaction forces depend sensitively on robot motion, requiring joint analysis of robot movement with proprioceptive force to determine mud properties correctly. Lastly, we deploy this method in a flipper-driven robot moving across muddy substrates of varying strengths, and demonstrate that the proposed method allows the robot to use the estimated mud properties to adapt its locomotion strategy, and successfully avoid locomotion failures. Our findings highlight the potential of proprioception-based terrain sensing to enhance robot mobility in complex, deformable natural environments, paving the way for more robust field exploration capabilities.","authors":["Shipeng Liu","Jiaze Tang","Siyuan Meng","Feifei Qian"],"url":"https://arxiv.org/abs/2504.19607"}
{"created":"2025-04-29","title":"The frequency $K_i$s for symmetrical traveling salesman problem","abstract":"The frequency $K_i$s ($i\\in[4,n]$) are studied for symmetrical traveling salesman problem ($TSP$) to identify the edges in optimal Hamiltonian cycle ($OHC$). A frequency $K_i$ is computed with a sort of ${{i}\\choose{2}}$ optimal $i$-vertex paths with given endpoints (optimal $i$-vertex path) in a corresponding $K_i$ in $K_n$. In frequency $K_i$, the frequency of an edge is the number of the optimal $i$-vertex paths containing the edge in the corresponding $K_i$. Given an $OHC$ edge related to $K_i$, it has a frequency bigger than $\\frac{1}{2}{{i}\\choose{2}}$ in the corresponding frequency $K_i$, and that of an ordinary edge not in $OHC$ is smaller than $\\frac{i+2}{2}$. On average, an $OHC$ edge in $K_i$ has a frequency bigger than $\\frac{i^2-4i+7}{2}$ whereas an ordinary edge has a frequency smaller than 2. Moreover, given a frequency $K_i$ containing an $OHC$ edge related to $K_n$, the frequency of the $OHC$ edge is bigger than $\\frac{1}{2}{{i}\\choose{2}}$ in the worst average case. It implies that the average frequency of an $OHC$ edge computed with frequency $K_i$s is bigger than $\\frac{1}{2}{{i}\\choose{2}}$. It also found that the probability that an $OHC$ edge is contained in optimal $i$-vertex paths keeps stable or increases according to $i\\in [4, n]$. As the frequency $K_i$s are used to compute the frequency of an edge, each $OHC$ edge has its own peak frequency at $i=P_0$ where $P_0=\\frac{n}{2} + 2$ for even $n$ or $\\frac{n+1}{2} + 1$ for odd $n$. For ordinary edges out of $OHC$, the probability that they are contained in optimal $i$-vertex paths decreases according to $i$. Moreover, the average frequency of an ordinary edge will be smaller than $\\frac{1}{2}{{i}\\choose{2}}$ if $i \\geq [0.3660n + 1.5849]$. Based on these findings, an algorithm is presented to find $OHC$ in $O(n^62^{0.3660n})$ time using dynamic programming.","authors":["Yong Wang"],"url":"https://arxiv.org/abs/2504.19608"}
{"created":"2025-04-29","title":"Scene2Hap: Combining LLMs and Physical Modeling for Automatically Generating Vibrotactile Signals for Full VR Scenes","abstract":"Haptic feedback contributes to immersive virtual reality (VR) experiences. Designing such feedback at scale, for all objects within a VR scene and their respective arrangements, remains a time-consuming task. We present Scene2Hap, an LLM-centered system that automatically designs object-level vibrotactile feedback for entire VR scenes based on the objects' semantic attributes and physical context. Scene2Hap employs a multimodal large language model to estimate the semantics and physical context of each object, including its material properties and vibration behavior, from the multimodal information present in the VR scene. This semantic and physical context is then used to create plausible vibrotactile signals by generating or retrieving audio signals and converting them to vibrotactile signals. For the more realistic spatial rendering of haptics in VR, Scene2Hap estimates the propagation and attenuation of vibration signals from their source across objects in the scene, considering the estimated material properties and physical context, such as the distance and contact between virtual objects. Results from two user studies confirm that Scene2Hap successfully estimates the semantics and physical context of VR scenes, and the physical modeling of vibration propagation improves usability, perceived materiality, and spatial awareness.","authors":["Arata Jingu","Easa AliAbbasi","Paul Strohmeier","J\\\"urgen Steimle"],"url":"https://arxiv.org/abs/2504.19611"}
{"created":"2025-04-29","title":"DiVE: Efficient Multi-View Driving Scenes Generation Based on Video Diffusion Transformer","abstract":"Collecting multi-view driving scenario videos to enhance the performance of 3D visual perception tasks presents significant challenges and incurs substantial costs, making generative models for realistic data an appealing alternative. Yet, the videos generated by recent works suffer from poor quality and spatiotemporal consistency, undermining their utility in advancing perception tasks under driving scenarios. To address this gap, we propose DiVE, a diffusion transformer-based generative framework meticulously engineered to produce high-fidelity, temporally coherent, and cross-view consistent multi-view videos, aligning seamlessly with bird's-eye view layouts and textual descriptions. DiVE leverages a unified cross-attention and a SketchFormer to exert precise control over multimodal data, while incorporating a view-inflated attention mechanism that adds no extra parameters, thereby guaranteeing consistency across views. Despite these advancements, synthesizing high-resolution videos under multimodal constraints introduces dual challenges: investigating the optimal classifier-free guidance coniguration under intricate multi-condition inputs and mitigating excessive computational latency in high-resolution rendering--both of which remain underexplored in prior researches. To resolve these limitations, we introduce two innovations: Multi-Control Auxiliary Branch Distillation, which streamlines multi-condition CFG selection while circumventing high computational overhead, and Resolution Progressive Sampling, a training-free acceleration strategy that staggers resolution scaling to reduce high latency due to high resolution. These innovations collectively achieve a 2.62x speedup with minimal quality degradation. Evaluated on the nuScenes dataset, DiVE achieves SOTA performance in multi-view video generation, yielding photorealistic outputs with exceptional temporal and cross-view coherence.","authors":["Junpeng Jiang","Gangyi Hong","Miao Zhang","Hengtong Hu","Kun Zhan","Rui Shao","Liqiang Nie"],"url":"https://arxiv.org/abs/2504.19614"}
{"created":"2025-04-29","title":"AI Alignment in Medical Imaging: Unveiling Hidden Biases Through Counterfactual Analysis","abstract":"Machine learning (ML) systems for medical imaging have demonstrated remarkable diagnostic capabilities, but their susceptibility to biases poses significant risks, since biases may negatively impact generalization performance. In this paper, we introduce a novel statistical framework to evaluate the dependency of medical imaging ML models on sensitive attributes, such as demographics. Our method leverages the concept of counterfactual invariance, measuring the extent to which a model's predictions remain unchanged under hypothetical changes to sensitive attributes. We present a practical algorithm that combines conditional latent diffusion models with statistical hypothesis testing to identify and quantify such biases without requiring direct access to counterfactual data. Through experiments on synthetic datasets and large-scale real-world medical imaging datasets, including \\textsc{cheXpert} and MIMIC-CXR, we demonstrate that our approach aligns closely with counterfactual fairness principles and outperforms standard baselines. This work provides a robust tool to ensure that ML diagnostic systems generalize well, e.g., across demographic groups, offering a critical step towards AI safety in healthcare. Code: https://github.com/Neferpitou3871/AI-Alignment-Medical-Imaging.","authors":["Haroui Ma","Francesco Quinzan","Theresa Willem","Stefan Bauer"],"url":"https://arxiv.org/abs/2504.19621"}
{"created":"2025-04-29","title":"From Evidence to Belief: A Bayesian Epistemology Approach to Language Models","abstract":"This paper investigates the knowledge of language models from the perspective of Bayesian epistemology. We explore how language models adjust their confidence and responses when presented with evidence with varying levels of informativeness and reliability. To study these properties, we create a dataset with various types of evidence and analyze language models' responses and confidence using verbalized confidence, token probability, and sampling. We observed that language models do not consistently follow Bayesian epistemology: language models follow the Bayesian confirmation assumption well with true evidence but fail to adhere to other Bayesian assumptions when encountering different evidence types. Also, we demonstrated that language models can exhibit high confidence when given strong evidence, but this does not always guarantee high accuracy. Our analysis also reveals that language models are biased toward golden evidence and show varying performance depending on the degree of irrelevance, helping explain why they deviate from Bayesian assumptions.","authors":["Minsu Kim","Sangryul Kim","James Thorne"],"url":"https://arxiv.org/abs/2504.19622"}
{"created":"2025-04-29","title":"ARMOR: Adaptive Meshing with Reinforcement Optimization for Real-time 3D Monitoring in Unexposed Scenes","abstract":"Unexposed environments, such as lava tubes, mines, and tunnels, are among the most complex yet strategically significant domains for scientific exploration and infrastructure development. Accurate and real-time 3D meshing of these environments is essential for applications including automated structural assessment, robotic-assisted inspection, and safety monitoring. Implicit neural Signed Distance Fields (SDFs) have shown promising capabilities in online meshing; however, existing methods often suffer from large projection errors and rely on fixed reconstruction parameters, limiting their adaptability to complex and unstructured underground environments such as tunnels, caves, and lava tubes. To address these challenges, this paper proposes ARMOR, a scene-adaptive and reinforcement learning-based framework for real-time 3D meshing in unexposed environments. The proposed method was validated across more than 3,000 meters of underground environments, including engineered tunnels, natural caves, and lava tubes. Experimental results demonstrate that ARMOR achieves superior performance in real-time mesh reconstruction, reducing geometric error by 3.96\\% compared to state-of-the-art baselines, while maintaining real-time efficiency. The method exhibits improved robustness, accuracy, and adaptability, indicating its potential for advanced 3D monitoring and mapping in challenging unexposed scenarios. The project page can be found at: https://yizhezhang0418.github.io/armor.github.io/","authors":["Yizhe Zhang","Jianping Li","Xin Zhao","Fuxun Liang","Zhen Dong","Bisheng Yang"],"url":"https://arxiv.org/abs/2504.19624"}
{"created":"2025-04-29","title":"Rulebook: bringing co-routines to reinforcement learning environments","abstract":"Reinforcement learning (RL) algorithms, due to their reliance on external systems to learn from, require digital environments (e.g., simulators) with very simple interfaces, which in turn constrain significantly the implementation of such environments. In particular, these environments are implemented either as separate processes or as state machines, leading to synchronization and communication overheads in the first case, and to unstructured programming in the second.","authors":["Massimo Fioravanti","Samuele Pasini","Giovanni Agosta"],"url":"https://arxiv.org/abs/2504.19625"}
{"created":"2025-04-29","title":"VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning","abstract":"Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM.","authors":["Run Luo","Renke Shan","Longze Chen","Ziqiang Liu","Lu Wang","Min Yang","Xiaobo Xia"],"url":"https://arxiv.org/abs/2504.19627"}
{"created":"2025-04-29","title":"NSegment : Noisy Segment Improves Remote Sensing Image Segmentation","abstract":"Labeling errors in remote sensing (RS) image segmentation datasets often remain implicit and subtle due to ambiguous class boundaries, mixed pixels, shadows, complex terrain features, and subjective annotator bias. Furthermore, the scarcity of annotated RS data due to high image acquisition and labeling costs complicates training noise-robust models. While sophisticated mechanisms such as label selection or noise correction might address this issue, they tend to increase training time and add implementation complexity. In this letter, we propose NSegment-a simple yet effective data augmentation solution to mitigate this issue. Unlike traditional methods, it applies elastic transformations only to segmentation labels, varying deformation intensity per sample in each training epoch to address annotation inconsistencies. Experimental results demonstrate that our approach improves the performance of RS image segmentation on various state-of-the-art models.","authors":["Yechan Kim","DongHo Yoon","SooYeon Kim","Moongu Jeon"],"url":"https://arxiv.org/abs/2504.19634"}
{"created":"2025-04-29","title":"Diffusion Stochastic Learning Over Adaptive Competing Networks","abstract":"This paper studies a stochastic dynamic game between two competing teams, each consisting of a network of collaborating agents. Unlike fully cooperative settings, where all agents share a common objective, each team in this game aims to minimize its own distinct objective. In the adversarial setting, their objectives could be conflicting as in zero-sum games. Throughout the competition, agents share strategic information within their own team while simultaneously inferring and adapting to the strategies of the opposing team. We propose diffusion learning algorithms to address two important classes of this network game: i) a zero-sum game characterized by weak cross-team subgraph interactions, and ii) a general non-zero-sum game exhibiting strong cross-team subgraph interactions. We analyze the stability performance of the proposed algorithms under reasonable assumptions and illustrate the theoretical results through experiments on Cournot team competition and decentralized GAN training.","authors":["Yike Zhao","Haoyuan Cai","Ali H. Sayed"],"url":"https://arxiv.org/abs/2504.19635"}
{"created":"2025-04-29","title":"Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search","abstract":"Large Language Models (LLMs) have demonstrated significant potential in algorithm design. However, when integrated into search frameworks for iterative algorithm search, the underlying fitness landscape--critical for understanding search behaviou--remains underexplored. In this paper, we illustrate and analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, where nodes represent algorithms and edges denote transitions between them. We conduct extensive evaluations across six algorithm design tasks and six commonly used LLMs. Our findings reveal that LAS landscapes are highly multimodal and rugged, particularly in combinatorial optimization tasks, with distinct structural variations across tasks and LLMs. For instance, heuristic design tasks exhibit dense clusters of high-performing algorithms, while symbolic regression tasks show sparse, scattered distributions. Additionally, we demonstrate how population size influences exploration-exploitation trade-offs and the evolving trajectory of elite algorithms. These insights not only advance our understanding of LAS landscapes but also provide practical guidance for designing more effective LAS methods.","authors":["Fei Liu","Qingfu Zhang","Xialiang Tong","Mingxuan Yuan","Kun Mao"],"url":"https://arxiv.org/abs/2504.19636"}
{"created":"2025-04-29","title":"Exploiting Inter-Sample Correlation and Intra-Sample Redundancy for Partially Relevant Video Retrieval","abstract":"Partially Relevant Video Retrieval (PRVR) aims to retrieve the target video that is partially relevant to the text query. The primary challenge in PRVR arises from the semantic asymmetry between textual and visual modalities, as videos often contain substantial content irrelevant to the query. Existing methods coarsely align paired videos and text queries to construct the semantic space, neglecting the critical cross-modal dual nature inherent in this task: inter-sample correlation and intra-sample redundancy. To this end, we propose a novel PRVR framework to systematically exploit these two characteristics. Our framework consists of three core modules. First, the Inter Correlation Enhancement (ICE) module captures inter-sample correlation by identifying semantically similar yet unpaired text queries and video moments, combining them to form pseudo-positive pairs for more robust semantic space construction. Second, the Intra Redundancy Mining (IRM) module mitigates intra-sample redundancy by mining redundant video moment features and treating them as hard negative samples, thereby encouraging the model to learn more discriminative representations. Finally, to reinforce these modules, we introduce the Temporal Coherence Prediction (TCP) module, which enhances feature discrimination by training the model to predict the original temporal order of randomly shuffled video frames and moments. Extensive experiments on three datasets demonstrate the superiority of our approach compared to previous methods, achieving state-of-the-art results.","authors":["Junlong Ren","Gangjian Zhang","Yu Hu","Jian Shu","Hao Wang"],"url":"https://arxiv.org/abs/2504.19637"}
{"created":"2025-04-29","title":"LODAP: On-Device Incremental Learning Via Lightweight Operations and Data Pruning","abstract":"Incremental learning that learns new classes over time after the model's deployment is becoming increasingly crucial, particularly for industrial edge systems, where it is difficult to communicate with a remote server to conduct computation-intensive learning. As more classes are expected to learn after their execution for edge devices. In this paper, we propose LODAP, a new on-device incremental learning framework for edge systems. The key part of LODAP is a new module, namely Efficient Incremental Module (EIM). EIM is composed of normal convolutions and lightweight operations. During incremental learning, EIM exploits some lightweight operations, called adapters, to effectively and efficiently learn features for new classes so that it can improve the accuracy of incremental learning while reducing model complexity as well as training overhead. The efficiency of LODAP is further enhanced by a data pruning strategy that significantly reduces the training data, thereby lowering the training overhead. We conducted extensive experiments on the CIFAR-100 and Tiny- ImageNet datasets. Experimental results show that LODAP improves the accuracy by up to 4.32\\% over existing methods while reducing around 50\\% of model complexity. In addition, evaluations on real edge systems demonstrate its applicability for on-device machine learning. The code is available at https://github.com/duanbiqing/LODAP.","authors":["Biqing Duan","Qing Wang","Di Liu","Wei Zhou","Zhenli He","Shengfa Miao"],"url":"https://arxiv.org/abs/2504.19638"}
{"created":"2025-04-29","title":"A Unified Benchmark of Federated Learning with Kolmogorov-Arnold Networks for Medical Imaging","abstract":"Federated Learning (FL) enables model training across decentralized devices without sharing raw data, thereby preserving privacy in sensitive domains like healthcare. In this paper, we evaluate Kolmogorov-Arnold Networks (KAN) architectures against traditional MLP across six state-of-the-art FL algorithms on a blood cell classification dataset. Notably, our experiments demonstrate that KAN can effectively replace MLP in federated environments, achieving superior performance with simpler architectures. Furthermore, we analyze the impact of key hyperparameters-grid size and network architecture-on KAN performance under varying degrees of Non-IID data distribution. Additionally, our ablation studies reveal that optimizing KAN width while maintaining minimal depth yields the best performance in federated settings. As a result, these findings establish KAN as a promising alternative for privacy-preserving medical imaging applications in distributed healthcare. To the best of our knowledge, this is the first comprehensive benchmark of KAN in FL settings for medical imaging task.","authors":["Youngjoon Lee","Jinu Gong","Joonhyuk Kang"],"url":"https://arxiv.org/abs/2504.19639"}
{"created":"2025-04-29","title":"From Paper Trails to Trust on Tracks: Adding Public Transparency to Railways via zk-SNARKs","abstract":"Railways provide a critical service and operate under strict regulatory frameworks for implementing changes or upgrades. Despite their impact on the public, these frameworks do not define means or mechanisms for transparency towards the public, leading to reduced trust and complex tracking processes.","authors":["Tarek Galal","Valeria Tisch","Katja Assaf","Andreas Polze"],"url":"https://arxiv.org/abs/2504.19640"}
{"created":"2025-04-29","title":"BARIS: Boundary-Aware Refinement with Environmental Degradation Priors for Robust Underwater Instance Segmentation","abstract":"Underwater instance segmentation is challenging due to adverse visual conditions such as light attenuation, scattering, and color distortion, which degrade model performance. In this work, we propose BARIS-Decoder (Boundary-Aware Refinement Decoder for Instance Segmentation), a framework that enhances segmentation accuracy through feature refinement. To address underwater degradations, we introduce the Environmental Robust Adapter (ERA), which efficiently models underwater degradation patterns while reducing trainable parameters by over 90\\% compared to full fine-tuning. The integration of BARIS-Decoder with ERA-tuning, referred to as BARIS-ERA, achieves state-of-the-art performance, surpassing Mask R-CNN by 3.4 mAP with a Swin-B backbone and 3.8 mAP with ConvNeXt V2. Our findings demonstrate the effectiveness of BARIS-ERA in advancing underwater instance segmentation, providing a robust and efficient solution.","authors":["Pin-Chi Pan","Soo-Chang Pei"],"url":"https://arxiv.org/abs/2504.19643"}
{"created":"2025-04-29","title":"A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks","abstract":"- The field of natural language processing (NLP) has dramatically expanded within the last decade. Many human-being applications are conducted daily via NLP tasks, starting from machine translation, speech recognition, text generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity Recognition (NER). However, low-resourced languages, such as the Central-Kurdish language (CKL), mainly remain unexamined due to shortage of necessary resources to support their development. The POS tagging task is the base of other NLP tasks; for example, the POS tag set has been used to standardized languages to provide the relationship between words among the sentences, followed by machine translation and text recommendation. Specifically, for the CKL, most of the utilized or provided POS tagsets are neither standardized nor comprehensive. To this end, this study presented an accurate and comprehensive POS tagset for the CKL to provide better performance of the Kurdish NLP tasks. The article also collected most of the POS tags from different studies as well as from Kurdish linguistic experts to standardized part-of-speech tags. The proposed POS tagset is designed to annotate a large CKL corpus and support Kurdish NLP tasks. The initial investigations of this study via comparison with the Universal Dependencies framework for standard languages, show that the proposed POS tagset can streamline or correct sentences more accurately for Kurdish NLP tasks.","authors":["Shadan Shukr Sabr","Nazira Sabr Mustafa","Talar Sabah Omar","Salah Hwayyiz Rasool","Nawzad Anwer Omer","Darya Sabir Hamad","Hemin Abdulhameed Shams","Omer Mahmood Kareem","Rozhan Noori Abdullah","Khabat Atar Abdullah","Mahabad Azad Mohammad","Haneen Al-Raghefy","Safar M. Asaad","Sara Jamal Mohammed","Twana Saeed Ali","Fazil Shawrow","Halgurd S. Maghdid"],"url":"https://arxiv.org/abs/2504.19645"}
{"created":"2025-04-29","title":"xEdgeFace: Efficient Cross-Spectral Face Recognition for Edge Devices","abstract":"Heterogeneous Face Recognition (HFR) addresses the challenge of matching face images across different sensing modalities, such as thermal to visible or near-infrared to visible, expanding the applicability of face recognition systems in real-world, unconstrained environments. While recent HFR methods have shown promising results, many rely on computation-intensive architectures, limiting their practicality for deployment on resource-constrained edge devices. In this work, we present a lightweight yet effective HFR framework by adapting a hybrid CNN-Transformer architecture originally designed for face recognition. Our approach enables efficient end-to-end training with minimal paired heterogeneous data while preserving strong performance on standard RGB face recognition tasks. This makes it a compelling solution for both homogeneous and heterogeneous scenarios. Extensive experiments across multiple challenging HFR and face recognition benchmarks demonstrate that our method consistently outperforms state-of-the-art approaches while maintaining a low computational overhead.","authors":["Anjith George","Sebastien Marcel"],"url":"https://arxiv.org/abs/2504.19646"}
{"created":"2025-04-29","title":"Resonances and computations","abstract":"The computation of time dynamics arising in nonlinear time-dependent partial differential equations is an ongoing challenge in numerical analysis, especially once roughness comes into play. Classical numerical schemes in general fail to resolve the oscillatory behaviour in the solution which leads to numerical instabilities and loss of convergence. Dispersive equations, e.g., nonlinear Schr\\\"odinger, Korteweg--de Vries and wave equations, thereby pose in particular a big problem as in contrast to the parabolic setting, no strong smoothing can be expected, i.e., if the initial data is rough, the solution stays rough which makes their approximation a delicate task. In this review we give an overview on a new numerical ansatz which aims to tackle the time dynamics of nonlinear dispersive partial differential equations even for very rough data. This is achieved by a resonance analysis and decorated tree formalism that draws its inpiration from the combinatorics used in the theory of regularity structures for solving singular SPDEs. One can hope to see this formalism applied in other contexts for dispersive PDEs and beyond.","authors":["Yvain Bruned","Fr\\'ed\\'eric Rousset","Katharina Schratz"],"url":"https://arxiv.org/abs/2504.19647"}
{"created":"2025-04-29","title":"Intelligent4DSE: Optimizing High-Level Synthesis Design Space Exploration with Graph Neural Networks and Large Language Models","abstract":"High-level synthesis (HLS) design space exploration (DSE) is an optimization process in electronic design automation (EDA) that systematically explores high-level design configurations to achieve Pareto-optimal hardware implementations balancing performance, area, and power (PPA). To optimize this process, HLS prediction tasks often employ message-passing neural networks (MPNNs), leveraging complex architectures to achieve high accuracy. These predictors serve as evaluators in the DSE process, effectively bypassing the time-consuming estimations traditionally required by HLS tools. However, existing models often prioritize structural complexity and minimization of training loss, overlooking task-specific characteristics. Additionally, while evolutionary algorithms are widely used in DSE, they typically require extensive domain-specific knowledge to design effective crossover and mutation operators. To address these limitations, we propose CoGNNs-LLMEA, a framework that integrates a graph neural network with task-adaptive message passing and a large language model-enhanced evolutionary algorithm. As a predictive model, CoGNNs directly leverages intermediate representations generated from source code after compiler front-end processing, enabling prediction of quality of results (QoR) without invoking HLS tools. Due to its strong adaptability to tasks, CoGNNs can be tuned to predict post-HLS and post-implementation outcomes, effectively bridging the gap between high-level abstractions and physical implementation characteristics. CoGNNs achieves state-of-the-art prediction accuracy in post-HLS QoR prediction, reducing mean prediction errors by 2.8$\\times$ for latency and 3.4$\\times$ for resource utilization compared to baseline models.","authors":["Lei Xu","Shanshan Wang","Emmanuel Casseau","Chenglong Xiao"],"url":"https://arxiv.org/abs/2504.19649"}
{"created":"2025-04-29","title":"Robot Motion Planning using One-Step Diffusion with Noise-Optimized Approximate Motions","abstract":"This paper proposes an image-based robot motion planning method using a one-step diffusion model. While the diffusion model allows for high-quality motion generation, its computational cost is too expensive to control a robot in real time. To achieve high quality and efficiency simultaneously, our one-step diffusion model takes an approximately generated motion, which is predicted directly from input images. This approximate motion is optimized by additive noise provided by our novel noise optimizer. Unlike general isotropic noise, our noise optimizer adjusts noise anisotropically depending on the uncertainty of each motion element. Our experimental results demonstrate that our method outperforms state-of-the-art methods while maintaining its efficiency by one-step diffusion.","authors":["Tomoharu Aizu","Takeru Oba","Yuki Kondo","Norimichi Ukita"],"url":"https://arxiv.org/abs/2504.19652"}
{"created":"2025-04-29","title":"GAN-SLAM: Real-Time GAN Aided Floor Plan Creation Through SLAM","abstract":"SLAM is a fundamental component of modern autonomous systems, providing robots and their operators with a deeper understanding of their environment. SLAM systems often encounter challenges due to the dynamic nature of robotic motion, leading to inaccuracies in mapping quality, particularly in 2D representations such as Occupancy Grid Maps. These errors can significantly degrade map quality, hindering the effectiveness of specific downstream tasks such as floor plan creation. To address this challenge, we introduce our novel 'GAN-SLAM', a new SLAM approach that leverages Generative Adversarial Networks to clean and complete occupancy grids during the SLAM process, reducing the impact of noise and inaccuracies introduced on the output map. We adapt and integrate accurate pose estimation techniques typically used for 3D SLAM into a 2D form. This enables the quality improvement 3D LiDAR-odometry has seen in recent years to be effective for 2D representations. Our results demonstrate substantial improvements in map fidelity and quality, with minimal noise and errors, affirming the effectiveness of GAN-SLAM for real-world mapping applications within large-scale complex environments. We validate our approach on real-world data operating in real-time, and on famous examples of 2D maps. The improved quality of the output map enables new downstream tasks, such as floor plan drafting, further enhancing the capabilities of autonomous systems. Our novel approach to SLAM offers a significant step forward in the field, improving the usability for SLAM in mapping-based tasks, and offers insight into the usage of GANs for OGM error correction.","authors":["Leon Davies","Baihua Li","Mohamad Saada","Simon S{\\o}lvsten","Qinggang Meng"],"url":"https://arxiv.org/abs/2504.19653"}
{"created":"2025-04-29","title":"Transformation & Translation Occupancy Grid Mapping: 2-Dimensional Deep Learning Refined SLAM","abstract":"SLAM (Simultaneous Localisation and Mapping) is a crucial component for robotic systems, providing a map of an environment, the current location and previous trajectory of a robot. While 3D LiDAR SLAM has received notable improvements in recent years, 2D SLAM lags behind. Gradual drifts in odometry and pose estimation inaccuracies hinder modern 2D LiDAR-odometry algorithms in large complex environments. Dynamic robotic motion coupled with inherent estimation based SLAM processes introduce noise and errors, degrading map quality. Occupancy Grid Mapping (OGM) produces results that are often noisy and unclear. This is due to the fact that evidence based mapping represents maps according to uncertain observations. This is why OGMs are so popular in exploration or navigation tasks. However, this also limits OGMs' effectiveness for specific mapping based tasks such as floor plan creation in complex scenes. To address this, we propose our novel Transformation and Translation Occupancy Grid Mapping (TT-OGM). We adapt and enable accurate and robust pose estimation techniques from 3D SLAM to the world of 2D and mitigate errors to improve map quality using Generative Adversarial Networks (GANs). We introduce a novel data generation method via deep reinforcement learning (DRL) to build datasets large enough for training a GAN for SLAM error correction. We demonstrate our SLAM in real-time on data collected at Loughborough University. We also prove its generalisability on a variety of large complex environments on a collection of large scale well-known 2D occupancy maps. Our novel approach enables the creation of high quality OGMs in complex scenes, far surpassing the capabilities of current SLAM algorithms in terms of quality, accuracy and reliability.","authors":["Leon Davies","Baihua Li","Mohamad Saada","Simon S{\\o}lvsten","Qinggang Meng"],"url":"https://arxiv.org/abs/2504.19654"}
{"created":"2025-04-29","title":"Auxiliary Artifacts in Requirements Traceability: A Systematic Mapping Study","abstract":"Background: Traceability between software artifacts enhances the value of the information those artifacts contain, but only when the links themselves are reliable. Link quality is known to depend on explicit factors such as the traced artifacts and the expertise of the practitioner who judges each connection. Other factors, however, remain largely unexplored. We contend that one of these factors is the set of auxiliary artifacts -- artifacts that are produced and/or used during the tracing process yet are neither the source nor target artifacts. Because such auxiliary artifacts can subtly steer how links are created and validated, they merit a literature survey to identify these artifacts and further investigate them. Objective: We identify and map auxiliary artifacts used in requirements tracing, which could be additional factors that affect the quality of the trace links. Method: We conducted a systematic mapping study on auxiliary artifacts in requirements traceability. Results: We found 110 studies in which auxiliary artifacts are used in requirements tracing, and identified 49 auxiliary artifacts, and 13 usage scenarios. Conclusion: This study provides a systematic mapping of auxiliary artifacts in requirement tracing, including their usage, origin, type and tool support. The use of auxiliary artifacts in requirements tracing seems to be the norm, thus, these artifacts should be studied in depth to identify how they effect the quality of traced links.","authors":["Waleed Abdeen","Michael Unterkalmsteiner","Krzysztof Wnuk"],"url":"https://arxiv.org/abs/2504.19658"}
{"created":"2025-04-29","title":"Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs","abstract":"The customizability of RISC-V makes it an attractive choice for accelerating deep neural networks (DNNs). It can be achieved through instruction set extensions and corresponding custom functional units. Yet, efficiently exploiting these opportunities requires a hardware/software co-design approach in which the DNN model, software, and hardware are designed together. In this paper, we propose novel RISC-V extensions for accelerating DNN models containing semi-structured and unstructured sparsity. While the idea of accelerating structured and unstructured pruning is not new, our novel design offers various advantages over other designs. To exploit semi-structured sparsity, we take advantage of the fine-grained (bit-level) configurability of FPGAs and suggest reserving a few bits in a block of DNN weights to encode the information about sparsity in the succeeding blocks. The proposed custom functional unit utilizes this information to skip computations. To exploit unstructured sparsity, we propose a variable cycle sequential multiply-and-accumulate unit that performs only as many multiplications as the non-zero weights. Our implementation of unstructured and semi-structured pruning accelerators can provide speedups of up to a factor of 3 and 4, respectively. We then propose a combined design that can accelerate both types of sparsities, providing speedups of up to a factor of 5. Our designs consume a small amount of additional FPGA resources such that the resulting co-designs enable the acceleration of DNNs even on small FPGAs. We benchmark our designs on standard TinyML applications such as keyword spotting, image classification, and person detection.","authors":["Muhammad Sabih","Abrarul Karim","Jakob Wittmann","Frank Hannig","J\\\"urgen Teich"],"url":"https://arxiv.org/abs/2504.19659"}
{"created":"2025-04-29","title":"Decentralization of Generative AI via Mixture of Experts for Wireless Networks: A Comprehensive Survey","abstract":"Mixture of Experts (MoE) has emerged as a promising paradigm for scaling model capacity while preserving computational efficiency, particularly in large-scale machine learning architectures such as large language models (LLMs). Recent advances in MoE have facilitated its adoption in wireless networks to address the increasing complexity and heterogeneity of modern communication systems. This paper presents a comprehensive survey of the MoE framework in wireless networks, highlighting its potential in optimizing resource efficiency, improving scalability, and enhancing adaptability across diverse network tasks. We first introduce the fundamental concepts of MoE, including various gating mechanisms and the integration with generative AI (GenAI) and reinforcement learning (RL). Subsequently, we discuss the extensive applications of MoE across critical wireless communication scenarios, such as vehicular networks, unmanned aerial vehicles (UAVs), satellite communications, heterogeneous networks, integrated sensing and communication (ISAC), and mobile edge networks. Furthermore, key applications in channel prediction, physical layer signal processing, radio resource management, network optimization, and security are thoroughly examined. Additionally, we present a detailed overview of open-source datasets that are widely used in MoE-based models to support diverse machine learning tasks. Finally, this survey identifies crucial future research directions for MoE, emphasizing the importance of advanced training techniques, resource-aware gating strategies, and deeper integration with emerging 6G technologies.","authors":["Yunting Xu","Jiacheng Wang","Ruichen Zhang","Changyuan Zhao","Dusit Niyato","Jiawen Kang","Zehui Xiong","Bo Qian","Haibo Zhou","Shiwen Mao","Abbas Jamalipour","Xuemin Shen","Dong In Kim"],"url":"https://arxiv.org/abs/2504.19660"}
{"created":"2025-04-29","title":"Ariel OS: An Embedded Rust Operating System for Networked Sensors & Multi-Core Microcontrollers","abstract":"Large swaths of low-level system software building blocks originally implemented in C/C++ are currently being swapped for equivalent rewrites in Rust, a relatively more secure and dependable programming language. So far, however, no embedded OS in Rust supports multicore preemptive scheduling on microcontrollers. In this paper, we thus fill this gap with a new operating system: Ariel OS. We describe its design, we provide the source code of its implementation, and we perform micro-benchmarks on the main 32-bit microcontroller architectures: ARM Cortex-M, RISC-V and Espressif Xtensa. We show how our scheduler takes advantage of several cores, while incurring only small overhead on single-core hardware. As such, Ariel OS provides a convenient embedded software platform for small networked devices, for both research and industry practitioners.","authors":["Elena Frank","Kaspar Schleiser","Romain Fouquet","Koen Zandberg","Christian Ams\\\"uss","Emmanuel Baccelli"],"url":"https://arxiv.org/abs/2504.19662"}
{"created":"2025-04-29","title":"A Tripartite Perspective on GraphRAG","abstract":"Large Language Models (LLMs) have shown remarkable capabilities across various domains, yet they struggle with knowledge-intensive tasks in areas that demand factual accuracy, e.g. industrial automation and healthcare. Key limitations include their tendency to hallucinate, lack of source traceability (provenance), and challenges in timely knowledge updates. Combining language models with knowledge graphs (GraphRAG) offers promising avenues for overcoming these deficits. However, a major challenge lies in creating such a knowledge graph in the first place. Here, we propose a novel approach that combines LLMs with a tripartite knowledge graph representation, which is constructed by connecting complex, domain-specific objects via a curated ontology of corresponding, domain-specific concepts to relevant sections within chunks of text through a concept-anchored pre-analysis of source documents starting from an initial lexical graph. As a consequence, our Tripartite-GraphRAG approach implements: i) a concept-specific, information-preserving pre-compression of textual chunks; ii) allows for the formation of a concept-specific relevance estimation of embedding similarities grounded in statistics; and iii) avoids common challenges w.r.t. continuous extendability, such as the need for entity resolution and deduplication. By applying a transformation to the knowledge graph, we formulate LLM prompt creation as an unsupervised node classification problem, drawing on ideas from Markov Random Fields. We evaluate our approach on a healthcare use case, involving multi-faceted analyses of patient anamneses given a set of medical concepts as well as clinical literature. Experiments indicate that it can optimize information density, coverage, and arrangement of LLM prompts while reducing their lengths, which may lead to reduced costs and more consistent and reliable LLM outputs.","authors":["Michael Banf","Johannes Kuhn"],"url":"https://arxiv.org/abs/2504.19667"}
{"created":"2025-04-29","title":"Multimodal Conditioned Diffusive Time Series Forecasting","abstract":"Diffusion models achieve remarkable success in processing images and text, and have been extended to special domains such as time series forecasting (TSF). Existing diffusion-based approaches for TSF primarily focus on modeling single-modality numerical sequences, overlooking the rich multimodal information in time series data. To effectively leverage such information for prediction, we propose a multimodal conditioned diffusion model for TSF, namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for time series modeling, especially for forecasting. Specifically, Timestamps are combined with time series to establish temporal and semantic correlations among different data points when aggregating information along the temporal dimension. Texts serve as supplementary descriptions of time series' history, and adaptively aligned with data points as well as dynamically controlled in a classifier-free manner. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed MCD-TSF model achieves state-of-the-art performance.","authors":["Chen Su","Yuanhe Tian","Yan Song"],"url":"https://arxiv.org/abs/2504.19669"}
{"created":"2025-04-29","title":"Generative AI in Education: Student Skills and Lecturer Roles","abstract":"Generative Artificial Intelligence (GenAI) tools such as ChatGPT are emerging as a revolutionary tool in education that brings both positive aspects and challenges for educators and students, reshaping how learning and teaching are approached. This study aims to identify and evaluate the key competencies students need to effectively engage with GenAI in education and to provide strategies for lecturers to integrate GenAI into teaching practices. The study applied a mixed method approach with a combination of a literature review and a quantitative survey involving 130 students from South Asia and Europe to obtain its findings. The literature review identified 14 essential student skills for GenAI engagement, with AI literacy, critical thinking, and ethical AI practices emerging as the most critical. The student survey revealed gaps in prompt engineering, bias awareness, and AI output management. In our study of lecturer strategies, we identified six key areas, with GenAI Integration and Curriculum Design being the most emphasised. Our findings highlight the importance of incorporating GenAI into education. While literature prioritized ethics and policy development, students favour hands-on, project-based learning and practical AI applications. To foster inclusive and responsible GenAI adoption, institutions should ensure equitable access to GenAI tools, establish clear academic integrity policies, and advocate for global GenAI research initiatives.","authors":["Stefanie Krause","Ashish Dalvi","Syed Khubaib Zaidi"],"url":"https://arxiv.org/abs/2504.19673"}
{"created":"2025-04-29","title":"$\\texttt{SAGE}$: A Generic Framework for LLM Safety Evaluation","abstract":"Safety evaluation of Large Language Models (LLMs) has made progress and attracted academic interest, but it remains challenging to keep pace with the rapid integration of LLMs across diverse applications. Different applications expose users to various harms, necessitating application-specific safety evaluations with tailored harms and policies. Another major gap is the lack of focus on the dynamic and conversational nature of LLM systems. Such potential oversights can lead to harms that go unnoticed in standard safety benchmarks. This paper identifies the above as key requirements for robust LLM safety evaluation and recognizing that current evaluation methodologies do not satisfy these, we introduce the $\\texttt{SAGE}$ (Safety AI Generic Evaluation) framework. $\\texttt{SAGE}$ is an automated modular framework designed for customized and dynamic harm evaluations. It utilizes adversarial user models that are system-aware and have unique personalities, enabling a holistic red-teaming evaluation. We demonstrate $\\texttt{SAGE}$'s effectiveness by evaluating seven state-of-the-art LLMs across three applications and harm policies. Our experiments with multi-turn conversational evaluations revealed a concerning finding that harm steadily increases with conversation length. Furthermore, we observe significant disparities in model behavior when exposed to different user personalities and scenarios. Our findings also reveal that some models minimize harmful outputs by employing severe refusal tactics that can hinder their usefulness. These insights highlight the necessity of adaptive and context-specific testing to ensure better safety alignment and safer deployment of LLMs in real-world scenarios.","authors":["Madhur Jindal","Hari Shrawgi","Parag Agrawal","Sandipan Dandapat"],"url":"https://arxiv.org/abs/2504.19674"}
{"created":"2025-04-29","title":"Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs","abstract":"This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts.","authors":["Osma Suominen","Juho Inkinen","Mona Lehtinen"],"url":"https://arxiv.org/abs/2504.19675"}
{"created":"2025-04-29","title":"From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review","abstract":"Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.","authors":["Mohamed Amine Ferrag","Norbert Tihanyi","Merouane Debbah"],"url":"https://arxiv.org/abs/2504.19678"}
{"created":"2025-04-29","title":"Explaining Vision GNNs: A Semantic and Visual Analysis of Graph-based Image Classification","abstract":"Graph Neural Networks (GNNs) have emerged as an efficient alternative to convolutional approaches for vision tasks such as image classification, leveraging patch-based representations instead of raw pixels. These methods construct graphs where image patches serve as nodes, and edges are established based on patch similarity or classification relevance. Despite their efficiency, the explainability of GNN-based vision models remains underexplored, even though graphs are naturally interpretable. In this work, we analyze the semantic consistency of the graphs formed at different layers of GNN-based image classifiers, focusing on how well they preserve object structures and meaningful relationships. A comprehensive analysis is presented by quantifying the extent to which inter-layer graph connections reflect semantic similarity and spatial coherence. Explanations from standard and adversarial settings are also compared to assess whether they reflect the classifiers' robustness. Additionally, we visualize the flow of information across layers through heatmap-based visualization techniques, thereby highlighting the models' explainability. Our findings demonstrate that the decision-making processes of these models can be effectively explained, while also revealing that their reasoning does not necessarily align with human perception, especially in deeper layers.","authors":["Nikolaos Chaidos","Angeliki Dimitriou","Nikolaos Spanos","Athanasios Voulodimos","Giorgos Stamou"],"url":"https://arxiv.org/abs/2504.19682"}
{"created":"2025-04-29","title":"GPA-RAM: Grasp-Pretraining Augmented Robotic Attention Mamba for Spatial Task Learning","abstract":"Most existing robot manipulation methods prioritize task learning by enhancing perception through complex deep network architectures. However, they face challenges in real-time collision-free planning. Hence, Robotic Attention Mamba (RAM) is designed for refined planning. Specifically, by integrating Mamba and parallel single-view attention, RAM aligns multi-view vision and task-related language features, ensuring efficient fine-grained task planning with linear complexity and robust real-time performance. Nevertheless, it has the potential for further improvement in high-precision grasping and manipulation. Thus, Grasp-Pretraining Augmentation (GPA) is devised, with a grasp pose feature extractor pretrained utilizing object grasp poses directly inherited from whole-task demonstrations. Subsequently, the extracted grasp features are fused with the spatially aligned planning features from RAM through attention-based Pre-trained Location Fusion, preserving high-resolution grasping cues overshadowed by an overemphasis on global planning. To summarize, we propose Grasp-Pretraining Augmented Robotic Attention Mamba (GPA-RAM), dividing spatial task learning into RAM for planning skill learning and GPA for grasping skill learning. GPA-RAM demonstrates superior performance across three robot systems with distinct camera configurations in simulation and the real world. Compared with previous state-of-the-art methods, it improves the absolute success rate by 8.2% (from 79.3% to 87.5%) on the RLBench multi-task benchmark and 40\\% (from 16% to 56%), 12% (from 86% to 98%) on the ALOHA bimanual manipulation tasks, while delivering notably faster inference. Furthermore, experimental results demonstrate that both RAM and GPA enhance task learning, with GPA proving robust to different architectures of pretrained grasp pose feature extractors. The website is: https://logssim.github.io/GPA\\_RAM\\_website/.","authors":["Juyi Sheng","Yangjun Liu","Sheng Xu","Zhixin Yang","Mengyuan Liu"],"url":"https://arxiv.org/abs/2504.19683"}
{"created":"2025-04-29","title":"ClearVision: Leveraging CycleGAN and SigLIP-2 for Robust All-Weather Classification in Traffic Camera Imagery","abstract":"Accurate weather classification from low-quality traffic camera imagery remains a challenging task, particularly under adverse nighttime conditions. In this study, we propose a scalable framework that combines generative domain adaptation with efficient contrastive learning to enhance classification performance. Using CycleGAN-based domain translation, we improve the quality of nighttime images, enabling better feature extraction by downstream models. While the baseline EVA-02 model employing CLIP-based contrastive loss achieves an overall accuracy of 96.55\\%, it exhibits a significant performance gap between daytime (97.21\\%) and nighttime conditions (63.40\\%). Replacing CLIP with the lightweight SigLIP-2 (Sigmoid contrastive loss) achieves a competitive overall accuracy of 94.00\\%, with substantial improvements in nighttime performance (85.90\\% accuracy). The combination of Vision-SigLIP-2, Text-SigLIP-2, CycleGAN, and contrastive training achieves the best nighttime accuracy (85.90\\%) among all models tested, while EVA-02 with CycleGAN maintains the highest overall accuracy (97.01\\%) and per-class accuracies. These findings demonstrate the potential of combining domain adaptation and efficient contrastive learning to build practical, resource-efficient weather classification systems for intelligent transportation infrastructure.","authors":["Anush Lakshman Sivaraman","Kojo Adu-Gyamfi","Ibne Farabi Shihab","Anuj Sharma"],"url":"https://arxiv.org/abs/2504.19684"}
{"created":"2025-04-29","title":"Tensegrity-based Robot Leg Design with Variable Stiffness","abstract":"Animals can finely modulate their leg stiffness to interact with complex terrains and absorb sudden shocks. In feats like leaping and sprinting, animals demonstrate a sophisticated interplay of opposing muscle pairs that actively modulate joint stiffness, while tendons and ligaments act as biological springs storing and releasing energy. Although legged robots have achieved notable progress in robust locomotion, they still lack the refined adaptability inherent in animal motor control. Integrating mechanisms that allow active control of leg stiffness presents a pathway towards more resilient robotic systems. This paper proposes a novel mechanical design to integrate compliancy into robot legs based on tensegrity - a structural principle that combines flexible cables and rigid elements to balance tension and compression. Tensegrity structures naturally allow for passive compliance, making them well-suited for absorbing impacts and adapting to diverse terrains. Our design features a robot leg with tensegrity joints and a mechanism to control the joint's rotational stiffness by modulating the tension of the cable actuation system. We demonstrate that the robot leg can reduce the impact forces of sudden shocks by at least 34.7 % and achieve a similar leg flexion under a load difference of 10.26 N by adjusting its stiffness configuration. The results indicate that tensegrity-based leg designs harbors potential towards more resilient and adaptable legged robots.","authors":["Erik Mortensen","Jan Petrs","Alexander Dittrich","Dario Floreano"],"url":"https://arxiv.org/abs/2504.19685"}
{"created":"2025-04-29","title":"Prompt Guiding Multi-Scale Adaptive Sparse Representation-driven Network for Low-Dose CT MAR","abstract":"Low-dose CT (LDCT) is capable of reducing X-ray radiation exposure, but it will potentially degrade image quality, even yields metal artifacts at the case of metallic implants. For simultaneous LDCT reconstruction and metal artifact reduction (LDMAR), existing deep learning-based efforts face two main limitations: i) the network design neglects multi-scale and within-scale information; ii) training a distinct model for each dose necessitates significant storage space for multiple doses. To fill these gaps, we propose a prompt guiding multi-scale adaptive sparse representation-driven network, abbreviated as PMSRNet, for LDMAR task. Specifically, we construct PMSRNet inspired from multi-scale sparsifying frames, and it can simultaneously employ within-scale characteristics and cross-scale complementarity owing to an elaborated prompt guiding scale-adaptive threshold generator (PSATG) and a built multi-scale coefficient fusion module (MSFuM). The PSATG can adaptively capture multiple contextual information to generate more faithful thresholds, achieved by fusing features from local, regional, and global levels. Furthermore, we elaborate a model interpretable dual domain LDMAR framework called PDuMSRNet, and train single model with a prompt guiding strategy for multiple dose levels. We build a prompt guiding module, whose input contains dose level, metal mask and input instance, to provide various guiding information, allowing a single model to accommodate various CT dose settings. Extensive experiments at various dose levels demonstrate that the proposed methods outperform the state-of-the-art LDMAR methods.","authors":["Baoshun Shi","Bing Chen","Shaolei Zhang","Huazhu Fu","Zhanli Hu"],"url":"https://arxiv.org/abs/2504.19687"}
{"created":"2025-04-29","title":"Data-Driven Sensor Fault Diagnosis with Proven Guarantees using Incrementally Stable Recurrent Neural Networks","abstract":"Robust Recurrent Neural Networks (R-RENs) are a class of neural networks that have built-in system-theoretic robustness and incremental stability properties. In this manuscript, we leverage these properties to construct a data-driven Fault Detection and Isolation (FDI) method for sensor faults with proven performance guarantees. The underlying idea behind the scheme is to construct a bank of multiple R-RENs (acting as fault isolation filters), each with different levels of sensitivity (increased or decreased) to faults at different sensors. That is, each R-REN is designed to be specifically sensitive to faults occurring in a particular sensor and robust against faults in all the others. The latter is guaranteed using the built-in incremental stability properties of R-RENs. The proposed method is unsupervised (as it does not require labeled data from faulty sensors) and data-driven (because it exploits available fault-free input-output system trajectories and does not rely on dynamic models of the system under study). Numerical simulations on a roll-plane model of a vehicle demonstrate the effectiveness and practical applicability of the proposed methodology.","authors":["Farhad Ghanipoor","Carlos Murguia","Giancarlo Ferrari Trecate","Nathan van de Wouw"],"url":"https://arxiv.org/abs/2504.19688"}
{"created":"2025-04-29","title":"SubGrapher: Visual Fingerprinting of Chemical Structures","abstract":"Automatic extraction of chemical structures from scientific literature plays a crucial role in accelerating research across fields ranging from drug discovery to materials science. Patent documents, in particular, contain molecular information in visual form, which is often inaccessible through traditional text-based searches. In this work, we introduce SubGrapher, a method for the visual fingerprinting of chemical structure images. Unlike conventional Optical Chemical Structure Recognition (OCSR) models that attempt to reconstruct full molecular graphs, SubGrapher focuses on extracting molecular fingerprints directly from chemical structure images. Using learning-based instance segmentation, SubGrapher identifies functional groups and carbon backbones, constructing a substructure-based fingerprint that enables chemical structure retrieval. Our approach is evaluated against state-of-the-art OCSR and fingerprinting methods, demonstrating superior retrieval performance and robustness across diverse molecular depictions. The dataset, models, and code will be made publicly available.","authors":["Lucas Morin","Gerhard Ingmar Meijer","Val\\'ery Weber","Luc Van Gool","Peter W. J. Staar"],"url":"https://arxiv.org/abs/2504.19695"}
{"created":"2025-04-29","title":"Interactive Discovery and Exploration of Visual Bias in Generative Text-to-Image Models","abstract":"Bias in generative Text-to-Image (T2I) models is a known issue, yet systematically analyzing such models' outputs to uncover it remains challenging. We introduce the Visual Bias Explorer (ViBEx) to interactively explore the output space of T2I models to support the discovery of visual bias. ViBEx introduces a novel flexible prompting tree interface in combination with zero-shot bias probing using CLIP for quick and approximate bias exploration. It additionally supports in-depth confirmatory bias analysis through visual inspection of forward, intersectional, and inverse bias queries. ViBEx is model-agnostic and publicly available. In four case study interviews, experts in AI and ethics were able to discover visual biases that have so far not been described in literature.","authors":["Johannes Eschner","Roberto Labadie-Tamayo","Matthias Zeppelzauer","Manuela Waldner"],"url":"https://arxiv.org/abs/2504.19703"}
{"created":"2025-04-29","title":"Guided Tensor Lifting","abstract":"Domain-specific languages (DSLs) for machine learning are revolutionizing the speed and efficiency of machine learning workloads as they enable users easy access to high-performance compiler optimizations and accelerators. However, to take advantage of these capabilities, a user must first translate their legacy code from the language it is currently written in, into the new DSL. The process of automatically lifting code into these DSLs has been identified by several recent works, which propose program synthesis as a solution. However, synthesis is expensive and struggles to scale without carefully designed and hard-wired heuristics. In this paper, we present an approach for lifting that combines an enumerative synthesis approach with a Large Language Model used to automatically learn the domain-specific heuristics for program lifting, in the form of a probabilistic grammar. Our approach outperforms the state-of-the-art tools in this area, despite only using learned heuristics.","authors":["Yixuan Li","Jos\\'e Wesley de Souza Magalh\\~aes","Alexander Brauckmann","Michael F. P. O'Boyle","Elizabeth Polgreen"],"url":"https://arxiv.org/abs/2504.19705"}
{"created":"2025-04-29","title":"Open-set Anomaly Segmentation in Complex Scenarios","abstract":"Precise segmentation of out-of-distribution (OoD) objects, herein referred to as anomalies, is crucial for the reliable deployment of semantic segmentation models in open-set, safety-critical applications, such as autonomous driving. Current anomalous segmentation benchmarks predominantly focus on favorable weather conditions, resulting in untrustworthy evaluations that overlook the risks posed by diverse meteorological conditions in open-set environments, such as low illumination, dense fog, and heavy rain. To bridge this gap, this paper introduces the ComsAmy, a challenging benchmark specifically designed for open-set anomaly segmentation in complex scenarios. ComsAmy encompasses a wide spectrum of adverse weather conditions, dynamic driving environments, and diverse anomaly types to comprehensively evaluate the model performance in realistic open-world scenarios. Our extensive evaluation of several state-of-the-art anomalous segmentation models reveals that existing methods demonstrate significant deficiencies in such challenging scenarios, highlighting their serious safety risks for real-world deployment. To solve that, we propose a novel energy-entropy learning (EEL) strategy that integrates the complementary information from energy and entropy to bolster the robustness of anomaly segmentation under complex open-world environments. Additionally, a diffusion-based anomalous training data synthesizer is proposed to generate diverse and high-quality anomalous images to enhance the existing copy-paste training data synthesizer. Extensive experimental results on both public and ComsAmy benchmarks demonstrate that our proposed diffusion-based synthesizer with energy and entropy learning (DiffEEL) serves as an effective and generalizable plug-and-play method to enhance existing models, yielding an average improvement of around 4.96% in $\\rm{AUPRC}$ and 9.87% in $\\rm{FPR}_{95}$.","authors":["Song Xia","Yi Yu","Henghui Ding","Wenhan Yang","Shifei Liu","Alex C. Kot","Xudong Jiang"],"url":"https://arxiv.org/abs/2504.19706"}
{"created":"2025-04-29","title":"Teaching Energy-Efficient Software -- An Experience Report","abstract":"Environmental sustainability is a major and relevant challenge facing computing. Therefore, we must start teaching theory, techniques, and practices that both increase an awareness in our student population as well a provide concrete advice to be applied in practical software development. In this experience report, we focus on energy consumption of executing software, and describe teaching approaches from three different universities that all address software energy consumption in various ways. Our main contribution is reporting lessons learned from these experiences and sketching some issues that teachers must be aware of when designing learning goals, teaching material and exercises.","authors":["Henrik B{\\ae}rbak Christensen","Maja Hanne Kirkeby","Bent Thomsen","Lone Leth Thomsen"],"url":"https://arxiv.org/abs/2504.19707"}
{"created":"2025-04-29","title":"Model-based controller assisted domain randomization in deep reinforcement learning: application to nonlinear powertrain control","abstract":"Complex mechanical systems such as vehicle powertrains are inherently subject to multiple nonlinearities and uncertainties arising from parametric variations. Modeling and calibration errors are therefore unavoidable, making the transfer of control systems from simulation to real-world systems a critical challenge. Traditional robust controls have limitations in handling certain types of nonlinearities and uncertainties, requiring a more practical approach capable of comprehensively compensating for these various constraints. This study proposes a new robust control approach using the framework of deep reinforcement learning (DRL). The key strategy lies in the synergy among domain randomization-based DRL, long short-term memory (LSTM)-based actor and critic networks, and model-based control (MBC). The problem setup is modeled via the latent Markov decision process (LMDP), a set of vanilla MDPs, for a controlled system subject to uncertainties and nonlinearities. In LMDP, the dynamics of an environment simulator is randomized during training to improve the robustness of the control system to real testing environments. The randomization increases training difficulties as well as conservativeness of the resultant control system; therefore, progress is assisted by concurrent use of a model-based controller based on a nominal system model. Compared to traditional DRL-based controls, the proposed controller design is smarter in that we can achieve a high level of generalization ability with a more compact neural network architecture and a smaller amount of training data. The proposed approach is verified via practical application to active damping for a complex powertrain system with nonlinearities and parametric variations. Comparative tests demonstrate the high robustness of the proposed approach.","authors":["Heisei Yonezawa","Ansei Yonezawa","Itsuro Kajiwara"],"url":"https://arxiv.org/abs/2504.19715"}
{"created":"2025-04-29","title":"QuickGrasp: Lightweight Antipodal Grasp Planning with Point Clouds","abstract":"Grasping has been a long-standing challenge in facilitating the final interface between a robot and the environment. As environments and tasks become complicated, the need to embed higher intelligence to infer from the surroundings and act on them has become necessary. Although most methods utilize techniques to estimate grasp pose by treating the problem via pure sampling-based approaches in the six-degree-of-freedom space or as a learning problem, they usually fail in real-life settings owing to poor generalization across domains. In addition, the time taken to generate the grasp plan and the lack of repeatability, owing to sampling inefficiency and the probabilistic nature of existing grasp planning approaches, severely limits their application in real-world tasks. This paper presents a lightweight analytical approach towards robotic grasp planning, particularly antipodal grasps, with little to no sampling in the six-degree-of-freedom space. The proposed grasp planning algorithm is formulated as an optimization problem towards estimating grasp points on the object surface instead of directly estimating the end-effector pose. To this extent, a soft-region-growing algorithm is presented for effective plane segmentation, even in the case of curved surfaces. An optimization-based quality metric is then used for the evaluation of grasp points to ensure indirect force closure. The proposed grasp framework is compared with the existing state-of-the-art grasp planning approach, Grasp pose detection (GPD), as a baseline over multiple simulated objects. The effectiveness of the proposed approach in comparison to GPD is also evaluated in a real-world setting using image and point-cloud data, with the planned grasps being executed using a ROBOTIQ gripper and UR5 manipulator.","authors":["Navin Sriram Ravie","Keerthi Vasan M","Asokan Thondiyath","Bijo Sebastian"],"url":"https://arxiv.org/abs/2504.19716"}
{"created":"2025-04-29","title":"Pixels2Points: Fusing 2D and 3D Features for Facial Skin Segmentation","abstract":"Face registration deforms a template mesh to closely fit a 3D face scan, the quality of which commonly degrades in non-skin regions (e.g., hair, beard, accessories), because the optimized template-to-scan distance pulls the template mesh towards the noisy scan surface. Improving registration quality requires a clean separation of skin and non-skin regions on the scan mesh. Existing image-based (2D) or scan-based (3D) segmentation methods however perform poorly. Image-based segmentation outputs multi-view inconsistent masks, and they cannot account for scan inaccuracies or scan-image misalignment, while scan-based methods suffer from lower spatial resolution compared to images. In this work, we introduce a novel method that accurately separates skin from non-skin geometry on 3D human head scans. For this, our method extracts features from multi-view images using a frozen image foundation model and aggregates these features in 3D. These lifted 2D features are then fused with 3D geometric features extracted from the scan mesh, to then predict a segmentation mask directly on the scan mesh. We show that our segmentations improve the registration accuracy over pure 2D or 3D segmentation methods by 8.89% and 14.3%, respectively. Although trained only on synthetic data, our model generalizes well to real data.","authors":["Victoria Yue Chen","Daoye Wang","Stephan Garbin","Sebastian Winberg","Timo Bolkart","Thabo Beeler"],"url":"https://arxiv.org/abs/2504.19718"}
{"created":"2025-04-29","title":"A computer vision method to estimate ventilation rate of Atlantic salmon in sea fish farms","abstract":"The increasing demand for aquaculture production necessitates the development of innovative, intelligent tools to effectively monitor and manage fish health and welfare. While non-invasive video monitoring has become a common practice in finfish aquaculture, existing intelligent monitoring methods predominantly focus on assessing body condition or fish swimming patterns and are often developed and evaluated in controlled tank environments, without demonstrating their applicability to real-world aquaculture settings in open sea farms. This underscores the necessity for methods that can monitor physiological traits directly within the production environment of sea fish farms. To this end, we have developed a computer vision method for monitoring ventilation rates of Atlantic salmon (Salmo salar), which was specifically designed for videos recorded in the production environment of commercial sea fish farms using the existing infrastructure. Our approach uses a fish head detection model, which classifies the mouth state as either open or closed using a convolutional neural network. This is followed with multiple object tracking to create temporal sequences of fish swimming across the field of view of the underwater video camera to estimate ventilation rates. The method demonstrated high efficiency, achieving a Pearson correlation coefficient of 0.82 between ground truth and predicted ventilation rates in a test set of 100 fish collected independently of the training data. By accurately identifying pens where fish exhibit signs of respiratory distress, our method offers broad applicability and the potential to transform fish health and welfare monitoring in finfish aquaculture.","authors":["Lukas Folkman","Quynh LK Vo","Colin Johnston","Bela Stantic","Kylie A Pitt"],"url":"https://arxiv.org/abs/2504.19719"}
{"created":"2025-04-29","title":"Taming the Titans: A Survey of Efficient LLM Inference Serving","abstract":"Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.","authors":["Ranran Zhen","Juntao Li","Yixin Ji","Zhenlin Yang","Tong Liu","Qingrong Xia","Xinyu Duan","Zhefeng Wang","Baoxing Huai","Min Zhang"],"url":"https://arxiv.org/abs/2504.19720"}
{"created":"2025-04-29","title":"The ATLAS of Traffic Lights: A Reliable Perception Framework for Autonomous Driving","abstract":"Traffic light perception is an essential component of the camera-based perception system for autonomous vehicles, enabling accurate detection and interpretation of traffic lights to ensure safe navigation through complex urban environments. In this work, we propose a modularized perception framework that integrates state-of-the-art detection models with a novel real-time association and decision framework, enabling seamless deployment into an autonomous driving stack. To address the limitations of existing public datasets, we introduce the ATLAS dataset, which provides comprehensive annotations of traffic light states and pictograms across diverse environmental conditions and camera setups. This dataset is publicly available at https://url.fzi.de/ATLAS. We train and evaluate several state-of-the-art traffic light detection architectures on ATLAS, demonstrating significant performance improvements in both accuracy and robustness. Finally, we evaluate the framework in real-world scenarios by deploying it in an autonomous vehicle to make decisions at traffic light-controlled intersections, highlighting its reliability and effectiveness for real-time operation.","authors":["Rupert Polley","Nikolai Polley","Dominik Heid","Marc Heinrich","Sven Ochs","J. Marius Z\\\"ollner"],"url":"https://arxiv.org/abs/2504.19722"}
{"created":"2025-04-29","title":"RepText: Rendering Visual Text via Replicating","abstract":"Although contemporary text-to-image generation models have achieved remarkable breakthroughs in producing visually appealing images, their capacity to generate precise and flexible typographic elements, especially non-Latin alphabets, remains constrained. To address these limitations, we start from an naive assumption that text understanding is only a sufficient condition for text rendering, but not a necessary condition. Based on this, we present RepText, which aims to empower pre-trained monolingual text-to-image generation models with the ability to accurately render, or more precisely, replicate, multilingual visual text in user-specified fonts, without the need to really understand them. Specifically, we adopt the setting from ControlNet and additionally integrate language agnostic glyph and position of rendered text to enable generating harmonized visual text, allowing users to customize text content, font and position on their needs. To improve accuracy, a text perceptual loss is employed along with the diffusion loss. Furthermore, to stabilize rendering process, at the inference phase, we directly initialize with noisy glyph latent instead of random initialization, and adopt region masks to restrict the feature injection to only the text region to avoid distortion of the background. We conducted extensive experiments to verify the effectiveness of our RepText relative to existing works, our approach outperforms existing open-source methods and achieves comparable results to native multi-language closed-source models. To be more fair, we also exhaustively discuss its limitations in the end.","authors":["Haofan Wang","Yujia Xu","Yimeng Li","Junchen Li","Chaowei Zhang","Jing Wang","Kejia Yang","Zhibo Chen"],"url":"https://arxiv.org/abs/2504.19724"}
{"created":"2025-04-29","title":"Hector UI: A Flexible Human-Robot User Interface for (Semi-)Autonomous Rescue and Inspection Robots","abstract":"The remote human operator's user interface (UI) is an important link to make the robot an efficient extension of the operator's perception and action. In rescue applications, several studies have investigated the design of operator interfaces based on observations during major robotics competitions or field deployments. Based on this research, guidelines for good interface design were empirically identified. The investigations on the UIs of teams participating in competitions are often based on external observations during UI application, which may miss some relevant requirements for UI flexibility. In this work, we present an open-source and flexibly configurable user interface based on established guidelines and its exemplary use for wheeled, tracked, and walking robots. We explain the design decisions and cover the insights we have gained during its highly successful applications in multiple robotics competitions and evaluations. The presented UI can also be adapted for other robots with little effort and is available as open source.","authors":["Stefan Fabian","Oskar von Stryk"],"url":"https://arxiv.org/abs/2504.19728"}
{"created":"2025-04-29","title":"Faster Dynamic $(\\Delta+1)$-Coloring Against Adaptive Adversaries","abstract":"We consider the problem of maintaining a proper $(\\Delta + 1)$-vertex coloring in a graph on $n$-vertices and maximum degree $\\Delta$ undergoing edge insertions and deletions. We give a randomized algorithm with amortized update time $\\widetilde{O}( n^{2/3} )$ against adaptive adversaries, meaning that updates may depend on past decisions by the algorithm. This improves on the very recent $\\widetilde{O}( n^{8/9} )$-update-time algorithm by Behnezhad, Rajaraman, and Wasim (SODA 2025) and matches a natural barrier for dynamic $(\\Delta+1)$-coloring algorithms. The main improvements are in the densest regions of the graph, where we use structural hints from the study of distributed graph algorithms.","authors":["Maxime Flin","Magn\\'us M. Halld\\'orsson"],"url":"https://arxiv.org/abs/2504.19729"}
{"created":"2025-04-29","title":"Evaluate-and-Purify: Fortifying Code Language Models Against Adversarial Attacks Using LLM-as-a-Judge","abstract":"The widespread adoption of code language models in software engineering tasks has exposed vulnerabilities to adversarial attacks, especially the identifier substitution attacks. Although existing identifier substitution attackers demonstrate high success rates, they often produce adversarial examples with unnatural code patterns. In this paper, we systematically assess the quality of adversarial examples using LLM-as-a-Judge. Our analysis reveals that over 80% of adversarial examples generated by state-of-the-art identifier substitution attackers (e.g., ALERT) are actually detectable. Based on this insight, we propose EP-Shield, a unified framework for evaluating and purifying identifier substitution attacks via naturalness-aware reasoning. Specifically, we first evaluate the naturalness of code and identify the perturbed adversarial code, then purify it so that the victim model can restore correct prediction. Extensive experiments demonstrate the superiority of EP-Shield over adversarial fine-tuning (up to 83.36% improvement) and its lightweight design 7B parameters) with GPT-4-level performance.","authors":["Wenhan Mu","Ling Xu","Shuren Pei","Le Mi","Huichi Zhou"],"url":"https://arxiv.org/abs/2504.19730"}
{"created":"2025-04-29","title":"LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding","abstract":"Dialogue data has been a key source for understanding learning processes, offering critical insights into how students engage in collaborative discussions and how these interactions shape their knowledge construction. The advent of Large Language Models (LLMs) has introduced promising opportunities for advancing qualitative research, particularly in the automated coding of dialogue data. However, the inherent contextual complexity of dialogue presents unique challenges for these models, especially in understanding and interpreting complex contextual information. This study addresses these challenges by developing a novel LLM-assisted automated coding approach for dialogue data. The novelty of our proposed framework is threefold: 1) We predict the code for an utterance based on dialogue-specific characteristics -- communicative acts and communicative events -- using separate prompts following the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We leveraged the interrelation between events and acts to implement consistency checking using GPT-4o. In particular, our contextual consistency checking provided a substantial accuracy improvement. We also found the accuracy of act predictions was consistently higher than that of event predictions. This study contributes a new methodological framework for enhancing the precision of automated coding of dialogue data as well as offers a scalable solution for addressing the contextual challenges inherent in dialogue analysis.","authors":["Ying Na","Shihui Feng"],"url":"https://arxiv.org/abs/2504.19734"}
{"created":"2025-04-29","title":"Measuring Train Driver Performance as Key to Approval of Driverless Trains","abstract":"Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation (EU) No. 402/2013 allow a simplified approach for the safety approval of computer vision systems for driverless trains, if they have 'similar' functions and interfaces as the replaced human driver. The human driver is not replaced one-to-one by a technical system - only a limited set of cognitive functions are replaced. However, performance in the most challenging function, obstacle detection, is difficult to quantify due to the deficiency of published measurement results. This article summarizes the data published so far. This article also goes a long way to remedy this situation by providing a new public and anonymized dataset of 711 train driver performance measurements from controlled experiments. The measurements are made for different speeds, obstacle sizes, train protection systems and obstacle color contrasts respectively. The measured values are reaction time and distance to the obstacle. The goal of this paper is an unbiased and exhaustive description of the presented dataset for research, standardization and regulation. Further project related information including the dataset and source code is available at https://atosense-02371c.usercontent.opencode.de/","authors":["Rustam Tagiew (German Centre for Rail Traffic Research at the Federal Railway Authority)","Prasannavenkatesh Balaji (German Centre for Rail Traffic Research at the Federal Railway Authority)"],"url":"https://arxiv.org/abs/2504.19735"}
{"created":"2025-04-29","title":"UTTG_ A Universal Teleoperation Approach via Online Trajectory Generation","abstract":"Teleoperation is crucial for hazardous environment operations and serves as a key tool for collecting expert demonstrations in robot learning. However, existing methods face robotic hardware dependency and control frequency mismatches between teleoperation devices and robotic platforms. Our approach automatically extracts kinematic parameters from unified robot description format (URDF) files, and enables pluggable deployment across diverse robots through uniform interfaces. The proposed interpolation algorithm bridges the frequency gap between low-rate human inputs and high-frequency robotic control commands through online continuous trajectory generation, \\n{while requiring no access to the closed, bottom-level control loop}. To enhance trajectory smoothness, we introduce a minimum-stretch spline that optimizes the motion quality. The system further provides precision and rapid modes to accommodate different task requirements. Experiments across various robotic platforms including dual-arm ones demonstrate generality and smooth operation performance of our methods. The code is developed in C++ with python interface, and available at https://github.com/IRMV-Manipulation-Group/UTTG.","authors":["Shengjian Fang","Yixuan Zhou","Yu Zheng","Pengyu Jiang","Siyuan Liu","Hesheng Wang"],"url":"https://arxiv.org/abs/2504.19736"}
{"created":"2025-04-29","title":"CoDEx: Combining Domain Expertise for Spatial Generalization in Satellite Image Analysis","abstract":"Global variations in terrain appearance raise a major challenge for satellite image analysis, leading to poor model performance when training on locations that differ from those encountered at test time. This remains true even with recent large global datasets. To address this challenge, we propose a novel domain-generalization framework for satellite images. Instead of trying to learn a single generalizable model, we train one expert model per training domain, while learning experts' similarity and encouraging similar experts to be consistent. A model selection module then identifies the most suitable experts for a given test sample and aggregates their predictions. Experiments on four datasets (DynamicEarthNet, MUDS, OSCD, and FMoW) demonstrate consistent gains over existing domain generalization and adaptation methods. Our code is publicly available at https://github.com/Abhishek19009/CoDEx.","authors":["Abhishek Kuriyal","Elliot Vincent","Mathieu Aubry","Loic Landrieu"],"url":"https://arxiv.org/abs/2504.19737"}
{"created":"2025-04-29","title":"Learning Efficiency Meets Symmetry Breaking","abstract":"Learning-based planners leveraging Graph Neural Networks can learn search guidance applicable to large search spaces, yet their potential to address symmetries remains largely unexplored. In this paper, we introduce a graph representation of planning problems allying learning efficiency with the ability to detect symmetries, along with two pruning methods, action pruning and state pruning, designed to manage symmetries during search. The integration of these techniques into Fast Downward achieves a first-time success over LAMA on the latest IPC learning track dataset. Code is released at: https://github.com/bybeye/Distincter.","authors":["Yingbin Bai","Sylvie Thiebaux","Felipe Trevizan"],"url":"https://arxiv.org/abs/2504.19738"}
{"created":"2025-04-29","title":"Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model","abstract":"In this paper, we introduce AffectVLM, a vision-language model designed to integrate multiviews for a semantically rich and visually comprehensive understanding of facial emotions from 3D/4D data. To effectively capture visual features, we propose a joint representation learning framework paired with a novel gradient-friendly loss function that accelerates model convergence towards optimal feature representation. Additionally, we introduce augmented textual prompts to enhance the model's linguistic capabilities and employ mixed view augmentation to expand the visual dataset. We also develop a Streamlit app for a real-time interactive inference and enable the model for distributed learning. Extensive experiments validate the superior performance of AffectVLM across multiple benchmarks.","authors":["Muzammil Behzad","Guoying Zhao"],"url":"https://arxiv.org/abs/2504.19739"}
{"created":"2025-04-29","title":"Graph Fourier Transformer with Structure-Frequency Information","abstract":"Graph Transformers (GTs) have shown advantages in numerous graph structure tasks but their self-attention mechanism ignores the generalization bias of graphs, with existing methods mainly compensating for this bias from aspects like position encoding, attention bias and relative distance yet still having sub-optimal performance and being insufficient by only considering the structural perspective of generalization bias. To address this, this paper proposes Grafourierformer, which innovatively combines GT with inductive bias containing Frequency-Structure information by applying Graph Fourier Transform to the Attention Matrix: specifically, eigenvalues from the Graph Laplacian matrix are used to construct an Eigenvalue matrix mask (reflecting node positions and structural relationships with neighboring nodes to enable consideration of node range structural characteristics and focus on local graph details), and inverse Fourier transform is employed to extract node high-frequency and low-frequency features, calculate low-frequency and high-frequency energy, and construct a node frequency-energy matrix to filter the eigenvalue matrix mask, allowing attention heads to incorporate both graph structural information and node frequency information optimization, adaptively distinguish global trends from local details, and effectively suppress redundant information interference. Extensive experiments on various benchmarks show Grafourierformer consistently outperforms GNN and GT-based models in graph classification and node classification tasks, with ablation experiments further validating the effectiveness and necessity of the method. Codes are available at https://github.com/Arichibald/Grafourierformer.git","authors":["Yonghui Zhai","Yang Zhang","Minghao Shang","Lihua Pang","Yaxin Ren"],"url":"https://arxiv.org/abs/2504.19740"}
{"created":"2025-04-29","title":"EcoWikiRS: Learning Ecological Representation of Satellite Images from Weak Supervision with Species Observations and Wikipedia","abstract":"The presence of species provides key insights into the ecological properties of a location such as land cover, climatic conditions or even soil properties. We propose a method to predict such ecological properties directly from remote sensing (RS) images by aligning them with species habitat descriptions. We introduce the EcoWikiRS dataset, consisting of high-resolution aerial images, the corresponding geolocated species observations, and, for each species, the textual descriptions of their habitat from Wikipedia. EcoWikiRS offers a scalable way of supervision for RS vision language models (RS-VLMs) for ecology. This is a setting with weak and noisy supervision, where, for instance, some text may describe properties that are specific only to part of the species' niche or is irrelevant to a specific image. We tackle this by proposing WINCEL, a weighted version of the InfoNCE loss. We evaluate our model on the task of ecosystem zero-shot classification by following the habitat definitions from the European Nature Information System (EUNIS). Our results show that our approach helps in understanding RS images in a more ecologically meaningful manner. The code and the dataset are available at https://github.com/eceo-epfl/EcoWikiRS.","authors":["Valerie Zermatten","Javiera Castillo-Navarro","Pallavi Jain","Devis Tuia","Diego Marcos"],"url":"https://arxiv.org/abs/2504.19742"}
{"created":"2025-04-29","title":"FineQ: Software-Hardware Co-Design for Low-Bit Fine-Grained Mixed-Precision Quantization of LLMs","abstract":"Large language models (LLMs) have significantly advanced the natural language processing paradigm but impose substantial demands on memory and computational resources. Quantization is one of the most effective ways to reduce memory consumption of LLMs. However, advanced single-precision quantization methods experience significant accuracy degradation when quantizing to ultra-low bits. Existing mixed-precision quantization methods are quantized by groups with coarse granularity. Employing high precision for group data leads to substantial memory overhead, whereas low precision severely impacts model accuracy. To address this issue, we propose FineQ, software-hardware co-design for low-bit fine-grained mixed-precision quantization of LLMs. First, FineQ partitions the weights into finer-grained clusters and considers the distribution of outliers within these clusters, thus achieving a balance between model accuracy and memory overhead. Then, we propose an outlier protection mechanism within clusters that uses 3 bits to represent outliers and introduce an encoding scheme for index and data concatenation to enable aligned memory access. Finally, we introduce an accelerator utilizing temporal coding that effectively supports the quantization algorithm while simplifying the multipliers in the systolic array. FineQ achieves higher model accuracy compared to the SOTA mixed-precision quantization algorithm at a close average bit-width. Meanwhile, the accelerator achieves up to 1.79x energy efficiency and reduces the area of the systolic array by 61.2%.","authors":["Xilong Xie","Liang Wang","Limin Xiao","Meng Han","Lin Sun","Shuai Zheng","Xiangrong Xu"],"url":"https://arxiv.org/abs/2504.19746"}
{"created":"2025-04-29","title":"STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction","abstract":"3D occupancy and scene flow offer a detailed and dynamic representation of 3D scene. Recognizing the sparsity and complexity of 3D space, previous vision-centric methods have employed implicit learning-based approaches to model spatial and temporal information. However, these approaches struggle to capture local details and diminish the model's spatial discriminative ability. To address these challenges, we propose a novel explicit state-based modeling method designed to leverage the occupied state to renovate the 3D features. Specifically, we propose a sparse occlusion-aware attention mechanism, integrated with a cascade refinement strategy, which accurately renovates 3D features with the guidance of occupied state information. Additionally, we introduce a novel method for modeling long-term dynamic interactions, which reduces computational costs and preserves spatial information. Compared to the previous state-of-the-art methods, our efficient explicit renovation strategy not only delivers superior performance in terms of RayIoU and mAVE for occupancy and scene flow prediction but also markedly reduces GPU memory usage during training, bringing it down to 8.7GB. Our code is available on https://github.com/lzzzzzm/STCOcc","authors":["Zhimin Liao","Ping Wei","Shuaijia Chen","Haoxuan Wang","Ziyang Ren"],"url":"https://arxiv.org/abs/2504.19749"}
{"created":"2025-04-29","title":"Spectral Analysis of Approximated Capacity Fade Curvature for Lithium-Ion Batteries","abstract":"The techno-economic benefits of incorporating battery degradation into advanced control strategies necessitate the development of degradation diagnosis as an advanced function in battery management systems (BMSs). To address this, a curvature-based knee identification method was proposed in our previous work [1]. Here, we further validate its effectiveness on a new battery aging dataset under a realistic driving profile and conduct spectral analysis of the approximated capacity fade curvature. The curvature-based method shows consistent knee identification performance on this dataset and the approximated curvature is found to correlate with underlying degradation modes and a shift of electrode material phase transition points. The method uses capacity data as the only input, which is easy to acquire in the lab and it is applicable in battery energy storage systems for grid applications.","authors":["Huang Zhang","Torsten Wik"],"url":"https://arxiv.org/abs/2504.19752"}
{"created":"2025-04-29","title":"Reconstructing Context: Evaluating Advanced Chunking Strategies for Retrieval-Augmented Generation","abstract":"Retrieval-augmented generation (RAG) has become a transformative approach for enhancing large language models (LLMs) by grounding their outputs in external knowledge sources. Yet, a critical question persists: how can vast volumes of external knowledge be managed effectively within the input constraints of LLMs? Traditional methods address this by chunking external documents into smaller, fixed-size segments. While this approach alleviates input limitations, it often fragments context, resulting in incomplete retrieval and diminished coherence in generation. To overcome these shortcomings, two advanced techniques, late chunking and contextual retrieval, have been introduced, both aiming to preserve global context. Despite their potential, their comparative strengths and limitations remain unclear. This study presents a rigorous analysis of late chunking and contextual retrieval, evaluating their effectiveness and efficiency in optimizing RAG systems. Our results indicate that contextual retrieval preserves semantic coherence more effectively but requires greater computational resources. In contrast, late chunking offers higher efficiency but tends to sacrifice relevance and completeness.","authors":["Carlo Merola","Jaspinder Singh"],"url":"https://arxiv.org/abs/2504.19754"}
{"created":"2025-04-29","title":"Hybrid Approach Combining Ultrasound and Blood Test Analysis with a Voting Classifier for Accurate Liver Fibrosis and Cirrhosis Assessment","abstract":"Liver cirrhosis is an insidious condition involving the substitution of normal liver tissue with fibrous scar tissue and causing major health complications. The conventional method of diagnosis using liver biopsy is invasive and, therefore, inconvenient for use in regular screening. In this paper,we present a hybrid model that combines machine learning techniques with clinical data and ultrasoundscans to improve liver fibrosis and cirrhosis detection accuracy is presented. The model integrates fixed blood test probabilities with deep learning model predictions (DenseNet-201) for ultrasonic images. The combined hybrid model achieved an accuracy of 92.5%. The findings establish the viability of the combined model in enhancing diagnosis accuracy and supporting early intervention in liver disease care.","authors":["Kapil Kashyap","Sean Fargose","Chrisil Dabre","Fatema Dolaria","Nilesh Patil","Aniket Kore"],"url":"https://arxiv.org/abs/2504.19755"}
{"created":"2025-04-29","title":"Logic-based Resilience Computation of Power Systems Against Frequency Requirements","abstract":"Incorporating renewable energy sources into modern power grids has significantly decreased system inertia, which has raised concerns about power system vulnerability to disturbances and frequency instability. The conventional methods for evaluating transient stability by bounding frequency deviations are often conservative and may not accurately reflect real-world conditions and operational constraints. To address this, we propose a framework for assessing the resilience of power systems against disturbances while adhering to realistic operational frequency constraints. Our approach leverages the Lure system representation of power system dynamics and Signal Temporal Logic (STL) to capture the essential frequency response requirements set by grid operators. This enables us to translate frequency constraints into formal robustness semantics. We then formulate an optimization problem to identify the maximum disturbance that the system can withstand without violating these constraints. The resulting optimization is translated into a scenario optimization while addressing the uncertainty in the obtained solution. The proposed methodology has been simulated on the Single Machine Infinite Bus case study and 9-Bus IEEE benchmark system, demonstrating its effectiveness in assessing resilience across various operating conditions and delivering promising results.","authors":["Negar Monir","Mahdieh S. Sadabadi","Sadegh Soudjani"],"url":"https://arxiv.org/abs/2504.19756"}
{"created":"2025-04-29","title":"vMODB: Unifying event and data management for distributed asynchronous applications","abstract":"Event-driven architecture (EDA) has emerged as a crucial architectural pattern for scalable cloud applications. However, its asynchronous and decoupled nature introduces challenges for meeting transactional requirements. Database systems, relegated to serving as storage engines for individual components, do not recognize transactions that span multiple components in EDAs. In contrast, messaging systems are unaware of the components' application states. Weaving such asynchronous and independent EDA components forces developers to relinquish transactional guarantees, resulting in data consistency issues. To address this challenge, we design vMODB, a distributed framework that enables the implementation of highly consistent and scalable cloud applications without compromising the envisioned benefits of EDA. We propose Virtual Micro Service (VMS), a novel programming model that provides familiar constructs to enable developers to specify the data model, constraints, and concurrency semantics of components, as well as transactions and data dependencies that span across components. vMODB leverages VMS semantics to enforce ACID properties by transparently unifying event logs and state management into a common event-driven execution framework. Our experiments using two benchmarks show that vMODB outperforms a widely adopted state-of-the-art competing framework that only offers eventual consistency by up to 3X. With its high performance, familiar programming constructs, and ACID properties, vMODB will significantly simplify the development of highly consistent and efficient EDAs.","authors":["Rodrigo Laigner","Yongluan Zhou"],"url":"https://arxiv.org/abs/2504.19757"}
{"created":"2025-04-29","title":"Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs","abstract":"In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB) to evaluate the moral reasoning abilities of large language models (LLMs) across five typologically diverse languages and three levels of contextual complexity: sentence, paragraph, and document. Our results show moral reasoning performance degrades with increasing context complexity, particularly for low-resource languages such as Vietnamese. We further fine-tune the open-source LLaMA-3-8B model using curated monolingual data for alignment and poisoning. Surprisingly, low-resource languages have a stronger impact on multilingual reasoning than high-resource ones, highlighting their critical role in multilingual NLP.","authors":["Huichi Zhou","Zehao Xu","Munan Zhao","Kaihong Li","Yiqiang Li","Hongtao Wang"],"url":"https://arxiv.org/abs/2504.19759"}
{"created":"2025-04-29","title":"Crafting a Personal Journaling Practice: Negotiating Ecosystems of Materials, Personal Context, and Community in Analog Journaling","abstract":"Analog journaling has grown in popularity, with journaling on paper encompassing a range of motivations, styles, and practices including planning, habit-tracking, and reflecting. Journalers develop strong personal preferences around the tools they use, the ideas they capture, and the layout in which they represent their ideas and memories. Understanding how analog journaling practices are individually shaped and crafted over time is critical to supporting the varied benefits associated with journaling, including improved mental health and positive support for identity development. To understand this development, we qualitatively analyzed publicly-shared journaling content from YouTube and Instagram and interviewed 11 journalers. We report on our identification of the journaling ecosystem in which journaling practices are shaped by materials, personal context, and communities, sharing how this ecosystem plays a role in the practices and identities of journalers as they customize their journaling routine to best suit their personal goals. Using these insights, we discuss design opportunities for how future tools can better align with and reflect the rich affordances and practices of journaling on paper.","authors":["Katherine Lin","Juna Kawai-Yue","Adira Sklar","Lucy Hecht","Sarah Sterman","Tiffany Tseng"],"url":"https://arxiv.org/abs/2504.19767"}
{"created":"2025-04-29","title":"On Solving the Dynamics of Constrained Rigid Multi-Body Systems with Kinematic Loops","abstract":"This technical report provides an in-depth evaluation of both established and state-of-the-art methods for simulating constrained rigid multi-body systems with hard-contact dynamics, using formulations of Nonlinear Complementarity Problems (NCPs). We are particularly interest in examining the simulation of highly coupled mechanical systems with multitudes of closed-loop bilateral kinematic joint constraints in the presence of additional unilateral constraints such as joint limits and frictional contacts with restitutive impacts. This work thus presents an up-to-date literature survey of the relevant fields, as well as an in-depth description of the approaches used for the formulation and solving of the numerical time-integration problem in a maximal coordinate setting. More specifically, our focus lies on a version of the overall problem that decomposes it into the forward dynamics problem followed by a time-integration using the states of the bodies and the constraint reactions rendered by the former. We then proceed to elaborate on the formulations used to model frictional contact dynamics and define a set of solvers that are representative of those currently employed in the majority of the established physics engines. A key aspect of this work is the definition of a benchmarking framework that we propose as a means to both qualitatively and quantitatively evaluate the performance envelopes of the set of solvers on a diverse set of challenging simulation scenarios. We thus present an extensive set of experiments that aim at highlighting the absolute and relative performance of all solvers on particular problems of interest as well as aggravatingly over the complete set defined in the suite.","authors":["Vassilios Tsounis","Ruben Grandia","Moritz B\\\"acher"],"url":"https://arxiv.org/abs/2504.19771"}
{"created":"2025-04-29","title":"Memento: Augmenting Personalized Memory via Practical Multimodal Wearable Sensing in Visual Search and Wayfinding Navigation","abstract":"Working memory involves the temporary retention of information over short periods. It is a critical cognitive function that enables humans to perform various online processing tasks, such as dialing a phone number, recalling misplaced items' locations, or navigating through a store. However, inherent limitations in an individual's capacity to retain information often result in forgetting important details during such tasks. Although previous research has successfully utilized wearable and assistive technologies to enhance long-term memory functions (e.g., episodic memory), their application to supporting short-term recall in daily activities remains underexplored. To address this gap, we present Memento, a framework that uses multimodal wearable sensor data to detect significant changes in cognitive state and provide intelligent in situ cues to enhance recall. Through two user studies involving 15 and 25 participants in visual search navigation tasks, we demonstrate that participants receiving visual cues from Memento achieved significantly better route recall, improving approximately 20-23% compared to free recall. Furthermore, Memento reduced cognitive load and review time by 46% while also substantially reducing computation time (3.86 seconds vs. 15.35 seconds), offering an average of 75% effectiveness compared to computer vision-based cue selection approaches.","authors":["Indrajeet Ghosh","Kasthuri Jayarajah","Nicholas Waytowich","Nirmalya Roy"],"url":"https://arxiv.org/abs/2504.19772"}
{"created":"2025-04-29","title":"Sliding Window Adversarial Channels","abstract":"In an arbitrarily varying channel (AVC), the channel has a state which is under the control of an adversarial jammer and the corresponding capacities are often functions of the \"power\" constraints on the transmitter and jammer. In this paper we propose a model in which the constraints must hold almost surely over contiguous subsequences of the codeword and state, which we call a sliding window constraint. We study oblivious jammers and codes with stochastic encoding under maximum probability of error. We show that this extra limitation on the jammer is beneficial for the transmitter: in some cases, the capacity for unique decoding with a sliding window constraint is equal to the capacity for list decoding in the standard model without sliding windows, roughly implying that the addition of window constraints reduces list decoding to unique decoding. The list decoding capacity in the standard model can be strictly larger than the unique decoding capacity.","authors":["Bikash Kumar Dey","Sidharth Jaggi","Michael Langberg","Anand D. Sarwate","Yihan Zhang"],"url":"https://arxiv.org/abs/2504.19773"}
{"created":"2025-04-29","title":"If Concept Bottlenecks are the Question, are Foundation Models the Answer?","abstract":"Concept Bottleneck Models (CBMs) are neural networks designed to conjoin high performance with ante-hoc interpretability. CBMs work by first mapping inputs (e.g., images) to high-level concepts (e.g., visible objects and their properties) and then use these to solve a downstream task (e.g., tagging or scoring an image) in an interpretable manner. Their performance and interpretability, however, hinge on the quality of the concepts they learn. The go-to strategy for ensuring good quality concepts is to leverage expert annotations, which are expensive to collect and seldom available in applications. Researchers have recently addressed this issue by introducing \"VLM-CBM\" architectures that replace manual annotations with weak supervision from foundation models. It is however unclear what is the impact of doing so on the quality of the learned concepts. To answer this question, we put state-of-the-art VLM-CBMs to the test, analyzing their learned concepts empirically using a selection of significant metrics. Our results show that, depending on the task, VLM supervision can sensibly differ from expert annotations, and that concept accuracy and quality are not strongly correlated. Our code is available at https://github.com/debryu/CQA.","authors":["Nicola Debole","Pietro Barbiero","Francesco Giannini","Andrea Passeggini","Stefano Teso","Emanuele Marconato"],"url":"https://arxiv.org/abs/2504.19774"}
{"created":"2025-04-29","title":"On the Complexity of Identifying Groups without Abelian Normal Subgroups: Parallel, First Order, and GI-Hardness","abstract":"In this paper, we exhibit an $\\textsf{AC}^{3}$ isomorphism test for groups without Abelian normal subgroups (a.k.a. Fitting-free groups), a class for which isomorphism testing was previously known to be in $\\mathsf{P}$ (Babai, Codenotti, and Qiao; ICALP '12). Here, we leverage the fact that $G/\\text{PKer}(G)$ can be viewed as permutation group of degree $O(\\log |G|)$. As $G$ is given by its multiplication table, we are able to implement the solution for the corresponding instance of Twisted Code Equivalence in $\\textsf{AC}^{3}$.","authors":["Joshua A. Grochow","Dan Johnson","Michael Levet"],"url":"https://arxiv.org/abs/2504.19777"}
{"created":"2025-04-29","title":"Learning Brenier Potentials with Convex Generative Adversarial Neural Networks","abstract":"Brenier proved that under certain conditions on a source and a target probability measure there exists a strictly convex function such that its gradient is a transport map from the source to the target distribution. This function is called the Brenier potential. Furthermore, detailed information on the H\\\"older regularity of the Brenier potential is available. In this work we develop the statistical learning theory of generative adversarial neural networks that learn the Brenier potential. As by the transformation of densities formula, the density of the generated measure depends on the second derivative of the Brenier potential, we develop the universal approximation theory of ReCU networks with cubic activation $\\mathtt{ReCU}(x)=\\max\\{0,x\\}^3$ that combines the favorable approximation properties of H\\\"older functions with a Lipschitz continuous density. In order to assure the convexity of such general networks, we introduce an adversarial training procedure for a potential function represented by the ReCU networks that combines the classical discriminator cross entropy loss with a penalty term that enforces (strict) convexity. We give a detailed decomposition of learning errors and show that for a suitable high penalty parameter all networks chosen in the adversarial min-max optimization problem are strictly convex. This is further exploited to prove the consistency of the learning procedure for (slowly) expanding network capacity. We also implement the described learning algorithm and apply it to a number of standard test cases from Gaussian mixture to image data as target distributions. As predicted in theory, we observe that the convexity loss becomes inactive during the training process and the potentials represented by the neural networks have learned convexity.","authors":["Claudia Drygala","Hanno Gottschalk","Thomas Kruse","S\\'egol\\`ene Martin","Annika M\\\"utze"],"url":"https://arxiv.org/abs/2504.19779"}
{"created":"2025-04-29","title":"Heterophily-informed Message Passing","abstract":"Graph neural networks (GNNs) are known to be vulnerable to oversmoothing due to their implicit homophily assumption. We mitigate this problem with a novel scheme that regulates the aggregation of messages, modulating the type and extent of message passing locally thereby preserving both the low and high-frequency components of information. Our approach relies solely on learnt embeddings, obviating the need for auxiliary labels, thus extending the benefits of heterophily-aware embeddings to broader applications, e.g., generative modelling. Our experiments, conducted across various data sets and GNN architectures, demonstrate performance enhancements and reveal heterophily patterns across standard classification benchmarks. Furthermore, application to molecular generation showcases notable performance improvements on chemoinformatics benchmarks.","authors":["Haishan Wang","Arno Solin","Vikas Garg"],"url":"https://arxiv.org/abs/2504.19785"}
{"created":"2025-04-29","title":"Contextures: The Mechanism of Representation Learning","abstract":"This dissertation establishes the contexture theory to mathematically characterize the mechanism of representation learning, or pretraining. Despite the remarkable empirical success of foundation models, it is not very clear what representations they learn, and why these representations are useful for various downstream tasks. A scientific understanding of representation learning is critical, especially at this point when scaling up the model size is producing diminishing returns, and designing new pretraining methods is imperative for further progress.","authors":["Runtian Zhai"],"url":"https://arxiv.org/abs/2504.19792"}
{"created":"2025-04-29","title":"Prompt Injection Attack to Tool Selection in LLM Agents","abstract":"Tool selection is a key component of LLM agents. The process operates through a two-step mechanism - \\emph{retrieval} and \\emph{selection} - to pick the most appropriate tool from a tool library for a given task. In this work, we introduce \\textit{ToolHijacker}, a novel prompt injection attack targeting tool selection in no-box scenarios. ToolHijacker injects a malicious tool document into the tool library to manipulate the LLM agent's tool selection process, compelling it to consistently choose the attacker's malicious tool for an attacker-chosen target task. Specifically, we formulate the crafting of such tool documents as an optimization problem and propose a two-phase optimization strategy to solve it. Our extensive experimental evaluation shows that ToolHijacker is highly effective, significantly outperforming existing manual-based and automated prompt injection attacks when applied to tool selection. Moreover, we explore various defenses, including prevention-based defenses (StruQ and SecAlign) and detection-based defenses (known-answer detection, perplexity detection, and perplexity windowed detection). Our experimental results indicate that these defenses are insufficient, highlighting the urgent need for developing new defense strategies.","authors":["Jiawen Shi","Zenghui Yuan","Guiyao Tie","Pan Zhou","Neil Zhenqiang Gong","Lichao Sun"],"url":"https://arxiv.org/abs/2504.19793"}
{"created":"2025-04-29","title":"Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge using FPGAs","abstract":"The increased demand for data privacy and security in machine learning (ML) applications has put impetus on effective edge training on Internet-of-Things (IoT) nodes. Edge training aims to leverage speed, energy efficiency and adaptability within the resource constraints of the nodes. Deploying and training Deep Neural Networks (DNNs)-based models at the edge, although accurate, posit significant challenges from the back-propagation algorithm's complexity, bit precision trade-offs, and heterogeneity of DNN layers. This paper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an alternative to DNN implementations. DTM utilizes logic-based on-chip inference with finite-state automata-driven learning within the same Field Programmable Gate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin Machine algorithms, the dynamic aspect of the accelerator design allows for a run-time reconfiguration targeting different datasets, model architectures, and model sizes without resynthesis. This makes the DTM suitable for targeting multivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer multiply-accumulates, devoid of derivative computation. It is a data-centric ML algorithm that learns by aligning Tsetlin automata with input data to form logical propositions enabling efficient Look-up-Table (LUT) mapping and frugal Block RAM usage in FPGA training implementations. The proposed accelerator offers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x less power than the next-best comparable design.","authors":["Gang Mao","Tousif Rahman","Sidharth Maheshwari","Bob Pattison","Zhuang Shao","Rishad Shafik","Alex Yakovlev"],"url":"https://arxiv.org/abs/2504.19797"}
{"created":"2025-04-29","title":"Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance","abstract":"Accurately forecasting the performance of Large Language Models (LLMs) before extensive fine-tuning or merging can substantially reduce both computational expense and development time. Although prior approaches like scaling laws account for global factors such as parameter size or training tokens, they often overlook explicit lineage relationships - i.e., which models are derived or merged from which parents. In this work, we propose a novel Lineage-Regularized Matrix Factorization (LRMF) framework that encodes ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging multi-hop parent-child connections, LRMF consistently outperforms conventional matrix factorization and collaborative filtering methods in both instance-level and benchmark-level performance prediction. Our large-scale study includes 2,934 publicly available Hugging Face models and 21,000+ instances across 6 major benchmarks, showing that lineage constraints yield up to 7-10 percentage points higher correlation with actual performance compared to baselines. Moreover, LRMF effectively addresses the cold-start problem, providing accurate estimates for newly derived or merged models even with minimal data. This lineage-guided strategy thus offers a resource-efficient way to inform hyperparameter tuning, data selection, and model combination in modern LLM development.","authors":["Takuya Tamura","Taro Yano","Masafumi Enomoto","Masafumi Oyamada"],"url":"https://arxiv.org/abs/2504.19811"}
{"created":"2025-04-29","title":"Hyper-differential sensitivity analysis with respect to model discrepancy: Prior Distributions","abstract":"Hyper-differential sensitivity analysis with respect to model discrepancy was recently developed to enable uncertainty quantification for optimization problems. The approach consists of two primary steps: (i) Bayesian calibration of the discrepancy between high- and low-fidelity models, and (ii) propagating the model discrepancy uncertainty through the optimization problem. When high-fidelity model evaluations are limited, as is common in practice, the prior discrepancy distribution plays a crucial role in the uncertainty analysis. However, specification of this prior is challenging due to its mathematical complexity and many hyper-parameters. This article presents a novel approach to specify the prior distribution. Our approach consists of two parts: (1) an algorithmic initialization of the prior hyper-parameters that uses existing data to initialize a hyper-parameter estimate, and (2) a visualization framework to systematically explore properties of the prior and guide tuning of the hyper-parameters to ensure that the prior captures the appropriate range of uncertainty. We provide detailed mathematical analysis and a collection of numerical examples that elucidate properties of the prior that are crucial to ensure uncertainty quantification.","authors":["Joseph Hart","Bart van Bloemen Waanders","Jixian Li","Timbwaoga A. J. Ouermi","Chris R. Johnson"],"url":"https://arxiv.org/abs/2504.19812"}
{"created":"2025-04-29","title":"High-Level Message Sequence Charts: Satisfiability and Realizability Revisited","abstract":"Message sequence charts (MSCs) visually represent interactions in distributed systems that communicate through FIFO channels. High-level MSCs (HMSCs) extend MSCs with choice, concatenation, and iteration, allowing for the specification of complex behaviors. This paper revisits two classical problems for HMSCs: satisfiability and realizability. Satisfiability (also known as reachability or nonemptiness) asks whether there exists a path in the HMSC that gives rise to a valid behavior. Realizability concerns translating HMSCs into communicating finite-state machines to ensure correct system implementations.","authors":["Benedikt Bollig","Marie Fortin","Paul Gastin"],"url":"https://arxiv.org/abs/2504.19814"}
{"created":"2025-04-29","title":"Digital Twin-based Out-of-Distribution Detection in Autonomous Vessels","abstract":"An autonomous vessel (AV) is a complex cyber-physical system (CPS) with software enabling many key functionalities, e.g., navigation software enables an AV to autonomously or semi-autonomously follow a path to its destination. Digital twins of such AVs enable advanced functionalities such as running what-if scenarios, performing predictive maintenance, and enabling fault diagnosis. Due to technological improvements, real-time analyses using continuous data from vessels' real-time operations have become increasingly possible. However, the literature has little explored developing advanced analyses in real-time data in AVs with digital twins built with machine learning techniques. To this end, we present a novel digital twin-based approach (ODDIT) to detect future out-of-distribution (OOD) states of an AV before reaching them, enabling proactive intervention. Such states may indicate anomalies requiring attention (e.g., manual correction by the ship master) and assist testers in scenario-centered testing. The digital twin consists of two machine-learning models predicting future vessel states and whether the predicted state will be OOD. We evaluated ODDIT with five vessels across waypoint and zigzag maneuvering under simulated conditions, including sensor and actuator noise and environmental disturbances i.e., ocean current. ODDIT achieved high accuracy in detecting OOD states, with AUROC and TNR@TPR95 scores reaching 99\\% across multiple vessels.","authors":["Erblin Isaku","Hassan Sartaj","Shaukat Ali"],"url":"https://arxiv.org/abs/2504.19816"}
{"created":"2025-04-29","title":"PhenoAssistant: A Conversational Multi-Agent AI System for Automated Plant Phenotyping","abstract":"Plant phenotyping increasingly relies on (semi-)automated image-based analysis workflows to improve its accuracy and scalability. However, many existing solutions remain overly complex, difficult to reimplement and maintain, and pose high barriers for users without substantial computational expertise. To address these challenges, we introduce PhenoAssistant: a pioneering AI-driven system that streamlines plant phenotyping via intuitive natural language interaction. PhenoAssistant leverages a large language model to orchestrate a curated toolkit supporting tasks including automated phenotype extraction, data visualisation and automated model training. We validate PhenoAssistant through several representative case studies and a set of evaluation tasks. By significantly lowering technical hurdles, PhenoAssistant underscores the promise of AI-driven methodologies to democratising AI adoption in plant biology.","authors":["Feng Chen","Ilias Stogiannidis","Andrew Wood","Danilo Bueno","Dominic Williams","Fraser Macfarlane","Bruce Grieve","Darren Wells","Jonathan A. Atkinson","Malcolm J. Hawkesford","Stephen A. Rolfe","Tracy Lawson","Tony Pridmore","Mario Valerio Giuffrida","Sotirios A. Tsaftaris"],"url":"https://arxiv.org/abs/2504.19818"}
{"created":"2025-04-29","title":"Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video","abstract":"Neural Radiance Fields (NeRF) has demonstrated its superior capability to represent 3D geometry but require accurately precomputed camera poses during training. To mitigate this requirement, existing methods jointly optimize camera poses and NeRF often relying on good pose initialisation or depth priors. However, these approaches struggle in challenging scenarios, such as large rotations, as they map each camera to a world coordinate system. We propose a novel method that eliminates prior dependencies by modeling continuous camera motions as time-dependent angular velocity and velocity. Relative motions between cameras are learned first via velocity integration, while camera poses can be obtained by aggregating such relative motions up to a world coordinate system defined at a single time step within the video. Specifically, accurate continuous camera movements are learned through a time-dependent NeRF, which captures local scene geometry and motion by training from neighboring frames for each time step. The learned motions enable fine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D and Scannet show our approach achieves superior camera pose and depth estimation and comparable novel-view synthesis performance compared to state-of-the-art methods. Our code is available at https://github.com/HoangChuongNguyen/cope-nerf.","authors":["Hoang Chuong Nguyen","Wei Mao","Jose M. Alvarez","Miaomiao Liu"],"url":"https://arxiv.org/abs/2504.19819"}
{"created":"2025-04-29","title":"Hierarchical Uncertainty-Aware Graph Neural Network","abstract":"Recent research on graph neural networks (GNNs) has explored mechanisms for capturing local uncertainty and exploiting graph hierarchies to mitigate data sparsity and leverage structural properties. However, the synergistic integration of these two approaches remains underexplored. In this work, we introduce a novel architecture, the Hierarchical Uncertainty-Aware Graph Neural Network (HU-GNN), which unifies multi-scale representation learning, principled uncertainty estimation, and self-supervised embedding diversity within a single end-to-end framework. Specifically, HU-GNN adaptively forms node clusters and estimates uncertainty at multiple structural scales from individual nodes to higher levels. These uncertainty estimates guide a robust message-passing mechanism and attention weighting, effectively mitigating noise and adversarial perturbations while preserving predictive accuracy on both node- and graph-level tasks. We also offer key theoretical contributions, including a probabilistic formulation, rigorous uncertainty-calibration guarantees, and formal robustness bounds. Finally, by incorporating recent advances in graph contrastive learning, HU-GNN maintains diverse, structurally faithful embeddings. Extensive experiments on standard benchmarks demonstrate that our model achieves state-of-the-art robustness and interpretability.","authors":["Yoonhyuk Choi","Chong-Kwon Kim"],"url":"https://arxiv.org/abs/2504.19820"}
{"created":"2025-04-29","title":"SILENT: A New Lens on Statistics in Software Timing Side Channels","abstract":"Cryptographic research takes software timing side channels seriously. Approaches to mitigate them include constant-time coding and techniques to enforce such practices. However, recent attacks like Meltdown [42], Spectre [37], and Hertzbleed [70] have challenged our understanding of what it means for code to execute in constant time on modern CPUs. To ensure that assumptions on the underlying hardware are correct and to create a complete feedback loop, developers should also perform \\emph{timing measurements} as a final validation step to ensure the absence of exploitable side channels. Unfortunately, as highlighted by a recent study by Jancar et al. [30], developers often avoid measurements due to the perceived unreliability of the statistical analysis and its guarantees.","authors":["Martin Dunsche","Patrick Bastian","Marcel Maehren","Nurullah Erinola","Robert Merget","Nicolai Bissantz","Holger Dette","J\\\"org Schwenk"],"url":"https://arxiv.org/abs/2504.19821"}
{"created":"2025-04-29","title":"Mj\\\"olnir: A Deep Learning Parametrization Framework for Global Lightning Flash Density","abstract":"Recent advances in AI-based weather forecasting models, such as FourCastNet, Pangu-Weather, and GraphCast, have demonstrated the remarkable ability of deep learning to emulate complex atmospheric dynamics. Building on this momentum, we propose Mj\\\"olnir, a novel deep learning-based framework for global lightning flash density parameterization. Trained on ERA5 atmospheric predictors and World Wide Lightning Location Network (WWLLN) observations at a daily temporal resolution and 1 degree spatial resolution, Mj\\\"olnir captures the nonlinear mapping between large-scale environmental conditions and lightning activity. The model architecture is based on the InceptionNeXt backbone with SENet, and a multi-task learning strategy to simultaneously predict lightning occurrence and magnitude. Extensive evaluations yield that Mollnir accurately reproduces the global distribution, seasonal variability, and regional characteristics of lightning activity, achieving a global Pearson correlation coefficient of 0.96 for annual mean fields. These results suggest that Mj\\\"olnir serves not only as an effective data-driven global lightning parameterization but also as a promising AI-based scheme for next-generation Earth system models (AI-ESMs).","authors":["Minjong Cheon"],"url":"https://arxiv.org/abs/2504.19822"}
{"created":"2025-04-29","title":"Taming the Randomness: Towards Label-Preserving Cropping in Contrastive Learning","abstract":"Contrastive learning (CL) approaches have gained great recognition as a very successful subset of self-supervised learning (SSL) methods. SSL enables learning from unlabeled data, a crucial step in the advancement of deep learning, particularly in computer vision (CV), given the plethora of unlabeled image data. CL works by comparing different random augmentations (e.g., different crops) of the same image, thus achieving self-labeling. Nevertheless, randomly augmenting images and especially random cropping can result in an image that is semantically very distant from the original and therefore leads to false labeling, hence undermining the efficacy of the methods. In this research, two novel parameterized cropping methods are introduced that increase the robustness of self-labeling and consequently increase the efficacy. The results show that the use of these methods significantly improves the accuracy of the model by between 2.7\\% and 12.4\\% on the downstream task of classifying CIFAR-10, depending on the crop size compared to that of the non-parameterized random cropping method.","authors":["Mohamed Hassan","Mohammad Wasil","Sebastian Houben"],"url":"https://arxiv.org/abs/2504.19824"}
{"created":"2025-04-29","title":"HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination","abstract":"We present HOIGaze - a novel learning-based approach for gaze estimation during hand-object interactions (HOI) in extended reality (XR). HOIGaze addresses the challenging HOI setting by building on one key insight: The eye, hand, and head movements are closely coordinated during HOIs and this coordination can be exploited to identify samples that are most useful for gaze estimator training - as such, effectively denoising the training data. This denoising approach is in stark contrast to previous gaze estimation methods that treated all training samples as equal. Specifically, we propose: 1) a novel hierarchical framework that first recognises the hand currently visually attended to and then estimates gaze direction based on the attended hand; 2) a new gaze estimator that uses cross-modal Transformers to fuse head and hand-object features extracted using a convolutional neural network and a spatio-temporal graph convolutional network; and 3) a novel eye-head coordination loss that upgrades training samples belonging to the coordinated eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin (ADT) datasets and show that it significantly outperforms state-of-the-art methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in mean angular error. To demonstrate the potential of our method, we further report significant performance improvements for the sample downstream task of eye-based activity recognition on ADT. Taken together, our results underline the significant information content available in eye-hand-head coordination and, as such, open up an exciting new direction for learning-based gaze estimation.","authors":["Zhiming Hu","Daniel Haeufle","Syn Schmitt","Andreas Bulling"],"url":"https://arxiv.org/abs/2504.19828"}
{"created":"2025-04-29","title":"AnimateAnywhere: Rouse the Background in Human Image Animation","abstract":"Human image animation aims to generate human videos of given characters and backgrounds that adhere to the desired pose sequence. However, existing methods focus more on human actions while neglecting the generation of background, which typically leads to static results or inharmonious movements. The community has explored camera pose-guided animation tasks, yet preparing the camera trajectory is impractical for most entertainment applications and ordinary users. As a remedy, we present an AnimateAnywhere framework, rousing the background in human image animation without requirements on camera trajectories. In particular, based on our key insight that the movement of the human body often reflects the motion of the background, we introduce a background motion learner (BML) to learn background motions from human pose sequences. To encourage the model to learn more accurate cross-frame correspondences, we further deploy an epipolar constraint on the 3D attention map. Specifically, the mask used to suppress geometrically unreasonable attention is carefully constructed by combining an epipolar mask and the current 3D attention map. Extensive experiments demonstrate that our AnimateAnywhere effectively learns the background motion from human pose sequences, achieving state-of-the-art performance in generating human animation results with vivid and realistic backgrounds. The source code and model will be available at https://github.com/liuxiaoyu1104/AnimateAnywhere.","authors":["Xiaoyu Liu","Mingshuai Yao","Yabo Zhang","Xianhui Lin","Peiran Ren","Xiaoming Li","Ming Liu","Wangmeng Zuo"],"url":"https://arxiv.org/abs/2504.19834"}
{"created":"2025-04-29","title":"Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production","abstract":"This study examines the digital value chain in automotive manufacturing, focusing on the identification, software flashing, customization, and commissioning of electronic control units in vehicle networks. A novel precedence graph design is proposed to optimize this process chain using an automated scheduling algorithm that employs mixed integer linear programming techniques. The results show significant improvements in key metrics. The algorithm reduces the number of production stations equipped with expensive hardware and software to execute digital value chain processes, while increasing capacity utilization through efficient scheduling and reduced idle time. Task parallelization is optimized, resulting in streamlined workflows and increased throughput. Compared to the traditional method, the automated approach has reduced preparation time by 50% and reduced scheduling activities, as it now takes two minutes to create the precedence graph. The flexibility of the algorithm's constraints allows for vehicle-specific configurations while maintaining high responsiveness, eliminating backup stations and facilitating the integration of new topologies. Automated scheduling significantly outperforms manual methods in efficiency, functionality, and adaptability.","authors":["Cornelius Hake","Christian Friedrich"],"url":"https://arxiv.org/abs/2504.19835"}
{"created":"2025-04-29","title":"LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects","abstract":"With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.","authors":["Guangyi Liu","Pengxiang Zhao","Liang Liu","Yaxuan Guo","Han Xiao","Weifeng Lin","Yuxiang Chai","Yue Han","Shuai Ren","Hao Wang","Xiaoyu Liang","Wenhao Wang","Tianze Wu","Linghao Li","Hao Wang","Guanjing Xiong","Yong Liu","Hongsheng Li"],"url":"https://arxiv.org/abs/2504.19838"}
{"created":"2025-04-29","title":"SRMF: A Data Augmentation and Multimodal Fusion Approach for Long-Tail UHR Satellite Image Segmentation","abstract":"The long-tail problem presents a significant challenge to the advancement of semantic segmentation in ultra-high-resolution (UHR) satellite imagery. While previous efforts in UHR semantic segmentation have largely focused on multi-branch network architectures that emphasize multi-scale feature extraction and fusion, they have often overlooked the importance of addressing the long-tail issue. In contrast to prior UHR methods that focused on independent feature extraction, we emphasize data augmentation and multimodal feature fusion to alleviate the long-tail problem. In this paper, we introduce SRMF, a novel framework for semantic segmentation in UHR satellite imagery. Our approach addresses the long-tail class distribution by incorporating a multi-scale cropping technique alongside a data augmentation strategy based on semantic reordering and resampling. To further enhance model performance, we propose a multimodal fusion-based general representation knowledge injection method, which, for the first time, fuses text and visual features without the need for individual region text descriptions, extracting more robust features. Extensive experiments on the URUR, GID, and FBP datasets demonstrate that our method improves mIoU by 3.33\\%, 0.66\\%, and 0.98\\%, respectively, achieving state-of-the-art performance. Code is available at: https://github.com/BinSpa/SRMF.git.","authors":["Yulong Guo","Zilun Zhang","Yongheng Shang","Tiancheng Zhao","Shuiguang Deng","Yingchun Yang","Jianwei Yin"],"url":"https://arxiv.org/abs/2504.19839"}
{"created":"2025-04-29","title":"Near-Optimal Minimum Cuts in Hypergraphs at Scale","abstract":"The hypergraph minimum cut problem aims to partition its vertices into two blocks while minimizing the total weight of the cut hyperedges. This fundamental problem arises in network reliability, VLSI design, and community detection. We present HeiCut, a scalable algorithm for computing near-optimal minimum cuts in both unweighted and weighted hypergraphs. HeiCut aggressively reduces the hypergraph size through a sequence of provably exact reductions that preserve the minimum cut, along with an optional heuristic contraction based on label propagation. It then solves a relaxed Binary Integer Linear Program (BIP) on the reduced hypergraph to compute a near-optimal minimum cut. Our extensive evaluation on over 500 real-world hypergraphs shows that HeiCut computes the exact minimum cut in over 85% of instances using our exact reductions alone, and offers the best solution quality across all instances. It solves over twice as many instances as the state-of-the-art within set computational limits, and is up to five orders of magnitude faster.","authors":["Adil Chhabra","Christian Schulz","Bora U\\c{c}ar","Loris Wilwert"],"url":"https://arxiv.org/abs/2504.19842"}
{"created":"2025-04-29","title":"Clustering-based Recurrent Neural Network Controller synthesis under Signal Temporal Logic Specifications","abstract":"Autonomous robotic systems require advanced control frameworks to achieve complex temporal objectives that extend beyond conventional stability and trajectory tracking. Signal Temporal Logic (STL) provides a formal framework for specifying such objectives, with robustness metrics widely employed for control synthesis. Existing optimization-based approaches using neural network (NN)-based controllers often rely on a single NN for both learning and control. However, variations in initial states and obstacle configurations can lead to discontinuous changes in the optimization solution, thereby degrading generalization and control performance. To address this issue, this study proposes a method to enhance recurrent neural network (RNN)-based control by clustering solution trajectories that satisfy STL specifications under diverse initial conditions. The proposed approach utilizes trajectory similarity metrics to generate clustering labels, which are subsequently used to train a classification network. This network assigns new initial states and obstacle configurations to the appropriate cluster, enabling the selection of a specialized controller. By explicitly accounting for variations in solution trajectories, the proposed method improves both estimation accuracy and control performance. Numerical experiments on a dynamic vehicle path planning problem demonstrate the effectiveness of the approach.","authors":["Kazunobu Serizawa","Kazumune Hashimoto","Wataru Hashimoto","Masako Kishida","Shigemasa Takai"],"url":"https://arxiv.org/abs/2504.19846"}
{"created":"2025-04-29","title":"Foundation Model-Driven Framework for Human-Object Interaction Prediction with Segmentation Mask Integration","abstract":"In this work, we introduce Segmentation to Human-Object Interaction (\\textit{\\textbf{Seg2HOI}}) approach, a novel framework that integrates segmentation-based vision foundation models with the human-object interaction task, distinguished from traditional detection-based Human-Object Interaction (HOI) methods. Our approach enhances HOI detection by not only predicting the standard triplets but also introducing quadruplets, which extend HOI triplets by including segmentation masks for human-object pairs. More specifically, Seg2HOI inherits the properties of the vision foundation model (e.g., promptable and interactive mechanisms) and incorporates a decoder that applies these attributes to HOI task. Despite training only for HOI, without additional training mechanisms for these properties, the framework demonstrates that such features still operate efficiently. Extensive experiments on two public benchmark datasets demonstrate that Seg2HOI achieves performance comparable to state-of-the-art methods, even in zero-shot scenarios. Lastly, we propose that Seg2HOI can generate HOI quadruplets and interactive HOI segmentation from novel text and visual prompts that were not used during training, making it versatile for a wide range of applications by leveraging this flexibility.","authors":["Juhan Park","Kyungjae Lee","Hyung Jin Chang","Jungchan Cho"],"url":"https://arxiv.org/abs/2504.19847"}
{"created":"2025-04-29","title":"Human-Centered AI and Autonomy in Robotics: Insights from a Bibliometric Study","abstract":"The development of autonomous robotic systems offers significant potential for performing complex tasks with precision and consistency. Recent advances in Artificial Intelligence (AI) have enabled more capable intelligent automation systems, addressing increasingly complex challenges. However, this progress raises questions about human roles in such systems. Human-Centered AI (HCAI) aims to balance human control and automation, ensuring performance enhancement while maintaining creativity, mastery, and responsibility. For real-world applications, autonomous robots must balance task performance with reliability, safety, and trustworthiness. Integrating HCAI principles enhances human-robot collaboration and ensures responsible operation.","authors":["Simona Casini","Pietro Ducange","Francesco Marcelloni","Lorenzo Pollini"],"url":"https://arxiv.org/abs/2504.19848"}
{"created":"2025-04-29","title":"To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels","abstract":"This article presents the results of a pilot study involving the reception of a fictional short story translated from English into Dutch under four conditions: machine translation (MT), post-editing (PE), human translation (HT) and original source text (ST). The aim is to understand how creativity and errors in different translation modalities affect readers, specifically regarding cognitive load. Eight participants filled in a questionnaire, read a story using an eye-tracker, and conducted a retrospective think-aloud (RTA) interview. The results show that units of creative potential (UCP) increase cognitive load and that this effect is highest for HT and lowest for MT; no effect of error was observed. Triangulating the data with RTAs leads us to hypothesize that the higher cognitive load in UCPs is linked to increases in reader enjoyment and immersion. The effect of translation creativity on cognitive load in different translation modalities at word-level is novel and opens up new avenues for further research. All the code and data are available at https://github.com/INCREC/Pilot_to_MT_or_not_to_MT","authors":["Kyo Gerrits","Ana Guerberof-Arenas"],"url":"https://arxiv.org/abs/2504.19850"}
{"created":"2025-04-29","title":"Do You Know the Way? Human-in-the-Loop Understanding for Fast Traversability Estimation in Mobile Robotics","abstract":"The increasing use of robots in unstructured environments necessitates the development of effective perception and navigation strategies to enable field robots to successfully perform their tasks. In particular, it is key for such robots to understand where in their environment they can and cannot travel -- a task known as traversability estimation. However, existing geometric approaches to traversability estimation may fail to capture nuanced representations of traversability, whereas vision-based approaches typically either involve manually annotating a large number of images or require robot experience. In addition, existing methods can struggle to address domain shifts as they typically do not learn during deployment. To this end, we propose a human-in-the-loop (HiL) method for traversability estimation that prompts a human for annotations as-needed. Our method uses a foundation model to enable rapid learning on new annotations and to provide accurate predictions even when trained on a small number of quickly-provided HiL annotations. We extensively validate our method in simulation and on real-world data, and demonstrate that it can provide state-of-the-art traversability prediction performance.","authors":["Andre Schreiber","Katherine Driggs-Campbell"],"url":"https://arxiv.org/abs/2504.19851"}
{"created":"2025-04-29","title":"A Formal Framework for Naturally Specifying and Verifying Sequential Algorithms","abstract":"Current approaches for formal verification of algorithms face important limitations. For specification, they cannot express algorithms naturally and concisely, especially for algorithms with states and flexible control flow. For verification, formal proof based on Hoare logic cannot reflect the logical structure of natural proof. To address these challenges, we introduce a formal framework for naturally specifying and verifying sequential algorithms in Coq. We use the state relation monad to integrate Coq's expressive type system with the flexible control flow of imperative languages. It supports nondeterministic operations and customizable program states, enabling specifying algorithms at an appropriate level of abstraction. For verification, we build a Hoare logic for the monad and propose a novel two-stage proof approach that separates natural logical reasoning from mechanical composition. It reflects the logical structure of natural proof, enhancing modularity and readability. We evaluate the framework by formalizing the Depth-First Search (DFS) algorithm and verifying the Knuth-Morris-Pratt (KMP) algorithm.","authors":["Chengxi Yang","Shushu Wu","Qinxiang Cao"],"url":"https://arxiv.org/abs/2504.19852"}
{"created":"2025-04-29","title":"NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks","abstract":"Existing Visual-Language-Action (VLA) models have shown promising performance in zero-shot scenarios, demonstrating impressive task execution and reasoning capabilities. However, a significant challenge arises from the limitations of visual encoding, which can result in failures during tasks such as object grasping. Moreover, these models typically suffer from high computational overhead due to their large sizes, often exceeding 7B parameters. While these models excel in reasoning and task planning, the substantial computational overhead they incur makes them impractical for real-time robotic environments, where speed and efficiency are paramount. To address the limitations of existing VLA models, we propose NORA, a 3B-parameter model designed to reduce computational overhead while maintaining strong task performance. NORA adopts the Qwen-2.5-VL-3B multimodal model as its backbone, leveraging its superior visual-semantic understanding to enhance visual reasoning and action grounding. Additionally, our \\model{} is trained on 970k real-world robot demonstrations and equipped with the FAST+ tokenizer for efficient action sequence generation. Experimental results demonstrate that NORA outperforms existing large-scale VLA models, achieving better task performance with significantly reduced computational overhead, making it a more practical solution for real-time robotic autonomy.","authors":["Chia-Yu Hung","Qi Sun","Pengfei Hong","Amir Zadeh","Chuan Li","U-Xuan Tan","Navonil Majumder","Soujanya Poria"],"url":"https://arxiv.org/abs/2504.19854"}
{"created":"2025-04-29","title":"The Automation Advantage in AI Red Teaming","abstract":"This paper analyzes Large Language Model (LLM) security vulnerabilities based on data from Crucible, encompassing 214,271 attack attempts by 1,674 users across 30 LLM challenges. Our findings reveal automated approaches significantly outperform manual techniques (69.5% vs 47.6% success rate), despite only 5.2% of users employing automation. We demonstrate that automated approaches excel in systematic exploration and pattern matching challenges, while manual approaches retain speed advantages in certain creative reasoning scenarios, often solving problems 5x faster when successful. Challenge categories requiring systematic exploration are most effectively targeted through automation, while intuitive challenges sometimes favor manual techniques for time-to-solve metrics. These results illuminate how algorithmic testing is transforming AI red-teaming practices, with implications for both offensive security research and defensive measures. Our analysis suggests optimal security testing combines human creativity for strategy development with programmatic execution for thorough exploration.","authors":["Rob Mulla","Will Pearce","Nick Landers","Brian Greunke","Brad Palm","Vincent Abruzzo","Ads Dawson"],"url":"https://arxiv.org/abs/2504.19855"}
{"created":"2025-04-29","title":"Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language","abstract":"Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., language masking. Although popular, it requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that this approach performs better than traditional DAPT by 3.5 of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times less computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.","authors":["Anastasia Zhukova","Christian E. Matt","Terry Ruas","Bela Gipp"],"url":"https://arxiv.org/abs/2504.19856"}
{"created":"2025-04-29","title":"CoherenDream: Boosting Holistic Text Coherence in 3D Generation via Multimodal Large Language Models Feedback","abstract":"Score Distillation Sampling (SDS) has achieved remarkable success in text-to-3D content generation. However, SDS-based methods struggle to maintain semantic fidelity for user prompts, particularly when involving multiple objects with intricate interactions. While existing approaches often address 3D consistency through multiview diffusion model fine-tuning on 3D datasets, this strategy inadvertently exacerbates text-3D alignment degradation. The limitation stems from SDS's inherent accumulation of view-independent biases during optimization, which progressively diverges from the ideal text alignment direction. To alleviate this limitation, we propose a novel SDS objective, dubbed as Textual Coherent Score Distillation (TCSD), which integrates alignment feedback from multimodal large language models (MLLMs). Our TCSD leverages cross-modal understanding capabilities of MLLMs to assess and guide the text-3D correspondence during the optimization. We further develop 3DLLaVA-CRITIC - a fine-tuned MLLM specialized for evaluating multiview text alignment in 3D generations. Additionally, we introduce an LLM-layout initialization that significantly accelerates optimization convergence through semantic-aware spatial configuration. Comprehensive evaluations demonstrate that our framework, CoherenDream, establishes state-of-the-art performance in text-aligned 3D generation across multiple benchmarks, including T$^3$Bench and TIFA subset. Qualitative results showcase the superior performance of CoherenDream in preserving textual consistency and semantic interactions. As the first study to incorporate MLLMs into SDS optimization, we also conduct extensive ablation studies to explore optimal MLLM adaptations for 3D generation tasks.","authors":["Chenhan Jiang","Yihan Zeng","Hang Xu","Dit-Yan Yeung"],"url":"https://arxiv.org/abs/2504.19860"}
{"created":"2025-04-29","title":"Towards Ball Spin and Trajectory Analysis in Table Tennis Broadcast Videos via Physically Grounded Synthetic-to-Real Transfer","abstract":"Analyzing a player's technique in table tennis requires knowledge of the ball's 3D trajectory and spin. While, the spin is not directly observable in standard broadcasting videos, we show that it can be inferred from the ball's trajectory in the video. We present a novel method to infer the initial spin and 3D trajectory from the corresponding 2D trajectory in a video. Without ground truth labels for broadcast videos, we train a neural network solely on synthetic data. Due to the choice of our input data representation, physically correct synthetic training data, and using targeted augmentations, the network naturally generalizes to real data. Notably, these simple techniques are sufficient to achieve generalization. No real data at all is required for training. To the best of our knowledge, we are the first to present a method for spin and trajectory prediction in simple monocular broadcast videos, achieving an accuracy of 92.0% in spin classification and a 2D reprojection error of 0.19% of the image diagonal.","authors":["Daniel Kienzle","Robin Sch\\\"on","Rainer Lienhart","Shin'Ichi Satoh"],"url":"https://arxiv.org/abs/2504.19863"}
{"created":"2025-04-29","title":"semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage","abstract":"Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.","authors":["Ke Hong","Lufang Chen","Zhong Wang","Xiuhong Li","Qiuli Mao","Jianping Ma","Chao Xiong","Guanyu Wu","Buhe Han","Guohao Dai","Yun Liang","Yu Wang"],"url":"https://arxiv.org/abs/2504.19867"}
{"created":"2025-04-29","title":"TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate","abstract":"Vector quantization, a problem rooted in Shannon's source coding theory, aims to quantize high-dimensional Euclidean vectors while minimizing distortion in their geometric structure. We propose TurboQuant to address both mean-squared error (MSE) and inner product distortion, overcoming limitations of existing methods that fail to achieve optimal distortion rates. Our data-oblivious algorithms, suitable for online applications, achieve near-optimal distortion rates (within a small constant factor) across all bit-widths and dimensions. TurboQuant achieves this by randomly rotating input vectors, inducing a concentrated Beta distribution on coordinates, and leveraging the near-independence property of distinct coordinates in high dimensions to simply apply optimal scalar quantizers per each coordinate. Recognizing that MSE-optimal quantizers introduce bias in inner product estimation, we propose a two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL (QJL) transform on the residual, resulting in an unbiased inner product quantizer. We also provide a formal proof of the information-theoretic lower bounds on best achievable distortion rate by any vector quantizer, demonstrating that TurboQuant closely matches these bounds, differing only by a small constant ($\\approx 2.7$) factor. Experimental results validate our theoretical findings, showing that for KV cache quantization, we achieve absolute quality neutrality with 3.5 bits per channel and marginal quality degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search tasks, our method outperforms existing product quantization techniques in recall while reducing indexing time to virtually zero.","authors":["Amir Zandieh","Majid Daliri","Majid Hadian","Vahab Mirrokni"],"url":"https://arxiv.org/abs/2504.19874"}
{"created":"2025-04-29","title":"DeeCLIP: A Robust and Generalizable Transformer-Based Framework for Detecting AI-Generated Images","abstract":"This paper introduces DeeCLIP, a novel framework for detecting AI-generated images using CLIP-ViT and fusion learning. Despite significant advancements in generative models capable of creating highly photorealistic images, existing detection methods often struggle to generalize across different models and are highly sensitive to minor perturbations. To address these challenges, DeeCLIP incorporates DeeFuser, a fusion module that combines high-level and low-level features, improving robustness against degradations such as compression and blurring. Additionally, we apply triplet loss to refine the embedding space, enhancing the model's ability to distinguish between real and synthetic content. To further enable lightweight adaptation while preserving pre-trained knowledge, we adopt parameter-efficient fine-tuning using low-rank adaptation (LoRA) within the CLIP-ViT backbone. This approach supports effective zero-shot learning without sacrificing generalization. Trained exclusively on 4-class ProGAN data, DeeCLIP achieves an average accuracy of 89.00% on 19 test subsets composed of generative adversarial network (GAN) and diffusion models. Despite having fewer trainable parameters, DeeCLIP outperforms existing methods, demonstrating superior robustness against various generative models and real-world distortions. The code is publicly available at https://github.com/Mamadou-Keita/DeeCLIP for research purposes.","authors":["Mamadou Keita","Wassim Hamidouche","Hessen Bougueffa Eutamene","Abdelmalik Taleb-Ahmed","Abdenour Hadid"],"url":"https://arxiv.org/abs/2504.19876"}
{"created":"2025-04-29","title":"FoldedHexaTorus: An Inter-Chiplet Interconnect Topology for Chiplet-based Systems using Organic and Glass Substrates","abstract":"Chiplet-based systems are rapidly gaining traction in the market. Two packaging options for such systems are the established organic substrates and the emerging glass substrates. These substrates are used to implement the inter-chiplet interconnect (ICI), which is crucial for overall system performance. To guide the development of ICIs, we introduce three design principles for ICI network topologies on organic and glass substrates. Based on our design principles, we propose the novel FoldedHexaTorus network topology. Our evaluation shows that the FoldedHexaTorus achieves significantly higher throughput than state-of-the-art topologies while maintaining low latency.","authors":["Patrick Iff","Maciej Besta","Torsten Hoefler"],"url":"https://arxiv.org/abs/2504.19878"}
{"created":"2025-04-29","title":"Using Fixed and Mobile Eye Tracking to Understand How Visitors View Art in a Museum: A Study at the Bowes Museum, County Durham, UK","abstract":"The following paper describes a collaborative project involving researchers at Durham University, and professionals at the Bowes Museum, Barnard Castle, County Durham, UK, during which we used fixed and mobile eye tracking to understand how visitors view art. Our study took place during summer 2024 and builds on work presented at DH2017 (Bailey-Ross et al., 2017). Our interdisciplinary team included researchers from digital humanities, psychology, art history and computer science, working in collaboration with professionals from the museum. We used fixed and mobile eye tracking to understand how museum visitors view art in a physical gallery setting. This research will enable us to make recommendations about how the Museum's collections could be more effectively displayed, encouraging visitors to engage with them more fully.","authors":["Claire Warwick","Andrew Beresford","Soazig Casteau","Hubert P. H. Shum","Dan Smith","Francis Xiatian Zhang"],"url":"https://arxiv.org/abs/2504.19881"}
{"created":"2025-04-29","title":"Federated Out-of-Distribution Generalization: A Causal Augmentation View","abstract":"Federated learning aims to collaboratively model by integrating multi-source information to obtain a model that can generalize across all client data. Existing methods often leverage knowledge distillation or data augmentation to mitigate the negative impact of data bias across clients. However, the limited performance of teacher models on out-of-distribution samples and the inherent quality gap between augmented and original data hinder their effectiveness and they typically fail to leverage the advantages of incorporating rich contextual information. To address these limitations, this paper proposes a Federated Causal Augmentation method, termed FedCAug, which employs causality-inspired data augmentation to break the spurious correlation between attributes and categories. Specifically, it designs a causal region localization module to accurately identify and decouple the background and objects in the image, providing rich contextual information for causal data augmentation. Additionally, it designs a causality-inspired data augmentation module that integrates causal features and within-client context to generate counterfactual samples. This significantly enhances data diversity, and the entire process does not require any information sharing between clients, thereby contributing to the protection of data privacy. Extensive experiments conducted on three datasets reveal that FedCAug markedly reduces the model's reliance on background to predict sample labels, achieving superior performance compared to state-of-the-art methods.","authors":["Runhui Zhang","Sijin Zhou","Zhuang Qi"],"url":"https://arxiv.org/abs/2504.19882"}
{"created":"2025-04-29","title":"Enhancing breast cancer detection on screening mammogram using self-supervised learning and a hybrid deep model of Swin Transformer and Convolutional Neural Network","abstract":"Purpose: The scarcity of high-quality curated labeled medical training data remains one of the major limitations in applying artificial intelligence (AI) systems to breast cancer diagnosis. Deep models for mammogram analysis and mass (or micro-calcification) detection require training with a large volume of labeled images, which are often expensive and time-consuming to collect. To reduce this challenge, we proposed a novel method that leverages self-supervised learning (SSL) and a deep hybrid model, named \\textbf{HybMNet}, which combines local self-attention and fine-grained feature extraction to enhance breast cancer detection on screening mammograms.","authors":["Han Chen","Anne L. Martel"],"url":"https://arxiv.org/abs/2504.19888"}
{"created":"2025-04-29","title":"CineVerse: Consistent Keyframe Synthesis for Cinematic Scene Composition","abstract":"We present CineVerse, a novel framework for the task of cinematic scene composition. Similar to traditional multi-shot generation, our task emphasizes the need for consistency and continuity across frames. However, our task also focuses on addressing challenges inherent to filmmaking, such as multiple characters, complex interactions, and visual cinematic effects. In order to learn to generate such content, we first create the CineVerse dataset. We use this dataset to train our proposed two-stage approach. First, we prompt a large language model (LLM) with task-specific instructions to take in a high-level scene description and generate a detailed plan for the overall setting and characters, as well as the individual shots. Then, we fine-tune a text-to-image generation model to synthesize high-quality visual keyframes. Experimental results demonstrate that CineVerse yields promising improvements in generating visually coherent and contextually rich movie scenes, paving the way for further exploration in cinematic video synthesis.","authors":["Quynh Phung","Long Mai","Fabian David Caba Heilbron","Feng Liu","Jia-Bin Huang","Cusuh Ham"],"url":"https://arxiv.org/abs/2504.19894"}
{"created":"2025-04-29","title":"GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets","abstract":"As a fundamental task in machine learning, text classification plays a crucial role in many areas. With the rapid scaling of Large Language Models (LLMs), particularly through reinforcement learning (RL), there is a growing need for more capable discriminators. Consequently, advances in classification are becoming increasingly vital for enhancing the overall capabilities of LLMs. Traditional discriminative methods map text to labels but overlook LLMs' intrinsic generative strengths. Generative classification addresses this by prompting the model to directly output labels. However, existing studies still rely on simple SFT alone, seldom probing the interplay between training and inference prompts, and no work has systematically leveraged RL for generative text classifiers and unified SFT, RL, and inference-time prompting in one framework. We bridge this gap with GenCLS++, a framework that jointly optimizes SFT and RL while systematically exploring five high-level strategy dimensions-in-context learning variants, category definitions, explicit uncertainty labels, semantically irrelevant numeric labels, and perplexity-based decoding-during both training and inference. After an SFT \"policy warm-up,\" we apply RL with a simple rule-based reward, yielding sizable extra gains. Across seven datasets, GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline; on public datasets, this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that benefit from explicit thinking processes, we find that classification tasks perform better without such reasoning steps. These insights into the role of explicit reasoning provide valuable guidance for future LLM applications.","authors":["Mingqian He","Fei Zhao","Chonggang Lu","Ziyan Liu","Yue Wang","Haofu Qian"],"url":"https://arxiv.org/abs/2504.19898"}
{"created":"2025-04-29","title":"Breast Cancer Detection from Multi-View Screening Mammograms with Visual Prompt Tuning","abstract":"Accurate detection of breast cancer from high-resolution mammograms is crucial for early diagnosis and effective treatment planning. Previous studies have shown the potential of using single-view mammograms for breast cancer detection. However, incorporating multi-view data can provide more comprehensive insights. Multi-view classification, especially in medical imaging, presents unique challenges, particularly when dealing with large-scale, high-resolution data. In this work, we propose a novel Multi-view Visual Prompt Tuning Network (MVPT-NET) for analyzing multiple screening mammograms. We first pretrain a robust single-view classification model on high-resolution mammograms and then innovatively adapt multi-view feature learning into a task-specific prompt tuning process. This technique selectively tunes a minimal set of trainable parameters (7\\%) while retaining the robustness of the pre-trained single-view model, enabling efficient integration of multi-view data without the need for aggressive downsampling. Our approach offers an efficient alternative to traditional feature fusion methods, providing a more robust, scalable, and efficient solution for high-resolution mammogram analysis. Experimental results on a large multi-institution dataset demonstrate that our method outperforms conventional approaches while maintaining detection efficiency, achieving an AUROC of 0.852 for distinguishing between Benign, DCIS, and Invasive classes. This work highlights the potential of MVPT-NET for medical imaging tasks and provides a scalable solution for integrating multi-view data in breast cancer detection.","authors":["Han Chen","Anne L. Martel"],"url":"https://arxiv.org/abs/2504.19900"}
{"created":"2025-04-29","title":"Attention Mechanism, Max-Affine Partition, and Universal Approximation","abstract":"We establish the universal approximation capability of single-layer, single-head self- and cross-attention mechanisms with minimal attached structures. Our key insight is to interpret single-head attention as an input domain-partition mechanism that assigns distinct values to subregions. This allows us to engineer the attention weights such that this assignment imitates the target function. Building on this, we prove that a single self-attention layer, preceded by sum-of-linear transformations, is capable of approximating any continuous function on a compact domain under the $L_\\infty$-norm. Furthermore, we extend this construction to approximate any Lebesgue integrable function under $L_p$-norm for $1\\leq p <\\infty$. Lastly, we also extend our techniques and show that, for the first time, single-head cross-attention achieves the same universal approximation guarantees.","authors":["Hude Liu","Jerry Yao-Chieh Hu","Zhao Song","Han Liu"],"url":"https://arxiv.org/abs/2504.19901"}
{"created":"2025-04-29","title":"Convergence Analysis of Asynchronous Federated Learning with Gradient Compression for Non-Convex Optimization","abstract":"Gradient compression is an effective technique for reducing communication costs in federated learning (FL), and error feedback (EF) is usually adopted to remedy the compression errors. However, there remains a lack of systematic study on these techniques in asynchronous FL. In this paper, we fill this gap by analyzing the convergence behaviors of FL under different frameworks. We firstly consider a basic asynchronous FL framework AsynFL, and provide an improved convergence analysis that relies on fewer assumptions and yields a superior convergence rate than prior studies. Then, we consider a variant framework with gradient compression, AsynFLC. We show sufficient conditions for its convergence to the optimum, indicating the interaction between asynchronous delay and compression rate. Our analysis also demonstrates that asynchronous delay amplifies the variance caused by compression, thereby hindering convergence, and such an impact is exacerbated by high data heterogeneity. Furthermore, we study the convergence of AsynFLC-EF, the framework that further integrates EF. We prove that EF can effectively reduce the variance of gradient estimation despite asynchronous delay, which enables AsynFLC-EF to match the convergence rate of AsynFL. We also show that the impact of asynchronous delay on EF is limited to slowing down the higher-order convergence term. Experimental results substantiate our analytical findings very well.","authors":["Diying Yang","Yingwei Hou","Danyang Xiao","Weigang Wu"],"url":"https://arxiv.org/abs/2504.19903"}
{"created":"2025-04-29","title":"Can AI Agents Design and Implement Drug Discovery Pipelines?","abstract":"The rapid advancement of artificial intelligence, particularly autonomous agentic systems based on Large Language Models (LLMs), presents new opportunities to accelerate drug discovery by improving in-silico modeling and reducing dependence on costly experimental trials. Current AI agent-based systems demonstrate proficiency in solving programming challenges and conducting research, indicating an emerging potential to develop software capable of addressing complex problems such as pharmaceutical design and drug discovery. This paper introduces DO Challenge, a benchmark designed to evaluate the decision-making abilities of AI agents in a single, complex problem resembling virtual screening scenarios. The benchmark challenges systems to independently develop, implement, and execute efficient strategies for identifying promising molecular structures from extensive datasets, while navigating chemical space, selecting models, and managing limited resources in a multi-objective context. We also discuss insights from the DO Challenge 2025, a competition based on the proposed benchmark, which showcased diverse strategies explored by human participants. Furthermore, we present the Deep Thought multi-agent system, which demonstrated strong performance on the benchmark, outperforming most human teams. Among the language models tested, Claude 3.7 Sonnet, Gemini 2.5 Pro and o3 performed best in primary agent roles, and GPT-4o, Gemini 2.0 Flash were effective in auxiliary roles. While promising, the system's performance still fell short of expert-designed solutions and showed high instability, highlighting both the potential and current limitations of AI-driven methodologies in transforming drug discovery and broader scientific research.","authors":["Khachik Smbatyan","Tsolak Ghukasyan","Tigran Aghajanyan","Hovhannes Dabaghyan","Sergey Adamyan","Aram Bughdaryan","Vahagn Altunyan","Gagik Navasardyan","Aram Davtyan","Anush Hakobyan","Aram Gharibyan","Arman Fahradyan","Artur Hakobyan","Hasmik Mnatsakanyan","Narek Ginoyan","Garik Petrosyan"],"url":"https://arxiv.org/abs/2504.19912"}
{"created":"2025-04-29","title":"Lossy Source Coding with Focal Loss","abstract":"Focal loss has recently gained significant popularity, particularly in tasks like object detection where it helps to address class imbalance by focusing more on hard-to-classify examples. This work proposes the focal loss as a distortion measure for lossy source coding. The paper provides single-shot converse and achievability bounds. These bounds are then used to characterize the distortion-rate trade-off in the infinite blocklength, which is shown to be the same as that for the log loss case. In the non-asymptotic case, the difference between focal loss and log loss is illustrated through a series of simulations.","authors":["Alex Dytso","Martina Cardone"],"url":"https://arxiv.org/abs/2504.19913"}
{"created":"2025-04-29","title":"An Achievability Bound for Type-Based Unsourced Multiple Access","abstract":"We derive an achievability bound to quantify the performance of a type-based unsourced multiple access system -- an information-theoretic model for grant-free multiple access with correlated messages. The bound extends available achievability results for the per-user error probability in the unsourced multiple access framework, where, different from our setup, message collisions are treated as errors. Specifically, we provide an upper bound on the total variation distance between the type (i.e., the empirical probability mass function) of the transmitted messages and its estimate over a Gaussian multiple access channel. Through numerical simulations, we illustrate that our bound can be used to determine the message type that is less efficient to transmit, because more difficult to detect. We finally show that a practical scheme for type estimation, based on coded compressed sensing with approximate message passing, operates approximately 3 dB away from the bound, for the parameters considered in the paper.","authors":["Deekshith Pathayappilly Krishnan","Kaan Okumus","Khac-Hoang Ngo","Giuseppe Durisi"],"url":"https://arxiv.org/abs/2504.19916"}
{"created":"2025-04-29","title":"Enhancing Surgical Documentation through Multimodal Visual-Temporal Transformers and Generative AI","abstract":"The automatic summarization of surgical videos is essential for enhancing procedural documentation, supporting surgical training, and facilitating post-operative analysis. This paper presents a novel method at the intersection of artificial intelligence and medicine, aiming to develop machine learning models with direct real-world applications in surgical contexts. We propose a multi-modal framework that leverages recent advancements in computer vision and large language models to generate comprehensive video summaries. %","authors":["Hugo Georgenthum","Cristian Cosentino","Fabrizio Marozzo","Pietro Li\\`o"],"url":"https://arxiv.org/abs/2504.19918"}
{"created":"2025-04-29","title":"Accelerating Mixture-of-Experts Training with Adaptive Expert Replication","abstract":"Mixture-of-Experts (MoE) models have become a widely adopted solution to continue scaling model sizes without a corresponding linear increase in compute. During MoE model training, each input token is dynamically routed to a subset of experts -- sparsely-activated feed-forward networks -- within each transformer layer. The distribution of tokens assigned to each expert varies widely and rapidly over the course of training. To handle the wide load imbalance across experts, current systems are forced to either drop tokens assigned to popular experts, degrading convergence, or frequently rebalance resources allocated to each expert based on popularity, incurring high state migration overheads.","authors":["Athinagoras Skiadopoulos","Mark Zhao","Swapnil Gandhi","Thomas Norrie","Shrijeet Mukherjee","Christos Kozyrakis"],"url":"https://arxiv.org/abs/2504.19925"}
{"created":"2025-04-29","title":"Skew generalized quasi-cyclic codes over non-chain ring $F_q+vF_q$","abstract":"For a prime $p$, let $F_q$ be the finite field of order $q= p^d$. This paper presents the study on skew generalized quasi-cyclic (SGQC) codes of length $n$ over the non-chain ring $F_q+vF_q$ where $v^2=v$ and $\\theta_t$ is the Galois automorphism. Here, first, we prove the dual of an SGQC code of length $n$ is also an SGQC code of the same length and derive a necessary and sufficient condition for the existence of a self-dual SGQC code. Then, we discuss the $1$-generator polynomial and the $\\rho$-generator polynomial for skew generalized quasi-cyclic codes. Further, we determine the dimension and BCH type bound for the 1-generator skew generalized quasi-cyclic codes. As a by-product, with the help of MAGMA software, we provide a few examples of SGQC codes and obtain some $2$-generator SGQC codes of index $2$.","authors":["Kundan Suxena","Indibar Debnath","Om Prakash"],"url":"https://arxiv.org/abs/2504.19926"}
{"created":"2025-04-29","title":"Automated decision-making for dynamic task assignment at scale","abstract":"The Dynamic Task Assignment Problem (DTAP) concerns matching resources to tasks in real time while minimizing some objectives, like resource costs or task cycle time. In this work, we consider a DTAP variant where every task is a case composed of a stochastic sequence of activities. The DTAP, in this case, involves the decision of which employee to assign to which activity to process requests as quickly as possible. In recent years, Deep Reinforcement Learning (DRL) has emerged as a promising tool for tackling this DTAP variant, but most research is limited to solving small-scale, synthetic problems, neglecting the challenges posed by real-world use cases. To bridge this gap, this work proposes a DRL-based Decision Support System (DSS) for real-world scale DTAPS. To this end, we introduce a DRL agent with two novel elements: a graph structure for observations and actions that can effectively represent any DTAP and a reward function that is provably equivalent to the objective of minimizing the average cycle time of tasks. The combination of these two novelties allows the agent to learn effective and generalizable assignment policies for real-world scale DTAPs. The proposed DSS is evaluated on five DTAP instances whose parameters are extracted from real-world logs through process mining. The experimental evaluation shows how the proposed DRL agent matches or outperforms the best baseline in all DTAP instances and generalizes on different time horizons and across instances.","authors":["Riccardo Lo Bianco","Willem van Jaarsveld","Jeroen Middelhuis","Luca Begnardi","Remco Dijkman"],"url":"https://arxiv.org/abs/2504.19933"}
{"created":"2025-04-29","title":"Enhancing Quality for VVC Compressed Videos with Omniscient Quality Enhancement Model","abstract":"The latest video coding standard H.266/VVC has shown its great improvement in terms of compression performance when compared to its predecessor HEVC standard. Though VVC was implemented with many advanced techniques, it still met the same challenges as its predecessor due to the need for even higher perceptual quality demand at the decoder side as well as the compression performance at the encoder side. The advancement of Artificial Intelligence (AI) technology, notably the deep learning-based video quality enhancement methods, was shown to be a promising approach to improving the perceptual quality experience. In this paper, we propose a novel Omniscient video quality enhancement Network for VVC compressed Videos. The Omniscient Network for compressed video quality enhancement was originally designed for HEVC compressed videos in which not only the spatial-temporal features but also cross-frequencies information were employed to augment the visual quality. Inspired by this work, we propose a modification of the OVQE model and integrate it into the lasted STD-VVC (Standard Versatile Video Coding) decoder architecture. As assessed in a rich set of test conditions, the proposed OVQE-VVC solution is able to achieve significant PSNR improvement, notably around 0.74 dB and up to 1.2 dB with respect to the original STD-VVC codec. This also corresponds to around 19.6% of bitrate saving while keeping a similar quality observation.","authors":["Xiem HoangVan","Hieu Bui Minh","Sang NguyenQuang","Wen-Hsiao Peng"],"url":"https://arxiv.org/abs/2504.19935"}
{"created":"2025-04-29","title":"Mesh-Learner: Texturing Mesh with Spherical Harmonics","abstract":"In this paper, we present a 3D reconstruction and rendering framework termed Mesh-Learner that is natively compatible with traditional rasterization pipelines. It integrates mesh and spherical harmonic (SH) texture (i.e., texture filled with SH coefficients) into the learning process to learn each mesh s view-dependent radiance end-to-end. Images are rendered by interpolating surrounding SH Texels at each pixel s sampling point using a novel interpolation method. Conversely, gradients from each pixel are back-propagated to the related SH Texels in SH textures. Mesh-Learner exploits graphic features of rasterization pipeline (texture sampling, deferred rendering) to render, which makes Mesh-Learner naturally compatible with tools (e.g., Blender) and tasks (e.g., 3D reconstruction, scene rendering, reinforcement learning for robotics) that are based on rasterization pipelines. Our system can train vast, unlimited scenes because we transfer only the SH textures within the frustum to the GPU for training. At other times, the SH textures are stored in CPU RAM, which results in moderate GPU memory usage. The rendering results on interpolation and extrapolation sequences in the Replica and FAST-LIVO2 datasets achieve state-of-the-art performance compared to existing state-of-the-art methods (e.g., 3D Gaussian Splatting and M2-Mapping). To benefit the society, the code will be available at https://github.com/hku-mars/Mesh-Learner.","authors":["Yunfei Wan","Jianheng Liu","Jiarong Lin","Fu Zhang"],"url":"https://arxiv.org/abs/2504.19938"}
{"created":"2025-04-29","title":"Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking","abstract":"The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.","authors":["Luigia Costabile","Gian Marco Orlando","Valerio La Gatta","Vincenzo Moscato"],"url":"https://arxiv.org/abs/2504.19940"}
{"created":"2025-04-29","title":"Probabilistic and Causal Satisfiability: Constraining the Model","abstract":"We study the complexity of satisfiability problems in probabilistic and causal reasoning. Given random variables $X_1, X_2,\\ldots$ over finite domains, the basic terms are probabilities of propositional formulas over atomic events $X_i = x_i$, such as $P(X_1 = x_1)$ or $P(X_1 = x_1 \\vee X_2 = x_2)$. The basic terms can be combined using addition (yielding linear terms) or multiplication (polynomial terms). The probabilistic satisfiability problem asks whether a joint probability distribution satisfies a Boolean combination of (in)equalities over such terms. Fagin et al. (1990) showed that for basic and linear terms, this problem is NP-complete, making it no harder than Boolean satisfiability, while Moss\\'e et al. (2022) proved that for polynomial terms, it is complete for the existential theory of the reals.","authors":["Markus Bl\\\"aser","Julian D\\\"orfler","Maciej Li\\'skiewicz","Benito van der Zander"],"url":"https://arxiv.org/abs/2504.19944"}
{"created":"2025-04-29","title":"Tendon-Actuated Concentric Tube Endonasal Robot (TACTER)","abstract":"Endoscopic endonasal approaches (EEA) have become more prevalent for minimally invasive skull base and sinus surgeries. However, rigid scopes and tools significantly decrease the surgeon's ability to operate in tight anatomical spaces and avoid critical structures such as the internal carotid artery and cranial nerves. This paper proposes a novel tendon-actuated concentric tube endonasal robot (TACTER) design in which two tendon-actuated robots are concentric to each other, resulting in an outer and inner robot that can bend independently. The outer robot is a unidirectionally asymmetric notch (UAN) nickel-titanium robot, and the inner robot is a 3D-printed bidirectional robot, with a nickel-titanium bending member. In addition, the inner robot can translate axially within the outer robot, allowing the tool to traverse through structures while bending, thereby executing follow-the-leader motion. A Cosserat-rod based mechanical model is proposed that uses tendon tension of both tendon-actuated robots and the relative translation between the robots as inputs and predicts the TACTER tip position for varying input parameters. The model is validated with experiments, and a human cadaver experiment is presented to demonstrate maneuverability from the nostril to the sphenoid sinus. This work presents the first tendon-actuated concentric tube (TACT) dexterous robotic tool capable of performing follow-the-leader motion within natural nasal orifices to cover workspaces typically required for a successful EEA.","authors":["Kent K. Yamamoto","Tanner J. Zachem","Pejman Kheradmand","Patrick Zheng","Jihad Abdelgadir","Jared Laurance Bailey","Kaelyn Pieter","Patrick J. Codd","Yash Chitalia"],"url":"https://arxiv.org/abs/2504.19948"}
{"created":"2025-04-29","title":"Capturing Aerodynamic Characteristics of ATTAS Aircraft with Evolving Intelligent System","abstract":"Accurate modeling of aerodynamic coefficients is crucial for understanding and optimizing the performance of modern aircraft systems. This paper presents the novel deployment of an Evolving Type-2 Quantum Fuzzy Neural Network (eT2QFNN) for modeling the aerodynamic coefficients of the ATTAS aircraft to express the aerodynamic characteristics. eT2QFNN can represent the nonlinear aircraft model by creating multiple linear submodels with its rule-based structure through an incremental learning strategy rather than a traditional batch learning approach. Moreover, it enhances robustness to uncertainties and data noise through its quantum membership functions, as well as its automatic rule-learning and parameter-tuning capabilities. During the estimation of the aerodynamic coefficients via the flight data of the ATTAS, two different studies are conducted in the training phase: one with a large amount of data and the other with a limited amount of data. The results show that the modeling performance of the eT2QFNN is superior in comparison to baseline counterparts. Furthermore, eT2QFNN estimated the aerodynamic model with fewer rules compared to Type-1 fuzzy counterparts. In addition, by applying the Delta method to the proposed approach, the stability and control derivatives of the aircraft are analyzed. The results prove the superiority of the proposed eT2QFNN in representing aerodynamic coefficients.","authors":["Aydo\\u{g}an Soylu","Tufan Kumbasar"],"url":"https://arxiv.org/abs/2504.19949"}
{"created":"2025-04-29","title":"Data-Driven Stabilization of Unknown Linear-Threshold Network Dynamics","abstract":"This paper studies the data-driven control of unknown linear-threshold network dynamics to stabilize the state to a reference value. We consider two types of controllers: (i) a state feedback controller with feed-forward reference input and (ii) an augmented feedback controller with error integration. The first controller features a simpler structure and is easier to design, while the second offers improved performance in the presence of system parameter changes and disturbances. Our design strategy employs state-input datasets to construct data-based representations of the closed-loop dynamics. Since these representations involve linear threshold functions, we rewrite them as switched linear systems, and formulate the design problem as that of finding a common controller for all the resulting modes. This gives rise to a set of linear matrix inequalities (LMIs) whose solutions corresponds to the controller gain matrices. We analyze the computational complexity of solving the LMIs and propose a simplified, sufficient set of conditions that scales linearly with the system state. Simulations on two case studies involving regulation of firing rate dynamics in rodent brains and of arousal level dynamics in humans demonstrate the effectiveness of the controller designs.","authors":["Xuan Wang","Duy Duong-Tran","Jorge Cort\\'es"],"url":"https://arxiv.org/abs/2504.19950"}
{"created":"2025-04-29","title":"Securing GenAI Multi-Agent Systems Against Tool Squatting: A Zero Trust Registry-Based Approach","abstract":"The rise of generative AI (GenAI) multi-agent systems (MAS) necessitates standardized protocols enabling agents to discover and interact with external tools. However, these protocols introduce new security challenges, particularly; tool squatting; the deceptive registration or representation of tools. This paper analyzes tool squatting threats within the context of emerging interoperability standards, such as Model Context Protocol (MCP) or seamless communication between agents protocols. It introduces a comprehensive Tool Registry system designed to mitigate these risks. We propose a security-focused architecture featuring admin-controlled registration, centralized tool discovery, fine grained access policies enforced via dedicated Agent and Tool Registry services, a dynamic trust scoring mechanism based on tool versioning and known vulnerabilities, and just in time credential provisioning. Based on its design principles, the proposed registry framework aims to effectively prevent common tool squatting vectors while preserving the flexibility and power of multi-agent systems. This work addresses a critical security gap in the rapidly evolving GenAI ecosystem and provides a foundation for secure tool integration in production environments.","authors":["Vineeth Sai Narajala","Ken Huang","Idan Habler"],"url":"https://arxiv.org/abs/2504.19951"}
{"created":"2025-04-29","title":"Type-Based Unsourced Multiple Access over Fading Channels with Cell-Free Massive MIMO","abstract":"Type-based unsourced multiple access (TUMA) is a recently proposed framework for type-based estimation in massive uncoordinated access networks. We extend the existing design of TUMA, developed for an additive white Gaussian channel, to a more realistic environment with fading and multiple antennas. Specifically, we consider a cell-free massive multiple-input multiple-output system and exploit spatial diversity to estimate the set of transmitted messages and the number of users transmitting each message. Our solution relies on a location-based codeword partition and on the use at the receiver of a multisource approximate message passing algorithm in both centralized and distributed implementations. The proposed TUMA framework results in a robust and scalable architecture for massive machine-type communications.","authors":["Kaan Okumus","Khac-Hoang Ngo","Giuseppe Durisi","Erik G. Str\\\"om"],"url":"https://arxiv.org/abs/2504.19954"}
{"created":"2025-04-29","title":"Robust Federated Personalised Mean Estimation for the Gaussian Mixture Model","abstract":"Federated learning with heterogeneous data and personalization has received significant recent attention. Separately, robustness to corrupted data in the context of federated learning has also been studied. In this paper we explore combining personalization for heterogeneous data with robustness, where a constant fraction of the clients are corrupted. Motivated by this broad problem, we formulate a simple instantiation which captures some of its difficulty. We focus on the specific problem of personalized mean estimation where the data is drawn from a Gaussian mixture model. We give an algorithm whose error depends almost linearly on the ratio of corrupted to uncorrupted samples, and show a lower bound with the same behavior, albeit with a gap of a constant factor.","authors":["Malhar A. Managoli","Vinod M. Prabhakaran","Suhas Diggavi"],"url":"https://arxiv.org/abs/2504.19955"}
{"created":"2025-04-29","title":"Securing Agentic AI: A Comprehensive Threat Model and Mitigation Framework for Generative AI Agents","abstract":"As generative AI (GenAI) agents become more common in enterprise settings, they introduce security challenges that differ significantly from those posed by traditional systems. These agents are not just LLMs; they reason, remember, and act, often with minimal human oversight. This paper introduces a comprehensive threat model tailored specifically for GenAI agents, focusing on how their autonomy, persistent memory access, complex reasoning, and tool integration create novel risks. This research work identifies 9 primary threats and organizes them across five key domains: cognitive architecture vulnerabilities, temporal persistence threats, operational execution vulnerabilities, trust boundary violations, and governance circumvention. These threats are not just theoretical they bring practical challenges such as delayed exploitability, cross-system propagation, cross system lateral movement, and subtle goal misalignments that are hard to detect with existing frameworks and standard approaches. To help address this, the research work present two complementary frameworks: ATFAA - Advanced Threat Framework for Autonomous AI Agents, which organizes agent-specific risks, and SHIELD, a framework proposing practical mitigation strategies designed to reduce enterprise exposure. While this work builds on existing work in LLM and AI security, the focus is squarely on what makes agents different and why those differences matter. Ultimately, this research argues that GenAI agents require a new lens for security. If we fail to adapt our threat models and defenses to account for their unique architecture and behavior, we risk turning a powerful new tool into a serious enterprise liability.","authors":["Vineeth Sai Narajala","Om Narayan"],"url":"https://arxiv.org/abs/2504.19956"}
{"created":"2025-04-29","title":"Revisiting Directed Disjoint Paths on tournaments (and relatives)","abstract":"In the Directed Disjoint Paths problem ($k$-DDP), we are given a digraph $k$ pairs of terminals, and the goal is to find $k$ pairwise vertex-disjoint paths connecting each pair of terminals. Bang-Jensen and Thomassen [SIAM J. Discrete Math. 1992] claimed that $k$-DDP is NP-complete on tournaments, and this result triggered a very active line of research about the complexity of the problem on tournaments and natural superclasses. We identify a flaw in their proof, which has been acknowledged by the authors, and provide a new NP-completeness proof. From an algorithmic point of view, Fomin and Pilipczuk [J. Comb. Theory B 2019] provided an FPT algorithm for the edge-disjoint version of the problem on semicomplete digraphs, and showed that their technique cannot work for the vertex-disjoint version. We overcome this obstacle by showing that the version of $k$-DDP where we allow congestion $c$ on the vertices is FPT on semicomplete digraphs provided that $c$ is greater than $k/2$. This is based on a quite elaborate irrelevant vertex argument inspired by the edge-disjoint version, and we show that our choice of $c$ is best possible for this technique, with a counterexample with no irrelevant vertices when $c \\leq k/2$. We also prove that $k$-DDP on digraphs that can be partitioned into $h$ semicomplete digraphs is $W[1]$-hard parameterized by $k+h$, which shows that the XP algorithm presented by Chudnovsky, Scott, and Seymour [J. Comb. Theory B 2019] is essentially optimal.","authors":["Guilherme C. M. Gomes","Raul Lopes","Ignasi Sau"],"url":"https://arxiv.org/abs/2504.19957"}
{"created":"2025-04-29","title":"From Concept to Practice: an Automated LLM-aided UVM Machine for RTL Verification","abstract":"Verification presents a major bottleneck in Integrated Circuit (IC) development, consuming nearly 70% of the total development effort. While the Universal Verification Methodology (UVM) is widely used in industry to improve verification efficiency through structured and reusable testbenches, constructing these testbenches and generating sufficient stimuli remain challenging. These challenges arise from the considerable manual coding effort required, repetitive manual execution of multiple EDA tools, and the need for in-depth domain expertise to navigate complex designs.Here, we present UVM^2, an automated verification framework that leverages Large Language Models (LLMs) to generate UVM testbenches and iteratively refine them using coverage feedback, significantly reducing manual effort while maintaining rigorous verification standards.To evaluate UVM^2, we introduce a benchmark suite comprising Register Transfer Level (RTL) designs of up to 1.6K lines of code.The results show that UVM^2 reduces testbench setup time by up to UVM^2 compared to experienced engineers, and achieve average code and function coverage of 87.44% and 89.58%, outperforming state-of-the-art solutions by 20.96% and 23.51%, respectively.","authors":["Junhao Ye","Yuchen Hu","Ke Xu","Dingrong Pan","Qichun Chen","Jie Zhou","Shuai Zhao","Xinwei Fang","Xi Wang","Nan Guan","Zhe Jiang"],"url":"https://arxiv.org/abs/2504.19959"}
{"created":"2025-04-29","title":"Analytical solutions for the Extracellular-Membrane-Intracellular model","abstract":"The Extracellular-Membrane-Intracellular (EMI) model is a novel mathematical framework for cardiac electrophysiology simulations. The EMI model provides a more detailed description of the heart's electrical activity compared to traditional monodomain and bidomain models, potentially making it better-suited for understanding the electrical dynamics of the heart under pathological conditions. In this paper, we derive and verify several analytical solutions for the EMI model. Specifically, we obtain a family of solutions for a single two-dimensional cell in polar coordinates and for a pair of coupled three-dimensional cells in spherical coordinates. We also introduce a manufactured solution for N three-dimensional cells in Cartesian coordinates. To verify the analytical solutions, we conduct numerical experiments using the mortar finite element method combined with operator splitting. The results demonstrate that the analytical solutions are effective for verifying the accuracy of numerical simulations of the EMI model.","authors":["Carlos Ballesteros","Alexei Cheviakov","Raymond J. Spiteri"],"url":"https://arxiv.org/abs/2504.19960"}
{"created":"2025-04-29","title":"Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error","abstract":"This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We compare the proposed method with existing approaches in a low-dimensional visualization example and a parametric static problem, and demonstrate its performance in a dynamics model of a space structure.","authors":["Akash Yadav","Ruda Zhang"],"url":"https://arxiv.org/abs/2504.19963"}
{"created":"2025-04-29","title":"Feelbert: A Feedback Linearization-based Embedded Real-Time Quadrupedal Locomotion Framework","abstract":"Quadruped robots have become quite popular for their ability to adapt their locomotion to generic uneven terrains. For this reason, over time, several frameworks for quadrupedal locomotion have been proposed, but with little attention to ensuring a predictable timing behavior of the controller.","authors":["Aristide Emanuele Casucci","Federico Nesti","Mauro Marinoni","Giorgio Buttazzo"],"url":"https://arxiv.org/abs/2504.19965"}
{"created":"2025-04-29","title":"Enhancing short-term traffic prediction by integrating trends and fluctuations with attention mechanism","abstract":"Traffic flow prediction is a critical component of intelligent transportation systems, yet accurately forecasting traffic remains challenging due to the interaction between long-term trends and short-term fluctuations. Standard deep learning models often struggle with these challenges because their architectures inherently smooth over fine-grained fluctuations while focusing on general trends. This limitation arises from low-pass filtering effects, gate biases favoring stability, and memory update mechanisms that prioritize long-term information retention. To address these shortcomings, this study introduces a hybrid deep learning framework that integrates both long-term trend and short-term fluctuation information using two input features processed in parallel, designed to capture complementary aspects of traffic flow dynamics. Further, our approach leverages attention mechanisms, specifically Bahdanau attention, to selectively focus on critical time steps within traffic data, enhancing the model's ability to predict congestion and other transient phenomena. Experimental results demonstrate that features learned from both branches are complementary, significantly improving the goodness-of-fit statistics across multiple prediction horizons compared to a baseline model. Notably, the attention mechanism enhances short-term forecast accuracy by directly targeting immediate fluctuations, though challenges remain in fully integrating long-term trends. This framework can contribute to more effective congestion mitigation and urban mobility planning by advancing the robustness and precision of traffic prediction models.","authors":["Adway Das","Agnimitra Sengupta","S. Ilgin Guler"],"url":"https://arxiv.org/abs/2504.19967"}
{"created":"2025-04-29","title":"How Group Lives Go Well","abstract":"This paper explores the ontological space of group well being, proposing a framework for representing collective welfare, group functions, and long term contributions within an ontology engineering context. Traditional well being theories focus on individual states, often relying on hedonistic, desire satisfaction, or objective list models. Such approaches struggle to account for cases where individual sacrifices contribute to broader social progress, a critical challenge in modeling group flourishing. To address this, the paper refines and extends the Counterfactual Account (CT) of well being, which evaluates goodness of an event by comparing an individual's actual well being with a hypothetical counterpart in a nearby possible world. While useful, this framework is insufficient for group level ontologies, where well being depends on functional persistence, institutional roles, and historical impact rather than immediate individual outcomes. Drawing on Basic Formal Ontology (BFO), the paper introduces a model in which group flourishing is evaluated in terms of group functional, where members bear roles and exhibit persistence conditions akin to biological systems or designed artifacts. This approach enables semantic interoperability for modeling longitudinal social contributions, allowing for structured reasoning about group welfare, social institutions, and group flourishing over time.","authors":["John Beverley","Regina Hurley"],"url":"https://arxiv.org/abs/2504.19968"}
{"created":"2025-04-29","title":"Shopformer: Transformer-Based Framework for Detecting Shoplifting via Human Pose","abstract":"Shoplifting remains a costly issue for the retail sector, but traditional surveillance systems, which are mostly based on human monitoring, are still largely ineffective, with only about 2% of shoplifters being arrested. Existing AI-based approaches rely on pixel-level video analysis which raises privacy concerns, is sensitive to environmental variations, and demands significant computational resources. To address these limitations, we introduce Shopformer, a novel transformer-based model that detects shoplifting by analyzing pose sequences rather than raw video. We propose a custom tokenization strategy that converts pose sequences into compact embeddings for efficient transformer processing. To the best of our knowledge, this is the first pose-sequence-based transformer model for shoplifting detection. Evaluated on real-world pose data, our method outperforms state-of-the-art anomaly detection models, offering a privacy-preserving, and scalable solution for real-time retail surveillance. The code base for this work is available at https://github.com/TeCSAR-UNCC/Shopformer.","authors":["Narges Rashvand","Ghazal Alinezhad Noghre","Armin Danesh Pazho","Babak Rahimi Ardabili","Hamed Tabkhi"],"url":"https://arxiv.org/abs/2504.19970"}
{"created":"2025-04-29","title":"Transfer Learning Under High-Dimensional Network Convolutional Regression Model","abstract":"Transfer learning enhances model performance by utilizing knowledge from related domains, particularly when labeled data is scarce. While existing research addresses transfer learning under various distribution shifts in independent settings, handling dependencies in networked data remains challenging. To address this challenge, we propose a high-dimensional transfer learning framework based on network convolutional regression (NCR), inspired by the success of graph convolutional networks (GCNs). The NCR model incorporates random network structure by allowing each node's response to depend on its features and the aggregated features of its neighbors, capturing local dependencies effectively. Our methodology includes a two-step transfer learning algorithm that addresses domain shift between source and target networks, along with a source detection mechanism to identify informative domains. Theoretically, we analyze the lasso estimator in the context of a random graph based on the Erdos-Renyi model assumption, demonstrating that transfer learning improves convergence rates when informative sources are present. Empirical evaluations, including simulations and a real-world application using Sina Weibo data, demonstrate substantial improvements in prediction accuracy, particularly when labeled data in the target domain is limited.","authors":["Liyuan Wang","Jiachen Chen","Kathryn L. Lunetta","Danyang Huang","Huimin Cheng","Debarghya Mukherjee"],"url":"https://arxiv.org/abs/2504.19979"}
{"created":"2025-04-29","title":"Accurate and Diverse LLM Mathematical Reasoning via Automated PRM-Guided GFlowNets","abstract":"Achieving both accuracy and diverse reasoning remains challenging for Large Language Models (LLMs) in complex domains like mathematics. A key bottleneck is evaluating intermediate reasoning steps to guide generation without costly human annotations. To address this, we first introduce a novel Process Reward Model (PRM) trained automatically using Monte Carlo Tree Search coupled with a similarity-based data augmentation technique, effectively capturing step-level reasoning quality. Leveraging this PRM, we then adapt Generative Flow Networks (GFlowNets) to operate at the reasoning step level. Unlike traditional reinforcement learning focused on maximizing a single reward, GFlowNets naturally sample diverse, high-quality solutions proportional to their rewards, as measured by our PRM. Empirical evaluation shows strong improvements in both accuracy and solution diversity on challenging mathematical benchmarks (e.g., +2.59% absolute accuracy on MATH Level 5 for Llama3.2-3B), with effective generalization to unseen datasets (+9.4% absolute on SAT MATH). Our work demonstrates the potential of PRM-guided, step-level GFlowNets for developing more robust and versatile mathematical reasoning in LLMs.","authors":["Adam Younsi","Abdalgader Abubaker","Mohamed El Amine Seddik","Hakim Hacid","Salem Lahlou"],"url":"https://arxiv.org/abs/2504.19981"}
{"created":"2025-04-29","title":"TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons","abstract":"Task-oriented dialogue (TOD) systems are experiencing a revolution driven by Large Language Models (LLMs), yet the evaluation methodologies for these systems remain insufficient for their growing sophistication. While traditional automatic metrics effectively assessed earlier modular systems, they focus solely on the dialogue level and cannot detect critical intermediate errors that can arise during user-agent interactions. In this paper, we introduce TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework that unifies fine-grained turn-level analysis with holistic dialogue-level comparisons. At turn level, we evaluate each response along three TOD-specific dimensions: conversation cohesion, backend knowledge consistency, and policy compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons to provide a measure of dialogue-level quality. Through experiments on MultiWOZ 2.4 and {\\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the conversational errors that conventional metrics miss. Furthermore, TD-EVAL exhibits better alignment with human judgments than traditional and LLM-based metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for TOD system evaluation, efficiently assessing both turn and system levels with a plug-and-play framework for future research.","authors":["Emre Can Acikgoz","Carl Guo","Suvodip Dey","Akul Datta","Takyoung Kim","Gokhan Tur","Dilek Hakkani-T\\\"ur"],"url":"https://arxiv.org/abs/2504.19982"}
{"created":"2025-04-29","title":"Emergence and scaling laws in SGD learning of shallow neural networks","abstract":"We study the complexity of online stochastic gradient descent (SGD) for learning a two-layer neural network with $P$ neurons on isotropic Gaussian data: $f_*(\\boldsymbol{x}) = \\sum_{p=1}^P a_p\\cdot \\sigma(\\langle\\boldsymbol{x},\\boldsymbol{v}_p^*\\rangle)$, $\\boldsymbol{x} \\sim \\mathcal{N}(0,\\boldsymbol{I}_d)$, where the activation $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ is an even function with information exponent $k_*>2$ (defined as the lowest degree in the Hermite expansion), $\\{\\boldsymbol{v}^*_p\\}_{p\\in[P]}\\subset \\mathbb{R}^d$ are orthonormal signal directions, and the non-negative second-layer coefficients satisfy $\\sum_{p} a_p^2=1$. We focus on the challenging ``extensive-width'' regime $P\\gg 1$ and permit diverging condition number in the second-layer, covering as a special case the power-law scaling $a_p\\asymp p^{-\\beta}$ where $\\beta\\in\\mathbb{R}_{\\ge 0}$. We provide a precise analysis of SGD dynamics for the training of a student two-layer network to minimize the mean squared error (MSE) objective, and explicitly identify sharp transition times to recover each signal direction. In the power-law setting, we characterize scaling law exponents for the MSE loss with respect to the number of training samples and SGD steps, as well as the number of parameters in the student neural network. Our analysis entails that while the learning of individual teacher neurons exhibits abrupt transitions, the juxtaposition of $P\\gg 1$ emergent learning curves at different timescales leads to a smooth scaling law in the cumulative objective.","authors":["Yunwei Ren","Eshaan Nichani","Denny Wu","Jason D. Lee"],"url":"https://arxiv.org/abs/2504.19983"}
{"created":"2025-04-29","title":"3D MPSoC with On-Chip Cache Support -- Design and Exploitation","abstract":"The increasing density of transistors in Integrated Circuits (ICs) has enabled the development of highly integrated Systems-on-Chip (SoCs) and, more recently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability challenges in communication and memory performance, three-dimensional (3D) Network-on-Chip (NoC) architectures have emerged, offering improvements in communication latency and throughput. However, memory system efficiency remains a critical bottleneck in NoC-based designs. This work proposes the design and experimental exploration of 3D MPSoCs with on-chip cache support by employing distinct communication infrastructures for inter-processor and memory interactions. Specifically, packet-based NoCs are adopted for inter-processor communication, while a crossbar-based infrastructure supports a cache coherence hierarchy for memory access. A two-layer system architecture is introduced, combining a Uniform Memory Access (UMA) model within clusters and a No Remote Memory Access (NORMA) model between clusters, aiming to balance scalability and coherence requirements. Emerging memory technologies such as PCRAM and MRAM are explored to optimize performance, energy consumption, and area usage. Experimental evaluations are conducted using the Gem5 simulator, targeting a model based on the ARM Versatile Express platform. The outcomes of this study aim to enhance MPSoC scalability while meeting the stringent demands of memory-centric applications.","authors":["Rodrigo Cataldo","Cesar Marcon","Debora Matos"],"url":"https://arxiv.org/abs/2504.19984"}
{"created":"2025-04-29","title":"Real-Time Imitation of Human Head Motions, Blinks and Emotions by Nao Robot: A Closed-Loop Approach","abstract":"This paper introduces a novel approach for enabling real-time imitation of human head motion by a Nao robot, with a primary focus on elevating human-robot interactions. By using the robust capabilities of the MediaPipe as a computer vision library and the DeepFace as an emotion recognition library, this research endeavors to capture the subtleties of human head motion, including blink actions and emotional expressions, and seamlessly incorporate these indicators into the robot's responses. The result is a comprehensive framework which facilitates precise head imitation within human-robot interactions, utilizing a closed-loop approach that involves gathering real-time feedback from the robot's imitation performance. This feedback loop ensures a high degree of accuracy in modeling head motion, as evidenced by an impressive R2 score of 96.3 for pitch and 98.9 for yaw. Notably, the proposed approach holds promise in improving communication for children with autism, offering them a valuable tool for more effective interaction. In essence, proposed work explores the integration of real-time head imitation and real-time emotion recognition to enhance human-robot interactions, with potential benefits for individuals with unique communication needs.","authors":["Keyhan Rayati","Amirhossein Feizi","Alireza Beigy","Pourya Shahverdi","Mehdi Tale Masouleh","Ahmad Kalhor"],"url":"https://arxiv.org/abs/2504.19985"}
{"created":"2025-04-29","title":"HJRNO: Hamilton-Jacobi Reachability with Neural Operators","abstract":"Ensuring the safety of autonomous systems under uncertainty is a critical challenge. Hamilton-Jacobi reachability (HJR) analysis is a widely used method for guaranteeing safety under worst-case disturbances. Traditional HJR methods provide safety guarantees but suffer from the curse of dimensionality, limiting their scalability to high-dimensional systems or varying environmental conditions. In this work, we propose HJRNO, a neural operator-based framework for solving backward reachable tubes (BRTs) efficiently and accurately. By leveraging the Fourier Neural Operator (FNO), HJRNO learns a mapping between value functions, enabling fast inference with strong generalization across different obstacle shapes, system configurations, and hyperparameters. We demonstrate that HJRNO achieves low error on random obstacle scenarios and generalizes effectively across varying system dynamics. These results suggest that HJRNO offers a promising foundation model approach for scalable, real-time safety analysis in autonomous systems.","authors":["Yankai Li","Mo Chen"],"url":"https://arxiv.org/abs/2504.19989"}
{"created":"2025-04-29","title":"Mitigating Societal Cognitive Overload in the Age of AI: Challenges and Directions","abstract":"Societal cognitive overload, driven by the deluge of information and complexity in the AI age, poses a critical challenge to human well-being and societal resilience. This paper argues that mitigating cognitive overload is not only essential for improving present-day life but also a crucial prerequisite for navigating the potential risks of advanced AI, including existential threats. We examine how AI exacerbates cognitive overload through various mechanisms, including information proliferation, algorithmic manipulation, automation anxieties, deregulation, and the erosion of meaning. The paper reframes the AI safety debate to center on cognitive overload, highlighting its role as a bridge between near-term harms and long-term risks. It concludes by discussing potential institutional adaptations, research directions, and policy considerations that arise from adopting an overload-resilient perspective on human-AI alignment, suggesting pathways for future exploration rather than prescribing definitive solutions.","authors":["Salem Lahlou"],"url":"https://arxiv.org/abs/2504.19990"}
{"created":"2025-04-29","title":"Mapping of Weed Management Methods in Orchards using Sentinel-2 and PlanetScope Data","abstract":"Effective weed management is crucial for improving agricultural productivity, as weeds compete with crops for vital resources like nutrients and water. Accurate maps of weed management methods are essential for policymakers to assess farmer practices, evaluate impacts on vegetation health, biodiversity, and climate, as well as ensure compliance with policies and subsidies. However, monitoring weed management methods is challenging as commonly rely on on-ground field surveys, which are often costly, time-consuming and subject to delays. In order to tackle this problem, we leverage Earth Observation (EO) data and Machine Learning (ML). Specifically, we developed an ML approach for mapping four distinct weed management methods (Mowing, Tillage, Chemical-spraying, and No practice) in orchards using satellite image time series (SITS) data from two different sources: Sentinel-2 (S2) and PlanetScope (PS). The findings demonstrate the potential of ML-driven remote sensing to enhance the efficiency and accuracy of weed management mapping in orchards.","authors":["Ioannis Kontogiorgakis","Iason Tsardanidis","Dimitrios Bormpoudakis","Ilias Tsoumas","Dimitra A. Loka","Christos Noulas","Alexandros Tsitouras","Charalampos Kontoes"],"url":"https://arxiv.org/abs/2504.19991"}
{"created":"2025-04-29","title":"Monitoring digestate application on agricultural crops using Sentinel-2 Satellite imagery","abstract":"The widespread use of Exogenous Organic Matter in agriculture necessitates monitoring to assess its effects on soil and crop health. This study evaluates optical Sentinel-2 satellite imagery for detecting digestate application, a practice that enhances soil fertility but poses environmental risks like microplastic contamination and nitrogen losses. In the first instance, Sentinel-2 satellite image time series (SITS) analysis of specific indices (EOMI, NDVI, EVI) was used to characterize EOM's spectral behavior after application on the soils of four different crop types in Thessaly, Greece. Furthermore, Machine Learning (ML) models (namely Random Forest, k-NN, Gradient Boosting and a Feed-Forward Neural Network), were used to investigate digestate presence detection, achieving F1-scores up to 0.85. The findings highlight the potential of combining remote sensing and ML for scalable and cost-effective monitoring of EOM applications, supporting precision agriculture and sustainability.","authors":["Andreas Kalogeras","Dimitrios Bormpoudakis","Iason Tsardanidis","Dimitra A. Loka","Charalampos Kontoes"],"url":"https://arxiv.org/abs/2504.19996"}
{"created":"2025-04-29","title":"Simplified and Secure MCP Gateways for Enterprise AI Integration","abstract":"The increased adoption of the Model Context Protocol (MCP) for AI Agents necessitates robust security for Enterprise integrations. This paper introduces the MCP Gateway to simplify self-hosted MCP server integration. The proposed architecture integrates security principles, authentication, intrusion detection, and secure tunneling, enabling secure self-hosting without exposing infrastructure. Key contributions include a reference architecture, threat model mapping, simplified integration strategies, and open-source implementation recommendations. This work focuses on the unique challenges of enterprise-centric, self-hosted AI integrations, unlike existing public MCP server solutions.","authors":["Ivo Brett"],"url":"https://arxiv.org/abs/2504.19997"}
{"created":"2025-04-29","title":"Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom","abstract":"Knowledge Distillation (KD) is one of the approaches to reduce the size of Large Language Models (LLMs). A LLM with smaller number of model parameters (student) is trained to mimic the performance of a LLM of a larger size (teacher model) on a specific task. For domain-specific tasks, it is not clear if teacher or student model, or both, must be considered for domain adaptation. In this work, we study this problem from perspective of telecom domain Question-Answering (QA) task. We systematically experiment with Supervised Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to KD. We design experiments to study the impact of vocabulary (same and different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the distilled model. Multi-faceted evaluation of the distillation using 14 different metrics (N-gram, embedding and LLM-based metrics) is considered. Experimental results show that SFT of teacher improves performance of distilled model when both models have same vocabulary, irrespective of algorithm and metrics. Overall, SFT of both teacher and student results in better performance across all metrics, although the statistical significance of the same depends on the vocabulary of the teacher models.","authors":["Rishika Sen","Sujoy Roychowdhury","Sumit Soman","H. G. Ranjani","Srikhetra Mohanty"],"url":"https://arxiv.org/abs/2504.20000"}
{"created":"2025-04-29","title":"Engineering Minimal k-Perfect Hash Functions","abstract":"Given a set S of n keys, a k-perfect hash function (kPHF) is a data structure that maps the keys to the first m integers, where each output integer can be hit by at most k input keys. When m=n/k, the resulting function is called a minimal k-perfect hash function (MkPHF). Applications of kPHFs can be found in external memory data structures or to create efficient 1-perfect hash functions, which in turn have a wide range of applications from databases to bioinformatics. Several papers from the 1980s look at external memory data structures with small internal memory indexes. However, actual k-perfect hash functions are surprisingly rare, and the area has not seen a lot of research recently. At the same time, recent research in 1-perfect hashing shows that there is a lack of efficient kPHFs. In this paper, we revive the area of k-perfect hashing, presenting four new constructions. Our implementations simultaneously dominate older approaches in space consumption, construction time, and query time. We see this paper as a possible starting point of an active line of research, similar to the area of 1-perfect hashing.","authors":["Stefan Hermann","Sebastian Kirmayer","Hans-Peter Lehmann","Peter Sanders","Stefan Walzer"],"url":"https://arxiv.org/abs/2504.20001"}
{"created":"2025-04-29","title":"Deciding summability via residues in theory and in practice","abstract":"In difference algebra, summability arises as a basic problem upon which rests the effective solution of other more elaborate problems, such as creative telescoping problems and the computation of Galois groups of difference equations. In 2012 Chen and Singer introduced discrete residues as a theoretical obstruction to summability for rational functions with respect to the shift and $q$-dilation difference operators. Since then analogous notions of discrete residues have been defined in other difference settings relevant for applications, such as for Mahler and elliptic shift difference operators. Very recently there have been some advances in making these theoretical obstructions computable in practice.","authors":["Carlos E. Arreche"],"url":"https://arxiv.org/abs/2504.20003"}
{"created":"2025-04-29","title":"Socially-Aware Autonomous Driving: Inferring Yielding Intentions for Safer Interactions","abstract":"Since the emergence of autonomous driving technology, it has advanced rapidly over the past decade. It is becoming increasingly likely that autonomous vehicles (AVs) would soon coexist with human-driven vehicles (HVs) on the roads. Currently, safety and reliable decision-making remain significant challenges, particularly when AVs are navigating lane changes and interacting with surrounding HVs. Therefore, precise estimation of the intentions of surrounding HVs can assist AVs in making more reliable and safe lane change decision-making. This involves not only understanding their current behaviors but also predicting their future motions without any direct communication. However, distinguishing between the passing and yielding intentions of surrounding HVs still remains ambiguous. To address the challenge, we propose a social intention estimation algorithm rooted in Directed Acyclic Graph (DAG), coupled with a decision-making framework employing Deep Reinforcement Learning (DRL) algorithms. To evaluate the method's performance, the proposed framework can be tested and applied in a lane-changing scenario within a simulated environment. Furthermore, the experiment results demonstrate how our approach enhances the ability of AVs to navigate lane changes safely and efficiently on roads.","authors":["Jing Wang","Yan Jin","Hamid Taghavifar","Fei Ding","Chongfeng Wei"],"url":"https://arxiv.org/abs/2504.20004"}
{"created":"2025-04-29","title":"Chatbot Arena Meets Nuggets: Towards Explanations and Diagnostics in the Evaluation of LLM Responses","abstract":"Battles, or side-by-side comparisons in so called arenas that elicit human preferences, have emerged as a popular approach to assessing the output quality of LLMs. Recently, this idea has been extended to retrieval-augmented generation (RAG) systems. While undoubtedly representing an advance in evaluation, battles have at least two drawbacks, particularly in the context of complex information-seeking queries: they are neither explanatory nor diagnostic. Recently, the nugget evaluation methodology has emerged as a promising approach to evaluate the quality of RAG answers. Nuggets decompose long-form LLM-generated answers into atomic facts, highlighting important pieces of information necessary in a \"good\" response. In this work, we apply our AutoNuggetizer framework to analyze data from roughly 7K Search Arena battles provided by LMArena in a fully automatic manner. Our results show a significant correlation between nugget scores and human preferences, showcasing promise in our approach to explainable and diagnostic system evaluations.","authors":["Sahel Sharifymoghaddam","Shivani Upadhyay","Nandan Thakur","Ronak Pradeep","Jimmy Lin"],"url":"https://arxiv.org/abs/2504.20006"}
{"created":"2025-04-29","title":"Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage","abstract":"This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating video, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. We present our methodology, computational techniques, and findings, outlining a practical approach for law enforcement while advancing the frontiers of knowledge discovery from police BWC data.","authors":["Anita Srbinovska","Angela Srbinovska","Vivek Senthil","Adrian Martin","John McCluskey","Ernest Fokou\\'e"],"url":"https://arxiv.org/abs/2504.20007"}
{"created":"2025-04-29","title":"Kinodynamic Trajectory Following with STELA: Simultaneous Trajectory Estimation & Local Adaptation","abstract":"State estimation and control are often addressed separately, leading to unsafe execution due to sensing noise, execution errors, and discrepancies between the planning model and reality. Simultaneous control and trajectory estimation using probabilistic graphical models has been proposed as a unified solution to these challenges. Previous work, however, relies heavily on appropriate Gaussian priors and is limited to holonomic robots with linear time-varying models. The current research extends graphical optimization methods to vehicles with arbitrary dynamical models via Simultaneous Trajectory Estimation and Local Adaptation (STELA). The overall approach initializes feasible trajectories using a kinodynamic, sampling-based motion planner. Then, it simultaneously: (i) estimates the past trajectory based on noisy observations, and (ii) adapts the controls to be executed to minimize deviations from the planned, feasible trajectory, while avoiding collisions. The proposed factor graph representation of trajectories in STELA can be applied for any dynamical system given access to first or second-order state update equations, and introduces the duration of execution between two states in the trajectory discretization as an optimization variable. These features provide both generalization and flexibility in trajectory following. In addition to targeting computational efficiency, the proposed strategy performs incremental updates of the factor graph using the iSAM algorithm and introduces a time-window mechanism. This mechanism allows the factor graph to be dynamically updated to operate over a limited history and forward horizon of the planned trajectory. This enables online updates of controls at a minimum of 10Hz. Experiments demonstrate that STELA achieves at least comparable performance to previous frameworks on idealized vehicles with linear dynamics.[...]","authors":["Edgar Granados","Sumanth Tangirala","Kostas E. Bekris"],"url":"https://arxiv.org/abs/2504.20009"}
{"created":"2025-04-29","title":"Towards Automated Scoping of AI for Social Good Projects","abstract":"Artificial Intelligence for Social Good (AI4SG) is an emerging effort that aims to address complex societal challenges with the powerful capabilities of AI systems. These challenges range from local issues with transit networks to global wildlife preservation. However, regardless of scale, a critical bottleneck for many AI4SG initiatives is the laborious process of problem scoping -- a complex and resource-intensive task -- due to a scarcity of professionals with both technical and domain expertise. Given the remarkable applications of large language models (LLM), we propose a Problem Scoping Agent (PSA) that uses an LLM to generate comprehensive project proposals grounded in scientific literature and real-world knowledge. We demonstrate that our PSA framework generates proposals comparable to those written by experts through a blind review and AI evaluations. Finally, we document the challenges of real-world problem scoping and note several areas for future work.","authors":["Jacob Emmerson","Rayid Ghani","Zheyuan Ryan Shi"],"url":"https://arxiv.org/abs/2504.20010"}
{"created":"2025-04-29","title":"LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation","abstract":"Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.","authors":["Beizhe Hu","Qiang Sheng","Juan Cao","Yang Li","Danding Wang"],"url":"https://arxiv.org/abs/2504.20013"}
{"created":"2025-04-29","title":"Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design","abstract":"In child-centered design, directly engaging children is crucial for deeply understanding their experiences. However, current research often prioritizes adult perspectives, as interviewing children involves unique challenges such as environmental sensitivities and the need for trust-building. AI-powered virtual humans (VHs) offer a promising approach to facilitate engaging and multimodal interactions with children. This study establishes key design guidelines for LLM-powered virtual humans tailored to child interviews, standardizing multimodal elements including color schemes, voice characteristics, facial features, expressions, head movements, and gestures. Using ChatGPT-based prompt engineering, we developed three distinct Human-AI workflows (LLM-Auto, LLM-Interview, and LLM-Analyze) and conducted a user study involving 15 children aged 6 to 12. The results indicated that the LLM-Analyze workflow outperformed the others by eliciting longer responses, achieving higher user experience ratings, and promoting more effective child engagement.","authors":["Linshi Li","Hanlin Cai"],"url":"https://arxiv.org/abs/2504.20016"}
{"created":"2025-04-29","title":"MINT: Multi-Vector Search Index Tuning","abstract":"Vector search plays a crucial role in many real-world applications. In addition to single-vector search, multi-vector search becomes important for multi-modal and multi-feature scenarios today. In a multi-vector database, each row is an item, each column represents a feature of items, and each cell is a high-dimensional vector. In multi-vector databases, the choice of indexes can have a significant impact on performance. Although index tuning for relational databases has been extensively studied, index tuning for multi-vector search remains unclear and challenging. In this paper, we define multi-vector search index tuning and propose a framework to solve it. Specifically, given a multi-vector search workload, we develop algorithms to find indexes that minimize latency and meet storage and recall constraints. Compared to the baseline, our latency achieves 2.1X to 8.3X speedup.","authors":["Jiongli Zhu","Yue Wang","Bailu Ding","Philip A. Bernstein","Vivek Narasayya","Surajit Chaudhuri"],"url":"https://arxiv.org/abs/2504.20018"}
{"created":"2025-04-29","title":"Modelling of Underwater Vehicles using Physics-Informed Neural Networks with Control","abstract":"Physics-informed neural networks (PINNs) integrate physical laws with data-driven models to improve generalization and sample efficiency. This work introduces an open-source implementation of the Physics-Informed Neural Network with Control (PINC) framework, designed to model the dynamics of an underwater vehicle. Using initial states, control actions, and time inputs, PINC extends PINNs to enable physically consistent transitions beyond the training domain. Various PINC configurations are tested, including differing loss functions, gradient-weighting schemes, and hyperparameters. Validation on a simulated underwater vehicle demonstrates more accurate long-horizon predictions compared to a non-physics-informed baseline","authors":["Abdelhakim Amer","David Felsager","Yury Brodskiy","Andriy Sarabakha"],"url":"https://arxiv.org/abs/2504.20019"}
{"created":"2025-04-29","title":"Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models","abstract":"Large language models (LLMs) have dramatically advanced machine learning research including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in reasoning, factual consistency, and interpretability. In this paper, we introduce a novel learning paradigm -- Modular Machine Learning (MML) -- as an essential approach toward new-generation LLMs. MML decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning, aiming to enhance LLMs' capability of counterfactual reasoning, mitigating hallucinations, as well as promoting fairness, safety, and transparency. Specifically, the proposed MML paradigm can: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable interpretable and logic-driven decision-making process. We present a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. We critically identify key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, the integration of the MML paradigm with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications.","authors":["Xin Wang","Haoyang Li","Zeyang Zhang","Haibo Chen","Wenwu Zhu"],"url":"https://arxiv.org/abs/2504.20020"}
{"created":"2025-04-29","title":"Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages","abstract":"Multilingual Large Language Models (LLMs) have demonstrated significant effectiveness across various languages, particularly in high-resource languages such as English. However, their performance in terms of factual accuracy across other low-resource languages, especially Indic languages, remains an area of investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o, Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in English and Indic languages using the IndicQuest dataset, which contains question-answer pairs in English and 19 Indic languages. By asking the same questions in English and their respective Indic translations, we analyze whether the models are more reliable for regional context questions in Indic languages or when operating in English. Our findings reveal that LLMs often perform better in English, even for questions rooted in Indic contexts. Notably, we observe a higher tendency for hallucination in responses generated in low-resource Indic languages, highlighting challenges in the multilingual understanding capabilities of current LLMs.","authors":["Pritika Rohera","Chaitrali Ginimav","Gayatri Sawant","Raviraj Joshi"],"url":"https://arxiv.org/abs/2504.20022"}
{"created":"2025-04-29","title":"SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning","abstract":"Recent studies in 3D spatial reasoning explore data-driven approaches and achieve enhanced spatial reasoning performance with reinforcement learning (RL). However, these methods typically perform spatial reasoning in an implicit manner, and it remains underexplored whether the acquired 3D knowledge generalizes to unseen question types at any stage of the training. In this work we introduce SpatialReasoner, a novel large vision-language model (LVLM) that address 3D spatial reasoning with explicit 3D representations shared between stages -- 3D perception, computation, and reasoning. Explicit 3D representations provide a coherent interface that supports advanced 3D spatial reasoning and enable us to study the factual errors made by LVLMs. Results show that our SpatialReasoner achieve improved performance on a variety of spatial reasoning benchmarks and generalizes better when evaluating on novel 3D spatial reasoning questions. Our study bridges the 3D parsing capabilities of prior visual foundation models with the powerful reasoning abilities of large language models, opening new directions for 3D spatial reasoning.","authors":["Wufei Ma","Yu-Cheng Chou","Qihao Liu","Xingrui Wang","Celso de Melo","Jieneng Chen","Jianwen Xie","Alan Yuille"],"url":"https://arxiv.org/abs/2504.20024"}
{"created":"2025-04-29","title":"LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields","abstract":"We present Large Inverse Rendering Model (LIRM), a transformer architecture that jointly reconstructs high-quality shape, materials, and radiance fields with view-dependent effects in less than a second. Our model builds upon the recent Large Reconstruction Models (LRMs) that achieve state-of-the-art sparse-view reconstruction quality. However, existing LRMs struggle to reconstruct unseen parts accurately and cannot recover glossy appearance or generate relightable 3D contents that can be consumed by standard Graphics engines. To address these limitations, we make three key technical contributions to build a more practical multi-view 3D reconstruction framework. First, we introduce an update model that allows us to progressively add more input views to improve our reconstruction. Second, we propose a hexa-plane neural SDF representation to better recover detailed textures, geometry and material parameters. Third, we develop a novel neural directional-embedding mechanism to handle view-dependent effects. Trained on a large-scale shape and material dataset with a tailored coarse-to-fine training scheme, our model achieves compelling results. It compares favorably to optimization-based dense-view inverse rendering methods in terms of geometry and relighting accuracy, while requiring only a fraction of the inference time.","authors":["Zhengqin Li","Dilin Wang","Ka Chen","Zhaoyang Lv","Thu Nguyen-Phuoc","Milim Lee","Jia-Bin Huang","Lei Xiao","Cheng Zhang","Yufeng Zhu","Carl S. Marshall","Yufeng Ren","Richard Newcombe","Zhao Dong"],"url":"https://arxiv.org/abs/2504.20026"}
{"created":"2025-04-29","title":"All-Subsets Important Separators with Applications to Sample Sets, Balanced Separators and Vertex Sparsifiers in Directed Graphs","abstract":"Given a directed graph $G$ with $n$ vertices and $m$ edges, a parameter $k$ and two disjoint subsets $S,T \\subseteq V(G)$, we show that the number of all-subsets important separators, which is the number of $A$-$B$ important vertex separators of size at most $k$ over all $A \\subseteq S$ and $B \\subseteq T$, is at most $\\beta(|S|, |T|, k) = 4^k {|S| \\choose \\leq k} {|T| \\choose \\leq 2k}$, where ${x \\choose \\leq c} = \\sum_{i = 1}^c {x \\choose i}$, and that they can be enumerated in time $O(\\beta(|S|,|T|,k)k^2(m+n))$. This is a generalization of the folklore result stating that the number of $A$-$B$ important separators for two fixed sets $A$ and $B$ is at most $4^k$ (first implicitly shown by Chen, Liu and Lu Algorithmica '09). From this result, we obtain the following applications: We give a construction for detection sets and sample sets in directed graphs, generalizing the results of Kleinberg (Internet Mathematics' 03) and Feige and Mahdian (STOC' 06) to directed graphs. Via our new sample sets, we give the first FPT algorithm for finding balanced separators in directed graphs parameterized by $k$, the size of the separator. Our algorithm runs in time $2^{O(k)} (m + n)$. We also give a $O({\\sqrt{\\log k}})$ approximation algorithm for the same problem. Finally, we present new results on vertex sparsifiers for preserving small cuts.","authors":["Aditya Anand","Euiwoong Lee","Jason Li","Thatchaphol Saranurak"],"url":"https://arxiv.org/abs/2504.20027"}
{"created":"2025-04-29","title":"More Clear, More Flexible, More Precise: A Comprehensive Oriented Object Detection benchmark for UAV","abstract":"Applications of unmanned aerial vehicle (UAV) in logistics, agricultural automation, urban management, and emergency response are highly dependent on oriented object detection (OOD) to enhance visual perception. Although existing datasets for OOD in UAV provide valuable resources, they are often designed for specific downstream tasks.Consequently, they exhibit limited generalization performance in real flight scenarios and fail to thoroughly demonstrate algorithm effectiveness in practical environments. To bridge this critical gap, we introduce CODrone, a comprehensive oriented object detection dataset for UAVs that accurately reflects real-world conditions. It also serves as a new benchmark designed to align with downstream task requirements, ensuring greater applicability and robustness in UAV-based OOD.Based on application requirements, we identify four key limitations in current UAV OOD datasets-low image resolution, limited object categories, single-view imaging, and restricted flight altitudes-and propose corresponding improvements to enhance their applicability and robustness.Furthermore, CODrone contains a broad spectrum of annotated images collected from multiple cities under various lighting conditions, enhancing the realism of the benchmark. To rigorously evaluate CODrone as a new benchmark and gain deeper insights into the novel challenges it presents, we conduct a series of experiments based on 22 classical or SOTA methods.Our evaluation not only assesses the effectiveness of CODrone in real-world scenarios but also highlights key bottlenecks and opportunities to advance OOD in UAV applications.Overall, CODrone fills the data gap in OOD from UAV perspective and provides a benchmark with enhanced generalization capability, better aligning with practical applications and future algorithm development.","authors":["Kai Ye","Haidi Tang","Bowen Liu","Pingyang Dai","Liujuan Cao","Rongrong Ji"],"url":"https://arxiv.org/abs/2504.20032"}
{"created":"2025-04-29","title":"Mitigating Catastrophic Forgetting in the Incremental Learning of Medical Images","abstract":"This paper proposes an Incremental Learning (IL) approach to enhance the accuracy and efficiency of deep learning models in analyzing T2-weighted (T2w) MRI medical images prostate cancer detection using the PI-CAI dataset. We used multiple health centers' artificial intelligence and radiology data, focused on different tasks that looked at prostate cancer detection using MRI (PI-CAI). We utilized Knowledge Distillation (KD), as it employs generated images from past tasks to guide the training of models for subsequent tasks. The approach yielded improved performance and faster convergence of the models. To demonstrate the versatility and robustness of our approach, we evaluated it on the PI-CAI dataset, a diverse set of medical imaging modalities including OCT and PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our results indicate that KD can be a promising technique for IL in medical image analysis in which data is sourced from individual health centers and the storage of large datasets is not feasible. By using generated images from prior tasks, our method enables the model to retain and apply previously acquired knowledge without direct access to the original data.","authors":["Sara Yavari","Jacob Furst"],"url":"https://arxiv.org/abs/2504.20033"}
{"created":"2025-04-29","title":"Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality","abstract":"Off-the-shelf smartphone-based AR systems typically use a single front-facing or rear-facing camera, which restricts user interactions to a narrow field of view and small screen size, thus reducing their practicality. We present \\textit{Cam-2-Cam}, an interaction concept implemented in three smartphone-based AR applications with interactions that span both cameras. Results from our qualitative analysis conducted on 30 participants presented two major design lessons that explore the interaction space of smartphone AR while maintaining critical AR interface attributes like embodiment and immersion: (1) \\textit{Balancing Contextual Relevance and Feedback Quality} serves to outline a delicate balance between implementing familiar interactions people do in the real world and the quality of multimodal AR responses and (2) \\textit{Preventing Disorientation using Simultaneous Capture and Alternating Cameras} which details how to prevent disorientation during AR interactions using the two distinct camera techniques we implemented in the paper. Additionally, we consider observed user assumptions or natural tendencies to inform future implementations of dual-camera setups for smartphone-based AR. We envision our design lessons as an initial pioneering step toward expanding the interaction space of smartphone-based AR, potentially driving broader adoption and overcoming limitations of single-camera AR.","authors":["Brandon Woodard","Melvin He","Mose Sakashita","Jing Qian","Zainab Iftikhar","Joseph LaViola Jr"],"url":"https://arxiv.org/abs/2504.20035"}
{"created":"2025-04-29","title":"AutoJudge: Judge Decoding Without Manual Annotation","abstract":"We introduce AutoJudge, a framework that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the generated response, relaxing the guarantee so that the \"unimportant\" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft model should be corrected to preserve quality, and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B (target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more accepted tokens per verification cycle with under 1% degradation in answer accuracy compared to standard speculative decoding and over 2x with small loss in accuracy. When applied to the LiveCodeBench benchmark, our approach automatically detects other, programming-specific important tokens and shows similar speedups, demonstrating its ability to generalize across tasks.","authors":["Roman Garipov","Fedor Velikonivtsev","Ruslan Svirschevski","Vage Egiazarian","Max Ryabinin"],"url":"https://arxiv.org/abs/2504.20039"}
{"created":"2025-04-29","title":"MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion","abstract":"While Structure-from-Motion (SfM) has seen much progress over the years, state-of-the-art systems are prone to failure when facing extreme viewpoint changes in low-overlap, low-parallax or high-symmetry scenarios. Because capturing images that avoid these pitfalls is challenging, this severely limits the wider use of SfM, especially by non-expert users. We overcome these limitations by augmenting the classical SfM paradigm with monocular depth and normal priors inferred by deep neural networks. Thanks to a tight integration of monocular and multi-view constraints, our approach significantly outperforms existing ones under extreme viewpoint changes, while maintaining strong performance in standard conditions. We also show that monocular priors can help reject faulty associations due to symmetries, which is a long-standing problem for SfM. This makes our approach the first capable of reliably reconstructing challenging indoor environments from few images. Through principled uncertainty propagation, it is robust to errors in the priors, can handle priors inferred by different models with little tuning, and will thus easily benefit from future progress in monocular depth and normal estimation. Our code is publicly available at https://github.com/cvg/mpsfm.","authors":["Zador Pataki","Paul-Edouard Sarlin","Johannes L. Sch\\\"onberger","Marc Pollefeys"],"url":"https://arxiv.org/abs/2504.20040"}
{"created":"2025-04-29","title":"Learning Streaming Video Representation via Multitask Training","abstract":"Understanding continuous video streams plays a fundamental role in real-time applications including embodied AI and autonomous driving. Unlike offline video understanding, streaming video understanding requires the ability to process video streams frame by frame, preserve historical information, and make low-latency decisions.To address these challenges, our main contributions are three-fold. (i) We develop a novel streaming video backbone, termed as StreamFormer, by incorporating causal temporal attention into a pre-trained vision transformer. This enables efficient streaming video processing while maintaining image representation capability.(ii) To train StreamFormer, we propose to unify diverse spatial-temporal video understanding tasks within a multitask visual-language alignment framework. Hence, StreamFormer learns global semantics, temporal dynamics, and fine-grained spatial relationships simultaneously. (iii) We conduct extensive experiments on online action detection, online video instance segmentation, and video question answering. StreamFormer achieves competitive results while maintaining efficiency, demonstrating its potential for real-time applications.","authors":["Yibin Yan","Jilan Xu","Shangzhe Di","Yikun Liu","Yudi Shi","Qirui Chen","Zeqian Li","Yifei Huang","Weidi Xie"],"url":"https://arxiv.org/abs/2504.20041"}
{"created":"2025-04-29","title":"CompleteMe: Reference-based Human Image Completion","abstract":"Recent methods for human image completion can reconstruct plausible body shapes but often fail to preserve unique details, such as specific clothing patterns or distinctive accessories, without explicit reference images. Even state-of-the-art reference-based inpainting approaches struggle to accurately capture and integrate fine-grained details from reference images. To address this limitation, we propose CompleteMe, a novel reference-based human image completion framework. CompleteMe employs a dual U-Net architecture combined with a Region-focused Attention (RFA) Block, which explicitly guides the model's attention toward relevant regions in reference images. This approach effectively captures fine details and ensures accurate semantic correspondence, significantly improving the fidelity and consistency of completed images. Additionally, we introduce a challenging benchmark specifically designed for evaluating reference-based human image completion tasks. Extensive experiments demonstrate that our proposed method achieves superior visual quality and semantic consistency compared to existing techniques. Project page: https://liagm.github.io/CompleteMe/","authors":["Yu-Ju Tsai","Brian Price","Qing Liu","Luis Figueroa","Daniil Pakhomov","Zhihong Ding","Scott Cohen","Ming-Hsuan Yang"],"url":"https://arxiv.org/abs/2504.20042"}
{"created":"2025-04-29","title":"Similarity matrix average for aggregating multiplex networks","abstract":"We introduce a methodology based on averaging similarity matrices with the aim of integrating the layers of a multiplex network into a single monoplex network. Multiplex networks are adopted for modelling a wide variety of real-world frameworks, such as multi-type relations in social, economic and biological structures. More specifically, multiplex networks are used when relations of different nature (layers) arise between a set of elements from a given population (nodes). A possible approach for investigating multiplex networks consists in aggregating the different layers in a single network (monoplex) which is a valid representation -- in some sense -- of all the layers. In order to obtain such an aggregated network, we propose a theoretical approach -- along with its practical implementation -- which stems on the concept of similarity matrix average. This methodology is finally applied to a multiplex similarity network of statistical journals, where the three considered layers express the similarity of the journals based on co-citations, common authors and common editors, respectively.","authors":["Federica Baccini","Lucio Barabesi","Eugenio Petrovich"],"url":"https://arxiv.org/abs/2208.06431"}
{"created":"2025-04-29","title":"Intelligent Spectrum Sharing in Integrated TN-NTNs: A Hierarchical Deep Reinforcement Learning Approach","abstract":"Integrating non-terrestrial networks (NTNs) with terrestrial networks (TNs) is key to enhancing coverage, capacity, and reliability in future wireless communications. However, the multi-tier, heterogeneous architecture of these integrated TN-NTNs introduces complex challenges in spectrum sharing and interference management. Conventional optimization approaches struggle to handle the high-dimensional decision space and dynamic nature of these networks. This paper proposes a novel hierarchical deep reinforcement learning (HDRL) framework to address these challenges and enable intelligent spectrum sharing. The proposed framework leverages the inherent hierarchy of the network, with separate policies for each tier, to learn and optimize spectrum allocation decisions at different timescales and levels of abstraction. By decomposing the complex spectrum sharing problem into manageable sub-tasks and allowing for efficient coordination among the tiers, the HDRL approach offers a scalable and adaptive solution for spectrum management in future TN-NTNs. Simulation results demonstrate the superior performance of the proposed framework compared to traditional approaches, highlighting its potential to enhance spectral efficiency and network capacity in dynamic, multi-tier environments.","authors":["Muhammad Umer","Muhammad Ahmed Mohsin","Ali Arshad Nasir","Hatem Abou-Zeid","Syed ALi Hassan"],"url":"https://arxiv.org/abs/2503.06720"}
{"created":"2025-04-29","title":"Sharp decay estimates and numerical analysis for weakly coupled systems of two subdiffusion equations","abstract":"This paper investigates the initial-boundary value problem for weakly coupled systems of time-fractional subdiffusion equations with spatially and temporally varying coupling coefficients. By combining the energy method with the coercivity of fractional derivatives, we convert the original partial differential equations into a coupled ordinary differential system. Through Laplace transform and maximum principle arguments, we reveal a dichotomy in decay behavior: When the highest fractional order is less than one, solutions exhibit sublinear decay, whereas systems with the highest order equal to one demonstrate a distinct superlinear decay pattern. This phenomenon fundamentally distinguishes coupled systems from single fractional diffusion equations, where such accelerated superlinear decay never occurs. Numerical experiments employing finite difference methods and implicit discretization schemes validate the theoretical findings.","authors":["Zhiyuan Li","Yikan Liu","Kazuma Wada"],"url":"https://arxiv.org/abs/2504.18295"}
{"created":"2025-04-29","title":"Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation","abstract":"Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption.","authors":["Sungnyun Kim","Sungwoo Cho","Sangmin Bae","Kangwook Jang","Se-Young Yun"],"url":"https://arxiv.org/abs/2504.18539"}
{"created":"2025-04-29","title":"Exploring Visual Complaints through a test battery in Acquired Brain Injury Patients: A Detailed Analysis of the DiaNAH Dataset","abstract":"This study investigated visual impairment complaints in a sample of 948 Acquired Brain Injury (ABI) patients using the DiaNAH dataset, emphasizing advanced machine learning techniques for managing missing data. Patients completed a CVS questionnaire capturing eight types of visual symptoms, including blurred vision and altered contrast perception. Due to incomplete data, 181 patients were excluded, resulting in an analytical subset of 767 individuals. To address the challenge of missing data, an automated machine learning (AutoML) approach was employed for data imputation, preserving the distributional characteristics of the original dataset. Patients were grouped according to singular and combined complaint clusters derived from the 40,320 potential combinations identified through the CVS questionnaire. A linear correlation analysis revealed minimal to no direct relationship between patient-reported visual complaints and standard visual perceptual function tests. This study represents an initial systematic attempt to understand the complex relationship between subjective visual complaints and objective visual perceptual assessments in ABI patients. Given the limitations of sample size and variability, further studies with larger populations are recommended to robustly explore these complaint clusters and their implications for visual perception following brain injury.","authors":["Gon\\c{c}alo Hora de Carvalho"],"url":"https://arxiv.org/abs/2504.18540"}
{"created":"2025-04-29","title":"Parameter Tuning of the Firefly Algorithm by Three Tuning Methods: Standard Monte Carlo, Quasi-Monte Carlo and Latin Hypercube Sampling Methods","abstract":"There are many different nature-inspired algorithms in the literature, and almost all such algorithms have algorithm-dependent parameters that need to be tuned. The proper setting and parameter tuning should be carried out to maximize the performance of the algorithm under consideration. This work is the extension of the recent work on parameter tuning by Joy et al. (2024) presented at the International Conference on Computational Science (ICCS 2024), and the Firefly Algorithm (FA) is tuned using three different methods: the Monte Carlo method, the Quasi-Monte Carlo method and the Latin Hypercube Sampling. The FA with the tuned parameters is then used to solve a set of six different optimization problems, and the possible effect of parameter setting on the quality of the optimal solutions is analyzed. Rigorous statistical hypothesis tests have been carried out, including Student's t-tests, F-tests, non-parametric Friedman tests and ANOVA. Results show that the performance of the FA is not influenced by the tuning methods used. In addition, the tuned parameter values are largely independent of the tuning methods used. This indicates that the FA can be flexible and equally effective in solving optimization problems, and any of the three tuning methods can be used to tune its parameters effectively.","authors":["Geethu Joy","Christian Huyck","Xin-She Yang"],"url":"https://arxiv.org/abs/2504.18545"}
{"created":"2025-04-29","title":"Dual-Modality Computational Ophthalmic Imaging with Deep Learning and Coaxial Optical Design","abstract":"The growing burden of myopia and retinal diseases necessitates more accessible and efficient eye screening solutions. This study presents a compact, dual-function optical device that integrates fundus photography and refractive error detection into a unified platform. The system features a coaxial optical design using dichroic mirrors to separate wavelength-dependent imaging paths, enabling simultaneous alignment of fundus and refraction modules. A Dense-U-Net-based algorithm with customized loss functions is employed for accurate pupil segmentation, facilitating automated alignment and focusing. Experimental evaluations demonstrate the system's capability to achieve high-precision pupil localization (EDE = 2.8 px, mIoU = 0.931) and reliable refractive estimation with a mean absolute error below 5%. Despite limitations due to commercial lens components, the proposed framework offers a promising solution for rapid, intelligent, and scalable ophthalmic screening, particularly suitable for community health settings.","authors":["Boyuan Peng","Jiaju Chen","Yiwei Zhang","Cuiyi Peng","Junyang Li","Jiaming Deng","Peiwu Qin"],"url":"https://arxiv.org/abs/2504.18549"}
{"created":"2025-04-29","title":"Modular Debiasing for Discrete Sources and Quantum Randomness","abstract":"We propose a modular debiasing technique for discrete random sources, addressing the fundamental challenge of extracting high-quality randomness from imperfect physical processes. The method involves summing the outcomes of multiple independent trials from a biased source and reducing the sum modulo the number of possible outcomes, $m$. We provide a rigorous theoretical framework demonstrating that this simple operation guarantees the convergence of the output distribution to the ideal uniform distribution over $\\{0, 1, \\dots, m-1\\}$. A key result is the method's remarkable robustness: convergence is proven for any initial bias (provided all outcomes have non-zero probability) and, critically, is maintained even under non-stationary conditions or time-dependent noise. Analytical bounds show an exponential rate of convergence, which is empirically validated by numerical simulations. This technique's simplicity, theoretical guarantees, robustness, and data efficiency make it particularly well-suited for practical implementation in quantum settings, such as spatial photon-detection-based Quantum Random Number Generators, enabling efficient extraction of high-quality randomness from experimentally imperfect sources.","authors":["Eduardo Gueron"],"url":"https://arxiv.org/abs/2504.18585"}
{"created":"2025-04-29","title":"Explainable Deep-Learning Based Potentially Hazardous Asteroids Classification Using Graph Neural Networks","abstract":"Classifying potentially hazardous asteroids (PHAs) is crucial for planetary defense and deep space navigation, yet traditional methods often overlook the dynamical relationships among asteroids. We introduce a Graph Neural Network (GNN) approach that models asteroids as nodes with orbital and physical features, connected by edges representing their similarities, using a NASA dataset of 958,524 records. Despite an extreme class imbalance with only 0.22% of the dataset with the hazardous label, our model achieves an overall accuracy of 99% and an AUC of 0.99, with a recall of 78% and an F1-score of 37% for hazardous asteroids after applying the Synthetic Minority Oversampling Technique. Feature importance analysis highlights albedo, perihelion distance, and semi-major axis as main predictors. This framework supports planetary defense missions and confirms AI's potential in enabling autonomous navigation for future missions such as NASA's NEO Surveyor and ESA's Ramses, offering an interpretable and scalable solution for asteroid hazard assessment.","authors":["Baimam Boukar Jean Jacques"],"url":"https://arxiv.org/abs/2504.18605"}
{"created":"2025-04-29","title":"Validation and Calibration of Semi-Analytical Models for the Event Horizon Telescope Observations of Sagittarius A*","abstract":"The Event Horizon Telescope (EHT) enables the exploration of black hole accretion flows at event-horizon scales. Fitting ray-traced physical models to EHT observations requires the generation of synthetic images, a task that is computationally demanding. This study leverages \\alinet, a generative machine learning model, to efficiently produce radiatively inefficient accretion flow (RIAF) images as a function of the specified physical parameters. \\alinet has previously been shown to be able to interpolate black hole images and their associated physical parameters after training on a computationally tractable set of library images. We utilize this model to estimate the uncertainty introduced by a number of anticipated unmodeled physical effects, including interstellar scattering and intrinsic source variability. We then use this to calibrate physical parameter estimates and their associated uncertainties from RIAF model fits to mock EHT data via a library of general relativistic magnetohydrodynamics models.","authors":["Ali SaraerToosi","Avery Broderick"],"url":"https://arxiv.org/abs/2504.18624"}
{"created":"2025-04-29","title":"Statistical Inference for Clustering-based Anomaly Detection","abstract":"Unsupervised anomaly detection (AD) is a fundamental problem in machine learning and statistics. A popular approach to unsupervised AD is clustering-based detection. However, this method lacks the ability to guarantee the reliability of the detected anomalies. In this paper, we propose SI-CLAD (Statistical Inference for CLustering-based Anomaly Detection), a novel statistical framework for testing the clustering-based AD results. The key strength of SI-CLAD lies in its ability to rigorously control the probability of falsely identifying anomalies, maintaining it below a pre-specified significance level $\\alpha$ (e.g., $\\alpha = 0.05$). By analyzing the selection mechanism inherent in clustering-based AD and leveraging the Selective Inference (SI) framework, we prove that false detection control is attainable. Moreover, we introduce a strategy to boost the true detection rate, enhancing the overall performance of SI-CLAD. Extensive experiments on synthetic and real-world datasets provide strong empirical support for our theoretical findings, showcasing the superior performance of the proposed method.","authors":["Nguyen Thi Minh Phu","Duong Tan Loc","Vo Nguyen Le Duy"],"url":"https://arxiv.org/abs/2504.18633"}
{"created":"2025-04-29","title":"Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: $\\sqrt{T}$-Regret","abstract":"Understanding how to efficiently learn while adhering to safety constraints is essential for using online reinforcement learning in practical applications. However, proving rigorous regret bounds for safety-constrained reinforcement learning is difficult due to the complex interaction between safety, exploration, and exploitation. In this work, we seek to establish foundations for safety-constrained reinforcement learning by studying the canonical problem of controlling a one-dimensional linear dynamical system with unknown dynamics. We study the safety-constrained version of this problem, where the state must with high probability stay within a safe region, and we provide the first safe algorithm that achieves regret of $\\tilde{O}_T(\\sqrt{T})$. Furthermore, the regret is with respect to the baseline of truncated linear controllers, a natural baseline of non-linear controllers that are well-suited for safety-constrained linear systems. In addition to introducing this new baseline, we also prove several desirable continuity properties of the optimal controller in this baseline. In showing our main result, we prove that whenever the constraints impact the optimal controller, the non-linearity of our controller class leads to a faster rate of learning than in the unconstrained setting.","authors":["Benjamin Schiffer","Lucas Janson"],"url":"https://arxiv.org/abs/2504.18657"}
{"created":"2025-04-29","title":"Local Polynomial Lp-norm Regression","abstract":"The local least squares estimator for a regression curve cannot provide optimal results when non-Gaussian noise is present. Both theoretical and empirical evidence suggests that residuals often exhibit distributional properties different from those of a normal distribution, making it worthwhile to consider estimation based on other norms. It is suggested that $L_p$-norm estimators be used to minimize the residuals when these exhibit non-normal kurtosis. In this paper, we propose a local polynomial $L_p$-norm regression that replaces weighted least squares estimation with weighted $L_p$-norm estimation for fitting the polynomial locally. We also introduce a new method for estimating the parameter $p$ from the residuals, enhancing the adaptability of the approach. Through numerical and theoretical investigation, we demonstrate our method's superiority over local least squares in one-dimensional data and show promising outcomes for higher dimensions, specifically in 2D.","authors":["Ladan Tazik (Dept. of Computer Science","Mathematics","Physics and Statistics","University of British Columbia","Okanagan campus)","James Stafford (Dept. of Statistical Sciences","University of Toronto)","John Braun (Dept. of Computer Science","Mathematics","Physics and Statistics","University of British Columbia","Okanagan campus)"],"url":"https://arxiv.org/abs/2504.18695"}
{"created":"2025-04-29","title":"The spectral map for weighted Cauchy matrices is an involution","abstract":"Let $N$ be a natural number. We consider weighted Cauchy matrices of the form \\[ \\mathcal{C}_{a,A}=\\left\\{\\frac{\\sqrt{A_j A_k}}{a_k+a_j}\\right\\}_{j,k=1}^N, \\] where $A_1,\\dots,A_N$ are positive real numbers and $a_1,\\dots,a_N$ are distinct positive real numbers, listed in increasing order. Let $b_1,\\dots,b_N$ be the eigenvalues of $\\mathcal{C}_{a,A}$, listed in increasing order. Let $B_k$ be positive real numbers such that $\\sqrt{B_k}$ is the Euclidean norm of the orthogonal projection of the vector \\[ v_A=(\\sqrt{A_1},\\dots,\\sqrt{A_N}) \\] onto the $k$'th eigenspace of $\\mathcal{C}_{a,A}$. We prove that the spectral map $(a,A)\\mapsto (b,B)$ is an involution and discuss simple properties of this map.","authors":["Alexander Pushnitski","Sergei Treil"],"url":"https://arxiv.org/abs/2504.18707"}
{"created":"2025-04-29","title":"Optimization of Next-Day Delivery Coverage using Constraint Programming and Random Key Optimizers","abstract":"We consider the logistics network of an e-commerce retailer, specifically the so-called \"middle mile\" network, that routes inventory from supply warehouses to distribution stations to be ingested into the terminal (\"last mile\") delivery network. The speed of packages through this middle mile network is a key determinant for the ultimate delivery speed to the end user. An important target for a retailer is to maximize the fraction of user orders that can be serviced within one day, i.e., next-day delivery. As such, we formulate the maximization of expected next-day delivery coverage within the middle-mile network as an optimization problem, involving a set of temporal and capacity-based constraints on the network and requiring the use of a black-box model to evaluate the objective function. We design both exact constraint programming (CP) and heuristic random-key optimizer (RKO) approaches, the former of which uses a proxy objective function. We perform experiments on large-scale, real-world problem instances and show that both approaches have merit, in that they can match or outperform the baseline solution, a bespoke greedy solver with integrated local search, in expected next-day delivery coverage. Our experiments focus on two high-level problem definitions, starting with a base problem and then adding more complexity, and also explore the generalization of the solvers across a range of problem instance sizes. We find that a hybrid model using RKO and a bespoke local search protocol performs best on the full problem definition with respect to expected next-day delivery (increase of +50 basis points [bps] over baseline) but can take days to run, whereas the hybrid model using CP and local search is slightly less competitive (+20 bps) but takes only hours to run.","authors":["Kyle Brubaker","Kyle E. C. Booth","Martin J. A. Schuetz","Philipp Loick","Jian Shen","Arun Ramamurthy","Georgios Paschos"],"url":"https://arxiv.org/abs/2504.18749"}
{"created":"2025-04-29","title":"Reservoir-enhanced Segment Anything Model for Subsurface Diagnosis","abstract":"Urban roads and infrastructure, vital to city operations, face growing threats from subsurface anomalies like cracks and cavities. Ground Penetrating Radar (GPR) effectively visualizes underground conditions employing electromagnetic (EM) waves; however, accurate anomaly detection via GPR remains challenging due to limited labeled data, varying subsurface conditions, and indistinct target boundaries. Although visually image-like, GPR data fundamentally represent EM waves, with variations within and between waves critical for identifying anomalies. Addressing these, we propose the Reservoir-enhanced Segment Anything Model (Res-SAM), an innovative framework exploiting both visual discernibility and wave-changing properties of GPR data. Res-SAM initially identifies apparent candidate anomaly regions given minimal prompts, and further refines them by analyzing anomaly-induced changing information within and between EM waves in local GPR data, enabling precise and complete anomaly region extraction and category determination. Real-world experiments demonstrate that Res-SAM achieves high detection accuracy (>85%) and outperforms state-of-the-art. Notably, Res-SAM requires only minimal accessible non-target data, avoids intensive training, and incorporates simple human interaction to enhance reliability. Our research provides a scalable, resource-efficient solution for rapid subsurface anomaly detection across diverse environments, improving urban safety monitoring while reducing manual effort and computational cost.","authors":["Xiren Zhou","Shikang Liu","Xinyu Yan","Yizhan Fan","Xiangyu Wang","Yu Kang","Jian Cheng","Huanhuan Chen"],"url":"https://arxiv.org/abs/2504.18802"}
{"created":"2025-04-29","title":"A Dictionary of Closed-Form Kernel Mean Embeddings","abstract":"Kernel mean embeddings -- integrals of a kernel with respect to a probability distribution -- are essential in Bayesian quadrature, but also widely used in other computational tools for numerical integration or for statistical inference based on the maximum mean discrepancy. These methods often require, or are enhanced by, the availability of a closed-form expression for the kernel mean embedding. However, deriving such expressions can be challenging, limiting the applicability of kernel-based techniques when practitioners do not have access to a closed-form embedding. This paper addresses this limitation by providing a comprehensive dictionary of known kernel mean embeddings, along with practical tools for deriving new embeddings from known ones. We also provide a Python library that includes minimal implementations of the embeddings.","authors":["Fran\\c{c}ois-Xavier Briol","Alexandra Gessner","Toni Karvonen","Maren Mahsereci"],"url":"https://arxiv.org/abs/2504.18830"}
{"created":"2025-04-29","title":"Predicting Stress in Two-phase Random Materials and Super-Resolution Method for Stress Images by Embedding Physical Information","abstract":"Stress analysis is an important part of material design. For materials with complex microstructures, such as two-phase random materials (TRMs), material failure is often accompanied by stress concentration. Phase interfaces in two-phase materials are critical for stress concentration. Therefore, the prediction error of stress at phase boundaries is crucial. In practical engineering, the pixels of the obtained material microstructure images are limited, which limits the resolution of stress images generated by deep learning methods, making it difficult to observe stress concentration regions. Existing Image Super-Resolution (ISR) technologies are all based on data-driven supervised learning. However, stress images have natural physical constraints, which provide new ideas for new ISR technologies. In this study, we constructed a stress prediction framework for TRMs. First, the framework uses a proposed Multiple Compositions U-net (MC U-net) to predict stress in low-resolution material microstructures. By considering the phase interface information of the microstructure, the MC U-net effectively reduces the problem of excessive prediction errors at phase boundaries. Secondly, a Mixed Physics-Informed Neural Network (MPINN) based method for stress ISR (SRPINN) was proposed. By introducing the constraints of physical information, the new method does not require paired stress images for training and can increase the resolution of stress images to any multiple. This enables a multiscale analysis of the stress concentration regions at phase boundaries. Finally, we performed stress analysis on TRMs with different phase volume fractions and loading states through transfer learning. The results show the proposed stress prediction framework has satisfactory accuracy and generalization ability.","authors":["Tengfei Xing","Xiaodan Ren","Jie Li"],"url":"https://arxiv.org/abs/2504.18854"}
{"created":"2025-04-29","title":"ReLU integral probability metric and its applications","abstract":"We propose a parametric integral probability metric (IPM) to measure the discrepancy between two probability measures. The proposed IPM leverages a specific parametric family of discriminators, such as single-node neural networks with ReLU activation, to effectively distinguish between distributions, making it applicable in high-dimensional settings. By optimizing over the parameters of the chosen discriminator class, the proposed IPM demonstrates that its estimators have good convergence rates and can serve as a surrogate for other IPMs that use smooth nonparametric discriminator classes. We present an efficient algorithm for practical computation, offering a simple implementation and requiring fewer hyperparameters. Furthermore, we explore its applications in various tasks, such as covariate balancing for causal inference and fair representation learning. Across such diverse applications, we demonstrate that the proposed IPM provides strong theoretical guarantees, and empirical experiments show that it achieves comparable or even superior performance to other methods.","authors":["Yuha Park","Kunwoong Kim","Insung Kong","Yongdai Kim"],"url":"https://arxiv.org/abs/2504.18897"}
{"created":"2025-04-29","title":"A Langevin sampling algorithm inspired by the Adam optimizer","abstract":"We present a framework for adaptive-stepsize MCMC sampling based on time-rescaled Langevin dynamics, in which the stepsize variation is dynamically driven by an additional degree of freedom. Our approach augments the phase space by an additional variable which in turn defines a time reparameterization. The use of an auxiliary relaxation equation allows accumulation of a moving average of a local monitor function and provides for precise control of the timestep while circumventing the need to modify the drift term in the physical system. Our algorithm is straightforward to implement and can be readily combined with any off-the-peg fixed-stepsize Langevin integrator. As a particular example, we consider control of the stepsize by monitoring the norm of the log-posterior gradient, which takes inspiration from the Adam optimizer, the stepsize being automatically reduced in regions of steep change of the log posterior and increased on plateaus, improving numerical stability and convergence speed. As in Adam, the stepsize variation depends on the recent history of the gradient norm, which enhances stability and improves accuracy compared to more immediate control approaches. We demonstrate the potential benefit of this method--both in accuracy and in stability--in numerical experiments including Neal's funnel and a Bayesian neural network for classification of MNIST data.","authors":["Benedict Leimkuhler","Ren\\'e Lohmann","Peter Whalley"],"url":"https://arxiv.org/abs/2504.18911"}
{"created":"2025-04-29","title":"Glider Path Design and Control for Reconstructing Three-Dimensional Structures of Oceanic Mesoscale Eddies","abstract":"Underwater gliders offer effective means in oceanic surveys with a major task in reconstructing the three-dimensional hydrographic field of a mesoscale eddy. This paper considers three key issues in the hydrographic reconstruction of mesoscale eddies with the sampled data from the underwater gliders. It first proposes using the Thin Plate Spline (TPS) as the interpolation method for the reconstruction with a blocking scheme to speed up the computation. It then formulates a procedure for selecting glider path design that minimizes the reconstruction errors among a set of pathway formations. Finally we provide a glider path control procedure to guide the glider to follow to designed pathways as much as possible in the presence of ocean current. A set of optimization algorithms are experimented and several with robust glider control performance on a simulated eddy are identified.","authors":["Wu Su","Xiaoyuan E","Zhao Jing","Song Xi Chen"],"url":"https://arxiv.org/abs/2504.18936"}
{"created":"2025-04-29","title":"Asynchronous Push-sum Dual Gradient Algorithm in Distributed Model Predictive Control","abstract":"This paper studies the distributed model predictive control (DMPC) problem for distributed discrete-time linear systems with both local and global constraints over directed communication networks. We establish an optimization problem to formulate the DMPC policy, including the design of terminal ingredients. To cope with the global constraint, we transform the primal optimization problem into its dual problem. Then, we propose a novel asynchronous push-sum dual gradient (APDG) algorithm with an adaptive step-size scheme to solve this dual problem in a fully asynchronous distributed manner. The proposed algorithm does not require synchronous waiting and any form of coordination, which greatly improves solving efficiency. We theoretically prove that the APDG algorithm converges at an R-linear rate as long as the step-size does not exceed the designed upper bound. Furthermore, we develop a distributed termination criterion to terminate the APDG algorithm when its output solution satisfies the specified suboptimality and the global constraint, thereby avoiding an infinite number of iterations. The recursive feasibility and the stability of the closed-loop system are also established. Finally, a numerical example clarifies and validates theoretical findings.","authors":["Pengbiao Wang","Xuemei Ren","Dongdong Zheng"],"url":"https://arxiv.org/abs/2504.18941"}
{"created":"2025-04-29","title":"From the Notebooks to the Investigations and Beyond","abstract":"The use of the open and searchable Wittgenstein's Nachlass (The Wittgenstein Archives at the University of Bergen (WAB)) has proved instrumental in the quest for a common thread of Wittgenstein's view on the connections between meaning, use and consequences, going from the Notebooks to later writings (including the Philosophical Investigations) and beyond. Here we take this as the basis for a proposal for a formal counterpart of a 'meaning-as-use' (dialogical/game-theoretical) semantics for the language of predicate logic. In order to further consolidate this perspective, we shall need to bring in key excerpts from Wittgenstein oeuvre (including the Nachlass) and from those formal semanticists who advocate a different perspective on the connections between proofs and meaning. With this in mind we consider several passages from Wittgenstein's published as well as unpublished writings to build a whole picture of a formal counterpart to 'meaning is use' based on the idea that explanations of consequences via 'movements within language' ought to be taken as a central aspect to Wittgenstein's shift from 'interpretation of symbols in a state of affairs' to 'use of symbols' which underpins his 'meaning is use' paradigm. As in the Investigations \"every interpretation hangs in the air together with what it interprets, and cannot give it any support. Interpretations by themselves do not determine meaning\", as well as in a remark from his transitional period (1929-30): \"Perhaps one should say that the expression 'interpretation of symbols' is misleading and one should instead say 'the use of symbols'.\" Significantly, we wish the present examination of the searchable Nachlass can make a relevant step towards a formal counterpart to the 'meaning is use' dictum, while highlighting an important common thread from Wittgenstein's very early to very late writings.","authors":["Ruy J. G. B. de Queiroz"],"url":"https://arxiv.org/abs/2504.18949"}
{"created":"2025-04-29","title":"Surgeons vs. Computer Vision: A comparative analysis on surgical phase recognition capabilities","abstract":"Purpose: Automated Surgical Phase Recognition (SPR) uses Artificial Intelligence (AI) to segment the surgical workflow into its key events, functioning as a building block for efficient video review, surgical education as well as skill assessment. Previous research has focused on short and linear surgical procedures and has not explored if temporal context influences experts' ability to better classify surgical phases. This research addresses these gaps, focusing on Robot-Assisted Partial Nephrectomy (RAPN) as a highly non-linear procedure. Methods: Urologists of varying expertise were grouped and tasked to indicate the surgical phase for RAPN on both single frames and video snippets using a custom-made web platform. Participants reported their confidence levels and the visual landmarks used in their decision-making. AI architectures without and with temporal context as trained and benchmarked on the Cholec80 dataset were subsequently trained on this RAPN dataset. Results: Video snippets and presence of specific visual landmarks improved phase classification accuracy across all groups. Surgeons displayed high confidence in their classifications and outperformed novices, who struggled discriminating phases. The performance of the AI models is comparable to the surgeons in the survey, with improvements when temporal context was incorporated in both cases. Conclusion: SPR is an inherently complex task for expert surgeons and computer vision, where both perform equally well when given the same context. Performance increases when temporal information is provided. Surgical tools and organs form the key landmarks for human interpretation and are expected to shape the future of automated SPR.","authors":["Marco Mezzina","Pieter De Backer","Tom Vercauteren","Matthew Blaschko","Alexandre Mottrie","Tinne Tuytelaars"],"url":"https://arxiv.org/abs/2504.18954"}
{"created":"2025-04-29","title":"A Preliminary Investigation on the Usage of Quantum Approximate Optimization Algorithms for Test Case Selection","abstract":"Regression testing is key in verifying that software works correctly after changes. However, running the entire regression test suite can be impractical and expensive, especially for large-scale systems. Test suite optimization methods are highly effective but often become infeasible due to their high computational demands. In previous work, Trovato et al. proposed SelectQA, an approach based on quantum annealing that outperforms the traditional state-of-the-art methods, i.e., Additional Greedy and DIV-GA, in efficiency.","authors":["Antonio Trovato","Martin Reseda","Dario Di Nucci"],"url":"https://arxiv.org/abs/2504.18955"}
{"created":"2025-04-29","title":"Modeling Regime Structure and Informational Drivers of Stock Market Volatility via the Financial Chaos Index","abstract":"This paper investigates the structural dynamics of stock market volatility through the Financial Chaos Index, a tensor- and eigenvalue-based measure designed to capture realized volatility via mutual fluctuations among asset prices. Motivated by empirical evidence of regime-dependent volatility behavior and perceptual time dilation during financial crises, we develop a regime-switching framework based on the Modified Lognormal Power-Law distribution. Analysis of the FCIX from January 1990 to December 2023 identifies three distinct market regimes, low-chaos, intermediate-chaos, and high-chaos, each characterized by differing levels of systemic stress, statistical dispersion and persistence characteristics. Building upon the segmented regime structure, we further examine the informational forces that shape forward-looking market expectations. Using sentiment-based predictors derived from the Equity Market Volatility tracker, we employ an elastic net regression model to forecast implied volatility, as proxied by the VIX index. Our findings indicate that shifts in macroeconomic, financial, policy, and geopolitical uncertainty exhibit strong predictive power for volatility dynamics across regimes. Together, these results offer a unified empirical perspective on how systemic uncertainty governs both the realized evolution of financial markets and the anticipatory behavior embedded in implied volatility measures.","authors":["Masoud Ataei"],"url":"https://arxiv.org/abs/2504.18958"}
{"created":"2025-04-29","title":"Spreading of highly cohesive metal powders with transverse oscillation kinematics","abstract":"Powder bed additive manufacturing processes such as laser powder bed fusion (LPBF) or binder jetting (BJ) benefit from using fine (D50 $\\leq20~\\mu m$) powders. However, the increasing level of cohesion with decreasing particle size makes spreading a uniform and continuous layer challenging. As a result, LPBF typically employs a coarser size distribution, and rotating roller mechanisms are used in BJ machines, that can create wave-like surface profiles due to roller run-out.","authors":["Reimar Weissbach","Garrett Adams","Patrick M. Praegla","Christoph Meier","A. John Hart"],"url":"https://arxiv.org/abs/2504.18981"}
{"created":"2025-04-29","title":"Learning Stochastic Thermodynamics Directly from Correlation and Trajectory-Fluctuation Currents","abstract":"Markedly increased computational power and data acquisition have led to growing interest in data-driven inverse dynamics problems. These seek to answer a fundamental question: What can we learn from time series measurements of a complex dynamical system? For small systems interacting with external environments, the effective dynamics are inherently stochastic, making it crucial to properly manage noise in data. Here, we explore this for systems obeying Langevin dynamics and, using currents, we construct a learning framework for stochastic modeling. Currents have recently gained increased attention for their role in bounding entropy production (EP) from thermodynamic uncertainty relations (TURs). We introduce a fundamental relationship between the cumulant currents there and standard machine-learning loss functions. Using this, we derive loss functions for several key thermodynamic functions directly from the system dynamics without the (common) intermediate step of deriving a TUR. These loss functions reproduce results derived both from TURs and other methods. More significantly, they open a path to discover new loss functions for previously inaccessible quantities. Notably, this includes access to per-trajectory entropy production, even if the observed system is driven far from its steady-state. We also consider higher order estimation. Our method is straightforward and unifies dynamic inference with recent approaches to entropy production estimation. Taken altogether, this reveals a deep connection between diffusion models in machine learning and entropy production estimation in stochastic thermodynamics.","authors":["Jinghao Lyu","Kyle J. Ray","James P. Crutchfield"],"url":"https://arxiv.org/abs/2504.19007"}
{"created":"2025-04-29","title":"Geometry-aware Active Learning of Spatiotemporal Dynamic Systems","abstract":"Rapid developments in advanced sensing and imaging have significantly enhanced information visibility, opening opportunities for predictive modeling of complex dynamic systems. However, sensing signals acquired from such complex systems are often distributed across 3D geometries and rapidly evolving over time, posing significant challenges in spatiotemporal predictive modeling. This paper proposes a geometry-aware active learning framework for modeling spatiotemporal dynamic systems. Specifically, we propose a geometry-aware spatiotemporal Gaussian Process (G-ST-GP) to effectively integrate the temporal correlations and geometric manifold features for reliable prediction of high-dimensional dynamic behaviors. In addition, we develop an adaptive active learning strategy to strategically identify informative spatial locations for data collection and further maximize the prediction accuracy. This strategy achieves the adaptive trade-off between the prediction uncertainty in the G-ST-GP model and the space-filling design guided by the geodesic distance across the 3D geometry. We implement the proposed framework to model the spatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments show that our framework outperforms traditional methods lacking the mechanism of geometric information incorporation or effective data collection.","authors":["Xizhuo (Cici)","Zhang","Bing Yao"],"url":"https://arxiv.org/abs/2504.19012"}
{"created":"2025-04-29","title":"Generative Models for Fast Simulation of Cherenkov Detectors at the Electron-Ion Collider","abstract":"The integration of Deep Learning (DL) into experimental nuclear and particle physics has driven significant progress in simulation and reconstruction workflows. However, traditional simulation frameworks such as Geant4 remain computationally intensive, especially for Cherenkov detectors, where simulating optical photon transport through complex geometries and reflective surfaces introduces a major bottleneck. To address this, we present an open, standalone fast simulation tool for Detection of Internally Reflected Cherenkov Light (DIRC) detectors, with a focus on the High-Performance DIRC (hpDIRC) at the future Electron-Ion Collider (EIC). Our framework incorporates a suite of generative models tailored to accelerate particle identification (PID) tasks by offering a scalable, GPU-accelerated alternative to full Geant4-based simulations. Designed with accessibility in mind, our simulation package enables both DL researchers and physicists to efficiently generate high-fidelity large-scale datasets on demand, without relying on complex traditional simulation stacks. This flexibility supports the development and benchmarking of novel DL-driven PID methods. Moreover, this fast simulation pipeline represents a critical step toward enabling EIC-wide PID strategies that depend on virtually unlimited simulated samples, spanning the full acceptance of the hpDIRC.","authors":["James Giroux","Michael Martinez","Cristiano Fanelli"],"url":"https://arxiv.org/abs/2504.19042"}
{"created":"2025-04-29","title":"Enhancing Cochlear Implant Signal Coding with Scaled Dot-Product Attention","abstract":"Cochlear implants (CIs) play a vital role in restoring hearing for individuals with severe to profound sensorineural hearing loss by directly stimulating the auditory nerve with electrical signals. While traditional coding strategies, such as the advanced combination encoder (ACE), have proven effective, they are constrained by their adaptability and precision. This paper investigates the use of deep learning (DL) techniques to generate electrodograms for CIs, presenting our model as an advanced alternative. We compared the performance of our model with the ACE strategy by evaluating the intelligibility of reconstructed audio signals using the short-time objective intelligibility (STOI) metric. The results indicate that our model achieves a STOI score of 0.6031, closely approximating the 0.6126 score of the ACE strategy, and offers potential advantages in flexibility and adaptability. This study underscores the benefits of incorporating artificial intelligent (AI) into CI technology, such as enhanced personalization and efficiency.","authors":["Billel Essaid","Hamza Kheddar","Noureddine Batel"],"url":"https://arxiv.org/abs/2504.19046"}
{"created":"2025-04-29","title":"AI Recommendations and Non-instrumental Image Concerns","abstract":"There is growing enthusiasm about the potential for humans and AI to collaborate by leveraging their respective strengths. Yet in practice, this promise often falls short. This paper uses an online experiment to identify non-instrumental image concerns as a key reason individuals underutilize AI recommendations. I show that concerns about how one is perceived, even when those perceptions carry no monetary consequences, lead participants to disregard AI advice and reduce task performance.","authors":["David Almog"],"url":"https://arxiv.org/abs/2504.19047"}
{"created":"2025-04-29","title":"QFGN: A Quantum Approach to High-Fidelity Implicit Neural Representations","abstract":"Implicit neural representations have shown potential in various applications. However, accurately reconstructing the image or providing clear details via image super-resolution remains challenging. This paper introduces Quantum Fourier Gaussian Network (QFGN), a quantum-based machine learning model for better signal representations. The frequency spectrum is well balanced by penalizing the low-frequency components, leading to the improved expressivity of quantum circuits. The results demonstrate that with minimal parameters, QFGN outperforms the current state-of-the-art (SOTA) models. Despite noise on hardware, the model achieves accuracy comparable to that of SIREN, highlighting the potential applications of quantum machine learning in this field.","authors":["Hongni Jin","Gurinder Singh","Kenneth M. Merz Jr"],"url":"https://arxiv.org/abs/2504.19053"}
{"created":"2025-04-29","title":"Versatile Framework for Song Generation with Prompt-based Control","abstract":"Song generation focuses on producing controllable high-quality songs based on various prompts. However, existing methods struggle to generate vocals and accompaniments with prompt-based control and proper alignment. Additionally, they fall short in supporting various tasks. To address these challenges, we introduce VersBand, a multi-task song generation framework for synthesizing high-quality, aligned songs with prompt-based control. VersBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for generating singing styles, pitches, and mel-spectrograms, allowing fast, high-quality vocal generation with style control. 2) AccompBand, a flow-based transformer model, incorporates the Band-MOE, selecting suitable experts for enhanced quality, alignment, and control. This model allows for generating controllable, high-quality accompaniments aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple prompts. Experimental results demonstrate that VersBand performs better over baseline models across multiple song generation tasks using objective and subjective metrics. Audio samples are available at https://VersBand.github.io.","authors":["Yu Zhang","Wenxiang Guo","Changhao Pan","Zhiyuan Zhu","Ruiqi Li","Jingyu Lu","Rongjie Huang","Ruiyuan Zhang","Zhiqing Hong","Ziyue Jiang","Zhou Zhao"],"url":"https://arxiv.org/abs/2504.19062"}
{"created":"2025-04-29","title":"Inverse-Transpilation: Reverse-Engineering Quantum Compiler Optimization Passes from Circuit Snapshots","abstract":"Circuit compilation, a crucial process for adapting quantum algorithms to hardware constraints, often operates as a ``black box,'' with limited visibility into the optimization techniques used by proprietary systems or advanced open-source frameworks. Due to fundamental differences in qubit technologies, efficient compiler design is an expensive process, further exposing these systems to various security threats. In this work, we take a first step toward evaluating one such challenge affecting compiler confidentiality, specifically, reverse-engineering compilation methodologies. We propose a simple ML-based framework to infer underlying optimization techniques by leveraging structural differences observed between original and compiled circuits. The motivation is twofold: (1) enhancing transparency in circuit optimization for improved cross-platform debugging and performance tuning, and (2) identifying potential intellectual property (IP)-protected optimizations employed by commercial systems. Our extensive evaluation across thousands of quantum circuits shows that a neural network performs the best in detecting optimization passes, with individual pass F1-scores reaching as high as 0.96. Thus, our initial study demonstrates the viability of this threat to compiler confidentiality and underscores the need for active research in this area.","authors":["Satwik Kundu","Swaroop Ghosh"],"url":"https://arxiv.org/abs/2504.19113"}
{"created":"2025-04-29","title":"Snake locomotion learning search","abstract":"This research introduces a novel heuristic algorithm known as the Snake Locomotion Learning Search algorithm (SLLS) designed to address optimization problems. The SLLS draws inspiration from the locomotion patterns observed in snakes, particularly serpentine and caterpillar locomotion. We leverage these two modes of snake locomotion to devise two distinct search mechanisms within the SLLS. In our quest to mimic a snake's natural adaptation to its surroundings, we incorporate a learning efficiency component generated from the Sigmoid function. This helps strike a balance between exploration and exploitation capabilities throughout the SLLS computation process. The efficacy and effectiveness of this innovative algorithm are demonstrated through its application to 60 standard benchmark optimization problems and seven well-known engineering optimization problems. The performance analysis reveals that in most cases, the SLLS outperforms other algorithms, and even in the remaining scenarios, it exhibits robust performance. This conforms to the No Free Lunch Theorem, affirming that the SLLS stands as a valuable heuristic algorithm with significant potential for effectively addressing specific optimization challenges.","authors":["Sheng-Xue He"],"url":"https://arxiv.org/abs/2504.19114"}
{"created":"2025-04-29","title":"Quasi-Monte Carlo confidence intervals using quantiles of randomized nets","abstract":"Recent advances in quasi-Monte Carlo integration have demonstrated that the median trick significantly enhances the convergence rate of linearly scrambled digital net estimators. In this work, we leverage the quantiles of such estimators to construct confidence intervals with asymptotically valid coverage for high-dimensional integrals. By analyzing the distribution of the integration error for a class of infinitely differentiable integrands, we prove that as the sample size grows, the error decomposes into an asymptotically symmetric component and a vanishing perturbation, which guarantees that a quantile-based interval for the median estimator asymptotically captures the target integral with the nominal coverage probability.","authors":["Zexin Pan"],"url":"https://arxiv.org/abs/2504.19138"}
{"created":"2025-04-29","title":"Global Climate Model Bias Correction Using Deep Learning","abstract":"Climate change affects ocean temperature, salinity and sea level, impacting monsoons and ocean productivity. Future projections by Global Climate Models based on shared socioeconomic pathways from the Coupled Model Intercomparison Project (CMIP) are widely used to understand the effects of climate change. However, CMIP models have significant bias compared to reanalysis in the Bay of Bengal for the time period when both projections and reanalysis are available. For example, there is a 1.5C root mean square error (RMSE) in the sea surface temperature (SST) projections of the climate model CNRM-CM6 compared to the Ocean Reanalysis System (ORAS5). We develop a suite of data-driven deep learning models for bias correction of climate model projections and apply it to correct SST projections of the Bay of Bengal. We propose the use of three different deep neural network architectures: convolutional encoder-decoder UNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression model and the Equi-Distant Cumulative Density Function (EDCDF) bias correction method for comparison and evaluating the impact of the new deep learning models. All bias correction models are trained using pairs of monthly CMIP6 projections and the corresponding month's ORAS5 as input and output. Historical data (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used for training and validation, including hyperparameter tuning. Testing is performed on future projection data from 2021 to 2024. Detailed analysis of the three deep neural models has been completed. We found that the UNet architecture trained using a climatology-removed CNRM-CM6 projection as input and climatology-removed ORAS5 as output gives the best bias-corrected projections. Our novel deep learning-based method for correcting CNRM-CM6 data has a 15% reduction in RMSE compared EDCDF.","authors":["Abhishek Pasula","Deepak N. Subramani"],"url":"https://arxiv.org/abs/2504.19145"}
{"created":"2025-04-29","title":"A Kinematic and Kinetic Dataset of Lower Limb Joints During Obstacle Crossing in Healthy Adults","abstract":"Crossing obstacles is a critical component of daily walking, especially for individuals with lower limb amputations, where the challenge and fall risk are heightened. While previous studies have examined obstacle crossing, they lack a systematic analysis of kinematic and kinetic changes in lower limb joints across the entire gait cycle when crossing obstacles of varying heights. This study develops a dataset of healthy individuals crossing obstacles and systematically analyzes the kinematics and kinetics of lower limb joints at different obstacle heights. Ten healthy adults participated in the experiment, crossing obstacles of varying heights (7.5 cm, 15 cm, 22.5 cm, and 30 cm), with biomechanical data including joint angles and torques of the hip, knee, and ankle recorded. As obstacle height increased, the proportion of the swing phase in the gait cycle significantly increased; the hip joint angle increased by approximately 1.5 times, and the knee joint angle increased by about 1.0 times. Both joints also exhibited significant increases in torque. In contrast, minimal changes were observed at the ankle joint, with torque remaining stable. Additionally, noticeable differences in the kinematics and kinetics between the dominant and non-dominant foot were observed, highlighting functional asymmetry. The dominant foot exhibited greater joint angles in the hip and knee joints, and less variation in the ankle joint compared to the non-dominant foot, demonstrating more coordinated movement. This detailed analysis of gait adjustments during obstacle crossing provides valuable insights into biomechanical changes in lower limb joints.","authors":["Jingwen Huang","Shucong Yin","Hanyang Xu","Zhaokai Chen","Chenglong Fu"],"url":"https://arxiv.org/abs/2504.19149"}
{"created":"2025-04-29","title":"Machine Learning-Based Modeling of the Anode Heel Effect in X-ray Beam Monte Carlo Simulations","abstract":"This study enhances Monte Carlo simulation accuracy in X-ray imaging by developing an AI-driven model for the anode heel effect, achieving improved beam intensity distribution and dosimetric precision. Through dynamic adjustments to beam weights on the anode and cathode sides of the X-ray tube, our machine learning model effectively replicates the asymmetry characteristic of clinical X-ray beams. Experimental results reveal dose rate increases of up to 9.6% on the cathode side and reductions of up to 12.5% on the anode side, for energy levels between 50 and 120 kVp. These experimentally optimized beam weights were integrated into the OpenGATE and GGEMS Monte Carlo toolkits, significantly advancing dosimetric simulation accuracy and the image quality which closely resembles the clinical imaging. Validation with fluence and dose actors demonstrated that the AI-based model closely mirrors clinical beam behavior, providing substantial improvements in dose consistency and accuracy over conventional X-ray models. This approach provides a robust framework for improving X-ray dosimetry, with potential applications in dose optimization, imaging quality enhancement, and radiation safety in both clinical and research settings.","authors":["Hussein Harb","Didier Benoit","Axel Rannou","Chi-Hieu Pham","Valentin Tissot","Bahaa Nasr","Julien Bert"],"url":"https://arxiv.org/abs/2504.19155"}
{"created":"2025-04-29","title":"Characterization of Split Comparability Graphs","abstract":"A split graph is a graph whose vertex set can be partitioned into a clique and an independent set. A split comparability graph is a split graph which is transitively orientable. In this work, we characterize split comparability graphs in terms of vertex labelling. Further, using this characterization, we prove that the permutation-representation number of a split comparability graph is at most three. This gives us an alternative proof of the result in order theory that the dimension of a split order is at most three.","authors":["Tithi Dwary","Khyodeno Mozhui","K. V. Krishna"],"url":"https://arxiv.org/abs/2504.19167"}
{"created":"2025-04-29","title":"Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography","abstract":"In situ synchrotron X-ray computed tomography enables dynamic material studies, but automated segmentation remains challenging due to complex imaging artefacts and limited training data. We present a methodology for deep learning-based segmentation by transforming high-quality ex situ laboratory data to train models for binary segmentation of in situ synchrotron data, demonstrated through copper oxide dissolution studies. Using a modified SegFormer architecture, our approach achieves high segmentation performance on unseen data while reducing processing time from hours to seconds per 3D dataset. The method maintains consistent performance over significant morphological changes during experiments, despite training only on static specimens. This methodology can be readily applied to diverse materials systems, accelerating the analysis of time-resolved tomographic data across scientific disciplines.","authors":["Tristan Manchester","Adam Anders","Julio Spadotto","Hannah Eccleston","William Beavan","Hugues Arcis","Brian J. Connolly"],"url":"https://arxiv.org/abs/2504.19200"}
{"created":"2025-04-29","title":"Expanding vertices to triangles in cubic graphs","abstract":"Contraction of triangles is a standard operation in the study of cubic graphs, as it reduces the order of the graph while typically preserving many of its properties. In this paper, we investigate the converse problem, wherein certain vertices of cubic graphs are expanded into triangles to achieve a desired property. We first focus on bridgeless cubic graphs and define the parameter $T(G)$ as the minimum number of vertices that need to be expanded into triangles so that the resulting cubic graph can be covered with four perfect matchings. We relate this parameter to the concept of shortest cycle cover. Furthermore, we show that if $5$-Cycle Double Cover Conejcture holds true, then $T(G)\\leq \\frac{2}{5} |V(G)|$. We conjecture a tighter bound, $T(G)\\leq \\frac{1}{10}|V(G)|$, which is optimal for the Petersen graph, and show that this bound follows from major conjectures like the Petersen Coloring Conjecture. In the second part of the paper, we introduce the parameter $t(G)$ as the minimum number of vertex expansions needed for the graph to admit a perfect matching. We prove a Gallai type identity: $t(G)+\\ell(G)=|V(G)|$, where $\\ell(G)$ is the number of edges in a largest even subgraph of $G$. Then we prove the general upper bound $t(G)< \\frac{1}{4}|V(G)|$ for cubic graphs, and $t(G)< \\frac{1}{6}|V(G)|$ for cubic graphs without parallel edges. We provide examples showing that these bounds are asymptotically tight. The paper concludes with a discussion of the computational complexity of determining these parameters.","authors":["Giuseppe Mazzuoccolo","Vahan Mkrtchyan"],"url":"https://arxiv.org/abs/2504.19201"}
{"created":"2025-04-29","title":"Improving Generalization in MRI-Based Deep Learning Models for Total Knee Replacement Prediction","abstract":"Knee osteoarthritis (KOA) is a common joint disease that causes pain and mobility issues. While MRI-based deep learning models have demonstrated superior performance in predicting total knee replacement (TKR) and disease progression, their generalizability remains challenging, particularly when applied to imaging data from different sources. In this study, we have shown that replacing batch normalization with instance normalization, using data augmentation, and applying contrastive loss improves model generalization in a baseline deep learning model for knee osteoarthritis (KOA) prediction. We trained and evaluated our model using MRI data from the Osteoarthritis Initiative (OAI) database, considering sagittal fat-suppressed intermediate-weighted turbo spin-echo (FS-IW-TSE) images as the source domain and sagittal fat-suppressed three-dimensional (3D) dual-echo in steady state (DESS) images as the target domain. The results demonstrate a statistically significant improvement in classification accuracy across both domains, with our approach outperforming the baseline model.","authors":["Ehsan Karami","Hamid Soltanian-Zadeh"],"url":"https://arxiv.org/abs/2504.19203"}
{"created":"2025-04-29","title":"Robust Nash equilibrium seeking based on semi-Markov switching topologies","abstract":"This paper investigates a distributed robust Nash Equilibrium (NE) seeking problem in fluctuating environments. Specifically, the players, subject to the second-order dynamics, are considered to be influenced by external disturbances and uncertain dynamics while communicating via semi-Markov switching topologies. In such constantly changing network circumstances, the existence of disturbances and uncertain dynamics may directly affect the performance of most existing NE seeking algorithms. Moreover, the semi-Markov switching topologies may cause communication uncertainty, which are considered in NE seeking for the first time. To accommodate the above concerns, the following targets require to be reached simultaneously: (1) Disturbances and uncertain dynamics rejection in finite time; (2) Distributed estimation on unknown information required for players' cost functions; (3) A reasonable estimation consensus protocol under semi-Markov switching; (4) NE seeking for the second-order players. By combining supertwisting-based Integral Sliding-Mode Control (ISMC) with average consensus tracking, a novel robust NE seeking algorithm is constructed, incorporating an effective leader-follower consensus protocol. Furthermore, to lessen dispensable information transmission, a sampled-data-based event-triggered mechanism is introduced. Incorporating the advantages of both semi-Markov switching and event-triggered mechanism, another NE seeking algorithm is proposed. Through designing an appropriate Lyapunov-Krasovskii functional, it is shown that the leader-follower consensus can be achieved in the mean-square sense under event-triggered mechanism. Finally, a connectivity control game is formulated to illustrate the validity of the designed algorithms.","authors":["Jianing Chen","Sitian Qin","Chuangyin Dang"],"url":"https://arxiv.org/abs/2504.19229"}
{"created":"2025-04-29","title":"Test Set Sizing for the Ridge Regression","abstract":"We derive the ideal train/test split for the ridge regression to high accuracy in the limit that the number of training rows m becomes large. The split must depend on the ridge tuning parameter, alpha, but we find that the dependence is weak and can asymptotically be ignored; all parameters vanish except for m and the number of features, n. This is the first time that such a split is calculated mathematically for a machine learning model in the large data limit. The goal of the calculations is to maximize \"integrity,\" so that the measured error in the trained model is as close as possible to what it theoretically should be. This paper's result for the ridge regression split matches prior art for the plain vanilla linear regression split to the first two terms asymptotically, and it appears that practically there is no difference.","authors":["Alexander Dubbs"],"url":"https://arxiv.org/abs/2504.19231"}
{"created":"2025-04-29","title":"The effect of the number of parameters and the number of local feature patches on loss landscapes in distributed quantum neural networks","abstract":"Quantum neural networks hold promise for tackling computationally challenging tasks that are intractable for classical computers. However, their practical application is hindered by significant optimization challenges, arising from complex loss landscapes characterized by barren plateaus and numerous local minima. These problems become more severe as the number of parameters or qubits increases, hampering effective training. To mitigate these optimization challenges, particularly for quantum machine learning applied to classical data, we employ an approach of distributing overlapping local patches across multiple quantum neural networks, processing each patch with an independent quantum neural network, and aggregating their outputs for prediction. In this study, we investigate how the number of parameters and patches affects the loss landscape geometry of this distributed quantum neural network architecture via Hessian analysis and loss landscape visualization. Our results confirm that increasing the number of parameters tends to lead to deeper and sharper loss landscapes. Crucially, we demonstrate that increasing the number of patches significantly reduces the largest Hessian eigenvalue at minima. This finding suggests that our distributed patch approach acts as a form of implicit regularization, promoting optimization stability and potentially enhancing generalization. Our study provides valuable insights into optimization challenges and highlights that the distributed patch approach is a promising strategy for developing more trainable and practical quantum machine learning models for classical data tasks.","authors":["Yoshiaki Kawase"],"url":"https://arxiv.org/abs/2504.19239"}
{"created":"2025-04-29","title":"Projective systems and bounds on the length of codes of non-zero defect","abstract":"In their 2007 book, Tsfasman and Vl\\v{a}du\\c{t} invite the reader to reinterpret existing coding theory results through the lens of projective systems. Redefining linear codes as projective systems provides a geometric vantage point. In this paper, we embrace this perspective, deriving bounds on the lengths of A$^s$MDS codes (codes with Singleton defect $s$). To help frame our discussions, we introduce the parameters $m^{s}(k,q)$, denoting the maximum length of an (non-degenerate) $[n,k,d]_q$ A$^s$MDS code, $m^{s}_t(k,q)$ denoting the maximum length of an (non-degenerate) $[n,k,d]_q$ A$^s$MDS code such that the dual code is an A$^t$MDS code, and $\\kappa(s,q)$, representing the maximum dimension $k$ for which there exists a linear code of (maximal) length $n=(s+1)(q+1)+k-2$. In particular, we address a gap in the literature by providing sufficient conditions on $n$ and $k$ under which the dual of an $[n,k,d]_q$ A$^s$MDS code is also an A$^s$MDS code. Our results subsume or improve several results in the literature. Some conjectures arise from our findings.","authors":["Tim L. Alderson","Zhipeng Zhang"],"url":"https://arxiv.org/abs/2504.19325"}
{"created":"2025-04-29","title":"Synthesis of Discrete-time Control Barrier Functions for Polynomial Systems Based on Sum-of-Squares Programming","abstract":"Discrete-time Control Barrier Functions (DTCBFs) are commonly utilized in the literature as a powerful tool for synthesizing control policies that guarantee safety of discrete-time dynamical systems. However, the systematic synthesis of DTCBFs in a computationally efficient way is at present an important open problem. This article first proposes a novel alternating-descent approach based on Sum-of-Squares programming to synthesize quadratic DTCBFs and corresponding polynomial control policies for discrete-time control-affine polynomial systems with input constraints and semi-algebraic safe sets. Subsequently, two distinct approaches are introduced to extend the proposed method to the synthesis of higher-degree polynomial DTCBFs. To demonstrate its efficacy, we apply the proposed method to numerical case studies.","authors":["Erfan Shakhesi (Maurice)","W. P. M. H. (Maurice)","Heemels","Alexander Katriniok"],"url":"https://arxiv.org/abs/2504.19330"}
{"created":"2025-04-29","title":"Contextual Online Uncertainty-Aware Preference Learning for Human Feedback","abstract":"Reinforcement Learning from Human Feedback (RLHF) has become a pivotal paradigm in artificial intelligence to align large models with human preferences. In this paper, we propose a novel statistical framework to simultaneously conduct the online decision-making and statistical inference on the optimal model using human preference data based on dynamic contextual information. Our approach introduces an efficient decision strategy that achieves both the optimal regret bound and the asymptotic distribution of the estimators. A key challenge in RLHF is handling the dependent online human preference outcomes with dynamic contexts. To address this, in the methodological aspect, we propose a two-stage algorithm starting with $\\epsilon$-greedy followed by exploitations; in the theoretical aspect, we tailor anti-concentration inequalities and matrix martingale concentration techniques to derive the uniform estimation rate and asymptotic normality of the estimators using dependent samples from both stages. Extensive simulation results demonstrate that our method outperforms state-of-the-art strategies. We apply the proposed framework to analyze the human preference data for ranking large language models on the Massive Multitask Language Understanding dataset, yielding insightful results on the performance of different large language models for medical anatomy knowledge.","authors":["Nan Lu","Ethan X. Fang","Junwei Lu"],"url":"https://arxiv.org/abs/2504.19342"}
{"created":"2025-04-29","title":"The Double Descent Behavior in Two Layer Neural Network for Binary Classification","abstract":"Recent studies observed a surprising concept on model test error called the double descent phenomenon, where the increasing model complexity decreases the test error first and then the error increases and decreases again. To observe this, we work on a two layer neural network model with a ReLU activation function designed for binary classification under supervised learning. Our aim is to observe and investigate the mathematical theory behind the double descent behavior of model test error for varying model sizes. We quantify the model size by the ratio of number of training samples to the dimension of the model. Due to the complexity of the empirical risk minimization procedure, we use the Convex Gaussian Min Max Theorem to find a suitable candidate for the global training loss.","authors":["Chathurika S Abeykoon","Aleksandr Beknazaryan","Hailin Sang"],"url":"https://arxiv.org/abs/2504.19351"}
{"created":"2025-04-29","title":"Metric Similarity and Manifold Learning of Circular Dichroism Spectra of Proteins","abstract":"We present a machine learning analysis of circular dichroism spectra of globular proteins from the SP175 database, using the optimal transport-based $1$-Wasserstein distance $\\mathcal{W}_1$ (with order $p=1$) and the manifold learning algorithm $t$-SNE. Our results demonstrate that $\\mathcal{W}_1$ is consistent with both Euclidean and Manhattan metrics while exhibiting robustness to noise. On the other hand, $t$-SNE uncovers meaningful structure in the high-dimensional data. The clustering in the $t$-SNE embedding is primarily determined by proteins with distinct secondary structure compositions: one cluster predominantly contains $\\beta$-rich proteins, while the other consists mainly of proteins with mixed $\\alpha/\\beta$ and $\\alpha$-helical content.","authors":["Gionni Marchetti"],"url":"https://arxiv.org/abs/2504.19355"}
{"created":"2025-04-29","title":"Low-Rank Adaptive Structural Priors for Generalizable Diabetic Retinopathy Grading","abstract":"Diabetic retinopathy (DR), a serious ocular complication of diabetes, is one of the primary causes of vision loss among retinal vascular diseases. Deep learning methods have been extensively applied in the grading of diabetic retinopathy (DR). However, their performance declines significantly when applied to data outside the training distribution due to domain shifts. Domain generalization (DG) has emerged as a solution to this challenge. However, most existing DG methods overlook lesion-specific features, resulting in insufficient accuracy. In this paper, we propose a novel approach that enhances existing DG methods by incorporating structural priors, inspired by the observation that DR grading is heavily dependent on vessel and lesion structures. We introduce Low-rank Adaptive Structural Priors (LoASP), a plug-and-play framework designed for seamless integration with existing DG models. LoASP improves generalization by learning adaptive structural representations that are finely tuned to the complexities of DR diagnosis. Extensive experiments on eight diverse datasets validate its effectiveness in both single-source and multi-source domain scenarios. Furthermore, visualizations reveal that the learned structural priors intuitively align with the intricate architecture of the vessels and lesions, providing compelling insights into their interpretability and diagnostic relevance.","authors":["Yunxuan Wang","Ray Yin","Yumei Tan","Hao Chen","Haiying Xia"],"url":"https://arxiv.org/abs/2504.19362"}
{"created":"2025-04-29","title":"Composable and adaptive design of machine learning interatomic potentials guided by Fisher-information analysis","abstract":"An adaptive physics-informed model design strategy for machine-learning interatomic potentials (MLIPs) is proposed. This strategy follows an iterative reconfiguration of composite models from single-term models, followed by a unified training procedure. A model evaluation method based on the Fisher information matrix (FIM) and multiple-property error metrics is proposed to guide model reconfiguration and hyperparameter optimization. Combining the model reconfiguration and the model evaluation subroutines, we provide an adaptive MLIP design strategy that balances flexibility and extensibility. In a case study of designing models against a structurally diverse niobium dataset, we managed to obtain an optimal configuration with 75 parameters generated by our framework that achieved a force RMSE of 0.172 eV/{\\AA} and an energy RMSE of 0.013 eV/atom.","authors":["Weishi Wang","Mark K. Transtrum","Vincenzo Lordi","Vasily V. Bulatov","Amit Samanta"],"url":"https://arxiv.org/abs/2504.19372"}
{"created":"2025-04-29","title":"GRADE: Grover-based Benchmarking Toolkit for Assessing Quantum Hardware","abstract":"Quantum computing holds the potential to provide speedups in solving complex problems that are currently difficult for classical computers. However, the realization of this potential is hindered by the issue of current hardware reliability, primarily due to noise and architectural imperfections. As quantum computing systems rapidly advance, there exists a need to create a generalizable benchmarking tool that can assess reliability across different hardware platforms. In this paper, we introduce GRADE (Grover-based Reliability Assessment for Device Evaluation), an open-source benchmarking toolkit to evaluate the reliability of quantum hardware using a generalized form of Grover's algorithm.","authors":["Shay Manor","Millan Kumar","Priyank Behera","Azain Khalid","Oliver Zeng"],"url":"https://arxiv.org/abs/2504.19387"}
{"created":"2025-04-29","title":"Innovative Integration of 4D Cardiovascular Reconstruction and Hologram: A New Visualization Tool for Coronary Artery Bypass Grafting Planning","abstract":"Background: Coronary artery bypass grafting (CABG) planning requires advanced spatial visualization and consideration of coronary artery depth, calcification, and pericardial adhesions. Objective: To develop and evaluate a dynamic cardiovascular holographic visualization tool for preoperative CABG planning. Methods: Using 4D cardiac computed tomography angiography data from 14 CABG candidates, we developed a semi-automated workflow for time-resolved segmentation of cardiac structures, epicardial adipose tissue (EAT), and coronary arteries with calcium scoring. The workflow incorporated methods for cardiac segmentation, coronary calcification quantification, visualization of coronary depth within EAT, and pericardial adhesion assessment through motion analysis. Dynamic cardiovascular holograms were displayed using the Looking Glass platform. Thirteen cardiac surgeons evaluated the tool using a Likert scale. Additionally, pericardial adhesion scores from holograms of 21 patients (including seven undergoing secondary cardiac surgeries) were compared with intraoperative findings. Results: Surgeons rated the visualization tool highly for preoperative planning utility (mean Likert score: 4.57/5.0). Hologram-based pericardial adhesion scoring strongly correlated with intraoperative findings (r=0.786, P<0.001). Conclusion: This study establishes a visualization framework for CABG planning that produces clinically relevant dynamic holograms from patient-specific data, with clinical feedback confirming its effectiveness for preoperative planning.","authors":["Shuo Wang","Tong Ren","Nan Cheng","Li Zhang","Rong Wang"],"url":"https://arxiv.org/abs/2504.19401"}
{"created":"2025-04-29","title":"Constrained Parameter Update Law for Adaptive Control","abstract":"In this paper, a constrained parameter update law is derived in the context of adaptive control. The parameter update law is based on constrained optimization technique where a Lagrangian is formulated to incorporate the constraints on the parameters using inverse Barrier function. The constrained parameter update law is used to develop a adaptive tracking controller and the overall stability of the adaptive controller along with the constrained parameter update law is shown using Lyapunov analysis and development in stability of constrained primal-dual dynamics. The performance of the constrained parameter update law is tested in simulation for keeping the parameters within constraints and convergence to true parameters.","authors":["Ashwin P. Dani"],"url":"https://arxiv.org/abs/2504.19412"}
{"created":"2025-04-29","title":"Sharp higher order convergence rates for the Adam optimizer","abstract":"Gradient descent based optimization methods are the methods of choice to train deep neural networks in machine learning. Beyond the standard gradient descent method, also suitable modified variants of standard gradient descent involving acceleration techniques such as the momentum method and/or adaptivity techniques such as the RMSprop method are frequently considered optimization methods. These days the most popular of such sophisticated optimization schemes is presumably the Adam optimizer that has been proposed in 2014 by Kingma and Ba. A highly relevant topic of research is to investigate the speed of convergence of such optimization methods. In particular, in 1964 Polyak showed that the standard gradient descent method converges in a neighborhood of a strict local minimizer with rate (x - 1)(x + 1)^{-1} while momentum achieves the (optimal) strictly faster convergence rate (\\sqrt{x} - 1)(\\sqrt{x} + 1)^{-1} where x \\in (1,\\infty) is the condition number (the ratio of the largest and the smallest eigenvalue) of the Hessian of the objective function at the local minimizer. It is the key contribution of this work to reveal that Adam also converges with the strictly faster convergence rate (\\sqrt{x} - 1)(\\sqrt{x} + 1)^{-1} while RMSprop only converges with the convergence rate (x - 1)(x + 1)^{-1}.","authors":["Steffen Dereich","Arnulf Jentzen","Adrian Riekert"],"url":"https://arxiv.org/abs/2504.19426"}
{"created":"2025-04-29","title":"Dual Attention Driven Lumbar Magnetic Resonance Image Feature Enhancement and Automatic Diagnosis of Herniation","abstract":"Lumbar disc herniation (LDH) is a common musculoskeletal disease that requires magnetic resonance imaging (MRI) for effective clinical management. However, the interpretation of MRI images heavily relies on the expertise of radiologists, leading to delayed diagnosis and high costs for training physicians. Therefore, this paper proposes an innovative automated LDH classification framework. To address these key issues, the framework utilizes T1-weighted and T2-weighted MRI images from 205 people. The framework extracts clinically actionable LDH features and generates standardized diagnostic outputs by leveraging data augmentation and channel and spatial attention mechanisms. These outputs can help physicians make confident and time-effective care decisions when needed. The proposed framework achieves an area under the receiver operating characteristic curve (AUC-ROC) of 0.969 and an accuracy of 0.9486 for LDH detection. The experimental results demonstrate the performance of the proposed framework. Our framework only requires a small number of datasets for training to demonstrate high diagnostic accuracy. This is expected to be a solution to enhance the LDH detection capabilities of primary hospitals.","authors":["Lingrui Zhang","Liang Guo","Xiao An","Feng Lin","Binlong Zheng","Jiankun Wang","Zhirui Li"],"url":"https://arxiv.org/abs/2504.19438"}
{"created":"2025-04-29","title":"Model uncertainty quantification using feature confidence sets for outcome excursions","abstract":"When implementing prediction models for high-stakes real-world applications such as medicine, finance, and autonomous systems, quantifying prediction uncertainty is critical for effective risk management. Traditional approaches to uncertainty quantification, such as confidence and prediction intervals, provide probability coverage guarantees for the expected outcomes $f(\\boldsymbol{x})$ or the realized outcomes $f(\\boldsymbol{x})+\\epsilon$. Instead, this paper introduces a novel, model-agnostic framework for quantifying uncertainty in continuous and binary outcomes using confidence sets for outcome excursions, where the goal is to identify a subset of the feature space where the expected or realized outcome exceeds a specific value. The proposed method constructs data-dependent inner and outer confidence sets that aim to contain the true feature subset for which the expected or realized outcomes of these features exceed a specified threshold. We establish theoretical guarantees for the probability that these confidence sets contain the true feature subset, both asymptotically and for finite sample sizes. The framework is validated through simulations and applied to real-world datasets, demonstrating its utility in contexts such as housing price prediction and time to sepsis diagnosis in healthcare. This approach provides a unified method for uncertainty quantification that is broadly applicable across various continuous and binary prediction models.","authors":["Junting Ren","Armin Schwartzman"],"url":"https://arxiv.org/abs/2504.19464"}
{"created":"2025-04-29","title":"A Cautionary Note on Quantum Oracles","abstract":"In recent years, the quantum oracle model introduced by Aaronson and Kuperberg (2007) has found a lot of use in showing oracle separations between complexity classes and cryptographic primitives. It is generally assumed that proof techniques that do not relativize with respect to quantum oracles will also not relativize with respect to classical oracles. In this note, we show that this is not the case: specifically, we show that there is a quantum oracle problem that is contained in the class QMA, but not in a class we call polyQCPH. The class polyQCPH is equal to PSPACE with respect to classical oracles, and it is a well-known result that QMA is contained in PSPACE (also with respect to classical oracles).","authors":["Avantika Agarwal","Srijita Kundu"],"url":"https://arxiv.org/abs/2504.19470"}
{"created":"2025-04-29","title":"Optimal Sequential Recommendations: Exploiting User and Item Structure","abstract":"We consider an online model for recommendation systems, with each user being recommended an item at each time-step and providing 'like' or 'dislike' feedback. A latent variable model specifies the user preferences: both users and items are clustered into types. The model captures structure in both the item and user spaces, as used by item-item and user-user collaborative filtering algorithms. We study the situation in which the type preference matrix has i.i.d. entries. Our main contribution is an algorithm that simultaneously uses both item and user structures, proved to be near-optimal via corresponding information-theoretic lower bounds. In particular, our analysis highlights the sub-optimality of using only one of item or user structure (as is done in most collaborative filtering algorithms).","authors":["Mina Karzand","Guy Bresler"],"url":"https://arxiv.org/abs/2504.19476"}
{"created":"2025-04-29","title":"Two-parameter superposable S-curves","abstract":"Straight line equation $y=mx$ with slope $m$, when singularly perturbed as $ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or S-curves on a real plane. As $a\\rightarrow 0$, we get back $y=mx$ which is a cumulative distribution function of a continuous uniform distribution that describes the occurrence of every event in an interval to be equally probable. As $a\\rightarrow\\infty$, the derivative of $y$ has finite support only at $y=0$ resembling a degenerate distribution. Based on these arguments, in this work, we propose that these S-curves can represent maximum entropy uniform distribution to a zero entropy single value. We also argue that these S-curves are superposable as they are only parametrically nonlinear but fundamentally linear. So far, the superposed forms have been used to capture the patterns of natural systems such as nonlinear dynamics of biological growth and kinetics of enzyme reactions. Here, we attempt to use the S-curve and its superposed form as a statistical model. We fit the models on a classical dataset containing flower measurements of iris plants and analyze their usefulness in pattern recognition. Based on these models, we claim that any non-uniform pattern can be represented as a singular perturbation to uniform distribution. However, our parametric estimation procedure have some limitations such as sensitivity to initial conditions depending on the data at hand.","authors":["Vijay Prakash S"],"url":"https://arxiv.org/abs/2504.19488"}
{"created":"2025-04-29","title":"Sliding motions on systems with non-Euclidean state spaces: A differential-geometric perspective","abstract":"This paper extends sliding-mode control theory to nonlinear systems evolving on smooth manifolds. Building on differential geometric methods, we reformulate Filippov's notion of solutions, characterize well-defined vector fields on quotient spaces, and provide a consistent geometric definition of higher-order sliding modes. We generalize the regular form to non-Euclidean settings and design explicit first- and second-order sliding-mode controllers that respect the manifold structure. Particular attention is given to the role of topological obstructions, which are illustrated through examples on the cylinder, M\\\"obius bundle, and 2-sphere. Our results highlight how geometric and topological properties fundamentally influence sliding dynamics and suggest new directions for robust control in nonlinear spaces.","authors":["Fernando Casta\\~nos"],"url":"https://arxiv.org/abs/2504.19504"}
{"created":"2025-04-29","title":"Lifecycle Management of Optical Networks with Dynamic-Updating Digital Twin: A Hybrid Data-Driven and Physics-Informed Approach","abstract":"Digital twin (DT) techniques have been proposed for the autonomous operation and lifecycle management of next-generation optical networks. To fully utilize potential capacity and accommodate dynamic services, the DT must dynamically update in sync with deployed optical networks throughout their lifecycle, ensuring low-margin operation. This paper proposes a dynamic-updating DT for the lifecycle management of optical networks, employing a hybrid approach that integrates data-driven and physics-informed techniques for fiber channel modeling. This integration ensures both rapid calculation speed and high physics consistency in optical performance prediction while enabling the dynamic updating of critical physical parameters for DT. The lifecycle management of optical networks, covering accurate performance prediction at the network deployment and dynamic updating during network operation, is demonstrated through simulation in a large-scale network. Up to 100 times speedup in prediction is observed compared to classical numerical methods. In addition, the fiber Raman gain strength, amplifier frequency-dependent gain profile, and connector loss between fiber and amplifier on C and L bands can be simultaneously updated. Moreover, the dynamic-updating DT is verified on a field-trial C+L-band transmission link, achieving a maximum accuracy improvement of 1.4 dB for performance estimation post-device replacement. Overall, the dynamic-updating DT holds promise for driving the next-generation optical networks towards lifecycle autonomous management.","authors":["Yuchen Song","Min Zhang","Yao Zhang","Yan Shi","Shikui Shen","Xiongyan Tang","Shanguo Huang","Danshi Wang"],"url":"https://arxiv.org/abs/2504.19564"}
{"created":"2025-04-29","title":"Faithful universal graphs for minor-closed classes","abstract":"It was proved by Huynh, Mohar, \\v{S}\\'amal, Thomassen and Wood in 2021 that any countable graph containing every countable planar graph as a subgraph has an infinite clique minor. We prove a finite, quantitative version of this result: for fixed $t$, if a graph $G$ is $K_t$-minor-free and contains every $n$-vertex planar graph as a subgraph, then $G$ has $2^{\\Omega(\\sqrt{n})}$ vertices. If $G$ contains every $n$-vertex toroidal graph instead, then $G$ has $2^{\\Omega(n)}$ vertices. On the other hand, we construct a polynomial size $K_4$-minor-free graph containing every $n$-vertex tree as an induced subgraph, and a polynomial size $K_7$-minor-free graph containing every $n$-vertex $K_4$-minor-free graph as induced subgraph. This answers several problems raised recently by Bergold, Ir\\v{s}i\\v{c}, Lauff, Orthaber, Scheucher and Wesolek.","authors":["Paul Bastide","Louis Esperet","Carla Groenland","Claire Hilaire","Cl\\'ement Rambaud","Alexandra Wesolek"],"url":"https://arxiv.org/abs/2504.19582"}
{"created":"2025-04-29","title":"Towards Robust Multimodal Physiological Foundation Models: Handling Arbitrary Missing Modalities","abstract":"Multimodal physiological signals, such as EEG, ECG, EOG, and EMG, are crucial for healthcare and brain-computer interfaces. While existing methods rely on specialized architectures and dataset-specific fusion strategies, they struggle to learn universal representations that generalize across datasets and handle missing modalities at inference time. To address these issues, we propose PhysioOmni, a foundation model for multimodal physiological signal analysis that models both homogeneous and heterogeneous features to decouple multimodal signals and extract generic representations while maintaining compatibility with arbitrary missing modalities. PhysioOmni trains a decoupled multimodal tokenizer, enabling masked signal pre-training via modality-invariant and modality-specific objectives. To ensure adaptability to diverse and incomplete modality combinations, the pre-trained encoders undergo resilient fine-tuning with prototype alignment on downstream datasets. Extensive experiments on four downstream tasks, emotion recognition, sleep stage classification, motor prediction, and mental workload detection, demonstrate that PhysioOmni achieves state-of-the-art performance while maintaining strong robustness to missing modalities. Our code and model weights will be released.","authors":["Xi Fu","Wei-Bang Jiang","Yi Ding","Cuntai Guan"],"url":"https://arxiv.org/abs/2504.19596"}
{"created":"2025-04-29","title":"A Comparative Study on Positional Encoding for Time-frequency Domain Dual-path Transformer-based Source Separation Models","abstract":"In this study, we investigate the impact of positional encoding (PE) on source separation performance and the generalization ability to long sequences (length extrapolation) in Transformer-based time-frequency (TF) domain dual-path models. The length extrapolation capability in TF-domain dual-path models is a crucial factor, as it affects not only their performance on long-duration inputs but also their generalizability to signals with unseen sampling rates. While PE is known to significantly impact length extrapolation, there has been limited research that explores the choice of PEs for TF-domain dual-path models from this perspective. To address this gap, we compare various PE methods using a recent state-of-the-art model, TF-Locoformer, as the base architecture. Our analysis yields the following key findings: (i) When handling sequences that are the same length as or shorter than those seen during training, models with PEs achieve better performance. (ii) However, models without PE exhibit superior length extrapolation. This trend is particularly pronounced when the model contains convolutional layers.","authors":["Kohei Saijo","Tetsuji Ogawa"],"url":"https://arxiv.org/abs/2504.19605"}
{"created":"2025-04-29","title":"Automatic Configuration Protocols for Optical Quantum Networks","abstract":"Before quantum networks can scale up to practical sizes, there are many deployment and configuration tasks that must be automated. Currently, quantum networking testbeds are largely manually configured: network nodes are constructed out of a combination of free-space and fiber optics before being connected to shared single-photon detectors, time-to-digital converters, and optical switches. Information about these connections must be tracked manually; mislabeling may result in experimental failure and protracted debugging sessions. In this paper, we propose protocols and algorithms to automate two such manual processes. First, we address the problem of automatically identifying connections between quantum network nodes and time-to-digital converters. Then, we turn to the more complex challenge of identifying the nodes attached to a quantum network's optical switches. Implementation of these protocols will help enable the development of other protocols necessary for quantum networks, such as network topology discovery, link quality monitoring, resource naming, and routing. We intend for this paper to serve as a roadmap for near-term implementation.","authors":["Amin Taherkhani","Andrew Todd","Kentaro Teramoto","Shota Nagayama","Rodney Van Meter"],"url":"https://arxiv.org/abs/2504.19613"}
{"created":"2025-04-29","title":"QFDNN: A Resource-Efficient Variational Quantum Feature Deep Neural Networks for Fraud Detection and Loan Prediction","abstract":"Social financial technology focuses on trust, sustainability, and social responsibility, which require advanced technologies to address complex financial tasks in the digital era. With the rapid growth in online transactions, automating credit card fraud detection and loan eligibility prediction has become increasingly challenging. Classical machine learning (ML) models have been used to solve these challenges; however, these approaches often encounter scalability, overfitting, and high computational costs due to complexity and high-dimensional financial data. Quantum computing (QC) and quantum machine learning (QML) provide a promising solution to efficiently processing high-dimensional datasets and enabling real-time identification of subtle fraud patterns. However, existing quantum algorithms lack robustness in noisy environments and fail to optimize performance with reduced feature sets. To address these limitations, we propose a quantum feature deep neural network (QFDNN), a novel, resource efficient, and noise-resilient quantum model that optimizes feature representation while requiring fewer qubits and simpler variational circuits. The model is evaluated using credit card fraud detection and loan eligibility prediction datasets, achieving competitive accuracies of 82.2% and 74.4%, respectively, with reduced computational overhead. Furthermore, we test QFDNN against six noise models, demonstrating its robustness across various error conditions. Our findings highlight QFDNN potential to enhance trust and security in social financial technology by accurately detecting fraudulent transactions while supporting sustainability through its resource-efficient design and minimal computational overhead.","authors":["Subham Das","Ashtakala Meghanath","Bikash K. Behera","Shahid Mumtaz","Saif Al-Kuwari","Ahmed Farouk"],"url":"https://arxiv.org/abs/2504.19632"}
{"created":"2025-04-29","title":"Neuronal correlations shape the scaling behavior of memory capacity and nonlinear computational capability of recurrent neural networks","abstract":"Reservoir computing is a powerful framework for real-time information processing, characterized by its high computational ability and quick learning, with applications ranging from machine learning to biological systems. In this paper, we demonstrate that the memory capacity of a reservoir recurrent neural network scales sublinearly with the number of readout neurons. To elucidate this phenomenon, we develop a theoretical framework for analytically deriving memory capacity, attributing the decaying growth of memory capacity to neuronal correlations. In addition, numerical simulations reveal that once memory capacity becomes sublinear, increasing the number of readout neurons successively enables nonlinear processing at progressively higher polynomial orders. Furthermore, our theoretical framework suggests that neuronal correlations govern not only memory capacity but also the sequential growth of nonlinear computational capabilities. Our findings establish a foundation for designing scalable and cost-effective reservoir computing, providing novel insights into the interplay among neuronal correlations, linear memory, and nonlinear processing.","authors":["Shotaro Takasu","Toshio Aoyagi"],"url":"https://arxiv.org/abs/2504.19657"}
{"created":"2025-04-29","title":"Lossy Beyond Diagonal Reconfigurable Intelligent Surfaces: Modeling and Optimization","abstract":"Beyond diagonal reconfigurable intelligent surface (BD-RIS) has emerged as an advancement and generalization of the conventional diagonal RIS (D-RIS) by introducing tunable interconnections between RIS elements, enabling smarter wave manipulation and enlarged coverage. While BD-RIS has demonstrated advantages over D-RIS in various aspects, most existing works rely on the assumption of a lossless model, leaving practical considerations unaddressed. This paper thus proposes a lossy BD-RIS model and develops corresponding optimization algorithms for various BD-RIS-aided communication systems. First, by leveraging admittance parameter analysis, we model each tunable admittance based on a lumped circuit with losses and derive an expression of a circle characterizing the real and imaginary parts of each tunable admittance. We then consider the received signal power maximization in single-user single-input single-output (SISO) systems with the proposed lossy BD-RIS model. To solve the optimization problem, we design an effective algorithm by carefully exploiting the problem structure. Specifically, an alternating direction method of multipliers (ADMM) framework is custom-designed to deal with the complicated constraints associated with lossy BD-RIS. Furthermore, we extend the proposed algorithmic framework to more general multiuser multiple-input single-output (MU-MISO) systems, where the transmit precoder and BD-RIS scattering matrix are jointly designed to maximize the sum-rate of the system. Finally, simulation results demonstrate that all BD-RIS architectures still outperform D-RIS in the presence of losses, but the optimal BD-RIS architectures in the lossless case are not necessarily optimal in the lossy case, e.g. group-connected BD-RIS can outperform fully- and tree-connected BD-RISs in SISO systems with relatively high losses, whereas the opposite always holds true in the lossless case.","authors":["Yiyang Peng","Hongyu Li","Zheyu Wu","Bruno Clerckx"],"url":"https://arxiv.org/abs/2504.19744"}
{"created":"2025-04-29","title":"Rediscovery","abstract":"We model search in settings where decision makers know what can be found but not where to find it. A searcher faces a set of choices arranged by an observable attribute. Each period, she either selects a choice and pays a cost to learn about its quality, or she concludes search to take her best discovery to date. She knows that similar choices have similar qualities and uses this to guide her search. We identify robustly optimal search policies with a simple structure. Search is directional, recall is never invoked, there is a threshold stopping rule, and the policy at each history depends only on a simple index.","authors":["Martino Banchio","Suraj Malladi"],"url":"https://arxiv.org/abs/2504.19761"}
{"created":"2025-04-29","title":"Determining a graph from its reconfiguration graph","abstract":"Given a graph $G$ and a natural number $k$, the $k$-recolouring graph $\\mathcal{C}_k(G)$ is the graph whose vertices are the $k$-colourings of $G$ and whose edges link pairs of colourings which differ at exactly one vertex of $G$. Recently, Hogan et al. proved that $G$ can be determined from $\\mathcal{C}_k(G)$ provided $k$ is large enough (quadratic in the number of vertices of $G$). We improve this bound by showing that $k=\\chi(G)+1$ colours suffice, and provide examples of families of graphs for which $k=\\chi(G)$ colours do not suffice.","authors":["Ga\\'etan Berthe","Caroline Brosse","Brian Hearn","Jan van den Heuvel","Pierre Hoppenot","Th\\'eo Pierron"],"url":"https://arxiv.org/abs/2504.19783"}
{"created":"2025-04-29","title":"Interpretable machine learning-guided design of Fe-based soft magnetic alloys","abstract":"We present a machine-learning guided approach to predict saturation magnetization (MS) and coercivity (HC) in Fe-rich soft magnetic alloys, particularly Fe-Si-B systems. ML models trained on experimental data reveals that increasing Si and B content reduces MS from 1.81T (DFT~2.04 T) to ~1.54 T (DFT~1.56T) in Fe-Si-B, which is attributed to decreased magnetic density and structural modifications. Experimental validation of ML predicted magnetic saturation on Fe-1Si-1B (2.09T), Fe-5Si-5B (2.01T) and Fe-10Si-10B (1.54T) alloy compositions further support our findings. These trends are consistent with density functional theory (DFT) predictions, which link increased electronic disorder and band broadening to lower MS values. Experimental validation on selected alloys confirms the predictive accuracy of the ML model, with good agreement across compositions. Beyond predictive accuracy, detailed uncertainty quantification and model interpretability including through feature importance and partial dependence analysis reveals that MS is governed by a nonlinear interplay between Fe content, early transition metal ratios, and annealing temperature, while HC is more sensitive to processing conditions such as ribbon thickness and thermal treatment windows. The ML framework was further applied to Fe-Si-B/Cr/Cu/Zr/Nb alloys in a pseudo-quaternary compositional space, which shows comparable magnetic properties to NANOMET (Fe84.8Si0.5B9.4Cu0.8 P3.5C1), FINEMET (Fe73.5Si13.5B9 Cu1Nb3), NANOPERM (Fe88Zr7B4Cu1), and HITPERM (Fe44Co44Zr7B4Cu1. Our fundings demonstrate the potential of ML framework for accelerated search of high-performance, Co- and Ni-free, soft magnetic materials.","authors":["Aditi Nachnani","Kai K. Li-Caldwell","Saptarshi Biswas","Prince Sharma","Gaoyuan Ouyang","Prashant Singh"],"url":"https://arxiv.org/abs/2504.19787"}
{"created":"2025-04-29","title":"Modified Control Barrier Function for Quadratic Program Based Control Design via Sum-of-Squares Programming","abstract":"We consider a nonlinear control affine system controlled by inputs generated by a quadratic program (QP) induced by a control barrier functions (CBF). Specifically, we slightly modify the condition satisfied by CBFs and study how the modification can positively impact the closed loop behavior of the system. We show that, QP-based controllers designed using the modified CBF condition preserves the desired properties of QP-based controllers using standard CBF conditions. Furthermore, using the generalized S-procedure for polynomial functions, we formulate the design of the modified CBFs as a Sum-Of-Squares (SOS) program, which can be solved efficiently. Via a numerical example, the proposed CBF design is shown to have superior performance over the standard CBF widely used in existing literature.","authors":["Yankai Lin","Michelle S. Chong","Carlos Murguia"],"url":"https://arxiv.org/abs/2504.19796"}
{"created":"2025-04-29","title":"Some PDE results in Heston model with applications","abstract":"We present here some results for the PDE related to the logHeston model. We present different regularity results and prove a verification theorem that shows that the solution produced via the Feynman-Kac theorem is the unique viscosity solution for a wide choice of initial data (even discontinuous) and source data. In addition, our techniques do not use Feller's condition at any time. In the end, we prove a convergence theorem to approximate this solution by means of a hybrid (finite differences/tree scheme) approach.","authors":["Edoardo Lombardo"],"url":"https://arxiv.org/abs/2504.19859"}
{"created":"2025-04-29","title":"Unravelling mean-field Lindblad equation","abstract":"We propose a mean-field particle Monte Carlo method for simulating the N-body Lindblad equation. We provide a convergence result showing that a system of interacting particles converges to the corresponding nonlinear Lindblad equation in the large N limit.","authors":["Sofiane Chalal","Nina H. Amini"],"url":"https://arxiv.org/abs/2504.19928"}
{"created":"2025-04-29","title":"Accelerated 3D-3D rigid registration of echocardiographic images obtained from apical window using particle filter","abstract":"The perfect alignment of 3D echocardiographic images captured from various angles has improved image quality and broadened the field of view. This study proposes an accelerated sequential Monte Carlo (SMC) algorithm for 3D-3D rigid registration of transthoracic echocardiographic images with significant and limited overlap taken from apical window that is robust to the noise and intensity variation in ultrasound images. The algorithm estimates the translational and rotational components of the rigid transform through an iterative process and requires an initial approximation of the rotation and translation limits. We perform registration in two ways: the image-based registration computes the transform to align the end-diastolic frame of the apical nonstandard image to the apical standard image and applies the same transform to all frames of the cardiac cycle, whereas the mask-based registration approach uses the binary masks of the left ventricle in the same way. The SMC and exhaustive search (EX) algorithms were evaluated for 4D temporal sequences recorded from 7 volunteers who participated in a study conducted at the Mazankowski Alberta Heart Institute. The evaluations demonstrate that the mask-based approach of the accelerated SMC yielded a Dice score value of 0.819 +/- 0.045 for the left ventricle and gained 16.7x speedup compared to the CPU version of the SMC algorithm.","authors":["Thanuja Uruththirakodeeswaran","Harald Becher","Michelle Noga","Lawrence H. Le","Pierre Boulanger","Jonathan Windram","Kumaradevan Punithakumar"],"url":"https://arxiv.org/abs/2504.19930"}
{"created":"2025-04-29","title":"SST-DUNet: Automated preclinical functional MRI skull stripping using Smart Swin Transformer and Dense UNet","abstract":"Skull stripping is a common preprocessing step that is often performed manually in Magnetic Resonance Imaging (MRI) pipelines, including functional MRI (fMRI). This manual process is time-consuming and operator dependent. Automating this process is challenging for preclinical data due to variations in brain geometry, resolution, and tissue contrast. While existing methods for MRI skull stripping exist, they often struggle with the low resolution and varying slice sizes in preclinical fMRI data. This study proposes a novel method called SST-DUNet, that integrates a dense UNet-based architecture with a feature extractor based on Smart Swin Transformer (SST) for fMRI skull stripping. The Smart Shifted Window Multi-Head Self-Attention (SSW-MSA) module in SST is adapted to replace the mask-based module in the Swin Transformer (ST), enabling the learning of distinct channel-wise features while focusing on relevant dependencies within brain structures. This modification allows the model to better handle the complexities of fMRI skull stripping, such as low resolution and variable slice sizes. To address the issue of class imbalance in preclinical data, a combined loss function using Focal and Dice loss is utilized. The model was trained on rat fMRI images and evaluated across three in-house datasets with a Dice similarity score of 98.65%, 97.86%, and 98.04%. The fMRI results obtained through automatic skull stripping using the SST-DUNet model closely align with those from manual skull stripping for both seed-based and independent component analyses. These results indicate that the SST-DUNet can effectively substitute manual brain extraction in rat fMRI analysis.","authors":["Sima Soltanpour","Rachel Utama","Arnold Chang","Md Taufiq Nasseef","Dan Madularu","Praveen Kulkarni","Craig Ferris","Chris Joslin"],"url":"https://arxiv.org/abs/2504.19937"}
{"created":"2025-04-29","title":"On Stopping Times of Power-one Sequential Tests: Tight Lower and Upper Bounds","abstract":"We prove two lower bounds for stopping times of sequential tests between general composite nulls and alternatives. The first lower bound is for the setting where the type-1 error level $\\alpha$ approaches zero, and equals $\\log(1/\\alpha)$ divided by a certain infimum KL divergence, termed $\\operatorname{KL_{inf}}$. The second lower bound applies to the setting where $\\alpha$ is fixed and $\\operatorname{KL_{inf}}$ approaches 0 (meaning that the null and alternative sets are not separated) and equals $c \\operatorname{KL_{inf}}^{-1} \\log \\log \\operatorname{KL_{inf}}^{-1}$ for a universal constant $c > 0$. We also provide a sufficient condition for matching the upper bounds and show that this condition is met in several special cases. Given past work, these upper and lower bounds are unsurprising in their form; our main contribution is the generality in which they hold, for example, not requiring reference measures or compactness of the classes.","authors":["Shubhada Agrawal","Aaditya Ramdas"],"url":"https://arxiv.org/abs/2504.19952"}
{"created":"2025-04-29","title":"Quantum circuit lower bounds in the magic hierarchy","abstract":"We introduce the magic hierarchy, a quantum circuit model that alternates between arbitrary-sized Clifford circuits and constant-depth circuits with two-qubit gates ($\\textsf{QNC}^0$). This model unifies existing circuit models, such as $\\textsf{QAC}^0_f$ and models with adaptive intermediate measurements. Despite its generality, we are able to prove nontrivial lower bounds.","authors":["Natalie Parham"],"url":"https://arxiv.org/abs/2504.19966"}
{"created":"2025-04-29","title":"Graph Neural Network Prediction of Nonlinear Optical Properties","abstract":"Nonlinear optical (NLO) materials for generating lasers via second harmonic generation (SHG) are highly sought in today's technology. However, discovering novel materials with considerable SHG is challenging due to the time-consuming and costly nature of both experimental methods and first-principles calculations. In this study, we present a deep learning approach using the Atomistic Line Graph Neural Network (ALIGNN) to predict NLO properties. Sourcing data from the Novel Opto-Electronic Materials Discovery (NOEMD) database and using the Kurtz-Perry (KP) coefficient as the key target, we developed a robust model capable of accurately estimating nonlinear optical responses. Our results demonstrate that the model achieves 82.5% accuracy at a tolerated absolute error up to 1 pm/V and relative error not exceeding 0.5. This work highlights the potential of deep learning in accelerating the discovery and design of advanced optical materials with desired properties.","authors":["Yomn Alkabakibi","Congwei Xie","Artem R. Oganov"],"url":"https://arxiv.org/abs/2504.19987"}
{"created":"2025-04-29","title":"Compositional Square Roots of $\\exp(x)$ and $1+x^2$","abstract":"Our work began as an effort to understand calculations by Morris & Szekeres (1961) and Walker (1991) regarding fractional iteration.","authors":["Steven Finch"],"url":"https://arxiv.org/abs/2504.19999"}
{"created":"2025-04-29","title":"Curiosity Driven Exploration to Optimize Structure-Property Learning in Microscopy","abstract":"Rapidly determining structure-property correlations in materials is an important challenge in better understanding fundamental mechanisms and greatly assists in materials design. In microscopy, imaging data provides a direct measurement of the local structure, while spectroscopic measurements provide relevant functional property information. Deep kernel active learning approaches have been utilized to rapidly map local structure to functional properties in microscopy experiments, but are computationally expensive for multi-dimensional and correlated output spaces. Here, we present an alternative lightweight curiosity algorithm which actively samples regions with unexplored structure-property relations, utilizing a deep-learning based surrogate model for error prediction. We show that the algorithm outperforms random sampling for predicting properties from structures, and provides a convenient tool for efficient mapping of structure-property relationships in materials science.","authors":["Aditya Vatsavai","Ganesh Narasimha","Yongtao Liu","Jan-Chi Yang","Hiroshu Funakubo","Maxim Ziatdinov","Rama Vasudevan"],"url":"https://arxiv.org/abs/2504.20011"}
{"created":"2025-04-29","title":"A Bayesian approach to modeling topic-metadata relationships","abstract":"The objective of advanced topic modeling is not only to explore latent topical structures, but also to estimate relationships between the discovered topics and theoretically relevant metadata. Methods used to estimate such relationships must take into account that the topical structure is not directly observed, but instead being estimated itself in an unsupervised fashion, usually by common topic models. A frequently used procedure to achieve this is the method of composition, a Monte Carlo sampling technique performing multiple repeated linear regressions of sampled topic proportions on metadata covariates. In this paper, we propose two modifications of this approach: First, we substantially refine the existing implementation of the method of composition from the R package stm by replacing linear regression with the more appropriate Beta regression. Second, we provide a fundamental enhancement of the entire estimation framework by substituting the current blending of frequentist and Bayesian methods with a fully Bayesian approach. This allows for a more appropriate quantification of uncertainty. We illustrate our improved methodology by investigating relationships between Twitter posts by German parliamentarians and different metadata covariates related to their electoral districts, using the Structural Topic Model to estimate topic proportions.","authors":["P. Schulze","S. Wiegrebe","P. W. Thurner","C. Heumann","M. A{\\ss}enmacher"],"url":"https://arxiv.org/abs/2104.02496"}
{"created":"2025-04-29","title":"Memory Regulation and Alignment toward Generalizer RGB-Infrared Person","abstract":"The domain shift, coming from unneglectable modality gap and non-overlapped identity classes between training and test sets, is a major issue of RGB-Infrared person re-identification. A key to tackle the inherent issue -- domain shift -- is to enforce the data distributions of the two domains to be similar. However, RGB-IR ReID always demands discriminative features, leading to over-rely feature sensitivity of seen classes, \\textit{e.g.}, via attention-based feature alignment or metric learning. Therefore, predicting the unseen query category from predefined training classes may not be accurate and leads to a sub-optimal adversarial gradient. In this paper, we uncover it in a more explainable way and propose a novel multi-granularity memory regulation and alignment module (MG-MRA) to solve this issue. By explicitly incorporating a latent variable attribute, from fine-grained to coarse semantic granularity, into intermediate features, our method could alleviate the over-confidence of the model about discriminative features of seen classes. Moreover, instead of matching discriminative features by traversing nearest neighbor, sparse attributes, \\textit{i.e.}, global structural pattern, are recollected with respect to features and assigned to measure pair-wise image similarity in hashing. Extensive experiments on RegDB \\cite{RegDB} and SYSU-MM01 \\cite{SYSU} show the superiority of the proposed method that outperforms existing state-of-the-art methods. Our code is available in https://github.com/Chenfeng1271/MGMRA.","authors":["Feng Chen","Fei Wu","Qi Wu","Zhiguo Wan"],"url":"https://arxiv.org/abs/2109.08843"}
{"created":"2025-04-29","title":"Understanding Dataset Difficulty with $\\mathcal{V}$-Usable Information","abstract":"Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\\mathcal{V}$ -- as the lack of $\\mathcal{V}$-$\\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\\mathcal{V}$. We further introduce $\\textit{pointwise $\\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\\mathcal{V}$-$\\textit{usable information}$ and PVI also permit the converse: for a given model $\\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.","authors":["Kawin Ethayarajh","Yejin Choi","Swabha Swayamdipta"],"url":"https://arxiv.org/abs/2110.08420"}
{"created":"2025-04-29","title":"Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach","abstract":"In recent years, the field of NLP has seen growing interest in modeling both semantic and pragmatic dimensions. Despite this progress, two key challenges persist: firstly, the complex task of mapping and analyzing the interactions between semantic and pragmatic features; secondly, the insufficient incorporation of relevant insights from related disciplines outside NLP. Addressing these issues, this study introduces a novel geometric metric that utilizes word co-occurrence patterns. This metric maps two fundamental properties - semantic typicality (cognitive) and pragmatic salience (socio-cultural) - for basic-level categories within a two-dimensional hyperbolic space. Our evaluations reveal that this semantic-pragmatic metric produces mappings for basic-level categories that not only surpass traditional cognitive semantics benchmarks but also demonstrate significant socio-cultural relevance. This finding proposes that basic-level categories, traditionally viewed as semantics-driven cognitive constructs, should be examined through the lens of both semantic and pragmatic dimensions, highlighting their role as a cognitive-cultural interface. The broad contribution of this paper lies in the development of medium-sized, interpretable, and human-centric language embedding models, which can effectively blend semantic and pragmatic dimensions to elucidate both the cognitive and socio-cultural significance of linguistic categories.","authors":["Eugene Yu Ji"],"url":"https://arxiv.org/abs/2112.06876"}
{"created":"2025-04-29","title":"Automated Machine Learning: A Case Study on Non-Intrusive Appliance Load Monitoring","abstract":"We propose a novel approach to enable Automated Machine Learning (AutoML) for Non-Intrusive Appliance Load Monitoring (NIALM), also known as Energy Disaggregation, through Bayesian Optimization. NIALM offers a cost-effective alternative to smart meters for measuring the energy consumption of electric devices and appliances. NIALM methods analyze the entire power consumption signal of a household and predict the type of appliances as well as their individual power consumption (i.e., their contributions to the aggregated signal). We enable NIALM domain experts and practitioners who typically have no deep data analytics or Machine Learning (ML) skills to benefit from state-of-the-art ML approaches to NIALM. Further, we conduct a survey and benchmarking of the state of the art and show that in many cases, simple and basic ML models and algorithms, such as Decision Trees, outperform the state of the art. Finally, we present our open-source tool, AutoML4NIALM, which will facilitate the exploitation of existing methods for NIALM in the industry.","authors":["Armin Moin","Ukrit Wattanavaekin","Alexandra Lungu","Stephan R\\\"ossler","Stephan G\\\"unnemann"],"url":"https://arxiv.org/abs/2203.02927"}
{"created":"2025-04-29","title":"Injecting Image Details into CLIP's Feature Space","abstract":"Although CLIP-like Visual Language Models provide a functional joint feature space for image and text, due to the limitation of the CILP-like model's image input size (e.g., 224), subtle details are lost in the feature representation if we input high-resolution images (e.g., 2240). In this work, we introduce an efficient framework that can produce a single feature representation for a high-resolution image that injects image details and shares the same semantic space as the original CLIP. In the framework, we train a feature fusing model based on CLIP features extracted from a carefully designed image patch method that can cover objects of any scale, weakly supervised by image-agnostic class prompted queries. We validate our framework by retrieving images from class prompted queries on the real world and synthetic datasets, showing significant performance improvement on these tasks. Furthermore, to fully demonstrate our framework's detail retrieval ability, we construct a CLEVR-like synthetic dataset called CLVER-DS, which is fully annotated and has a controllable object scale.","authors":["Zilun Zhang","Cuifeng Shen","Yuan Shen","Huixin Xiong","Xinyu Zhou"],"url":"https://arxiv.org/abs/2208.14649"}
{"created":"2025-04-29","title":"Task-Agnostic Learning to Accomplish New Tasks","abstract":"Reinforcement Learning (RL) and Imitation Learning (IL) have made great progress in robotic decision-making in recent years. However, these methods show obvious deterioration for new tasks that need to be completed through new combinations of actions. RL methods suffer from reward functions and distribution shifts, while IL methods are limited by expert demonstrations which do not cover new tasks. In contrast, humans can easily complete these tasks with the fragmented knowledge learned from task-agnostic experience. Inspired by this observation, this paper proposes a task-agnostic learning method (TAL for short) that can learn fragmented knowledge only from task-agnostic data to accomplish new tasks. TAL consists of four stages. First, the task-agnostic exploration is performed to collect data from interactions with the environment. The collected data is organized via a knowledge graph. Second, an action feature extractor is proposed and trained using the collected knowledge graph data for task-agnostic fragmented knowledge learning. Third, a candidate action generator is designed, which applies the action feature extractor on a new task to generate multiple candidate action sets. Finally, an action proposal network is designed to produce the probabilities for actions in a new task according to the environmental information. The probabilities are then used to generate order information for selecting actions to be executed from multiple candidate action sets to form the plan. Experiments on a virtual indoor scene show that the proposed method outperforms the state-of-the-art offline RL methods and IL methods by more than 20%.","authors":["Xianqi Zhang","Xingtao Wang","Xu Liu","Wenrui Wang","Xiaopeng Fan","Debin Zhao"],"url":"https://arxiv.org/abs/2209.04100"}
{"created":"2025-04-29","title":"Fast algorithms for least square problems with Kronecker lower subsets","abstract":"While leverage score sampling provides powerful tools for approximating solutions to large least squares problems, the cost of computing exact scores and sampling often prohibits practical application. This paper addresses this challenge by developing a new and efficient algorithm for exact leverage score sampling applicable to matrices that are lower column subsets of Kronecker product matrices. We synthesize relevant approximation guarantees and detail the algorithm that specifically leverages this structural property for computational efficiency. Through numerical examples, we demonstrate that utilizing efficiently computed exact leverage scores via our methods significantly reduces approximation errors, as compared to established approximate leverage score sampling strategies when applied to this important class of structured matrices.","authors":["Osman Asif Malik","Yiming Xu","Nuojin Cheng","Stephen Becker","Alireza Doostan","Akil Narayan"],"url":"https://arxiv.org/abs/2209.05662"}
{"created":"2025-04-29","title":"Optimal Acceptance of Incompatible Kidneys","abstract":"Incompatibility between patient and donor is a major barrier in kidney transplantation (KT). The increasing shortage of kidney donors has driven the development of desensitization techniques to overcome this immunological challenge. Compared with compatible KT, patients undergoing incompatible KTs are more likely to experience rejection, infection, malignancy, and graft loss. We study the optimal acceptance of possibly incompatible kidneys for individual end-stage kidney disease patients. To capture the effect of incompatibility, we propose a Markov Decision Process (MDP) model that explicitly includes compatibility as a state variable. The resulting higher-dimensional model makes it more challenging to analyze, but under suitable conditions, we derive structural properties including control limit-type optimal policies that are easy to compute and implement. Numerical examples illustrate the behavior of the optimal policy under different mismatch levels and highlight the importance of explicitly incorporating the incompatibility level into the acceptance decision when desensitization therapy is an option.","authors":["Xingyu Ren","Michael C. Fu","Steven I. Marcus"],"url":"https://arxiv.org/abs/2212.01808"}
{"created":"2025-04-29","title":"The Semantic Scholar Open Data Platform","abstract":"The volume of scientific output is creating an urgent need for automated tools to help scientists keep up with developments in their field. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientific literature. We combine public and proprietary data sources using state-of-the-art techniques for scholarly PDF content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientific literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated APIs offered by the platform. We will update this living document to reflect changes as we add new data offerings and improve existing services.","authors":["Rodney Kinney","Chloe Anastasiades","Russell Authur","Iz Beltagy","Jonathan Bragg","Alexandra Buraczynski","Isabel Cachola","Stefan Candra","Yoganand Chandrasekhar","Arman Cohan","Miles Crawford","Doug Downey","Jason Dunkelberger","Oren Etzioni","Rob Evans","Sergey Feldman","Joseph Gorney","David Graham","Fangzhou Hu","Regan Huff","Daniel King","Sebastian Kohlmeier","Bailey Kuehl","Michael Langan","Daniel Lin","Haokun Liu","Kyle Lo","Jaron Lochner","Kelsey MacMillan","Tyler Murray","Chris Newell","Smita Rao","Shaurya Rohatgi","Paul Sayre","Zejiang Shen","Amanpreet Singh","Luca Soldaini","Shivashankar Subramanian","Amber Tanaka","Alex D. Wade","Linda Wagner","Lucy Lu Wang","Chris Wilhelm","Caroline Wu","Jiangjiang Yang","Angele Zamarron","Madeleine Van Zuylen","Daniel S. Weld"],"url":"https://arxiv.org/abs/2301.10140"}
{"created":"2025-04-29","title":"DEVICE: Depth and Visual Concepts Aware Transformer for OCR-based Image Captioning","abstract":"OCR-based image captioning is an important but under-explored task, aiming to generate descriptions containing visual objects and scene text. Recent studies have made encouraging progress, but they are still suffering from a lack of overall understanding of scenes and generating inaccurate captions. One possible reason is that current studies mainly focus on constructing the plane-level geometric relationship of scene text without depth information. This leads to insufficient scene text relational reasoning so that models may describe scene text inaccurately. The other possible reason is that existing methods fail to generate fine-grained descriptions of some visual objects. In addition, they may ignore essential visual objects, leading to the scene text belonging to these ignored objects not being utilized. To address the above issues, we propose a Depth and Visual Concepts Aware Transformer (DEVICE) for OCR-based image captinong. Concretely, to construct three-dimensional geometric relations, we introduce depth information and propose a depth-enhanced feature updating module to ameliorate OCR token features. To generate more precise and comprehensive captions, we introduce semantic features of detected visual concepts as auxiliary information, and propose a semantic-guided alignment module to improve the model's ability to utilize visual concepts. Our DEVICE is capable of comprehending scenes more comprehensively and boosting the accuracy of described visual entities. Sufficient experiments demonstrate the effectiveness of our proposed DEVICE, which outperforms state-of-the-art models on the TextCaps test set.","authors":["Dongsheng Xu","Qingbao Huang","Xingmao Zhang","Haonan Cheng","Feng Shuang","Yi Cai"],"url":"https://arxiv.org/abs/2302.01540"}
{"created":"2025-04-29","title":"Keystroke Dynamics: Concepts, Techniques, and Applications","abstract":"Reliably identifying and verifying subjects remains integral to computer system security. Various novel authentication techniques, such as biometric authentication systems, have been developed in recent years. This paper provides a detailed review of keystroke-based authentication systems and their applications. Keystroke dynamics is a behavioral biometric that is emerging as an important tool for cybersecurity as it promises to be non-intrusive and cost-effective. In addition, no additional hardware is required, making it convenient to deploy. This survey covers novel keystroke datasets, state-of-the-art keystroke authentication algorithms, keystroke authentication on touch screen and mobile devices, and various prominent applications of such techniques beyond authentication. The paper covers all the significant aspects of keystroke dynamics and can be considered a reference for future researchers in this domain. The paper includes a discussion of the latest keystroke datasets, providing researchers with an up-to-date resource for analysis and experimentation. In addition, this survey covers the state-of-the-art algorithms adopted within this domain, offering insights into the cutting-edge techniques utilized for keystroke analysis. Moreover, this paper explains the diverse applications of keystroke dynamics, particularly focusing on security, verification, and identification uses. Furthermore, this paper presents a summary of future research opportunities, highlighting potential areas for exploration and development within the realm of keystroke dynamics. This forward-looking perspective aims to inspire further inquiry and innovation, guiding the trajectory of future studies in this dynamic field.","authors":["Rashik Shadman","Ahmed Anu Wahab","Michael Manno","Matthew Lukaszewski","Daqing Hou","Faraz Hussain"],"url":"https://arxiv.org/abs/2303.04605"}
{"created":"2025-04-29","title":"Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles","abstract":"The ability to explain why a machine learning model arrives at a particular prediction is crucial when used as decision support by human operators of critical systems. The provided explanations must be provably correct, and preferably without redundant information, called minimal explanations. In this paper, we aim at finding explanations for predictions made by tree ensembles that are not only minimal, but also minimum with respect to a cost function.","authors":["John T\\\"ornblom","Emil Karlsson","Simin Nadjm-Tehrani"],"url":"https://arxiv.org/abs/2303.09271"}
{"created":"2025-04-29","title":"NoisyHate: Mining Online Human-Written Perturbations for Realistic Robustness Benchmarking of Content Moderation Models","abstract":"Online texts with toxic content are a clear threat to the users on social media in particular and society in general. Although many platforms have adopted various measures (e.g., machine learning-based hate-speech detection systems) to diminish their effect, toxic content writers have also attempted to evade such measures by using cleverly modified toxic words, so-called human-written text perturbations. Therefore, to help build automatic detection tools to recognize those perturbations, prior methods have developed sophisticated techniques to generate diverse adversarial samples. However, we note that these ``algorithms\"-generated perturbations do not necessarily capture all the traits of ``human\"-written perturbations. Therefore, in this paper, we introduce a novel, high-quality dataset of human-written perturbations, named as NoisyHate, that was created from real-life perturbations that are both written and verified by human-in-the-loop. We show that perturbations in NoisyHate have different characteristics than prior algorithm-generated toxic datasets show, and thus can be in particular useful to help develop better toxic speech detection solutions. We thoroughly validate NoisyHate against state-of-the-art language models, such as BERT and RoBERTa, and black box APIs, such as Perspective API, on two tasks, such as perturbation normalization and understanding.","authors":["Yiran Ye","Thai Le","Dongwon Lee"],"url":"https://arxiv.org/abs/2303.10430"}
{"created":"2025-04-29","title":"Local-Global Temporal Difference Learning for Satellite Video Super-Resolution","abstract":"Optical-flow-based and kernel-based approaches have been extensively explored for temporal compensation in satellite Video Super-Resolution (VSR). However, these techniques are less generalized in large-scale or complex scenarios, especially in satellite videos. In this paper, we propose to exploit the well-defined temporal difference for efficient and effective temporal compensation. To fully utilize the local and global temporal information within frames, we systematically modeled the short-term and long-term temporal discrepancies since we observed that these discrepancies offer distinct and mutually complementary properties. Specifically, we devise a Short-term Temporal Difference Module (S-TDM) to extract local motion representations from RGB difference maps between adjacent frames, which yields more clues for accurate texture representation. To explore the global dependency in the entire frame sequence, a Long-term Temporal Difference Module (L-TDM) is proposed, where the differences between forward and backward segments are incorporated and activated to guide the modulation of the temporal feature, leading to a holistic global compensation. Moreover, we further propose a Difference Compensation Unit (DCU) to enrich the interaction between the spatial distribution of the target frame and temporal compensated results, which helps maintain spatial consistency while refining the features to avoid misalignment. Rigorous objective and subjective evaluations conducted across five mainstream video satellites demonstrate that our method performs favorably against state-of-the-art approaches. Code will be available at https://github.com/XY-boy/LGTD","authors":["Yi Xiao","Qiangqiang Yuan","Kui Jiang","Xianyu Jin","Jiang He","Liangpei Zhang","Chia-Wen Lin"],"url":"https://arxiv.org/abs/2304.04421"}
{"created":"2025-04-29","title":"Mining the Characteristics of Jupyter Notebooks in Data Science Projects","abstract":"Nowadays, numerous industries have exceptional demand for skills in data science, such as data analysis, data mining, and machine learning. The computational notebook (e.g., Jupyter Notebook) is a well-known data science tool adopted in practice. Kaggle and GitHub are two platforms where data science communities are used for knowledge-sharing, skill-practicing, and collaboration. While tutorials and guidelines for novice data science are available on both platforms, there is a low number of Jupyter Notebooks that received high numbers of votes from the community. The high-voted notebook is considered well-documented, easy to understand, and applies the best data science and software engineering practices. In this research, we aim to understand the characteristics of high-voted Jupyter Notebooks on Kaggle and the popular Jupyter Notebooks for data science projects on GitHub. We plan to mine and analyse the Jupyter Notebooks on both platforms. We will perform exploratory analytics, data visualization, and feature importances to understand the overall structure of these notebooks and to identify common patterns and best-practice features separating the low-voted and high-voted notebooks. Upon the completion of this research, the discovered insights can be applied as training guidelines for aspiring data scientists and machine learning practitioners looking to improve their performance from novice ranking Jupyter Notebook on Kaggle to a deployable project on GitHub.","authors":["Morakot Choetkiertikul","Apirak Hoonlor","Chaiyong Ragkhitwetsagul","Siripen Pongpaichet","Thanwadee Sunetnanta","Tasha Settewong","Vacharavich Jiravatvanich","Urisayar Kaewpichai","Raula Gaikovina Kula"],"url":"https://arxiv.org/abs/2304.05325"}
{"created":"2025-04-29","title":"Generative Meta-Learning for Zero-Shot Relation Triplet Extraction","abstract":"Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation triplets from texts containing unseen relation types. This capability benefits various downstream information retrieval (IR) tasks. The primary challenge lies in enabling models to generalize effectively to unseen relation categories. Existing approaches typically leverage the knowledge embedded in pre-trained language models to accomplish the generalization process. However, these methods focus solely on fitting the training data during training, without specifically improving the model's generalization performance, resulting in limited generalization capability. For this reason, we explore the integration of bi-level optimization (BLO) with pre-trained language models for learning generalized knowledge directly from the training data, and propose a generative meta-learning framework which exploits the `learning-to-learn' ability of meta-learning to boost the generalization capability of generative models.","authors":["Wanli Li","Tieyun Qian","Yi Song","Zeyu Zhang","Jiawei Li","Zhuang Chen","Lixin Zou"],"url":"https://arxiv.org/abs/2305.01920"}
{"created":"2025-04-29","title":"Representing states in iterated belief revision","abstract":"Iterated belief revision requires information about the current beliefs. This information is represented by mathematical structures called doxastic states. Most literature concentrates on how to revise a doxastic state and neglects that it may exponentially grow. This problem is studied for the most common ways of storing a doxastic state. All four methods are able to store every doxastic state, but some do it in less space than others. In particular, the explicit representation (an enumeration of the current beliefs) is the more wasteful on space. The level representation (a sequence of propositional formulae) and the natural representation (a history of natural revisions) are more compact than it. The lexicographic representation (a history of lexicographic revision) is even more compact than them.","authors":["Paolo Liberatore"],"url":"https://arxiv.org/abs/2305.09200"}
{"created":"2025-04-29","title":"FEDORA: Flying Event Dataset fOr Reactive behAvior","abstract":"The ability of resource-constrained biological systems such as fruitflies to perform complex and high-speed maneuvers in cluttered environments has been one of the prime sources of inspiration for developing vision-based autonomous systems. To emulate this capability, the perception pipeline of such systems must integrate information cues from tasks including optical flow and depth estimation, object detection and tracking, and segmentation, among others. However, the conventional approach of employing slow, synchronous inputs from standard frame-based cameras constrains these perception capabilities, particularly during high-speed maneuvers. Recently, event-based sensors have emerged as low latency and low energy alternatives to standard frame-based cameras for capturing high-speed motion, effectively speeding up perception and hence navigation. For coherence, all the perception tasks must be trained on the same input data. However, present-day datasets are curated mainly for a single or a handful of tasks and are limited in the rate of the provided ground truths. To address these limitations, we present Flying Event Dataset fOr Reactive behAviour (FEDORA) - a fully synthetic dataset for perception tasks, with raw data from frame-based cameras, event-based cameras, and Inertial Measurement Units (IMU), along with ground truths for depth, pose, and optical flow at a rate much higher than existing datasets.","authors":["Amogh Joshi","Adarsh Kosta","Wachirawit Ponghiran","Manish Nagaraj","Kaushik Roy"],"url":"https://arxiv.org/abs/2305.14392"}
{"created":"2025-04-29","title":"Benchmarking large language models for biomedical natural language processing applications and recommendations","abstract":"The rapid growth of biomedical literature poses challenges for manual knowledge curation and synthesis. Biomedical Natural Language Processing (BioNLP) automates the process. While Large Language Models (LLMs) have shown promise in general domains, their effectiveness in BioNLP tasks remains unclear due to limited benchmarks and practical guidelines.","authors":["Qingyu Chen","Yan Hu","Xueqing Peng","Qianqian Xie","Qiao Jin","Aidan Gilson","Maxwell B. Singer","Xuguang Ai","Po-Ting Lai","Zhizheng Wang","Vipina Kuttichi Keloth","Kalpana Raja","Jiming Huang","Huan He","Fongci Lin","Jingcheng Du","Rui Zhang","W. Jim Zheng","Ron A. Adelman","Zhiyong Lu","Hua Xu"],"url":"https://arxiv.org/abs/2305.16326"}
{"created":"2025-04-29","title":"Long-Tailed Continual Learning For Visual Food Recognition","abstract":"Deep learning-based food recognition has made significant progress in predicting food types from eating occasion images. However, two key challenges hinder real-world deployment: (1) continuously learning new food classes without forgetting previously learned ones, and (2) handling the long-tailed distribution of food images, where a few common classes and many more rare classes. To address these, food recognition methods should focus on long-tailed continual learning. In this work, We introduce a dataset that encompasses 186 American foods along with comprehensive annotations. We also introduce three new benchmark datasets, VFN186-LT, VFN186-INSULIN and VFN186-T2D, which reflect real-world food consumption for healthy populations, insulin takers and individuals with type 2 diabetes without taking insulin. We propose a novel end-to-end framework that improves the generalization ability for instance-rare food classes using a knowledge distillation-based predictor to avoid misalignment of representation during continual learning. Additionally, we introduce an augmentation technique by integrating class-activation-map (CAM) and CutMix to improve generalization on instance-rare food classes. Our method, evaluated on Food101-LT, VFN-LT, VFN186-LT, VFN186-INSULIN, and VFN186-T2DM, shows significant improvements over existing methods. An ablation study highlights further performance enhancements, demonstrating its potential for real-world food recognition applications.","authors":["Jiangpeng He","Xiaoyan Zhang","Luotao Lin","Jack Ma","Heather A. Eicher-Miller","Fengqing Zhu"],"url":"https://arxiv.org/abs/2307.00183"}
{"created":"2025-04-29","title":"An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration","abstract":"In the field of computer vision, fine-tuning pre-trained models has become a prevalent strategy for out-of-distribution (OOD) generalization tasks. Different from most prior work that has focused on advancing learning algorithms, we systematically examined how pre-trained model size, pre-training dataset size, and training strategies impact generalization and confidence calibration on downstream tasks. We evaluated 100 models across diverse pre-trained model sizes, five pre-training datasets, and five data augmentations through extensive experiments on four distribution shift datasets totaling over 120,000 GPU hours. Our results demonstrate the significant impact of pre-trained model selection, with optimal choices substantially improving OOD accuracy over algorithm improvement alone. Additionally, we find that larger models and bigger pre-training datasets not only enhance OOD performance but also improve calibration, helping to mitigate overconfidence, contrary to some prior studies that found modern deep networks to calibrate worse than classical shallow models. Our work underscores the overlooked importance of pre-trained model selection for out-of-distribution generalization and calibration.","authors":["Hiroki Naganuma","Ryuichiro Hataya","Kotaro Yoshida","Ioannis Mitliagkas"],"url":"https://arxiv.org/abs/2307.08187"}
{"created":"2025-04-29","title":"Early Prediction of Alzheimers Disease Leveraging Symptom Occurrences from Longitudinal Electronic Health Records of US Military Veterans","abstract":"Early prediction of Alzheimer's disease (AD) is crucial for timely intervention and treatment. This study aims to use machine learning approaches to analyze longitudinal electronic health records (EHRs) of patients with AD and identify signs and symptoms that can predict AD onset earlier. We used a case-control design with longitudinal EHRs from the U.S. Department of Veterans Affairs Veterans Health Administration (VHA) from 2004 to 2021. Cases were VHA patients with AD diagnosed after 1/1/2016 based on ICD-10-CM codes, matched 1:9 with controls by age, sex and clinical utilization with replacement. We used a panel of AD-related keywords and their occurrences over time in a patient's longitudinal EHRs as predictors for AD prediction with four machine learning models. We performed subgroup analyses by age, sex, and race/ethnicity, and validated the model in a hold-out and \"unseen\" VHA stations group. Model discrimination, calibration, and other relevant metrics were reported for predictions up to ten years before ICD-based diagnosis. The study population included 16,701 cases and 39,097 matched controls. The average number of AD-related keywords (e.g., \"concentration\", \"speaking\") per year increased rapidly for cases as diagnosis approached, from around 10 to over 40, while remaining flat at 10 for controls. The best model achieved high discriminative accuracy (ROCAUC 0.997) for predictions using data from at least ten years before ICD-based diagnoses. The model was well-calibrated (Hosmer-Lemeshow goodness-of-fit p-value = 0.99) and consistent across subgroups of age, sex and race/ethnicity, except for patients younger than 65 (ROCAUC 0.746). Machine learning models using AD-related keywords identified from EHR notes can predict future AD diagnoses, suggesting its potential use for identifying AD risk using EHR notes, offering an affordable way for early screening on large population.","authors":["Rumeng Li","Xun Wang","Dan Berlowitz","Brian Silver","Wen Hu","Heather Keating","Raelene Goodwin","Weisong Liu","Honghuang Lin","Hong Yu"],"url":"https://arxiv.org/abs/2307.12369"}
{"created":"2025-04-29","title":"OccluTrack: Rethinking Awareness of Occlusion for Enhancing Multiple Pedestrian Tracking","abstract":"Multiple pedestrian tracking is crucial for enhancing safety and efficiency in intelligent transport and autonomous driving systems by predicting movements and enabling adaptive decision-making in dynamic environments. It optimizes traffic flow, facilitates human interaction, and ensures compliance with regulations. However, it faces the challenge of tracking pedestrians in the presence of occlusion. Existing methods overlook effects caused by abnormal detections during partial occlusion. Subsequently, these abnormal detections can lead to inaccurate motion estimation, unreliable appearance features, and unfair association. To address these issues, we propose an adaptive occlusion-aware multiple pedestrian tracker, OccluTrack, to mitigate the effects caused by partial occlusion. Specifically, we first introduce a plug-and-play abnormal motion suppression mechanism into the Kalman Filter to adaptively detect and suppress outlier motions caused by partial occlusion. Second, we develop a pose-guided re-identification (Re-ID) module to extract discriminative part features for partially occluded pedestrians. Last, we develop a new occlusion-aware association method towards fair Intersection over Union (IoU) and appearance embedding distance measurement for occluded pedestrians. Extensive evaluation results demonstrate that our method outperforms state-of-the-art methods on MOTChallenge and DanceTrack datasets. Particularly, the performance improvements on IDF1 and ID Switches, as well as visualized results, demonstrate the effectiveness of our method in multiple pedestrian tracking.","authors":["Jianjun Gao","Yi Wang","Kim-Hui Yap","Kratika Garg","Boon Siew Han"],"url":"https://arxiv.org/abs/2309.10360"}
{"created":"2025-04-29","title":"Ragas: Automated Evaluation of Retrieval Augmented Generation","abstract":"We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \\textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.","authors":["Shahul Es","Jithin James","Luis Espinosa-Anke","Steven Schockaert"],"url":"https://arxiv.org/abs/2309.15217"}
{"created":"2025-04-29","title":"Fast algorithm for centralized multi-agent maze exploration","abstract":"Recent advances in robotics have paved the way for robots to replace humans in perilous situations, such as searching for victims in burning buildings, in earthquake-damaged structures, in uncharted caves, traversing minefields or patrolling crime-ridden streets. These challenges can be generalized as problems where agents have to explore unknown mazes. We propose a cooperative multi-agent system of automated mobile agents for exploring unknown mazes and localizing stationary targets. The Heat Equation-Driven Area Coverage (HEDAC) algorithm for maze exploration employs a potential field to guide the exploration of the maze and integrates cooperative behaviors of the agents such as collision avoidance, coverage coordination, and path planning. In contrast to previous applications for continuous static domains, we adapt the HEDAC method for mazes on expanding rectilinear grids. The proposed algorithm guarantees the exploration of the entire maze and can ensure the avoidance of collisions and deadlocks. Moreover, this is the first application of the HEDAC algorithm to domains that expand over time. To cope with the dynamically changing domain, succesive over-relaxation (SOR) iterative linear solver has been adapted and implemented, which significantly reduced the computational complexity of the presented algorithm when compared to standard direct and iterative linear solvers. The results highlight significant improvements and show the applicability of the algorithm in different mazes. They confirm its robustness, adaptability, scalability and simplicity, which enables centralized parallel computation to control multiple agents/robots in the maze.","authors":["Bojan Crnkovi\\'c","Stefan Ivi\\'c","Mila Zovko"],"url":"https://arxiv.org/abs/2310.02121"}
{"created":"2025-04-29","title":"Beyond Worst-Case Online Allocation via Dynamic Max-min Fairness","abstract":"We study the allocation of shared resources over multiple rounds among competing agents, via the dynamic max-min fair (DMMF) mechanism: the good in each round is allocated to the requesting agent with the least number of allocations received to date. We show that in large markets when an agent has i.i.d. values across rounds, under mild distributional assumptions (e.g., bounded PDF function), the DMMF mechanism allows each agent to realize a $1 - o(1)$ fraction of her ideal utility -- her highest achievable utility given her nominal share of resources. This guarantee holds under arbitrary behavior by other agents and is achieved by characterizing the agent's utility under a rich space of strategies, wherein an agent can tune how aggressive to be in requesting the item. Our techniques also allow us to handle settings where an agent's values are correlated across rounds, thereby allowing an adversary to predict and block her future values. By tuning the aggressiveness, an agent can guarantee $\\Omega(\\gamma)$ fraction of her ideal utility, where $\\gamma\\in [0, 1]$ is a parameter that quantifies dependence across rounds (with $\\gamma = 1$ indicating full independence and lower values indicating more correlation). Finally, we extend our efficiency results to the case of reusable resources, where an agent might need to hold the item over multiple rounds to receive utility. Our results subsume previous guarantees obtained using a more complicated mechanism proving a half ideal utility guarantee under i.i.d. values sampled from worst-case distributions.","authors":["Giannis Fikioris","Siddhartha Banerjee","\\'Eva Tardos"],"url":"https://arxiv.org/abs/2310.08881"}
{"created":"2025-04-29","title":"Shape-centered Representation Learning for Visible-Infrared Person Re-identification","abstract":"Visible-Infrared Person Re-Identification (VI-ReID) plays a critical role in all-day surveillance systems. However, existing methods primarily focus on learning appearance features while overlooking body shape features, which not only complement appearance features but also exhibit inherent robustness to modality variations. Despite their potential, effectively integrating shape and appearance features remains challenging. Appearance features are highly susceptible to modality variations and background noise, while shape features often suffer from inaccurate infrared shape estimation due to the limitations of auxiliary models. To address these challenges, we propose the Shape-centered Representation Learning (ScRL) framework, which enhances VI-ReID performance by innovatively integrating shape and appearance features. Specifically, we introduce Infrared Shape Restoration (ISR) to restore inaccuracies in infrared body shape representations at the feature level by leveraging infrared appearance features. In addition, we propose Shape Feature Propagation (SFP), which enables the direct extraction of shape features from original images during inference with minimal computational complexity. Furthermore, we design Appearance Feature Enhancement (AFE), which utilizes shape features to emphasize shape-related appearance features while effectively suppressing identity-unrelated noise. Benefiting from the effective integration of shape and appearance features, ScRL demonstrates superior performance through extensive experiments. On the SYSU-MM01, HITSZ-VCM, and RegDB datasets, it achieves Rank-1 (mAP) accuracies of 76.1% (72.6%), 71.2% (52.9%), and 92.4% (86.7%), respectively, surpassing existing state-of-the-art methods.","authors":["Shuang Li","Jiaxu Leng","Ji Gan","Mengjingcheng Mo","Xinbo Gao"],"url":"https://arxiv.org/abs/2310.17952"}
{"created":"2025-04-29","title":"MENTOR: Human Perception-Guided Pretraining for Increased Generalization","abstract":"Leveraging human perception into training of convolutional neural networks (CNN) has boosted generalization capabilities of such models in open-set recognition tasks. One of the active research questions is where (in the model architecture or training pipeline) and how to efficiently incorporate always limited human perceptual data into training strategies of models. In this paper, we introduce MENTOR (huMan pErceptioN-guided preTraining fOr increased geneRalization), which addresses this question through two unique rounds of training CNNs tasked with open-set anomaly detection. First, we train an autoencoder to learn human saliency maps given an input image, without any class labels. The autoencoder is thus tasked with discovering domain-specific salient features which mimic human perception. Second, we remove the decoder part, add a classification layer on top of the encoder, and train this new model conventionally, now using class labels. We show that MENTOR successfully raises the generalization performance across three different CNN backbones in a variety of anomaly detection tasks (demonstrated for detection of unknown iris presentation attacks, synthetically-generated faces, and anomalies in chest X-ray images) compared to traditional pretraining methods (e.g., sourcing the weights from ImageNet), and as well as state-of-the-art methods that incorporate human perception guidance into training. In addition, we demonstrate that MENTOR can be flexibly applied to existing human perception-guided methods and subsequently increasing their generalization with no architectural modifications.","authors":["Colton R. Crum","Adam Czajka"],"url":"https://arxiv.org/abs/2310.19545"}
{"created":"2025-04-29","title":"Infusion: internal diffusion for inpainting of dynamic textures and complex motion","abstract":"Video inpainting is the task of filling a region in a video in a visually convincing manner. It is very challenging due to the high dimensionality of the data and the temporal consistency required for obtaining convincing results. Recently, diffusion models have shown impressive results in modeling complex data distributions, including images and videos. Such models remain nonetheless very expensive to train and to perform inference with, which strongly reduce their applicability to videos, and yields unreasonable computational loads. We show that in the case of video inpainting, thanks to the highly auto-similar nature of videos, the training data of a diffusion model can be restricted to the input video and still produce very satisfying results. With this internal learning approach, where the training data is limited to a single video, our lightweight models perform very well with only half a million parameters, in contrast to the very large networks with billions of parameters typically found in the literature. We also introduce a new method for efficient training and inference of diffusion models in the context of internal learning, by splitting the diffusion process into different learning intervals corresponding to different noise levels of the diffusion process. We show qualitative and quantitative results, demonstrating that our method reaches or exceeds state of the art performance in the case of dynamic textures and complex dynamic backgrounds","authors":["Nicolas Cherel","Andr\\'es Almansa","Yann Gousseau","Alasdair Newson"],"url":"https://arxiv.org/abs/2311.01090"}
{"created":"2025-04-29","title":"Generative AI has lowered the barriers to computational social sciences","abstract":"Generative artificial intelligence (AI) has revolutionized the field of computational social science (CSS), unleashing new possibilities for collecting and analyzing multimodal data, especially for scholars who may not have extensive programming expertise. This breakthrough carries profound implications for social scientists. First, generative AI can significantly enhance the productivity of social scientists by automating the generation, annotation, and debugging of code. Second, it empowers researchers to delve into sophisticated data analysis through the innovative use of prompt engineering. Last, the educational sphere of CSS stands to benefit immensely from these tools, given their exceptional ability to annotate and elucidate complex codes for learners, thereby simplifying the learning process and making the technology more accessible.","authors":["Yongjun Zhang"],"url":"https://arxiv.org/abs/2311.10833"}
{"created":"2025-04-29","title":"Breaking XOR Arbiter PUFs with Chosen Challenge Attack","abstract":"The XOR Arbiter PUF was introduced as a strong PUF in 2007 and was broken in 2015 by a Machine Learning (ML) attack, which allows the underlying Arbiter PUFs to be modeled individually by exploiting reliability information of the measured responses. To mitigate the reliability-based attacks, state-of-the-art understanding shows that the reliability of individual Arbiter PUFs and the overall XOR Arbiter PUF can be boosted to an arbitrarily high level, thus rendering all known reliability-based ML attacks infeasible; alternatively, an access control interface around the XOR Arbiter PUF can prevent the same challenge-response pairs from being accessed repeatedly, thus eliminating the leakage of reliability information.","authors":["Niloufar Sayadi","Phuong Ha Nguyen","Marten van Dijk","Chenglu Jin"],"url":"https://arxiv.org/abs/2312.01256"}
{"created":"2025-04-29","title":"Restivo Salemi property for $\\alpha$-power free languages with $\\alpha\\geq 5$ and $k\\geq 3$ letters","abstract":"In 2009, Shur published the following conjecture: Let $L$ be a power-free language and let $e(L)\\subseteq L$ be the set of words of $L$ that can be extended to a bi-infinite word respecting the given power-freeness. If $u, v \\in e(L)$ then $uwv \\in e(L)$ for some word $w$. Let $L_{k,\\alpha}$ denote an $\\alpha$-power free language over an alphabet with $k$ letters, where $\\alpha$ is a positive rational number and $k$ is positive integer. We prove the conjecture for the languages $L_{k,\\alpha}$, where $\\alpha\\geq 5$ and $k\\geq 3$.","authors":["Josef Rukavicka"],"url":"https://arxiv.org/abs/2312.10061"}
{"created":"2025-04-29","title":"FetaFix: Automatic Fault Localization and Repair of Deep Learning Model Conversions","abstract":"Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.","authors":["Nikolaos Louloudakis","Perry Gibson","Jos\\'e Cano","Ajitha Rajan"],"url":"https://arxiv.org/abs/2312.15101"}
{"created":"2025-04-29","title":"Robust Collaborative Inference with Vertically Split Data Over Dynamic Device Environments","abstract":"When each edge device of a network only perceives a local part of the environment, collaborative inference across multiple devices is often needed to predict global properties of the environment. In safety-critical applications, collaborative inference must be robust to significant network failures caused by environmental disruptions or extreme weather. Existing collaborative learning approaches, such as privacy-focused Vertical Federated Learning (VFL), typically assume a centralized setup or that one device never fails. However, these assumptions make prior approaches susceptible to significant network failures. To address this problem, we first formalize the problem of robust collaborative inference over a dynamic network of devices that could experience significant network faults. Then, we develop a minimalistic yet impactful method called Multiple Aggregation with Gossip Rounds and Simulated Faults (MAGS) that synthesizes simulated faults via dropout, replication, and gossiping to significantly improve robustness over baselines. We also theoretically analyze our proposed approach to explain why each component enhances robustness. Extensive empirical results validate that MAGS is robust across a range of fault rates-including extreme fault rates.","authors":["Surojit Ganguli","Zeyu Zhou","Christopher G. Brinton","David I. Inouye"],"url":"https://arxiv.org/abs/2312.16638"}
{"created":"2025-04-29","title":"Optimization and Identification of Lattice Quantizers","abstract":"Lattices with minimal normalized second moments are designed using a new numerical optimization algorithm. Starting from a random lower-triangular generator matrix and applying stochastic gradient descent, all elements are updated towards the negative gradient, which makes it the most efficient algorithm proposed so far for this purpose. A graphical illustration of the theta series, called theta image, is introduced and shown to be a powerful tool for converting numerical lattice representations into their underlying exact forms. As a proof of concept, optimized lattices are designed in dimensions up to 16. In all dimensions, the algorithm converges to either the previously best known lattice or a better one. The dual of the 15-dimensional laminated lattice is conjectured to be optimal in its dimension and its exact normalized second moment is computed.","authors":["Erik Agrell","Daniel Pook-Kolb","Bruce Allen"],"url":"https://arxiv.org/abs/2401.01799"}
{"created":"2025-04-29","title":"End-To-End Planning of Autonomous Driving in Industry and Academia: 2022-2023","abstract":"This paper aims to provide a quick review of the methods including the technologies in detail that are currently reported in industry and academia. Specifically, this paper reviews the end-to-end planning, including Tesla FSD V12, Momenta 2023, Horizon Robotics 2023, Motional RoboTaxi 2022, Woven Planet (Toyota): Urban Driver, and Nvidia. In addition, we review the state-of-the-art academic studies that investigate end-to-end planning of autonomous driving. This paper provides readers with a concise structure and fast learning of state-of-the-art end-to-end planning for 2022-2023. This article provides a meaningful overview as introductory material for beginners to follow the state-of-the-art end-to-end planning of autonomous driving in industry and academia, as well as supplementary material for advanced researchers.","authors":["Gongjin Lan","Qi Hao"],"url":"https://arxiv.org/abs/2401.08658"}
{"created":"2025-04-29","title":"Distributed Multi-Task Learning for Stochastic Bandits with Context Distribution and Stage-wise Constraints","abstract":"We present conservative distributed multi-task learning in stochastic linear contextual bandits with heterogeneous agents. This extends conservative linear bandits to a distributed setting where M agents tackle different but related tasks while adhering to stage-wise performance constraints. The exact context is unknown, and only a context distribution is available to the agents as in many practical applications that involve a prediction mechanism to infer context, such as stock market prediction and weather forecast. We propose a distributed upper confidence bound (UCB) algorithm, DiSC-UCB. Our algorithm constructs a pruned action set during each round to ensure the constraints are met. Additionally, it includes synchronized sharing of estimates among agents via a central server using well-structured synchronization steps. We prove the regret and communication bounds on the algorithm. We extend the problem to a setting where the agents are unaware of the baseline reward. For this setting, we provide a modified algorithm, DiSC-UCB2, and we show that the modified algorithm achieves the same regret and communication bounds. We empirically validated the performance of our algorithm on synthetic data and real-world Movielens-100K data.","authors":["Jiabin Lin","Shana Moothedath"],"url":"https://arxiv.org/abs/2401.11563"}
{"created":"2025-04-29","title":"Rate-Distortion-Perception Tradeoff Based on the Conditional-Distribution Perception Measure","abstract":"This paper studies the rate-distortion-perception (RDP) tradeoff for a memoryless source model in the asymptotic limit of large block-lengths. The perception measure is based on a divergence between the distributions of the source and reconstruction sequences \\emph{conditioned} on the encoder output, first proposed by Mentzer et al. We consider the case when there is no shared randomness between the encoder and the decoder and derive a single-letter characterization of the RDP function for the case of discrete memoryless sources. This is in contrast to the marginal-distribution metric case (introduced by Blau and Michaeli), whose RDP characterization remains open when there is no shared randomness. The achievability scheme is based on lossy source coding with a posterior reference map. For the case of continuous valued sources under the squared error distortion measure and the squared quadratic Wasserstein perception measure, we also derive a single-letter characterization and show that the decoder can be restricted to a noise-adding mechanism. Interestingly, the RDP function characterized for the case of zero perception loss coincides with that of the marginal metric, and further zero perception loss can be achieved with a 3-dB penalty in minimum distortion. Finally we specialize to the case of Gaussian sources, and derive the RDP function for Gaussian vector case and propose a reverse water-filling type solution. We also partially characterize the RDP function for a mixture of Gaussian vector sources.","authors":["Sadaf Salehkalaibar","Jun Chen","Ashish Khisti","Wei Yu"],"url":"https://arxiv.org/abs/2401.12207"}
{"created":"2025-04-29","title":"Semantic-Syntactic Discrepancy in Images (SSDI): Learning Meaning and Order of Features from Natural Images","abstract":"Despite considerable progress in image classification tasks, classification models seem unaffected by the images that significantly deviate from those that appear natural to human eyes. Specifically, while human perception can easily identify abnormal appearances or compositions in images, classification models overlook any alterations in the arrangement of object parts as long as they are present in any order, even if unnatural. Hence, this work exposes the vulnerability of having semantic and syntactic discrepancy in images (SSDI) in the form of corruptions that remove or shuffle image patches or present images in the form of puzzles. To address this vulnerability, we propose the concept of \"image grammar\", comprising \"image semantics\" and \"image syntax\". Image semantics pertains to the interpretation of parts or patches within an image, whereas image syntax refers to the arrangement of these parts to form a coherent object. We present a semi-supervised two-stage method for learning the image grammar of visual elements and environments solely from natural images. While the first stage learns the semantic meaning of individual object parts, the second stage learns how their relative arrangement constitutes an entire object. The efficacy of the proposed approach is then demonstrated by achieving SSDI detection rates ranging from 70% to 90% on corruptions generated from CelebA and SUN-RGBD datasets. Code is publicly available at: https://github.com/ChunTao1999/SSDI/","authors":["Chun Tao","Timur Ibrayev","Kaushik Roy"],"url":"https://arxiv.org/abs/2401.17515"}
{"created":"2025-04-29","title":"SIR: Multi-view Inverse Rendering with Decomposable Shadow Under Indoor Intense Lighting","abstract":"We propose SIR, an efficient method to decompose differentiable shadows for inverse rendering on indoor scenes using multi-view data, addressing the challenges in accurately decomposing the materials and lighting conditions. Unlike previous methods that struggle with shadow fidelity in complex lighting environments, our approach explicitly learns shadows for enhanced realism in material estimation under unknown light positions. Utilizing posed HDR images as input, SIR employs an SDF-based neural radiance field for comprehensive scene representation. Then, SIR integrates a shadow term with a three-stage material estimation approach to improve SVBRDF quality. Specifically, SIR is designed to learn a differentiable shadow, complemented by BRDF regularization, to optimize inverse rendering accuracy. Extensive experiments on both synthetic and real-world indoor scenes demonstrate the superior performance of SIR over existing methods in both quantitative metrics and qualitative analysis. The significant decomposing ability of SIR enables sophisticated editing capabilities like free-view relighting, object insertion, and material replacement. The code and data are available at https://xiaokangwei.github.io/SIR/.","authors":["Xiaokang Wei","Zhuoman Liu","Ping Li","Yan Luximon"],"url":"https://arxiv.org/abs/2402.06136"}
{"created":"2025-04-29","title":"Predictive Churn with the Set of Good Models","abstract":"Issues can arise when research focused on fairness, transparency, or safety is conducted separately from research driven by practical deployment concerns and vice versa. This separation creates a growing need for translational work that bridges the gap between independently studied concepts that may be fundamentally related. This paper explores connections between two seemingly unrelated concepts of predictive inconsistency that share intriguing parallels. The first, known as predictive multiplicity, occurs when models that perform similarly (e.g., nearly equivalent training loss) produce conflicting predictions for individual samples. This concept is often emphasized in algorithmic fairness research as a means of promoting transparency in ML model development. The second concept, predictive churn, examines the differences in individual predictions before and after model updates, a key challenge in deploying ML models in consumer-facing applications. We present theoretical and empirical results that uncover links between these previously disconnected concepts.","authors":["Jamelle Watson-Daniels","Flavio du Pin Calmon","Alexander D'Amour","Carol Long","David C. Parkes","Berk Ustun"],"url":"https://arxiv.org/abs/2402.07745"}
{"created":"2025-04-29","title":"Algorithmic Fairness and Color-blind Racism: Navigating the Intersection","abstract":"Our focus lies at the intersection between two broader research perspectives: (1) the scientific study of algorithms and (2) the scholarship on race and racism. Many streams of research related to algorithmic fairness have been born out of interest at this intersection. We think about this intersection as the product of work derived from both sides. From (1) algorithms to (2) racism, the starting place might be an algorithmic question or method connected to a conceptualization of racism. On the other hand, from (2) racism to (1) algorithms, the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting. In either direction, meaningful disconnection can occur when conducting research at the intersection of racism and algorithms. The present paper urges collective reflection on research directions at this intersection. Despite being primarily motivated by instances of racial bias, research in algorithmic fairness remains mostly disconnected from scholarship on racism. In particular, there has not been an examination connecting algorithmic fairness discussions directly to the ideology of color-blind racism; we aim to fill this gap. We begin with a review of an essential account of color-blind racism then we review racial discourse within algorithmic fairness research and underline significant patterns, shifts and disconnects. Ultimately, we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively re-orienting towards maintaining meaningful connections across interdisciplinary lines.","authors":["Jamelle Watson-Daniels"],"url":"https://arxiv.org/abs/2402.07778"}
{"created":"2025-04-29","title":"The dynamic interplay between in-context and in-weight learning in humans and neural networks","abstract":"Human learning embodies a striking duality: sometimes, we appear capable of following logical, compositional rules and benefit from structured curricula (e.g., in formal education), while other times, we rely on an incremental approach or trial-and-error, learning better from curricula that are randomly interleaved. Influential psychological theories explain this seemingly disparate behavioral evidence by positing two qualitatively different learning systems -- one for rapid, rule-based inferences and another for slow, incremental adaptation. It remains unclear how to reconcile such theories with neural networks, which learn via incremental weight updates and are thus a natural model for the latter type of learning, but are not obviously compatible with the former. However, recent evidence suggests that metalearning neural networks and large language models are capable of \"in-context learning\" (ICL) -- the ability to flexibly grasp the structure of a new task from a few examples. Here, we show that the dynamic interplay between ICL and default in-weight learning (IWL) naturally captures a broad range of learning phenomena observed in humans, reproducing curriculum effects on category-learning and compositional tasks, and recapitulating a tradeoff between flexibility and retention. Our work shows how emergent ICL can equip neural networks with fundamentally different learning properties that can coexist with their native IWL, thus offering a novel perspective on dual-process theories and human cognitive flexibility.","authors":["Jacob Russin","Ellie Pavlick","Michael J. Frank"],"url":"https://arxiv.org/abs/2402.08674"}
{"created":"2025-04-29","title":"Spectral Independence Beyond Uniqueness with. the topological method -- An extended view","abstract":"We present novel results for fast mixing of Glauber dynamics using the newly introduced and powerful Spectral Independence method from [Anari, Liu, Oveis-Gharan: FOCS 2020]. We mainly focus on the Hard-core model and the Ising model. We obtain bounds for fast mixing with the parameters expressed in terms of the spectral radius of the adjacency matrix, improving on the seminal work in [Hayes: FOCS 2006]. Furthermore, we go beyond the adjacency matrix and establish -- for the first time -- rapid mixing results for Glauber dynamics expressed in terms of the spectral radius of the Hashimoto non-backtracking matrix of the underlying graph $G$. Working with the non-backtracking spectrum is extremely challenging, but also more desirable. Its eigenvalues are less correlated with the high-degree vertices than those of the adjacency matrix and express more accurately invariants of the graph such as the growth rate. Our results require ``weak normality\" from the Hashimoto matrix. This condition is mild and allows us to obtain very interesting bound. We study the pairwise influence matrix ${I}^{\\Lambda,\\tau}_{G}$ by exploiting the connection between the matrix and the trees of self-avoiding walks, however, we go beyond the standard treatment of the distributional recursions. The common framework that underlies our techniques we call the topological method. Our approach is novel and gives new insights into how to establish Spectral Independence for Gibbs distributions. More importantly, it allows us to derive new -- improved -- rapid mixing bounds for Glauber dynamics on distributions such as the Hard-core model and the Ising model for graphs that the spectral radius is smaller than the maximum degree.","authors":["Charilaos Efthymiou"],"url":"https://arxiv.org/abs/2402.11647"}
{"created":"2025-04-29","title":"Can we forget how we learned? Doxastic redundancy in iterated belief revision","abstract":"Forgetting a belief acquisition episode may not cause information loss because of the others. Checking whether it does is not obvious, as the contribution of each belief revision is not isolated from the others, and the same information may be given not directly but by deduction. An algorithm for checking whether forgetting reduces information is given for a number of iterated belief revision operators: lexicographic, natural, severe, plain severe, moderate severe, restrained, very radical and full meet revisions. It may take exponential time in the worst case, which is expected given that the problem is coNP-hard, even in the Horn restriction. It is in coNP for homogeneous sequences of lexicographic revisions.","authors":["Paolo Liberatore"],"url":"https://arxiv.org/abs/2402.15445"}
{"created":"2025-04-29","title":"WKB-based third order method for the highly oscillatory 1D stationary Schr\\\"odinger equation","abstract":"This paper introduces an efficient high-order numerical method for solving the 1D stationary Schr\\\"odinger equation in the highly oscillatory regime. Building upon the ideas from [Arnold, Ben Abdallah, Negulescu, SIAM J. Numer. Anal., 2011], we first analytically transform the given equation into a smoother (i.e. less oscillatory) equation. By developing sufficiently accurate quadratures for several (iterated) oscillatory integrals occurring in the Picard approximation of the solution, we obtain a one-step method that is third order w.r.t. the step size. The accuracy and efficiency of the method are illustrated through several numerical examples.","authors":["Anton Arnold","Jannis K\\\"orner"],"url":"https://arxiv.org/abs/2402.18406"}
{"created":"2025-04-29","title":"Explicit Motion Handling and Interactive Prompting for Video Camouflaged Object Detection","abstract":"Camouflage poses challenges in distinguishing a static target, whereas any movement of the target can break this disguise. Existing video camouflaged object detection (VCOD) approaches take noisy motion estimation as input or model motion implicitly, restricting detection performance in complex dynamic scenes. In this paper, we propose a novel Explicit Motion handling and Interactive Prompting framework for VCOD, dubbed EMIP, which handles motion cues explicitly using a frozen pre-trained optical flow fundamental model. EMIP is characterized by a two-stream architecture for simultaneously conducting camouflaged segmentation and optical flow estimation. Interactions across the dual streams are realized in an interactive prompting way that is inspired by emerging visual prompt learning. Two learnable modules, i.e., the camouflaged feeder and motion collector, are designed to incorporate segmentation-to-motion and motion-to-segmentation prompts, respectively, and enhance outputs of the both streams. The prompt fed to the motion stream is learned by supervising optical flow in a self-supervised manner. Furthermore, we show that long-term historical information can also be incorporated as a prompt into EMIP and achieve more robust results with temporal consistency. Experimental results demonstrate that our EMIP achieves new state-of-the-art records on popular VCOD benchmarks. Our code is made publicly available at https://github.com/zhangxin06/EMIP.","authors":["Xin Zhang","Tao Xiao","Gepeng Ji","Xuan Wu","Keren Fu","Qijun Zhao"],"url":"https://arxiv.org/abs/2403.01968"}
{"created":"2025-04-29","title":"TokenMark: A Modality-Agnostic Watermark for Pre-trained Transformers","abstract":"Watermarking is a critical tool for model ownership verification. However, existing watermarking techniques are often designed for specific data modalities and downstream tasks, without considering the inherent architectural properties of the model. This lack of generality and robustness underscores the need for a more versatile watermarking approach. In this work, we investigate the properties of Transformer models and propose TokenMark, a modality-agnostic, robust watermarking system for pre-trained models, leveraging the permutation equivariance property. TokenMark embeds the watermark by fine-tuning the pre-trained model on a set of specifically permuted data samples, resulting in a watermarked model that contains two distinct sets of weights -- one for normal functionality and the other for watermark extraction, the latter triggered only by permuted inputs. Extensive experiments on state-of-the-art pre-trained models demonstrate that TokenMark significantly improves the robustness, efficiency, and universality of model watermarking, highlighting its potential as a unified watermarking solution.","authors":["Hengyuan Xu","Liyao Xiang","Borui Yang","Xingjun Ma","Siheng Chen","Baochun Li"],"url":"https://arxiv.org/abs/2403.05842"}
{"created":"2025-04-29","title":"Hyperparameters in Continual Learning: A Reality Check","abstract":"Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters (e.g., learning rate, mini-batch size, regularization strengths, etc.) within a given scenario and then evaluates the algorithms using these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose the Generalizable Two-phase Evaluation Protocol (GTEP) consisting of hyperparameter tuning and evaluation phases. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol. Our implementation can be found in https://github.com/csm9493/GTEP.","authors":["Sungmin Cha","Kyunghyun Cho"],"url":"https://arxiv.org/abs/2403.09066"}
{"created":"2025-04-29","title":"Real-Time Image Segmentation via Hybrid Convolutional-Transformer Architecture Search","abstract":"Image segmentation is one of the most fundamental problems in computer vision and has drawn a lot of attention due to its vast applications in image understanding and autonomous driving. However, designing effective and efficient segmentation neural architectures is a labor-intensive process that may require numerous trials by human experts. In this paper, we address the challenge of integrating multi-head self-attention into high-resolution representation CNNs efficiently by leveraging architecture search. Manually replacing convolution layers with multi-head self-attention is non-trivial due to the costly overhead in memory to maintain high resolution. By contrast, we develop a multi-target multi-branch supernet method, which not only fully utilizes the advantages of high-resolution features but also finds the proper location for placing the multi-head self-attention module. Our search algorithm is optimized towards multiple objectives (e.g., latency and mIoU) and is capable of finding architectures on the Pareto frontier with an arbitrary number of branches in a single search. We further present a series of models via the Hybrid Convolutional-Transformer Architecture Search (HyCTAS) method that searches for the best hybrid combination of light-weight convolution layers and memory-efficient self-attention layers between branches from different resolutions and fuses them to high resolution for both efficiency and effectiveness. Extensive experiments demonstrate that HyCTAS outperforms previous methods in both semantic segmentation and panoptic segmentation tasks. Code and models are available at https://github.com/MarvinYu1995/HyCTAS.","authors":["Hongyuan Yu","Cheng Wan","Xiyang Dai","Mengchen Liu","Dongdong Chen","Bin Xiao","Yan Huang","Yuan Lu","Liang Wang"],"url":"https://arxiv.org/abs/2403.10413"}
{"created":"2025-04-29","title":"Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications","abstract":"We present a new antithetic multilevel Monte Carlo (MLMC) method for the estimation of expectations with respect to laws of diffusion processes that can be elliptic or hypo-elliptic. In particular, we consider the case where one has to resort to time discretization of the diffusion and numerical simulation of such schemes. Inspired by recent works, we introduce a new MLMC estimator of expectations, which does not require any L\\'evy area simulation and has a strong error of order 2 and a weak error of order 2. We then show how this approach can be used in the context of the filtering problem associated to partially observed diffusions with discrete time observations. We illustrate that in numerical simulations our new approaches provide efficiency gains for several problems, particularly when the diffusion process is hypo-elliptic, relative to some existing methods.","authors":["Yuga Iguchi","Ajay Jasra","Mohamed Maama","Alexandros Beskos"],"url":"https://arxiv.org/abs/2403.13489"}
{"created":"2025-04-29","title":"Fine-Grained Assertion-Based Test Selection","abstract":"For large software applications, running the whole test suite after each code change is time- and resource-intensive. Regression test selection techniques aim at reducing test execution time by selecting only the tests that are affected by code changes. However, existing techniques select test entities at coarse granularity levels such as test class, which causes imprecise test selection and executing unaffected tests. We propose a novel approach that increases the selection precision by analyzing test code at statement level and treating test assertions as the unit for selection. We implement our fine-grained test selection approach in a tool called \\toolname and evaluate it by comparing against two state-of-the-art test selection techniques using 11 open-source subjects. Our results show that \\toolname increases selection precision for all the subjects. Our test selection reduces, on average, 63\\% of the overall test time, making regression testing 7--38\\% faster than the other techniques. Our results also indicate that subjects with longer test execution time benefit more by our fine-grained selection technique.","authors":["Sijia Gu","Ali Mesbah"],"url":"https://arxiv.org/abs/2403.16001"}
{"created":"2025-04-29","title":"NonlinearSolve.jl: High-Performance and Robust Solvers for Systems of Nonlinear Equations in Julia","abstract":"Efficiently solving nonlinear equations underpins numerous scientific and engineering disciplines, yet scaling these solutions for challenging system models remains a challenge. This paper presents NonlinearSolve.jl -- a suite of high-performance open-source nonlinear equation solvers implemented natively in the Julia programming language. NonlinearSolve.jl distinguishes itself by offering a unified API that accommodates a diverse range of solver specifications alongside features such as automatic algorithm selection based on runtime analysis, support for GPU-accelerated computation through static array kernels, and the utilization of sparse automatic differentiation and Jacobian-free Krylov methods for large-scale problem-solving. Through rigorous comparison with established tools such as PETSc SNES, Sundials KINSOL, and MINPACK, NonlinearSolve.jl demonstrates robustness and efficiency, achieving significant advancements in solving nonlinear equations while being implemented in a high-level programming language. The capabilities of NonlinearSolve.jl unlock new potentials in modeling and simulation across various domains, making it a valuable addition to the computational toolkit of researchers and practitioners alike.","authors":["Avik Pal","Flemming Holtorf","Axel Larsson","Torkel Loman","Utkarsh","Frank Sch\\\"aefer","Qingyu Qu","Alan Edelman","Chris Rackauckas"],"url":"https://arxiv.org/abs/2403.16341"}
{"created":"2025-04-29","title":"Solving the unique continuation problem for Schr\\\"odinger equations with low regularity solutions using a stabilized finite element method","abstract":"In this paper, we consider the unique continuation problem for the Schr\\\"odinger equations. We prove a H\\\"older type conditional stability estimate and build up a parameterized stabilized finite element scheme adaptive to the \\textit{a priori} knowledge of the solution, achieving error estimates in interior domains with convergence up to continuous stability. The approximability of the scheme to solutions with only $H^1$-regularity is studied and the convergence rate for solutions with regularity higher than $H^1$ is also shown. Comparisons in terms of different parameterization for different regularities will be illustrated with respect to the convergence and condition numbers of the linear systems. Finally, numerical experiments will be given to illustrate the theory.","authors":["Erik Burman","Mingfei Lu","Lauri Oksanen"],"url":"https://arxiv.org/abs/2403.16914"}
{"created":"2025-04-29","title":"Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation","abstract":"Prompt engineering is an effective but labor-intensive way to control text-to-image (T2I) generative models. Its time-intensive nature and complexity have spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, or produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically produces human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompt distribution built upon the reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles, and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.","authors":["Yutong He","Alexander Robey","Naoki Murata","Yiding Jiang","Joshua Nathaniel Williams","George J. Pappas","Hamed Hassani","Yuki Mitsufuji","Ruslan Salakhutdinov","J. Zico Kolter"],"url":"https://arxiv.org/abs/2403.19103"}
{"created":"2025-04-29","title":"Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models","abstract":"This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs.","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Qing Yu","Go Irie","Yixuan Li","Hai Li","Ziwei Liu","Kiyoharu Aizawa"],"url":"https://arxiv.org/abs/2403.20331"}
{"created":"2025-04-29","title":"Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking","abstract":"Contrastive learning has gained widespread adoption for retrieval tasks due to its minimal requirement for manual annotations. However, popular training frameworks typically learn from binary (positive/negative) relevance, making them ineffective at incorporating desired rankings. As a result, the poor ranking performance of these models forces systems to employ a re-ranker, which increases complexity, maintenance effort and inference time. To address this, we introduce Generalized Contrastive Learning (GCL), a training framework designed to learn from continuous ranking scores beyond binary relevance. GCL encodes both relevance and ranking information into a unified embedding space by applying ranking scores to the loss function. This enables a single-stage retrieval system. In addition, during our research, we identified a lack of public multi-modal datasets that benchmark both retrieval and ranking capabilities. To facilitate this and future research for ranked retrieval, we curated a large-scale MarqoGS-10M dataset using GPT-4 and Google Shopping, providing ranking scores for each of the 10 million query-document pairs. Our results show that GCL achieves a 29.3% increase in NDCG@10 for in-domain evaluations and 6.0% to 10.0% increases for cold-start evaluations compared to the finetuned CLIP baseline with MarqoGS-10M. Additionally, we evaluated GCL offline on a proprietary user interaction data. GCL shows an 11.2% gain for in-domain evaluations. The dataset and the method are available at: https://github.com/marqo-ai/GCL.","authors":["Tianyu Zhu","Myong Chol Jung","Jesse Clark"],"url":"https://arxiv.org/abs/2404.08535"}
{"created":"2025-04-29","title":"Variational Bayesian Optimal Experimental Design with Normalizing Flows","abstract":"Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.","authors":["Jiayuan Dong","Christian Jacobsen","Mehdi Khalloufi","Maryam Akram","Wanjiao Liu","Karthik Duraisamy","Xun Huan"],"url":"https://arxiv.org/abs/2404.13056"}
{"created":"2025-04-29","title":"LaneCorrect: Self-supervised Lane Detection","abstract":"Lane detection has evolved highly functional autonomous driving system to understand driving scenes even under complex environments. In this paper, we work towards developing a generalized computer vision system able to detect lanes without using any annotation. We make the following contributions: (i) We illustrate how to perform unsupervised 3D lane segmentation by leveraging the distinctive intensity of lanes on the LiDAR point cloud frames, and then obtain the noisy lane labels in the 2D plane by projecting the 3D points; (ii) We propose a novel self-supervised training scheme, dubbed LaneCorrect, that automatically corrects the lane label by learning geometric consistency and instance awareness from the adversarial augmentations; (iii) With the self-supervised pre-trained model, we distill to train a student network for arbitrary target lane (e.g., TuSimple) detection without any human labels; (iv) We thoroughly evaluate our self-supervised method on four major lane detection benchmarks (including TuSimple, CULane, CurveLanes and LLAMAS) and demonstrate excellent performance compared with existing supervised counterpart, whilst showing more effective results on alleviating the domain gap, i.e., training on CULane and test on TuSimple.","authors":["Ming Nie","Xinyue Cai","Hang Xu","Li Zhang"],"url":"https://arxiv.org/abs/2404.14671"}
{"created":"2025-04-29","title":"On Function-Correcting Codes","abstract":"Function-correcting codes were introduced in the work \"Function-Correcting Codes\" (FCC) by Lenz et al. 2023, which provides a graphical representation for the problem of constructing function-correcting codes. We use this function dependent graph to get a lower bound on the redundancy required for function correction codes. By considering the function to be a bijection, such an approach leads to a lower bound on the redundancy required for classical systematic error correcting codes (ECCs). We propose a range of parameters for which the bound is tight. For single error correcting codes, we show that this bound is at least as good as a bound proposed by Zinoviev, Litsyn, and Laihonen in 1998. Thus, this framework helps to study systematic classical error correcting codes. Further, we study the structure of this function dependent graph for linear functions, which leads to bounds on the redundancy of linear-function correcting codes. We show that the Plotkin-like bound for function-correcting codes proposed by Lenz et.al 2023 is simplified for linear functions. We identify a class of linear functions for which an upper bound proposed by Lenz et al., is tight and also identify a class of functions for which coset-wise coding is equivalent to a lower dimensional classical error correction problem.","authors":["Rohit Premlal","B. Sundar Rajan"],"url":"https://arxiv.org/abs/2404.15135"}
{"created":"2025-04-29","title":"RUMOR: Reinforcement learning for Understanding a Model of the Real World for Navigation in Dynamic Environments","abstract":"Autonomous navigation in dynamic environments is a complex but essential task for autonomous robots, with recent deep reinforcement learning approaches showing promising results. However, the complexity of the real world makes it infeasible to train agents in every possible scenario configuration. Moreover, existing methods typically overlook factors such as robot kinodynamic constraints, or assume perfect knowledge of the environment. In this work, we present RUMOR, a novel planner for differential-drive robots that uses deep reinforcement learning to navigate in highly dynamic environments. Unlike other end-to-end DRL planners, it uses a descriptive robocentric velocity space model to extract the dynamic environment information, enhancing training effectiveness and scenario interpretation. Additionally, we propose an action space that inherently considers robot kinodynamics and train it in a simulator that reproduces the real world problematic aspects, reducing the gap between the reality and simulation. We extensively compare RUMOR with other state-of-the-art approaches, demonstrating a better performance, and provide a detailed analysis of the results. Finally, we validate RUMOR's performance in real-world settings by deploying it on a ground robot. Our experiments, conducted in crowded scenarios and unseen environments, confirm the algorithm's robustness and transferability.","authors":["Diego Martinez-Baselga","Luis Riazuelo","Luis Montano"],"url":"https://arxiv.org/abs/2404.16672"}
{"created":"2025-04-29","title":"DIRESA, a distance-preserving nonlinear dimension reduction technique based on regularized autoencoders","abstract":"In meteorology, finding similar weather patterns or analogs in historical datasets can be useful for data assimilation, forecasting, and postprocessing. In climate science, analogs in historical and climate projection data are used for attribution and impact studies. However, most of the time, those large weather and climate datasets are nearline. This means that they must be downloaded, which takes a lot of bandwidth and disk space, before the computationally expensive search can be executed. We propose a dimension reduction technique based on autoencoder (AE) neural networks to compress the datasets and perform the search in an interpretable, compressed latent space. A distance-regularized Siamese twin autoencoder (DIRESA) architecture is designed to preserve distance in latent space while capturing the nonlinearities in the datasets. Using conceptual climate models of different complexities, we show that the latent components thus obtained provide physical insight into the dominant modes of variability in the system. Compressing datasets with DIRESA reduces the online storage and keeps the latent components uncorrelated, while the distance (ordering) preservation and reconstruction fidelity robustly outperform Principal Component Analysis (PCA) and other dimension reduction techniques such as UMAP or variational autoencoders.","authors":["Geert De Paepe","Lesley De Cruz"],"url":"https://arxiv.org/abs/2404.18314"}
{"created":"2025-04-29","title":"LLMPot: Dynamically Configured LLM-based Honeypot for Industrial Protocol and Physical Process Emulation","abstract":"Industrial Control Systems (ICS) are extensively used in critical infrastructures ensuring efficient, reliable, and continuous operations. However, their increasing connectivity and addition of advanced features make them vulnerable to cyber threats, potentially leading to severe disruptions in essential services. In this context, honeypots play a vital role by acting as decoy targets within ICS networks, or on the Internet, helping to detect, log, analyze, and develop mitigations for ICS-specific cyber threats. Deploying ICS honeypots, however, is challenging due to the necessity of accurately replicating industrial protocols and device characteristics, a crucial requirement for effectively mimicking the unique operational behavior of different industrial systems. Moreover, this challenge is compounded by the significant manual effort required in also mimicking the control logic the PLC would execute, in order to capture attacker traffic aiming to disrupt critical infrastructure operations. In this paper, we propose LLMPot, a novel approach for designing honeypots in ICS networks harnessing the potency of Large Language Models (LLMs). LLMPot aims to automate and optimize the creation of realistic honeypots with vendor-agnostic configurations, and for any control logic, aiming to eliminate the manual effort and specialized knowledge traditionally required in this domain. We conducted extensive experiments focusing on a wide array of parameters, demonstrating that our LLM-based approach can effectively create honeypot devices implementing different industrial protocols and diverse control logic.","authors":["Christoforos Vasilatos","Dunia J. Mahboobeh","Hithem Lamri","Manaar Alam","Michail Maniatakos"],"url":"https://arxiv.org/abs/2405.05999"}
{"created":"2025-04-29","title":"Cons-training Tensor Networks: Embedding and Optimization Over Discrete Linear Constraints","abstract":"In this study, we introduce a novel family of tensor networks, termed constrained matrix product states (MPS), designed to incorporate exactly arbitrary discrete linear constraints, including inequalities, into sparse block structures. These tensor networks are particularly tailored for modeling distributions with support strictly over the feasible space, offering benefits such as reducing the search space in optimization problems, alleviating overfitting, improving training efficiency, and decreasing model size. Central to our approach is the concept of a quantum region, an extension of quantum numbers traditionally used in U(1) symmetric tensor networks, adapted to capture any linear constraint, including the unconstrained scenario. We further develop a novel canonical form for these new MPS, which allow for the merging and factorization of tensor blocks according to quantum region fusion rules and permit optimal truncation schemes. Utilizing this canonical form, we apply an unsupervised training strategy to optimize arbitrary objective functions subject to discrete linear constraints. Our method's efficacy is demonstrated by solving the quadratic knapsack problem, achieving superior performance compared to a leading nonlinear integer programming solver. Additionally, we analyze the complexity and scalability of our approach, demonstrating its potential in addressing complex constrained combinatorial optimization problems.","authors":["Javier Lopez-Piqueres","Jing Chen"],"url":"https://arxiv.org/abs/2405.09005"}
{"created":"2025-04-29","title":"Hierarchical Coded Caching with Low Subpacketization and Coding Delay using Combinatorial t-Designs","abstract":"Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \\textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \\textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \\textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.","authors":["Rashid Ummer N. T.","B. Sundar Rajan"],"url":"https://arxiv.org/abs/2405.12747"}
{"created":"2025-04-29","title":"Large language models for newspaper sentiment analysis during COVID-19: The Guardian","abstract":"During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion","authors":["Rohitash Chandra","Baicheng Zhu","Qingying Fang","Eka Shinjikashvili"],"url":"https://arxiv.org/abs/2405.13056"}
{"created":"2025-04-29","title":"ComFace: Facial Representation Learning with Synthetic Data for Comparing Faces","abstract":"Daily monitoring of intra-personal facial changes associated with health and emotional conditions has great potential to be useful for medical, healthcare, and emotion recognition fields. However, the approach for capturing intra-personal facial changes is relatively unexplored due to the difficulty of collecting temporally changing face images. In this paper, we propose a facial representation learning method using synthetic images for comparing faces, called ComFace, which is designed to capture intra-personal facial changes. For effective representation learning, ComFace aims to acquire two feature representations, i.e., inter-personal facial differences and intra-personal facial changes. The key point of our method is the use of synthetic face images to overcome the limitations of collecting real intra-personal face images. Facial representations learned by ComFace are transferred to three extensive downstream tasks for comparing faces: estimating facial expression changes, weight changes, and age changes from two face images of the same individual. Our ComFace, trained using only synthetic data, achieves comparable to or better transfer performance than general pre-training and state-of-the-art representation learning methods trained using real images.","authors":["Yusuke Akamatsu","Terumi Umematsu","Hitoshi Imaoka","Shizuko Gomi","Hideo Tsurushima"],"url":"https://arxiv.org/abs/2405.16016"}
{"created":"2025-04-29","title":"Left-Linear Completion with AC Axioms","abstract":"We revisit completion modulo equational theories for left-linear term rewrite systems where unification modulo the theory is avoided and the normal rewrite relation can be used in order to decide validity questions. To that end, we give a new correctness proof for finite runs and establish a simulation result between the two inference systems known from the literature. Given a concrete reduction order, novel canonicity results show that the resulting complete systems are unique up to the representation of their rules' right-hand sides. Furthermore, we show how left-linear AC completion can be simulated by general AC completion. In particular, this result allows us to switch from the former to the latter at any point during a completion process.","authors":["Johannes Niederhauser","Nao Hirokawa","Aart Middeldorp"],"url":"https://arxiv.org/abs/2405.17109"}
{"created":"2025-04-29","title":"EM-GANSim: Real-time and Accurate EM Simulation Using Conditional GANs for 3D Indoor Scenes","abstract":"We present a novel machine-learning (ML) approach (EM-GANSim) for real-time electromagnetic (EM) propagation that is used for wireless communication simulation in 3D indoor environments. Our approach uses a modified conditional Generative Adversarial Network (GAN) that incorporates encoded geometry and transmitter location while adhering to the electromagnetic propagation theory. The overall physically-inspired learning is able to predict the power distribution in 3D scenes, which is represented using heatmaps. We evaluated our method on 15 complex 3D indoor environments, with 4 additional scenarios later included in the results, showcasing the generalizability of the model across diverse conditions. Our overall accuracy is comparable to ray tracing-based EM simulation, as evidenced by lower mean squared error values. Furthermore, our GAN-based method drastically reduces the computation time, achieving a 5X speedup on complex benchmarks. In practice, it can compute the signal strength in a few milliseconds on any location in 3D indoor environments. We also present a large dataset of 3D models and EM ray tracing-simulated heatmaps. To the best of our knowledge, EM-GANSim is the first real-time algorithm for EM simulation in complex 3D indoor environments. We plan to release the code and the dataset.","authors":["Ruichen Wang","Dinesh Manocha"],"url":"https://arxiv.org/abs/2405.17366"}
{"created":"2025-04-29","title":"Formalising the Local Compactness of the Adele Ring","abstract":"The adele ring of a number field is a central object in modern number theory. Its status as a locally compact topological ring is one of the key reasons why. We describe a formal proof that the adele ring of a number field is locally compact implemented in the Lean 4 theorem prover. Our work includes the formalisations of new types, including the completion of a number field at an infinite place, the infinite adele ring and the finite $S$-adele ring, as well as formal proofs that completions of a number field are locally compact and that their rings of integers at finite places are compact.","authors":["Salvatore Mercuri"],"url":"https://arxiv.org/abs/2405.19270"}
{"created":"2025-04-29","title":"Keyword-driven Retrieval-Augmented Large Language Models for Cold-start User Recommendations","abstract":"Recent advancements in Large Language Models (LLMs) have shown significant potential in enhancing recommender systems. However, addressing the cold-start recommendation problem, where users lack historical data, remains a considerable challenge. In this paper, we introduce KALM4Rec (Keyword-driven Retrieval-Augmented Large Language Models for Cold-start User Recommendations), a novel framework specifically designed to tackle this problem by requiring only a few input keywords from users in a practical scenario of cold-start user restaurant recommendations. KALM4Rec operates in two main stages: candidates retrieval and LLM-based candidates re-ranking. In the first stage, keyword-driven retrieval models are used to identify potential candidates, addressing LLMs' limitations in processing extensive tokens and reducing the risk of generating misleading information. In the second stage, we employ LLMs with various prompting strategies, including zero-shot and few-shot techniques, to re-rank these candidates by integrating multiple examples directly into the LLM prompts. Our evaluation, using a Yelp restaurant dataset with user reviews from three English-speaking cities, shows that our proposed framework significantly improves recommendation quality. Specifically, the integration of in-context instructions with LLMs for re-ranking markedly enhances the performance of the cold-start user recommender system.","authors":["Hai-Dang Kieu","Minh Duc Nguyen","Thanh-Son Nguyen","Dung D. Le"],"url":"https://arxiv.org/abs/2405.19612"}
{"created":"2025-04-29","title":"A Unified Longitudinal Trajectory Dataset for Automated Vehicle","abstract":"Automated Vehicles (AVs) promise significant advances in transportation. Critical to these improvements is understanding AVs' longitudinal behavior, relying heavily on real-world trajectory data. Existing open-source trajectory datasets of AV, however, often fall short in refinement, reliability, and completeness, hindering effective performance metrics analysis and model development. This study addresses these challenges by creating a Unified Longitudinal TRAjectory dataset for AVs (Ultra-AV) to analyze their microscopic longitudinal driving behaviors. This dataset compiles data from 13 distinct sources, encompassing various AV types, test sites, and experiment scenarios. We established a three-step data processing: 1. extraction of longitudinal trajectory data, 2. general data cleaning, and 3. data-specific cleaning to obtain the longitudinal trajectory data and car-following trajectory data. The validity of the processed data is affirmed through performance evaluations across safety, mobility, stability, and sustainability, along with an analysis of the relationships between variables in car-following models. Our work not only furnishes researchers with standardized data and metrics for longitudinal AV behavior studies but also sets guidelines for data collection and model development.","authors":["Hang Zhou","Ke Ma","Shixiao Liang","Xiaopeng Li","Xiaobo Qu"],"url":"https://arxiv.org/abs/2406.00009"}
{"created":"2025-04-29","title":"Leader-Follower Density Control of Spatial Dynamics in Large-Scale Multi-Agent Systems","abstract":"We address the problem of controlling the density of a large ensemble of follower agents by acting on a group of leader agents that interact with them. Using coupled partial integro-differential equations to describe leader and follower density dynamics, we establish feasibility conditions and develop two control architectures ensuring global stability. The first employs feed-forward control on the followers' and a feedback on the leaders' density. The second implements a dual feedback loop through a reference-governor that adapts the leaders' density based on both populations' measurements. Our methods, initially developed in a one-dimensional setting, are extended to multi-dimensional cases, and validated through numerical simulations for representative control applications, both for groups of infinite and finite size.","authors":["Gian Carlo Maffettone","Alain Boldini","Maurizio Porfiri","Mario di Bernardo"],"url":"https://arxiv.org/abs/2406.01804"}
{"created":"2025-04-29","title":"ReDistill: Residual Encoded Distillation for Peak Memory Reduction of CNNs","abstract":"The expansion of neural network sizes and the enhanced resolution of modern image sensors result in heightened memory and power demands to process modern computer vision models. In order to deploy these models in extremely resource-constrained edge devices, it is crucial to reduce their peak memory, which is the maximum memory consumed during the execution of a model. A naive approach to reducing peak memory is aggressive down-sampling of feature maps via pooling with large stride, which often results in unacceptable degradation in network performance. To mitigate this problem, we propose residual encoded distillation (ReDistill) for peak memory reduction in a teacher-student framework, in which a student network with less memory is derived from the teacher network using aggressive pooling. We apply our distillation method to multiple problems in computer vision, including image classification and diffusion-based image generation. For image classification, our method yields 4x-5x theoretical peak memory reduction with less degradation in accuracy for most CNN-based architectures. For diffusion-based image generation, our proposed distillation method yields a denoising network with 4x lower theoretical peak memory while maintaining decent diversity and fidelity for image generation. Experiments demonstrate our method's superior performance compared to other feature-based and response-based distillation methods when applied to the same student network. The code is available at https://github.com/mengtang-lab/ReDistill.","authors":["Fang Chen","Gourav Datta","Mujahid Al Rafi","Hyeran Jeon","Meng Tang"],"url":"https://arxiv.org/abs/2406.03744"}
{"created":"2025-04-29","title":"Practical Transmitters for Molecular Communication: Functionalized Nanodevices Employing Cooperative Transmembrane Transport Proteins","abstract":"This paper introduces a novel optically controllable molecular communication (MC) transmitter (TX) design based on vesicular nanodevices (NDs). The NDs are functionalized for the controlled release of signaling molecules (SMs) via transmembrane proteins. The proposed design contributes to overcoming the current barrier between MC theory and practical implementation, as all components of the system are chemically realizable. The NDs possess an optical-to-chemical conversion capability, therefore, the proposed NDs can be employed as externally controllable TXs in various MC systems. The proposed ND design comprises two cooperating modules, namely an energizing module and a release module, and, depending on the specific choices for the modules, allows for the release of different types of SMs. After introducing the general system model for the proposed realistic TX design, we provide a detailed mathematical analysis of a specific TX realization. In particular, we derive both an exact and a closed-form approximate analytical solution for the concentration of the released SMs and validate our results by comparison with a numerical solution. Moreover, we model the impact of a buffering medium, which is typically present in liquid environments, e.g., in experimental settings or in in-body applications. This allows the evaluation of the feasibility of our proposed TX design in practical chemical implementations. We consider various forms of parameter randomness occurring during vesicle synthesis, i.e., deviations which are unavoidable during experiments. We show that considering random distributions of the parameter values, such as the ND size, the number of incorporated proteins on the vesicle surface, and the vesicle membrane permeability, is crucial for an adequate kinetic analysis of the system.","authors":["Teena tom Dieck","Lukas Brand","Lea Erbacher","Daniela Wegner","Sebastian Lotter","Kathrin Castiglione","Robert Schober","Maximilian Sch\\\"afer"],"url":"https://arxiv.org/abs/2406.06147"}
{"created":"2025-04-29","title":"Hybrid Video Anomaly Detection for Anomalous Scenarios in Autonomous Driving","abstract":"In autonomous driving, the most challenging scenarios can only be detected within their temporal context. Most video anomaly detection approaches focus either on surveillance or traffic accidents, which are only a subfield of autonomous driving. We present HF$^2$-VAD$_{AD}$, a variation of the HF$^2$-VAD surveillance video anomaly detection method for autonomous driving. We learn a representation of normality from a vehicle's ego perspective and evaluate pixel-wise anomaly detections in rare and critical scenarios.","authors":["Daniel Bogdoll","Jan Imhof","Tim Joseph","Svetlana Pavlitska","J. Marius Z\\\"ollner"],"url":"https://arxiv.org/abs/2406.06423"}
{"created":"2025-04-29","title":"A Comprehensive Survey on Machine Learning Driven Material Defect Detection","abstract":"Material defects (MD) represent a primary challenge affecting product performance and giving rise to safety issues in related products. The rapid and accurate identification and localization of MD constitute crucial research endeavors in addressing contemporary challenges associated with MD. In recent years, propelled by the swift advancement of machine learning (ML) technologies, particularly exemplified by deep learning, ML has swiftly emerged as the core technology and a prominent research direction for material defect detection (MDD). Through a comprehensive review of the latest literature, we systematically survey the ML techniques applied in MDD into five categories: unsupervised learning, supervised learning, semi-supervised learning, reinforcement learning, and generative learning. We provide a detailed analysis of the main principles and techniques used, together with the advantages and potential challenges associated with these techniques. Furthermore, the survey focuses on the techniques for defect detection in composite materials, which are important types of materials enjoying increasingly wide application in various industries such as aerospace, automotive, construction, and renewable energy. Finally, the survey explores potential future directions in MDD utilizing ML technologies. This survey consolidates ML-based MDD literature and provides a foundation for future research and practice.","authors":["Jun Bai","Di Wu","Tristan Shelley","Peter Schubel","David Twine","John Russell","Xuesen Zeng","Ji Zhang"],"url":"https://arxiv.org/abs/2406.07880"}
{"created":"2025-04-29","title":"RoTipBot: Robotic Handling of Thin and Flexible Objects using Rotatable Tactile Sensors","abstract":"This paper introduces RoTipBot, a novel robotic system for handling thin, flexible objects. Different from previous works that are limited to singulating them using suction cups or soft grippers, RoTipBot can count multiple layers and then grasp them simultaneously in a single grasp closure. Specifically, we first develop a vision-based tactile sensor named RoTip that can rotate and sense contact information around its tip. Equipped with two RoTip sensors, RoTipBot rolls and feeds multiple layers of thin, flexible objects into the centre between its fingers, enabling effective grasping. Moreover, we design a tactile-based grasping strategy that uses RoTip's sensing ability to ensure both fingers maintain secure contact with the object while accurately counting the number of fed objects. Extensive experiments demonstrate the efficacy of the RoTip sensor and the RoTipBot approach. The results show that RoTipBot not only achieves a higher success rate but also grasps and counts multiple layers simultaneously -- capabilities not possible with previous methods. Furthermore, RoTipBot operates up to three times faster than state-of-the-art methods. The success of RoTipBot paves the way for future research in object manipulation using mobilised tactile sensors. All the materials used in this paper are available at https://sites.google.com/view/rotipbot.","authors":["Jiaqi Jiang","Xuyang Zhang","Daniel Fernandes Gomes","Thanh-Toan Do","Shan Luo"],"url":"https://arxiv.org/abs/2406.09332"}
{"created":"2025-04-29","title":"Think Deep and Fast: Learning Neural Nonlinear Opinion Dynamics from Inverse Dynamic Games for Split-Second Interactions","abstract":"Non-cooperative interactions commonly occur in multi-agent scenarios such as car racing, where an ego vehicle can choose to overtake the rival, or stay behind it until a safe overtaking \"corridor\" opens. While an expert human can do well at making such time-sensitive decisions, autonomous agents are incapable of rapidly reasoning about complex, potentially conflicting options, leading to suboptimal behaviors such as deadlocks. Recently, the nonlinear opinion dynamics (NOD) model has proven to exhibit fast opinion formation and avoidance of decision deadlocks. However, NOD modeling parameters are oftentimes assumed fixed, limiting their applicability in complex and dynamic environments. It remains an open challenge to determine such parameters automatically and adaptively, accounting for the ever-changing environment. In this work, we propose for the first time a learning-based and game-theoretic approach to synthesize a Neural NOD model from expert demonstrations, given as a dataset containing (possibly incomplete) state and action trajectories of interacting agents. We demonstrate Neural NOD's ability to make fast and deadlock-free decisions in a simulated autonomous racing example. We find that Neural NOD consistently outperforms the state-of-the-art data-driven inverse game baseline in terms of safety and overtaking performance.","authors":["Haimin Hu","Jaime Fern\\'andez Fisac","Naomi Ehrich Leonard","Deepak Gopinath","Jonathan DeCastro","Guy Rosman"],"url":"https://arxiv.org/abs/2406.09810"}
{"created":"2025-04-29","title":"Learning Temporal Logic Predicates from Data with Statistical Guarantees","abstract":"Temporal logic rules are often used in control and robotics to provide structured, human-interpretable descriptions of trajectory data. These rules have numerous applications including safety validation using formal methods, constraining motion planning among autonomous agents, and classifying data. However, existing methods for learning temporal logic predicates from data do not provide assurances about the correctness of the resulting predicate. We present a novel method to learn temporal logic predicates from data with finite-sample correctness guarantees. Our approach leverages expression optimization and conformal prediction to learn predicates that correctly describe future trajectories under mild statistical assumptions. We provide experimental results showing the performance of our approach on a simulated trajectory dataset and perform ablation studies to understand how each component of our algorithm contributes to its performance.","authors":["Emi Soroka","Rohan Sinha","Sanjay Lall"],"url":"https://arxiv.org/abs/2406.10449"}
{"created":"2025-04-29","title":"Embedded Hierarchical MPC for Autonomous Navigation","abstract":"To efficiently deploy robotic systems in society, mobile robots must move autonomously and safely through complex environments. Nonlinear model predictive control (MPC) methods provide a natural way to find a dynamically feasible trajectory through the environment without colliding with nearby obstacles. However, the limited computation power available on typical embedded robotic systems, such as quadrotors, poses a challenge to running MPC in real time, including its most expensive tasks: constraints generation and optimization. To address this problem, we propose a novel hierarchical MPC scheme that consists of a planning and a tracking layer. The planner constructs a trajectory with a long prediction horizon at a slow rate, while the tracker ensures trajectory tracking at a relatively fast rate. We prove that the proposed framework avoids collisions and is recursively feasible. Furthermore, we demonstrate its effectiveness in simulations and lab experiments with a quadrotor that needs to reach a goal position in a complex static environment. The code is efficiently implemented on the quadrotor's embedded computer to ensure real-time feasibility. Compared to a state-of-the-art single-layer MPC formulation, this allows us to increase the planning horizon by a factor of 5, which results in significantly better performance.","authors":["Dennis Benders","Johannes K\\\"ohler","Thijs Niesten","Robert Babu\\v{s}ka","Javier Alonso-Mora","Laura Ferranti"],"url":"https://arxiv.org/abs/2406.11506"}
{"created":"2025-04-29","title":"When Are Bias-Free ReLU Networks Effectively Linear Networks?","abstract":"We investigate the implications of removing bias in ReLU networks regarding their expressivity and learning dynamics. We first show that two-layer bias-free ReLU networks have limited expressivity: the only odd function two-layer bias-free ReLU networks can express is a linear one. We then show that, under symmetry conditions on the data, these networks have the same learning dynamics as linear networks. This enables us to give analytical time-course solutions to certain two-layer bias-free (leaky) ReLU networks outside the lazy learning regime. While deep bias-free ReLU networks are more expressive than their two-layer counterparts, they still share a number of similarities with deep linear networks. These similarities enable us to leverage insights from linear networks to understand certain ReLU networks. Overall, our results show that some properties previously established for bias-free ReLU networks arise due to equivalence to linear networks.","authors":["Yedi Zhang","Andrew Saxe","Peter E. Latham"],"url":"https://arxiv.org/abs/2406.12615"}
{"created":"2025-04-29","title":"The effect of control barrier functions on energy transfers in controlled physical systems","abstract":"Using a port-Hamiltonian formalism, we show the qualitative and quantitative effect of safety-critical control implemented with control barrier functions (CBFs) on the power balance of controlled physical systems. The presented results will provide novel tools to design CBFs inducing desired energetic behaviors of the closed-loop system, including nontrivial damping injection effects and non-passive control actions, effectively injecting energy in the system in a controlled manner. Simulations validate the stated results.","authors":["Federico Califano","Riccardo Zanella","Alessandro Macchelli","Stefano Stramigioli"],"url":"https://arxiv.org/abs/2406.13420"}
{"created":"2025-04-29","title":"Accessible, At-Home Detection of Parkinson's Disease via Multi-task Video Analysis","abstract":"Limited accessibility to neurological care leads to underdiagnosed Parkinson's Disease (PD), preventing early intervention. Existing AI-based PD detection methods primarily focus on unimodal analysis of motor or speech tasks, overlooking the multifaceted nature of the disease. To address this, we introduce a large-scale, multi-task video dataset consisting of 1102 sessions (each containing videos of finger tapping, facial expression, and speech tasks captured via webcam) from 845 participants (272 with PD). We propose a novel Uncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal data to enhance diagnostic accuracy. UFNet employs independent task-specific networks, trained with Monte Carlo Dropout for uncertainty quantification, followed by self-attended fusion of features, with attention weights dynamically adjusted based on task-specific uncertainties. To ensure patient-centered evaluation, the participants were randomly split into three sets: 60% for training, 20% for model selection, and 20% for final performance evaluation. UFNet significantly outperformed single-task models in terms of accuracy, area under the ROC curve (AUROC), and sensitivity while maintaining non-inferior specificity. Withholding uncertain predictions further boosted the performance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9% sensitivity, and 92.6+-0.3% specificity, at the expense of not being able to predict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further analysis suggests that the trained model does not exhibit any detectable bias across sex and ethnic subgroups and is most effective for individuals aged between 50 and 80. Requiring only a webcam and microphone, our approach facilitates accessible home-based PD screening, especially in regions with limited healthcare resources.","authors":["Md Saiful Islam","Tariq Adnan","Jan Freyberg","Sangwu Lee","Abdelrahman Abdelkader","Meghan Pawlik","Cathe Schwartz","Karen Jaffe","Ruth B. Schneider","E Ray Dorsey","Ehsan Hoque"],"url":"https://arxiv.org/abs/2406.14856"}
{"created":"2025-04-29","title":"GS-ROR$^2$: Bidirectional-guided 3DGS and SDF for Reflective Object Relighting and Reconstruction","abstract":"3D Gaussian Splatting (3DGS) has shown a powerful capability for novel view synthesis due to its detailed expressive ability and highly efficient rendering speed. Unfortunately, creating relightable 3D assets and reconstructing faithful geometry with 3DGS is still problematic, particularly for reflective objects, as its discontinuous representation raises difficulties in constraining geometries. Volumetric signed distance field (SDF) methods provide robust geometry reconstruction, while the expensive ray marching hinders its real-time application and slows the training. Besides, these methods struggle to capture sharp geometric details. To this end, we propose to guide 3DGS and SDF bidirectionally in a complementary manner, including an SDF-aided Gaussian splatting for efficient optimization of the relighting model and a GS-guided SDF enhancement for high-quality geometry reconstruction. At the core of our SDF-aided Gaussian splatting is the mutual supervision of the depth and normal between blended Gaussians and SDF, which avoids the expensive volume rendering of SDF. Thanks to this mutual supervision, the learned blended Gaussians are well-constrained with a minimal time cost. As the Gaussians are rendered in a deferred shading mode, the alpha-blended Gaussians are smooth, while individual Gaussians may still be outliers, yielding floater artifacts. Therefore, we introduce an SDF-aware pruning strategy to remove Gaussian outliers located distant from the surface defined by SDF, avoiding floater issue. This way, our GS framework provides reasonable normal and achieves realistic relighting, while the mesh from depth is still problematic. Therefore, we design a GS-guided SDF refinement, which utilizes the blended normal from Gaussians to finetune SDF. With this enhancement, our method can further provide high-quality meshes for reflective objects at the cost of 17% extra training time.","authors":["Zuo-Liang Zhu","Beibei Wang","Jian Yang"],"url":"https://arxiv.org/abs/2406.18544"}
{"created":"2025-04-29","title":"On the Centralization and Regionalization of the Web","abstract":"Over the past decade, Internet centralization and its implications for both people and the resilience of the Internet has become a topic of active debate. While the networking community informally agrees on the definition of centralization, we lack a formal metric for quantifying centralization, which limits research beyond descriptive analysis. In this work, we introduce a statistical measure for Internet centralization, which we use to better understand how the web is centralized across four layers of web infrastructure (hosting providers, DNS infrastructure, TLDs, and certificate authorities) in 150~countries. Our work uncovers significant geographical variation, as well as a complex interplay between centralization and sociopolitically driven regionalization. We hope that our work can serve as the foundation for more nuanced analysis to inform this important debate.","authors":["Gautam Akiwate","Kimberly Ruth","Rumaisa Habib","Zakir Durumeric"],"url":"https://arxiv.org/abs/2406.19569"}
{"created":"2025-04-29","title":"CIS: Composable Instruction Set for Data Streaming Applications","abstract":"The enhanced efficiency of hardware accelerators, including Single Instruction Multiple Data (SIMD) architectures and Coarse-Grained Reconfigurable Architectures (CGRAs), is driving significant advancements in Artificial Intelligence and Machine Learning (AI/ML) applications. These applications frequently involve data streaming operations comprised of numerous vector calculations inherently amenable to parallelization. However, despite considerable progress in hardware accelerator design, their potential remains constrained by conventional instruction set architectures (ISAs). Traditional ISAs, primarily designed for microprocessors and accelerators, emphasize computation while often neglecting instruction composability and inter-instruction cooperation. This limitation results in rigid ISAs that are difficult to extend and suffer from large control overhead in their hardware implementations. To address this, we present a novel composable instruction set (CIS) architecture, designed with both spatial and temporal composability, making it well-suited for data streaming applications. The proposed CIS utilizes a small instruction set, yet efficiently implements complex, multi-level loop structures essential for accelerating data streaming workloads. Furthermore, CIS adopts a resource-centric approach, facilitating straightforward extension through the integration of new hardware resources, enabling the creation of custom, heterogeneous computing platforms. Our results comparing performance between the proposed CIS and other state-of-the-art ISAs demonstrate that a CIS-based architecture significantly outperforms existing solutions, achieving near-optimal processing element (PE) utilization.","authors":["Yu Yang","Jordi Altay\\'o Gonz\\'alez","Paul Delestrac","Ahmed Hemani"],"url":"https://arxiv.org/abs/2407.00207"}
{"created":"2025-04-29","title":"Adaptive RKHS Fourier Features for Compositional Gaussian Process Models","abstract":"Deep Gaussian Processes (DGPs) leverage a compositional structure to model non-stationary processes. DGPs typically rely on local inducing point approximations across intermediate GP layers. Recent advances in DGP inference have shown that incorporating global Fourier features from the Reproducing Kernel Hilbert Space (RKHS) can enhance the DGPs' capability to capture complex non-stationary patterns. This paper extends the use of these features to compositional GPs involving linear transformations. In particular, we introduce Ordinary Differential Equation(ODE)--based RKHS Fourier features that allow for adaptive amplitude and phase modulation through convolution operations. This convolutional formulation relates our work to recently proposed deep latent force models, a multi-layer structure designed for modelling nonlinear dynamical systems. By embedding these adjustable RKHS Fourier features within a doubly stochastic variational inference framework, our model exhibits improved predictive performance across various regression tasks.","authors":["Xinxing Shi","Thomas Baldwin-McDonald","Mauricio A. \\'Alvarez"],"url":"https://arxiv.org/abs/2407.01856"}
{"created":"2025-04-29","title":"Fake News Detection: It's All in the Data!","abstract":"This comprehensive survey serves as an indispensable resource for researchers embarking on the journey of fake news detection. By highlighting the pivotal role of dataset quality and diversity, it underscores the significance of these elements in the effectiveness and robustness of detection models. The survey meticulously outlines the key features of datasets, various labeling systems employed, and prevalent biases that can impact model performance. Additionally, it addresses critical ethical issues and best practices, offering a thorough overview of the current state of available datasets. Our contribution to this field is further enriched by the provision of GitHub repository, which consolidates publicly accessible datasets into a single, user-friendly portal. This repository is designed to facilitate and stimulate further research and development efforts aimed at combating the pervasive issue of fake news.","authors":["Soveatin Kuntur","Anna Wr\\'oblewska","Marcin Paprzycki","Maria Ganzha"],"url":"https://arxiv.org/abs/2407.02122"}
{"created":"2025-04-29","title":"Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality","abstract":"In networked virtual reality (VR), user behaviors, individual differences, and group dynamics can serve as important signals into future speech behaviors, such as who the next speaker will be and the timing of turn-taking behaviors. The ability to predict and understand these behaviors offers opportunities to provide adaptive and personalized assistance, for example helping users with varying sensory abilities navigate complex social scenes and instantiating virtual moderators with natural behaviors. In this work, we predict turn-taking behaviors using features extracted based on social dynamics literature. We discuss results from a large-scale VR classroom dataset consisting of 77 sessions and 1660 minutes of small-group social interactions collected over four weeks. In our evaluation, gradient boosting classifiers achieved the best performance, with accuracies of 0.71--0.78 AUC (area under the ROC curve) across three tasks concerning the \"what\", \"who\", and \"when\" of turn-taking behaviors. In interpreting these models, we found that group size, listener personality, speech-related behavior (e.g., time elapsed since the listener's last speech event), group gaze (e.g., how much the group looks at the speaker), as well as the listener's and previous speaker's head pitch, head y-axis position, and left hand y-axis position more saliently influenced predictions. Results suggested that these features remain reliable indicators in novel social VR settings, as prediction performance is robust over time and with groups and activities not used in the training dataset. We discuss theoretical and practical implications of the work.","authors":["Portia Wang","Eugy Han","Anna C. M. Queiroz","Cyan DeVeaux","Jeremy N. Bailenson"],"url":"https://arxiv.org/abs/2407.02896"}
{"created":"2025-04-29","title":"Optimal Quantized Compressed Sensing via Projected Gradient Descent","abstract":"This paper provides a unified treatment to the recovery of structured signals living in a star-shaped set from general quantized measurements $\\mathcal{Q}(\\mathbf{A}\\mathbf{x}-\\mathbf{\\tau})$, where $\\mathbf{A}$ is a sensing matrix, $\\mathbf{\\tau}$ is a vector of (possibly random) quantization thresholds, and $\\mathcal{Q}$ denotes an $L$-level quantizer. The ideal estimator with consistent quantized measurements is optimal in some important instances but typically infeasible to compute. To this end, we study the projected gradient descent (PGD) algorithm with respect to the one-sided $\\ell_1$-loss and identify the conditions under which PGD achieves the same error rate, up to logarithmic factors. These conditions include estimates of the separation probability, small-ball probability and some moment bounds that are easy to validate. For multi-bit case, we also develop a complementary approach based on product embedding to show global convergence. When applied to popular models such as 1-bit compressed sensing with Gaussian $\\mathbf{A}$ and zero $\\mathbf{\\tau}$ and the dithered 1-bit/multi-bit models with sub-Gaussian $\\mathbf{A}$ and uniform dither $\\mathbf{\\tau}$, our unified treatment yields error rates that improve on or match the sharpest results in all instances. Particularly, PGD achieves the information-theoretic optimal rate $\\tilde{O}(\\frac{k}{mL})$ for recovering $k$-sparse signals, and the rate $\\tilde{O}((\\frac{k}{mL})^{1/3})$ for effectively sparse signals. For 1-bit compressed sensing of sparse signals, our result recovers the optimality of normalized binary iterative hard thresholding (NBIHT) that was proved very recently.","authors":["Junren Chen","Ming Yuan"],"url":"https://arxiv.org/abs/2407.04951"}
{"created":"2025-04-29","title":"A boundary integral equation formulation for transient electromagnetic transmission problems on Lipschitz domains","abstract":"We propose a boundary integral formulation for the dynamic problem of electromagnetic scattering and transmission by homogeneous dielectric obstacles. In the spirit of Costabel and Stephan, we use the transmission conditions to reduce the number of unknown densities and to formulate a system of coupled boundary integral equations describing the scattered and transmitted waves. The system is transformed into the Laplace domain where it is proven to be stable and uniquely solvable. The Laplace domain stability estimates are then used to establish the stability and unique solvability of the original time domain problem. Finally, we show how the bounds obtained in both Laplace and time domains can be used to derive error estimates for semi discrete Galerkin discretizations in space and for fully discrete numerical schemes that use Convolution Quadrature for time discretization and a conforming Galerkin method for discretization of the space variables.","authors":["Tonatiuh S\\'anchez-Vizuet"],"url":"https://arxiv.org/abs/2407.05823"}
{"created":"2025-04-29","title":"Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment","abstract":"Large Language Models (LLMs) have seen widespread adoption due to their remarkable natural language capabilities. However, when deploying them in real-world settings, it is important to align LLMs to generate texts according to acceptable human standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO) have enabled significant progress in refining LLMs using human preference data. However, the privacy concerns inherent in utilizing such preference data have yet to be adequately studied. In this paper, we investigate the vulnerability of LLMs aligned using two widely used methods - DPO and PPO - to membership inference attacks (MIAs). Our study has two main contributions: first, we theoretically motivate that DPO models are more vulnerable to MIA compared to PPO models; second, we introduce a novel reference-based attack framework specifically for analyzing preference data called PREMIA (\\uline{Pre}ference data \\uline{MIA}). Using PREMIA and existing baselines we empirically show that DPO models have a relatively heightened vulnerability towards MIA.","authors":["Qizhang Feng","Siva Rajesh Kasa","Santhosh Kumar Kasa","Hyokun Yun","Choon Hui Teo","Sravan Babu Bodapati"],"url":"https://arxiv.org/abs/2407.06443"}
{"created":"2025-04-29","title":"Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews","abstract":"Systematic literature reviews (SLRs) are essential but labor-intensive due to high publication volumes and inefficient keyword-based filtering. To streamline this process, we evaluate Large Language Models (LLMs) for enhancing efficiency and accuracy in corpus filtration while minimizing manual effort. Our open-source tool LLMSurver presents a visual interface to utilize LLMs for literature filtration, evaluate the results, and refine queries in an interactive way. We assess the real-world performance of our approach in filtering over 8.3k articles during a recent survey construction, comparing results with human efforts. The findings show that recent LLM models can reduce filtering time from weeks to minutes. A consensus scheme ensures recall rates >98.8%, surpassing typical human error thresholds and improving selection accuracy. This work advances literature review methodologies and highlights the potential of responsible human-AI collaboration in academic research.","authors":["Lucas Joos","Daniel A. Keim","Maximilian T. Fischer"],"url":"https://arxiv.org/abs/2407.10652"}
{"created":"2025-04-29","title":"EuroCropsML: A Time Series Benchmark Dataset For Few-Shot Crop Type Classification","abstract":"We introduce EuroCropsML, an analysis-ready remote sensing machine learning dataset for time series crop type classification of agricultural parcels in Europe. It is the first dataset designed to benchmark transnational few-shot crop type classification algorithms that supports advancements in algorithmic development and research comparability. It comprises 706 683 multi-class labeled data points across 176 classes, featuring annual time series of per-parcel median pixel values from Sentinel-2 L1C data for 2021, along with crop type labels and spatial coordinates. Based on the open-source EuroCrops collection, EuroCropsML is publicly available on Zenodo.","authors":["Joana Reuss","Jan Macdonald","Simon Becker","Lorenz Richter","Marco K\\\"orner"],"url":"https://arxiv.org/abs/2407.17458"}
{"created":"2025-04-29","title":"DD-rPPGNet: De-interfering and Descriptive Feature Learning for Unsupervised rPPG Estimation","abstract":"Remote Photoplethysmography (rPPG) aims to measure physiological signals and Heart Rate (HR) from facial videos. Recent unsupervised rPPG estimation methods have shown promising potential in estimating rPPG signals from facial regions without relying on ground truth rPPG signals. However, these methods seem oblivious to interference existing in rPPG signals and still result in unsatisfactory performance. In this paper, we propose a novel De-interfered and Descriptive rPPG Estimation Network (DD-rPPGNet) to eliminate the interference within rPPG features for learning genuine rPPG signals. First, we investigate the characteristics of local spatial-temporal similarities of interference and design a novel unsupervised model to estimate the interference. Next, we propose an unsupervised de-interfered method to learn genuine rPPG signals with two stages. In the first stage, we estimate the initial rPPG signals by contrastive learning from both the training data and their augmented counterparts. In the second stage, we use the estimated interference features to derive de-interfered rPPG features and encourage the rPPG signals to be distinct from the interference. In addition, we propose an effective descriptive rPPG feature learning by developing a strong 3D Learnable Descriptive Convolution (3DLDC) to capture the subtle chrominance changes for enhancing rPPG estimation. Extensive experiments conducted on five rPPG benchmark datasets demonstrate that the proposed DD-rPPGNet outperforms previous unsupervised rPPG estimation methods and achieves competitive performances with state-of-the-art supervised rPPG methods. The code is available at: https://github.com/Pei-KaiHuang/TIFS2025-DD-rPPGNet","authors":["Pei-Kai Huang","Tzu-Hsien Chen","Ya-Ting Chan","Kuan-Wen Chen","Chiou-Ting Hsu"],"url":"https://arxiv.org/abs/2407.21402"}
{"created":"2025-04-29","title":"Pula: Training Large Language Models for Setswana","abstract":"In this work we present Pula, a suite of bilingual language models proficient in both Setswana and English. Leveraging recent advancements in data availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks and achieve state-of-the-art performance on Setswana reasoning tasks for their size. We release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and training and evaluation code. Alongside Pula, we release the largest-ever Setswana text corpus, Marothodi, and the first comprehensive Setswana instruction-tuning dataset, Medupi, consisting of reformatted datasets, translated corpora, and synthetic LLM-generated text. To accompany this data, we release the code used for dataset construction, formatting, filtering, and scraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and GSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.","authors":["Nathan Brown","Vukosi Marivate"],"url":"https://arxiv.org/abs/2408.02239"}
{"created":"2025-04-29","title":"Sequential Conditional Transport on Probabilistic Graphs for Interpretable Counterfactual Fairness","abstract":"In this paper, we link two existing approaches to derive counterfactuals: adaptations based on a causal graph, and optimal transport. We extend \"Knothe's rearrangement\" and \"triangular transport\" to probabilistic graphical models, and use this counterfactual approach, referred to as sequential transport, to discuss fairness at the individual level. After establishing the theoretical foundations of the proposed method, we demonstrate its application through numerical experiments on both synthetic and real datasets.","authors":["Agathe Fernandes Machado","Arthur Charpentier","Ewen Gallic"],"url":"https://arxiv.org/abs/2408.03425"}
{"created":"2025-04-29","title":"On the choice of the non-trainable internal weights in random feature maps","abstract":"The computationally cheap machine learning architecture of random feature maps can be viewed as a single-layer feedforward network in which the weights of the hidden layer are random but fixed and only the outer weights are learned via linear regression. The internal weights are typically chosen from a prescribed distribution. The choice of the internal weights significantly impacts the accuracy of random feature maps. We address here the task of how to best select the internal weights. In particular, we consider the forecasting problem whereby random feature maps are used to learn a one-step propagator map for a dynamical system. We provide a computationally cheap hit-and-run algorithm to select good internal weights which lead to good forecasting skill. We show that the number of good features is the main factor controlling the forecasting skill of random feature maps and acts as an effective feature dimension. Lastly, we compare random feature maps with single-layer feedforward neural networks in which the internal weights are now learned using gradient descent. We find that random feature maps have superior forecasting capabilities whilst having several orders of magnitude lower computational cost.","authors":["Pinak Mandal","Georg A. Gottwald","Nicholas Cranch"],"url":"https://arxiv.org/abs/2408.03626"}
{"created":"2025-04-29","title":"AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising","abstract":"With the increase in the fluency of ad texts automatically created by natural language generation technology, there is high demand to verify the quality of these creatives in a real-world setting. We propose AdTEC (Ad Text Evaluation Benchmark by CyberAgent), the first public benchmark to evaluate ad texts from multiple perspectives within practical advertising operations. Our contributions are as follows: (i) Defining five tasks for evaluating the quality of ad texts, as well as building a Japanese dataset based on the practical operational experiences of building a Japanese dataset based on the practical operational experiences of advertising agencies, which are typically kept in-house. (ii) Validating the performance of existing pre-trained language models (PLMs) and human evaluators on the dataset. (iii) Analyzing the characteristics and providing challenges of the benchmark. The results show that while PLMs have already reached practical usage level in several tasks, humans still outperform in certain domains, implying that there is significant room for improvement in this area.","authors":["Peinan Zhang","Yusuke Sakai","Masato Mita","Hiroki Ouchi","Taro Watanabe"],"url":"https://arxiv.org/abs/2408.05906"}
{"created":"2025-04-29","title":"W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering","abstract":"In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.","authors":["Jinming Nian","Zhiyuan Peng","Qifan Wang","Yi Fang"],"url":"https://arxiv.org/abs/2408.08444"}
{"created":"2025-04-29","title":"Position: From Correlation to Causation: Max-Pooling-Based Multi-Instance Learning Leads to More Robust Whole Slide Image Classification","abstract":"Although attention-based multi-instance learning (MIL) algorithms have achieved impressive performance on slide-level whole slide image (WSI) classification tasks, they are prone to mistakenly focusing on irrelevant patterns such as staining conditions and tissue morphology, leading to incorrect patch-level predictions and unreliable interpretability. In this paper, we analyze why attention-based methods tend to rely on spurious correlations in their predictions. Furthermore, we revisit max-pooling-based approaches and examine the reasons behind the underperformance of existing methods. We argue that well-trained max-pooling-based MIL models can make predictions based on causal factors and avoid relying on spurious correlations. Building on these insights, we propose a simple yet effective max-pooling-based MIL method (FocusMIL) that outperforms existing mainstream attention-based methods on two datasets. In this position paper, we advocate renewed attention to max-pooling-based methods to achieve more robust and interpretable predictions.","authors":["Xin Liu","Weijia Zhang","Min-Ling Zhang"],"url":"https://arxiv.org/abs/2408.09449"}
{"created":"2025-04-29","title":"Hierarchical Attention Diffusion Networks with Object Priors for Video Change Detection","abstract":"We present a unified change detection pipeline that combines instance level masking, multi\\-scale attention within a denoising diffusion model, and per pixel semantic classification, all refined via SSIM to match human perception. By first isolating only temporally novel objects with Mask R\\-CNN, then guiding diffusion updates through hierarchical cross attention to object and global contexts, and finally categorizing each pixel into one of C change types, our method delivers detailed, interpretable multi\\-class maps. It outperforms traditional differencing, Siamese CNNs, and GAN\\-based detectors by 10\\-25 points in F1 and IoU on both synthetic and real world benchmarks, marking a new state of the art in remote sensing change detection.","authors":["Andrew Kiruluta","Eric Lundy","Andreas Lemos"],"url":"https://arxiv.org/abs/2408.10619"}
{"created":"2025-04-29","title":"A prototype-based model for set classification","abstract":"Classification of sets of inputs (e.g., images and texts) is an active area of research within both computer vision (CV) and natural language processing (NLP). A common way to represent a set of vectors is to model them as linear subspaces. In this contribution, we present a prototype-based approach for learning on the manifold formed from such linear subspaces, the Grassmann manifold. Our proposed method learns a set of subspace prototypes capturing the representative characteristics of classes and a set of relevance factors automating the selection of the dimensionality of the subspaces. This leads to a transparent classifier model which presents the computed impact of each input vector on its decision. Through experiments on benchmark image and text datasets, we have demonstrated the efficiency of our proposed classifier, compared to the transformer-based models in terms of not only performance and explainability but also computational resource requirements.","authors":["Mohammad Mohammadi","Sreejita Ghosh"],"url":"https://arxiv.org/abs/2408.13720"}
{"created":"2025-04-29","title":"Geo-Llama: Leveraging LLMs for Human Mobility Trajectory Generation with Spatiotemporal Constraints","abstract":"Generating realistic human mobility data is essential for various application domains, including transportation, urban planning, and epidemic control, as real data is often inaccessible to researchers due to high costs and privacy concerns. Existing deep generative models learn from real trajectories to generate synthetic ones. Despite the progress, most of them suffer from training stability issues and scale poorly with increasing data size. More importantly, they often lack control mechanisms to guide the generated trajectories under constraints such as enforcing specific visits. To address these limitations, we formally define the controlled trajectory generation problem for effectively handling multiple spatiotemporal constraints. We introduce Geo-Llama, a novel LLM finetuning framework that can enforce multiple explicit visit constraints while maintaining contextual coherence of the generated trajectories. In this approach, pre-trained LLMs are fine-tuned on trajectory data with a visit-wise permutation strategy where each visit corresponds to a specific time and location. This strategy enables the model to capture spatiotemporal patterns regardless of visit orders while maintaining flexible and in-context constraint integration through prompts during generation. Extensive experiments on real-world and synthetic datasets validate the effectiveness of Geo-Llama, demonstrating its versatility and robustness in handling a broad range of constraints to generate more realistic trajectories compared to existing methods.","authors":["Siyu Li","Toan Tran","Haowen Lin","John Krumm","Cyrus Shahabi","Lingyi Zhao","Khurram Shafique","Li Xiong"],"url":"https://arxiv.org/abs/2408.13918"}
{"created":"2025-04-29","title":"A higher-order Otto calculus approach to the Gaussian completely monotone conjecture","abstract":"The Gaussian completely monotone (GCM) conjecture states that the $m$-th time-derivative of the entropy along the heat flow on $\\mathbb{R}^d$ is positive for $m$ even and negative for $m$ odd. We prove the GCM conjecture for orders up to $m=5$, assuming that the initial measure is log-concave, in any dimension. Our proof differs significantly from previous approaches to the GCM conjecture: it is based on Otto calculus and on the interpretation of the heat flow as the Wasserstein gradient flow of the entropy. Crucial to our methodology is the observation that the convective derivative behaves as a flat connection over probability measures on $\\mathbb{R}^d$. In particular we prove a form of the univariate Faa di Bruno's formula on the Wasserstein space (despite it being curved), and we compute the higher-order Wasserstein differentials of internal energy functionals (including the entropy), both of which are of independent interest.","authors":["Guillaume Wang"],"url":"https://arxiv.org/abs/2408.13957"}
{"created":"2025-04-29","title":"Retrieval Augmented Generation for Dynamic Graph Modeling","abstract":"Modeling dynamic graphs, such as those found in social networks, recommendation systems, and e-commerce platforms, is crucial for capturing evolving relationships and delivering relevant insights over time. Traditional approaches primarily rely on graph neural networks with temporal components or sequence generation models, which often focus narrowly on the historical context of target nodes. This limitation restricts the ability to adapt to new and emerging patterns in dynamic graphs. To address this challenge, we propose a novel framework, Retrieval-Augmented Generation for Dynamic Graph modeling (RAG4DyG), which enhances dynamic graph predictions by incorporating contextually and temporally relevant examples from broader graph structures. Our approach includes a time- and context-aware contrastive learning module to identify high-quality demonstrations and a graph fusion strategy to effectively integrate these examples with historical contexts. The proposed framework is designed to be effective in both transductive and inductive scenarios, ensuring adaptability to previously unseen nodes and evolving graph structures. Extensive experiments across multiple real-world datasets demonstrate the effectiveness of RAG4DyG in improving predictive accuracy and adaptability for dynamic graph modeling. The code and datasets are publicly available at https://github.com/YuxiaWu/RAG4DyG.","authors":["Yuxia Wu","Lizi Liao","Yuan Fang"],"url":"https://arxiv.org/abs/2408.14523"}
{"created":"2025-04-29","title":"Attention-Guided Multi-scale Interaction Network for Face Super-Resolution","abstract":"Recently, CNN and Transformer hybrid networks demonstrated excellent performance in face super-resolution (FSR) tasks. Since numerous features at different scales in hybrid networks, how to fuse these multi-scale features and promote their complementarity is crucial for enhancing FSR. However, existing hybrid network-based FSR methods ignore this, only simply combining the Transformer and CNN. To address this issue, we propose an attention-guided Multi-scale interaction network (AMINet), which contains local and global feature interactions and encoder-decoder phase feature interactions. Specifically, we propose a Local and Global Feature Interaction Module (LGFI) to promote fusions of global features and different receptive fields' local features extracted by our Residual Depth Feature Extraction Module (RDFE). Additionally, we propose a Selective Kernel Attention Fusion Module (SKAF) to adaptively select fusions of different features within LGFI and encoder-decoder phases. Our above design allows the free flow of multi-scale features from within modules and between encoder and decoder, which can promote the complementarity of different scale features to enhance FSR. Comprehensive experiments confirm that our method consistently performs well with less computational consumption and faster inference.","authors":["Xujie Wan","Wenjie Li","Guangwei Gao","Huimin Lu","Jian Yang","Chia-Wen Lin"],"url":"https://arxiv.org/abs/2409.00591"}
{"created":"2025-04-29","title":"Free-DyGS: Camera-Pose-Free Scene Reconstruction for Dynamic Surgical Videos with Gaussian Splatting","abstract":"High-fidelity reconstruction of surgical scene is a fundamentally crucial task to support many applications, such as intra-operative navigation and surgical education. However, most existing methods assume the ideal surgical scenarios - either focus on dynamic reconstruction with deforming tissue yet assuming a given fixed camera pose, or allow endoscope movement yet reconstructing the static scenes. In this paper, we target at a more realistic yet challenging setup - free-pose reconstruction with a moving camera for highly dynamic surgical scenes. Meanwhile, we take the first step to introduce Gaussian Splitting (GS) technique to tackle this challenging setting and propose a novel GS-based framework for fast reconstruction, termed \\textit{Free-DyGS}. Concretely, our model embraces a novel scene initialization in which a pre-trained Sparse Gaussian Regressor (SGR) can efficiently parameterize the initial attributes. For each subsequent frame, we propose to jointly optimize the deformation model and 6D camera poses in a frame-by-frame manner, easing training given the limited deformation differences between consecutive frames. A Scene Expansion scheme is followed to expand the GS model for the unseen regions introduced by the moving camera. Moreover, the framework is equipped with a novel Retrospective Deformation Recapitulation (RDR) strategy to preserve the entire-clip deformations throughout the frame-by-frame training scheme. The efficacy of the proposed Free-DyGS is substantiated through extensive experiments on two datasets: StereoMIS and Hamlyn datasets. The experimental outcomes underscore that Free-DyGS surpasses other advanced methods in both rendering accuracy and efficiency. Code will be available.","authors":["Qian Li","Shuojue Yang","Daiyun Shen","Jimmy Bok Yan So","Jing Qin","Yueming Jin"],"url":"https://arxiv.org/abs/2409.01003"}
{"created":"2025-04-29","title":"PatternPaint: Practical Layout Pattern Generation Using Diffusion-Based Inpainting","abstract":"Generating diverse VLSI layout patterns is essential for various downstream tasks in design for manufacturing, as design rules continually evolve during the development of new technology nodes. However, existing training-based methods for layout pattern generation rely on large datasets. In practical scenarios, especially when developing a new technology node, obtaining such extensive layout data is challenging. Consequently, training models with large datasets becomes impractical, limiting the scalability and adaptability of prior approaches. To this end, we propose PatternPaint, a diffusion-based framework capable of generating legal patterns with limited design-rule-compliant training samples. PatternPaint simplifies complex layout pattern generation into a series of inpainting processes with a template-based denoising scheme. Furthermore, we perform few-shot finetuning on a pretrained image foundation model with only 20 design-rule-compliant samples. Experimental results show that using a sub-3nm technology node (Intel 18A), our model is the only one that can generate legal patterns in complex 2D metal interconnect design rule settings among all previous works and achieves a high diversity score. Additionally, our few-shot finetuning can boost the legality rate with 1.87X improvement compared to the original pretrained model. As a result, we demonstrate a production-ready approach for layout pattern generation in developing new technology nodes.","authors":["Guanglei Zhou","Bhargav Korrapati","Gaurav Rajavendra Reddy","Chen-Chia Chang","Jingyu Pan","Jiang Hu","Yiran Chen","Dipto G. Thakurta"],"url":"https://arxiv.org/abs/2409.01348"}
{"created":"2025-04-29","title":"GraphEx: A Graph-based Extraction Method for Advertiser Keyphrase Recommendation","abstract":"Online sellers and advertisers are recommended keyphrases for their listed products, which they bid on to enhance their sales. One popular paradigm that generates such recommendations is Extreme Multi-Label Classification (XMC), which involves tagging/mapping keyphrases to items. We outline the limitations of using traditional item-query based tagging or mapping techniques for keyphrase recommendations on E-Commerce platforms. We introduce GraphEx, an innovative graph-based approach that recommends keyphrases to sellers using extraction of token permutations from item titles. Additionally, we demonstrate that relying on traditional metrics such as precision/recall can be misleading in practical applications, thereby necessitating a combination of metrics to evaluate performance in real-world scenarios. These metrics are designed to assess the relevance of keyphrases to items and the potential for buyer outreach. GraphEx outperforms production models at eBay, achieving the objectives mentioned above. It supports near real-time inferencing in resource-constrained production environments and scales effectively for billions of items.","authors":["Ashirbad Mishra","Soumik Dey","Marshall Wu","Jinyu Zhao","He Yu","Kaichen Ni","Binbin Li","Kamesh Madduri"],"url":"https://arxiv.org/abs/2409.03140"}
{"created":"2025-04-29","title":"Revisiting Privacy-Utility Trade-off for DP Training with Pre-existing Knowledge","abstract":"Differential privacy (DP) provides a provable framework for protecting individuals by customizing a random mechanism over a privacy-sensitive dataset. Deep learning models have demonstrated privacy risks in model exposure as an established learning model unintentionally records membership-level privacy leakage. Differentially private stochastic gradient descent (DP-SGD) has been proposed to safeguard training individuals by adding random Gaussian noise to gradient updates in the backpropagation. Researchers identify that DP-SGD causes utility loss since the injected homogeneous noise can alter the gradient updates calculated at each iteration. Namely, all elements in the gradient are contaminated regardless of their importance in updating model parameters. In this work, we argue that the utility can be optimized by involving the heterogeneity of the the injected noise. Consequently, we propose a generic differential privacy framework with heterogeneous noise (DP-Hero) by defining a heterogeneous random mechanism to abstract its property. The insight of DP-Hero is to leverage the knowledge encoded in the previously trained model to guide the subsequent allocation of noise heterogeneity, thereby leveraging the statistical perturbation and achieving enhanced utility. Atop DP-Hero, we instantiate a heterogeneous version of DP-SGD, where the noise injected into gradients is heterogeneous and guided by prior-established model parameters. We conduct comprehensive experiments to verify and explain the effectiveness of the proposed DP-Hero, showing improved training accuracy compared with state-of-the-art works. Broadly, we shed light on improving the privacy-utility space by learning the noise guidance from the pre-existing leaked knowledge encoded in the previously trained model, showing a different perspective of understanding the utility-improved DP training.","authors":["Yu Zheng","Wenchao Zhang","Yonggang Zhang","Wei Song","Kai Zhou","Bo Han"],"url":"https://arxiv.org/abs/2409.03344"}
{"created":"2025-04-29","title":"LaMsS: When Large Language Models Meet Self-Skepticism","abstract":"Hallucination is a major challenge for large language models (LLMs), preventing their further application in some fields. The skeptical thinking of humankind could be useful for LLMs to self-cognition, self-reflection and alleviate their hallucinations. Inspired by this consideration, we propose a novel approach called LaMsS, which combines the semantic understanding capability of LLMs with self-skepticism. By introducing a series of skepticism tokens and augmenting them into the vocabulary, we conduct both pertaining and finetuning, which allow the LLM to decode each normal token followed by a skeptical token, representing different skepticism levels. By calculating the response skepticism given a query, one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold. By examining the accuracy, AUC and AP of willingly answering questions, we demonstrate that LaMsS achieves better performance than baselines on both multi-choice questions and open-domain question-answering benchmarks, and can generalize to multi-task and out-of-domain settings. Our study sheds some lights on the self-skepticism modeling on further artificial intelligence. Project code and model checkpoints can be found in https://anonymous.4open.science/r/SM-1E76.","authors":["Yetao Wu","Yihong Wang","Teng Chen","Ningyuan Xi","Qingqing Gu","Hongyang Lei","Luo Ji"],"url":"https://arxiv.org/abs/2409.06601"}
{"created":"2025-04-29","title":"FreeRide: Harvesting Bubbles in Pipeline Parallelism","abstract":"The occurrence of bubbles in pipeline parallelism is an inherent limitation that can account for more than 40% of the large language model (LLM) training time and is one of the main reasons for the underutilization of GPU resources in LLM training. Harvesting these bubbles for GPU side tasks can increase resource utilization and reduce training costs but comes with challenges. First, because bubbles are discontinuous with various shapes, programming side tasks becomes difficult while requiring excessive engineering effort. Second, a side task can compete with pipeline training for GPU resources and incur significant overhead. To address these challenges, we propose FreeRide, a system designed to harvest bubbles in pipeline parallelism for side tasks. FreeRide provides programmers with interfaces to implement side tasks easily, manages bubbles and side tasks during pipeline training, and controls access to GPU resources by side tasks to reduce overhead. We demonstrate that FreeRide achieves 7.8% average cost savings with a negligible overhead of about 1% in training LLMs while serving model training, graph analytics, and image processing side tasks.","authors":["Jiashu Zhang (Yiming)","Zihan Pan (Yiming)","Molly (Yiming)","Xu","Khuzaima Daudjee","Sihang Liu"],"url":"https://arxiv.org/abs/2409.06941"}
{"created":"2025-04-29","title":"What Should We Engineer in Prompts? Training Humans in Requirement-Driven LLM Use","abstract":"Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot) needs humans to clearly articulate customized requirements (e.g., \"start the response with a tl;dr\"). However, existing prompt engineering instructions often lack focused training on requirement articulation and instead tend to emphasize increasingly automatable strategies (e.g., tricks like adding role-plays and \"think step-by-step\"). To address the gap, we introduce Requirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human attention on generating clear, complete requirements during prompting. We implement ROPE through an assessment and training suite that provides deliberate practice with LLM-generated feedback. In a randomized controlled experiment with 30 novices, ROPE significantly outperforms conventional prompt engineering training (20% vs. 1% gains), a gap that automatic prompt optimization cannot close. Furthermore, we demonstrate a direct correlation between the quality of input requirements and LLM outputs. Our work paves the way to empower more end-users to build complex LLM applications.","authors":["Qianou Ma","Weirui Peng","Chenyang Yang","Hua Shen","Kenneth Koedinger","Tongshuang Wu"],"url":"https://arxiv.org/abs/2409.08775"}
{"created":"2025-04-29","title":"AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents","abstract":"Truthfulness (adherence to factual accuracy) and utility (satisfying human needs and instructions) are both fundamental aspects of Large Language Models, yet these goals often conflict (e.g., sell a car with known flaws), which makes it challenging to achieve both in real-world deployments. We propose AI-LieDar, a framework to study how LLM-based agents navigate these scenarios in an multi-turn interactive setting. We design a set of real-world scenarios where language agents are instructed to achieve goals that are in conflict with being truthful during a multi-turn conversation with simulated human agents. To evaluate the truthfulness at large scale, we develop a truthfulness detector inspired by psychological literature to assess the agents' responses. Our experiment demonstrates that all models are truthful less than 50% of the time, though truthfulness and goal achievement (utility) rates vary across models. We further test the steerability of LLMs towards truthfulness, finding that models can be directed to be truthful or deceptive, and even truth-steered models still lie. These findings reveal the complex nature of truthfulness in LLMs and underscore the importance of further research to ensure the safe and reliable deployment of LLMs and LLM-based agents.","authors":["Zhe Su","Xuhui Zhou","Sanketh Rangreji","Anubha Kabra","Julia Mendelsohn","Faeze Brahman","Maarten Sap"],"url":"https://arxiv.org/abs/2409.09013"}
{"created":"2025-04-29","title":"Multi-Scale Grouped Prototypes for Interpretable Semantic Segmentation","abstract":"Prototypical part learning is emerging as a promising approach for making semantic segmentation interpretable. The model selects real patches seen during training as prototypes and constructs the dense prediction map based on the similarity between parts of the test image and the prototypes. This improves interpretability since the user can inspect the link between the predicted output and the patterns learned by the model in terms of prototypical information. In this paper, we propose a method for interpretable semantic segmentation that leverages multi-scale image representation for prototypical part learning. First, we introduce a prototype layer that explicitly learns diverse prototypical parts at several scales, leading to multi-scale representations in the prototype activation output. Then, we propose a sparse grouping mechanism that produces multi-scale sparse groups of these scale-specific prototypical parts. This provides a deeper understanding of the interactions between multi-scale object representations while enhancing the interpretability of the segmentation model. The experiments conducted on Pascal VOC, Cityscapes, and ADE20K demonstrate that the proposed method increases model sparsity, improves interpretability over existing prototype-based methods, and narrows the performance gap with the non-interpretable counterpart models. Code is available at github.com/eceo-epfl/ScaleProtoSeg.","authors":["Hugo Porta","Emanuele Dalsasso","Diego Marcos","Devis Tuia"],"url":"https://arxiv.org/abs/2409.09497"}
{"created":"2025-04-29","title":"Complexity and algorithms for Swap median and relation to other consensus problems","abstract":"Genome rearrangements are events in which large blocks of DNA exchange pieces during evolution. The analysis of such events is a tool for understanding evolutionary genomics, based on finding the minimum number of rearrangements to transform one genome into another, which can be modeled as permutations of integers. In a general scenario, more than two genomes are considered, and new challenges arise. Given three input permutations, the Median problem consists of finding a permutation s that minimizes the sum of the distances between s and each of the three input permutations, according to a specified distance measure. We prove that Median problem over swap distances is NP-complete, a problem whose computational complexity has remained unsolved for nearly 20 years (Eriksen, Theor. Comput. Sci., 2007).","authors":["Lu\\'is Cunha","Thiago Lopes","Arnaud Mary"],"url":"https://arxiv.org/abs/2409.09734"}
{"created":"2025-04-29","title":"SHIRE: Enhancing Sample Efficiency using Human Intuition in REinforcement Learning","abstract":"The ability of neural networks to perform robotic perception and control tasks such as depth and optical flow estimation, simultaneous localization and mapping (SLAM), and automatic control has led to their widespread adoption in recent years. Deep Reinforcement Learning has been used extensively in these settings, as it does not have the unsustainable training costs associated with supervised learning. However, DeepRL suffers from poor sample efficiency, i.e., it requires a large number of environmental interactions to converge to an acceptable solution. Modern RL algorithms such as Deep Q Learning and Soft Actor-Critic attempt to remedy this shortcoming but can not provide the explainability required in applications such as autonomous robotics. Humans intuitively understand the long-time-horizon sequential tasks common in robotics. Properly using such intuition can make RL policies more explainable while enhancing their sample efficiency. In this work, we propose SHIRE, a novel framework for encoding human intuition using Probabilistic Graphical Models (PGMs) and using it in the Deep RL training pipeline to enhance sample efficiency. Our framework achieves 25-78% sample efficiency gains across the environments we evaluate at negligible overhead cost. Additionally, by teaching RL agents the encoded elementary behavior, SHIRE enhances policy explainability. A real-world demonstration further highlights the efficacy of policies trained using our framework.","authors":["Amogh Joshi","Adarsh Kumar Kosta","Kaushik Roy"],"url":"https://arxiv.org/abs/2409.09990"}
{"created":"2025-04-29","title":"ID-Free Not Risk-Free: LLM-Powered Agents Unveil Risks in ID-Free Recommender Systems","abstract":"Recent advances in ID-free recommender systems have attracted significant attention for effectively addressing the cold start problem. However, their vulnerability to malicious attacks remains largely unexplored. In this paper, we unveil a critical yet overlooked risk: LLM-powered agents can be strategically deployed to attack ID-free recommenders, stealthily promoting low-quality items in black-box settings. This attack exploits a novel rewriting-based deception strategy, where malicious agents synthesize deceptive textual descriptions by simulating the characteristics of popular items. To achieve this, the attack mechanism integrates two primary components: (1) a popularity extraction component that captures essential characteristics of popular items and (2) a multi-agent collaboration mechanism that enables iterative refinement of promotional textual descriptions through independent thinking and team discussion. To counter this risk, we further introduce a detection method to identify suspicious text generated by our discovered attack. By unveiling this risk, our work aims to underscore the urgent need to enhance the security of ID-free recommender systems.","authors":["Zongwei Wang","Min Gao","Junliang Yu","Xinyi Gao","Quoc Viet Hung Nguyen","Shazia Sadiq","Hongzhi Yin"],"url":"https://arxiv.org/abs/2409.11690"}
{"created":"2025-04-29","title":"Informative Input Design for Dynamic Mode Decomposition","abstract":"Efficiently estimating system dynamics from data is essential for minimizing data collection costs and improving model performance. This work addresses the challenge of designing future control inputs to maximize information gain, thereby improving the efficiency of the system identification process. We propose an approach that integrates informative input design into the Dynamic Mode Decomposition with control (DMDc) framework, which is well-suited for high-dimensional systems. By formulating an approximate convex optimization problem that minimizes the trace of the estimation error covariance matrix, we are able to efficiently reduce uncertainty in the model parameters while respecting constraints on the system states and control inputs. This method outperforms traditional techniques like Pseudo-Random Binary Sequences (PRBS) and orthogonal multisines, which do not adapt to the current system model and often gather redundant information. We validate our approach using aircraft and fluid dynamics simulations to demonstrate the practical applicability and effectiveness of our method. Our results show that strategically planning control inputs based on the current model enhances the accuracy of system identification while requiring less data. Furthermore, we provide our implementation and simulation interfaces as an open-source software package, facilitating further research development and use by industry practitioners.","authors":["Joshua Ott","Mykel J. Kochenderfer","Stephen Boyd"],"url":"https://arxiv.org/abs/2409.13088"}
{"created":"2025-04-29","title":"MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety","abstract":"While robust optimal control theory provides a rigorous framework to compute robot control policies that are provably safe, it struggles to scale to high-dimensional problems, leading to increased use of deep learning for tractable synthesis of robot safety. Unfortunately, existing neural safety synthesis methods often lack convergence guarantees and solution interpretability. In this paper, we present Minimax Actors Guided by Implicit Critic Stackelberg (MAGICS), a novel adversarial reinforcement learning (RL) algorithm that guarantees local convergence to a minimax equilibrium solution. We then build on this approach to provide local convergence guarantees for a general deep RL-based robot safety synthesis algorithm. Through both simulation studies on OpenAI Gym environments and hardware experiments with a 36-dimensional quadruped robot, we show that MAGICS can yield robust control policies outperforming the state-of-the-art neural safety synthesis methods.","authors":["Justin Wang","Haimin Hu","Duy Phuong Nguyen","Jaime Fern\\'andez Fisac"],"url":"https://arxiv.org/abs/2409.13867"}
{"created":"2025-04-29","title":"FedSlate:A Federated Deep Reinforcement Learning Recommender System","abstract":"Reinforcement learning methods have been used to optimize long-term user engagement in recommendation systems. However, existing reinforcement learning-based recommendation systems do not fully exploit the relevance of individual user behavior across different platforms. One potential solution is to aggregate data from various platforms in a centralized location and use the aggregated data for training. However, this approach raises economic and legal concerns, including increased communication costs and potential threats to user privacy. To address these challenges, we propose \\textbf{FedSlate}, a federated reinforcement learning recommendation algorithm that effectively utilizes information that is prohibited from being shared at a legal level. We employ the SlateQ algorithm to assist FedSlate in learning users' long-term behavior and evaluating the value of recommended content. We extend the existing application scope of recommendation systems from single-user single-platform to single-user multi-platform and address cross-platform learning challenges by introducing federated learning. We use RecSim to construct a simulation environment for evaluating FedSlate and compare its performance with state-of-the-art benchmark recommendation models. Experimental results demonstrate the superior effects of FedSlate over baseline methods in various environmental settings, and FedSlate facilitates the learning of recommendation strategies in scenarios where baseline methods are completely inapplicable. Code is available at \\textit{https://github.com/TianYaDY/FedSlate}.","authors":["Yongxin Deng","Xihe Qiu","Xiaoyu Tan","Yaochu Jin"],"url":"https://arxiv.org/abs/2409.14872"}
{"created":"2025-04-29","title":"A Realistic Simulation Framework for Analog/Digital Neuromorphic Architectures","abstract":"Developing dedicated mixed-signal neuromorphic computing systems optimized for real-time sensory-processing in extreme edge-computing applications requires time-consuming design, fabrication, and deployment of full-custom neuromorphic processors. To ensure that initial prototyping efforts, exploring the properties of different network architectures and parameter settings, lead to realistic results, it is important to use simulation frameworks that match as best as possible the properties of the final hardware. This is particularly challenging for neuromorphic hardware platforms made using mixed-signal analog/digital circuits, due to the variability and noise sensitivity of their components. In this paper, we address this challenge by developing a software spiking neural network simulator explicitly designed to account for the properties of mixed-signal neuromorphic circuits, including device mismatch variability.","authors":["Fernando M. Quintana","Maryada","Pedro L. Galindo","Elisa Donati","Giacomo Indiveri","Fernando Perez-Pe\\~na"],"url":"https://arxiv.org/abs/2409.14918"}
{"created":"2025-04-29","title":"Dynamic Integration of Task-Specific Adapters for Class Incremental Learning","abstract":"Non-exemplar class Incremental Learning (NECIL) enables models to continuously acquire new classes without retraining from scratch and storing old task exemplars, addressing privacy and storage issues. However, the absence of data from earlier tasks exacerbates the challenge of catastrophic forgetting in NECIL. In this paper, we propose a novel framework called Dynamic Integration of task-specific Adapters (DIA), which comprises two key components: Task-Specific Adapter Integration (TSAI) and Patch-Level Model Alignment. TSAI boosts compositionality through a patch-level adapter integration strategy, which provides a more flexible compositional solution while maintaining low computation costs. Patch-Level Model Alignment maintains feature consistency and accurate decision boundaries via two specialized mechanisms: Patch-Level Distillation Loss (PDL) and Patch-Level Feature Reconstruction method (PFR). Specifically, the PDL preserves feature-level consistency between successive models by implementing a distillation loss based on the contributions of patch tokens to new class learning. The PFR facilitates accurate classifier alignment by reconstructing old class features from previous tasks that adapt to new task knowledge. Extensive experiments validate the effectiveness of our DIA, revealing significant improvements on benchmark datasets in the NECIL setting, maintaining an optimal balance between computational complexity and accuracy.","authors":["Jiashuo Li","Shaokun Wang","Bo Qian","Yuhang He","Xing Wei","Qiang Wang","Yihong Gong"],"url":"https://arxiv.org/abs/2409.14983"}
{"created":"2025-04-29","title":"Geometric Design and Gait Co-Optimization for Soft Continuum Robots Swimming at Low and High Reynolds Numbers","abstract":"Recent advancements in soft actuators have enabled soft continuum swimming robots to achieve higher efficiency and more closely mimic the behaviors of real marine animals. However, optimizing the design and control of these soft continuum robots remains a significant challenge. In this paper, we present a practical framework for the co-optimization of the design and control of soft continuum robots, approached from a geometric locomotion analysis perspective. This framework is based on the principles of geometric mechanics, accounting for swimming at both low and high Reynolds numbers. By generalizing geometric principles to continuum bodies, we achieve efficient geometric variational co-optimization of designs and gaits across different power consumption metrics and swimming environments. The resulting optimal designs and gaits exhibit greater efficiencies at both low and high Reynolds numbers compared to three-link or serpenoid swimmers with the same degrees of freedom, approaching or even surpassing the efficiencies of infinitely flexible swimmers and those with higher degrees of freedom.","authors":["Yanhao Yang","Ross L. Hatton"],"url":"https://arxiv.org/abs/2409.15220"}
{"created":"2025-04-29","title":"SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization","abstract":"We propose a novel unsupervised cross-modal homography estimation learning framework, named Split Supervised Homography estimation Network (SSHNet). SSHNet reformulates the unsupervised cross-modal homography estimation into two supervised sub-problems, each addressed by its specialized network: a homography estimation network and a modality transfer network. To realize stable training, we introduce an effective split optimization strategy to train each network separately within its respective sub-problem. We also formulate an extra homography feature space supervision to enhance feature consistency, further boosting the estimation accuracy. Moreover, we employ a simple yet effective distillation training technique to reduce model parameters and improve cross-domain generalization ability while maintaining comparable performance. The training stability of SSHNet enables its cooperation with various homography estimation architectures. Experiments reveal that the SSHNet using IHN as homography estimation network, namely SSHNet-IHN, outperforms previous unsupervised approaches by a significant margin. Even compared to supervised approaches MHN and LocalTrans, SSHNet-IHN achieves 47.4% and 85.8% mean average corner errors (MACEs) reduction on the challenging OPT-SAR dataset.","authors":["Junchen Yu","Si-Yuan Cao","Runmin Zhang","Chenghao Zhang","Zhu Yu","Shujie Chen","Bailin Yang","Hui-liang Shen"],"url":"https://arxiv.org/abs/2409.17993"}
{"created":"2025-04-29","title":"LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness","abstract":"Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D scene understanding capabilities has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D visual understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we utilize the 3D position embeddings to enhance the 2D CLIP Patches with 3D spatial context information and construct 3D patches. By integrating the 3D position embeddings into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D visual understanding and 3D scene understanding. In contrast to previous 3D LMMs, LLaVA-3D supports decoding accurate 3D spatial perception outputs, e.g., 3D bounding boxes, directly from these 3D patches, without relying on the time-consuming off-the-shelf 3D segmentors. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D visual understanding and vision-language conversation capabilities with LLaVA.","authors":["Chenming Zhu","Tai Wang","Wenwei Zhang","Jiangmiao Pang","Xihui Liu"],"url":"https://arxiv.org/abs/2409.18125"}
{"created":"2025-04-29","title":"Domain Consistency Representation Learning for Lifelong Person Re-Identification","abstract":"Lifelong person re-identification (LReID) exhibits a contradictory relationship between intra-domain discrimination and inter-domain gaps when learning from continuous data. Intra-domain discrimination focuses on individual nuances (i.e., clothing type, accessories, etc.), while inter-domain gaps emphasize domain consistency. Achieving a trade-off between maximizing intra-domain discrimination and minimizing inter-domain gaps is a crucial challenge for improving LReID performance. Most existing methods strive to reduce inter-domain gaps through knowledge distillation to maintain domain consistency. However, they often ignore intra-domain discrimination. To address this challenge, we propose a novel domain consistency representation learning (DCR) model that explores global and attribute-wise representations as a bridge to balance intra-domain discrimination and inter-domain gaps. At the intra-domain level, we explore the complementary relationship between global and attribute-wise representations to improve discrimination among similar identities. Excessive learning intra-domain discrimination can lead to catastrophic forgetting. We further develop an attribute-oriented anti-forgetting (AF) strategy that explores attribute-wise representations to enhance inter-domain consistency, and propose a knowledge consolidation (KC) strategy to facilitate knowledge transfer. Extensive experiments show that our DCR model achieves superior performance compared to state-of-the-art LReID methods. Our code is publicly available at https://github.com/LiuShiBen/DCR.","authors":["Shiben Liu","Qiang Wang","Huijie Fan","Weihong Ren","Baojie Fan","Yandong Tang"],"url":"https://arxiv.org/abs/2409.19954"}
{"created":"2025-04-29","title":"Acceleration Meets Inverse Maintenance: Faster $\\ell_{\\infty}$-Regression","abstract":"We propose a randomized multiplicative weight update (MWU) algorithm for $\\ell_{\\infty}$ regression that runs in $\\widetilde{O}\\left(n^{2+1/22.5} \\text{poly}(1/\\epsilon)\\right)$ time when $\\omega = 2+o(1)$, improving upon the previous best $\\widetilde{O}\\left(n^{2+1/18} \\text{poly} \\log(1/\\epsilon)\\right)$ runtime in the low-accuracy regime. Our algorithm combines state-of-the-art inverse maintenance data structures with acceleration. In order to do so, we propose a novel acceleration scheme for MWU that exhibits {\\it stabiliy} and {\\it robustness}, which are required for the efficient implementations of the inverse maintenance data structures.","authors":["Deeksha Adil","Shunhua Jiang","Rasmus Kyng"],"url":"https://arxiv.org/abs/2409.20030"}
{"created":"2025-04-29","title":"Adaptive Finite Element Method for Phase Field Fracture Models Based on Recovery Error Estimates","abstract":"The phase field model is a widely used mathematical approach for describing crack propagation in continuum damage fractures. In the context of phase field fracture simulations, adaptive finite element methods (AFEM) are often employed to address the mesh size dependency of the model. However, existing AFEM approaches for this application frequently rely on heuristic adjustments and empirical parameters for mesh refinement. In this paper, we introduce an adaptive finite element method based on a recovery type posteriori error estimates approach grounded in theoretical analysis. This method transforms the gradient of the numerical solution into a smoother function space, using the difference between the recovered gradient and the original numerical gradient as an error indicator for adaptive mesh refinement. This enables the automatic capture of crack propagation directions without the need for empirical parameters. We have implemented this adaptive method for the Hybrid formulation of the phase field model using the open-source software package FEALPy. The accuracy and efficiency of the proposed approach are demonstrated through simulations of classical 2D and 3D brittle fracture examples, validating the robustness and effectiveness of our implementation.","authors":["Tian Tian","Chen Chunyu","He Liang","Wei Huayi"],"url":"https://arxiv.org/abs/2410.01177"}
{"created":"2025-04-29","title":"Windowed MAPF with Completeness Guarantees","abstract":"Traditional multi-agent path finding (MAPF) methods try to compute entire start-goal paths which are collision free. However, computing an entire path can take too long for MAPF systems where agents need to replan fast. Methods that address this typically employ a \"windowed\" approach and only try to find collision free paths for a small windowed timestep horizon. This adaptation comes at the cost of incompleteness; all current windowed approaches can become stuck in deadlock or livelock. Our main contribution is to introduce our framework, WinC-MAPF, for Windowed MAPF that enables completeness. Our framework uses heuristic update insights from single-agent real-time heuristic search algorithms as well as agent independence ideas from MAPF algorithms. We also develop Single-Step CBS (SS-CBS), an instantiation of this framework using a novel modification to CBS. We show how SS-CBS, which only plans a single step and updates heuristics, can effectively solve tough scenarios where existing windowed approaches fail.","authors":["Rishi Veerapaneni","Muhammad Suhail Saleem","Jiaoyang Li","Maxim Likhachev"],"url":"https://arxiv.org/abs/2410.01798"}
{"created":"2025-04-29","title":"Channel-Aware Throughput Maximization for Cooperative Data Fusion in CAV","abstract":"Connected and autonomous vehicles (CAVs) have garnered significant attention due to their extended perception range and enhanced sensing coverage. To address challenges such as blind spots and obstructions, CAVs employ vehicle-to-vehicle (V2V) communications to aggregate sensory data from surrounding vehicles. However, cooperative perception is often constrained by the limitations of achievable network throughput and channel quality. In this paper, we propose a channel-aware throughput maximization approach to facilitate CAV data fusion, leveraging a self-supervised autoencoder for adaptive data compression. We formulate the problem as a mixed integer programming (MIP) model, which we decompose into two sub-problems to derive optimal data rate and compression ratio solutions under given link conditions. An autoencoder is then trained to minimize bitrate with the determined compression ratio, and a fine-tuning strategy is employed to further reduce spectrum resource consumption. Experimental evaluation on the OpenCOOD platform demonstrates the effectiveness of our proposed algorithm, showing more than 20.19\\% improvement in network throughput and a 9.38\\% increase in average precision (AP@IoU) compared to state-of-the-art methods, with an optimal latency of 19.99 ms.","authors":["Haonan An","Zhengru Fang","Yuang Zhang","Senkang Hu","Xianhao Chen","Guowen Xu","Yuguang Fang"],"url":"https://arxiv.org/abs/2410.04320"}
{"created":"2025-04-29","title":"Online Dynamic Pricing for Electric Vehicle Charging Stations with Reservations","abstract":"This paper introduces a novel model for online dynamic pricing of electric vehicle charging services that integrates reservation, parking, and charging into a comprehensive bundle priced as a whole. Our approach focuses on the individual high-demand, fast-charging location, employing a Poisson process as a model of charging reservation arrivals, and develops an online dynamic pricing strategy optimized through a Markov Decision Process (MDP). A key contribution is the novel analysis of discretization error introduced when incorporating the continuous-time Poisson process into the discrete MDP framework. The MDP model's feasibility is demonstrated with a heuristic dynamic pricing method based on Monte-Carlo tree search, offering a viable path for real-world applications.","authors":["Jan Mrkos","Anton\\'in Komenda","David Fiedler","Ji\\v{r}\\'i Vok\\v{r}\\'inek"],"url":"https://arxiv.org/abs/2410.05538"}
{"created":"2025-04-29","title":"LightRAG: Simple and Fast Retrieval-Augmented Generation","abstract":"Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our LightRAG open-source and available at the link: https://github.com/HKUDS/LightRAG","authors":["Zirui Guo","Lianghao Xia","Yanhua Yu","Tu Ao","Chao Huang"],"url":"https://arxiv.org/abs/2410.05779"}
{"created":"2025-04-29","title":"Variable Bitrate Residual Vector Quantization for Audio Coding","abstract":"Recent state-of-the-art neural audio compression models have progressively adopted residual vector quantization (RVQ). Despite this success, these models employ a fixed number of codebooks per frame, which can be suboptimal in terms of rate-distortion tradeoff, particularly in scenarios with simple input audio, such as silence. To address this limitation, we propose variable bitrate RVQ (VRVQ) for audio codecs, which allows for more efficient coding by adapting the number of codebooks used per frame. Furthermore, we propose a gradient estimation method for the non-differentiable masking operation that transforms from the importance map to the binary importance mask, improving model training via a straight-through estimator. We demonstrate that the proposed training framework achieves superior results compared to the baseline method and shows further improvement when applied to the current state-of-the-art codec.","authors":["Yunkee Chae","Woosung Choi","Yuhta Takida","Junghyun Koo","Yukara Ikemiya","Zhi Zhong","Kin Wai Cheuk","Marco A. Mart\\'inez-Ram\\'irez","Kyogu Lee","Wei-Hsiang Liao","Yuki Mitsufuji"],"url":"https://arxiv.org/abs/2410.06016"}
{"created":"2025-04-29","title":"Estimating the Number of HTTP/3 Responses in QUIC Using Deep Learning","abstract":"QUIC, a new and increasingly used transport protocol, enhances TCP by offering improved security, performance, and stream multiplexing. These features, however, also impose challenges for network middle-boxes that need to monitor and analyze web traffic. This paper proposes a novel method to estimate the number of HTTP/3 responses in a given QUIC connection by an observer. This estimation reveals server behavior, client-server interactions, and data transmission efficiency, which is crucial for various applications such as designing a load balancing solution and detecting HTTP/3 flood attacks. The proposed scheme transforms QUIC connection traces into image sequences and uses machine learning (ML) models, guided by a tailored loss function, to predict response counts. Evaluations on more than seven million images-derived from 100,000 traces collected across 44,000 websites over four months-achieve up to 97% accuracy in both known and unknown server settings and 92% accuracy on previously unseen complete QUIC traces.","authors":["Barak Gahtan","Robert J. Shahla","Reuven Cohen","Alex M. Bronstein"],"url":"https://arxiv.org/abs/2410.06140"}
{"created":"2025-04-29","title":"$\\Gamma$-convergence of an Enhanced Finite Element Method for Mani\\`a's Problem Exhibiting the Lavrentiev Gap Phenomenon","abstract":"It is well-known that numerically approximating calculus of variations problems possessing a Lavrentiev Gap Phenomenon (LGP) is challenging, and the standard numerical methodologies such as finite element, finite difference and discontinuous Galerkin methods fail to give convergent methods because they cannot overcome the gap. This paper is a continuation of previous work by Schnake and Feng, where a promising enhanced finite element method was proposed to overcome the LGP in the classical Mani\\`a's problem. The goal of this paper is to provide a complete $\\Gamma$-convergence proof for this enhanced finite element method, hence, establishing a theoretical foundation for the method. The cruxes of the convergence analysis are taking advantage of the regularity of the minimizer and viewing the minimization problem as posed over the fractional Sobolev space $W^{1 + s, p}$ (for $s > 0$) rather than the original admissible space.","authors":["Xiaobing Feng","Joshua M. Siktar"],"url":"https://arxiv.org/abs/2410.06434"}
{"created":"2025-04-29","title":"Towards Interpreting Visual Information Processing in Vision-Language Models","abstract":"Vision-Language Models (VLMs) are powerful tools for processing and understanding text and images. We study the processing of visual tokens in the language model component of LLaVA, a prominent VLM. Our approach focuses on analyzing the localization of object information, the evolution of visual token representations across layers, and the mechanism of integrating visual information for predictions. Through ablation studies, we demonstrated that object identification accuracy drops by over 70\\% when object-specific tokens are removed. We observed that visual token representations become increasingly interpretable in the vocabulary space across layers, suggesting an alignment with textual tokens corresponding to image content. Finally, we found that the model extracts object information from these refined representations at the last token position for prediction, mirroring the process in text-only language models for factual association tasks. These findings provide crucial insights into how VLMs process and integrate visual information, bridging the gap between our understanding of language and vision models, and paving the way for more interpretable and controllable multimodal systems.","authors":["Clement Neo","Luke Ong","Philip Torr","Mor Geva","David Krueger","Fazl Barez"],"url":"https://arxiv.org/abs/2410.07149"}
{"created":"2025-04-29","title":"Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content","abstract":"With the continuous progress of visual generation technologies, the scale of video datasets has grown exponentially. The quality of these datasets plays a pivotal role in the performance of video generation models. We assert that temporal splitting, detailed captions, and video quality filtering are three crucial determinants of dataset quality. However, existing datasets exhibit various limitations in these areas. To address these challenges, we introduce Koala-36M, a large-scale, high-quality video dataset featuring accurate temporal splitting, detailed captions, and superior video quality. The essence of our approach lies in improving the consistency between fine-grained conditions and video content. Specifically, we employ a linear classifier on probability distributions to enhance the accuracy of transition detection, ensuring better temporal consistency. We then provide structured captions for the splitted videos, with an average length of 200 words, to improve text-video alignment. Additionally, we develop a Video Training Suitability Score (VTSS) that integrates multiple sub-metrics, allowing us to filter high-quality videos from the original corpus. Finally, we incorporate several metrics into the training process of the generation model, further refining the fine-grained conditions. Our experiments demonstrate the effectiveness of our data processing pipeline and the quality of the proposed Koala-36M dataset. Our dataset and code have been released at https://koala36m.github.io/.","authors":["Qiuheng Wang","Yukai Shi","Jiarong Ou","Rui Chen","Ke Lin","Jiahao Wang","Boyuan Jiang","Haotian Yang","Mingwu Zheng","Xin Tao","Fei Yang","Pengfei Wan","Di Zhang"],"url":"https://arxiv.org/abs/2410.08260"}
{"created":"2025-04-29","title":"Data Processing for the OpenGPT-X Model Family","abstract":"This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.","authors":["Nicolo' Brandizzi","Hammam Abdelwahab","Anirban Bhowmick","Lennard Helmer","Benny J\\\"org Stein","Pavel Denisov","Qasid Saleem","Michael Fromm","Mehdi Ali","Richard Rutmann","Farzad Naderi","Mohamad Saif Agy","Alexander Schwirjow","Fabian K\\\"uch","Luzian Hahn","Malte Ostendorff","Pedro Ortiz Suarez","Georg Rehm","Dennis Wegener","Nicolas Flores-Herr","Joachim K\\\"ohler","Johannes Leveling"],"url":"https://arxiv.org/abs/2410.08800"}
{"created":"2025-04-29","title":"Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization","abstract":"Direct Preference Optimization (DPO) and its variants are increasingly used for aligning language models with human preferences. Although these methods are designed to teach a model to generate preferred responses more frequently relative to dispreferred responses, prior work has observed that the likelihood of preferred responses often decreases during training. The current work sheds light on the causes and implications of this counter-intuitive phenomenon, which we term likelihood displacement. We demonstrate that likelihood displacement can be catastrophic, shifting probability mass from preferred responses to responses with an opposite meaning. As a simple example, training a model to prefer $\\texttt{No}$ over $\\texttt{Never}$ can sharply increase the probability of $\\texttt{Yes}$. Moreover, when aligning the model to refuse unsafe prompts, we show that such displacement can unintentionally lead to unalignment, by shifting probability mass from preferred refusal responses to harmful responses (e.g., reducing the refusal rate of Llama-3-8B-Instruct from 74.4% to 33.4%). We theoretically characterize that likelihood displacement is driven by preferences that induce similar embeddings, as measured by a centered hidden embedding similarity (CHES) score. Empirically, the CHES score enables identifying which training samples contribute most to likelihood displacement in a given dataset. Filtering out these samples effectively mitigated unintentional unalignment in our experiments. More broadly, our results highlight the importance of curating data with sufficiently distinct preferences, for which we believe the CHES score may prove valuable.","authors":["Noam Razin","Sadhika Malladi","Adithya Bhaskar","Danqi Chen","Sanjeev Arora","Boris Hanin"],"url":"https://arxiv.org/abs/2410.08847"}
{"created":"2025-04-29","title":"Measurability in the Fundamental Theorem of Statistical Learning","abstract":"The Fundamental Theorem of Statistical Learning states that a hypothesis space is PAC learnable if and only if its VC dimension is finite. For the agnostic model of PAC learning, the literature so far presents proofs of this theorem that often tacitly impose several measurability assumptions on the involved sets and functions. We scrutinize these proofs from a measure-theoretic perspective in order to explicitly extract the assumptions needed for a rigorous argument. This leads to a sound statement as well as a detailed and self-contained proof of the Fundamental Theorem of Statistical Learning in the agnostic setting, showcasing the minimal measurability requirements needed. As the Fundamental Theorem of Statistical Learning underpins a wide range of further theoretical developments, our results are of foundational importance: A careful analysis of measurability aspects is essential, especially when the theorem is used in settings where measure-theoretic subtleties play a role. We particularly discuss applications in Model Theory, considering NIP and o-minimal structures. Our main theorem presents sufficient conditions for the PAC learnability of hypothesis spaces defined over o-minimal expansions of the reals. This class of hypothesis spaces covers all artificial neural networks for binary classification that use commonly employed activation functions like ReLU and the sigmoid function.","authors":["Lothar Sebastian Krapp","Laura Wirth"],"url":"https://arxiv.org/abs/2410.10243"}
{"created":"2025-04-29","title":"Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling","abstract":"Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.","authors":["Wenda Xu","Rujun Han","Zifeng Wang","Long T. Le","Dhruv Madeka","Lei Li","William Yang Wang","Rishabh Agarwal","Chen-Yu Lee","Tomas Pfister"],"url":"https://arxiv.org/abs/2410.11325"}
{"created":"2025-04-29","title":"Open Domain Question Answering with Conflicting Contexts","abstract":"Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts.","authors":["Siyi Liu","Qiang Ning","Kishaloy Halder","Wei Xiao","Zheng Qi","Phu Mon Htut","Yi Zhang","Neha Anna John","Bonan Min","Yassine Benajiba","Dan Roth"],"url":"https://arxiv.org/abs/2410.12311"}
{"created":"2025-04-29","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","abstract":"Recent self-rewarding large language models (LLM) have successfully applied LLM-as-a-Judge to iteratively improve the alignment performance without the need of human annotations for preference data. These methods commonly utilize the same LLM to act as both the policy model (which generates responses) and the reward model (which scores and ranks those responses). The ranked responses are then used as preference pairs to train the LLM via direct alignment technologies (e.g. DPO). However, it is noteworthy that throughout this process, there is no guarantee of accuracy in the rewarding and ranking, which is critical for ensuring accurate rewards and high-quality preference data. Empirical results from relatively small LLMs (e.g., 7B parameters) also indicate that improvements from self-rewarding may diminish after several iterations in certain situations, which we hypothesize is due to accumulated bias in the reward system. This bias can lead to unreliable preference data for training the LLM. To address this issue, we first formulate and analyze the generalized iterative preference fine-tuning framework for self-rewarding language model. We then introduce the regularization to this generalized framework to mitigate the overconfident preference labeling in the self-rewarding process. Based on this theoretical insight, we propose a Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages the consistency of rewards across different iterations to regularize the self-rewarding training, helping the model to learn from more reliable preference data. With this explicit regularization, our empirical results demonstrate the superiority of CREAM in improving both reward consistency and alignment performance. The code is publicly available at https://github.com/Raibows/CREAM.","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"url":"https://arxiv.org/abs/2410.12735"}
{"created":"2025-04-29","title":"AgileRate: Bringing Adaptivity and Robustness to DeFi Lending Markets","abstract":"Decentralized Finance (DeFi) has revolutionized lending by replacing intermediaries with algorithm-driven liquidity pools. However, existing platforms like Aave and Compound rely on static interest rate curves and collateral requirements that struggle to adapt to rapid market changes, leading to inefficiencies in utilization and increased risks of liquidations. In this work, we propose a dynamic model of the lending market based on evolving demand and supply curves, alongside an adaptive interest rate controller that responds in real-time to shifting market conditions. Using a Recursive Least Squares algorithm, our controller tracks the external market and achieves stable utilization, while also controlling default and liquidation risk. We provide theoretical guarantees on the interest rate convergence and utilization stability of our algorithm. We establish bounds on the system's vulnerability to adversarial manipulation compared to static curves, while quantifying the trade-off between adaptivity and adversarial robustness. We propose two complementary approaches to mitigating adversarial manipulation: an algorithmic method that detects extreme demand and supply fluctuations and a market-based strategy that enhances elasticity, potentially via interest rate derivative markets. Our dynamic curve demand/supply model demonstrates a low best-fit error on Aave data, while our interest rate controller significantly outperforms static curve protocols in maintaining optimal utilization and minimizing liquidations.","authors":["Mahsa Bastankhah","Viraj Nadkarni","Xuechao Wang","Pramod Viswanath"],"url":"https://arxiv.org/abs/2410.13105"}
{"created":"2025-04-29","title":"From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization","abstract":"Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, we use existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, we manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. We release our dataset and code at github.com/megagonlabs/Hallucination_MDS.","authors":["Catarina G. Belem","Pouya Pezeshkpour","Hayate Iso","Seiji Maekawa","Nikita Bhutani","Estevam Hruschka"],"url":"https://arxiv.org/abs/2410.13961"}
{"created":"2025-04-29","title":"Multi-Agent LLMs Ensemble for Efficient Atrial Fibrillation Annotation of ECG Reports","abstract":"This study introduces a novel multiagent ensemble method powered by LLMs to address a key challenge in ML - data labeling, particularly in large-scale EHR datasets. Manual labeling of such datasets requires domain expertise and is labor-intensive, time-consuming, expensive, and error-prone. To overcome this bottleneck, we developed an ensemble LLMs method and demonstrated its effectiveness in two real-world tasks: (1) labeling a large-scale unlabeled ECG dataset in MIMIC-IV; (2) identifying social determinants of health (SDOH) from the clinical notes of EHR. Trading off benefits and cost, we selected a pool of diverse open source LLMs with satisfactory performance. We treat each LLM's prediction as a vote and apply a mechanism of majority voting with minimal winning threshold for ensemble. We implemented an ensemble LLMs application for EHR data labeling tasks. By using the ensemble LLMs and natural language processing, we labeled MIMIC-IV ECG dataset of 623,566 ECG reports with an estimated accuracy of 98.2%. We applied the ensemble LLMs method to identify SDOH from social history sections of 1,405 EHR clinical notes, also achieving competitive performance. Our experiments show that the ensemble LLMs can outperform individual LLM even the best commercial one, and the method reduces hallucination errors. From the research, we found that (1) the ensemble LLMs method significantly reduces the time and effort required for labeling large-scale EHR data, automating the process with high accuracy and quality; (2) the method generalizes well to other text data labeling tasks, as shown by its application to SDOH identification; (3) the ensemble of a group of diverse LLMs can outperform or match the performance of the best individual LLM; and (4) the ensemble method substantially reduces hallucination errors. This approach provides a scalable and efficient solution to data-labeling challenges.","authors":["Jingwei Huang","Kuroush Nezafati","Ismael Villanueva-Miranda","Zifan Gu","Yueshuang Xu","Ann Marie Navar","Tingyi Wanyan","Qin Zhou","Bo Yao","Ruichen Rong","Xiaowei Zhan","Guanghua Xiao","Eric D. Peterson","Donghan M. Yang","Wenqi Shi","Yang Xie"],"url":"https://arxiv.org/abs/2410.16543"}
{"created":"2025-04-29","title":"Progressive Compositionality in Text-to-Image Generative Models","abstract":"Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges by optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, ConPair, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose EvoGen, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks.","authors":["Evans Xu Han","Linghao Jin","Xiaofeng Liu","Paul Pu Liang"],"url":"https://arxiv.org/abs/2410.16719"}
{"created":"2025-04-29","title":"Evolution of Societies via Reinforcement Learning","abstract":"The universe involves many independent co-learning agents as an ever-evolving part of our observed environment. Yet, in practice, Multi-Agent Reinforcement Learning (MARL) applications are typically constrained to small, homogeneous populations and remain computationally intensive. We propose a methodology that enables simulating populations of Reinforcement Learning agents at evolutionary scale. More specifically, we derive a fast, parallelizable implementation of Policy Gradient (PG) and Opponent-Learning Awareness (LOLA), tailored for evolutionary simulations where agents undergo random pairwise interactions in stateless normal-form games. We demonstrate our approach by simulating the evolution of very large populations made of heterogeneous co-learning agents, under both naive and advanced learning strategies. In our experiments, 200,000 PG or LOLA agents evolve in the classic games of Hawk-Dove, Stag-Hunt, and Rock-Paper-Scissors. Each game provides distinct insights into how populations evolve under both naive and advanced MARL rules, including compelling ways in which Opponent-Learning Awareness affects social evolution.","authors":["Yann Bouteiller","Karthik Soma","Giovanni Beltrame"],"url":"https://arxiv.org/abs/2410.17466"}
{"created":"2025-04-29","title":"Asynchronous RLHF: Faster and More Efficient Off-Policy RL for Language Models","abstract":"The dominant paradigm for RLHF is online and on-policy RL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, this paradigm is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online but off-policy RLHF: learning on samples from previous iterations of our model which give a worse training signal. We tackle the fundamental challenge in this regime: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we test, online DPO is found to be most robust to off-policy data, and robustness increases with the scale of the policy model. We study further compute optimizations for asynchronous RLHF but find that they come at a performance cost, giving rise to a trade-off. We verify the scalability of asynchronous RLHF by training a general-purpose chatbot from LLaMA 3.1 8B on an instruction-following task ~40% faster than a synchronous run while matching final performance. Finally, we extend our results to math and reasoning to demonstrate asynchronous RL can finetune Rho 1B on GSM8k ~70% faster while matching synchronous accuracy.","authors":["Michael Noukhovitch","Shengyi Huang","Sophie Xhonneux","Arian Hosseini","Rishabh Agarwal","Aaron Courville"],"url":"https://arxiv.org/abs/2410.18252"}
{"created":"2025-04-29","title":"FedBaF: Federated Learning Aggregation Biased by a Foundation Model","abstract":"Foundation models are now a major focus of leading technology organizations due to their ability to generalize across diverse tasks. Existing approaches for adapting foundation models to new applications often rely on Federated Learning (FL) and disclose the foundation model weights to clients when using it to initialize the global model. While these methods ensure client data privacy, they compromise model and information security. In this paper, we introduce Federated Learning Aggregation Biased by a Foundation Model (FedBaF), a novel method for dynamically integrating pre-trained foundation model weights during the FL aggregation phase. Unlike conventional methods, FedBaF preserves the confidentiality of the foundation model while still leveraging its power to train more accurate models, especially in non-IID and adversarial scenarios. Our comprehensive experiments use Pre-ResNet and foundation models like Vision Transformer to demonstrate that FedBaF not only matches, but often surpasses the test accuracy of traditional weight initialization methods by up to 11.4% in IID and up to 15.8% in non-IID settings. Additionally, FedBaF applied to a Transformer-based language model significantly reduced perplexity by up to 39.2%.","authors":["Jong-Ik Park","Srinivasa Pranav","Jos\\'e M. F. Moura","Carlee Joe-Wong"],"url":"https://arxiv.org/abs/2410.18352"}
{"created":"2025-04-29","title":"MetaTrading: An Immersion-Aware Model Trading Framework for Vehicular Metaverse Services","abstract":"Timely updating of Internet of Things (IoT) data is crucial for immersive vehicular metaverse services. However, challenges such as latency caused by massive data transmissions, privacy risks associated with user data, and computational burdens on metaverse service providers (MSPs) hinder continuous collection of high-quality data. To address these issues, we propose an immersion-aware model trading framework that facilitates data provision for services while ensuring privacy through federated learning (FL). Specifically, we first develop a novel multi-dimensional metric, the immersion of model (IoM), which assesses model value comprehensively by considering freshness and accuracy of learning models, as well as the amount and potential value of raw data used for training. Then, we design an incentive mechanism to incentivize metaverse users (MUs) to contribute high-value models under resource constraints. The trading interactions between MSPs and MUs are modeled as an equilibrium problem with equilibrium constraints (EPEC) to analyze and balance their costs and gains, where MSPs as leaders determine rewards, while MUs as followers optimize resource allocation. Furthermore, considering dynamic network conditions and privacy concerns, we formulate the reward decisions of MSPs as a multi-agent Markov decision process. To solve this, we develop a fully distributed dynamic reward algorithm based on deep reinforcement learning, without accessing any private information about MUs and other MSPs. Experimental results demonstrate that the proposed framework outperforms state-of-the-art benchmarks, achieving improvements in IoM of 38.3% and 37.2%, and reductions in training time to reach the target accuracy of 43.5% and 49.8%, on average, for the MNIST and GTSRB datasets, respectively.","authors":["Hongjia Wu","Hui Zeng","Zehui Xiong","Jiawen Kang","Zhiping Cai","Tse-Tin Chan","Dusit Niyato","Zhu Han"],"url":"https://arxiv.org/abs/2410.19665"}
{"created":"2025-04-29","title":"DivShift: Exploring Domain-Specific Distribution Shift in Large-Scale, Volunteer-Collected Biodiversity Datasets","abstract":"Large-scale, volunteer-collected datasets of community-identified natural world imagery like iNaturalist have enabled marked performance gains for fine-grained visual classification of species using machine learning methods. However, such data -- sometimes referred to as citizen science data -- are opportunistic and lack a structured sampling strategy. This volunteer-collected biodiversity data contains geographic, temporal, taxonomic, observers, and sociopolitical biases that can have significant effects on biodiversity model performance, but whose impacts are unclear for fine-grained species recognition performance. Here we introduce Diversity Shift (DivShift), a framework for quantifying the effects of domain-specific distribution shifts on machine learning model performance. To diagnose the performance effects of biases specific to volunteer-collected biodiversity data, we also introduce DivShift - North American West Coast (DivShift-NAWC), a curated dataset of almost 7.5 million iNaturalist images across the western coast of North America partitioned across five types of expert-verified bias. We compare species recognition performance across these bias partitions using a diverse variety of species- and ecosystem-focused accuracy metrics. We observe that these biases confound model performance less than expected from the underlying label distribution shift, and that more data leads to better model performance but the magnitude of these improvements are bias-specific. These findings imply that while the structure within natural world images provides generalization improvements for biodiversity monitoring tasks, the biases present in volunteer-collected biodiversity data can also affect model performance; thus these models should be used with caution in downstream biodiversity monitoring tasks.","authors":["Elena Sierra","Lauren E. Gillespie","Salim Soltani","Moises Exposito-Alonso","Teja Kattenborn"],"url":"https://arxiv.org/abs/2410.19816"}
{"created":"2025-04-29","title":"Centaur: a foundation model of human cognition","abstract":"Establishing a unified theory of cognition has been a major goal of psychology. While there have been previous attempts to instantiate such theories by building computational models, we currently do not have one model that captures the human mind in its entirety. A first step in this direction is to create a model that can predict human behavior in a wide range of settings. Here we introduce Centaur, a computational model that can predict and simulate human behavior in any experiment expressible in natural language. We derived Centaur by finetuning a state-of-the-art language model on a novel, large-scale data set called Psych-101. Psych-101 reaches an unprecedented scale, covering trial-by-trial data from over 60,000 participants performing over 10,000,000 choices in 160 experiments. Centaur not only captures the behavior of held-out participants better than existing cognitive models, but also generalizes to new cover stories, structural task modifications, and entirely new domains. Furthermore, we find that the model's internal representations become more aligned with human neural activity after finetuning. Taken together, our results demonstrate that it is possible to discover computational models that capture human behavior across a wide range of domains. We believe that such models provide tremendous potential for guiding the development of cognitive theories and present a case study to demonstrate this.","authors":["Marcel Binz","Elif Akata","Matthias Bethge","Franziska Br\\\"andle","Fred Callaway","Julian Coda-Forno","Peter Dayan","Can Demircan","Maria K. Eckstein","No\\'emi \\'Eltet\\H{o}","Thomas L. Griffiths","Susanne Haridi","Akshay K. Jagadish","Li Ji-An","Alexander Kipnis","Sreejan Kumar","Tobias Ludwig","Marvin Mathony","Marcelo Mattar","Alireza Modirshanechi","Surabhi S. Nath","Joshua C. Peterson","Milena Rmus","Evan M. Russek","Tankred Saanum","Johannes A. Schubert","Luca M. Schulze Buschoff","Nishad Singhi","Xin Sui","Mirko Thalmann","Fabian Theis","Vuong Truong","Vishaal Udandarao","Konstantinos Voudouris","Robert Wilson","Kristin Witte","Shuchen Wu","Dirk Wulff","Huadong Xiong","Eric Schulz"],"url":"https://arxiv.org/abs/2410.20268"}
{"created":"2025-04-29","title":"A Unified Solution to Diverse Heterogeneities in One-shot Federated Learning","abstract":"One-Shot Federated Learning (OSFL) restricts communication between the server and clients to a single round, significantly reducing communication costs and minimizing privacy leakage risks compared to traditional Federated Learning (FL), which requires multiple rounds of communication. However, existing OSFL frameworks remain vulnerable to distributional heterogeneity, as they primarily focus on model heterogeneity while neglecting data heterogeneity. To bridge this gap, we propose FedHydra, a unified, data-free, OSFL framework designed to effectively address both model and data heterogeneity. Unlike existing OSFL approaches, FedHydra introduces a novel two-stage learning mechanism. Specifically, it incorporates model stratification and heterogeneity-aware stratified aggregation to mitigate the challenges posed by both model and data heterogeneity. By this design, the data and model heterogeneity issues are simultaneously monitored from different aspects during learning. Consequently, FedHydra can effectively mitigate both issues by minimizing their inherent conflicts. We compared FedHydra with five SOTA baselines on four benchmark datasets. Experimental results show that our method outperforms the previous OSFL methods in both homogeneous and heterogeneous settings. Our code is available at https://anonymous.4open.science/r/Fed-SA-A4D7.","authors":["Jun Bai","Yiliao Song","Di Wu","Atul Sajjanhar","Yong Xiang","Wei Zhou","Xiaohui Tao","Yan Li","Yue Li"],"url":"https://arxiv.org/abs/2410.21119"}
{"created":"2025-04-29","title":"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference","abstract":"With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.","authors":["Hanshi Sun","Li-Wen Chang","Wenlei Bao","Size Zheng","Ningxin Zheng","Xin Liu","Harry Dong","Yuejie Chi","Beidi Chen"],"url":"https://arxiv.org/abs/2410.21465"}
{"created":"2025-04-29","title":"Deterministic complexity analysis of Hermitian eigenproblems","abstract":"In this work we revisit the arithmetic and bit complexity of Hermitian eigenproblems. Recently, [BGVKS, FOCS 2020] proved that a (non-Hermitian) matrix can be diagonalized with a randomized algorithm in $O(n^{\\omega}\\log^2(n/\\epsilon))$ arithmetic operations, where $\\omega\\lesssim 2.371$ is the square matrix multiplication exponent, and [Shah, SODA 2025] significantly improved the bit complexity for the Hermitian case. Our main goal is to obtain similar deterministic complexity bounds for various Hermitian eigenproblems. In the Real RAM model, we show that a Hermitian matrix can be diagonalized deterministically in $O(n^{\\omega}\\log(n)+n^2\\mathrm{polylog}(n/\\epsilon))$ arithmetic operations, improving the classic deterministic $\\widetilde O(n^3)$ algorithms, and derandomizing the aforementioned state-of-the-art. The main technical step is a complete, detailed analysis of a well-known divide-and-conquer tridiagonal eigensolver of Gu and Eisenstat [GE95], when accelerated with the Fast Multipole Method, asserting that it can accurately diagonalize a symmetric tridiagonal matrix in nearly-$O(n^2)$ operations. In finite precision, we show that an algorithm by Sch\\\"onhage [Sch72] to reduce a Hermitian matrix to tridiagonal form is stable in the floating point model, using $O(\\log(n/\\epsilon))$ bits of precision. This leads to a deterministic algorithm to compute all the eigenvalues of a Hermitian matrix in $O\\left(n^{\\omega}\\mathcal{F}\\left(\\log(n/\\epsilon)\\right)+n^2\\mathrm{polylog}(n/\\epsilon)\\right)$ bit operations, where $\\mathcal{F}(b)\\in\\widetilde O(b)$ is the bit complexity of a single floating point operation on $b$ bits. This improves the best known $\\widetilde{O}(n^3)$ deterministic and $O\\left(n^{\\omega}\\log^2(n/\\epsilon)\\mathcal{F}\\left(\\log(n/\\epsilon)\\right)\\right)$ randomized complexities. We conclude with some other useful subroutines and with open problems.","authors":["Aleksandros Sobczyk"],"url":"https://arxiv.org/abs/2410.21550"}
{"created":"2025-04-29","title":"Toward Understanding In-context vs. In-weight Learning","abstract":"It has recently been demonstrated empirically that in-context learning emerges in transformers when certain distributional properties are present in the training data, but this ability can also diminish upon further training. We provide a new theoretical understanding of these phenomena by identifying simplified distributional properties that give rise to the emergence and eventual disappearance of in-context learning. We do so by first analyzing a simplified model that uses a gating mechanism to choose between an in-weight and an in-context predictor. Through a combination of a generalization error and regret analysis we identify conditions where in-context and in-weight learning emerge. These theoretical findings are then corroborated experimentally by comparing the behaviour of a full transformer on the simplified distributions to that of the stylized model, demonstrating aligned results. We then extend the study to a full large language model, showing how fine-tuning on various collections of natural language prompts can elicit similar in-context and in-weight learning behaviour.","authors":["Bryan Chan","Xinyi Chen","Andr\\'as Gy\\\"orgy","Dale Schuurmans"],"url":"https://arxiv.org/abs/2410.23042"}
{"created":"2025-04-29","title":"WikiNER-fr-gold: A Gold-Standard NER Corpus","abstract":"We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.","authors":["Danrun Cao (IRISA","EXPRESSION)","Nicolas B\\'echet (IRISA","UBS","EXPRESSION)","Pierre-Fran\\c{c}ois Marteau (IRISA","UBS","EXPRESSION)"],"url":"https://arxiv.org/abs/2411.00030"}
{"created":"2025-04-29","title":"Revealing Floating-Point Accumulation Orders in Software/Hardware Implementations","abstract":"Accumulation-based operations, such as summation and matrix multiplication, are fundamental to numerous computational domains. However, their accumulation orders are often undocumented in existing software and hardware implementations, making it difficult for developers to ensure consistent results across systems. To address this issue, we introduce FPRev, a diagnostic tool designed to reveal the accumulation order in the software and hardware implementations through numerical testing. With FPRev, developers can identify and compare accumulation orders, enabling developers to create reproducible software and verify implementation equivalence.","authors":["Peichen Xie","Yanjie Gao","Yang Wang","Jilong Xue"],"url":"https://arxiv.org/abs/2411.00442"}
{"created":"2025-04-29","title":"Lorentz-Equivariant Quantum Graph Neural Network for High-Energy Physics","abstract":"The rapid data surge from the high-luminosity Large Hadron Collider introduces critical computational challenges requiring novel approaches for efficient data processing in particle physics. Quantum machine learning, with its capability to leverage the extensive Hilbert space of quantum hardware, offers a promising solution. However, current quantum graph neural networks (GNNs) lack robustness to noise and are often constrained by fixed symmetry groups, limiting adaptability in complex particle interaction modeling. This paper demonstrates that replacing the Lorentz Group Equivariant Block modules in LorentzNet with a dressed quantum circuit significantly enhances performance despite using nearly 5.5 times fewer parameters. Additionally, quantum circuits effectively replace MLPs by inherently preserving symmetries, with Lorentz symmetry integration ensuring robust handling of relativistic invariance. Our Lorentz-Equivariant Quantum Graph Neural Network (Lorentz-EQGNN) achieved $74.00\\%$ test accuracy and an AUC of $87.38\\%$ on the Quark-Gluon jet tagging dataset, outperforming the classical and quantum GNNs with a reduced architecture using only 4 qubits. On the Electron-Photon dataset, Lorentz-EQGNN reached $67.00\\%$ test accuracy and an AUC of $68.20\\%$, demonstrating competitive results with just 800 training samples. Evaluation of our model on generic MNIST and FashionMNIST datasets confirmed Lorentz-EQGNN's efficiency, achieving $88.10\\%$ and $74.80\\%$ test accuracy, respectively. Ablation studies validated the impact of quantum components on performance, with notable improvements in background rejection rates over classical counterparts. These results highlight Lorentz-EQGNN's potential for immediate applications in noise-resilient jet tagging, event classification, and broader data-scarce HEP tasks.","authors":["Md Abrar Jahin","Md. Akmol Masud","Md Wahiduzzaman Suva","M. F. Mridha","Nilanjan Dey"],"url":"https://arxiv.org/abs/2411.01641"}
{"created":"2025-04-29","title":"Minder: Faulty Machine Detection for Large-scale Distributed Model Training","abstract":"Large-scale distributed model training requires simultaneous training on up to thousands of machines. Faulty machine detection is critical when an unexpected fault occurs in a machine. From our experience, a training task can encounter two faults per day on average, possibly leading to a halt for hours. To address the drawbacks of the time-consuming and labor-intensive manual scrutiny, we propose Minder, an automatic faulty machine detector for distributed training tasks. The key idea of Minder is to automatically and efficiently detect faulty distinctive monitoring metric patterns, which could last for a period before the entire training task comes to a halt. Minder has been deployed in our production environment for over one year, monitoring daily distributed training tasks where each involves up to thousands of machines. In our real-world fault detection scenarios, Minder can accurately and efficiently react to faults within 3.6 seconds on average, with a precision of 0.904 and F1-score of 0.893.","authors":["Yangtao Deng","Xiang Shi","Zhuo Jiang","Xingjian Zhang","Lei Zhang","Zhang Zhang","Bo Li","Zuquan Song","Hang Zhu","Gaohong Liu","Fuliang Li","Shuguang Wang","Haibin Lin","Jianxi Ye","Minlan Yu"],"url":"https://arxiv.org/abs/2411.01791"}
{"created":"2025-04-29","title":"Free-Mask: A Novel Paradigm of Integration Between the Segmentation Diffusion Model and Image Editing","abstract":"Current semantic segmentation models typically require a substantial amount of manually annotated data, a process that is both time-consuming and resource-intensive. Alternatively, leveraging advanced text-to-image models such as Midjourney and Stable Diffusion has emerged as an efficient strategy, enabling the automatic generation of synthetic data in place of manual annotations. However, previous methods have been limited to generating single-instance images, as the generation of multiple instances with Stable Diffusion has proven unstable. To address this limitation and expand the scope and diversity of synthetic datasets, we propose a framework \\textbf{Free-Mask} that combines a Diffusion Model for segmentation with advanced image editing capabilities, allowing for the integration of multiple objects into images via text-to-image models. Our method facilitates the creation of highly realistic datasets that closely emulate open-world environments while generating accurate segmentation masks. It reduces the labor associated with manual annotation and also ensures precise mask generation. Experimental results demonstrate that synthetic data generated by \\textbf{Free-Mask} enables segmentation models to outperform those trained on real data, especially in zero-shot settings. Notably, \\textbf{Free-Mask} achieves new state-of-the-art results on previously unseen classes in the VOC 2012 benchmark.","authors":["Bo Gao","Jianhui Wang","Xinyuan Song","Yangfan He","Fangxu Xing","Tianyu Shi"],"url":"https://arxiv.org/abs/2411.01819"}
{"created":"2025-04-29","title":"A Guide to Misinformation Detection Data and Evaluation","abstract":"Misinformation is a complex societal issue, and mitigating solutions are difficult to create due to data deficiencies. To address this, we have curated the largest collection of (mis)information datasets in the literature, totaling 75. From these, we evaluated the quality of 36 datasets that consist of statements or claims, as well as the 9 datasets that consist of data in purely paragraph form. We assess these datasets to identify those with solid foundations for empirical work and those with flaws that could result in misleading and non-generalizable results, such as spurious correlations, or examples that are ambiguous or otherwise impossible to assess for veracity. We find the latter issue is particularly severe and affects most datasets in the literature. We further provide state-of-the-art baselines on all these datasets, but show that regardless of label quality, categorical labels may no longer give an accurate evaluation of detection model performance. Finally, we propose and highlight Evaluation Quality Assurance (EQA) as a tool to guide the field toward systemic solutions rather than inadvertently propagating issues in evaluation. Overall, this guide aims to provide a roadmap for higher quality data and better grounded evaluations, ultimately improving research in misinformation detection. All datasets and other artifacts are available at misinfo-datasets.complexdatalab.com.","authors":["Camille Thibault","Jacob-Junqi Tian","Gabrielle Peloquin-Skulski","Taylor Lynn Curtis","James Zhou","Florence Laflamme","Yuxiang Guan","Reihaneh Rabbany","Jean-Fran\\c{c}ois Godbout","Kellin Pelrine"],"url":"https://arxiv.org/abs/2411.05060"}
{"created":"2025-04-29","title":"Enhancing Robustness in Language-Driven Robotics: A Modular Approach to Failure Reduction","abstract":"Recent advances in large language models (LLMs) have led to significant progress in robotics, enabling embodied agents to better understand and execute open-ended tasks. However, existing approaches using LLMs face limitations in grounding their outputs within the physical environment and aligning with the capabilities of the robot. This challenge becomes even more pronounced with smaller language models, which are more computationally efficient but less robust in task planning and execution. In this paper, we present a novel modular architecture designed to enhance the robustness of LLM-driven robotics by addressing these grounding and alignment issues. We formalize the task planning problem within a goal-conditioned POMDP framework, identify key failure modes in LLM-driven planning, and propose targeted design principles to mitigate these issues. Our architecture introduces an ``expected outcomes'' module to prevent mischaracterization of subgoals and a feedback mechanism to enable real-time error recovery. Experimental results, both in simulation and on physical robots, demonstrate that our approach significantly improves task success rates for pick-and-place and manipulation tasks compared to both larger LLMs and standard baselines. Through hardware experiments, we also demonstrate how our architecture can be run efficiently and locally. This work highlights the potential of smaller, locally-executable LLMs in robotics and provides a scalable, efficient solution for robust task execution.","authors":["\\'Emiland Garrab\\'e","Pierre Teixeira","Mahdi Khoramshahi","St\\'ephane Doncieux"],"url":"https://arxiv.org/abs/2411.05474"}
{"created":"2025-04-29","title":"StiffGIPC: Advancing GPU IPC for stiff affine-deformable simulation","abstract":"Incremental Potential Contact (IPC) is a widely used, robust, and accurate method for simulating complex frictional contact behaviors. However, achieving high efficiency remains a major challenge, particularly as material stiffness increases, which leads to slower Preconditioned Conjugate Gradient (PCG) convergence, even with the state-of-the-art preconditioners. In this paper, we propose a fully GPU-optimized IPC simulation framework capable of handling materials across a wide range of stiffnesses, delivering consistent high performance and scalability with up to 10x speedup over state-of-the-art GPU IPC methods. Our framework introduces three key innovations: 1) A novel connectivity-enhanced Multilevel Additive Schwarz (MAS) preconditioner on the GPU, designed to efficiently capture both stiff and soft elastodynamics and improve PCG convergence at a reduced preconditioning cost. 2) A C2-continuous cubic energy with an analytic eigensystem for strain limiting, enabling more parallel-friendly simulations of stiff membranes, such as cloth, without membrane locking. 3) For extremely stiff behaviors where elastic waves are barely visible, we employ affine body dynamics (ABD) with a hash-based multi-layer reduction strategy for fast Hessian assembly and efficient affine-deformable coupling. We conduct extensive performance analyses and benchmark studies to compare our framework against state-of-the-art methods and alternative design choices. Our system consistently delivers the fastest performance across soft, stiff, and hybrid simulation scenarios, even in cases with high resolution, large deformations, and high-speed impacts. Our framework will be fully open-sourced upon acceptance.","authors":["Kemeng Huang","Xinyu Lu","Huancheng Lin","Taku Komura","Minchen Li"],"url":"https://arxiv.org/abs/2411.06224"}
{"created":"2025-04-29","title":"Quasi-random Multi-Sample Inference for Large Language Models","abstract":"Large language models (LLMs) are often equipped with multi-sample decoding strategies. An LLM implicitly defines an arithmetic code book, facilitating efficient and embarrassingly parallelizable \\textbf{arithmetic sampling} to produce multiple samples using quasi-random codes. Traditional text generation methods, such as beam search and sampling-based techniques, have notable limitations: they lack parallelizability or diversity of sampled sequences. This study explores the potential of arithmetic sampling, contrasting it with ancestral sampling across two decoding tasks that employ multi-sample inference: chain-of-thought reasoning with self-consistency and machine translation with minimum Bayes risk decoding. Our results demonstrate that arithmetic sampling produces more diverse samples, significantly improving reasoning and translation performance as the sample size increases. We observe a $\\mathbf{3\\text{-}5\\%}$ point increase in accuracy on the GSM8K dataset and a $\\mathbf{0.45\\text{-}0.89\\%}$ point increment in COMET score for WMT19 tasks using arithmetic sampling without any significant computational overhead.","authors":["Aditya Parashar","Aditya Vikram Singh","Avinash Amballa","Jinlin Lai","Benjamin Rozonoyer"],"url":"https://arxiv.org/abs/2411.06251"}
{"created":"2025-04-29","title":"On the optimal choice of the illumination function in photoacoustic tomography","abstract":"This work studies the inverse problem of photoacoustic tomography (more precisely, the acoustic subproblem) as the identification of a space-dependent source parameter. The model consists of a wave equation involving a time-fractional damping term to account for power law frequency dependence of the attenuation, as relevant in ultrasonics. We solve the inverse problem in a Bayesian framework using a Maximum A Posteriori (MAP) estimate, and for this purpose derive an explicit expression for the adjoint operator. On top of this, we consider optimization of the choice of the laser excitation function, which is the time-dependent part of the source in this model, to enhance the reconstruction result. The method employs the $A$-optimality criterion for Bayesian optimal experimental design with Gaussian prior and Gaussian noise. To efficiently approximate the cost functional, we introduce an approximation scheme based on projection onto finite-dimensional subspaces. Finally, we present numerical results that illustrate the theory.","authors":["Phuoc-Truong Huynh","Barbara Kaltenbacher"],"url":"https://arxiv.org/abs/2411.06609"}
{"created":"2025-04-29","title":"OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision","abstract":"Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/","authors":["Cong Wei","Zheyang Xiong","Weiming Ren","Xinrun Du","Ge Zhang","Wenhu Chen"],"url":"https://arxiv.org/abs/2411.07199"}
{"created":"2025-04-29","title":"On sampling two spin models using the local connective constant","abstract":"This work establishes novel optimum mixing bounds for the Glauber dynamics on the Hard-core and Ising models. These bounds are expressed in terms of the local connective constant of the underlying graph $G$. This is a notion of effective degree for $G$.","authors":["Charilaos Efthymiou"],"url":"https://arxiv.org/abs/2411.08179"}
{"created":"2025-04-29","title":"Fair Resource Allocation in Weakly Coupled Markov Decision Processes","abstract":"We consider fair resource allocation in sequential decision-making environments modeled as weakly coupled Markov decision processes, where resource constraints couple the action spaces of $N$ sub-Markov decision processes (sub-MDPs) that would otherwise operate independently. We adopt a fairness definition using the generalized Gini function instead of the traditional utilitarian (total-sum) objective. After introducing a general but computationally prohibitive solution scheme based on linear programming, we focus on the homogeneous case where all sub-MDPs are identical. For this case, we show for the first time that the problem reduces to optimizing the utilitarian objective over the class of \"permutation invariant\" policies. This result is particularly useful as we can exploit Whittle index policies in the restless bandits setting while, for the more general setting, we introduce a count-proportion-based deep reinforcement learning approach. Finally, we validate our theoretical findings with comprehensive experiments, confirming the effectiveness of our proposed method in achieving fairness.","authors":["Xiaohui Tu","Yossiri Adulyasak","Nima Akbarzadeh","Erick Delage"],"url":"https://arxiv.org/abs/2411.09804"}
{"created":"2025-04-29","title":"I Know What You Sync: Covert and Side Channel Attacks on File Systems via syncfs","abstract":"Operating Systems enforce logical isolation using abstractions such as processes, containers, and isolation technologies to protect a system from malicious or buggy code. In this paper, we show new types of side channels through the file system that break this logical isolation. The file system plays a critical role in the operating system, managing all I/O activities between the application layer and the physical storage device. We observe that the file system implementation is shared, leading to timing leakage when using common I/O system calls. Specifically, we found that modern operating systems take advantage of any flush operation (which saves cached blocks in memory to the SSD or disk) to flush all of the I/O buffers, even those used by other isolation domains. Thus, by measuring the delay of syncfs, the attacker can infer the I/O behavior of victim programs. We then demonstrate a syncfs covert channel attack on multiple file systems, including both Linux native file systems and the Windows file system, achieving a maximum bandwidth of 5 Kbps with an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on Windows. In addition, we construct three side-channel attacks targeting both Linux and Android devices. On Linux devices, we implement a website fingerprinting attack and a video fingerprinting attack by tracking the write patterns of temporary buffering files. On Android devices, we design an application fingerprinting attack that leaks application write patterns during boot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally, we demonstrate that these attacks can be exploited across containers implementing a container detection technique and a cross-container covert channel attack.","authors":["Cheng Gu","Yicheng Zhang","Nael Abu-Ghazaleh"],"url":"https://arxiv.org/abs/2411.10883"}
{"created":"2025-04-29","title":"BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration","abstract":"Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks. Yet the substantial memory footprint of LLMs significantly hinders their deployment. In this paper, we improve the accessibility of LLMs through BitMoD, an algorithm-hardware co-design solution that enables efficient LLM acceleration at low weight precision. On the algorithm side, BitMoD introduces fine-grained data type adaptation that uses a different numerical data type to quantize a group of (e.g., 128) weights. Through the careful design of these new data types, BitMoD is able to quantize LLM weights to very low precision (e.g., 4 bits and 3 bits) while maintaining high accuracy. On the hardware side, BitMoD employs a bit-serial processing element to easily support multiple numerical precisions and data types; our hardware design includes two key innovations: First, it employs a unified representation to process different weight data types, thus reducing the hardware cost. Second, it adopts a bit-serial dequantization unit to rescale the per-group partial sum with minimal hardware overhead. Our evaluation on six representative LLMs demonstrates that BitMoD significantly outperforms state-of-the-art LLM quantization and acceleration methods. For discriminative tasks, BitMoD can quantize LLM weights to 4-bit with $<\\!0.5\\%$ accuracy loss on average. For generative tasks, BitMoD is able to quantize LLM weights to 3-bit while achieving better perplexity than prior LLM quantization scheme. Combining the superior model performance with an efficient accelerator design, BitMoD achieves an average of $1.69\\times$ and $1.48\\times$ speedups compared to prior LLM accelerators ANT and OliVe, respectively.","authors":["Yuzong Chen","Ahmed F. AbouElhamayed","Xilai Dai","Yang Wang","Marta Andronic","George A. Constantinides","Mohamed S. Abdelfattah"],"url":"https://arxiv.org/abs/2411.11745"}
{"created":"2025-04-29","title":"FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training","abstract":"Language-image pre-training faces significant challenges due to limited data in specific formats and the constrained capacities of text encoders. While prevailing methods attempt to address these issues through data augmentation and architecture modifications, they continue to struggle with processing long-form text inputs, and the inherent limitations of traditional CLIP text encoders lead to suboptimal downstream generalization. In this paper, we propose FLAME (Frozen Large lAnguage Models Enable data-efficient language-image pre-training) that leverages frozen large language models as text encoders, naturally processing long text inputs and demonstrating impressive multilingual generalization. FLAME comprises two key components: 1) a multifaceted prompt distillation technique for extracting diverse semantic representations from long captions, which better aligns with the multifaceted nature of images, and 2) a facet-decoupled attention mechanism, complemented by an offline embedding strategy, to ensure efficient computation. Extensive empirical evaluations demonstrate FLAME's superior performance. When trained on CC3M, FLAME surpasses the previous state-of-the-art by 4.9% in ImageNet top-1 accuracy. On YFCC15M, FLAME surpasses the WIT-400M-trained CLIP by 44.4\\% in average image-to-text recall@1 across 36 languages, and by 34.6% in text-to-image recall@1 for long-context retrieval on Urban-1k. Code is available at https://github.com/MIV-XJTU/FLAME.","authors":["Anjia Cao","Xing Wei","Zhiheng Ma"],"url":"https://arxiv.org/abs/2411.11927"}
{"created":"2025-04-29","title":"SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input","abstract":"Stereo video synthesis from a monocular input is a demanding task in the fields of spatial computing and virtual reality. The main challenges of this task lie on the insufficiency of high-quality paired stereo videos for training and the difficulty of maintaining the spatio-temporal consistency between frames. Existing methods primarily address these issues by directly applying novel view synthesis (NVS) techniques to video, while facing limitations such as the inability to effectively represent dynamic scenes and the requirement for large amounts of training data. In this paper, we introduce a novel self-supervised stereo video synthesis paradigm via a video diffusion model, termed SpatialDreamer, which meets the challenges head-on. Firstly, to address the stereo video data insufficiency, we propose a Depth based Video Generation module DVG, which employs a forward-backward rendering mechanism to generate paired videos with geometric and temporal priors. Leveraging data generated by DVG, we propose RefinerNet along with a self-supervised synthetic framework designed to facilitate efficient and dedicated training. More importantly, we devise a consistency control module, which consists of a metric of stereo deviation strength and a Temporal Interaction Learning module TIL for geometric and temporal consistency ensurance respectively. We evaluated the proposed method against various benchmark methods, with the results showcasing its superior performance.","authors":["Zhen Lv","Yangqi Long","Congzhentao Huang","Cao Li","Chengfei Lv","Hao Ren","Dian Zheng"],"url":"https://arxiv.org/abs/2411.11934"}
{"created":"2025-04-29","title":"Enhancing Exploration with Diffusion Policies in Hybrid Off-Policy RL: Application to Non-Prehensile Manipulation","abstract":"Learning diverse policies for non-prehensile manipulation is essential for improving skill transfer and generalization to out-of-distribution scenarios. In this work, we enhance exploration through a two-fold approach within a hybrid framework that tackles both discrete and continuous action spaces. First, we model the continuous motion parameter policy as a diffusion model, and second, we incorporate this into a maximum entropy reinforcement learning framework that unifies both the discrete and continuous components. The discrete action space, such as contact point selection, is optimized through Q-value function maximization, while the continuous part is guided by a diffusion-based policy. This hybrid approach leads to a principled objective, where the maximum entropy term is derived as a lower bound using structured variational inference. We propose the Hybrid Diffusion Policy algorithm (HyDo) and evaluate its performance on both simulation and zero-shot sim2real tasks. Our results show that HyDo encourages more diverse behavior policies, leading to significantly improved success rates across tasks - for example, increasing from 53% to 72% on a real-world 6D pose alignment task. Project page: https://leh2rng.github.io/hydo","authors":["Huy Le","Tai Hoang","Miroslav Gabriel","Gerhard Neumann","Ngo Anh Vien"],"url":"https://arxiv.org/abs/2411.14913"}
{"created":"2025-04-29","title":"Why do Machine Learning Notebooks Crash? An Empirical Study on Public Python Jupyter Notebooks","abstract":"Jupyter notebooks have become central in data science, integrating code, text and output in a flexible environment. With the rise of machine learning (ML), notebooks are increasingly used for prototyping and data analysis. However, due to their dependence on complex ML libraries and the flexible notebook semantics that allow cells to be run in any order, notebooks are susceptible to software bugs that may lead to program crashes. This paper presents a comprehensive empirical study focusing on crashes in publicly available Python ML notebooks. We collect 64,031 notebooks containing 92,542 crashes from GitHub and Kaggle, and manually analyze a sample of 746 crashes across various aspects, including crash types and root causes. Our analysis identifies unique ML-specific crash types, such as tensor shape mismatches and dataset value errors that violate API constraints. Additionally, we highlight unique root causes tied to notebook semantics, including out-of-order execution and residual errors from previous cells, which have been largely overlooked in prior research. Furthermore, we identify the most error-prone ML libraries, and analyze crash distribution across ML pipeline stages. We find that over 40% of crashes stem from API misuse and notebook-specific issues. Crashes frequently occur when using ML libraries like TensorFlow/Keras and Torch. Additionally, over 70% of the crashes occur during data preparation, model training, and evaluation or prediction stages of the ML pipeline, while data visualization errors tend to be unique to ML notebooks.","authors":["Yiran Wang","Willem Meijer","Jos\\'e Antonio Hern\\'andez L\\'opez","Ulf Nilsson","D\\'aniel Varr\\'o"],"url":"https://arxiv.org/abs/2411.16795"}
{"created":"2025-04-29","title":"An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese","abstract":"In this paper, we aimed to develop a neural parser for Vietnamese based on simplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora, VietTreebank and VnDT, had around 15% of constituency and dependency tree pairs that did not adhere to simplified HPSG rules. To attempt to address the issue of the corpora not adhering to simplified HPSG rules, we randomly permuted samples from the training and development sets to make them compliant with simplified HPSG. We then modified the first simplified HPSG Neural Parser for the Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which can encode Vietnamese texts. We conducted experiments on our modified VietTreebank and VnDT corpora. Our extensive experiments showed that the simplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82% for constituency parsing when using the same predicted part-of-speech (POS) tags as the self-attentive constituency parser. Additionally, it outperformed previous studies in dependency parsing with a higher Unlabeled Attachment Score (UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores likely due to our focus on arc permutation without changing the original labels, as we did not consult with a linguistic expert. Lastly, the research findings of this paper suggest that simplified HPSG should be given more attention to linguistic expert when developing treebanks for Vietnamese natural language processing.","authors":["Duc-Vu Nguyen","Thang Chau Phan","Quoc-Nam Nguyen","Kiet Van Nguyen","Ngan Luu-Thuy Nguyen"],"url":"https://arxiv.org/abs/2411.17270"}
{"created":"2025-04-29","title":"The RQR algorithm","abstract":"Pole-swapping algorithms, generalizations of bulge-chasing algorithms, have been shown to be a viable alternative to the bulge-chasing QZ algorithm for solving the generalized eigenvalue problem for a matrix pencil A - {\\lambda}B. It is natural to try to devise a pole-swapping algorithm that solves the standard eigenvalue problem for a single matrix A. This paper introduces such an algorithm and shows that it is competitive with Francis's bulge-chasing QR algorithm.","authors":["Daan Camps","Thomas Mach","Raf Vandebril","David S. Watkins"],"url":"https://arxiv.org/abs/2411.17671"}
{"created":"2025-04-29","title":"Improved parallel derandomization via finite automata with applications","abstract":"A central approach to algorithmic derandomization is the construction of small-support probability distributions that \"fool\" randomized algorithms, often enabling efficient parallel (NC) implementations. An abstraction of this idea is fooling polynomial-space statistical tests computed via finite automata (Sivakumar 2002); this encompasses a wide range of properties including $k$-wise independence and sums of random variables.","authors":["Jeff Giliberti","David G. Harris"],"url":"https://arxiv.org/abs/2411.18028"}
{"created":"2025-04-29","title":"Lifting Motion to the 3D World via 2D Diffusion","abstract":"Estimating 3D motion from 2D observations is a long-standing research challenge. Prior work typically requires training on datasets containing ground truth 3D motions, limiting their applicability to activities well-represented in existing motion capture data. This dependency particularly hinders generalization to out-of-distribution scenarios or subjects where collecting 3D ground truth is challenging, such as complex athletic movements or animal motion. We introduce MVLift, a novel approach to predict global 3D motion -- including both joint rotations and root trajectories in the world coordinate system -- using only 2D pose sequences for training. Our multi-stage framework leverages 2D motion diffusion models to progressively generate consistent 2D pose sequences across multiple views, a key step in recovering accurate global 3D motion. MVLift generalizes across various domains, including human poses, human-object interactions, and animal poses. Despite not requiring 3D supervision, it outperforms prior work on five datasets, including those methods that require 3D supervision.","authors":["Jiaman Li","C. Karen Liu","Jiajun Wu"],"url":"https://arxiv.org/abs/2411.18808"}
{"created":"2025-04-29","title":"RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation through LLM Activation Analysis","abstract":"Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving information from the relevant knowledge database, enabling them to produce responses that are more accurate and contextually appropriate. It is worth noting that the knowledge database, being sourced from publicly available channels such as Wikipedia, inevitably introduces a new attack surface. RAG poisoning involves injecting malicious texts into the knowledge database, ultimately leading to the generation of the attacker's target response (also called poisoned response). However, there are currently limited methods available for detecting such poisoning attacks. We aim to bridge the gap in this work. Particularly, we introduce RevPRAG, a flexible and automated detection pipeline that leverages the activations of LLMs for poisoned response detection. Our investigation uncovers distinct patterns in LLMs' activations when generating correct responses versus poisoned responses. Our results on multiple benchmark datasets and RAG architectures show our approach could achieve 98% true positive rate, while maintaining false positive rates close to 1%.","authors":["Xue Tan","Hao Luan","Mingyu Luo","Xiaoyan Sun","Ping Chen","Jun Dai"],"url":"https://arxiv.org/abs/2411.18948"}
{"created":"2025-04-29","title":"Perception of Visual Content: Differences Between Humans and Foundation Models","abstract":"Human-annotated content is often used to train machine learning (ML) models. However, recently, language and multi-modal foundational models have been used to replace and scale-up human annotator's efforts. This study explores the similarity between human-generated and ML-generated annotations of images across diverse socio-economic contexts (RQ1) and their impact on ML model performance and bias (RQ2). We aim to understand differences in perception and identify potential biases in content interpretation. Our dataset comprises images of people from various geographical regions and income levels, covering various daily activities and home environments. ML captions and human labels show highest similarity at a low-level, i.e., types of words that appear and sentence structures, but all annotations are consistent in how they perceive images across regions. ML Captions resulted in best overall region classification performance, while ML Objects and ML Captions performed best overall for income regression. ML annotations worked best for action categories, while human input was more effective for non-action categories. These findings highlight the notion that both human and machine annotations are important, and that human-generated annotations are yet to be replaceable.","authors":["Nardiena A. Pratama","Shaoyang Fan","Gianluca Demartini"],"url":"https://arxiv.org/abs/2411.18968"}
{"created":"2025-04-29","title":"Reactive Orchestration for Hierarchical Federated Learning Under a Communication Cost Budget","abstract":"Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server. This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC. In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy. Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost). Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria. By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime.","authors":["Ivan \\v{C}ili\\'c","Anna Lackinger","Pantelis Frangoudis","Ivana Podnar \\v{Z}arko","Alireza Furutanpey","Ilir Murturi","Schahram Dustdar"],"url":"https://arxiv.org/abs/2412.03385"}
{"created":"2025-04-29","title":"A Hybrid Deep-Learning Model for El Ni\\~no Southern Oscillation in the Low-Data Regime","abstract":"While deep-learning models have demonstrated skillful El Ni\\~no Southern Oscillation (ENSO) forecasts up to one year in advance, they are predominantly trained on climate model simulations that provide thousands of years of training data at the expense of introducing climate model biases. Simpler Linear Inverse Models (LIMs) trained on the much shorter observational record also make skillful ENSO predictions but do not capture predictable nonlinear processes. This motivates a hybrid approach, combining the LIMs modest data needs with a deep-learning non-Markovian correction of the LIM. For O(100 yr) datasets, our resulting Hybrid model is more skillful than the LIM while also exceeding the skill of a full deep-learning model. Additionally, while the most predictable ENSO events are still identified in advance by the LIM, they are better predicted by the Hybrid model, especially in the western tropical Pacific for leads beyond about 9 months, by capturing the subsequent asymmetric (warm versus cold phases) evolution of ENSO.","authors":["Jakob Schloer","Matthew Newman","Jannik Thuemmel","Antonietta Capotondi","Bedartha Goswami"],"url":"https://arxiv.org/abs/2412.03743"}
{"created":"2025-04-29","title":"Pre-train, Align, and Disentangle: Empowering Sequential Recommendation with Large Language Models","abstract":"Sequential Recommendation (SR) aims to leverage the sequential patterns in users' historical interactions to accurately track their preferences. However, the primary reliance of existing SR methods on collaborative data results in challenges such as the cold-start problem and sub-optimal performance. Concurrently, despite the proven effectiveness of large language models (LLMs), their integration into commercial recommender systems is impeded by issues such as high inference latency, incomplete capture of all distribution statistics, and catastrophic forgetting. To address these issues, we introduce a novel Pre-train, Align, and Disentangle (PAD) framework to enhance SR models with LLMs. In particular, we initially pre-train both the SR and LLM models to obtain collaborative and textual embeddings. Subsequently, we propose a characteristic recommendation-anchored alignment loss using multi-kernel maximum mean discrepancy with Gaussian kernels. Lastly, a triple-experts architecture, comprising aligned and modality-specific experts with disentangled embeddings, is fine-tuned in a frequency-aware manner. Experimental results on three public datasets validate the efficacy of PAD, indicating substantial enhancements and compatibility with various SR backbone models, particularly for cold items. The code and datasets are accessible for reproduction at https://github.com/Applied-Machine-Learning-Lab/PAD.","authors":["Yuhao Wang","Junwei Pan","Pengyue Jia","Wanyu Wang","Maolin Wang","Zhixiang Feng","Xiaotian Li","Jie Jiang","Xiangyu Zhao"],"url":"https://arxiv.org/abs/2412.04107"}
{"created":"2025-04-29","title":"Hidden in the Noise: Two-Stage Robust Watermarking for Images","abstract":"As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.","authors":["Kasra Arabi","Benjamin Feuer","R. Teal Witter","Chinmay Hegde","Niv Cohen"],"url":"https://arxiv.org/abs/2412.04653"}
{"created":"2025-04-29","title":"A Unified Approach for Multi-Granularity Search over Spatial Datasets","abstract":"There has been increased interest in data search as a means to find relevant datasets or data points in data lakes and repositories. Although approaches have been proposed to support spatial dataset search and data point search, they consider the two types of searches independently. To enable search operations ranging from the coarse-grained dataset level to the fine-grained data point level, we provide an integrated one that supports diverse query types and distance metrics. In this paper, we focus on designing a multi-granularity spatial data search system, called Spadas, that supports both dataset and data point search operations. To address the challenges of the high cost of indexing and susceptibility to outliers, we propose a unified index that can drastically improve query efficiency in various scenarios by organizing data reasonably and removing outliers in datasets. Moreover, to accelerate all data search operations, we propose a set of pruning mechanisms based on the unified index, including fast bound estimation, approximation technique with error bound, and pruning in batch techniques, to effectively filter out non-relevant datasets and points. Finally, we report the results of a detailed experimental evaluation using six spatial data repositories, achieving orders of magnitude faster than the state-of-the-art algorithms and demonstrating the effectiveness by case study. An online spatial data search system of Spadas is also implemented and made accessible to users.","authors":["Wenzhe Yang","Sheng Wang","Shixun Huang","Yuyang Liao","Yuan Sun","Juliana Freire","Zhiyong Peng"],"url":"https://arxiv.org/abs/2412.04805"}
{"created":"2025-04-29","title":"Achieving Group Fairness through Independence in Predictive Process Monitoring","abstract":"Predictive process monitoring focuses on forecasting future states of ongoing process executions, such as predicting the outcome of a particular case. In recent years, the application of machine learning models in this domain has garnered significant scientific attention. When using historical execution data, which may contain biases or exhibit unfair behavior, these biases may be encoded into the trained models. Consequently, when such models are deployed to make decisions or guide interventions for new cases, they risk perpetuating this unwanted behavior. This work addresses group fairness in predictive process monitoring by investigating independence, i.e. ensuring predictions are unaffected by sensitive group membership. We explore independence through metrics for demographic parity such as $\\Delta$DP, as well as recently introduced, threshold-independent distribution-based alternatives. Additionally, we propose a composite loss function existing of binary cross-entropy and a distribution-based loss (Wasserstein) to train models that balance predictive performance and fairness, and allow for customizable trade-offs. The effectiveness of both the fairness metrics and the composite loss functions is validated through a controlled experimental setup.","authors":["Jari Peeperkorn","Simon De Vos"],"url":"https://arxiv.org/abs/2412.04914"}
{"created":"2025-04-29","title":"Americans' Support for AI Development -- Measured Daily with Open Data and Methods","abstract":"A confluence of maturing Web technologies and Web platforms affords a new form of scientific communication: free and open nowcasting of public opinion. Here, I present the first open-source system to do so. The automated system gathers new human responses to survey items daily, anonymizes and publicly distributes microdata, and presents analyses through a publicly viewable Web dashboard. A demonstration implementation tracked support for further development of artificial intelligence at daily resolution. As of 2025-04-28, the system had collected 4805 responses and autonomously produced daily and monthly estimates of support. Three trends emerged: On average, American adults increasingly supported further development of AI. A crossover interaction of political party affiliation and time suggests AI support changed at different rates for Democrats and Republicans. Those generally less willing to takes risks were less supportive of AI development. I argue that more scientists should adopt the method of open nowcasting, because it encourages transparency in research design and eases replication.","authors":["Jason Jeffrey Jones"],"url":"https://arxiv.org/abs/2412.05163"}
{"created":"2025-04-29","title":"Incentivized Symbiosis: A Paradigm for Human-Agent Coevolution","abstract":"Cooperation is vital to our survival and progress. Evolutionary game theory offers a lens to understand the structures and incentives that enable cooperation to be a successful strategy. As artificial intelligence agents become integral to human systems, the dynamics of cooperation take on unprecedented significance. The convergence of human-agent teaming, contract theory, and decentralized frameworks like Web3, grounded in transparency, accountability, and trust, offers a foundation for fostering cooperation by establishing enforceable rules and incentives for humans and AI agents. We conceptualize Incentivized Symbiosis as a social contract between humans and AI, inspired by Web3 principles and encoded in blockchain technology, to define and enforce rules, incentives, and consequences for both parties. By exploring this paradigm, we aim to catalyze new research at the intersection of systems thinking in AI, Web3, and society, fostering innovative pathways for cooperative human-agent coevolution.","authors":["Tomer Jordi Chaffer","Justin Goldston","Gemach D. A. T. A. I"],"url":"https://arxiv.org/abs/2412.06855"}
{"created":"2025-04-29","title":"Should We Learn Contact-Rich Manipulation Policies from Sampling-Based Planners?","abstract":"The tremendous success of behavior cloning (BC) in robotic manipulation has been largely confined to tasks where demonstrations can be effectively collected through human teleoperation. However, demonstrations for contact-rich manipulation tasks that require complex coordination of multiple contacts are difficult to collect due to the limitations of current teleoperation interfaces. We investigate how to leverage model-based planning and optimization to generate training data for contact-rich dexterous manipulation tasks. Our analysis reveals that popular sampling-based planners like rapidly exploring random tree (RRT), while efficient for motion planning, produce demonstrations with unfavorably high entropy. This motivates modifications to our data generation pipeline that prioritizes demonstration consistency while maintaining solution diversity. Combined with a diffusion-based goal-conditioned BC approach, our method enables effective policy learning and zero-shot transfer to hardware for two challenging contact-rich manipulation tasks.","authors":["Huaijiang Zhu","Tong Zhao","Xinpei Ni","Jiuguang Wang","Kuan Fang","Ludovic Righetti","Tao Pang"],"url":"https://arxiv.org/abs/2412.09743"}
{"created":"2025-04-29","title":"Temporal Logic Control for Nonlinear Stochastic Systems Under Unknown Disturbances","abstract":"In this paper, we present a novel framework to synthesize robust strategies for discrete-time nonlinear systems with random disturbances that are unknown, against temporal logic specifications. The proposed framework is data-driven and abstraction-based: leveraging observations of the system, our approach learns a high-confidence abstraction of the system in the form of an uncertain Markov decision process (UMDP). The uncertainty in the resulting UMDP is used to formally account for both the error in abstracting the system and for the uncertainty coming from the data. Critically, we show that for any given state-action pair in the resulting UMDP, the uncertainty in the transition probabilities can be represented as a convex polytope obtained by a two-layer state discretization and concentration inequalities. This allows us to obtain tighter uncertainty estimates compared to existing approaches, and guarantees efficiency, as we tailor a synthesis algorithm exploiting the structure of this UMDP. We empirically validate our approach on several case studies, showing substantially improved performance compared to the state-of-the-art.","authors":["Ibon Gracia","Luca Laurenti","Manuel Mazo Jr.","Alessandro Abate","Morteza Lahijanian"],"url":"https://arxiv.org/abs/2412.11343"}
{"created":"2025-04-29","title":"Leveraging Large Language Models for Effective Label-free Node Classification in Text-Attributed Graphs","abstract":"Graph neural networks (GNNs) have become the preferred models for node classification in graph data due to their robust capabilities in integrating graph structures and attributes. However, these models heavily depend on a substantial amount of high-quality labeled data for training, which is often costly to obtain. With the rise of large language models (LLMs), a promising approach is to utilize their exceptional zero-shot capabilities and extensive knowledge for node labeling. Despite encouraging results, this approach either requires numerous queries to LLMs or suffers from reduced performance due to noisy labels generated by LLMs. To address these challenges, we introduce Locle, an active self-training framework that does Label-free node Classification with LLMs cost-Effectively. Locle iteratively identifies small sets of \"critical\" samples using GNNs and extracts informative pseudo-labels for them with both LLMs and GNNs, serving as additional supervision signals to enhance model training. Specifically, Locle comprises three key components: (i) an effective active node selection strategy for initial annotations; (ii) a careful sample selection scheme to identify \"critical\" nodes based on label disharmonicity and entropy; and (iii) a label refinement module that combines LLMs and GNNs with a rewired topology. Extensive experiments on five benchmark text-attributed graph datasets demonstrate that Locle significantly outperforms state-of-the-art methods under the same query budget to LLMs in terms of label-free node classification. Notably, on the DBLP dataset with 14.3k nodes, Locle achieves an 8.08% improvement in accuracy over the state-of-the-art at a cost of less than one cent. Our code is available at https://github.com/HKBU-LAGAS/Locle.","authors":["Taiyan Zhang","Renchi Yang","Yurui Lai","Mingyu Yan","Xiaochun Ye","Dongrui Fan"],"url":"https://arxiv.org/abs/2412.11983"}
{"created":"2025-04-29","title":"Wonderland: Navigating 3D Scenes from a Single Image","abstract":"How can one efficiently generate high-quality, wide-scope 3D scenes from arbitrary single images? Existing methods suffer several drawbacks, such as requiring multi-view data, time-consuming per-scene optimization, distorted geometry in occluded areas, and low visual quality in backgrounds. Our novel 3D scene reconstruction pipeline overcomes these limitations to tackle the aforesaid challenge. Specifically, we introduce a large-scale reconstruction model that leverages latents from a video diffusion model to predict 3D Gaussian Splattings of scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that encode multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive learning strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets affirm that our model significantly outperforms existing single-view 3D scene generation methods, especially with out-of-domain images. Thus, we demonstrate for the first time that a 3D reconstruction model can effectively be built upon the latent space of a diffusion model in order to realize efficient 3D scene generation.","authors":["Hanwen Liang","Junli Cao","Vidit Goel","Guocheng Qian","Sergei Korolev","Demetri Terzopoulos","Konstantinos N. Plataniotis","Sergey Tulyakov","Jian Ren"],"url":"https://arxiv.org/abs/2412.12091"}
{"created":"2025-04-29","title":"TrainMover: An Interruption-Resilient and Reliable ML Training Runtime","abstract":"Large-scale ML training jobs are frequently interrupted by hardware and software anomalies, failures, and management events. Existing solutions like checkpointing or runtime reconfiguration suffer from long downtimes, degraded performance, or undesired changes to training strategies. We present TrainMover, a resilient runtime that leverages standby machines to handle interruptions with minimal downtime and zero memory overhead. To achieve these goals, TrainMover introduces two key techniques: two-phase, delta-based communication group setups and communication-free sandboxed shadow iterations. Our evaluation shows that TrainMover consistently achieves second-level downtime across all evaluated models during migration, maintaining 99\\% training efficiency during periodic 10-minute rebalancing. We also demonstrate the effectiveness of TrainMover in handling various interruptions.","authors":["ChonLam Lao","Minlan Yu","Aditya Akella","Jiamin Cao","Yu Guan","Pengcheng Zhang","Zhilong Zheng","Yichi Xu","Ennan Zhai","Dennis Cai","Jiaqi Gao"],"url":"https://arxiv.org/abs/2412.12636"}
{"created":"2025-04-29","title":"Closing the Gap: A User Study on the Real-world Usefulness of AI-powered Vulnerability Detection & Repair in the IDE","abstract":"This paper presents the first empirical study of a vulnerability detection and fix tool with professional software developers on real projects that they own. We implemented DeepVulGuard, an IDE-integrated tool based on state-of-the-art detection and fix models, and show that it has promising performance on benchmarks of historic vulnerability data. DeepVulGuard scans code for vulnerabilities (including identifying the vulnerability type and vulnerable region of code), suggests fixes, provides natural-language explanations for alerts and fixes, leveraging chat interfaces. We recruited 17 professional software developers at Microsoft, observed their usage of the tool on their code, and conducted interviews to assess the tool's usefulness, speed, trust, relevance, and workflow integration. We also gathered detailed qualitative feedback on users' perceptions and their desired features. Study participants scanned a total of 24 projects, 6.9k files, and over 1.7 million lines of source code, and generated 170 alerts and 50 fix suggestions. We find that although state-of-the-art AI-powered detection and fix tools show promise, they are not yet practical for real-world use due to a high rate of false positives and non-applicable fixes. User feedback reveals several actionable pain points, ranging from incomplete context to lack of customization for the user's codebase. Additionally, we explore how AI features, including confidence scores, explanations, and chat interaction, can apply to vulnerability detection and fixing. Based on these insights, we offer practical recommendations for evaluating and deploying AI detection and fix models. Our code and data are available at https://doi.org/10.6084/m9.figshare.26367139.","authors":["Benjamin Steenhoek","Kalpathy Sivaraman","Renata Saldivar Gonzalez","Yevhen Mohylevskyy","Roshanak Zilouchian Moghaddam","Wei Le"],"url":"https://arxiv.org/abs/2412.14306"}
{"created":"2025-04-29","title":"Semantic Foundations of Reductive Reasoning","abstract":"The development of logic has largely been through the 'deductive' paradigm: conclusions are inferred from established premisses. However, the use of logic in the context of both human and machine reasoning is typically through the dual 'reductive' perspective: collections of sufficient premisses are generated from putative conclusions. We call this paradigm, 'reductive logic'. This expression of logic encompass as diverse reasoning activities as proving a formula in a formal system to seeking to meet a friend before noon on Saturday. This paper is a semantical analysis of reductive logic. In particular, we provide mathematical foundations for representing and reasoning about 'reduction operators'. Heuristically, reduction operators may be thought of as `backwards' inference rules. In this paper, we address their mathematical representation, how they are used in the context of reductive reasoning, and, crucially, what makes them 'valid'.","authors":["Alexander V. Gheorghiu","David J. Pym"],"url":"https://arxiv.org/abs/2412.14758"}
{"created":"2025-04-29","title":"A Multi-modal Approach to Dysarthria Detection and Severity Assessment Using Speech and Text Information","abstract":"Automatic detection and severity assessment of dysarthria are crucial for delivering targeted therapeutic interventions to patients. While most existing research focuses primarily on speech modality, this study introduces a novel approach that leverages both speech and text modalities. By employing cross-attention mechanism, our method learns the acoustic and linguistic similarities between speech and text representations. This approach assesses specifically the pronunciation deviations across different severity levels, thereby enhancing the accuracy of dysarthric detection and severity assessment. All the experiments have been performed using UA-Speech dysarthric database. Improved accuracies of 99.53% and 93.20% in detection, and 98.12% and 51.97% for severity assessment have been achieved when speaker-dependent and speaker-independent, unseen and seen words settings are used. These findings suggest that by integrating text information, which provides a reference linguistic knowledge, a more robust framework has been developed for dysarthric detection and assessment, thereby potentially leading to more effective diagnoses.","authors":["Anuprabha M","Krishna Gurugubelli","V Kesavaraj","Anil Kumar Vuppala"],"url":"https://arxiv.org/abs/2412.16874"}
{"created":"2025-04-29","title":"Investigating Length Issues in Document-level Machine Translation","abstract":"Transformer architectures are increasingly effective at processing and generating very long chunks of texts, opening new perspectives for document-level machine translation (MT). In this work, we challenge the ability of MT systems to handle texts comprising up to several thousands of tokens. We design and implement a new approach designed to precisely measure the effect of length increments on MT outputs. Our experiments with two representative architectures unambiguously show that (a)~translation performance decreases with the length of the input text; (b)~the position of sentences within the document matters, and translation quality is higher for sentences occurring earlier in a document. We further show that manipulating the distribution of document lengths and of positional embeddings only marginally mitigates such problems. Our results suggest that even though document-level MT is computationally feasible, it does not yet match the performance of sentence-based MT.","authors":["Ziqian Peng","Rachel Bawden","Fran\\c{c}ois Yvon"],"url":"https://arxiv.org/abs/2412.17592"}
{"created":"2025-04-29","title":"LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context","abstract":"While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.","authors":["Kai Ruan","Xuan Wang","Jixiang Hong","Peng Wang","Yang Liu","Hao Sun"],"url":"https://arxiv.org/abs/2412.17596"}
{"created":"2025-04-29","title":"LMV-RPA: Large Model Voting-based Robotic Process Automation","abstract":"Automating high-volume unstructured data processing is essential for operational efficiency. Optical Character Recognition (OCR) is critical but often struggles with accuracy and efficiency in complex layouts and ambiguous text. These challenges are especially pronounced in large-scale tasks requiring both speed and precision. This paper introduces LMV-RPA, a Large Model Voting-based Robotic Process Automation system to enhance OCR workflows. LMV-RPA integrates outputs from OCR engines such as Paddle OCR, Tesseract OCR, Easy OCR, and DocTR with Large Language Models (LLMs) like LLaMA 3 and Gemini-1.5-pro. Using a majority voting mechanism, it processes OCR outputs into structured JSON formats, improving accuracy, particularly in complex layouts. The multi-phase pipeline processes text extracted by OCR engines through LLMs, combining results to ensure the most accurate outputs. LMV-RPA achieves 99 percent accuracy in OCR tasks, surpassing baseline models with 94 percent, while reducing processing time by 80 percent. Benchmark evaluations confirm its scalability and demonstrate that LMV-RPA offers a faster, more reliable, and efficient solution for automating large-scale document processing tasks.","authors":["Osama Abdellatif","Ahmed Ayman","Ali Hamdi"],"url":"https://arxiv.org/abs/2412.17965"}
{"created":"2025-04-29","title":"FineVQ: Fine-Grained User Generated Content Video Quality Assessment","abstract":"The rapid growth of user-generated content (UGC) videos has produced an urgent need for effective video quality assessment (VQA) algorithms to monitor video quality and guide optimization and recommendation procedures. However, current VQA models generally only give an overall rating for a UGC video, which lacks fine-grained labels for serving video processing and recommendation applications. To address the challenges and promote the development of UGC videos, we establish the first large-scale Fine-grained Video quality assessment Database, termed FineVD, which comprises 6104 UGC videos with fine-grained quality scores and descriptions across multiple dimensions. Based on this database, we propose a Fine-grained Video Quality assessment (FineVQ) model to learn the fine-grained quality of UGC videos, with the capabilities of quality rating, quality scoring, and quality attribution. Extensive experimental results demonstrate that our proposed FineVQ can produce fine-grained video-quality results and achieve state-of-the-art performance on FineVD and other commonly used UGC-VQA datasets.","authors":["Huiyu Duan","Qiang Hu","Jiarui Wang","Liu Yang","Zitong Xu","Lu Liu","Xiongkuo Min","Chunlei Cai","Tianxiao Ye","Xiaoyun Zhang","Guangtao Zhai"],"url":"https://arxiv.org/abs/2412.19238"}
{"created":"2025-04-29","title":"SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object Detection","abstract":"While modern visual recognition systems have made significant advancements, many continue to struggle with the open problem of learning from few exemplars. This paper focuses on the task of object detection in the setting where object classes follow a natural long-tailed distribution. Existing methods for long-tailed detection resort to external ImageNet labels to augment the low-shot training instances. However, such dependency on a large labeled database has limited utility in practical scenarios. We propose a versatile and scalable approach to leverage optional unlabeled images, which are easy to collect without the burden of human annotations. Our SimLTD framework is straightforward and intuitive, and consists of three simple steps: (1) pre-training on abundant head classes; (2) transfer learning on scarce tail classes; and (3) fine-tuning on a sampled set of both head and tail classes. Our approach can be viewed as an improved head-to-tail model transfer paradigm without the added complexities of meta-learning or knowledge distillation, as was required in past research. By harnessing supplementary unlabeled images, without extra image labels, SimLTD establishes new record results on the challenging LVIS v1 benchmark across both supervised and semi-supervised settings.","authors":["Phi Vu Tran"],"url":"https://arxiv.org/abs/2412.20047"}
{"created":"2025-04-29","title":"Interruption Handling for Conversational Robots","abstract":"Interruptions, a fundamental component of human communication, can enhance the dynamism and effectiveness of conversations, but only when effectively managed by all parties involved. Despite advancements in robotic systems, state-of-the-art systems still have limited capabilities in handling user-initiated interruptions in real-time. Prior research has primarily focused on post hoc analysis of interruptions. To address this gap, we present a system that detects user-initiated interruptions and manages them in real-time based on the interrupter's intent (i.e., cooperative agreement, cooperative assistance, cooperative clarification, or disruptive interruption). The system was designed based on interaction patterns identified from human-human interaction data. We integrated our system into an LLM-powered social robot and validated its effectiveness through a timed decision-making task and a contentious discussion task with 21 participants. Our system successfully handled 93.69% (n=104/111) of user-initiated interruptions. We discuss our learnings and their implications for designing interruption-handling behaviors in conversational robots.","authors":["Shiye Cao","Jiwon Moon","Amama Mahmood","Victor Nikhil Antony","Ziang Xiao","Anqi Liu","Chien-Ming Huang"],"url":"https://arxiv.org/abs/2501.01568"}
{"created":"2025-04-29","title":"CrossView-GS: Cross-view Gaussian Splatting For Large-scale Scene Reconstruction","abstract":"3D Gaussian Splatting (3DGS) leverages densely distributed Gaussian primitives for high-quality scene representation and reconstruction. While existing 3DGS methods perform well in scenes with minor view variation, large view changes from cross-view data pose optimization challenges for these methods. To address these issues, we propose a novel cross-view Gaussian Splatting method for large-scale scene reconstruction based on multi-branch construction and fusion. Our method independently reconstructs models from different sets of views as multiple independent branches to establish the baselines of Gaussian distribution, providing reliable priors for cross-view reconstruction during initialization and densification. Specifically, a gradient-aware regularization strategy is introduced to mitigate smoothing issues caused by significant view disparities. Additionally, a unique Gaussian supplementation strategy is utilized to incorporate complementary information of multi-branch into the cross-view model. Extensive experiments on benchmark datasets demonstrate that our method achieves superior performance in novel view synthesis compared to state-of-the-art methods.","authors":["Chenhao Zhang","Yuanping Cao","Lei Zhang"],"url":"https://arxiv.org/abs/2501.01695"}
{"created":"2025-04-29","title":"Unsupervised Tomato Split Anomaly Detection using Hyperspectral Imaging and Variational Autoencoders","abstract":"Tomato anomalies/damages pose a significant challenge in greenhouse farming. While this method of cultivation benefits from efficient resource utilization, anomalies can significantly degrade the quality of farm produce. A common anomaly associated with tomatoes is splitting, characterized by the development of cracks on the tomato skin, which degrades its quality. Detecting this type of anomaly is challenging due to dynamic variations in appearance and sizes, compounded by dataset scarcity. We address this problem in an unsupervised manner by utilizing a tailored variational autoencoder (VAE) with hyperspectral input. Preliminary analysis of the dataset enabled us to select the optimal range of wavelengths for detecting this anomaly. Our findings indicate that the 530nm - 550nm range is suitable for identifying tomato dry splits. The proposed VAE model achieved a 97% detection accuracy for tomato split anomalies in the test data. The analysis on reconstruction loss allow us to not only detect the anomalies but also to some degree estimate the anomalous regions.","authors":["Mahmoud Abdulsalam","Usman Zahidi","Bradley Hurst","Simon Pearson","Grzegorz Cielniak","James Brown"],"url":"https://arxiv.org/abs/2501.02921"}
{"created":"2025-04-29","title":"Multi-Source Urban Traffic Flow Forecasting with Drone and Loop Detector Data","abstract":"Traffic forecasting is a fundamental task in transportation research, however the scope of current research has mainly focused on a single data modality of loop detectors. Recently, the advances in Artificial Intelligence and drone technologies have made possible novel solutions for efficient, accurate and flexible aerial observations of urban traffic. As a promising traffic monitoring approach, drone-captured data can create an accurate multi-sensor mobility observatory for large-scale urban networks, when combined with existing infrastructure. Therefore, this paper investigates the problem of multi-source traffic speed prediction, simultaneously using drone and loop detector data. A simple yet effective graph-based model HiMSNet is proposed to integrate multiple data modalities and learn spatio-temporal correlations. Detailed analysis shows that predicting accurate segment-level speed is more challenging than the regional speed, especially under high-demand scenarios with heavier congestions and varying traffic dynamics. Utilizing both drone and loop detector data, the prediction accuracy can be improved compared to single-modality cases, when the sensors have lower coverages and are subject to noise. Our simulation study based on vehicle trajectories in a real urban road network has highlighted the added value of integrating drones in traffic forecasting and monitoring.","authors":["Weijiang Xiong","Robert Fonod","Alexandre Alahi","Nikolas Geroliminis"],"url":"https://arxiv.org/abs/2501.03492"}
{"created":"2025-04-29","title":"Transparent Transformations for Timing Side-Channel Analysis","abstract":"Side-channel analysis frameworks often lift programs into an intermediate representation (IR) before analyzing them. IRs are subject to transformations, which we call ex ante (XA) transformations, that improve the efficiency and accuracy of the analysis, thereby directly impacting the validity of analysis results. This paper explores the impact of XA transformations in the setting of timing-based side-channel analysis frameworks, focusing on the constant-time policy that many cryptographic libraries adopt.","authors":["Santiago Arranz-Olmos","Gilles Barthe","Lionel Blatter","S\\\"oren van der Wall","Zhiyuan Zhang"],"url":"https://arxiv.org/abs/2501.04183"}
{"created":"2025-04-29","title":"Extracting Participation in Collective Action from Social Media","abstract":"Social media play a key role in mobilizing collective action, holding the potential for studying the pathways that lead individuals to actively engage in addressing global challenges. However, quantitative research in this area has been limited by the absence of granular and large-scale ground truth about the level of participation in collective action among individual social media users. To address this limitation, we present a novel suite of text classifiers designed to identify expressions of participation in collective action from social media posts, in a topic-agnostic fashion. Grounded in the theoretical framework of social movement mobilization, our classification captures participation and categorizes it into four levels: recognizing collective issues, engaging in calls-to-action, expressing intention of action, and reporting active involvement. We constructed a labeled training dataset of Reddit comments through crowdsourcing, which we used to train BERT classifiers and fine-tune Llama3 models. Our findings show that smaller language models can reliably detect expressions of participation (weighted F1=0.71), and rival larger models in capturing nuanced levels of participation. By applying our methodology to Reddit, we illustrate its effectiveness as a robust tool for characterizing online communities in innovative ways compared to topic modeling, stance detection, and keyword-based methods. Our framework contributes to Computational Social Science research by providing a new source of reliable annotations useful for investigating the social dynamics of collective action.","authors":["Arianna Pera","Luca Maria Aiello"],"url":"https://arxiv.org/abs/2501.07368"}
{"created":"2025-04-29","title":"Joint Detection and Decoding: A Graph Neural Network Approach","abstract":"Narrowing the performance gap between optimal and feasible detection in inter-symbol interference (ISI) channels, this paper proposes to use graph neural networks (GNNs) for detection that can also be used to perform joint detection and decoding (JDD). For detection, the GNN is build upon the factor graph representations of the channel, while for JDD, the factor graph is expanded by the Tanner graph of the parity-check matrix (PCM) of the channel code, sharing the variable nodes (VNs). A particularly advantageous property of the GNN is a) the robustness against cycles in the factor graphs which is the main problem for sum-product algorithm (SPA)-based detection, and b) the robustness against channel state information (CSI) uncertainty at the receiver. Additionally, we propose using an input embedding resulting in a GNN independent of the channel impulse response (CIR). Consequently, a fully deep learning-based receiver enables joint optimization instead of individual optimization of the components, so-called end-to-end learning. Furthermore, we propose a parallel flooding schedule that also reduces the latency, which turns out to improve the error correcting performance. The proposed approach is analyzed and compared to state-of-the-art baselines for different modulations and codes in terms of error correcting capability and latency. The gain compared to SPA-based detection might be explained with improved messages between nodes and adaptive damping of messages. For a higher order modulation in a high-rate turbo detection and decoding (TDD) scenario the GNN shows a, at first glance, surprisingly high gain of 6.25 dB compared to the best, feasible non-neural baseline.","authors":["Jannis Clausius","Marvin R\\\"ubenacke","Daniel Tandler","Stephan ten Brink"],"url":"https://arxiv.org/abs/2501.08871"}
{"created":"2025-04-29","title":"Degradedness Under Cooperation","abstract":"We study cooperation problems in broadcast and relay networks, where the receivers do not satisfy the classical physical degradedness assumptions. New notions of degradedness, strongly less noisy and strongly more capable are introduced. We show that under these conditions, decode and forward (D&amp;F) is optimal for classes of cooperative systems with limited conference rates, thus yielding new capacity results for these systems. In particular, we derive bounds on the capacity region of a class of broadcast channels with cooperation, that are tight on part of the capacity region. It is shown that the cut-set bound is tight for classes of primitive relay and diamond channels, beyond the physically or stochastically degraded models.","authors":["Yossef Steinberg"],"url":"https://arxiv.org/abs/2501.08987"}
{"created":"2025-04-29","title":"A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks","abstract":"LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects: task definition, LLM profiling, and search procedures, making direct comparisons challenging. Moreover, the search algorithms employed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. This survey aims to provide a comprehensive but integrated technical review on existing LIS frameworks. Specifically, we unify task definitions under Markov Decision Process (MDP) and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM inference frameworks while highlighting their departures from conventional search algorithms. We also discuss the applicability, performance, and efficiency of these methods. For ongoing paper updates, please refer to our GitHub repository: https://github.com/xinzhel/LLM-Search.","authors":["Xinzhe Li"],"url":"https://arxiv.org/abs/2501.10069"}
{"created":"2025-04-29","title":"Grey-Box Fuzzing in Constrained Ultra-Large Systems: Lessons for SE Community","abstract":"Testing ultra-large microservices-based FinTech systems presents significant challenges, including restricted access to production environments, complex dependencies, and stringent security constraints. We propose SandBoxFuzz, a scalable grey-box fuzzing technique that addresses these limitations by leveraging aspect-oriented programming and runtime reflection to enable dynamic specification mining, generating targeted inputs for constrained environments. SandBoxFuzz also introduces a log-based coverage mechanism, seamlessly integrated into the build pipeline, eliminating the need for runtime coverage agents that are often infeasible in industrial settings. SandBoxFuzz has been successfully deployed to Ant Group's production line and, compared to an initial solution built on a state-of-the-art fuzzing framework, it demonstrates superior performance in their microservices software. SandBoxFuzz achieves a 7.5% increase in branch coverage, identifies 1,850 additional exceptions, and reduces setup time from hours to minutes, highlighting its effectiveness and practical utility in a real-world industrial environment. By open-sourcing SandBoxFuzz, we provide a practical and effective tool for researchers and practitioners to test large-scale microservices systems.","authors":["Jiazhao Yu","Yanlun Tu","Zhanlei Zhang","Tiehua Zhang","Cheng Xu","Weigang Wu","Hong Jin Kang","Xi Zheng"],"url":"https://arxiv.org/abs/2501.10269"}
{"created":"2025-04-29","title":"Uncertainty-Aware Digital Twins: Robust Model Predictive Control using Time-Series Deep Quantile Learning","abstract":"Digital Twins, virtual replicas of physical systems that enable real-time monitoring, model updates, predictions, and decision-making, present novel avenues for proactive control strategies for autonomous systems. However, achieving real-time decision-making in Digital Twins considering uncertainty necessitates an efficient uncertainty quantification (UQ) approach and optimization driven by accurate predictions of system behaviors, which remains a challenge for learning-based methods. This paper presents a simultaneous multi-step robust model predictive control (MPC) framework that incorporates real-time decision-making with uncertainty awareness for Digital Twin systems. Leveraging a multistep ahead predictor named Time-Series Dense Encoder (TiDE) as the surrogate model, this framework differs from conventional MPC models that provide only one-step ahead predictions. In contrast, TiDE can predict future states within the prediction horizon in a one-shot, significantly accelerating MPC. Furthermore, quantile regression is employed with the training of TiDE to perform flexible while computationally efficient UQ on data uncertainty. Consequently, with the deep learning quantiles, the robust MPC problem is formulated into a deterministic optimization problem and provides a safety buffer that accommodates disturbances to enhance constraint satisfaction rate. As a result, the proposed method outperforms existing robust MPC methods by providing less-conservative UQ and has demonstrated efficacy in an engineering case study involving Directed Energy Deposition (DED) additive manufacturing. This proactive while uncertainty-aware control capability positions the proposed method as a potent tool for future Digital Twin applications and real-time process control in engineering systems.","authors":["Yi-Ping Chen","Ying-Kuan Tsai","Vispi Karkaria","Wei Chen"],"url":"https://arxiv.org/abs/2501.10337"}
{"created":"2025-04-29","title":"Online Clustering with Bandit Information","abstract":"We study the problem of online clustering within the multi-armed bandit framework under the fixed confidence setting. In this multi-armed bandit problem, we have $M$ arms, each providing i.i.d. samples that follow a multivariate Gaussian distribution with an {\\em unknown} mean and a known unit covariance. The arms are grouped into $K$ clusters based on the distance between their means using the Single Linkage (SLINK) clustering algorithm on the means of the arms. Since the true means are unknown, the objective is to obtain the above clustering of the arms with the minimum number of samples drawn from the arms, subject to an upper bound on the error probability. We introduce a novel algorithm, Average Tracking Bandit Online Clustering (ATBOC), and prove that this algorithm is order optimal, meaning that the upper bound on its expected sample complexity for given error probability $\\delta$ is within a factor of 2 of an instance-dependent lower bound as $\\delta \\rightarrow 0$. Furthermore, we propose a computationally more efficient algorithm, Lower and Upper Confidence Bound-based Bandit Online Clustering (LUCBBOC), inspired by the LUCB algorithm for best arm identification. Simulation results demonstrate that the performance of LUCBBOC is comparable to that of ATBOC. We numerically assess the effectiveness of the proposed algorithms through numerical experiments on both synthetic datasets and the real-world MovieLens dataset. To the best of our knowledge, this is the first work on bandit online clustering that allows arms with different means in a cluster and $K$ greater than 2.","authors":["G Dhinesh Chandran","Srinivas Reddy Kota","Srikrishna Bhashyam"],"url":"https://arxiv.org/abs/2501.11421"}
{"created":"2025-04-29","title":"An Incremental Sampling and Segmentation-Based Approach for Motion Planning Infeasibility","abstract":"We present a simple and easy-to-implement algorithm to detect plan infeasibility in kinematic motion planning. Our method involves approximating the robot's configuration space to a discrete space, where each degree of freedom has a finite set of values. The obstacle region separates the free configuration space into different connected regions. For a path to exist between the start and goal configurations, they must lie in the same connected region of the free space. Thus, to ascertain plan infeasibility, we merely need to sample adequate points from the obstacle region that isolate start and goal. Accordingly, we progressively construct the configuration space by sampling from the discretized space and updating the bitmap cells representing obstacle regions. Subsequently, we partition this partially built configuration space to identify different connected components within it and assess the connectivity of the start and goal cells. We illustrate this methodology on five different scenarios with configuration spaces having up to 5 degree-of-freedom (DOF).","authors":["Antony Thomas","Fulvio Mastrogiovanni","Marco Baglietto"],"url":"https://arxiv.org/abs/2501.11434"}
{"created":"2025-04-29","title":"Multi-Stage Active Sequential Hypothesis Testing with Clustered Hypotheses","abstract":"We consider the problem where an active Decision-Maker (DM) is tasked to identify the true hypothesis using as few as possible observations while maintaining accuracy. The DM collects observations according to its determined actions and knows the distributions under each hypothesis. We propose a deterministic and adaptive multi-stage hypothesis-elimination strategy where the DM selects an action, applies it repeatedly, and discards hypotheses in light of its obtained observations. The DM selects actions based on maximal separation expressed by the distance between the parameter vectors of each distribution under each hypothesis. Close distributions can be clustered, simplifying the search and significantly reducing the number of required observations.","authors":["George Vershinin","Asaf Cohen","Omer Gurewitz"],"url":"https://arxiv.org/abs/2501.11459"}
{"created":"2025-04-29","title":"Sampled-Data Control using Hermite-Obreschkoff Methods with an IDA-PBC Example","abstract":"The motivation for this paper is the implementation of nonlinear state feedback control, designed based on the continuous-time plant model, in a sampled control loop under relatively slow sampling. In previous work we have shown that using one-step predictions of the target dynamics with higher order integration schemes, together with possibly higher order input shaping, is a simple and effective way to increase the feasible sampling times until performance degradation and instability occur. In this contribution we present a unifying derivation for arbitrary orders of the previously used Lobatto IIIA collocation and Hermite interpolation schemes through the Hermite-Obreschkoff formula. We derive, moreover, an IDA-PBC controller for a magnetic levitation system, which requires a non-constant target interconnection matrix, and show experimental results.","authors":["Le Zhang","Paul Kotyczka"],"url":"https://arxiv.org/abs/2501.11495"}
{"created":"2025-04-29","title":"Improved Decoding of Tanner Codes","abstract":"In this paper, we present improved decoding algorithms for expander-based Tanner codes.","authors":["Zhaienhe Zhou","Zeyu Guo"],"url":"https://arxiv.org/abs/2501.12293"}
{"created":"2025-04-29","title":"Exploring Unknown Social Networks for Discovering Hidden Nodes","abstract":"In this paper, we address the challenge of discovering hidden nodes in unknown social networks, formulating three types of hidden-node discovery problems, namely, Sybil-node discovery, peripheral-node discovery, and influencer discovery. We tackle these problems by employing a graph exploration framework grounded in machine learning. Leveraging the structure of the subgraph gradually obtained from graph exploration, we construct prediction models to identify target hidden nodes in unknown social graphs. Through empirical investigations of real social graphs, we investigate the efficiency of graph exploration strategies in uncovering hidden nodes. Our results show that our graph exploration strategies discover hidden nodes with an efficiency comparable to that when the graph structure is known. Specifically, the query cost of discovering 10% of the hidden nodes is at most only 1.2 times that when the topology is known, and the query-cost multiplier for discovering 90% of the hidden nodes is at most only 1.4. Furthermore, our results suggest that using node embeddings, which are low-dimensional vector representations of nodes, for hidden-node discovery is a double-edged sword: it is effective in certain scenarios but sometimes degrades the efficiency of node discovery. Guided by this observation, we examine the effectiveness of using a bandit algorithm to combine the prediction models that use node embeddings with those that do not, and our analysis shows that the bandit-based graph exploration strategy achieves efficient node discovery across a wide array of settings.","authors":["Sho Tsugawa","Hiroyuki Ohsaki"],"url":"https://arxiv.org/abs/2501.12571"}
{"created":"2025-04-29","title":"You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations","abstract":"Bimanual robotic manipulation is a long-standing challenge of embodied intelligence due to its characteristics of dual-arm spatial-temporal coordination and high-dimensional action spaces. Previous studies rely on pre-defined action taxonomies or direct teleoperation to alleviate or circumvent these issues, often making them lack simplicity, versatility and scalability. Differently, we believe that the most effective and efficient way for teaching bimanual manipulation is learning from human demonstrated videos, where rich features such as spatial-temporal positions, dynamic postures, interaction states and dexterous transitions are available almost for free. In this work, we propose the YOTO (You Only Teach Once), which can extract and then inject patterns of bimanual actions from as few as a single binocular observation of hand movements, and teach dual robot arms various complex tasks. Furthermore, based on keyframes-based motion trajectories, we devise a subtle solution for rapidly generating training demonstrations with diverse variations of manipulated objects and their locations. These data can then be used to learn a customized bimanual diffusion policy (BiDP) across diverse scenes. In experiments, YOTO achieves impressive performance in mimicking 5 intricate long-horizon bimanual tasks, possesses strong generalization under different visual and spatial conditions, and outperforms existing visuomotor imitation learning methods in accuracy and efficiency. Our project link is https://hnuzhy.github.io/projects/YOTO.","authors":["Huayi Zhou","Ruixiang Wang","Yunxin Tai","Yueci Deng","Guiliang Liu","Kui Jia"],"url":"https://arxiv.org/abs/2501.14208"}
{"created":"2025-04-29","title":"An Explainable Biomedical Foundation Model via Large-Scale Concept-Enhanced Vision-Language Pre-training","abstract":"The clinical adoption of artificial intelligence (AI) in medical imaging requires models that are both diagnostically accurate and interpretable to clinicians. While current multimodal biomedical foundation models prioritize performance, their black-box nature hinders explaining the decision-making process in clinically meaningful concepts. Here, we present ConceptCLIP, the first explainable biomedical foundation model that achieves state-of-the-art diagnostic accuracy while delivering human-interpretable explanations across diverse imaging modalities. We curate MedConcept-23M, the largest pre-training dataset comprising 23 million image-text-concept triplets across diverse medical modalities, where clinical concepts are derived from the Unified Medical Language System. Leveraging this dataset, we develop ConceptCLIP through a novel dual-alignment approach that simultaneously learns global image-text representations and fine-grained region-concept associations for precise and interpretable medical image analysis. We curate the most extensive evaluation benchmark for multimodal biomedical foundation models, covering 52 clinical tasks spanning 10 imaging modalities. Extensive experiments demonstrate that ConceptCLIP outperforms existing state-of-the-art multimodal biomedical foundation models. Importantly, ConceptCLIP demonstrates superior diagnostic performance while providing human-understandable explanations validated by clinical experts. As the first precise and interpretable biomedical foundation model, ConceptCLIP represents a critical milestone toward the widespread clinical adoption of AI, thereby advancing trustworthy AI in medicine.","authors":["Yuxiang Nie","Sunan He","Yequan Bie","Yihui Wang","Zhixuan Chen","Shu Yang","Zhiyuan Cai","Hongmei Wang","Xi Wang","Luyang Luo","Mingxiang Wu","Xian Wu","Ronald Cheong Kin Chan","Yuk Ming Lau","Yefeng Zheng","Pranav Rajpurkar","Hao Chen"],"url":"https://arxiv.org/abs/2501.15579"}
{"created":"2025-04-29","title":"Complexity of Minimal Faithful Permutation Degree for Fitting-free Groups","abstract":"In this paper, we investigate the complexity of computing the minimal faithful permutation degree for groups without abelian normal subgroups. When our groups are given as quotients of permutation groups, we establish that this problem is in $\\textsf{P}$. Furthermore, in the setting of permutation groups, we obtain an upper bound of $\\textsf{NC}$ for this problem. This improves upon the work of Das and Thakkar (STOC 2024), who established a Las Vegas polynomial-time algorithm for this class in the setting of permutation groups.","authors":["Michael Levet","Pranjal Srivastava","Dhara Thakkar"],"url":"https://arxiv.org/abs/2501.16039"}
{"created":"2025-04-29","title":"Fast and Accurate Identification of Hardware Trojan Locations in Gate-Level Netlist using Nearest Neighbour Approach integrated with Machine Learning Technique","abstract":"In the evolving landscape of integrated circuit design, detecting Hardware Trojans (HTs) within a multi entity based design cycle presents significant challenges. This research proposes an innovative machine learning-based methodology for identifying malicious logic gates in gate-level netlists. By focusing on path retrace algorithms. The methodology is validated across three distinct cases, each employing different machine learning models to classify HTs. Case I utilizes a decision tree algorithm for node-to-node comparisons, significantly improving detection accuracy through the integration of Principal Component Analysis (PCA). Case II introduces a graph-to-graph classification using a Graph Neural Network (GNN) model, enabling the differentiation between normal and Trojan-infected circuit designs. Case III applies GNN-based node classification to identify individual compromised nodes and its location. Additionally, nearest neighbor (NN) method has been combined with GNN graph-to-graph in Case II and GNN node-to-node in Case III. Despite the potential of GNN model graph-to-graph classification, NN approach demonstrated superior performance, with the first nearest neighbor (1st NN) achieving 73.2% accuracy and the second nearest neighbor (2nd NN) method reaching 97.7%. In comparison, the GNN model achieved an accuracy of 62.8%. Similarly, GNN model node-to-node classification, NN approach demonstrated superior performance, with the 1st NN achieving 93% accuracy and the 2nd NN method reaching 97.7%. In comparison, the GNN model achieved an accuracy of 79.8%. However, higher and higher NN will lead to large code coverage for the identification of HTs.","authors":["Anindita Chattopadhyay","Siddharth Bisariya","Vijay Kumar Sutrakar"],"url":"https://arxiv.org/abs/2501.16347"}
{"created":"2025-04-29","title":"Compression and Distillation of Data Quadruplets in Non-intrusive Reduced-order Modeling","abstract":"This paper introduces a quadrature-free, data-driven approach to balanced truncation for both continuous-time and discrete-time systems. The method non-intrusively constructs reduced-order models using available transfer function samples from the right half of the $s$-plane. It is highlighted that the proposed data-driven balanced truncation and existing quadrature-based balanced truncation algorithms share a common feature: both compress their respective data quadruplets to derive reduced-order models. Additionally, it is demonstrated that by using different compression strategies, these quadruplets can be utilized to develop three data-driven formulations of the IRKA for both continuous-time and discrete-time systems. These formulations non-intrusively generate reduced models using transfer function samples from the $j\\omega$-axis or the right half of the $s$-plane, or impulse response samples. Notably, these IRKA formulations eliminate the necessity of computing new transfer function samples as IRKA iteratively updates the interpolation points. The efficacy of the proposed algorithms is validated through numerical examples, which show that the proposed data-driven approaches perform comparably to their intrusive counterparts.","authors":["Umair Zulfiqar"],"url":"https://arxiv.org/abs/2501.16683"}
{"created":"2025-04-29","title":"Channel Estimation for XL-MIMO Systems with Decentralized Baseband Processing: Integrating Local Reconstruction with Global Refinement","abstract":"In this paper, we investigate the channel estimation problem for extremely large-scale multiple-input multiple-output (XL-MIMO) systems with a hybrid analog-digital architecture, implemented within a decentralized baseband processing (DBP) framework with a star topology. Existing centralized and fully decentralized channel estimation methods face limitations due to excessive computational complexity or degraded performance. To overcome these challenges, we propose a novel two-stage channel estimation scheme that integrates local sparse reconstruction with global fusion and refinement. Specifically, in the first stage, by exploiting the sparsity of channels in the angular-delay domain, the local reconstruction task is formulated as a sparse signal recovery problem. To solve it, we develop a graph neural networks-enhanced sparse Bayesian learning (SBL-GNNs) algorithm, which effectively captures dependencies among channel coefficients, significantly improving estimation accuracy. In the second stage, the local estimates from the local processing units (LPUs) are aligned into a global angular domain for fusion at the central processing unit (CPU). Based on the aggregated observations, the channel refinement is modeled as a Bayesian denoising problem. To efficiently solve it, we devise a variational message passing algorithm that incorporates a Markov chain-based hierarchical sparse prior, effectively leveraging both the sparsity and the correlations of the channels in the global angular-delay domain. Simulation results validate the effectiveness and superiority of the proposed SBL-GNNs algorithm over existing methods, demonstrating improved estimation performance and reduced computational complexity.","authors":["Anzheng Tang","Jun-Bo Wang","Yijin Pan","Cheng Zeng","Yijian Chen","Hongkang Yu","Ming Xiao","Rodrigo C. de Lamare","Jiangzhou Wang"],"url":"https://arxiv.org/abs/2501.17059"}
{"created":"2025-04-29","title":"Exploring the Potential of Wireless-enabled Multi-Chip AI Accelerators","abstract":"The insatiable appetite of Artificial Intelligence (AI) workloads for computing power is pushing the industry to develop faster and more efficient accelerators. The rigidity of custom hardware, however, conflicts with the need for scalable and versatile architectures capable of catering to the needs of the evolving and heterogeneous pool of Machine Learning (ML) models in the literature. In this context, multi-chiplet architectures assembling multiple (perhaps heterogeneous) accelerators are an appealing option that is unfortunately hindered by the still rigid and inefficient chip-to-chip interconnects. In this paper, we explore the potential of wireless technology as a complement to existing wired interconnects in this multi-chiplet approach. Using an evaluation framework from the state-of-the-art, we show that wireless interconnects can lead to speedups of 10% on average and 20% maximum. We also highlight the importance of load balancing between the wired and wireless interconnects, which will be further explored in future work.","authors":["Emmanuel Irabor","Mariam Musavi","Abhijit Das","Sergi Abadal"],"url":"https://arxiv.org/abs/2501.17567"}
{"created":"2025-04-29","title":"Neuro-LIFT: A Neuromorphic, LLM-based Interactive Framework for Autonomous Drone FlighT at the Edge","abstract":"The integration of human-intuitive interactions into autonomous systems has been limited. Traditional Natural Language Processing (NLP) systems struggle with context and intent understanding, severely restricting human-robot interaction. Recent advancements in Large Language Models (LLMs) have transformed this dynamic, allowing for intuitive and high-level communication through speech and text, and bridging the gap between human commands and robotic actions. Additionally, autonomous navigation has emerged as a central focus in robotics research, with artificial intelligence (AI) increasingly being leveraged to enhance these systems. However, existing AI-based navigation algorithms face significant challenges in latency-critical tasks where rapid decision-making is critical. Traditional frame-based vision systems, while effective for high-level decision-making, suffer from high energy consumption and latency, limiting their applicability in real-time scenarios. Neuromorphic vision systems, combining event-based cameras and spiking neural networks (SNNs), offer a promising alternative by enabling energy-efficient, low-latency navigation. Despite their potential, real-world implementations of these systems, particularly on physical platforms such as drones, remain scarce. In this work, we present Neuro-LIFT, a real-time neuromorphic navigation framework implemented on a Parrot Bebop2 quadrotor. Leveraging an LLM for natural language processing, Neuro-LIFT translates human speech into high-level planning commands which are then autonomously executed using event-based neuromorphic vision and physics-driven planning. Our framework demonstrates its capabilities in navigating in a dynamic environment, avoiding obstacles, and adapting to human instructions in real-time.","authors":["Amogh Joshi","Sourav Sanyal","Kaushik Roy"],"url":"https://arxiv.org/abs/2501.19259"}
{"created":"2025-04-29","title":"Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora","abstract":"A numeration system encodes abstract numeric quantities as concrete strings of written characters. The numeration systems used by modern scripts tend to be precise and unambiguous, but this was not so for the ancient and partially-deciphered proto-Elamite (PE) script, where written numerals can have up to four distinct readings depending on the system that is used to read them. We consider the task of disambiguating between these readings in order to determine the values of the numeric quantities recorded in this corpus. We algorithmically extract a list of possible readings for each PE numeral notation, and contribute two disambiguation techniques based on structural properties of the original documents and classifiers learned with the bootstrapping algorithm. We also contribute a test set for evaluating disambiguation techniques, as well as a novel approach to cautious rule selection for bootstrapped classifiers. Our analysis confirms existing intuitions about this script and reveals previously-unknown correlations between tablet content and numeral magnitude. This work is crucial to understanding and deciphering PE, as the corpus is heavily accounting-focused and contains many more numeric tokens than tokens of text.","authors":["Logan Born","M. Willis Monroe","Kathryn Kelley","Anoop Sarkar"],"url":"https://arxiv.org/abs/2502.00090"}
{"created":"2025-04-29","title":"Evaluating Deep Human-in-the-Loop Optimization for Retinal Implants Using Sighted Participants","abstract":"Human-in-the-loop optimization (HILO) is a promising approach for personalizing visual prostheses by iteratively refining stimulus parameters based on user feedback. Previous work demonstrated HILO's efficacy in simulation, but its performance with human participants remains untested. Here we evaluate HILO using sighted participants viewing simulated prosthetic vision to assess its ability to optimize stimulation strategies under realistic conditions. Participants selected between phosphenes generated by competing encoders to iteratively refine a deep stimulus encoder (DSE). We tested HILO in three conditions: standard optimization, threshold misspecifications, and out-of-distribution parameter sampling. Participants consistently preferred HILO-generated stimuli over both a naive encoder and the DSE alone, with log odds favoring HILO across all conditions. We also observed key differences between human and simulated decision-making, highlighting the importance of validating optimization strategies with human participants. These findings support HILO as a viable approach for adapting visual prostheses to individuals. Clinical relevance: Validating HILO with sighted participants viewing simulated prosthetic vision is an important step toward personalized calibration of future visual prostheses.","authors":["Eirini Schoinas","Adyah Rastogi","Anissa Carter","Jacob Granley","Michael Beyeler"],"url":"https://arxiv.org/abs/2502.00177"}
{"created":"2025-04-29","title":"CAIMAN: Causal Action Influence Detection for Sample-efficient Loco-manipulation","abstract":"Enabling legged robots to perform non-prehensile loco-manipulation is crucial for enhancing their versatility. Learning behaviors such as whole-body object pushing often requires sophisticated planning strategies or extensive task-specific reward shaping, especially in unstructured environments. In this work, we present CAIMAN, a practical reinforcement learning framework that encourages the agent to gain control over other entities in the environment. CAIMAN leverages causal action influence as an intrinsic motivation objective, allowing legged robots to efficiently acquire object pushing skills even under sparse task rewards. We employ a hierarchical control strategy, combining a low-level locomotion module with a high-level policy that generates task-relevant velocity commands and is trained to maximize the intrinsic reward. To estimate causal action influence, we learn the dynamics of the environment by integrating a kinematic prior with data collected during training.We empirically demonstrate CAIMAN's superior sample efficiency and adaptability to diverse scenarios in simulation, as well as its successful transfer to real-world systems without further fine-tuning.","authors":["Yuanchen Yuan","Jin Cheng","N\\'uria Armengol Urp\\'i","Stelian Coros"],"url":"https://arxiv.org/abs/2502.00835"}
{"created":"2025-04-29","title":"ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills","abstract":"Humanoid robots hold the potential for unparalleled versatility in performing human-like, whole-body skills. However, achieving agile and coordinated whole-body motions remains a significant challenge due to the dynamics mismatch between simulation and the real world. Existing approaches, such as system identification (SysID) and domain randomization (DR) methods, often rely on labor-intensive parameter tuning or result in overly conservative policies that sacrifice agility. In this paper, we present ASAP (Aligning Simulation and Real-World Physics), a two-stage framework designed to tackle the dynamics mismatch and enable agile humanoid whole-body skills. In the first stage, we pre-train motion tracking policies in simulation using retargeted human motion data. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Then, ASAP fine-tunes pre-trained policies with the delta action model integrated into the simulator to align effectively with real-world dynamics. We evaluate ASAP across three transfer scenarios: IsaacGym to IsaacSim, IsaacGym to Genesis, and IsaacGym to the real-world Unitree G1 humanoid robot. Our approach significantly improves agility and whole-body coordination across various dynamic motions, reducing tracking error compared to SysID, DR, and delta dynamics learning baselines. ASAP enables highly agile motions that were previously difficult to achieve, demonstrating the potential of delta action learning in bridging simulation and real-world dynamics. These results suggest a promising sim-to-real direction for developing more expressive and agile humanoids.","authors":["Tairan He","Jiawei Gao","Wenli Xiao","Yuanhang Zhang","Zi Wang","Jiashun Wang","Zhengyi Luo","Guanqi He","Nikhil Sobanbab","Chaoyi Pan","Zeji Yi","Guannan Qu","Kris Kitani","Jessica Hodgins","Linxi \"Jim\" Fan","Yuke Zhu","Changliu Liu","Guanya Shi"],"url":"https://arxiv.org/abs/2502.01143"}
{"created":"2025-04-29","title":"Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity","abstract":"Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs. This inefficiency primarily arises from the quadratic computational complexity of 3D Full Attention with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (SVG) that leverages the inherent sparsity in 3D Full Attention to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) Spatial Head, where only spatially-related tokens within each frame dominate the attention output, and (2) Temporal Head, where only temporally-related tokens across different frames dominate. Based on this insight, SVG proposes an online profiling strategy to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor layout transformation and customized kernel implementations, SVG achieves up to 2.28x and 2.33x end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality. Our code is open-sourced and is available at https://github.com/svg-project/Sparse-VideoGen","authors":["Haocheng Xi","Shuo Yang","Yilong Zhao","Chenfeng Xu","Muyang Li","Xiuyu Li","Yujun Lin","Han Cai","Jintao Zhang","Dacheng Li","Jianfei Chen","Ion Stoica","Kurt Keutzer","Song Han"],"url":"https://arxiv.org/abs/2502.01776"}
{"created":"2025-04-29","title":"The Wisdom of Intellectually Humble Networks","abstract":"People's collectively shared beliefs can have significant social implications, including on democratic processes and policies. Unfortunately, as people interact with peers to form and update their beliefs, various cognitive and social biases can hinder their collective wisdom. In this paper, we probe whether and how the psychological construct of intellectual humility can modulate collective wisdom in a networked interaction setting. Through agent-based modeling and data-calibrated simulations, we provide a proof of concept demonstrating that intellectual humility can foster more accurate estimations while mitigating polarization in social networks. We investigate the mechanisms behind the performance improvements and confirm robustness across task settings and network structures. Our work can guide intervention designs to capitalize on the promises of intellectual humility in boosting collective wisdom in social networks.","authors":["Mohammad Ratul Mahjabin","Raiyan Abdul Baten"],"url":"https://arxiv.org/abs/2502.02015"}
{"created":"2025-04-29","title":"VerteNet -- A Multi-Context Hybrid CNN Transformer for Accurate Vertebral Landmark Localization in Lateral Spine DXA Images","abstract":"Lateral Spine Image (LSI) analysis is important for medical diagnosis, treatment planning, and detailed spinal health assessments. Although modalities like Computed Tomography and Digital X-ray Imaging are commonly used, Dual Energy X-ray Absorptiometry (DXA) is often preferred due to lower radiation exposure, seamless capture, and cost-effectiveness. Accurate Vertebral Landmark Localization (VLL) on LSIs is important to detect spinal conditions like kyphosis and lordosis, as well as assessing Abdominal Aortic Calcification (AAC) using Inter-Vertebral Guides (IVGs). Nonetheless, few automated VLL methodologies have concentrated on DXA LSIs. We present VerteNet, a hybrid CNN-Transformer model featuring a novel dual-resolution attention mechanism in self and cross-attention domains, referred to as Dual Resolution Self-Attention (DRSA) and Dual Resolution Cross-Attention (DRCA). These mechanisms capture the diverse frequencies in DXA images by operating at two different feature map resolutions. Additionally, we design a Multi-Context Feature Fusion Block (MCFB) that efficiently integrates the features using DRSA and DRCA. We train VerteNet on 620 DXA LSIs from various machines and achieve superior results compared to existing methods. We also design an algorithm that utilizes VerteNet's predictions in estimating the Region of Interest (ROI) to detect potential abdominal aorta cropping, where inadequate soft tissue hinders calcification assessment. Additionally, we present a small proof-of-concept study to show that IVGs generated from VLL information can improve inter-reader correlation in AAC scoring, addressing two key areas of disagreement in expert AAC-24 scoring: IVG placement and quality control for full abdominal aorta assessment. The code for this work can be found at https://github.com/zaidilyas89/VerteNet.","authors":["Zaid Ilyas","Arooba Maqsood","Afsah Saleem","Erchuan Zhang","David Suter","Parminder Raina","Jonathan M. Hodgson","John T. Schousboe","William D. Leslie","Joshua R. Lewis","Syed Zulqarnain Gilani"],"url":"https://arxiv.org/abs/2502.02097"}
{"created":"2025-04-29","title":"An Anatomy of 488 Faults from Defects4J Based on the Control- and Data-Flow Graph Representations of Programs","abstract":"Software fault datasets such as Defects4J provide for each individual fault its location and repair, but do not characterize the faults. Current classifications use the repairs as proxies, but these do not capture the intrinsic nature of the fault. In this paper, we propose a new, direct fault classification scheme based on the control- and data-flow graph representations of programs. Our scheme comprises six control-flow and two data-flow fault classes. We manually apply this scheme to 488 faults from seven projects in the Defects4J dataset. We find that the majority of the faults are assigned between one and three classes. We also find that one of the data-flow fault classes (definition fault) is the most common individual class but that the majority of faults are classified with at least one control-flow fault class. Our proposed classification can be applied to other fault datasets and can be used to improve fault localization and automated program repair techniques for specific fault classes.","authors":["Alexandra van der Spuy","Bernd Fischer"],"url":"https://arxiv.org/abs/2502.02299"}
{"created":"2025-04-29","title":"Cross-modality Force and Language Embeddings for Natural Human-Robot Communication","abstract":"A method for cross-modality embedding of force profile and words is presented for synergistic coordination of verbal and haptic communication. When two people carry a large, heavy object together, they coordinate through verbal communication about the intended movements and physical forces applied to the object. This natural integration of verbal and physical cues enables effective coordination. Similarly, human-robot interaction could achieve this level of coordination by integrating verbal and haptic communication modalities. This paper presents a framework for embedding words and force profiles in a unified manner, so that the two communication modalities can be integrated and coordinated in a way that is effective and synergistic. Here, it will be shown that, although language and physical force profiles are deemed completely different, the two can be embedded in a unified latent space and proximity between the two can be quantified. In this latent space, a force profile and words can a) supplement each other, b) integrate the individual effects, and c) substitute in an exchangeable manner. First, the need for cross-modality embedding is addressed, and the basic architecture and key building block technologies are presented. Methods for data collection and implementation challenges will be addressed, followed by experimental results and discussions.","authors":["Ravi Tejwani","Karl Velazquez","John Payne","Paolo Bonato","Harry Asada"],"url":"https://arxiv.org/abs/2502.02772"}
{"created":"2025-04-29","title":"Variations on the Expectation due to Changes in the Probability Measure","abstract":"In this paper, closed-form expressions are presented for the variation of the expectation of a given function due to changes in the probability measure used for the expectation. They unveil interesting connections with Gibbs probability measures, mutual information, and lautum information.","authors":["Samir M. Perlaza","Gaetan Bisson"],"url":"https://arxiv.org/abs/2502.02887"}
{"created":"2025-04-29","title":"Gait-Net-augmented Implicit Kino-dynamic MPC for Dynamic Variable-frequency Humanoid Locomotion over Discrete Terrains","abstract":"Reduced-order-based optimal control techniques for humanoid locomotion struggle to adapt step duration and placement simultaneously in dynamic walking gaits due to their reliance on fixed-time discretization, which limits responsiveness to various disturbances and results in suboptimal performance in challenging conditions. In this work, we propose a Gait-Net-augmented implicit kino-dynamic model-predictive control (MPC) to simultaneously optimize step location, step duration, and contact forces for natural variable-frequency locomotion. The proposed method incorporates a Gait-Net-augmented Sequential Convex MPC algorithm to solve multi-linearly constrained variables by iterative quadratic programs. At its core, a lightweight Gait-frequency Network (Gait-Net) determines the preferred step duration in terms of variable MPC sampling times, simplifying step duration optimization to the parameter level. Additionally, it enhances and updates the spatial reference trajectory within each sequential iteration by incorporating local solutions, allowing the projection of kinematic constraints to the design of reference trajectories. We validate the proposed algorithm in high-fidelity simulations and on small-size humanoid hardware, demonstrating its capability for variable-frequency and 3-D discrete terrain locomotion with only a one-step preview of terrain data.","authors":["Junheng Li","Ziwei Duan","Junchao Ma","Quan Nguyen"],"url":"https://arxiv.org/abs/2502.02934"}
{"created":"2025-04-29","title":"Brain Tumor Identification using Improved YOLOv8","abstract":"Identifying the extent of brain tumors is a significant challenge in brain cancer treatment. The main difficulty is in the approximate detection of tumor size. Magnetic resonance imaging (MRI) has become a critical diagnostic tool. However, manually detecting the boundaries of brain tumors from MRI scans is a labor-intensive task that requires extensive expertise. Deep learning and computer-aided detection techniques have led to notable advances in machine learning for this purpose. In this paper, we propose a modified You Only Look Once (YOLOv8) model to accurately detect the tumors within the MRI images. The proposed model replaced the Non-Maximum Suppression (NMS) algorithm with a Real-Time Detection Transformer (RT- DETR) in the detection head. NMS filters out redundant or overlapping bounding boxes in the detected tumors, but they are hand-designed and pre-set. RT-DETR removes hand-designed components. The second improvement was made by replacing the normal convolution block with ghost convolution. Ghost Convolution reduces computational and memory costs while maintaining high accuracy and enabling faster inference, making it ideal for resource-constrained environments and real-time applications. The third improvement was made by introducing a vision transformer block in the backbone of YOLOv8 to extract context-aware features. We used a publicly available dataset of brain tumors in the proposed model. The proposed model performed better than the original YOLOv8 model and also performed better than other object detectors (Faster R- CNN, Mask R-CNN, YOLO, YOLOv3, YOLOv4, YOLOv5, SSD, RetinaNet, EfficientDet, and DETR). The proposed model achieved 0.91 mAP (mean Average Precision)@0.5.","authors":["Rupesh Dulal","Rabin Dulal"],"url":"https://arxiv.org/abs/2502.03746"}
{"created":"2025-04-29","title":"Dynamic Rank Adjustment in Diffusion Policies for Efficient and Flexible Training","abstract":"Diffusion policies trained via offline behavioral cloning have recently gained traction in robotic motion generation. While effective, these policies typically require a large number of trainable parameters. This model size affords powerful representations but also incurs high computational cost during training. Ideally, it would be beneficial to dynamically adjust the trainable portion as needed, balancing representational power with computational efficiency. For example, while overparameterization enables diffusion policies to capture complex robotic behaviors via offline behavioral cloning, the increased computational demand makes online interactive imitation learning impractical due to longer training time. To address this challenge, we present a framework, called DRIFT, that uses the Singular Value Decomposition to enable dynamic rank adjustment during diffusion policy training. We implement and demonstrate the benefits of this framework in DRIFT-DAgger, an imitation learning algorithm that can seamlessly slide between an offline bootstrapping phase and an online interactive phase. We perform extensive experiments to better understand the proposed framework, and demonstrate that DRIFT-DAgger achieves improved sample efficiency and faster training with minimal impact on model performance. The project website is available at: https://apollo-lab-yale.github.io/25-RSS-DRIFT-website/.","authors":["Xiatao Sun","Shuo Yang","Yinxing Chen","Francis Fan","Yiyan Liang","Daniel Rakita"],"url":"https://arxiv.org/abs/2502.03822"}
{"created":"2025-04-29","title":"Joint State and Noise Covariance Estimation","abstract":"This paper tackles the problem of jointly estimating the noise covariance matrix alongside states (parameters such as poses and points) from measurements corrupted by Gaussian noise and, if available, prior information. In such settings, the noise covariance matrix determines the weights assigned to individual measurements in the least squares problem. We show that the joint problem exhibits a convex structure and provide a full characterization of the optimal noise covariance estimate (with analytical solutions) within joint maximum a posteriori and likelihood frameworks and several variants. Leveraging this theoretical result, we propose two novel algorithms that jointly estimate the primary parameters and the noise covariance matrix. Our BCD algorithm can be easily integrated into existing nonlinear least squares solvers, with negligible per-iteration computational overhead. To validate our approach, we conduct extensive experiments across diverse scenarios and offer practical insights into their application in robotics and computer vision estimation problems with a particular focus on SLAM.","authors":["Kasra Khosoussi","Iman Shames"],"url":"https://arxiv.org/abs/2502.04584"}
{"created":"2025-04-29","title":"Building Rome with Convex Optimization","abstract":"Global bundle adjustment is made easy by depth prediction and convex optimization. We (i) propose a scaled bundle adjustment (SBA) formulation that lifts 2D keypoint measurements to 3D with learned depth, (ii) design an empirically tight convex semidfinite program (SDP) relaxation that solves SBA to certfiable global optimality, (iii) solve the SDP relaxations at extreme scale with Burer-Monteiro factorization and a CUDA-based trust-region Riemannian optimizer (dubbed XM), (iv) build a structure from motion (SfM) pipeline with XM as the optimization engine and show that XM-SfM compares favorably with existing pipelines in terms of reconstruction quality while being significantly faster, more scalable, and initialization-free.","authors":["Haoyu Han","Heng Yang"],"url":"https://arxiv.org/abs/2502.04640"}
{"created":"2025-04-29","title":"Implicit bias of Normalized Steepest Descent in Multiclass Classification: Sign Descent, Spectral Descent, and Adam","abstract":"In the optimization of overparameterized models, different gradient-based methods can achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. Despite a decade of research on implicit optimization bias, important questions remain open even in the foundational case of linear classification with separable data. We address this gap by characterizing the implicit bias of both Adam and Sign gradient descent (SignGD) in multi-class cross-entropy minimization: we prove that their iterates converge to solutions maximizing the margin with respect to the classifier matrix's max-norm, and we establish the corresponding convergence rates. We then generalize our analysis to p-norm normalized steepest descent (NSD) algorithms. This includes Spectral Descent, which we show converges to the max-margin solution with respect to the spectral norm. A key insight is that the analysis of general entry-wise and Schatten p-norms can be reduced to the analysis of NSD with max-norm (i.e., SignGD) by exploiting a natural ordering property between all p-norms relative to the max-norm and its dual sum-norm. Our results demonstrate that the multi-class linear setting, which is inherently richer than the binary counterpart, provides the most transparent playground for studying implicit biases of matrix-parameter optimization algorithms.","authors":["Chen Fan","Mark Schmidt","Christos Thrampoulidis"],"url":"https://arxiv.org/abs/2502.04664"}
{"created":"2025-04-29","title":"On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark","abstract":"This work investigates the theoretical boundaries of creating publicly-detectable schemes to enable the provenance of watermarked imagery. Metadata-based approaches like C2PA provide unforgeability and public-detectability. ML techniques offer robust retrieval and watermarking. However, no existing scheme combines robustness, unforgeability, and public-detectability. In this work, we formally define such a scheme and establish its existence. Although theoretically possible, we find that at present, it is intractable to build certain components of our scheme without a leap in deep learning capabilities. We analyze these limitations and propose research directions that need to be addressed before we can practically realize robust and publicly-verifiable provenance.","authors":["Jaiden Fairoze","Guillermo Ortiz-Jimenez","Mel Vecerik","Somesh Jha","Sven Gowal"],"url":"https://arxiv.org/abs/2502.04901"}
{"created":"2025-04-29","title":"REASSEMBLE: A Multimodal Dataset for Contact-rich Robotic Assembly and Disassembly","abstract":"Robotic manipulation remains a core challenge in robotics, particularly for contact-rich tasks such as industrial assembly and disassembly. Existing datasets have significantly advanced learning in manipulation but are primarily focused on simpler tasks like object rearrangement, falling short of capturing the complexity and physical dynamics involved in assembly and disassembly. To bridge this gap, we present REASSEMBLE (Robotic assEmbly disASSEMBLy datasEt), a new dataset designed specifically for contact-rich manipulation tasks. Built around the NIST Assembly Task Board 1 benchmark, REASSEMBLE includes four actions (pick, insert, remove, and place) involving 17 objects. The dataset contains 4,551 demonstrations, of which 4,035 were successful, spanning a total of 781 minutes. Our dataset features multi-modal sensor data, including event cameras, force-torque sensors, microphones, and multi-view RGB cameras. This diverse dataset supports research in areas such as learning contact-rich manipulation, task condition identification, action segmentation, and task inversion learning. The REASSEMBLE will be a valuable resource for advancing robotic manipulation in complex, real-world scenarios. The dataset is publicly available on our project website: https://tuwien-asl.github.io/REASSEMBLE_page/.","authors":["Daniel Sliwowski","Shail Jadav","Sergej Stanovcic","Jedrzej Orbik","Johannes Heidersberger","Dongheui Lee"],"url":"https://arxiv.org/abs/2502.05086"}
{"created":"2025-04-29","title":"VideoRoPE: What Makes for Good Video Rotary Position Embedding?","abstract":"While Rotary Position Embedding (RoPE) and its variants are widely adopted for their long-context capabilities, the extension of the 1D RoPE to video, with its complex spatio-temporal structure, remains an open challenge. This work first introduces a comprehensive analysis that identifies four key characteristics essential for the effective adaptation of RoPE to video, which have not been fully considered in prior work. As part of our analysis, we introduce a challenging V-NIAH-D (Visual Needle-In-A-Haystack with Distractors) task, which adds periodic distractors into V-NIAH. The V-NIAH-D task demonstrates that previous RoPE variants, lacking appropriate temporal dimension allocation, are easily misled by distractors. Based on our analysis, we introduce \\textbf{VideoRoPE}, with a \\textit{3D structure} designed to preserve spatio-temporal relationships. VideoRoPE features \\textit{low-frequency temporal allocation} to mitigate periodic oscillations, a \\textit{diagonal layout} to maintain spatial symmetry, and \\textit{adjustable temporal spacing} to decouple temporal and spatial indexing. VideoRoPE consistently surpasses previous RoPE variants, across diverse downstream tasks such as long video retrieval, video understanding, and video hallucination. Our code will be available at \\href{https://github.com/Wiselnn570/VideoRoPE}{https://github.com/Wiselnn570/VideoRoPE}.","authors":["Xilin Wei","Xiaoran Liu","Yuhang Zang","Xiaoyi Dong","Pan Zhang","Yuhang Cao","Jian Tong","Haodong Duan","Qipeng Guo","Jiaqi Wang","Xipeng Qiu","Dahua Lin"],"url":"https://arxiv.org/abs/2502.05173"}
{"created":"2025-04-29","title":"Adaptive Domain Scaling for Personalized Sequential Modeling in Recommenders","abstract":"Users generally exhibit complex behavioral patterns and diverse intentions in multiple business scenarios of super applications like Douyin, presenting great challenges to current industrial multi-domain recommenders. To mitigate the discrepancies across diverse domains, researches and industrial practices generally emphasize sophisticated network structures to accomodate diverse data distributions, while neglecting the inherent understanding of user behavioral sequence from the multi-domain perspective. In this paper, we present Adaptive Domain Scaling (ADS) model, which comprehensively enhances the personalization capability in target-aware sequence modeling across multiple domains. Specifically, ADS comprises of two major modules, including personalized sequence representation generation (PSRG) and personalized candidate representation generation (PCRG). The modules contribute to the tailored multi-domain learning by dynamically learning both the user behavioral sequence item representation and the candidate target item representation under different domains, facilitating adaptive user intention understanding. Experiments are performed on both a public dataset and two billion-scaled industrial datasets, and the extensive results verify the high effectiveness and compatibility of ADS. Besides, we conduct online experiments on two influential business scenarios including Douyin Advertisement Platform and Douyin E-commerce Service Platform, both of which show substantial business improvements. Currently, ADS has been fully deployed in many recommendation services at ByteDance, serving billions of users.","authors":["Zheng Chai","Hui Lu","Di Chen","Qin Ren","Yuchao Zheng","Xun Zhou"],"url":"https://arxiv.org/abs/2502.05523"}
{"created":"2025-04-29","title":"Optimizing Information Freshness of IEEE 802.11ax Uplink OFDMA-Based Random Access","abstract":"The latest WiFi standard, IEEE 802.11ax (WiFi 6), introduces a novel uplink random access mechanism called uplink orthogonal frequency division multiple access-based random access (UORA). While existing work has evaluated the performance of UORA using conventional performance metrics, such as throughput and delay, its information freshness performance has not been thoroughly investigated in the literature. This is of practical significance as WiFi 6 and beyond are expected to support real-time applications. This paper presents the first attempt to fill this gap by investigating the information freshness, quantified by the Age of Information (AoI) metric, in UORA networks. We establish an analytical framework comprising two discrete-time Markov chains (DTMCs) to characterize the transmission states of stations (STAs) in UORA networks. Building on the formulated DTMCs, we derive an analytical expression for the long-term average AoI (AAoI), facilitating the optimization of UORA parameters for enhanced AoI performance through exhaustive search. To gain deeper design insights and improve the effectiveness of UORA parameter optimization, we derive a closed-form expression for the AAoI and its approximated lower bound for a simplified scenario characterized by a fixed backoff contention window and generate-at-will status updates. By analyzing the approximated lower bound of the AAoI, we propose efficient UORA parameter optimization algorithms that can be realized with only a few comparisons of different possible values of the parameters to be optimized. Simulation results validate our analysis and demonstrate that the AAoI achieved through our proposed parameter optimization algorithm closely approximates the optimal AoI performance obtained via exhaustive search, outperforming the round-robin and max-AoI policies in large and low-traffic networks.","authors":["Jingwei Liu","Qian Wang","He Chen"],"url":"https://arxiv.org/abs/2502.05588"}
{"created":"2025-04-29","title":"Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage","abstract":"Mainstream media, through their decisions on what to cover and how to frame the stories they cover, can mislead readers without using outright falsehoods. Therefore, it is crucial to have tools that expose these editorial choices underlying media bias. In this paper, we introduce the Media Bias Detector, a tool for researchers, journalists, and news consumers. By integrating large language models, we provide near real-time granular insights into the topics, tone, political lean, and facts of news articles aggregated to the publisher level. We assessed the tool's impact by interviewing 13 experts from journalism, communications, and political science, revealing key insights into usability and functionality, practical applications, and AI's role in powering media bias tools. We explored this in more depth with a follow-up survey of 150 news consumers. This work highlights opportunities for AI-driven tools that empower users to critically engage with media content, particularly in politically charged environments.","authors":["Jenny S Wang","Samar Haider","Amir Tohidi","Anushkaa Gupta","Yuxuan Zhang","Chris Callison-Burch","David Rothschild","Duncan J Watts"],"url":"https://arxiv.org/abs/2502.06009"}
{"created":"2025-04-29","title":"DROP: Poison Dilution via Knowledge Distillation for Federated Learning","abstract":"Federated Learning is vulnerable to adversarial manipulation, where malicious clients can inject poisoned updates to influence the global model's behavior. While existing defense mechanisms have made notable progress, they fail to protect against adversaries that aim to induce targeted backdoors under different learning and attack configurations. To address this limitation, we introduce DROP (Distillation-based Reduction Of Poisoning), a novel defense mechanism that combines clustering and activity-tracking techniques with extraction of benign behavior from clients via knowledge distillation to tackle stealthy adversaries that manipulate low data poisoning rates and diverse malicious client ratios within the federation. Through extensive experimentation, our approach demonstrates superior robustness compared to existing defenses across a wide range of learning configurations. Finally, we evaluate existing defenses and our method under the challenging setting of non-IID client data distribution and highlight the challenges of designing a resilient FL defense in this setting.","authors":["Georgios Syros","Anshuman Suri","Farinaz Koushanfar","Cristina Nita-Rotaru","Alina Oprea"],"url":"https://arxiv.org/abs/2502.07011"}
{"created":"2025-04-29","title":"Robust high-order low-rank BUG integrators based on explicit Runge-Kutta methods","abstract":"In this work, we propose high-order basis-update & Galerkin (BUG) integrators based on explicit Runge-Kutta methods for large-scale matrix differential equations. These dynamical low-rank integrators are high-order extensions of the BUG integrator and are constructed by performing a BUG step at each stage of the Runge-Kutta method. In this way, the resulting Runge-Kutta BUG integrator is robust to the presence of small singular values and does not involve backward time-integration steps. We provide an error bound, which shows that the Runge-Kutta BUG integrator retains the order of convergence of the associated Runge-Kutta method until the error reaches a plateau corresponding to the low-rank truncation error and which vanishes as the rank becomes full. This error bound is finally validated experimentally on three numerical test cases. The results demonstrate the high-order convergence of the Runge-Kutta BUG integrator and its superior accuracy compared to other dynamical low-rank integrators proposed in the literature.","authors":["Fabio Nobile","S\\'ebastien Riffaud"],"url":"https://arxiv.org/abs/2502.07040"}
{"created":"2025-04-29","title":"Zero-Knowledge Proof Frameworks: A Systematic Survey","abstract":"Zero-Knowledge Proofs (ZKPs) are a cryptographic primitive that allows a prover to demonstrate knowledge of a secret value to a verifier without revealing anything about the secret itself. ZKPs have shown to be an extremely powerful tool, as evidenced in both industry and academic settings. In recent years, the utilization of user data in practical applications has necessitated the rapid development of privacy-preserving techniques, including ZKPs. This has led to the creation of several robust open-source ZKP frameworks. However, there remains a significant gap in understanding the capabilities and real-world applications of these frameworks. Furthermore, identifying the most suitable frameworks for the developers' specific applications and settings is a challenge, given the variety of options available. The primary goal of our work is to lower the barrier to entry for understanding and building applications with open-source ZKP frameworks.","authors":["Nojan Sheybani","Anees Ahmed","Michel Kinsy","Farinaz Koushanfar"],"url":"https://arxiv.org/abs/2502.07063"}
{"created":"2025-04-29","title":"One-Shot Learning for k-SAT","abstract":"Consider a $k$-SAT formula $\\Phi$ where every variable appears at most $d$ times, and let $\\sigma$ be a satisfying assignment of $\\Phi$ sampled proportionally to $e^{\\beta m(\\sigma)}$ where $m(\\sigma)$ is the number of variables set to true and $\\beta$ is a real parameter. Given $\\Phi$ and $\\sigma$, can we learn the value of $\\beta$ efficiently?","authors":["Andreas Galanis","Leslie Ann Goldberg","Xusheng Zhang"],"url":"https://arxiv.org/abs/2502.07135"}
{"created":"2025-04-29","title":"From Brainwaves to Brain Scans: A Robust Neural Network for EEG-to-fMRI Synthesis","abstract":"While functional magnetic resonance imaging (fMRI) offers valuable insights into brain activity, it is limited by high operational costs and significant infrastructural demands. In contrast, electroencephalography (EEG) provides millisecond-level precision in capturing electrical activity but lacks the spatial fidelity necessary for precise neural localization. To bridge these gaps, we propose E2fNet, a simple yet effective deep learning model for synthesizing fMRI images from low-cost EEG data. E2fNet is an encoder-decoder network specifically designed to capture and translate meaningful multi-scale features from EEG across electrode channels into accurate fMRI representations. Extensive evaluations across three public datasets demonstrate that E2fNet consistently outperforms existing CNN- and transformer-based methods, achieving state-of-the-art results in terms of the structural similarity index measure (SSIM). These results demonstrate that E2fNet is a promising, cost-effective solution for enhancing neuroimaging capabilities. The code is available at https://github.com/kgr20/E2fNet.","authors":["Kristofer Grover Roos","Atsushi Fukuda","Quan Huu Cap"],"url":"https://arxiv.org/abs/2502.08025"}
{"created":"2025-04-29","title":"Keep your distance: learning dispersed embeddings on $\\mathbb{S}_d$","abstract":"Learning well-separated features in high-dimensional spaces, such as text or image embeddings, is crucial for many machine learning applications. Achieving such separation can be effectively accomplished through the dispersion of embeddings, where unrelated vectors are pushed apart as much as possible. By constraining features to be on a hypersphere, we can connect dispersion to well-studied problems in mathematics and physics, where optimal solutions are known for limited low-dimensional cases. However, in representation learning we typically deal with a large number of features in high-dimensional space, and moreover, dispersion is usually traded off with some other task-oriented training objective, making existing theoretical and numerical solutions inapplicable. Therefore, it is common to rely on gradient-based methods to encourage dispersion, usually by minimizing some function of the pairwise distances. In this work, we first give an overview of existing methods from disconnected literature, making new connections and highlighting similarities. Next, we introduce some new angles. We propose to reinterpret pairwise dispersion using a maximum mean discrepancy (MMD) motivation. We then propose an online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an effective alternative regularizer for dispersion on generic domains. Finally, we derive a novel dispersion method that directly exploits properties of the hypersphere. Our experiments show the importance of dispersion in image classification and natural language processing tasks, and how algorithms exhibit different trade-offs in different regimes.","authors":["Evgeniia Tokarchuk","Hua Chang Bakker","Vlad Niculae"],"url":"https://arxiv.org/abs/2502.08231"}
{"created":"2025-04-29","title":"Cost Preserving Dependent Rounding for Allocation Problems","abstract":"We present a dependent randomized rounding scheme, which rounds fractional solutions to integral solutions satisfying certain hard constraints on the output while preserving Chernoff-like concentration properties. In contrast to previous dependent rounding schemes, our algorithm guarantees that the cost of the rounded integral solution does not exceed that of the fractional solution. Our algorithm works for a class of assignment problems with restrictions similar to those of prior works.","authors":["Lars Rohwedder","Arman Rouhani","Leo Wennmann"],"url":"https://arxiv.org/abs/2502.08267"}
{"created":"2025-04-29","title":"CordViP: Correspondence-based Visuomotor Policy for Dexterous Manipulation in Real-World","abstract":"Achieving human-level dexterity in robots is a key objective in the field of robotic manipulation. Recent advancements in 3D-based imitation learning have shown promising results, providing an effective pathway to achieve this goal. However, obtaining high-quality 3D representations presents two key problems: (1) the quality of point clouds captured by a single-view camera is significantly affected by factors such as camera resolution, positioning, and occlusions caused by the dexterous hand; (2) the global point clouds lack crucial contact information and spatial correspondences, which are necessary for fine-grained dexterous manipulation tasks. To eliminate these limitations, we propose CordViP, a novel framework that constructs and learns correspondences by leveraging the robust 6D pose estimation of objects and robot proprioception. Specifically, we first introduce the interaction-aware point clouds, which establish correspondences between the object and the hand. These point clouds are then used for our pre-training policy, where we also incorporate object-centric contact maps and hand-arm coordination information, effectively capturing both spatial and temporal dynamics. Our method demonstrates exceptional dexterous manipulation capabilities, achieving state-of-the-art performance in six real-world tasks, surpassing other baselines by a large margin. Experimental results also highlight the superior generalization and robustness of CordViP to different objects, viewpoints, and scenarios. Code and videos are available on https://aureleopku.github.io/CordViP.","authors":["Yankai Fu","Qiuxuan Feng","Ning Chen","Zichen Zhou","Mengzhen Liu","Mingdong Wu","Tianxing Chen","Shanyu Rong","Jiaming Liu","Hao Dong","Shanghang Zhang"],"url":"https://arxiv.org/abs/2502.08449"}
{"created":"2025-04-29","title":"Optimizing GPT for Video Understanding: Zero-Shot Performance and Prompt Engineering","abstract":"In this study, we tackle industry challenges in video content classification by exploring and optimizing GPT-based models for zero-shot classification across seven critical categories of video quality. We contribute a novel approach to improving GPT's performance through prompt optimization and policy refinement, demonstrating that simplifying complex policies significantly reduces false negatives. Additionally, we introduce a new decomposition-aggregation-based prompt engineering technique, which outperforms traditional single-prompt methods. These experiments, conducted on real industry problems, show that thoughtful prompt design can substantially enhance GPT's performance without additional finetuning, offering an effective and scalable solution for improving video classification.","authors":["Mark Beliaev","Victor Yang","Madhura Raju","Jiachen Sun","Xinghai Hu"],"url":"https://arxiv.org/abs/2502.09573"}
{"created":"2025-04-29","title":"BeamDojo: Learning Agile Humanoid Locomotion on Sparse Footholds","abstract":"Traversing risky terrains with sparse footholds poses a significant challenge for humanoid robots, requiring precise foot placements and stable locomotion. Existing learning-based approaches often struggle on such complex terrains due to sparse foothold rewards and inefficient learning processes. To address these challenges, we introduce BeamDojo, a reinforcement learning (RL) framework designed for enabling agile humanoid locomotion on sparse footholds. BeamDojo begins by introducing a sampling-based foothold reward tailored for polygonal feet, along with a double critic to balancing the learning process between dense locomotion rewards and sparse foothold rewards. To encourage sufficient trial-and-error exploration, BeamDojo incorporates a two-stage RL approach: the first stage relaxes the terrain dynamics by training the humanoid on flat terrain while providing it with task-terrain perceptive observations, and the second stage fine-tunes the policy on the actual task terrain. Moreover, we implement a onboard LiDAR-based elevation map to enable real-world deployment. Extensive simulation and real-world experiments demonstrate that BeamDojo achieves efficient learning in simulation and enables agile locomotion with precise foot placement on sparse footholds in the real world, maintaining a high success rate even under significant external disturbances.","authors":["Huayi Wang","Zirui Wang","Junli Ren","Qingwei Ben","Tao Huang","Weinan Zhang","Jiangmiao Pang"],"url":"https://arxiv.org/abs/2502.10363"}
{"created":"2025-04-29","title":"ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences","abstract":"We introduce ReStyle3D, a novel framework for scene-level appearance transfer from a single style image to a real-world scene represented by multiple views. The method combines explicit semantic correspondences with multi-view consistency to achieve precise and coherent stylization. Unlike conventional stylization methods that apply a reference style globally, ReStyle3D uses open-vocabulary segmentation to establish dense, instance-level correspondences between the style and real-world images. This ensures that each object is stylized with semantically matched textures. It first transfers the style to a single view using a training-free semantic-attention mechanism in a diffusion model. It then lifts the stylization to additional views via a learned warp-and-refine network guided by monocular depth and pixel-wise correspondences. Experiments show that ReStyle3D consistently outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence. User studies further validate its ability to produce photo-realistic, semantically faithful results. Our code, pretrained models, and dataset will be publicly released, to support new applications in interior design, virtual staging, and 3D-consistent stylization.","authors":["Liyuan Zhu","Shengqu Cai","Shengyu Huang","Gordon Wetzstein","Naji Khosravan","Iro Armeni"],"url":"https://arxiv.org/abs/2502.10377"}
{"created":"2025-04-29","title":"Corotational Hinge-based Thin Plates/Shells","abstract":"We present six thin plate/shell models, derived from three distinct types of curvature operators formulated within the corotational frame, for simulating both rest-flat and rest-curved triangular meshes. Each curvature operator derives a curvature expression corresponding to both a plate model and a shell model. The corotational edge-based hinge model uses an edge-based stencil to compute directional curvature, while the corotational FVM hinge model utilizes a triangle-centered stencil, applying the finite volume method (FVM) to superposition directional curvatures across edges, yielding a generalized curvature. The corotational smoothed hinge model also employs a triangle-centered stencil but transforms directional curvatures into a generalized curvature based on a quadratic surface fit. All models assume small strain and small curvature, leading to constant bending energy Hessians, which benefit implicit integrators. Through quantitative benchmarks and qualitative elastodynamic simulations with large time steps, we demonstrate the accuracy, efficiency, and stability of these models. Our contributions enhance the thin plate/shell library for use in both computer graphics and engineering applications.","authors":["Qixin Liang"],"url":"https://arxiv.org/abs/2502.10872"}
{"created":"2025-04-29","title":"Verti-Bench: A General and Scalable Off-Road Mobility Benchmark for Vertically Challenging Terrain","abstract":"Recent advancement in off-road autonomy has shown promises in deploying autonomous mobile robots in outdoor off-road environments. Encouraging results have been reported from both simulated and real-world experiments. However, unlike evaluating off-road perception tasks on static datasets, benchmarking off-road mobility still faces significant challenges due to a variety of factors, including variations in vehicle platforms and terrain properties. Furthermore, different vehicle-terrain interactions need to be unfolded during mobility evaluation, which requires the mobility systems to interact with the environments instead of comparing against a pre-collected dataset. In this paper, we present Verti-Bench, a mobility benchmark that focuses on extremely rugged, vertically challenging off-road environments. 100 unique off-road environments and 1000 distinct navigation tasks with millions of off-road terrain properties, including a variety of geometry and semantics, rigid and deformable surfaces, and large natural obstacles, provide standardized and objective evaluation in high-fidelity multi-physics simulation. Verti-Bench is also scalable to various vehicle platforms with different scales and actuation mechanisms. We also provide datasets from expert demonstration, random exploration, failure cases (rolling over and getting stuck), as well as a gym-like interface for reinforcement learning. We use Verti-Bench to benchmark ten off-road mobility systems, present our findings, and identify future off-road mobility research directions.","authors":["Tong Xu","Chenhui Pan","Madhan B. Rao","Aniket Datar","Anuj Pokhrel","Yuanjie Lu","Xuesu Xiao"],"url":"https://arxiv.org/abs/2502.11426"}
{"created":"2025-04-29","title":"Exploring LLM-based Student Simulation for Metacognitive Cultivation","abstract":"Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.","authors":["Haoxuan Li","Jifan Yu","Xin Cong","Yang Dang","Daniel Zhang-li","Yisi Zhan","Huiqin Liu","Zhiyuan Liu"],"url":"https://arxiv.org/abs/2502.11678"}
{"created":"2025-04-29","title":"3D Gaussian Inpainting with Depth-Guided Cross-View Consistency","abstract":"When performing 3D inpainting using novel-view rendering methods like Neural Radiance Field (NeRF) or 3D Gaussian Splatting (3DGS), how to achieve texture and geometry consistency across camera views has been a challenge. In this paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided by the rendered depth information from each training view, our 3DGIC exploits background pixels visible across different views for updating the inpainting mask, allowing us to refine the 3DGS for inpainting purposes.Through extensive experiments on benchmark datasets, we confirm that our 3DGIC outperforms current state-of-the-art 3D inpainting methods quantitatively and qualitatively.","authors":["Sheng-Yu Huang","Zi-Ting Chou","Yu-Chiang Frank Wang"],"url":"https://arxiv.org/abs/2502.11801"}
{"created":"2025-04-29","title":"Blank Space: Adaptive Causal Coding for Streaming Communications Over Multi-Hop Networks","abstract":"In this work, we introduce Blank Space AC-RLNC (BS), a novel Adaptive and Causal Network Coding (AC-RLNC) solution designed to mitigate the triplet trade-off between throughput-delay-efficiency in multi-hop networks. BS leverages the network's physical limitations considering the bottleneck from each node to the destination. In particular, BS introduces a light-computational re-encoding algorithm, called Network AC-RLNC (NET), implemented independently at intermediate nodes. NET adaptively adjusts the Forward Error Correction (FEC) rates and schedules idle periods. It incorporates two distinct suspension mechanisms: 1) Blank Space Period, accounting for the forward-channels bottleneck, and 2) No-New No-FEC approach, based on data availability. The experimental results achieve significant improvements in resource efficiency, demonstrating a 20% reduction in channel usage compared to baseline RLNC solutions. Notably, these efficiency gains are achieved while maintaining competitive throughput and delay performance, ensuring improved resource utilization does not compromise network performance.","authors":["Adina Waxman","Shai Ginzach","Aviel Glam","Alejandro Cohen"],"url":"https://arxiv.org/abs/2502.11984"}
{"created":"2025-04-29","title":"Learning Getting-Up Policies for Real-World Humanoid Robots","abstract":"Automatic fall recovery is a crucial prerequisite before humanoid robots can be reliably deployed. Hand-designing controllers for getting up is difficult because of the varied configurations a humanoid can end up in after a fall and the challenging terrains humanoid robots are expected to operate on. This paper develops a learning framework to produce controllers that enable humanoid robots to get up from varying configurations on varying terrains. Unlike previous successful applications of learning to humanoid locomotion, the getting-up task involves complex contact patterns (which necessitates accurately modeling of the collision geometry) and sparser rewards. We address these challenges through a two-phase approach that induces a curriculum. The first stage focuses on discovering a good getting-up trajectory under minimal constraints on smoothness or speed / torque limits. The second stage then refines the discovered motions into deployable (i.e. smooth and slow) motions that are robust to variations in initial configuration and terrains. We find these innovations enable a real-world G1 humanoid robot to get up from two main situations that we considered: a) lying face up and b) lying face down, both tested on flat, deformable, slippery surfaces and slopes (e.g., sloppy grass and snowfield). This is one of the first successful demonstrations of learned getting-up policies for human-sized humanoid robots in the real world.","authors":["Xialin He","Runpei Dong","Zixuan Chen","Saurabh Gupta"],"url":"https://arxiv.org/abs/2502.12152"}
{"created":"2025-04-29","title":"InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context","abstract":"Large language models excel at following explicit instructions, but they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses instead of seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. This benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue by asking clarifying questions before providing appropriate responses. Our evaluation of both open and closed models reveals that, while proprietary models generally perform better, all current assistants struggle to gather critical information effectively. They often require multiple turns to infer user intent and frequently default to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models' information-seeking capabilities, which can be leveraged to automatically generate data for self-improvement. We also offer insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.","authors":["Bryan L. M. de Oliveira","Luana G. B. Martins","Bruno Brand\\~ao","Luckeciano C. Melo"],"url":"https://arxiv.org/abs/2502.12257"}
{"created":"2025-04-29","title":"HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit","abstract":"Generalizable humanoid loco-manipulation poses significant challenges, requiring coordinated whole-body control and precise, contact-rich object manipulation. To address this, this paper introduces HOMIE, a semi-autonomous teleoperation system that combines a reinforcement learning policy for body control mapped to a pedal, an isomorphic exoskeleton arm for arm control, and motion-sensing gloves for hand control, forming a unified cockpit to freely operate humanoids and establish a data flywheel. The policy incorporates novel designs, including an upper-body pose curriculum, a height-tracking reward, and symmetry utilization. These features enable the system to perform walking and squatting to specific heights while seamlessly adapting to arbitrary upper-body poses. The exoskeleton, by eliminating the reliance on inverse dynamics, delivers faster and more precise arm control. The gloves utilize Hall sensors instead of servos, allowing even compact devices to achieve 15 or more degrees of freedom and freely adapt to any model of dexterous hands. Compared to previous teleoperation systems, HOMIE stands out for its exceptional efficiency, completing tasks in half the time; its expanded working range, allowing users to freely reach high and low areas as well as interact with any objects; and its affordability, with a price of just $500. The system is fully open-source, demos and code can be found in our https://homietele.github.io/.","authors":["Qingwei Ben","Feiyu Jia","Jia Zeng","Junting Dong","Dahua Lin","Jiangmiao Pang"],"url":"https://arxiv.org/abs/2502.13013"}
{"created":"2025-04-29","title":"Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation","abstract":"Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.","authors":["Sha Li","Naren Ramakrishnan"],"url":"https://arxiv.org/abs/2502.13019"}
{"created":"2025-04-29","title":"MatterChat: A Multi-Modal LLM for Material Science","abstract":"Understanding and predicting the properties of inorganic materials is crucial for accelerating advancements in materials science and driving applications in energy, electronics, and beyond. Integrating material structure data with language-based information through multi-modal large language models (LLMs) offers great potential to support these efforts by enhancing human-AI interaction. However, a key challenge lies in integrating atomic structures at full resolution into LLMs. In this work, we introduce MatterChat, a versatile structure-aware multi-modal LLM that unifies material structural data and textual inputs into a single cohesive model. MatterChat employs a bridging module to effectively align a pretrained machine learning interatomic potential with a pretrained LLM, reducing training costs and enhancing flexibility. Our results demonstrate that MatterChat significantly improves performance in material property prediction and human-AI interaction, surpassing general-purpose LLMs such as GPT-4. We also demonstrate its usefulness in applications such as more advanced scientific reasoning and step-by-step material synthesis.","authors":["Yingheng Tang","Wenbin Xu","Jie Cao","Weilu Gao","Steve Farrell","Benjamin Erichson","Michael W. Mahoney","Andy Nonaka","Zhi Yao"],"url":"https://arxiv.org/abs/2502.13107"}
{"created":"2025-04-29","title":"Atomic Proximal Policy Optimization for Electric Robo-Taxi Dispatch and Charger Allocation","abstract":"Pioneering companies such as Waymo have deployed robo-taxi services in several U.S. cities. These robo-taxis are electric vehicles, and their operations require the joint optimization of ride matching, vehicle repositioning, and charging scheduling in a stochastic environment. We model the operations of the ride-hailing system with robo-taxis as a discrete-time, average-reward Markov Decision Process with an infinite horizon. As the fleet size grows, dispatching becomes challenging, as both the system state space and the fleet dispatching action space grow exponentially with the number of vehicles. To address this, we introduce a scalable deep reinforcement learning algorithm, called Atomic Proximal Policy Optimization (Atomic-PPO), that reduces the action space using atomic action decomposition. We evaluate our algorithm using real-world NYC for-hire vehicle trip records and measure its performance by the long-run average reward achieved by the dispatching policy, relative to a fluid-based upper bound. Our experiments demonstrate the superior performance of Atomic-PPO compared to benchmark methods. Furthermore, we conduct extensive numerical experiments to analyze the efficient allocation of charging facilities and assess the impact of vehicle range and charger speed on system performance.","authors":["Jim Dai","Manxi Wu","Zhanhao Zhang"],"url":"https://arxiv.org/abs/2502.13392"}
{"created":"2025-04-29","title":"PSCon: Product Search Through Conversations","abstract":"Conversational Product Search ( CPS ) systems interact with users via natural language to offer personalized and context-aware product lists. However, most existing research on CPS is limited to simulated conversations, due to the lack of a real CPS dataset driven by human-like language. Moreover, existing conversational datasets for e-commerce are constructed for a particular market or a particular language and thus can not support cross-market and multi-lingual usage. In this paper, we propose a CPS data collection protocol and create a new CPS dataset, called PSCon, which assists product search through conversations with human-like language. The dataset is collected by a coached human-human data collection protocol and is available for dual markets and two languages. By formulating the task of CPS, the dataset allows for comprehensive and in-depth research on six subtasks: user intent detection, keyword extraction, system action prediction, question selection, item ranking, and response generation. Moreover, we present a concise analysis of the dataset and propose a benchmark model on the proposed CPS dataset. Our proposed dataset and model will be helpful for facilitating future research on CPS.","authors":["Jie Zou","Mohammad Aliannejadi","Evangelos Kanoulas","Shuxi Han","Heli Ma","Zheng Wang","Yang Yang","Heng Tao Shen"],"url":"https://arxiv.org/abs/2502.13881"}
{"created":"2025-04-29","title":"Grid Labeling: Crowdsourcing Task-Specific Importance from Visualizations","abstract":"Knowing where people look in visualizations is key to effective design. Yet, existing research primarily focuses on free-viewing-based saliency models - although visual attention is inherently task-dependent. Collecting task-relevant importance data remains a resource-intensive challenge. To address this, we introduce Grid Labeling - a novel annotation method for collecting task-specific importance data to enhance saliency prediction models. Grid Labeling dynamically segments visualizations into Adaptive Grids, enabling efficient, low-effort annotation while adapting to visualization structure. We conducted a human subject study comparing Grid Labeling with existing annotation methods, ImportAnnots, and BubbleView across multiple metrics. Results show that Grid Labeling produces the least noisy data and the highest inter-participant agreement with fewer participants while requiring less physical (e.g., clicks/mouse movements) and cognitive effort. An interactive demo is available at https://jangsus1.github.io/Grid-Labeling.","authors":["Minsuk Chang","Yao Wang","Huichen Will Wang","Andreas Bulling","Cindy Xiong Bearfield"],"url":"https://arxiv.org/abs/2502.13902"}
{"created":"2025-04-29","title":"Reachability in 3-VASS is Elementary","abstract":"The reachability problem in 3-dimensional vector addition systems with states (3-VASS) is known to be PSpace-hard, and to belong to Tower. We significantly narrow down the complexity gap by proving the problem to be solvable in doubly-exponential space. The result follows from a new upper bound on the length of the shortest path: if there is a path between two configurations of a 3-VASS then there is also one of at most triply-exponential length. We show it by introducing a novel technique of approximating the reachability sets of 2-VASS by small semi-linear sets.","authors":["Wojciech Czerwi\\'nski","Isma\\\"el Jecker","S{\\l}awomir Lasota","{\\L}ukasz Orlikowski"],"url":"https://arxiv.org/abs/2502.13916"}
{"created":"2025-04-29","title":"On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective","abstract":"Generative Foundation Models (GenFMs) have emerged as transformative tools. However, their widespread adoption raises critical concerns regarding trustworthiness across dimensions. This paper presents a comprehensive framework to address these challenges through three key contributions. First, we systematically review global AI governance laws and policies from governments and regulatory bodies, as well as industry practices and standards. Based on this analysis, we propose a set of guiding principles for GenFMs, developed through extensive multidisciplinary collaboration that integrates technical, ethical, legal, and societal perspectives. Second, we introduce TrustGen, the first dynamic benchmarking platform designed to evaluate trustworthiness across multiple dimensions and model types, including text-to-image, large language, and vision-language models. TrustGen leverages modular components--metadata curation, test case generation, and contextual variation--to enable adaptive and iterative assessments, overcoming the limitations of static evaluation methods. Using TrustGen, we reveal significant progress in trustworthiness while identifying persistent challenges. Finally, we provide an in-depth discussion of the challenges and future directions for trustworthy GenFMs, which reveals the complex, evolving nature of trustworthiness, highlighting the nuanced trade-offs between utility and trustworthiness, and consideration for various downstream applications, identifying persistent challenges and providing a strategic roadmap for future research. This work establishes a holistic framework for advancing trustworthiness in GenAI, paving the way for safer and more responsible integration of GenFMs into critical applications. To facilitate advancement in the community, we release the toolkit for dynamic evaluation.","authors":["Yue Huang","Chujie Gao","Siyuan Wu","Haoran Wang","Xiangqi Wang","Yujun Zhou","Yanbo Wang","Jiayi Ye","Jiawen Shi","Qihui Zhang","Yuan Li","Han Bao","Zhaoyi Liu","Tianrui Guan","Dongping Chen","Ruoxi Chen","Kehan Guo","Andy Zou","Bryan Hooi Kuen-Yew","Caiming Xiong","Elias Stengel-Eskin","Hongyang Zhang","Hongzhi Yin","Huan Zhang","Huaxiu Yao","Jaehong Yoon","Jieyu Zhang","Kai Shu","Kaijie Zhu","Ranjay Krishna","Swabha Swayamdipta","Taiwei Shi","Weijia Shi","Xiang Li","Yiwei Li","Yuexing Hao","Yuexing Hao","Zhihao Jia","Zhize Li","Xiuying Chen","Zhengzhong Tu","Xiyang Hu","Tianyi Zhou","Jieyu Zhao","Lichao Sun","Furong Huang","Or Cohen Sasson","Prasanna Sattigeri","Anka Reuel","Max Lamparth","Yue Zhao","Nouha Dziri","Yu Su","Huan Sun","Heng Ji","Chaowei Xiao","Mohit Bansal","Nitesh V. Chawla","Jian Pei","Jianfeng Gao","Michael Backes","Philip S. Yu","Neil Zhenqiang Gong","Pin-Yu Chen","Bo Li","Dawn Song","Xiangliang Zhang"],"url":"https://arxiv.org/abs/2502.14296"}
{"created":"2025-04-29","title":"LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning","abstract":"Long context understanding remains challenging for large language models due to their limited context windows. This paper presents Long Input Fine-Tuning (LIFT), a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs in context, chooses to store and absorb the long input in parameter. By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL. We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.","authors":["Yansheng Mao","Yufei Xu","Jiaqi Li","Fanxu Meng","Haotong Yang","Zilong Zheng","Xiyuan Wang","Muhan Zhang"],"url":"https://arxiv.org/abs/2502.14644"}
{"created":"2025-04-29","title":"Low degree conjecture implies sharp computational thresholds in stochastic block model","abstract":"We investigate implications of the (extended) low-degree conjecture (recently formalized in [MW23]) in the context of the symmetric stochastic block model. Assuming the conjecture holds, we establish that no polynomial-time algorithm can weakly recover community labels below the Kesten-Stigum (KS) threshold. In particular, we rule out polynomial-time estimators that, with constant probability, achieve correlation with the true communities that is significantly better than random. Whereas, above the KS threshold, polynomial-time algorithms are known to achieve constant correlation with the true communities with high probability[Mas14,AS15].","authors":["Jingqiu Ding","Yiding Hua","Lucas Slot","David Steurer"],"url":"https://arxiv.org/abs/2502.15024"}
{"created":"2025-04-29","title":"DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot Trajectories","abstract":"Diffusion models excel at creating images and videos thanks to their multimodal generative capabilities. These same capabilities have made diffusion models increasingly popular in robotics research, where they are used for generating robot motion. However, the stochastic nature of diffusion models is fundamentally at odds with the precise dynamical equations describing the feasible motion of robots. Hence, generating dynamically admissible robot trajectories is a challenge for diffusion models. To alleviate this issue, we introduce DDAT: Diffusion policies for Dynamically Admissible Trajectories to generate provably admissible trajectories of black-box robotic systems using diffusion models. A sequence of states is a dynamically admissible trajectory if each state of the sequence belongs to the reachable set of its predecessor by the robot's equations of motion. To generate such trajectories, our diffusion policies project their predictions onto a dynamically admissible manifold during both training and inference to align the objective of the denoiser neural network with the dynamical admissibility constraint. The auto-regressive nature of these projections along with the black-box nature of robot dynamics render these projections immensely challenging. We thus enforce admissibility by iteratively sampling a polytopic under-approximation of the reachable set of a state onto which we project its predicted successor, before iterating this process with the projected successor. By producing accurate trajectories, this projection eliminates the need for diffusion models to continually replan, enabling one-shot long-horizon trajectory planning. We demonstrate that our framework generates higher quality dynamically admissible robot trajectories through extensive simulations on a quadcopter and various MuJoCo environments, along with real-world experiments on a Unitree GO1 and GO2.","authors":["Jean-Baptiste Bouvier","Kanghyun Ryu","Kartik Nagpal","Qiayuan Liao","Koushil Sreenath","Negar Mehr"],"url":"https://arxiv.org/abs/2502.15043"}
{"created":"2025-04-29","title":"Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision","abstract":"Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM's reasoning ability and drops when the data is noisy or beyond LLM's knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM's instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short.","authors":["Zhouhang Xie","Tushar Khot","Bhavana Dalvi Mishra","Harshit Surana","Julian McAuley","Peter Clark","Bodhisattwa Prasad Majumder"],"url":"https://arxiv.org/abs/2502.15147"}
{"created":"2025-04-29","title":"Verification and Validation for Trustworthy Scientific Machine Learning","abstract":"Scientific machine learning (SciML) models are transforming many scientific disciplines. However, the development of good modeling practices to increase the trustworthiness of SciML has lagged behind its application, limiting its potential impact. The goal of this paper is to start a discussion on establishing consensus-based good practices for predictive SciML. We identify key challenges in applying existing computational science and engineering guidelines, such as verification and validation protocols, and provide recommendations to address these challenges. Our discussion focuses on predictive SciML, which uses machine learning models to learn, improve, and accelerate numerical simulations of physical systems. While centered on predictive applications, our 16 recommendations aim to help researchers conduct and document their modeling processes rigorously across all SciML domains.","authors":["John D. Jakeman","Lorena A. Barba","Joaquim R. R. A. Martins","Thomas O'Leary-Roseberry"],"url":"https://arxiv.org/abs/2502.15496"}
{"created":"2025-04-29","title":"TutorLLM: Customizing Learning Recommendations with Knowledge Tracing and Retrieval-Augmented Generation","abstract":"The integration of AI in education offers significant potential to enhance learning efficiency. Large Language Models (LLMs), such as ChatGPT, Gemini, and Llama, allow students to query a wide range of topics, providing unprecedented flexibility. However, LLMs face challenges, such as handling varying content relevance and lack of personalization. To address these challenges, we propose TutorLLM, a personalized learning recommender LLM system based on Knowledge Tracing (KT) and Retrieval-Augmented Generation (RAG). The novelty of TutorLLM lies in its unique combination of KT and RAG techniques with LLMs, which enables dynamic retrieval of context-specific knowledge and provides personalized learning recommendations based on the student's personal learning state. Specifically, this integration allows TutorLLM to tailor responses based on individual learning states predicted by the Multi-Features with Latent Relations BERT-based KT (MLFBK) model and to enhance response accuracy with a Scraper model. The evaluation includes user assessment questionnaires and performance metrics, demonstrating a 10% improvement in user satisfaction and a 5\\% increase in quiz scores compared to using general LLMs alone.","authors":["Zhaoxing Li","Vahid Yazdanpanah","Jindi Wang","Wen Gu","Lei Shi","Alexandra I. Cristea","Sarah Kiden","Sebastian Stein"],"url":"https://arxiv.org/abs/2502.15709"}
{"created":"2025-04-29","title":"TabulaTime: A Novel Multimodal Deep Learning Framework for Advancing Acute Coronary Syndrome Prediction through Environmental and Clinical Data Integration","abstract":"Acute Coronary Syndromes (ACS), including ST-segment elevation myocardial infarctions (STEMI) and non-ST-segment elevation myocardial infarctions (NSTEMI), remain a leading cause of mortality worldwide. Traditional cardiovascular risk scores rely primarily on clinical data, often overlooking environmental influences like air pollution that significantly impact heart health. Moreover, integrating complex time-series environmental data with clinical records is challenging.","authors":["Xin Zhang","Liangxiu Han","Stephen White","Saad Hassan","Philip A Kalra","James Ritchie","Carl Diver","Jennie Shorley"],"url":"https://arxiv.org/abs/2502.17049"}
{"created":"2025-04-29","title":"Inversion-free feed-forward and feedback control of MSM based actuator with large non-smooth input hysteresis","abstract":"Dynamic systems with a large and non-smooth hysteresis in the feedforward channel challenge the design of feedback control since the instantaneous input gain is varying during the operation, in the worst case between zero and infinity. Magnetic shape memory (MSM) actuators with multi-stable transitions represent such untypical system plant with only the output displacement being measured. This paper provides a case study of designing the feedforward and feedback control system for an MSM-based actuator setup with a fairly high level of the output sensing noise. First, the recently introduced inversion-free feedforward hysteresis compensator is adapted for the Krasnoselskii-Pokrovskii operator model. Then, a robust feedback proportional-integral (PI) loop shaping is performed, while taking into account the lagging behavior of the low-pass filtering and system uncertainties. Experimental results show that the parallel action of feedforward and feedback parts improves the overall performance of position control.","authors":["Michael Ruderman","Gianluca Giostra","Matteo Sette"],"url":"https://arxiv.org/abs/2502.18444"}
{"created":"2025-04-29","title":"Safe and usable kernel extensions with Rex","abstract":"Safe kernel extensions have gained significant traction, evolving from simple packet filters to large, complex programs that customize storage, networking, and scheduling. Existing kernel extension mechanisms like eBPF rely on in-kernel verifiers to ensure safety of kernel extensions by static verification using symbolic execution. We identify significant usability issues -- safe extensions being rejected by the verifier -- due to the language-verifier gap, a mismatch between developers' expectation of program safety provided by a contract with the programming language, and the verifier's expectation.","authors":["Jinghao Jia","Ruowen Qin","Milo Craun","Egor Lukiyanov","Ayush Bansal","Michael V. Le","Hubertus Franke","Hani Jamjoom","Tianyin Xu","Dan Williams"],"url":"https://arxiv.org/abs/2502.18832"}
{"created":"2025-04-29","title":"Evaluating Membership Inference Attacks in heterogeneous-data setups","abstract":"Among all privacy attacks against Machine Learning (ML), membership inference attacks (MIA) attracted the most attention. In these attacks, the attacker is given an ML model and a data point, and they must infer whether the data point was used for training. The attacker also has an auxiliary dataset to tune their inference algorithm.","authors":["Bram van Dartel","Marc Damie","Florian Hahn"],"url":"https://arxiv.org/abs/2502.18986"}
{"created":"2025-04-29","title":"RAMEN: Real-time Asynchronous Multi-agent Neural Implicit Mapping","abstract":"Multi-agent neural implicit mapping allows robots to collaboratively capture and reconstruct complex environments with high fidelity. However, existing approaches often rely on synchronous communication, which is impractical in real-world scenarios with limited bandwidth and potential communication interruptions. This paper introduces RAMEN: Real-time Asynchronous Multi-agEnt Neural implicit mapping, a novel approach designed to address this challenge. RAMEN employs an uncertainty-weighted multi-agent consensus optimization algorithm that accounts for communication disruptions. When communication is lost between a pair of agents, each agent retains only an outdated copy of its neighbor's map, with the uncertainty of this copy increasing over time since the last communication. Using gradient update information, we quantify the uncertainty associated with each parameter of the neural network map. Neural network maps from different agents are brought to consensus on the basis of their levels of uncertainty, with consensus biased towards network parameters with lower uncertainty. To achieve this, we derive a weighted variant of the decentralized consensus alternating direction method of multipliers (C-ADMM) algorithm, facilitating robust collaboration among agents with varying communication and update frequencies. Through extensive evaluations on real-world datasets and robot hardware experiments, we demonstrate RAMEN's superior mapping performance under challenging communication conditions.","authors":["Hongrui Zhao","Boris Ivanovic","Negar Mehr"],"url":"https://arxiv.org/abs/2502.19592"}
{"created":"2025-04-29","title":"Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success","abstract":"Recent vision-language-action models (VLAs) build upon pretrained vision-language models and leverage diverse robot datasets to demonstrate strong task execution, language following ability, and semantic generalization. Despite these successes, VLAs struggle with novel robot setups and require fine-tuning to achieve good performance, yet how to most effectively fine-tune them is unclear given many possible strategies. In this work, we study key VLA adaptation design choices such as different action decoding schemes, action representations, and learning objectives for fine-tuning, using OpenVLA as our representative base model. Our empirical analysis informs an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, a continuous action representation, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and flexibility in the model's input-output specifications. We propose OpenVLA-OFT, an instantiation of this recipe, which sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76.5% to 97.1% while increasing action generation throughput by 26$\\times$. In real-world evaluations, our fine-tuning recipe enables OpenVLA to successfully execute dexterous, high-frequency control tasks on a bimanual ALOHA robot and outperform other VLAs ($\\pi_0$ and RDT-1B) fine-tuned using their default recipes, as well as strong imitation learning policies trained from scratch (Diffusion Policy and ACT) by up to 15% (absolute) in average success rate. We release code for OFT and pretrained model checkpoints at https://openvla-oft.github.io/.","authors":["Moo Jin Kim","Chelsea Finn","Percy Liang"],"url":"https://arxiv.org/abs/2502.19645"}
{"created":"2025-04-29","title":"OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels","abstract":"Top-down attention plays a crucial role in the human vision system, wherein the brain initially obtains a rough overview of a scene to discover salient cues (i.e., overview first), followed by a more careful finer-grained examination (i.e., look closely next). However, modern ConvNets remain confined to a pyramid structure that successively downsamples the feature map for receptive field expansion, neglecting this crucial biomimetic principle. We present OverLoCK, the first pure ConvNet backbone architecture that explicitly incorporates a top-down attention mechanism. Unlike pyramid backbone networks, our design features a branched architecture with three synergistic sub-networks: 1) a Base-Net that encodes low/mid-level features; 2) a lightweight Overview-Net that generates dynamic top-down attention through coarse global context modeling (i.e., overview first); and 3) a robust Focus-Net that performs finer-grained perception guided by top-down attention (i.e., look closely next). To fully unleash the power of top-down attention, we further propose a novel context-mixing dynamic convolution (ContMix) that effectively models long-range dependencies while preserving inherent local inductive biases even when the input resolution increases, addressing critical limitations in existing convolutions. Our OverLoCK exhibits a notable performance improvement over existing methods. For instance, OverLoCK-T achieves a Top-1 accuracy of 84.2%, significantly surpassing ConvNeXt-B while using only around one-third of the FLOPs/parameters. On object detection, our OverLoCK-S clearly surpasses MogaNet-B by 1% in AP^b. On semantic segmentation, our OverLoCK-T remarkably improves UniRepLKNet-T by 1.7% in mIoU. Code is publicly available at https://github.com/LMMMEng/OverLoCK.","authors":["Meng Lou","Yizhou Yu"],"url":"https://arxiv.org/abs/2502.20087"}
{"created":"2025-04-29","title":"Sanity Checking Causal Representation Learning on a Simple Real-World System","abstract":"We evaluate methods for causal representation learning (CRL) on a simple, real-world system where these methods are expected to work. The system consists of a controlled optical experiment specifically built for this purpose, which satisfies the core assumptions of CRL and where the underlying causal factors (the inputs to the experiment) are known, providing a ground truth. We select methods representative of different approaches to CRL and find that they all fail to recover the underlying causal factors. To understand the failure modes of the evaluated algorithms, we perform an ablation on the data by substituting the real data-generating process with a simpler synthetic equivalent. The results reveal a reproducibility problem, as most methods already fail on this synthetic ablation despite its simple data-generating process. Additionally, we observe that common assumptions on the mixing function are crucial for the performance of some of the methods but do not hold in the real data. Our efforts highlight the contrast between the theoretical promise of the state of the art and the challenges in its application. We hope the benchmark serves as a simple, real-world sanity check to further develop and validate methodology, bridging the gap towards CRL methods that work in practice. We make all code and datasets publicly available at github.com/simonbing/CRLSanityCheck","authors":["Juan L. Gamella","Simon Bing","Jakob Runge"],"url":"https://arxiv.org/abs/2502.20099"}
{"created":"2025-04-29","title":"Protecting multimodal large language models against misleading visualizations","abstract":"Visualizations play a pivotal role in daily communication in an increasingly data-driven world. Research on multimodal large language models (MLLMs) for automated chart understanding has accelerated massively, with steady improvements on standard benchmarks. However, for MLLMs to be reliable, they must be robust to misleading visualizations, charts that distort the underlying data, leading readers to draw inaccurate conclusions that may support disinformation. Here, we uncover an important vulnerability: MLLM question-answering accuracy on misleading visualizations drops on average to the level of a random baseline. To address this, we introduce the first inference-time methods to improve performance on misleading visualizations, without compromising accuracy on non-misleading ones. The most effective method extracts the underlying data table and uses a text-only LLM to answer the question based on the table. Our findings expose a critical blind spot in current research and establish benchmark results to guide future efforts in reliable MLLMs.","authors":["Jonathan Tonglet","Tinne Tuytelaars","Marie-Francine Moens","Iryna Gurevych"],"url":"https://arxiv.org/abs/2502.20503"}
{"created":"2025-04-29","title":"NutriGen: Personalized Meal Plan Generator Leveraging Large Language Models to Enhance Dietary and Nutritional Adherence","abstract":"Maintaining a balanced diet is essential for overall health, yet many individuals struggle with meal planning due to nutritional complexity, time constraints, and lack of dietary knowledge. Personalized food recommendations can help address these challenges by tailoring meal plans to individual preferences, habits, and dietary restrictions. However, existing dietary recommendation systems often lack adaptability, fail to consider real-world constraints such as food ingredient availability, and require extensive user input, making them impractical for sustainable and scalable daily use. To address these limitations, we introduce NutriGen, a framework based on large language models (LLM) designed to generate personalized meal plans that align with user-defined dietary preferences and constraints. By building a personalized nutrition database and leveraging prompt engineering, our approach enables LLMs to incorporate reliable nutritional references like the USDA nutrition database while maintaining flexibility and ease-of-use. We demonstrate that LLMs have strong potential in generating accurate and user-friendly food recommendations, addressing key limitations in existing dietary recommendation systems by providing structured, practical, and scalable meal plans. Our evaluation shows that Llama 3.1 8B and GPT-3.5 Turbo achieve the lowest percentage errors of 1.55\\% and 3.68\\%, respectively, producing meal plans that closely align with user-defined caloric targets while minimizing deviation and improving precision. Additionally, we compared the performance of DeepSeek V3 against several established models to evaluate its potential in personalized nutrition planning.","authors":["Saman Khamesian","Asiful Arefeen","Stephanie M. Carpenter","Hassan Ghasemzadeh"],"url":"https://arxiv.org/abs/2502.20601"}
{"created":"2025-04-29","title":"HiMo: High-Speed Objects Motion Compensation in Point Clouds","abstract":"LiDAR point cloud is essential for autonomous vehicles, but motion distortions from dynamic objects degrade the data quality. While previous work has considered distortions caused by ego motion, distortions caused by other moving objects remain largely overlooked, leading to errors in object shape and position. This distortion is particularly pronounced in high-speed environments such as highways and in multi-LiDAR configurations, a common setup for heavy vehicles. To address this challenge, we introduce HiMo, a pipeline that repurposes scene flow estimation for non-ego motion compensation, correcting the representation of dynamic objects in point clouds. During the development of HiMo, we observed that existing self-supervised scene flow estimators often produce degenerate or inconsistent estimates under high-speed distortion. We further propose SeFlow++, a real-time scene flow estimator that achieves state-of-the-art performance on both scene flow and motion compensation. Since well-established motion distortion metrics are absent in the literature, we introduce two evaluation metrics: compensation accuracy at a point level and shape similarity of objects. We validate HiMo through extensive experiments on Argoverse 2, ZOD, and a newly collected real-world dataset featuring highway driving and multi-LiDAR-equipped heavy vehicles. Our findings show that HiMo improves the geometric consistency and visual fidelity of dynamic objects in LiDAR point clouds, benefiting downstream tasks such as semantic segmentation and 3D detection. See https://kin-zhang.github.io/HiMo for more details.","authors":["Qingwen Zhang","Ajinkya Khoche","Yi Yang","Li Ling","Sina Sharif Mansouri","Olov Andersson","Patric Jensfelt"],"url":"https://arxiv.org/abs/2503.00803"}
{"created":"2025-04-29","title":"CAGN-GAT Fusion: A Hybrid Contrastive Attentive Graph Neural Network for Network Intrusion Detection","abstract":"Cybersecurity threats are growing, making network intrusion detection essential. Traditional machine learning models remain effective in resource-limited environments due to their efficiency, requiring fewer parameters and less computational time. However, handling short and highly imbalanced datasets remains challenging. In this study, we propose the fusion of a Contrastive Attentive Graph Network and Graph Attention Network (CAGN-GAT Fusion) and benchmark it against 15 other models, including both Graph Neural Networks (GNNs) and traditional ML models. Our evaluation is conducted on four benchmark datasets (KDD-CUP-1999, NSL-KDD, UNSW-NB15, and CICIDS2017) using a short and proportionally imbalanced dataset with a constant size of 5000 samples to ensure fairness in comparison. Results show that CAGN-GAT Fusion demonstrates stable and competitive accuracy, recall, and F1-score, even though it does not achieve the highest performance in every dataset. Our analysis also highlights the impact of adaptive graph construction techniques, including small changes in connections (edge perturbation) and selective hiding of features (feature masking), improving detection performance. The findings confirm that GNNs, particularly CAGN-GAT Fusion, are robust and computationally efficient, making them well-suited for resource-constrained environments. Future work will explore GraphSAGE layers and multiview graph construction techniques to further enhance adaptability and detection accuracy.","authors":["Md Abrar Jahin","Shahriar Soudeep","Fahmid Al Farid","M. F. Mridha","Raihan Kabir","Md Rashedul Islam","Hezerul Abdul Karim"],"url":"https://arxiv.org/abs/2503.00961"}
{"created":"2025-04-29","title":"MRI super-resolution reconstruction using efficient diffusion probabilistic model with residual shifting","abstract":"Objective:This study introduces a residual error-shifting mechanism that drastically reduces sampling steps while preserving critical anatomical details, thus accelerating MRI reconstruction. Approach:We propose a novel diffusion-based SR framework called Res-SRDiff, which integrates residual error shifting into the forward diffusion process. This enables efficient HR image reconstruction by aligning the degraded HR and LR distributions.We evaluated Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional denoising diffusion probabilistic model with vision transformer backbone (TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), gradient magnitude similarity deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main results: Res-SRDiff significantly outperformed all comparative methods in terms of PSNR, SSIM, and GMSD across both datasets, with statistically significant improvements (p-values<<0.05). The model achieved high-fidelity image restoration with only four sampling steps, drastically reducing computational time to under one second per slice, which is substantially faster than conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses further demonstrated that Res-SRDiff effectively preserved fine anatomical details and lesion morphology in both brain and pelvic MRI images. Significance: Our findings show that Res-SRDiff is an efficient and accurate MRI SR method, markedly improving computational efficiency and image quality. Integrating residual error shifting into the diffusion process allows for rapid and robust HR image reconstruction, enhancing clinical MRI workflows and advancing medical imaging research. The source at:https://github.com/mosaf/Res-SRDiff","authors":["Mojtaba Safari","Shansong Wang","Zach Eidex","Qiang Li","Erik H. Middlebrooks","David S. Yu","Xiaofeng Yang"],"url":"https://arxiv.org/abs/2503.01576"}
{"created":"2025-04-29","title":"WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation","abstract":"Object Goal Navigation-requiring an agent to locate a specific object in an unseen environment-remains a core challenge in embodied AI. Although recent progress in Vision-Language Model (VLM)-based agents has demonstrated promising perception and decision-making abilities through prompting, none has yet established a fully modular world model design that reduces risky and costly interactions with the environment by predicting the future state of the world. We introduce WMNav, a novel World Model-based Navigation framework powered by Vision-Language Models (VLMs). It predicts possible outcomes of decisions and builds memories to provide feedback to the policy module. To retain the predicted state of the environment, WMNav proposes the online maintained Curiosity Value Map as part of the world model memory to provide dynamic configuration for navigation policy. By decomposing according to a human-like thinking process, WMNav effectively alleviates the impact of model hallucination by making decisions based on the feedback difference between the world model plan and observation. To further boost efficiency, we implement a two-stage action proposer strategy: broad exploration followed by precise localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses existing zero-shot benchmarks in both success rate and exploration efficiency (absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.","authors":["Dujun Nie","Xianda Guo","Yiqun Duan","Ruijun Zhang","Long Chen"],"url":"https://arxiv.org/abs/2503.02247"}
{"created":"2025-04-29","title":"TFHE-SBC: Software Designs for Fully Homomorphic Encryption over the Torus on Single Board Computers","abstract":"Fully homomorphic encryption (FHE) is a technique that enables statistical processing and machine learning while protecting data, including sensitive information collected by single board computers (SBCs), on a cloud server. Among FHE schemes, the TFHE scheme is capable of homomorphic NAND operations and, unlike other FHE schemes, can perform various operations such as minimum, maximum, and comparison. However, TFHE requires Torus Learning With Error (TLWE) encryption, which encrypts one bit at a time, leading to less efficient encryption and larger ciphertext size compared to other schemes. Additionally, SBCs have a limited number of hardware accelerators compared to servers, making it challenging to achieve the same level of optimization as on servers. In this study, we propose a novel SBC-specific design, \\textsf{TFHE-SBC}, to accelerate client-side TFHE operations and enhance communication and energy efficiency. Experimental results demonstrate that \\textsf{TFHE-SBC} encryption is up to 2486 times faster, improves communication efficiency by 512 times, and achieves 12 to 2004 times greater energy efficiency than the state-of-the-art.","authors":["Marin Matsumoto","Ai Nozaki","Hideki Takase","Masato Oguchi"],"url":"https://arxiv.org/abs/2503.02559"}
{"created":"2025-04-29","title":"SoK: Knowledge is All You Need: Accelerating Last Mile Delivery for Automated Provenance-based Intrusion Detection with LLMs","abstract":"Recently, provenance-based intrusion detection systems (PIDSes) have been widely proposed for endpoint threat analysis. However, due to the lack of systematic integration and utilization of knowledge, existing PIDSes still require significant manual intervention for practical deployment, making full automation challenging. This paper presents a disruptive innovation by categorizing PIDSes according to the types of knowledge they utilize. In response to the prevalent issue of ``knowledge silos problem'' in existing research, we introduce a novel knowledge-driven provenance-based intrusion detection framework, powered by large language models (LLMs). We also present OmniSec, a best practice system built upon this framework. By integrating attack representation knowledge, threat intelligence knowledge, and benign behavior knowledge, OmniSec outperforms the state-of-the-art approaches on public benchmark datasets. OmniSec is available online at https://anonymous.4open.science/r/PIDS-with-LLM-613B.","authors":["Wenrui Cheng","Tiantian Zhu","Chunlin Xiong","Haofei Sun","Zijun Wang","Shunan Jing","Mingqi Lv","Yan Chen"],"url":"https://arxiv.org/abs/2503.03108"}
{"created":"2025-04-29","title":"The Beginner's Textbook for Fully Homomorphic Encryption","abstract":"Fully Homomorphic Encryption (FHE) is a cryptographic scheme that enables computations to be performed directly on encrypted data, as if the data were in plaintext. After all computations are performed on the encrypted data, it can be decrypted to reveal the result. The decrypted value matches the result that would have been obtained if the same computations were applied to the plaintext data.","authors":["Ronny Ko"],"url":"https://arxiv.org/abs/2503.05136"}
{"created":"2025-04-29","title":"Fluid Antenna Meets RIS: Random Matrix Analysis and Two-Timescale Design for Multi-User Communications","abstract":"The reconfigurability of fluid antenna systems (FASs) and reconfigurable intelligent surfaces (RISs) provides significant flexibility in optimizing channel conditions by jointly adjusting the positions of fluid antennas and the phase shifts of RISs. However, it is challenging to acquire the instantaneous channel state information (CSI) for both fluid antennas and RISs, while frequent adjustment of antenna positions and phase shifts will significantly increase the system complexity. To tackle this issue, this paper investigates the two-timescale design for FAS-RIS multi-user systems with linear precoding, where only the linear precoder design requires instantaneous CSI of the end-to-end channel, while the FAS and RIS optimization relies on statistical CSI. The main challenge comes from the complex structure of channel and inverse operations in linear precoding, such as regularized zero-forcing (RZF) and zero-forcing (ZF). Leveraging on random matrix theory (RMT), we first investigate the fundamental limits of FAS-RIS systems with RZF/ZF precoding by deriving the ergodic sum rate (ESR). This result is utilized to determine the minimum number of activated antennas to achieve a given ESR. Based on the evaluation result, we propose an algorithm to jointly optimize the antenna selection, regularization factor of RZF, and phase shifts at the RIS. Numerical results validate the accuracy of performance evaluation and demonstrate that the performance gain brought by joint FAS and RIS design is more pronounced with a larger number of users.","authors":["Xin Zhang","Dongfang Xu","Jingjing Wang","Shenghui Song","Derrick Wing Kwan Ng","M\\'erouane Debbah"],"url":"https://arxiv.org/abs/2503.06080"}
{"created":"2025-04-29","title":"Desirable Unfamiliarity: Insights from Eye Movements on Engagement and Readability of Dictation Interfaces","abstract":"Dictation interfaces support efficient text input, but the transcribed text can be hard to read. To understand how users read and review dictated text, we conducted a controlled eye-tracking experiment with 20 participants to compare five dictation interfaces: PLAIN (real-time transcription), AOC (periodic corrections), RAKE (keyword highlights), GP-TSM (grammar-preserving highlights), and SUMMARY (LLM-generated abstraction summary). The study analyzed participants' gaze patterns during their speech composition and reviewing processes. The findings show that during composition, participants spent only 7--11% of their time actively reading, and they favored real-time feedback and avoided distracting interface changes. During reviewing, although SUMMARY introduced unfamiliar words (requiring longer and more frequent fixation), they were easier to read (requiring fewer regressions). Participants preferred SUMMARY for the polished text that preserved fidelity to original meanings. RAKE guided the reading of self-produced text better than GP-TSM. RAKE guides the reading of self-produced text better than GP-TSM. These surprising findings suggest that dictation interfaces could consider showing summaries or key information to support recall instead of raw transcripts.","authors":["Zhaohui Liang","Yonglin Chen","Naser Al Madi","Can Liu"],"url":"https://arxiv.org/abs/2503.08539"}
{"created":"2025-04-29","title":"Differential Privacy Personalized Federated Learning Based on Dynamically Sparsified Client Updates","abstract":"Personalized federated learning is extensively utilized in scenarios characterized by data heterogeneity, facilitating more efficient and automated local training on data-owning terminals. This includes the automated selection of high-performance model parameters for upload, thereby enhancing the overall training process. However, it entails significant risks of privacy leakage. Existing studies have attempted to mitigate these risks by utilizing differential privacy. Nevertheless, these studies present two major limitations: (1) The integration of differential privacy into personalized federated learning lacks sufficient personalization, leading to the introduction of excessive noise into the model. (2) It fails to adequately control the spatial scope of model update information, resulting in a suboptimal balance between data privacy and model effectiveness in differential privacy federated learning. In this paper, we propose a differentially private personalized federated learning approach that employs dynamically sparsified client updates through reparameterization and adaptive norm(DP-pFedDSU). Reparameterization training effectively selects personalized client update information, thereby reducing the quantity of updates. This approach minimizes the introduction of noise to the greatest extent possible. Additionally, dynamic adaptive norm refers to controlling the norm space of model updates during the training process, mitigating the negative impact of clipping on the update information. These strategies substantially enhance the effective integration of differential privacy and personalized federated learning. Experimental results on EMNIST, CIFAR-10, and CIFAR-100 demonstrate that our proposed scheme achieves superior performance and is well-suited for more complex personalized federated learning scenarios.","authors":["Chuanyin Wang","Yifei Zhang","Neng Gao","Qiang Luo"],"url":"https://arxiv.org/abs/2503.09192"}
{"created":"2025-04-29","title":"ST-FlowNet: An Efficient Spiking Neural Network for Event-Based Optical Flow Estimation","abstract":"Spiking Neural Networks (SNNs) have emerged as a promising tool for event-based optical flow estimation tasks due to their ability to leverage spatio-temporal information and low-power capabilities. However, the performance of SNN models is often constrained, limiting their application in real-world scenarios. In this work, we address this gap by proposing a novel neural network architecture, ST-FlowNet, specifically tailored for optical flow estimation from event-based data. The ST-FlowNet architecture integrates ConvGRU modules to facilitate cross-modal feature augmentation and temporal alignment of the predicted optical flow, improving the network's ability to capture complex motion dynamics. Additionally, to overcome the challenges associated with training SNNs, we introduce a novel approach to derive SNN models from pre-trained artificial neural networks (ANNs) through ANN-to-SNN conversion or our proposed BISNN method. Notably, the BISNN method alleviates the complexities involved in biological parameter selection, further enhancing the robustness of SNNs in optical flow estimation tasks. Extensive evaluations on three benchmark event-based datasets demonstrate that the SNN-based ST-FlowNet model outperforms state-of-the-art methods, delivering superior performance in accurate optical flow estimation across a diverse range of dynamic visual scenes. Furthermore, the inherent energy efficiency of SNN models is highlighted, establishing a compelling advantage for their practical deployment. Overall, our work presents a novel framework for optical flow estimation using SNNs and event-based data, contributing to the advancement of neuromorphic vision applications.","authors":["Hongze Sun","Jun Wang","Wuque Cai","Duo Chen","Qianqian Liao","Jiayi He","Yan Cui","Dezhong Yao","Daqing Guo"],"url":"https://arxiv.org/abs/2503.10195"}
{"created":"2025-04-29","title":"Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models","abstract":"Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead.","authors":["Andy Zhou"],"url":"https://arxiv.org/abs/2503.10617"}
{"created":"2025-04-29","title":"Siege: Autonomous Multi-Turn Jailbreaking of Large Language Models with Tree Search","abstract":"We introduce Siege, a multi-turn adversarial framework that models the gradual erosion of Large Language Model (LLM) safety through a tree search perspective. Unlike single-turn jailbreaks that rely on one meticulously engineered prompt, Siege expands the conversation at each turn in a breadth-first fashion, branching out multiple adversarial prompts that exploit partial compliance from previous responses. By tracking these incremental policy leaks and re-injecting them into subsequent queries, Siege reveals how minor concessions can accumulate into fully disallowed outputs. Evaluations on the JailbreakBench dataset show that Siege achieves a 100% success rate on GPT-3.5-turbo and 97% on GPT-4 in a single multi-turn run, using fewer queries than baselines such as Crescendo or GOAT. This tree search methodology offers an in-depth view of how model safeguards degrade over successive dialogue turns, underscoring the urgency of robust multi-turn testing procedures for language models.","authors":["Andy Zhou"],"url":"https://arxiv.org/abs/2503.10619"}
{"created":"2025-04-29","title":"Heterogenous graph neural networks for species distribution modeling","abstract":"Species distribution models (SDMs) are necessary for measuring and predicting occurrences and habitat suitability of species and their relationship with environmental factors. We introduce a novel presence-only SDM with graph neural networks (GNN). In our model, species and locations are treated as two distinct node sets, and the learning task is predicting detection records as the edges that connect locations to species. Using GNN for SDM allows us to model fine-grained interactions between species and the environment. We evaluate the potential of this methodology on the six-region dataset compiled by National Center for Ecological Analysis and Synthesis (NCEAS) for benchmarking SDMs. For each of the regions, the heterogeneous GNN model is comparable to or outperforms previously-benchmarked single-species SDMs as well as a feed-forward neural network baseline model.","authors":["Lauren Harrell","Christine Kaeser-Chen","Burcu Karagol Ayan","Keith Anderson","Michelangelo Conserva","Elise Kleeman","Maxim Neumann","Matt Overlan","Melissa Chapman","Drew Purves"],"url":"https://arxiv.org/abs/2503.11900"}
{"created":"2025-04-29","title":"Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis","abstract":"This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data (often inaccessible during online inference) and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop \\textbf{Point-Cache}, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.","authors":["Hongyu Sun","Qiuhong Ke","Ming Cheng","Yongcai Wang","Deying Li","Chenhui Gou","Jianfei Cai"],"url":"https://arxiv.org/abs/2503.12150"}
{"created":"2025-04-29","title":"Toward Foundation Models for Online Complex Event Detection in CPS-IoT: A Case Study","abstract":"Complex events (CEs) play a crucial role in CPS-IoT applications, enabling high-level decision-making in domains such as smart monitoring and autonomous systems. However, most existing models focus on short-span perception tasks, lacking the long-term reasoning required for CE detection. CEs consist of sequences of short-time atomic events (AEs) governed by spatiotemporal dependencies. Detecting them is difficult due to long, noisy sensor data and the challenge of filtering out irrelevant AEs while capturing meaningful patterns. This work explores CE detection as a case study for CPS-IoT foundation models capable of long-term reasoning. We evaluate three approaches: (1) leveraging large language models (LLMs), (2) employing various neural architectures that learn CE rules from data, and (3) adopting a neurosymbolic approach that integrates neural models with symbolic engines embedding human knowledge. Our results show that the state-space model, Mamba, which belongs to the second category, outperforms all methods in accuracy and generalization to longer, unseen sensor traces. These findings suggest that state-space models could be a strong backbone for CPS-IoT foundation models for long-span reasoning tasks.","authors":["Liying Han","Gaofeng Dong","Xiaomin Ouyang","Lance Kaplan","Federico Cerutti","Mani Srivastava"],"url":"https://arxiv.org/abs/2503.12282"}
{"created":"2025-04-29","title":"Train Robots in a JIF: Joint Inverse and Forward Dynamics with Human and Robot Demonstrations","abstract":"Pre-training on large datasets of robot demonstrations is a powerful technique for learning diverse manipulation skills but is often limited by the high cost and complexity of collecting robot-centric data, especially for tasks requiring tactile feedback. This work addresses these challenges by introducing a novel method for pre-training with multi-modal human demonstrations. Our approach jointly learns inverse and forward dynamics to extract latent state representations, towards learning manipulation specific representations. This enables efficient fine-tuning with only a small number of robot demonstrations, significantly improving data efficiency. Furthermore, our method allows for the use of multi-modal data, such as combination of vision and touch for manipulation. By leveraging latent dynamics modeling and tactile sensing, this approach paves the way for scalable robot manipulation learning based on human demonstrations.","authors":["Gagan Khandate","Boxuan Wang","Sarah Park","Weizhe Ni","Joaquin Palacios","Kathyrn Lampo","Philippe Wu","Rosh Ho","Eric Chang","Matei Ciocarlie"],"url":"https://arxiv.org/abs/2503.12297"}
{"created":"2025-04-29","title":"AR-1-to-3: Single Image to Consistent 3D Object Generation via Next-View Prediction","abstract":"Novel view synthesis (NVS) is a cornerstone for image-to-3d creation. However, existing works still struggle to maintain consistency between the generated views and the input views, especially when there is a significant camera pose difference, leading to poor-quality 3D geometries and textures. We attribute this issue to their treatment of all target views with equal priority according to our empirical observation that the target views closer to the input views exhibit higher fidelity. With this inspiration, we propose AR-1-to-3, a novel next-view prediction paradigm based on diffusion models that first generates views close to the input views, which are then utilized as contextual information to progressively synthesize farther views. To encode the generated view subsequences as local and global conditions for the next-view prediction, we accordingly develop a stacked local feature encoding strategy (Stacked-LE) and an LSTM-based global feature encoding strategy (LSTM-GE). Extensive experiments demonstrate that our method significantly improves the consistency between the generated views and the input views, producing high-fidelity 3D assets.","authors":["Xuying Zhang","Yupeng Zhou","Kai Wang","Yikai Wang","Zhen Li","Shaohui Jiao","Daquan Zhou","Qibin Hou","Ming-Ming Cheng"],"url":"https://arxiv.org/abs/2503.12929"}
{"created":"2025-04-29","title":"Permutation Learning with Only N Parameters: From SoftSort to Self-Organizing Gaussians","abstract":"Sorting and permutation learning are key concepts in optimization and machine learning, especially when organizing high-dimensional data into meaningful spatial layouts. The Gumbel-Sinkhorn method, while effective, requires N*N parameters to determine a full permutation matrix, making it computationally expensive for large datasets. Low-rank matrix factorization approximations reduce memory requirements to 2NM (with M << N), but they still struggle with very large problems. SoftSort, by providing a continuous relaxation of the argsort operator, allows differentiable 1D sorting, but it faces challenges with multidimensional data and complex permutations. In this paper, we present a novel method for learning permutations using only N parameters, which dramatically reduces storage costs. Our method extends SoftSort by iteratively shuffling the N indices of the elements and applying a few SoftSort optimization steps per iteration. This modification significantly improves sorting quality, especially for multidimensional data and complex optimization criteria, and outperforms pure SoftSort. Our method offers improved memory efficiency and scalability compared to existing approaches, while maintaining high-quality permutation learning. Its dramatically reduced memory requirements make it particularly well-suited for large-scale optimization tasks, such as \"Self-Organizing Gaussians\", where efficient and scalable permutation learning is critical.","authors":["Kai Uwe Barthel","Florian Barthel","Peter Eisert"],"url":"https://arxiv.org/abs/2503.13051"}
{"created":"2025-04-29","title":"Arab Spring's Impact on Science through the Lens of Scholarly Attention, Funding, and Migration","abstract":"The Arab Spring is a major socio-political movement that reshaped democratic aspirations in the Middle East and North Africa, attracting global attention through news, social media, and academic discourse. However, its consequences on the academic landscape in the region are still unclear. Here, we conduct the first study of scholarly attention toward 10 target countries affected by the Arab Spring by analyzing more than 25 million articles published from 2002 to 2019. Using a difference-in-difference statistical framework, we find that most target countries have experienced a significant increase in scholarly attention post-Arab Spring compared to the rest of the world, with Egypt attracting the most attention. We investigate how funding and migration networks relate to scholarly attention and reveal that Saudi Arabia has emerged as a key player among Western nations by attracting researchers and funding projects that shape research on the region.","authors":["Yasaman Asgari","Hongyu Zhou","Ozgur Kadir Ozer","Rezvaneh Rezapour","Mary Ellen Sloane","Alexandre Bovet"],"url":"https://arxiv.org/abs/2503.13238"}
{"created":"2025-04-29","title":"8-Calves Image dataset","abstract":"We introduce the 8-Calves dataset, a benchmark for evaluating object detection and identity preservation in occlusion-rich, temporally consistent environments. Comprising a 1-hour video (67,760 frames) of eight Holstein Friesian calves with unique coat patterns and 900 static frames, the dataset emphasizes real-world challenges like prolonged occlusions, motion blur, and pose variation. By fine-tuning 28 object detectors (YOLO variants, transformers) and evaluating 23 pretrained backbones (ResNet, ConvNextV2, ViTs), we expose critical architectural trade-offs: smaller models (e.g., ConvNextV2 Nano, 15.6M parameters) excel in efficiency and retrieval accuracy, while pure vision transformers lag in occlusion-heavy settings. The dataset's structured design-fixed camera views, natural motion, and verified identities-provides a reproducible testbed for object detection challenges (mAP50:95: 56.5-66.4%), bridging synthetic simplicity and domain-specific complexity. The dataset and benchmark code are all publicly available at https://huggingface.co/datasets/tonyFang04/8-calves. Limitations include partial labeling and detector bias, addressed in later sections.","authors":["Xuyang Fang","Sion Hannuna","Neill Campbell"],"url":"https://arxiv.org/abs/2503.13777"}
{"created":"2025-04-29","title":"Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization","abstract":"We address the problem of semi-supervised domain generalization (SSDG), where the distributions of train and test data differ, and only a small amount of labeled data along with a larger amount of unlabeled data are available during training. Existing SSDG methods that leverage only the unlabeled samples for which the model's predictions are highly confident (confident-unlabeled samples), limit the full utilization of the available unlabeled data. To the best of our knowledge, we are the first to explore a method for incorporating the unconfident-unlabeled samples that were previously disregarded in SSDG setting. To this end, we propose UPCSC to utilize these unconfident-unlabeled samples in SSDG that consists of two modules: 1) Unlabeled Proxy-based Contrastive learning (UPC) module, treating unconfident-unlabeled samples as additional negative pairs and 2) Surrogate Class learning (SC) module, generating positive pairs for unconfident-unlabeled samples using their confusing class set. These modules are plug-and-play and do not require any domain labels, which can be easily integrated into existing approaches. Experiments on four widely used SSDG benchmarks demonstrate that our approach consistently improves performance when attached to baselines and outperforms competing plug-and-play methods. We also analyze the role of our method in SSDG, showing that it enhances class-level discriminability and mitigates domain gaps. The code is available at https://github.com/dongkwani/UPCSC.","authors":["Dongkwan Lee","Kyomin Hwang","Nojun Kwak"],"url":"https://arxiv.org/abs/2503.13915"}
{"created":"2025-04-29","title":"Borsuk-Ulam and Replicable Learning of Large-Margin Halfspaces","abstract":"Recent remarkable advances in learning theory have established that, for total concept classes, list replicability, global stability, differentially private (DP) learnability, and shared-randomness replicability all coincide with the finiteness of Littlestone dimension. Does this equivalence extend to partial concept classes?","authors":["Ari Blondal","Hamed Hatami","Pooya Hatami","Chavdar Lalov","Sivan Tretiak"],"url":"https://arxiv.org/abs/2503.15294"}
{"created":"2025-04-29","title":"APEX-MR: Multi-Robot Asynchronous Planning and Execution for Cooperative Assembly","abstract":"Compared to a single-robot workstation, a multi-robot system offers several advantages: 1) it expands the system's workspace, 2) improves task efficiency, and, more importantly, 3) enables robots to achieve significantly more complex and dexterous tasks, such as cooperative assembly. However, coordinating the tasks and motions of multiple robots is challenging due to issues, e.g. system uncertainty, task efficiency, algorithm scalability, and safety concerns. To address these challenges, this paper studies multi-robot coordination and proposes APEX-MR, an asynchronous planning and execution framework designed to safely and efficiently coordinate multiple robots to achieve cooperative assembly, e.g. LEGO assembly. In particular, APEX-MR provides a systematic approach to post-process multi-robot tasks and motion plans to enable robust asynchronous execution under uncertainty. Experimental results demonstrate that APEX-MR can significantly speed up the execution time of many long-horizon LEGO assembly tasks by 48% compared to sequential planning and 36% compared to synchronous planning on average. To further demonstrate performance, we deploy APEX-MR in a dual-arm system to perform physical LEGO assembly. To our knowledge, this is the first robotic system capable of performing customized LEGO assembly using commercial LEGO bricks. The experimental results demonstrate that the dual-arm system, with APEX-MR, can safely coordinate robot motions, efficiently collaborate, and construct complex LEGO structures. Our project website is available at https://intelligent-control-lab.github.io/APEX-MR/.","authors":["Philip Huang","Ruixuan Liu","Changliu Liu","Jiaoyang Li"],"url":"https://arxiv.org/abs/2503.15836"}
{"created":"2025-04-29","title":"Random-sketching Techniques to Enhance the Numerical Stability of Block Orthogonalization Algorithms for s-step GMRES","abstract":"We integrate random sketching techniques into block orthogonalization schemes needed for s-step GMRES. The resulting block orthogonalization schemes generate the basis vectors whose overall orthogonality error is bounded by machine precision as long as each of the corresponding block vectors are numerically full rank. We implement these randomized block orthogonalization schemes using standard distributed-memory linear algebra kernels for s-step GMRES available in the Trilinos software packages. Our performance results on the Perlmutter supercomputer (with four NVIDIA A100 GPUs per node) demonstrate that these randomized techniques can enhance the numerical stability of the orthogonalization and overall solver, without a significant increase in the execution time.","authors":["Ichitaro Yamazaki","Andrew J. Higgins","Erik G. Boman","Daniel B. Szyld"],"url":"https://arxiv.org/abs/2503.16717"}
{"created":"2025-04-29","title":"An Efficient Alternating Algorithm for ReLU-based Symmetric Matrix Decomposition","abstract":"Symmetric matrix decomposition is an active research area in machine learning. This paper focuses on exploiting the low-rank structure of non-negative and sparse symmetric matrices via the rectified linear unit (ReLU) activation function. We propose the ReLU-based nonlinear symmetric matrix decomposition (ReLU-NSMD) model, introduce an accelerated alternating partial Bregman (AAPB) method for its solution, and present the algorithm's convergence results. Our algorithm leverages the Bregman proximal gradient framework to overcome the challenge of estimating the global $L$-smooth constant in the classic proximal gradient algorithm. Numerical experiments on synthetic and real datasets validate the effectiveness of our model and algorithm.","authors":["Qingsong Wang"],"url":"https://arxiv.org/abs/2503.16846"}
{"created":"2025-04-29","title":"AIJIM: A Scalable Model for Real-Time AI in Environmental Journalism","abstract":"This paper introduces AIJIM, the Artificial Intelligence Journalism Integration Model -- a novel framework for integrating real-time AI into environmental journalism. AIJIM combines Vision Transformer-based hazard detection, crowdsourced validation with 252 validators, and automated reporting within a scalable, modular architecture. A dual-layer explainability approach ensures ethical transparency through fast CAM-based visual overlays and optional LIME-based box-level interpretations. Validated in a 2024 pilot on the island of Mallorca using the NamicGreen platform, AIJIM achieved 85.4\\% detection accuracy and 89.7\\% agreement with expert annotations, while reducing reporting latency by 40\\%. Unlike conventional approaches such as Data-Driven Journalism or AI Fact-Checking, AIJIM provides a transferable model for participatory, community-driven environmental reporting, advancing journalism, artificial intelligence, and sustainability in alignment with the UN Sustainable Development Goals and the EU AI Act.","authors":["Torsten Tiltack"],"url":"https://arxiv.org/abs/2503.17401"}
{"created":"2025-04-29","title":"A Case Study of Scalable Content Annotation Using Multi-LLM Consensus and Human Review","abstract":"Content annotation at scale remains challenging, requiring substantial human expertise and effort. This paper presents a case study in code documentation analysis, where we explore the balance between automation efficiency and annotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a novel semi-automated framework that enhances annotation scalability through the systematic integration of multiple LLMs and targeted human review. Our framework introduces a structured consensus-building mechanism among LLMs and an adaptive review protocol that strategically engages human expertise. Through our case study, we demonstrate that MCHR reduces annotation time by 32% to 100% compared to manual annotation while maintaining high accuracy (85.5% to 98%) across different difficulty levels, from basic binary classification to challenging open-set scenarios.","authors":["Mingyue Yuan","Jieshan Chen","Zhenchang Xing","Gelareh Mohammadi","Aaron Quigley"],"url":"https://arxiv.org/abs/2503.17620"}
{"created":"2025-04-29","title":"Progressive Prompt Detailing for Improved Alignment in Text-to-Image Generative Models","abstract":"Text-to-image generative models often struggle with long prompts detailing complex scenes, diverse objects with distinct visual characteristics and spatial relationships. In this work, we propose SCoPE (Scheduled interpolation of Coarse-to-fine Prompt Embeddings), a training-free method to improve text-to-image alignment by progressively refining the input prompt in a coarse-to-fine-grained manner. Given a detailed input prompt, we first decompose it into multiple sub-prompts which evolve from describing broad scene layout to highly intricate details. During inference, we interpolate between these sub-prompts and thus progressively introduce finer-grained details into the generated image. Our training-free plug-and-play approach significantly enhances prompt alignment, achieves an average improvement of up to +4% in Visual Question Answering (VQA) scores over the Stable Diffusion baselines on 85% of the prompts from the GenAI-Bench dataset.","authors":["Ketan Suhaas Saichandran","Xavier Thomas","Prakhar Kaushik","Deepti Ghadiyaram"],"url":"https://arxiv.org/abs/2503.17794"}
{"created":"2025-04-29","title":"Research impact evaluation based on effective authorship contribution sensitivity: h-leadership index","abstract":"The evaluation of a researcher's performance has traditionally relied on various bibliometric measures, with the h-index being one of the most prominent. However, the h-index only accounts for the number of citations received in a publication and does not account for other factors such as the number of authors or their specific contributions in collaborative works. Therefore, the h-index has been placed on scrutiny as it has motivated academic integrity issues where non-contributing authors get authorship merely for raising their h-index. In this study, we comprehensively evaluate existing metrics in their ability to account for authorship contribution by their position and introduce a novel variant of the h-index, known as the h-leadership index. The h-leadership index aims to advance the fair evaluation of academic contributions in multi-authored publications by giving importance to authorship position beyond the first and last authors, focused by Stanford's ranking of the top 2 \\% of world scientists. We assign weighted citations based on a modified complementary unit Gaussian curve, ensuring that the contributions of middle authors are appropriately recognised. We apply the h-leadership index to analyse the top 50 researchers across the Group of 8 (Go8) universities in Australia, demonstrating its potential to provide a more balanced assessment of research performance. We provide open-source software for extending the work further.","authors":["Hardik A. Jain","Rohitash Chandra"],"url":"https://arxiv.org/abs/2503.18236"}
{"created":"2025-04-29","title":"Video-XL-Pro: Reconstructive Token Compression for Extremely Long Video Understanding","abstract":"Despite advanced token compression techniques, existing multimodal large language models (MLLMs) still struggle with hour-long video understanding. In this work, we propose Video-XL-Pro, an efficient method for extremely long video understanding, built upon Reconstructive Compression of Tokens (ReCoT), a learnable module that leverages self-supervised learning to generate comprehensive and compact video tokens. ReCoT introduces two key components: (i) Dynamic Token Synthesizer (DTS): DTS generates pseudo-video tokens from static image tokens by learning intra-token relationships, which are then used in masked video modeling. (ii) Semantic-Guided Masking (SGM): SGM adaptively masks redundant visual tokens to facilitate more effective reconstructive learning. To improve training efficiency in MLLMs fine-tuning, we introduce a video-specific dataset pruning strategy and design a simple yet Query-aware Selector that enables the model to precisely locate query-relevant video tokens. With only 3B parameters, Video-XL-Pro outperforms most 7B models trained on larger datasets across multiple long video understanding benchmarks. Moreover, it can process over 8K frames on a single A100 GPU while maintaining high-quality performance.","authors":["Xiangrui Liu","Yan Shu","Zheng Liu","Ao Li","Yang Tian","Bo Zhao"],"url":"https://arxiv.org/abs/2503.18478"}
{"created":"2025-04-29","title":"Efficient Distributed Algorithms for Shape Reduction via Reconfigurable Circuits","abstract":"Autonomous reconfiguration of agent-based systems is a key challenge in the study of programmable matter, distributed robotics, and molecular self-assembly. While substantial prior work has focused on size-preserving transformations, much less is known about size-changing transformations. Such transformations find application in natural processes, active self-assembly, and dynamical systems, where structures may evolve through the addition or removal of components controlled by local rules. In this paper, we study efficient distributed algorithms for transforming 2D geometric configurations of simple agents, called shapes, using only local size-changing operations. A novelty of our approach is the use of reconfigurable circuits as the underlying communication model, a recently proposed model enabling instant node-to-node communication via primitive signals. Unlike previous work, we integrate collision avoidance as a core responsibility of the distributed algorithm. We consider two graph update models: connectivity and adjacency. Let $n$ denote the number of agents and $k$ the number of turning points in the initial shape. In the connectivity model, we show that any tree-shaped configuration can be reduced to a single agent using only shrinking operations in $O(k \\log n)$ rounds w.h.p., and to its incompressible form in $O(\\log n)$ rounds w.h.p. given prior knowledge of the incompressible nodes, or in $O(k \\log n)$ rounds otherwise. When both shrinking and growth operations are available, we give an algorithm that transforms any tree to a topologically equivalent one in $O(k \\log n + \\log^2 n)$ rounds w.h.p. On the negative side, we show that one cannot hope for $o(\\log^2 n)$-round transformations for all shapes of $\\Theta(\\log n)$ turning points. In the adjacency model, we show that any connected shape can reduce itself to a single node using only shrinking in $O(\\log n)$ rounds w.h.p.","authors":["Nada Almalki","Siddharth Gupta","Othon Michail","Andreas Padalkin"],"url":"https://arxiv.org/abs/2503.18663"}
{"created":"2025-04-29","title":"Contact-based Grasp Control and Inverse Kinematics for a Five-fingered Robotic Hand","abstract":"This paper presents an implementation and analysis of a five-fingered robotic grasping system that combines contact-based control with inverse kinematics solutions. Using the PyBullet simulation environment and the DexHand v2 model, we demonstrate a comprehensive approach to achieving stable grasps through contact point optimization with force closure validation. Our method achieves movement efficiency ratings between 0.966-0.996 for non-thumb fingers and 0.879 for the thumb, while maintaining positional accuracy within 0.0267-0.0283m for non-thumb digits and 0.0519m for the thumb. The system demonstrates rapid position stabilization at 240Hz simulation frequency and maintains stable contact configurations throughout the grasp execution. Experimental results validate the effectiveness of our approach, while also identifying areas for future enhancement in thumb opposition movements and horizontal plane control.","authors":["Robinson Umeike"],"url":"https://arxiv.org/abs/2503.19171"}
{"created":"2025-04-29","title":"CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications","abstract":"We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is compact, light, low-cost, and robust with an average mean-squared error of 0.11N for force and 0.84mNm for moment when the input ranges from 0~10N and 0~4N in normal and shear directions, respectively. CoinFT is a stack of two rigid PCBs with comb-shaped electrodes connected by an array of silicone rubber pillars. The microcontroller interrogates the electrodes in different subsets in order to enhance sensitivity for measuring 6-axis F/T. The combination of desirable features of CoinFT enables various contact-rich robot interactions at a scale, across different embodiment domains including drones, robot end-effectors, and wearable haptic devices. We demonstrate the utility of CoinFT on drones by performing an attitude-based force control to perform tasks that require careful contact force modulation. The design, fabrication, and firmware of CoinFT are open-sourced at https://hojung-choi.github.io/coinft.github.io/.","authors":["Hojung Choi","Jun En Low","Tae Myung Huh","Gabriela A. Uribe","Seongheon Hong","Kenneth A. W. Hoffman","Julia Di","Tony G. Chen","Andrew A. Stanley","Mark R. Cutkosky"],"url":"https://arxiv.org/abs/2503.19225"}
{"created":"2025-04-29","title":"Unveiling Ruby: Insights from Stack Overflow and Developer Survey","abstract":"Ruby is a widely used open-source programming language, valued for its simplicity, especially in web development. Despite its popularity, with over one million users on GitHub, little is known about the issues faced by Ruby developers. This study aims to investigate the key topics, trends, and difficulties faced by Ruby developers by analyzing over 498,000 Ruby-related questions on Stack Overflow (SO), followed by a survey of 154 Ruby developers. We employed BERTopic modeling and manual analysis to develop a taxonomy of 35 topics, grouped into six main categories. Our findings reveal that Web Application Development is the most commonly discussed category, while Ruby Gem Installation and Configuration Issues emerged as the most challenging topic. Analysis of trends on SO showed a steady decline. A survey of 154 Ruby developers demonstrated that 31.6% of the participants find the Core Ruby Concepts category particularly difficult, while Application Quality and Security is found to be difficult for over 40% of experienced developers. Notably, a comparison between survey responses and SO metrics highlights a misalignment, suggesting that perceived difficulty and objective indicators from SO differ; emphasizing the need for improved metrics to better capture developer challenges. Our study provides insights about the challenges Ruby developers face and strong implications for researchers.","authors":["Nikta Akbarpour (University of British Columbia","Department of Computer Science","Mathematics","Physics","Statistics","Kelowna","BC","Canada)","Ahmad Saleem Mirza (University of British Columbia","Department of Computer Science","Mathematics","Physics","Statistics","Kelowna","BC","Canada)","Erfan Raoofian (University of British Columbia","Department of Computer Science","Mathematics","Physics","Statistics","Kelowna","BC","Canada)","Fatemeh Fard (University of British Columbia","Department of Computer Science","Mathematics","Physics","Statistics","Kelowna","BC","Canada)","Gema Rodr\\'iguez-P\\'erez (University of British Columbia","Department of Computer Science","Mathematics","Physics","Statistics","Kelowna","BC","Canada)"],"url":"https://arxiv.org/abs/2503.19238"}
{"created":"2025-04-29","title":"A Probabilistic Neuro-symbolic Layer for Algebraic Constraint Satisfaction","abstract":"In safety-critical applications, guaranteeing the satisfaction of constraints over continuous environments is crucial, e.g., an autonomous agent should never crash into obstacles or go off-road. Neural models struggle in the presence of these constraints, especially when they involve intricate algebraic relationships. To address this, we introduce a differentiable probabilistic layer that guarantees the satisfaction of non-convex algebraic constraints over continuous variables. This probabilistic algebraic layer (PAL) can be seamlessly plugged into any neural architecture and trained via maximum likelihood without requiring approximations. PAL defines a distribution over conjunctions and disjunctions of linear inequalities, parameterized by polynomials. This formulation enables efficient and exact renormalization via symbolic integration, which can be amortized across different data points and easily parallelized on a GPU. We showcase PAL and our integration scheme on a number of benchmarks for algebraic constraint integration and on real-world trajectory data.","authors":["Leander Kurscheidt","Paolo Morettin","Roberto Sebastiani","Andrea Passerini","Antonio Vergari"],"url":"https://arxiv.org/abs/2503.19466"}
{"created":"2025-04-29","title":"Rethinking Graph Structure Learning in the Era of LLMs","abstract":"Recently, the emergence of LLMs has prompted researchers to integrate language descriptions into graphs, aiming to enhance model encoding capabilities from a data-centric perspective. This graph representation is called text-attributed graphs (TAGs). A review of prior advancements highlights that graph structure learning (GSL) is a pivotal technique for improving data utility, making it highly relevant to efficient TAG learning. However, most GSL methods are tailored for traditional graphs without textual information, underscoring the necessity of developing a new GSL paradigm. Despite clear motivations, it remains challenging: (1) How can we define a reasonable optimization objective for GSL in the era of LLMs, considering the massive parameters in LLM? (2) How can we design an efficient model architecture that enables seamless integration of LLM for this optimization objective? For Question 1, we reformulate existing GSL optimization objectives as a tree optimization framework, shifting the focus from obtaining a well-trained edge predictor to a language-aware tree sampler. For Question 2, we propose decoupled and training-free model design principles for LLM integration, shifting the focus from computation-intensive fine-tuning to more efficient inference. Based on this, we propose Large Language and Tree Assistant (LLaTA), which leverages tree-based LLM in-context learning to enhance the understanding of topology and text, enabling reliable inference and generating improved graph structure. Extensive experiments on 10 datasets demonstrate that LLaTA enjoys flexibility-incorporated with any backbone; scalability-outperforms other LLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive performance.","authors":["Zhihan Zhang","Xunkai Li","Guang Zeng","Hongchao Qin","Ronghua Li","Guoren Wang"],"url":"https://arxiv.org/abs/2503.21223"}
{"created":"2025-04-29","title":"Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community","abstract":"Open-source software communities thrive on global collaboration and contributions from diverse participants. This study explores the Rust programming language ecosystem to understand its contributors' demographic composition and interaction patterns. Our objective is to investigate the phenomenon of participation inequality in key Rust projects and the presence of diversity among them. We studied GitHub pull request data from the year leading up to the release of the latest completed Rust community annual survey in 2023. Specifically, we extracted information from three leading repositories: Rust, Rust Analyzer, and Cargo, and used social network graphs to visualize the interactions and identify central contributors and sub-communities. Social network analysis has shown concerning disparities in gender and geographic representation among contributors who play pivotal roles in collaboration networks and the presence of varying diversity levels in the sub-communities formed. These results suggest that while the Rust community is globally active, the contributor base does not fully reflect the diversity of the wider user community. We conclude that there is a need for more inclusive practices to encourage broader participation and ensure that the contributor base aligns more closely with the diverse global community that utilizes Rust.","authors":["Rohit Dandamudi","Ifeoma Adaji","Gema Rodr\\'iguez-P\\'erez"],"url":"https://arxiv.org/abs/2503.22066"}
{"created":"2025-04-29","title":"Graph Kolmogorov-Arnold Networks for Multi-Cancer Classification and Biomarker Identification, An Interpretable Multi-Omics Approach","abstract":"The integration of heterogeneous multi-omics datasets at a systems level remains a central challenge for developing analytical and computational models in precision cancer diagnostics. This paper introduces Multi-Omics Graph Kolmogorov-Arnold Network (MOGKAN), a deep learning framework that utilizes messenger-RNA, micro-RNA sequences, and DNA methylation samples together with Protein-Protein Interaction (PPI) networks for cancer classification across 31 different cancer types. The proposed approach combines differential gene expression with DESeq2, Linear Models for Microarray (LIMMA), and Least Absolute Shrinkage and Selection Operator (LASSO) regression to reduce multi-omics data dimensionality while preserving relevant biological features. The model architecture is based on the Kolmogorov-Arnold theorem principle and uses trainable univariate functions to enhance interpretability and feature analysis. MOGKAN achieves classification accuracy of 96.28 percent and exhibits low experimental variability in comparison to related deep learning-based models. The biomarkers identified by MOGKAN were validated as cancer-related markers through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis. By integrating multi-omics data with graph-based deep learning, our proposed approach demonstrates robust predictive performance and interpretability with potential to enhance the translation of complex multi-omics data into clinically actionable cancer diagnostics.","authors":["Fadi Alharbi","Nishant Budhiraja","Aleksandar Vakanski","Boyu Zhang","Murtada K. Elbashir","Hrshith Gudur","Mohanad Mohammed"],"url":"https://arxiv.org/abs/2503.22939"}
{"created":"2025-04-29","title":"VideoGen-Eval: Agent-based System for Video Generation Evaluation","abstract":"The rapid advancement of video generation has rendered existing evaluation systems inadequate for assessing state-of-the-art models, primarily due to simple prompts that cannot showcase the model's capabilities, fixed evaluation operators struggling with Out-of-Distribution (OOD) cases, and misalignment between computed metrics and human preferences. To bridge the gap, we propose VideoGen-Eval, an agent evaluation system that integrates LLM-based content structuring, MLLM-based content judgment, and patch tools designed for temporal-dense dimensions, to achieve a dynamic, flexible, and expandable video generation evaluation. Additionally, we introduce a video generation benchmark to evaluate existing cutting-edge models and verify the effectiveness of our evaluation system. It comprises 700 structured, content-rich prompts (both T2V and I2V) and over 12,000 videos generated by 20+ models, among them, 8 cutting-edge models are selected as quantitative evaluation for the agent and human. Extensive experiments validate that our proposed agent-based evaluation system demonstrates strong alignment with human preferences and reliably completes the evaluation, as well as the diversity and richness of the benchmark.","authors":["Yuhang Yang","Ke Fan","Shangkun Sun","Hongxiang Li","Ailing Zeng","FeiLin Han","Wei Zhai","Wei Liu","Yang Cao","Zheng-Jun Zha"],"url":"https://arxiv.org/abs/2503.23452"}
{"created":"2025-04-29","title":"GAL-MAD: Towards Explainable Anomaly Detection in Microservice Applications Using Graph Attention Networks","abstract":"The transition to microservices has revolutionized software architectures, offering enhanced scalability and modularity. However, the distributed and dynamic nature of microservices introduces complexities in ensuring system reliability, making anomaly detection crucial for maintaining performance and functionality. Anomalies stemming from network and performance issues must be swiftly identified and addressed. Existing anomaly detection techniques often rely on statistical models or machine learning methods that struggle with the high-dimensional, interdependent data inherent in microservice applications. Current techniques and available datasets predominantly focus on system traces and logs, limiting their ability to support advanced detection models. This paper addresses these gaps by introducing the RS-Anomic dataset generated using the open-source RobotShop microservice application. The dataset captures multivariate performance metrics and response times under normal and anomalous conditions, encompassing ten types of anomalies. We propose a novel anomaly detection model called Graph Attention and LSTM-based Microservice Anomaly Detection (GAL-MAD), leveraging Graph Attention and Long Short-Term Memory architectures to capture spatial and temporal dependencies in microservices. We utilize SHAP values to localize anomalous services and identify root causes to enhance explainability. Experimental results demonstrate that GAL-MAD outperforms state-of-the-art models on the RS-Anomic dataset, achieving higher accuracy and recall across varying anomaly rates. The explanations provide actionable insights into service anomalies, which benefits system administrators.","authors":["Lahiru Akmeemana","Chamodya Attanayake","Husni Faiz","Sandareka Wickramanayake"],"url":"https://arxiv.org/abs/2504.00058"}
{"created":"2025-04-29","title":"Learning from Disengagements: An Analysis of Safety Driver Interventions during Remote Driving","abstract":"This study investigates disengagements of Remote Driving Systems (RDS) based on interventions by an in-vehicle Safety Drivers (SD) in real-world Operational Design Domains (ODD) with a focus on Remote Driver (RD) performance during their driving training. Based on an analysis of over 14,000 km on remote driving data, the relationship between the driving experience of 25 RD and the frequency of disengagements is systematically investigated. The results show that the number of SD interventions decreases significantly within the first 400 km of driving experience, which illustrates a clear learning curve of the RD. In addition, the most common causes for 183 disengagements analyzed are identified and categorized, whereby four main scenarios for SD interventions were identified and illustrated. The results emphasize the need for experience-based and targeted training programs aimed at developing basic driving skills early on, thereby increasing the safety, controllability and efficiency of RDS, especially in complex urban environment ODDs.","authors":["Ole Hans","J\\\"urgen Adamy"],"url":"https://arxiv.org/abs/2504.00246"}
{"created":"2025-04-29","title":"Extending MovieLens-32M to Provide New Evaluation Objectives","abstract":"Offline evaluation of recommender systems has traditionally treated the problem as a machine learning problem. In the classic case of recommending movies, where the user has provided explicit ratings of which movies they like and don't like, each user's ratings are split into test and train sets, and the evaluation task becomes to predict the held out test data using the training data. This machine learning style of evaluation makes the objective to recommend the movies that a user has watched and rated highly, which is not the same task as helping the user find movies that they would enjoy if they watched them. This mismatch in objective between evaluation and task is a compromise to avoid the cost of asking a user to evaluate recommendations by watching each movie. We offer an extension to the MovieLens-32M dataset that provides for new evaluation objectives. Our primary objective is to predict the movies that a user would be interested in watching, i.e. predict their watchlist. To construct this extension, we recruited MovieLens users, collected their profiles, made recommendations with a diverse set of algorithms, pooled the recommendations, and had the users assess the pools. This paper demonstrates the feasibility of using pooling to construct a test collection for recommender systems. Notably, we found that the traditional machine learning style of evaluation ranks the Popular algorithm, which recommends movies based on total number of ratings in the system, in the middle of the twenty-two recommendation runs we used to build the pools. In contrast, when we rank the runs by users' interest in watching movies, we find that recommending popular movies as a recommendation algorithm becomes one of the worst performing runs. It appears that by asking users to assess their personal recommendations, we can alleviate the issue of popularity bias in the evaluation of top-n recommendation.","authors":["Mark D. Smucker","Houmaan Chamani"],"url":"https://arxiv.org/abs/2504.01863"}
{"created":"2025-04-29","title":"CHARMS: A Cognitive Hierarchical Agent for Reasoning and Motion Stylization in Autonomous Driving","abstract":"To address the challenge of insufficient interactivity and behavioral diversity in autonomous driving decision-making, this paper proposes a Cognitive Hierarchical Agent for Reasoning and Motion Stylization (CHARMS). By leveraging Level-k game theory, CHARMS captures human-like reasoning patterns through a two-stage training pipeline comprising reinforcement learning pretraining and supervised fine-tuning. This enables the resulting models to exhibit diverse and human-like behaviors, enhancing their decision-making capacity and interaction fidelity in complex traffic environments. Building upon this capability, we further develop a scenario generation framework that utilizes the Poisson cognitive hierarchy theory to control the distribution of vehicles with different driving styles through Poisson and binomial sampling. Experimental results demonstrate that CHARMS is capable of both making intelligent driving decisions as an ego vehicle and generating diverse, realistic driving scenarios as environment vehicles. The code for CHARMS is released at https://github.com/chuduanfeng/CHARMS.","authors":["Jingyi Wang","Duanfeng Chu","Zejian Deng","Liping Lu","Jinxiang Wang","Chen Sun"],"url":"https://arxiv.org/abs/2504.02450"}
{"created":"2025-04-29","title":"SCMPPI: Supervised Contrastive Multimodal Framework for Predicting Protein-Protein Interactions","abstract":"Protein-protein interaction (PPI) prediction plays a pivotal role in deciphering cellular functions and disease mechanisms. To address the limitations of traditional experimental methods and existing computational approaches in cross-modal feature fusion and false-negative suppression, we propose SCMPPI-a novel supervised contrastive multimodal framework. By effectively integrating sequence-based features (AAC, DPC, ESMC-CKSAAP) with network topology (Node2Vec embeddings) and incorporating an enhanced contrastive learning strategy with negative sample filtering, SCMPPI achieves superior prediction performance. Extensive experiments on eight benchmark datasets demonstrate its state-of-the-art accuracy(98.13%) and AUC(99.69%), along with excellent cross-species generalization (AUC>99%). Successful applications in CD9 networks, Wnt pathway analysis, and cancer-specific networks further highlight its potential for disease target discovery, establishing SCMPPI as a powerful tool for multimodal biological data analysis.","authors":["Shengrui XU","Tianchi Lu","Zikun Wang","Jixiu Zhai"],"url":"https://arxiv.org/abs/2504.02698"}
{"created":"2025-04-29","title":"Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool Empowered LLM Agents","abstract":"Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools. However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows. In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. We identify a novel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes multiple attack vectors to first hijack the normal control flows of agent tasks, and then collect and pollute confidential or private information within LLM agent systems. To understand the impact of this threat, we developed Chord, a dynamic scanning tool designed to automatically detect real-world agent tools susceptible to XTHP attacks. Our evaluation of 66 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 75\\% are vulnerable to XTHP attacks, highlighting the prevalence of this threat.","authors":["Zichuan Li","Jian Cui","Xiaojing Liao","Luyi Xing"],"url":"https://arxiv.org/abs/2504.03111"}
{"created":"2025-04-29","title":"Unlocking Neural Transparency: Jacobian Maps for Explainable AI in Alzheimer's Detection","abstract":"Alzheimer's disease (AD) leads to progressive cognitive decline, making early detection crucial for effective intervention. While deep learning models have shown high accuracy in AD diagnosis, their lack of interpretability limits clinical trust and adoption. This paper introduces a novel pre-model approach leveraging Jacobian Maps (JMs) within a multi-modal framework to enhance explainability and trustworthiness in AD detection. By capturing localized brain volume changes, JMs establish meaningful correlations between model predictions and well-known neuroanatomical biomarkers of AD. We validate JMs through experiments comparing a 3D CNN trained on JMs versus on traditional preprocessed data, which demonstrates superior accuracy. We also employ 3D Grad-CAM analysis to provide both visual and quantitative insights, further showcasing improved interpretability and diagnostic reliability.","authors":["Yasmine Mustafa","Mohamed Elmahallawy","Tie Luo"],"url":"https://arxiv.org/abs/2504.03230"}
{"created":"2025-04-29","title":"Revisiting Outage for Edge Inference Systems","abstract":"One of the key missions of sixth-generation (6G) mobile networks is to deploy large-scale artificial intelligence (AI) models at the network edge to provide remote-inference services for edge devices. The resultant platform, known as edge inference, will support a wide range of Internet-of-Things applications, such as autonomous driving, industrial automation, and augmented reality. Given the mission-critical and time-sensitive nature of these tasks, it is essential to design edge inference systems that are both reliable and capable of meeting stringent end-to-end (E2E) latency constraints. Existing studies, which primarily focus on communication reliability as characterized by channel outage probability, may fail to guarantee E2E performance, specifically in terms of E2E inference accuracy and latency. To address this limitation, we propose a theoretical framework that introduces and mathematically characterizes the inference outage (InfOut) probability, which quantifies the likelihood that the E2E inference accuracy falls below a target threshold. Under an E2E latency constraint, this framework establishes a fundamental tradeoff between communication overhead (i.e., uploading more sensor observations) and inference reliability as quantified by the InfOut probability. To find a tractable way to optimize this tradeoff, we derive accurate surrogate functions for InfOut probability by applying a Gaussian approximation to the distribution of the received discriminant gain. Experimental results demonstrate the superiority of the proposed design over conventional communication-centric approaches in terms of E2E inference reliability.","authors":["Zhanwei Wang","Qunsong Zeng","Haotian Zheng","Kaibin Huang"],"url":"https://arxiv.org/abs/2504.03686"}
{"created":"2025-04-29","title":"Distillation and Refinement of Reasoning in Small Language Models for Document Re-ranking","abstract":"We present a novel approach for training small language models for reasoning-intensive document ranking that combines knowledge distillation with reinforcement learning optimization. While existing methods often rely on expensive human annotations or large black-box language models, our methodology leverages web data and a teacher LLM to automatically generate high-quality training examples with relevance explanations. By framing document ranking as a reinforcement learning problem and incentivizing explicit reasoning capabilities, we train a compact 3B parameter language model that achieves state-of-the-art performance on the BRIGHT benchmark. Our model ranks third on the leaderboard while using substantially fewer parameters than other approaches, outperforming models that are over 20 times larger. Through extensive experiments, we demonstrate that generating explanations during inference, rather than directly predicting relevance scores, enables more effective reasoning with smaller language models. The self-supervised nature of our method offers a scalable and interpretable solution for modern information retrieval systems.","authors":["Chris Samarinas","Hamed Zamani"],"url":"https://arxiv.org/abs/2504.03947"}
{"created":"2025-04-29","title":"I Can Hear You Coming: RF Sensing for Uncooperative Satellite Evasion","abstract":"This work presents a novel method for leveraging intercepted Radio Frequency (RF) signals to inform a constrained Reinforcement Learning (RL) policy for robust control of a satellite operating in contested environments. Uncooperative satellite engagements with nation-state actors prompts the need for enhanced maneuverability and agility on-orbit. However, robust, autonomous and rapid adversary avoidance capabilities for the space environment is seldom studied. Further, the capability constrained nature of many space vehicles does not afford robust space situational awareness capabilities that can be used for well informed maneuvering. We present a \"Cat & Mouse\" system for training optimal adversary avoidance algorithms using RL. We propose the novel approach of utilizing intercepted radio frequency communication and dynamic spacecraft state as multi-modal input that could inform paths for a mouse to outmaneuver the cat satellite. Given the current ubiquitous use of RF communications, our proposed system can be applicable to a diverse array of satellites. In addition to providing a comprehensive framework for training and implementing a constrained RL policy capable of providing control for robust adversary avoidance, we also explore several optimization based methods for adversarial avoidance. These methods were then tested on real-world data obtained from the Space Surveillance Network (SSN) to analyze the benefits and limitations of different avoidance methods.","authors":["Cameron Mehlman","Gregory Falco"],"url":"https://arxiv.org/abs/2504.03983"}
{"created":"2025-04-29","title":"MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender","abstract":"Large language models (LLMs), known for their comprehension capabilities and extensive knowledge, have been increasingly applied to recommendation systems (RS). Given the fundamental gap between the mechanism of LLMs and the requirement of RS, researchers have focused on fine-tuning LLMs with recommendation-specific data to enhance their performance. Language Modeling Loss (LML), originally designed for language generation tasks, is commonly adopted. However, we identify two critical limitations of LML: 1) it exhibits significant divergence from the recommendation objective; 2) it erroneously treats all fictitious item descriptions as negative samples, introducing misleading training signals.","authors":["Bohao Wang","Feng Liu","Jiawei Chen","Xingyu Lou","Changwang Zhang","Jun Wang","Yuegang Sun","Yan Feng","Chun Chen","Can Wang"],"url":"https://arxiv.org/abs/2504.04178"}
{"created":"2025-04-29","title":"NCL-CIR: Noise-aware Contrastive Learning for Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) seeks to find a target image using a multi-modal query, which combines an image with modification text to pinpoint the target. While recent CIR methods have shown promise, they mainly focus on exploring relationships between the query pairs (image and text) through data augmentation or model design. These methods often assume perfect alignment between queries and target images, an idealized scenario rarely encountered in practice. In reality, pairs are often partially or completely mismatched due to issues like inaccurate modification texts, low-quality target images, and annotation errors. Ignoring these mismatches leads to numerous False Positive Pair (FFPs) denoted as noise pairs in the dataset, causing the model to overfit and ultimately reducing its performance. To address this problem, we propose the Noise-aware Contrastive Learning for CIR (NCL-CIR), comprising two key components: the Weight Compensation Block (WCB) and the Noise-pair Filter Block (NFB). The WCB coupled with diverse weight maps can ensure more stable token representations of multi-modal queries and target images. Meanwhile, the NFB, in conjunction with the Gaussian Mixture Model (GMM) predicts noise pairs by evaluating loss distributions, and generates soft labels correspondingly, allowing for the design of the soft-label based Noise Contrastive Estimation (NCE) loss function. Consequently, the overall architecture helps to mitigate the influence of mismatched and partially matched samples, with experimental results demonstrating that NCL-CIR achieves exceptional performance on the benchmark datasets.","authors":["Peng Gao","Yujian Lee","Zailong Chen","Hui zhang","Xubo Liu","Yiyang Hu","Guquang Jing"],"url":"https://arxiv.org/abs/2504.04339"}
{"created":"2025-04-29","title":"Statistical Management of the False Discovery Rate in Medical Instance Segmentation Based on Conformal Risk Control","abstract":"Instance segmentation plays a pivotal role in medical image analysis by enabling precise localization and delineation of lesions, tumors, and anatomical structures. Although deep learning models such as Mask R-CNN and BlendMask have achieved remarkable progress, their application in high-risk medical scenarios remains constrained by confidence calibration issues, which may lead to misdiagnosis. To address this challenge, we propose a robust quality control framework based on conformal prediction theory. This framework innovatively constructs a risk-aware dynamic threshold mechanism that adaptively adjusts segmentation decision boundaries according to clinical requirements.Specifically, we design a \\textbf{calibration-aware loss function} that dynamically tunes the segmentation threshold based on a user-defined risk level $\\alpha$. Utilizing exchangeable calibration data, this method ensures that the expected FNR or FDR on test data remains below $\\alpha$ with high probability. The framework maintains compatibility with mainstream segmentation models (e.g., Mask R-CNN, BlendMask+ResNet-50-FPN) and datasets (PASCAL VOC format) without requiring architectural modifications. Empirical results demonstrate that we rigorously bound the FDR metric marginally over the test set via our developed calibration framework.","authors":["Mengxia Dai","Wenqian Luo","Tianyang Li"],"url":"https://arxiv.org/abs/2504.04482"}
{"created":"2025-04-29","title":"Synthetic Data Generation & Multi-Step RL for Reasoning & Tool Use","abstract":"Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus shifts toward more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5%, 12.3%, 14.8%, 11.1%, and 15.3% in relative accuracy on GSM8K, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8K (a math dataset) by a relative 16.9%.","authors":["Anna Goldie","Azalia Mirhoseini","Hao Zhou","Irene Cai","Christopher D. Manning"],"url":"https://arxiv.org/abs/2504.04736"}
{"created":"2025-04-29","title":"Automating the Search for Small Hard Examples to Approximation Algorithms","abstract":"Given an approximation algorithm $A$, we want to find the input with the worst approximation ratio, i.e., the input for which $A$'s output's objective value is the worst possible compared to the optimal solution's objective value. Such hard examples shed light on the approximation algorithm's weaknesses, and could help us design better approximation algorithms. When the inputs are discrete (e.g., unweighted graphs), one can find hard examples for small input sizes using brute-force enumeration. However, it's not obvious how to do this when the input space is continuous, as in makespan minimization or bin packing.","authors":["Eklavya Sharma"],"url":"https://arxiv.org/abs/2504.04738"}
{"created":"2025-04-29","title":"AD-Det: Boosting Object Detection in UAV Images with Focused Small Objects and Balanced Tail Classes","abstract":"Object detection in Unmanned Aerial Vehicle (UAV) images poses significant challenges due to complex scale variations and class imbalance among objects. Existing methods often address these challenges separately, overlooking the intricate nature of UAV images and the potential synergy between them. In response, this paper proposes AD-Det, a novel framework employing a coherent coarse-to-fine strategy that seamlessly integrates two pivotal components: Adaptive Small Object Enhancement (ASOE) and Dynamic Class-balanced Copy-paste (DCC). ASOE utilizes a high-resolution feature map to identify and cluster regions containing small objects. These regions are subsequently enlarged and processed by a fine-grained detector. On the other hand, DCC conducts object-level resampling by dynamically pasting tail classes around the cluster centers obtained by ASOE, main-taining a dynamic memory bank for each tail class. This approach enables AD-Det to not only extract regions with small objects for precise detection but also dynamically perform reasonable resampling for tail-class objects. Consequently, AD-Det enhances the overall detection performance by addressing the challenges of scale variations and class imbalance in UAV images through a synergistic and adaptive framework. We extensively evaluate our approach on two public datasets, i.e., VisDrone and UAVDT, and demonstrate that AD-Det significantly outperforms existing competitive alternatives. Notably, AD-Det achieves a 37.5% Average Precision (AP) on the VisDrone dataset, surpassing its counterparts by at least 3.1%.","authors":["Zhenteng Li","Sheng Lian","Dengfeng Pan","Youlin Wang","Wei Liu"],"url":"https://arxiv.org/abs/2504.05601"}
{"created":"2025-04-29","title":"Why is Normalization Necessary for Linear Recommenders?","abstract":"Despite their simplicity, linear autoencoder (LAE)-based models have shown comparable or even better performance with faster inference speed than neural recommender models. However, LAEs face two critical challenges: (i) popularity bias, which tends to recommend popular items, and (ii) neighborhood bias, which overly focuses on capturing local item correlations. To address these issues, this paper first analyzes the effect of two existing normalization methods for LAEs, i.e., random-walk and symmetric normalization. Our theoretical analysis reveals that normalization highly affects the degree of popularity and neighborhood biases among items. Inspired by this analysis, we propose a versatile normalization solution, called Data-Adaptive Normalization (DAN), which flexibly controls the popularity and neighborhood biases by adjusting item- and user-side normalization to align with unique dataset characteristics. Owing to its model-agnostic property, DAN can be easily applied to various LAE-based models. Experimental results show that DAN-equipped LAEs consistently improve existing LAE-based models across six benchmark datasets, with significant gains of up to 128.57% and 12.36% for long-tail items and unbiased evaluations, respectively. Refer to our code in https://github.com/psm1206/DAN.","authors":["Seongmin Park","Mincheol Yoon","Hye-young Kim","Jongwuk Lee"],"url":"https://arxiv.org/abs/2504.05805"}
{"created":"2025-04-29","title":"Higher-order meshless schemes for hyperbolic equations","abstract":"We discuss the order, efficiency, stability and positivity of several meshless schemes for linear scalar hyperbolic equations. Meshless schemes are Generalised Finite Difference Methods (GFDMs) for arbitrary irregular grids in which there is no connectivity between the grid points. We propose a new MUSCL-like meshless scheme that uses a central stencil, with which we can achieve arbitrarily high orders, and compare it to existing meshless upwind schemes and meshless WENO schemes. The stability of the newly proposed scheme is guaranteed by an upwind reconstruction to the midpoints of the stencil. The new meshless MUSCL scheme is also efficient due to the reuse of the GFDM solution in the reconstruction. We combine the new MUSCL scheme with a Multi-dimensional Optimal Order Detection (MOOD) procedure to avoid spurious oscillations at discontinuities. In one spatial dimension, our fourth order MUSCL scheme outperforms existing WENO and upwind schemes in terms of stability and accuracy. In two spatial dimensions, our MUSCL scheme achieves similar accuracy to an existing WENO scheme but is significantly more stable.","authors":["Klaas Willems","Giovanni Samaey","Axel Klar"],"url":"https://arxiv.org/abs/2504.05942"}
{"created":"2025-04-29","title":"Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games","abstract":"Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.","authors":["Seungwon Lim","Seungbeen Lee","Dongjun Min","Youngjae Yu"],"url":"https://arxiv.org/abs/2504.06868"}
{"created":"2025-04-29","title":"Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation","abstract":"Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training.","authors":["Thomas Kerdreux","Alexandre Tuel","Quentin Febvre","Alexis Mouche","Bertrand Chapron"],"url":"https://arxiv.org/abs/2504.06962"}
{"created":"2025-04-29","title":"Towards a Higher Roofline for Matrix-Vector Multiplication in Matrix-Free HOSFEM","abstract":"The high-order/spectral finite element method (HOSFEM) is a widely used numerical method for solving PDEs, with its performance primarily relying on axhelm, a matrix-free kernel for element-local matrix-vector multiplications. In axhelm, geometric factors account for over half of memory access but minimally contribute to computational workload. This imbalance significantly constrains the performance roofline, indicating that further optimization of tensor contraction, the core computation in axhelm, yields only minimal improvements. To overcome this bottleneck, we propose a low-cost on-the-fly recalculation of geometric factors for trilinear elements, thereby unlocking substantial potential for optimizing tensor contraction. The proposed approach is implemented in Nekbone, a standard HOSFEM benchmark. With optimizations such as merging scalar factors, partial recalculation, Tensor Core acceleration, and constant memory utilization, performance reaches 85%-100% of the higher roofline. The optimized kernels achieve speedups of 1.74x-4.10x on NVIDIA A100 and 1.99x-3.77x on DCU K100. This leads to a 1.12x-1.40x speedup for Nekbone.","authors":["Zijian Cao","Qiao Sun","Tiangong Zhang","Huiyuan Li"],"url":"https://arxiv.org/abs/2504.07042"}
{"created":"2025-04-29","title":"OmniCaptioner: One Captioner to Rule Them All","abstract":"We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.","authors":["Yiting Lu","Jiakang Yuan","Zhen Li","Shitian Zhao","Qi Qin","Xinyue Li","Le Zhuo","Licheng Wen","Dongyang Liu","Yuewen Cao","Xiangchao Yan","Xin Li","Tianshuo Peng","Shufei Zhang","Botian Shi","Tao Chen","Zhibo Chen","Lei Bai","Bo Zhang","Peng Gao"],"url":"https://arxiv.org/abs/2504.07089"}
{"created":"2025-04-29","title":"MM-IFEngine: Towards Multimodal Instruction Following","abstract":"The Instruction Following (IF) ability measures how well Multi-modal Large Language Models (MLLMs) understand exactly what users are telling them and whether they are doing it right. Existing multimodal instruction following training data is scarce, the benchmarks are simple with atomic instructions, and the evaluation strategies are imprecise for tasks demanding exact output constraints. To address this, we present MM-IFEngine, an effective pipeline to generate high-quality image-instruction pairs. Our MM-IFEngine pipeline yields large-scale, diverse, and high-quality training data MM-IFInstruct-23k, which is suitable for Supervised Fine-Tuning (SFT) and extended as MM-IFDPO-23k for Direct Preference Optimization (DPO). We further introduce MM-IFEval, a challenging and diverse multi-modal instruction-following benchmark that includes (1) both compose-level constraints for output responses and perception-level constraints tied to the input images, and (2) a comprehensive evaluation pipeline incorporating both rule-based assessment and judge model. We conduct SFT and DPO experiments and demonstrate that fine-tuning MLLMs on MM-IFInstruct-23k and MM-IFDPO-23k achieves notable gains on various IF benchmarks, such as MM-IFEval (+10.2$\\%$), MIA (+7.6$\\%$), and IFEval (+12.3$\\%$). We have fully open-sourced the datasets (both SFT and DPO), evaluation code and training scripts at https://github.com/SYuan03/MM-IFEngine.","authors":["Shengyuan Ding","Shenxi Wu","Xiangyu Zhao","Yuhang Zang","Haodong Duan","Xiaoyi Dong","Pan Zhang","Yuhang Cao","Dahua Lin","Jiaqi Wang"],"url":"https://arxiv.org/abs/2504.07957"}
{"created":"2025-04-29","title":"Ozaki Scheme II: A GEMM-oriented emulation of floating-point matrix multiplication using an integer modular technique","abstract":"This paper addresses emulation algorithms for matrix multiplication. General Matrix-Matrix Multiplication (GEMM), a fundamental operation in the Basic Linear Algebra Subprograms (BLAS), is typically optimized for specific hardware architectures. The Ozaki scheme is a well-established GEMM-based emulation method for matrix multiplication, wherein input matrices are decomposed into several low-precision components to ensure that the resulting matrix product is computed exactly through numerical operations. This study proposes a novel GEMM-based emulation method for matrix multiplication that leverages the Chinese Remainder Theorem. The proposed method inherits the computational efficiency of highly optimized GEMM routines and further enables control over the number of matrix multiplications, which can enhance computational accuracy. We present numerical experiments featuring INT8 Tensor Core operations on GPUs and FP64 arithmetic on CPUs as case studies. The results demonstrate that FP64 emulation using the proposed method achieves performance levels of up to 7.4 to 9.8 TFLOPS on the NVIDIA RTX 4090 and 56.6 to 80.2 TFLOPS on the NVIDIA GH200, exceeding the measured performance of native FP64 arithmetic. Furthermore, for FP64 computations on CPUs, the proposed method achieved up to a 2.3x speedup in emulating quadruple-precision arithmetic compared to the conventional Ozaki scheme.","authors":["Katsuhisa Ozaki","Yuki Uchino","Toshiyuki Imamura"],"url":"https://arxiv.org/abs/2504.08009"}
{"created":"2025-04-29","title":"Learning from Elders: Making an LLM-powered Chatbot for Retirement Communities more Accessible through User-centered Design","abstract":"Low technology and eHealth literacy among older adults in retirement communities hinder engagement with digital tools. To address this, we designed an LLM-powered chatbot prototype using a human-centered approach for a local retirement community. Through interviews and persona development, we prioritized accessibility and dual functionality: simplifying internal information retrieval and improving technology and eHealth literacy. A pilot trial with residents demonstrated high satisfaction and ease of use, but also identified areas for further improvement. Based on the feedback, we refined the chatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt engineering to deliver concise responses. Accessible features like adjustable font size, interface theme and personalized follow-up responses were implemented. Future steps include enabling voice-to-text function and longitudinal intervention studies. Together, our results highlight the potential of LLM-driven chatbots to empower older adults through accessible, personalized interactions, bridging literacy gaps in retirement communities.","authors":["Luna Xingyu Li","Ray-yuan Chung","Feng Chen","Wenyu Zeng","Yein Jeon","Oleg Zaslavsky"],"url":"https://arxiv.org/abs/2504.08985"}
{"created":"2025-04-29","title":"MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation","abstract":"We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features.","authors":["Changhao Li","Yu Xin","Xiaowei Zhou","Ariel Shamir","Hao Zhang","Ligang Liu","Ruizhen Hu"],"url":"https://arxiv.org/abs/2504.09149"}
{"created":"2025-04-29","title":"Conformal Calibration: Ensuring the Reliability of Black-Box AI in Wireless Systems","abstract":"AI is poised to revolutionize telecommunication networks by boosting efficiency, automation, and decision-making. However, the black-box nature of most AI models introduces substantial risk, possibly deterring adoption by network operators. These risks are not addressed by the current prevailing deployment strategy, which typically follows a best-effort train-and-deploy paradigm. This paper reviews conformal calibration, a general framework that moves beyond the state of the art by adopting computationally lightweight, advanced statistical tools that offer formal reliability guarantees without requiring further training or fine-tuning. Conformal calibration encompasses pre-deployment calibration via uncertainty quantification or hyperparameter selection; online monitoring to detect and mitigate failures in real time; and counterfactual post-deployment performance analysis to address \"what if\" diagnostic questions after deployment. By weaving conformal calibration into the AI model lifecycle, network operators can establish confidence in black-box AI models as a dependable enabling technology for wireless systems.","authors":["Osvaldo Simeone","Sangwoo Park","Matteo Zecchin"],"url":"https://arxiv.org/abs/2504.09310"}
{"created":"2025-04-29","title":"Towards Optimal Differentially Private Regret Bounds in Linear MDPs","abstract":"We study regret minimization under privacy constraints in episodic inhomogeneous linear Markov Decision Processes (MDPs), motivated by the growing use of reinforcement learning (RL) in personalized decision-making systems that rely on sensitive user data. In this setting, both transition probabilities and reward functions are assumed to be linear in a feature mapping $\\phi(s, a)$, and we aim to ensure privacy through joint differential privacy (JDP), a relaxation of differential privacy suited to online learning. Prior work has established suboptimal regret bounds by privatizing the LSVI-UCB algorithm, which achieves $\\widetilde{O}(\\sqrt{d^3 H^4 K})$ regret in the non-private setting. Building on recent advances that improve this to near minimax optimal regret $\\widetilde{O}(d\\sqrt{H^{3}K})$ via LSVI-UCB++ with Bernstein-style bonuses, we design a new differentially private algorithm by privatizing LSVI-UCB++ and adapting techniques for variance-aware analysis from offline RL. Our algorithm achieves a regret bound of $\\widetilde{O}(d \\sqrt{H^3 K} + H^{15/4} d^{7/6} K^{1/2} / \\epsilon)$, improving over previous private methods. Empirical results show that our algorithm retains near-optimal utility compared to non-private baselines, indicating that privacy can be achieved with minimal performance degradation in this setting.","authors":["Sharan Sahu"],"url":"https://arxiv.org/abs/2504.09339"}
{"created":"2025-04-29","title":"FROG: Effective Friend Recommendation in Online Games via Modality-aware User Preferences","abstract":"Due to the convenience of mobile devices, the online games have become an important part for user entertainments in reality, creating a demand for friend recommendation in online games. However, none of existing approaches can effectively incorporate the multi-modal user features (e.g., images and texts) with the structural information in the friendship graph, due to the following limitations: (1) some of them ignore the high-order structural proximity between users, (2) some fail to learn the pairwise relevance between users at modality-specific level, and (3) some cannot capture both the local and global user preferences on different modalities. By addressing these issues, in this paper, we propose an end-to-end model FROG that better models the user preferences on potential friends. Comprehensive experiments on both offline evaluation and online deployment at Tencent have demonstrated the superiority of FROG over existing approaches.","authors":["Qiwei Wang","Dandan Lin","Wenqing Lin","Ziming Wu"],"url":"https://arxiv.org/abs/2504.09428"}
{"created":"2025-04-29","title":"Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish","abstract":"The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.","authors":["Ay\\c{s}e Aysu Cengiz","Ahmet Kaan Sever","Elif Ecem \\\"Um\\\"utl\\\"u","Naime \\c{S}eyma Erdem","Burak Aytan","B\\\"u\\c{s}ra Tufan","Abdullah Topraksoy","Esra Dar{\\i}c{\\i}","Cagri Toraman"],"url":"https://arxiv.org/abs/2504.09714"}
{"created":"2025-04-29","title":"Constrained Error-Correcting Codes for Efficient DNA Synthesis","abstract":"DNA synthesis is considered as one of the most expensive components in current DNA storage systems. In this paper, focusing on a common synthesis machine, which generates multiple DNA strands in parallel following a fixed supersequence,we propose constrained codes with polynomial-time encoding and decoding algorithms. Compared to the existing works, our codes simultaneously satisfy both l-runlength limited and {\\epsilon}-balanced constraints. By enumerating all valid sequences, our codes achieve the maximum rate, matching the capacity. Additionally, we design constrained error-correcting codes capable of correcting one insertion or deletion in the obtained DNA sequence while still adhering to the constraints.","authors":["Yajuan Liu","Tolga M. Duman"],"url":"https://arxiv.org/abs/2504.09950"}
{"created":"2025-04-29","title":"Invariance Matters: Empowering Social Recommendation via Graph Invariant Learning","abstract":"Graph-based social recommendation systems have shown significant promise in enhancing recommendation performance, particularly in addressing the issue of data sparsity in user behaviors. Typically, these systems leverage Graph Neural Networks (GNNs) to capture user preferences by incorporating high-order social influences from observed social networks. However, existing graph-based social recommendations often overlook the fact that social networks are inherently noisy, containing task-irrelevant relationships that can hinder accurate user preference learning. The removal of these redundant social relations is crucial, yet it remains challenging due to the lack of ground truth. In this paper, we approach the social denoising problem from the perspective of graph invariant learning and propose a novel method, Social Graph Invariant Learning(SGIL). Specifically,SGIL aims to uncover stable user preferences within the input social graph, thereby enhancing the robustness of graph-based social recommendation systems. To achieve this goal, SGIL first simulates multiple noisy social environments through graph generators. It then seeks to learn environment-invariant user preferences by minimizing invariant risk across these environments. To further promote diversity in the generated social environments, we employ an adversarial training strategy to simulate more potential social noisy distributions. Extensive experimental results demonstrate the effectiveness of the proposed SGIL. The code is available at https://github.com/yimutianyang/SIGIR2025-SGIL.","authors":["Yonghui Yang","Le Wu","Yuxin Liao","Zhuangzhuang He","Pengyang Shao","Richang Hong","Meng Wang"],"url":"https://arxiv.org/abs/2504.10432"}
{"created":"2025-04-29","title":"Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs","abstract":"Large language models (LLMs) perform well in medical QA, but their effectiveness in Japanese contexts is limited due to privacy constraints that prevent the use of commercial models like GPT-4 in clinical settings. As a result, recent efforts focus on instruction-tuning open-source LLMs, though the potential of combining them with retrieval-augmented generation (RAG) remains underexplored. To bridge this gap, we are the first to explore a knowledge graph-based (KG) RAG framework for Japanese medical QA small-scale open-source LLMs. Experimental results show that KG-based RAG has only a limited impact on Japanese medical QA using small-scale open-source LLMs. Further case studies reveal that the effectiveness of the RAG is sensitive to the quality and relevance of the external retrieved content. These findings offer valuable insights into the challenges and potential of applying RAG in Japanese medical QA, while also serving as a reference for other low-resource languages.","authors":["Yingjian Chen","Feiyang Li","Xingyu Song","Tianxiao Li","Zixin Xu","Xiujie Chen","Issey Sukeda","Irene Li"],"url":"https://arxiv.org/abs/2504.10982"}
{"created":"2025-04-29","title":"DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environment","abstract":"Reliable traffic data are essential for understanding urban mobility and developing effective traffic management strategies. This study introduces the DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale urban traffic dataset collected systematically from synchronized drone videos at approximately 250 meters altitude, covering nine interconnected intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle trajectories that include directional information, processed through video synchronization and orthomap alignment, resulting in a comprehensive dataset of 81,699 vehicle trajectories. Through our DRIFT dataset, researchers can simultaneously analyze traffic at multiple scales - from individual vehicle maneuvers like lane-changes and safety metrics such as time-to-collision to aggregate network flow dynamics across interconnected urban intersections. The DRIFT dataset is structured to enable immediate use without additional preprocessing, complemented by open-source models for object detection and trajectory extraction, as well as associated analytical tools. DRIFT is expected to significantly contribute to academic research and practical applications, such as traffic flow analysis and simulation studies. The dataset and related resources are publicly accessible at https://github.com/AIxMobility/The-DRIFT.","authors":["Hyejin Lee","Seokjun Hong","Jeonghoon Song","Haechan Cho","Zhixiong Jin","Byeonghun Kim","Joobin Jin","Jaegyun Im","Byeongjoon Noh","Hwasoo Yeo"],"url":"https://arxiv.org/abs/2504.11019"}
{"created":"2025-04-29","title":"A mixed-integer framework for analyzing neural network-based controllers for piecewise affine systems with bounded disturbances","abstract":"We present a method for representing the closed-loop dynamics of piecewise affine (PWA) systems with bounded additive disturbances and neural network-based controllers as mixed-integer (MI) linear constraints. We show that such representations enable the computation of robustly positively invariant (RPI) sets for the specified system class by solving MI linear programs. These RPI sets can subsequently be used to certify stability and constraint satisfaction. Furthermore, the approach allows to handle non-linear systems based on suitable PWA approximations and corresponding error bounds, which can be interpreted as the bounded disturbances from above.","authors":["Dieter Teichrib","Moritz Schulze Darup"],"url":"https://arxiv.org/abs/2504.11125"}
{"created":"2025-04-29","title":"AutoRAN: Automated and Zero-Touch Open RAN Systems","abstract":"[...] This paper presents AutoRAN, an automated, intent-driven framework for zero-touch provisioning of open, programmable cellular networks. Leveraging cloud-native principles, AutoRAN employs virtualization, declarative infrastructure-as-code templates, and disaggregated micro-services to abstract physical resources and protocol stacks. Its orchestration engine integrates Language Models (LLMs) to translate high-level intents into machine-readable configurations, enabling closed-loop control via telemetry-driven observability. Implemented on a multi-architecture OpenShift cluster with heterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access Network (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of O-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS core, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines. Experimental results demonstrate that AutoRAN is capable of deploying an end-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput, validating its ability to streamline configuration, accelerate testing, and reduce manual intervention with similar performance than non cloud-based implementations. With its novel LLM-assisted intent translation mechanism, and performance-optimized automation workflow for multi-vendor environments, AutoRAN has the potential of advancing the robustness of next-generation cellular supply chains through reproducible, intent-based provisioning across public and private deployments.","authors":["Stefano Maxenti","Ravis Shirkhani","Maxime Elkael","Leonardo Bonati","Salvatore D'Oro","Tommaso Melodia","Michele Polese"],"url":"https://arxiv.org/abs/2504.11233"}
{"created":"2025-04-29","title":"Embodied World Models Emerge from Navigational Task in Open-Ended Environments","abstract":"Spatial reasoning in partially observable environments has often been approached through passive predictive models, yet theories of embodied cognition suggest that genuinely useful representations arise only when perception is tightly coupled to action. Here we ask whether a recurrent agent, trained solely by sparse rewards to solve procedurally generated planar mazes, can autonomously internalize metric concepts such as direction, distance and obstacle layout. After training, the agent consistently produces near-optimal paths in unseen mazes, behavior that hints at an underlying spatial model. To probe this possibility, we cast the closed agent-environment loop as a hybrid dynamical system, identify stable limit cycles in its state space, and characterize behavior with a Ridge Representation that embeds whole trajectories into a common metric space. Canonical correlation analysis exposes a robust linear alignment between neural and behavioral manifolds, while targeted perturbations of the most informative neural dimensions sharply degrade navigation performance. Taken together, these dynamical, representational, and causal signatures show that sustained sensorimotor interaction is sufficient for the spontaneous emergence of compact, embodied world models, providing a principled path toward interpretable and transferable navigation policies.","authors":["Li Jin","Liu Jia"],"url":"https://arxiv.org/abs/2504.11419"}
{"created":"2025-04-29","title":"Support is All You Need for Certified VAE Training","abstract":"Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees.","authors":["Changming Xu","Debangshu Banerjee","Deepak Vasisht","Gagandeep Singh"],"url":"https://arxiv.org/abs/2504.11831"}
{"created":"2025-04-29","title":"SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes","abstract":"We present the Mu-SHROOM shared task which is focused on detecting hallucinations and other overgeneration mistakes in the output of instruction-tuned large language models (LLMs). Mu-SHROOM addresses general-purpose LLMs in 14 languages, and frames the hallucination detection problem as a span-labeling task. We received 2,618 submissions from 43 participating teams employing diverse methodologies. The large number of submissions underscores the interest of the community in hallucination detection. We present the results of the participating systems and conduct an empirical analysis to identify key factors contributing to strong performance in this task. We also emphasize relevant current challenges, notably the varying degree of hallucinations across languages and the high annotator disagreement when labeling hallucination spans.","authors":["Ra\\'ul V\\'azquez","Timothee Mickus","Elaine Zosa","Teemu Vahtola","J\\\"org Tiedemann","Aman Sinha","Vincent Segonne","Fernando S\\'anchez-Vega","Alessandro Raganato","Jind\\v{r}ich Libovick\\'y","Jussi Karlgren","Shaoxiong Ji","Jind\\v{r}ich Helcl","Liane Guillou","Ona de Gibert","Jaione Bengoetxea","Joseph Attieh","Marianna Apidianaki"],"url":"https://arxiv.org/abs/2504.11975"}
{"created":"2025-04-29","title":"Gauging Overprecision in LLMs: An Empirical Study","abstract":"Recently, overconfidence in large language models (LLMs) has garnered considerable attention due to its fundamental importance in quantifying the trustworthiness of LLM generation. However, existing approaches prompt the \\textit{black box LLMs} to produce their confidence (\\textit{verbalized confidence}), which can be subject to many biases and hallucinations. Inspired by a different aspect of overconfidence in cognitive science called \\textit{overprecision}, we designed a framework for its study in black box LLMs. This framework contains three main phases: 1) generation, 2) refinement and 3) evaluation. In the generation phase we prompt the LLM to generate answers to numerical questions in the form of intervals with a certain level of confidence. This confidence level is imposed in the prompt and not required for the LLM to generate as in previous approaches. We use various prompting techniques and use the same prompt multiple times to gauge the effects of randomness in the generation process. In the refinement phase, answers from the previous phase are refined to generate better answers. The LLM answers are evaluated and studied in the evaluation phase to understand its internal workings. This study allowed us to gain various insights into LLM overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) there is no correlation between the length of the interval and the imposed confidence level, which can be symptomatic of a a) lack of understanding of the concept of confidence or b) inability to adjust self-confidence by following instructions, {3) LLM numerical precision differs depending on the task, scale of answer and prompting technique 4) Refinement of answers doesn't improve precision in most cases. We believe this study offers new perspectives on LLM overconfidence and serves as a strong baseline for overprecision in LLMs.","authors":["Adil Bahaj","Hamed Rahimi","Mohamed Chetouani","Mounir Ghogho"],"url":"https://arxiv.org/abs/2504.12098"}
{"created":"2025-04-29","title":"Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting Methods for Clarification Generation","abstract":"In information retrieval (IR), providing appropriate clarifications to better understand users' information needs is crucial for building a proactive search-oriented dialogue system. Due to the strong in-context learning ability of large language models (LLMs), recent studies investigate prompting methods to generate clarifications using few-shot or Chain of Thought (CoT) prompts. However, vanilla CoT prompting does not distinguish the characteristics of different information needs, making it difficult to understand how LLMs resolve ambiguities in user queries. In this work, we focus on the concept of ambiguity for clarification, seeking to model and integrate ambiguities in the clarification process. To this end, we comprehensively study the impact of prompting schemes based on reasoning and ambiguity for clarification. The idea is to enhance the reasoning abilities of LLMs by limiting CoT to predict first ambiguity types that can be interpreted as instructions to clarify, then correspondingly generate clarifications. We name this new prompting scheme Ambiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various datasets containing human-annotated clarifying questions to compare AT-CoT with multiple baselines. We also perform user simulations to implicitly measure the quality of generated clarifications under various IR scenarios.","authors":["Anfu Tang","Laure Soulier","Vincent Guigue"],"url":"https://arxiv.org/abs/2504.12113"}
{"created":"2025-04-29","title":"Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals","abstract":"Identifying spatial regions where biodiversity is threatened is crucial for effective ecosystem conservation and monitoring. In this stydy, we assessed varios machine learning methods to detect grazing trails automatically. We tested five semantic segmentation models combined with 14 different encoder networks. The best combination was UNet with MambaOut encoder. The solution proposed could be used as the basis for tools aiming at mapping and tracking changes in grazing trails on a continuous temporal basis.","authors":["Jose Francisco Diez-Pastor","Francisco Javier Gonzalez-Moya","Pedro Latorre-Carmona","Francisco Javier Perez-Barber\\'ia","Ludmila I. Kuncheva","Antonio Canepa-Oneto","Alvar Arnaiz-Gonz\\'alez","Cesar Garcia-Osorio"],"url":"https://arxiv.org/abs/2504.12121"}
{"created":"2025-04-29","title":"Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification","abstract":"We study continual learning on multiple linear classification tasks by sequentially running gradient descent (GD) for a fixed budget of iterations per task. When all tasks are jointly linearly separable and are presented in a cyclic/random order, we show the directional convergence of the trained linear classifier to the joint (offline) max-margin solution. This is surprising because GD training on a single task is implicitly biased towards the individual max-margin solution for the task, and the direction of the joint max-margin solution can be largely different from these individual solutions. Additionally, when tasks are given in a cyclic order, we present a non-asymptotic analysis on cycle-averaged forgetting, revealing that (1) alignment between tasks is indeed closely tied to catastrophic forgetting and backward knowledge transfer and (2) the amount of forgetting vanishes to zero as the cycle repeats. Lastly, we analyze the case where the tasks are no longer jointly separable and show that the model trained in a cyclic order converges to the unique minimum of the joint loss function.","authors":["Hyunji Jung","Hanseul Cho","Chulhee Yun"],"url":"https://arxiv.org/abs/2504.12712"}
{"created":"2025-04-29","title":"Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs","abstract":"Many works have succeeded in reconstructing Gaussian human avatars from multi-view videos. However, they either struggle to capture pose-dependent appearance details with a single MLP, or rely on a computationally intensive neural network to reconstruct high-fidelity appearance but with rendering performance degraded to non-real-time. We propose a novel Gaussian human avatar representation that can reconstruct high-fidelity pose-dependence appearance with details and meanwhile can be rendered in real time. Our Gaussian avatar is empowered by spatially distributed MLPs which are explicitly located on different positions on human body. The parameters stored in each Gaussian are obtained by interpolating from the outputs of its nearby MLPs based on their distances. To avoid undesired smooth Gaussian property changing during interpolation, for each Gaussian we define a set of Gaussian offset basis, and a linear combination of basis represents the Gaussian property offsets relative to the neutral properties. Then we propose to let the MLPs output a set of coefficients corresponding to the basis. In this way, although Gaussian coefficients are derived from interpolation and change smoothly, the Gaussian offset basis is learned freely without constraints. The smoothly varying coefficients combined with freely learned basis can still produce distinctly different Gaussian property offsets, allowing the ability to learn high-frequency spatial signals. We further use control points to constrain the Gaussians distributed on a surface layer rather than allowing them to be irregularly distributed inside the body, to help the human avatar generalize better when animated under novel poses. Compared to the state-of-the-art method, our method achieves better appearance quality with finer details while the rendering speed is significantly faster under novel views and novel poses.","authors":["Youyi Zhan","Tianjia Shao","Yin Yang","Kun Zhou"],"url":"https://arxiv.org/abs/2504.12909"}
{"created":"2025-04-29","title":"Tilus: A Virtual Machine for Arbitrary Low-Precision GPGPU Computation in LLM Serving","abstract":"Serving Large Language Models (LLMs) is critical for AI-powered applications but demands substantial computational resources, particularly in memory bandwidth and computational throughput. Low-precision computation has emerged as a key technique to improve efficiency while reducing resource consumption. Existing approaches for generating low-precision kernels are limited to weight bit widths that are powers of two and suffer from suboptimal performance due to high-level GPU programming abstractions. These abstractions restrict critical optimizations, such as fine-grained register management and optimized memory access patterns, which are essential for efficient low-precision computations. In this paper, we introduce a virtual machine (VM) designed for General-Purpose GPU (GPGPU) computing, enabling support for low-precision data types with arbitrary bit widths while maintaining GPU programmability. The proposed VM features a thread-block-level programming model, a hierarchical memory space, a novel algebraic layout system, and extensive support for diverse low-precision data types. VM programs are compiled into highly efficient GPU programs with automatic vectorization and instruction selection. Extensive experiments demonstrate that our VM efficiently supports a full spectrum of low-precision data types, and outperforms state-of-the-art low-precision kernels on their supported types. Compared to existing compilers like Triton and Ladder, as well as hand-optimized kernels such as QuantLLM and Marlin, our VM achieves performance improvements of 1.75x, 2.61x, 1.29x and 1.03x, respectively.","authors":["Yaoyao Ding","Bohan Hou","Xiao Zhang","Allan Lin","Tianqi Chen","Cody Yu Hao","Yida Wang","Gennady Pekhimenko"],"url":"https://arxiv.org/abs/2504.12984"}
{"created":"2025-04-29","title":"U-Shape Mamba: State Space Model for faster diffusion","abstract":"Diffusion models have become the most popular approach for high-quality image generation, but their high computational cost still remains a significant challenge. To address this problem, we propose U-Shape Mamba (USM), a novel diffusion model that leverages Mamba-based layers within a U-Net-like hierarchical structure. By progressively reducing sequence length in the encoder and restoring it in the decoder through Mamba blocks, USM significantly lowers computational overhead while maintaining strong generative capabilities. Experimental results against Zigma, which is currently the most efficient Mamba-based diffusion model, demonstrate that USM achieves one-third the GFlops, requires less memory and is faster, while outperforming Zigma in image quality. Frechet Inception Distance (FID) is improved by 15.3, 0.84 and 2.7 points on AFHQ, CelebAHQ and COCO datasets, respectively. These findings highlight USM as a highly efficient and scalable solution for diffusion-based generative models, making high-quality image synthesis more accessible to the research community while reducing computational costs.","authors":["Alex Ergasti","Filippo Botti","Tomaso Fontanini","Claudio Ferrari","Massimo Bertozzi","Andrea Prati"],"url":"https://arxiv.org/abs/2504.13499"}
{"created":"2025-04-29","title":"Compile Scene Graphs with Reinforcement Learning","abstract":"Next token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. Given the structured nature of scene graphs, we design a graph-centric reward function that integrates node-level rewards, edge-level rewards, and a format consistency reward. Our experiments demonstrate that rule-based RL substantially enhances model performance in the SGG task, achieving a zero failure rate--unlike supervised fine-tuning (SFT), which struggles to generalize effectively. Our code is available at https://github.com/gpt4vision/R1-SGG.","authors":["Zuyao Chen","Jinlin Wu","Zhen Lei","Marc Pollefeys","Chang Wen Chen"],"url":"https://arxiv.org/abs/2504.13617"}
{"created":"2025-04-29","title":"Generative AI Act II: Test Time Scaling Drives Cognition Engineering","abstract":"The first generation of Large Language Models - what might be called \"Act I\" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations such as knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of \"Act II\" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI's second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering","authors":["Shijie Xia","Yiwei Qin","Xuefeng Li","Yan Ma","Run-Ze Fan","Steffi Chern","Haoyang Zou","Fan Zhou","Xiangkun Hu","Jiahe Jin","Yanheng He","Yixin Ye","Yixiu Liu","Pengfei Liu"],"url":"https://arxiv.org/abs/2504.13828"}
{"created":"2025-04-29","title":"A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust","abstract":"The integration of Artificial Intelligence (AI) into high-stakes domains such as healthcare, finance, and autonomous systems is often constrained by concerns over transparency, interpretability, and trust. While Human-Centered AI (HCAI) emphasizes alignment with human values, Explainable AI (XAI) enhances transparency by making AI decisions more understandable. However, the lack of a unified approach limits AI's effectiveness in critical decision-making scenarios. This paper presents a novel three-layered framework that bridges HCAI and XAI to establish a structured explainability paradigm. The framework comprises (1) a foundational AI model with built-in explainability mechanisms, (2) a human-centered explanation layer that tailors explanations based on cognitive load and user expertise, and (3) a dynamic feedback loop that refines explanations through real-time user interaction. The framework is evaluated across healthcare, finance, and software development, demonstrating its potential to enhance decision-making, regulatory compliance, and public trust. Our findings advance Human-Centered Explainable AI (HCXAI), fostering AI systems that are transparent, adaptable, and ethically aligned.","authors":["Chameera De Silva","Thilina Halloluwa","Dhaval Vyas"],"url":"https://arxiv.org/abs/2504.13926"}
{"created":"2025-04-29","title":"HyDra: SOT-CAM Based Vector Symbolic Macro for Hyperdimensional Computing","abstract":"Hyperdimensional computing (HDC) is a brain-inspired paradigm valued for its noise robustness, parallelism, energy efficiency, and low computational overhead. Hardware accelerators are being explored to further enhance their performance, but current solutions are often limited by application specificity and the latency of encoding and similarity search. This paper presents a generalized, reconfigurable on-chip training and inference architecture for HDC, utilizing spin-orbit-torque magnetic random access memory (SOT-MRAM) based content-addressable memory (SOT-CAM). The proposed SOT-CAM array integrates storage and computation, enabling in-memory execution of key HDC operations: binding (bitwise multiplication), permutation (bit shfiting), and efficient similarity search. Furthermore, a novel bit drop method-based permutation backed by holographic information representation of HDC is proposed which replaces conventional permutation execution in hardware resulting in a 6x latency improvement, and an HDC-specific adder reduces energy and area by 1.51X and 1.43x, respectively. To mitigate the parasitic effect of interconnects in the similarity search, a four-stage voltage scaling scheme has been proposed to ensure an accurate representation of the Hamming distance. Benchmarked at 7nm, the architecture achieves energy reductions of 21.5x, 552.74x, 1.45x, and 282.57x for addition, permutation, multiplication, and search operations, respectively, compared to CMOS-based HDC. Against state-of-the-art HDC accelerators, it achieves a 2.27x lower energy consumption and outperforms CPU and eGPU implementations by 2702x and 23161x, respectively, with less than 3% drop in accuracy.","authors":["Md Mizanur Rahaman Nayan","Che-Kai Liu","Zishen Wan","Arijit Raychowdhury","Azad J Naeemi"],"url":"https://arxiv.org/abs/2504.14020"}
{"created":"2025-04-29","title":"A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning","abstract":"This paper demonstrates a probabilistic bit physics inspired solver with 440 spins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area efficiency is maximized through a current-mode implementation of the neuron update circuit, standard cell design for analog blocks pitch-matched to digital blocks, and a shared power supply for both digital and analog components. Process variation related mismatches introduced by this approach are effectively mitigated using a hardware aware contrastive divergence algorithm during training. We validate the chip's ability to perform probabilistic computing tasks such as modeling logic gates and full adders, as well as optimization tasks such as MaxCut, demonstrating its potential for AI and machine learning applications.","authors":["Jinesh Jhonsa","William Whitehead","David McCarthy","Shuvro Chowdhury","Kerem Camsari","Luke Theogarajan"],"url":"https://arxiv.org/abs/2504.14070"}
{"created":"2025-04-29","title":"ThyroidEffi 1.0: A Cost-Effective System for High-Performance Multi-Class Thyroid Carcinoma Classification","abstract":"Background: Automated classification of thyroid Fine Needle Aspiration Biopsy (FNAB) images faces challenges in limited data, inter-observer variability, and computational cost. Efficient, interpretable models are crucial for clinical support.","authors":["Hai Pham-Ngoc","De Nguyen-Van","Dung Vu-Tien","Phuong Le-Hong"],"url":"https://arxiv.org/abs/2504.14139"}
{"created":"2025-04-29","title":"Dual-channel Heterophilic Message Passing for Graph Fraud Detection","abstract":"Fraudulent activities have significantly increased across various domains, such as e-commerce, online review platforms, and social networks, making fraud detection a critical task. Spatial Graph Neural Networks (GNNs) have been successfully applied to fraud detection tasks due to their strong inductive learning capabilities. However, existing spatial GNN-based methods often enhance the graph structure by excluding heterophilic neighbors during message passing to align with the homophilic bias of GNNs. Unfortunately, this approach can disrupt the original graph topology and increase uncertainty in predictions. To address these limitations, this paper proposes a novel framework, Dual-channel Heterophilic Message Passing (DHMP), for fraud detection. DHMP leverages a heterophily separation module to divide the graph into homophilic and heterophilic subgraphs, mitigating the low-pass inductive bias of traditional GNNs. It then applies shared weights to capture signals at different frequencies independently and incorporates a customized sampling strategy for training. This allows nodes to adaptively balance the contributions of various signals based on their labels. Extensive experiments on three real-world datasets demonstrate that DHMP outperforms existing methods, highlighting the importance of separating signals with different frequencies for improved fraud detection. The code is available at https://github.com/shaieesss/DHMP.","authors":["Wenxin Zhang","Jingxing Zhong","Guangzhen Yao","Renda Han","Xiaojian Lin","Zeyu Zhang","Cuicui Luo"],"url":"https://arxiv.org/abs/2504.14205"}
{"created":"2025-04-29","title":"The Dark Side of the Web: Towards Understanding Various Data Sources in Cyber Threat Intelligence","abstract":"Cyber threats have become increasingly prevalent and sophisticated. Prior work has extracted actionable cyber threat intelligence (CTI), such as indicators of compromise, tactics, techniques, and procedures (TTPs), or threat feeds from various sources: open source data (e.g., social networks), internal intelligence (e.g., log data), and ``first-hand'' communications from cybercriminals (e.g., underground forums, chats, darknet websites). However, \"first-hand\" data sources remain underutilized because it is difficult to access or scrape their data.","authors":["Saskia Laura Schr\\\"oer","No\\'e Canevascini","Irdin Pekaric","Philine Widmer","Pavel Laskov"],"url":"https://arxiv.org/abs/2504.14235"}
{"created":"2025-04-29","title":"A New Impossibility Result for Online Bipartite Matching Problems","abstract":"Online Bipartite Matching with random user arrival is a fundamental problem in the online advertisement ecosystem. Over the last 30 years, many algorithms and impossibility results have been developed for this problem. In particular, the latest impossibility result was established by Manshadi, Oveis Gharan and Saberi in 2011. Since then, several algorithms have been published in an effort to narrow the gap between the upper and the lower bounds on the competitive ratio.","authors":["Flavio Chierichetti","Mirko Giacchini","Alessandro Panconesi","Andrea Vattani"],"url":"https://arxiv.org/abs/2504.14251"}
{"created":"2025-04-29","title":"On the Redundancy of Function-Correcting Codes over Finite Fields","abstract":"Function-correcting codes (FCCs) protect specific function evaluations of a message against errors. This condition imposes a less stringent distance requirement than classical error-correcting codes (ECCs), allowing for reduced redundancy. FCCs were introduced by Lenz et al. (2021), who also established a lower bound on the optimal redundancy for FCCs over the binary field. Here, we derive an upper bound within a logarithmic factor of this lower bound. We show that the same lower bound holds for any finite field. Moreover, we show that this bound is tight for sufficiently large fields by demonstrating that it also serves as an upper bound. Furthermore, we construct an encoding scheme that achieves this optimal redundancy. Finally, motivated by these two extreme regimes, we conjecture that our bound serves as a valid upper bound across all finite fields.","authors":["Hoang Ly","Emina Soljanin"],"url":"https://arxiv.org/abs/2504.14410"}
{"created":"2025-04-29","title":"Data Selection for ERMs","abstract":"Learning theory has traditionally followed a model-centric approach, focusing on designing optimal algorithms for a fixed natural learning task (e.g., linear classification or regression). In this paper, we adopt a complementary data-centric perspective, whereby we fix a natural learning rule and focus on optimizing the training data. Specifically, we study the following question: given a learning rule $\\mathcal{A}$ and a data selection budget $n$, how well can $\\mathcal{A}$ perform when trained on at most $n$ data points selected from a population of $N$ points? We investigate when it is possible to select $n \\ll N$ points and achieve performance comparable to training on the entire population.","authors":["Steve Hanneke","Shay Moran","Alexander Shlimovich","Amir Yehudayoff"],"url":"https://arxiv.org/abs/2504.14572"}
{"created":"2025-04-29","title":"Wireless Large AI Model: Shaping the AI-Native Future of 6G and Beyond","abstract":"The emergence of sixth-generation and beyond communication systems is expected to fundamentally transform digital experiences through introducing unparalleled levels of intelligence, efficiency, and connectivity. A promising technology poised to enable this revolutionary vision is the wireless large AI model (WLAM), characterized by its exceptional capabilities in data processing, inference, and decision-making. In light of these remarkable capabilities, this paper provides a comprehensive survey of WLAM, elucidating its fundamental principles, diverse applications, critical challenges, and future research opportunities. We begin by introducing the background of WLAM and analyzing the key synergies with wireless networks, emphasizing the mutual benefits. Subsequently, we explore the foundational characteristics of WLAM, delving into their unique relevance in wireless environments. Then, the role of WLAM in optimizing wireless communication systems across various use cases and the reciprocal benefits are systematically investigated. Furthermore, we discuss the integration of WLAM with emerging technologies, highlighting their potential to enable transformative capabilities and breakthroughs in wireless communication. Finally, we thoroughly examine the high-level challenges hindering the practical implementation of WLAM and discuss pivotal future research directions.","authors":["Fenghao Zhu","Xinquan Wang","Xinyi Li","Maojun Zhang","Yixuan Chen","Chongwen Huang","Zhaohui Yang","Xiaoming Chen","Zhaoyang Zhang","Richeng Jin","Yongming Huang","Wei Feng","Tingting Yang","Baoming Bai","Feifei Gao","Kun Yang","Yuanwei Liu","Sami Muhaidat","Chau Yuen","Kaibin Huang","Kai-Kit Wong","Dusit Niyato","M\\'erouane Debbah"],"url":"https://arxiv.org/abs/2504.14653"}
{"created":"2025-04-29","title":"Reinforcement Learning from Multi-level and Episodic Human Feedback","abstract":"Designing an effective reward function has long been a challenge in reinforcement learning, particularly for complex tasks in unstructured environments. To address this, various learning paradigms have emerged that leverage different forms of human input to specify or refine the reward function. Reinforcement learning from human feedback is a prominent approach that utilizes human comparative feedback, expressed as a preference for one behavior over another, to tackle this problem. In contrast to comparative feedback, we explore multi-level human feedback, which is provided in the form of a score at the end of each episode. This type of feedback offers more coarse but informative signals about the underlying reward function than binary feedback. Additionally, it can handle non-Markovian rewards, as it is based on the evaluation of an entire episode. We propose an algorithm to efficiently learn both the reward function and the optimal policy from this form of feedback. Moreover, we show that the proposed algorithm achieves sublinear regret and demonstrate its empirical effectiveness through extensive simulations.","authors":["Muhammad Qasim Elahi","Somtochukwu Oguchienti","Maheed H. Ahmed","Mahsa Ghasemi"],"url":"https://arxiv.org/abs/2504.14732"}
{"created":"2025-04-29","title":"ReCraft: Self-Contained Split, Merge, and Membership Change of Raft Protocol","abstract":"Designing reconfiguration schemes for consensus protocols is challenging because subtle corner cases during reconfiguration could invalidate the correctness of the protocol. Thus, most systems that embed consensus protocols conservatively implement the reconfiguration and refrain from developing an efficient scheme. Existing implementations often stop the entire system during reconfiguration and rely on a centralized coordinator, which can become a single point of failure. We present ReCraft, a novel reconfiguration protocol for Raft, which supports multi- and single-cluster-level reconfigurations. ReCraft does not rely on external coordinators and blocks minimally. ReCraft enables the sharding of Raft clusters with split and merge reconfigurations and adds a membership change scheme that improves Raft. We prove the safety and liveness of ReCraft and demonstrate its efficiency through implementations in etcd.","authors":["Kezhi Xiong","Soonwon Moon","Joshua Kang","Bryant Curto","Jieung Kim","Ji-Yong Shin"],"url":"https://arxiv.org/abs/2504.14802"}
{"created":"2025-04-29","title":"vApps: Verifiable Applications at Internet Scale","abstract":"Blockchain technology promises a decentralized, trustless, and interoperable infrastructure. However, widespread adoption remains hindered by issues such as limited scalability, high transaction costs, and the complexity of maintaining coherent verification logic across different blockchain layers. This paper introduces Verifiable Applications (vApps), a novel development framework designed to streamline the creation and deployment of verifiable blockchain computing applications. vApps offer a unified Rust-based Domain-Specific Language (DSL) within a comprehensive SDK, featuring modular abstractions for verification, proof generation, and inter-chain connectivity. This eases the developer's burden in securing diverse software components, allowing them to focus on application logic. The DSL also ensures that applications can automatically take advantage of specialized precompiles and hardware acceleration to achieve consistently high performance with minimal developer effort, as demonstrated by benchmark results for zero-knowledge virtual machines (zkVMs). Experiments show that native Rust execution eliminates interpretation overhead, delivering up to an 832x cycle count improvement compared to EVM-based approaches. Precompiled circuits can accelerate the proof by more than 95%, while GPU acceleration increases throughput by up to 30x and recursion compresses the proof size by up to 230x, enabling succinct and efficient verification. The framework also supports seamless integration with the Web2 and Web3 systems, enabling developers to focus solely on their application logic. Through modular architecture, robust security guarantees, and composability, vApps pave the way toward a trust-minimized and verifiable Internet-scale application environment.","authors":["Isaac Zhang","Kshitij Kulkarni","Tan Li","Daniel Wong","Thomas Kim","John Guibas","Uma Roy","Bryan Pellegrino","Ryan Zarick"],"url":"https://arxiv.org/abs/2504.14809"}
{"created":"2025-04-29","title":"A Basic Evaluation of Neural Networks Trained with the Error Diffusion Learning Algorithm","abstract":"Artificial neural networks are powerful tools capable of addressing various tasks. Although the backpropagation algorithm has become a standard training method for these neural networks, its lack of biological plausibility has inspired the development of alternative learning approaches. One such alternative is Kaneko's Error Diffusion Learning Algorithm (EDLA), a biologically motivated approach wherein a single global error signal diffuses throughout a network composed of paired excitatory-inhibitory sublayers, thereby eliminating the necessity for layer-wise backpropagation. This study presents a contemporary formulation of the EDLA framework and evaluates its effectiveness through parity check, regression, and image classification tasks. Our experimental results indicate that EDLA networks can consistently achieve high accuracy across these benchmarks, with performance efficiency and convergence speed notably influenced by the choice of learning rate, neuron count, and network depth. Further investigation of the internal representations formed by EDLA networks reveals their capacity for meaningful feature extraction, similar to traditional neural networks. These results suggest that EDLA is a biologically motivated alternative for training feedforward networks and will motivate future work on extending this method to biologically inspired neural networks.","authors":["Kazuhisa Fujita"],"url":"https://arxiv.org/abs/2504.14814"}
{"created":"2025-04-29","title":"SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale SQL Generation","abstract":"High quality SQL corpus is essential for intelligent database. For example, Text-to-SQL requires SQL queries and correspond natural language questions as training samples. However, collecting such query corpus remains challenging in practice due to the high cost of manual annotation, which highlights the importance of automatic SQL generation. Despite recent advances, existing generation methods still face limitations in achieving both diversity and cost-effectiveness. Besides, many methods also treat all tables equally, which overlooks schema complexity and leads to under-utilization of structurally rich tables. To address these issues, this paper proposes a multi-agent framework for high-quality and large-scale SQL generation, dubbed SQL-Factory. It decomposes the generation process into three collaborative teams: the Generation Team explores diverse query structures using a powerful language model, the Expansion Team scales promising patterns via a lightweight language model, and the Management Team adaptively schedules the workflow and evaluates the quality of synthesized queries. This modular framework ensures a balanced trade-off between diversity, scalability, and generation cost. We apply SQL-Factory to four widely used benchmarks and generate over 300,000 SQL queries with less than $200 API cost. Our generated queries achieve higher diversity compared to other methods, and extensive experiments demonstrate that the generated queries significantly improve the model performance in various downstream tasks.","authors":["Jiahui Li","Tongwang Wu","Yuren Mao","Yunjun Gao","Yajie Feng","Huaizhong Liu"],"url":"https://arxiv.org/abs/2504.14837"}
{"created":"2025-04-29","title":"Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL","abstract":"Large Language Models (LLMs) have shown impressive capabilities in transforming natural language questions about relational databases into SQL queries. Despite recent improvements, small LLMs struggle to handle questions involving multiple tables and complex SQL patterns under a Zero-Shot Learning (ZSL) setting. Supervised Fine-Tuning (SFT) partially compensates for the knowledge deficits in pretrained models but falls short while dealing with queries involving multi-hop reasoning. To bridge this gap, different LLM training strategies to reinforce reasoning capabilities have been proposed, ranging from leveraging a thinking process within ZSL, including reasoning traces in SFT, or adopt Reinforcement Learning (RL) strategies. However, the influence of reasoning on Text2SQL performance is still largely unexplored. This paper investigates to what extent LLM reasoning capabilities influence their Text2SQL performance on four benchmark datasets. To this end, it considers the following LLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT, with and without task-specific reasoning traces; (3) RL, exploring the use of different rewarding functions, both the established EXecution accuracy (EX) and a mix with fine-grained ones that also account the precision, recall, and cardinality of partially correct answers; (4) SFT+RL, i.e, a two-stage approach that combines SFT and RL. The results show that general-purpose reasoning under ZSL proves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit from SFT with reasoning much more than larger ones. RL is generally beneficial across all tested models and datasets. The use of the fine-grained metrics turns out to be the most effective RL strategy. Thanks to RL and the novel text2SQL rewards, the 7B Qwen-Coder-2.5 model performs on par with 400+ Billion ones (including gpt-4o) on the Bird dataset.","authors":["Simone Papicchio","Simone Rossi","Luca Cagliero","Paolo Papotti"],"url":"https://arxiv.org/abs/2504.15077"}
{"created":"2025-04-29","title":"VistaDepth: Frequency Modulation With Bias Reweighting For Enhanced Long-Range Depth Estimation","abstract":"Monocular depth estimation (MDE) aims to predict per-pixel depth values from a single RGB image. Recent advancements have positioned diffusion models as effective MDE tools by framing the challenge as a conditional image generation task. Despite their progress, these methods often struggle with accurately reconstructing distant depths, due largely to the imbalanced distribution of depth values and an over-reliance on spatial-domain features. To overcome these limitations, we introduce VistaDepth, a novel framework that integrates adaptive frequency-domain feature enhancements with an adaptive weight-balancing mechanism into the diffusion process. Central to our approach is the Latent Frequency Modulation (LFM) module, which dynamically refines spectral responses in the latent feature space, thereby improving the preservation of structural details and reducing noisy artifacts. Furthermore, we implement an adaptive weighting strategy that modulates the diffusion loss in real-time, enhancing the model's sensitivity towards distant depth reconstruction. These innovations collectively result in superior depth perception performance across both distance and detail. Experimental evaluations confirm that VistaDepth achieves state-of-the-art performance among diffusion-based MDE techniques, particularly excelling in the accurate reconstruction of distant regions.","authors":["Mingxia Zhan","Li Zhang","Xiaomeng Chu","Beibei Wang"],"url":"https://arxiv.org/abs/2504.15095"}
{"created":"2025-04-29","title":"Instance-Adaptive Keypoint Learning with Local-to-Global Geometric Aggregation for Category-Level Object Pose Estimation","abstract":"Category-level object pose estimation aims to predict the 6D pose and size of previously unseen instances from predefined categories, requiring strong generalization across diverse object instances. Although many previous methods attempt to mitigate intra-class variations, they often struggle with instances exhibiting complex geometries or significant deviations from canonical shapes. To address this challenge, we propose INKL-Pose, a novel category-level object pose estimation framework that enables INstance-adaptive Keypoint Learning with local-to-global geometric aggregation. Specifically, our approach first predicts semantically consistent and geometric informative keypoints through an Instance-Adaptive Keypoint Generator, then refines them with: (1) a Local Keypoint Feature Aggregator capturing fine-grained geometries, and (2) a Global Keypoint Feature Aggregator using bidirectional Mamba for structural consistency. To enable bidirectional modeling in Mamba, we introduce a Feature Sequence Flipping strategy that preserves spatial coherence while constructing backward feature sequences. Additionally, we design a surface loss and a separation loss to enforce uniform coverage and spatial diversity in keypoint distribution. The generated keypoints are finally mapped to a canonical space for regressing the object's 6D pose and size. Extensive experiments on CAMERA25, REAL275, and HouseCat6D demonstrate that INKL-Pose achieves state-of-the-art performance and significantly outperforms existing methods.","authors":["Xiao Zhang","Lu Zou","Tao Lu","Yuan Yao","Zhangjin Huang","Guoping Wang"],"url":"https://arxiv.org/abs/2504.15134"}
{"created":"2025-04-29","title":"Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs","abstract":"Multi-view understanding, the ability to reconcile visual information across diverse viewpoints for effective navigation, manipulation, and 3D scene comprehension, is a fundamental challenge in Multi-Modal Large Language Models (MLLMs) to be used as embodied agents. While recent MLLMs have shown impressive advances in high-level reasoning and planning, they frequently fall short when confronted with multi-view geometric consistency and cross-view correspondence. To comprehensively evaluate the challenges of MLLMs in multi-view scene reasoning, we propose All-Angles Bench, a benchmark of over 2,100 human carefully annotated multi-view question-answer pairs across 90 diverse real-world scenes. Our six tasks (counting, attribute identification, relative distance, relative direction, object manipulation, and camera pose estimation) specifically test model's geometric correspondence and the capacity to align information consistently across views. Our extensive experiments, benchmark on 27 representative MLLMs including Gemini-2.0-Flash, Claude-3.7-Sonnet, and GPT-4o against human evaluators reveals a substantial performance gap, indicating that current MLLMs remain far from human-level proficiency. Through in-depth analysis, we show that MLLMs are particularly underperforming under two aspects: (1) cross-view correspondence for partially occluded views and (2) establishing the coarse camera poses. These findings highlight the necessity of domain-specific refinements or modules that embed stronger multi-view awareness. We believe that our All-Angles Bench offers valuable insights and contribute to bridging the gap between MLLMs and human-level multi-view understanding. The project and benchmark are publicly available at https://danielchyeh.github.io/All-Angles-Bench/.","authors":["Chun-Hsiao Yeh","Chenyu Wang","Shengbang Tong","Ta-Ying Cheng","Ruoyu Wang","Tianzhe Chu","Yuexiang Zhai","Yubei Chen","Shenghua Gao","Yi Ma"],"url":"https://arxiv.org/abs/2504.15280"}
{"created":"2025-04-29","title":"Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models","abstract":"In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying more complex language model circuits by building up from a minimal circuit.","authors":["Tyler A. Chang","Benjamin K. Bergen"],"url":"https://arxiv.org/abs/2504.15471"}
{"created":"2025-04-29","title":"T2VShield: Model-Agnostic Jailbreak Defense for Text-to-Video Models","abstract":"The rapid development of generative artificial intelligence has made text to video models essential for building future multimodal world simulators. However, these models remain vulnerable to jailbreak attacks, where specially crafted prompts bypass safety mechanisms and lead to the generation of harmful or unsafe content. Such vulnerabilities undermine the reliability and security of simulation based applications. In this paper, we propose T2VShield, a comprehensive and model agnostic defense framework designed to protect text to video models from jailbreak threats. Our method systematically analyzes the input, model, and output stages to identify the limitations of existing defenses, including semantic ambiguities in prompts, difficulties in detecting malicious content in dynamic video outputs, and inflexible model centric mitigation strategies. T2VShield introduces a prompt rewriting mechanism based on reasoning and multimodal retrieval to sanitize malicious inputs, along with a multi scope detection module that captures local and global inconsistencies across time and modalities. The framework does not require access to internal model parameters and works with both open and closed source systems. Extensive experiments on five platforms show that T2VShield can reduce jailbreak success rates by up to 35 percent compared to strong baselines. We further develop a human centered audiovisual evaluation protocol to assess perceptual safety, emphasizing the importance of visual level defense in enhancing the trustworthiness of next generation multimodal simulators.","authors":["Siyuan Liang","Jiayang Liu","Jiecheng Zhai","Tianmeng Fang","Rongcheng Tu","Aishan Liu","Xiaochun Cao","Dacheng Tao"],"url":"https://arxiv.org/abs/2504.15512"}
{"created":"2025-04-29","title":"Potential for Polynomial Solution for NP-Complete Problems using Quantum Computation","abstract":"In this paper, we propose two new methods for solving Set Constraint Problems, as well as a potential polynomial solution for NP-Complete problems using quantum computation. While current methods of solving Set Constraint Problems focus on classical techniques, we offer both a quantum-inspired matrix method and a quantum matrix method that neutralizes common contradictions and inconsistencies that appear in these types of problems. We then use our new method to show how a potential polynomial solution for NP-Complete problems could be found using quantum computation. We state this as a potential solution, rather than an actual solution, as the outcome of any quantum computation may not be the same as the expected outcome. We start by formally defining a Set Constraint Problem. We then explain current, classical methods that are used to solve these problems and the drawbacks of such methods. After this, we explain a new quantum-inspired matrix method that allows us to solve these problems, with classical limitations. Then, we explain a new quantum matrix method that solves these problems using quantum information science. Finally, we describe how we can extend this method to potentially solve NP-Complete problems in polynomial time using quantum computation.","authors":["Neema Rustin Badihian"],"url":"https://arxiv.org/abs/2504.15529"}
{"created":"2025-04-29","title":"Exploring the Role of Large Language Models in Cybersecurity: A Systematic Survey","abstract":"With the rapid development of technology and the acceleration of digitalisation, the frequency and complexity of cyber security threats are increasing. Traditional cybersecurity approaches, often based on static rules and predefined scenarios, are struggling to adapt to the rapidly evolving nature of modern cyberattacks. There is an urgent need for more adaptive and intelligent defence strategies. The emergence of Large Language Model (LLM) provides an innovative solution to cope with the increasingly severe cyber threats, and its potential in analysing complex attack patterns, predicting threats and assisting real-time response has attracted a lot of attention in the field of cybersecurity, and exploring how to effectively use LLM to defend against cyberattacks has become a hot topic in the current research field. This survey examines the applications of LLM from the perspective of the cyber attack lifecycle, focusing on the three phases of defense reconnaissance, foothold establishment, and lateral movement, and it analyzes the potential of LLMs in Cyber Threat Intelligence (CTI) tasks. Meanwhile, we investigate how LLM-based security solutions are deployed and applied in different network scenarios. It also summarizes the internal and external risk issues faced by LLM during its application. Finally, this survey also points out the facing risk issues and possible future research directions in this domain.","authors":["Shuang Tian","Tao Zhang","Jiqiang Liu","Jiacheng Wang","Xuangou Wu","Xiaoqiang Zhu","Ruichen Zhang","Weiting Zhang","Zhenhui Yuan","Shiwen Mao","Dong In Kim"],"url":"https://arxiv.org/abs/2504.15622"}
{"created":"2025-04-29","title":"FaceInsight: A Multimodal Large Language Model for Face Perception","abstract":"Recent advances in multimodal large language models (MLLMs) have demonstrated strong capabilities in understanding general visual content. However, these general-domain MLLMs perform poorly in face perception tasks, often producing inaccurate or misleading responses to face-specific queries. To address this gap, we propose FaceInsight, the versatile face perception MLLM that provides fine-grained facial information. Our approach introduces visual-textual alignment of facial knowledge to model both uncertain dependencies and deterministic relationships among facial information, mitigating the limitations of language-driven reasoning. Additionally, we incorporate face segmentation maps as an auxiliary perceptual modality, enriching the visual input with localized structural cues to enhance semantic understanding. Comprehensive experiments and analyses across three face perception tasks demonstrate that FaceInsight consistently outperforms nine compared MLLMs under both training-free and fine-tuned settings.","authors":["Jingzhi Li","Changjiang Luo","Ruoyu Chen","Hua Zhang","Wenqi Ren","Jianhou Gan","Xiaochun Cao"],"url":"https://arxiv.org/abs/2504.15624"}
{"created":"2025-04-29","title":"A Study on Mixup-Inspired Augmentation Methods for Software Vulnerability Detection","abstract":"Various deep learning (DL) methods have recently been utilized to detect software vulnerabilities. Real-world software vulnerability datasets are rare and hard to acquire, as there is no simple metric for classifying vulnerability. Such datasets are heavily imbalanced, and none of the current datasets are considered huge for DL models. To tackle these problems, a recent work has tried to augment the dataset using the source code and generate realistic single-statement vulnerabilities, which is not quite practical and requires manual checking of the generated vulnerabilities. In this paper, we aim to explore the augmentation of vulnerabilities at the representation level to help current models learn better, which has never been done before to the best of our knowledge. We implement and evaluate five augmentation techniques that augment the embedding of the data and have recently been used for code search, which is a completely different software engineering task. We also introduced a conditioned version of those augmentation methods, which ensures the augmentation does not change the vulnerable section of the vector representation. We show that such augmentation methods can be helpful and increase the F1-score by up to 9.67%, yet they cannot beat Random Oversampling when balancing datasets, which increases the F1-score by 10.82%.","authors":["Seyed Shayan Daneshvar","Da Tan","Shaowei Wang","Carson Leung"],"url":"https://arxiv.org/abs/2504.15632"}
{"created":"2025-04-29","title":"Promoting Real-Time Reflection in Synchronous Communication with Generative AI","abstract":"Real-time reflection plays a vital role in synchronous communication. It enables users to adjust their communication strategies dynamically, thereby improving the effectiveness of their communication. Generative AI holds significant potential to enhance real-time reflection due to its ability to comprehensively understand the current context and generate personalized and nuanced content. However, it is challenging to design the way of interaction and information presentation to support the real-time workflow rather than disrupt it. In this position paper, we present a review of existing research on systems designed for reflection in different synchronous communication scenarios. Based on that, we discuss design implications on how to design human-AI interaction to support reflection in real time.","authors":["Yi Wen","Meng Xia"],"url":"https://arxiv.org/abs/2504.15647"}
{"created":"2025-04-29","title":"A Vision-Enabled Prosthetic Hand for Children with Upper Limb Disabilities","abstract":"This paper introduces a novel AI vision-enabled pediatric prosthetic hand designed to assist children aged 10-12 with upper limb disabilities. The prosthesis features an anthropomorphic appearance, multi-articulating functionality, and a lightweight design that mimics a natural hand, making it both accessible and affordable for low-income families. Using 3D printing technology and integrating advanced machine vision, sensing, and embedded computing, the prosthetic hand offers a low-cost, customizable solution that addresses the limitations of current myoelectric prostheses. A micro camera is interfaced with a low-power FPGA for real-time object detection and assists with precise grasping. The onboard DL-based object detection and grasp classification models achieved accuracies of 96% and 100% respectively. In the force prediction, the mean absolute error was found to be 0.018. The features of the proposed prosthetic hand can thus be summarized as: a) a wrist-mounted micro camera for artificial sensing, enabling a wide range of hand-based tasks; b) real-time object detection and distance estimation for precise grasping; and c) ultra-low-power operation that delivers high performance within constrained power and resource limits.","authors":["Md Abdul Baset Sarker","Art Nguyen","Sigmond Kukla","Kevin Fite","Masudul H. Imtiaz"],"url":"https://arxiv.org/abs/2504.15654"}
{"created":"2025-04-29","title":"Trusted Compute Units: A Framework for Chained Verifiable Computations","abstract":"Blockchain and distributed ledger technologies (DLTs) facilitate decentralized computations across trust boundaries. However, ensuring complex computations with low gas fees and confidentiality remains challenging. Recent advances in Confidential Computing -- leveraging hardware-based Trusted Execution Environments (TEEs) -- and Proof-carrying Data -- employing cryptographic Zero-Knowledge Virtual Machines (zkVMs) -- hold promise for secure, privacy-preserving off-chain and layer-2 computations. On the other side, a homogeneous reliance on a single technology, such as TEEs or zkVMs, is impractical for decentralized environments with heterogeneous computational requirements. This paper introduces the Trusted Compute Unit (TCU), a unifying framework that enables composable and interoperable verifiable computations across heterogeneous technologies. Our approach allows decentralized applications (dApps) to flexibly offload complex computations to TCUs, obtaining proof of correctness. These proofs can be anchored on-chain for automated dApp interactions, while ensuring confidentiality of input data, and integrity of output data. We demonstrate how TCUs can support a prominent blockchain use case, such as federated learning. By enabling secure off-chain interactions without incurring on-chain confirmation delays or gas fees, TCUs significantly improve system performance and scalability. Experimental insights and performance evaluations confirm the feasibility and practicality of this unified approach, advancing the state of the art in verifiable off-chain services for the blockchain ecosystem.","authors":["Fernando Castillo","Jonathan Heiss","Sebastian Werner","Stefan Tai"],"url":"https://arxiv.org/abs/2504.15717"}
{"created":"2025-04-29","title":"SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning","abstract":"Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to \"think before answering.\" Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.","authors":["Cheng Wen","Tingwei Guo","Shuaijiang Zhao","Wei Zou","Xiangang Li"],"url":"https://arxiv.org/abs/2504.15900"}
{"created":"2025-04-29","title":"FreeGraftor: Training-Free Cross-Image Feature Grafting for Subject-Driven Text-to-Image Generation","abstract":"Subject-driven image generation aims to synthesize novel scenes that faithfully preserve subject identity from reference images while adhering to textual guidance, yet existing methods struggle with a critical trade-off between fidelity and efficiency. Tuning-based approaches rely on time-consuming and resource-intensive subject-specific optimization, while zero-shot methods fail to maintain adequate subject consistency. In this work, we propose FreeGraftor, a training-free framework that addresses these limitations through cross-image feature grafting. Specifically, FreeGraftor employs semantic matching and position-constrained attention fusion to transfer visual details from reference subjects to the generated image. Additionally, our framework incorporates a novel noise initialization strategy to preserve geometry priors of reference subjects for robust feature matching. Extensive qualitative and quantitative experiments demonstrate that our method enables precise subject identity transfer while maintaining text-aligned scene synthesis. Without requiring model fine-tuning or additional training, FreeGraftor significantly outperforms existing zero-shot and training-free approaches in both subject fidelity and text alignment. Furthermore, our framework can seamlessly extend to multi-subject generation, making it practical for real-world deployment. Our code is available at https://github.com/Nihukat/FreeGraftor.","authors":["Zebin Yao","Lei Ren","Huixing Jiang","Chen Wei","Xiaojie Wang","Ruifan Li","Fangxiang Feng"],"url":"https://arxiv.org/abs/2504.15958"}
{"created":"2025-04-29","title":"The Paradox of Poetic Intent in Back-Translation: Evaluating the Quality of Large Language Models in Chinese Translation","abstract":"The rapid advancement of large language models (LLMs) has reshaped the landscape of machine translation, yet challenges persist in preserving poetic intent, cultural heritage, and handling specialized terminology in Chinese-English translation. This study constructs a diverse corpus encompassing Chinese scientific terminology, historical translation paradoxes, and literary metaphors. Utilizing a back-translation and Friedman test-based evaluation system (BT-Fried), we evaluate BLEU, CHRF, TER, and semantic similarity metrics across six major LLMs (e.g., GPT-4.5, DeepSeek V3) and three traditional translation tools. Key findings include: (1) Scientific abstracts often benefit from back-translation, while traditional tools outperform LLMs in linguistically distinct texts; (2) LLMs struggle with cultural and literary retention, exemplifying the \"paradox of poetic intent\"; (3) Some models exhibit \"verbatim back-translation\", reflecting emergent memory behavior; (4) A novel BLEU variant using Jieba segmentation and n-gram weighting is proposed. The study contributes to the empirical evaluation of Chinese NLP performance and advances understanding of cultural fidelity in AI-mediated translation.","authors":["Li Weigang","Pedro Carvalho Brom"],"url":"https://arxiv.org/abs/2504.16286"}
{"created":"2025-04-29","title":"Fast Online Adaptive Neural MPC via Meta-Learning","abstract":"Data-driven model predictive control (MPC) has demonstrated significant potential for improving robot control performance in the presence of model uncertainties. However, existing approaches often require extensive offline data collection and computationally intensive training, limiting their ability to adapt online. To address these challenges, this paper presents a fast online adaptive MPC framework that leverages neural networks integrated with Model-Agnostic Meta-Learning (MAML). Our approach focuses on few-shot adaptation of residual dynamics - capturing the discrepancy between nominal and true system behavior - using minimal online data and gradient steps. By embedding these meta-learned residual models into a computationally efficient L4CasADi-based MPC pipeline, the proposed method enables rapid model correction, enhances predictive accuracy, and improves real-time control performance. We validate the framework through simulation studies on a Van der Pol oscillator, a Cart-Pole system, and a 2D quadrotor. Results show significant gains in adaptation speed and prediction accuracy over both nominal MPC and nominal MPC augmented with a freshly initialized neural network, underscoring the effectiveness of our approach for real-time adaptive robot control.","authors":["Yu Mei","Xinyu Zhou","Shuyang Yu","Vaibhav Srivastava","Xiaobo Tan"],"url":"https://arxiv.org/abs/2504.16369"}
{"created":"2025-04-29","title":"PixelWeb: The First Web GUI Dataset with Pixel-Wise Labels","abstract":"Graphical User Interface (GUI) datasets are crucial for various downstream tasks. However, GUI datasets often generate annotation information through automatic labeling, which commonly results in inaccurate GUI element BBox annotations, including missing, duplicate, or meaningless BBoxes. These issues can degrade the performance of models trained on these datasets, limiting their effectiveness in real-world applications. Additionally, existing GUI datasets only provide BBox annotations visually, which restricts the development of visually related GUI downstream tasks. To address these issues, we introduce PixelWeb, a large-scale GUI dataset containing over 100,000 annotated web pages. PixelWeb is constructed using a novel automatic annotation approach that integrates visual feature extraction and Document Object Model (DOM) structure analysis through two core modules: channel derivation and layer analysis. Channel derivation ensures accurate localization of GUI elements in cases of occlusion and overlapping elements by extracting BGRA four-channel bitmap annotations. Layer analysis uses the DOM to determine the visibility and stacking order of elements, providing precise BBox annotations. Additionally, PixelWeb includes comprehensive metadata such as element images, contours, and mask annotations. Manual verification by three independent annotators confirms the high quality and accuracy of PixelWeb annotations. Experimental results on GUI element detection tasks show that PixelWeb achieves performance on the mAP95 metric that is 3-7 times better than existing datasets. We believe that PixelWeb has great potential for performance improvement in downstream tasks such as GUI generation and automated user interaction.","authors":["Qi Yang","Weichen Bi","Haiyang Shen","Yaoqi Guo","Yun Ma"],"url":"https://arxiv.org/abs/2504.16419"}
{"created":"2025-04-29","title":"QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM Pretraining","abstract":"Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.","authors":["Fengze Liu","Weidong Zhou","Binbin Liu","Zhimiao Yu","Yifan Zhang","Haobin Lin","Yifeng Yu","Bingni Zhang","Xiaohuan Zhou","Taifeng Wang","Yong Cao"],"url":"https://arxiv.org/abs/2504.16511"}
{"created":"2025-04-29","title":"Enhancing LLM-Based Agents via Global Planning and Hierarchical Execution","abstract":"Intelligent agent systems based on Large Language Models (LLMs) have shown great potential in real-world applications. However, existing agent frameworks still face critical limitations in task planning and execution, restricting their effectiveness and generalizability. Specifically, current planning methods often lack clear global goals, leading agents to get stuck in local branches, or produce non-executable plans. Meanwhile, existing execution mechanisms struggle to balance complexity and stability, and their limited action space restricts their ability to handle diverse real-world tasks. To address these limitations, we propose GoalAct, a novel agent framework that introduces a continuously updated global planning mechanism and integrates a hierarchical execution strategy. GoalAct decomposes task execution into high-level skills, including searching, coding, writing and more, thereby reducing planning complexity while enhancing the agents' adaptability across diverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark with multiple types of legal tasks that require the use of multiple types of tools. Experimental results demonstrate that GoalAct achieves state-of-the-art (SOTA) performance, with an average improvement of 12.22% in success rate. These findings highlight GoalAct's potential to drive the development of more advanced intelligent agent systems, making them more effective across complex real-world applications. Our code can be found at https://github.com/cjj826/GoalAct.","authors":["Junjie Chen","Haitao Li","Jingli Yang","Yiqun Liu","Qingyao Ai"],"url":"https://arxiv.org/abs/2504.16563"}
{"created":"2025-04-29","title":"EHGCN: Hierarchical Euclidean-Hyperbolic Fusion via Motion-Aware GCN for Hybrid Event Stream Perception","abstract":"Event cameras, with microsecond temporal resolution and high dynamic range (HDR) characteristics, emit high-speed event stream for perception tasks. Despite the recent advancement in GNN-based perception methods, they are prone to use straightforward pairwise connectivity mechanisms in the pure Euclidean space where they struggle to capture long-range dependencies and fail to effectively characterize the inherent hierarchical structures of non-uniformly distributed event stream. To this end, in this paper we propose a novel approach named EHGCN, which is a pioneer to perceive event stream in both Euclidean and hyperbolic spaces for event vision. In EHGCN, we introduce an adaptive sampling strategy to dynamically regulate sampling rates, retaining discriminative events while attenuating chaotic noise. Then we present a Markov Vector Field (MVF)-driven motion-aware hyperedge generation method based on motion state transition probabilities, thereby eliminating cross-target spurious associations and providing critically topological priors while capturing long-range dependencies between events. Finally, we propose a Euclidean-Hyperbolic GCN to fuse the information locally aggregated and globally hierarchically modeled in Euclidean and hyperbolic spaces, respectively, to achieve hybrid event perception. Experimental results on event perception tasks such as object detection and recognition validate the effectiveness of our approach.","authors":["Haosheng Chen","Lian Luo","Mengjingcheng Mo","Zhanjie Wu","Guobao Xiao","Ji Gan","Jiaxu Leng","Xinbo Gao"],"url":"https://arxiv.org/abs/2504.16616"}
{"created":"2025-04-29","title":"A Survey of AI Agent Protocols","abstract":"The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide the first comprehensive analysis of existing agent protocols, proposing a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols. These characteristics include adaptability, privacy preservation, and group-based interaction, as well as trends toward layered architectures and collective intelligence infrastructures. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.","authors":["Yingxuan Yang","Huacan Chai","Yuanyi Song","Siyuan Qi","Muning Wen","Ning Li","Junwei Liao","Haoyi Hu","Jianghao Lin","Gaowei Chang","Weiwen Liu","Ying Wen","Yong Yu","Weinan Zhang"],"url":"https://arxiv.org/abs/2504.16736"}
{"created":"2025-04-29","title":"Evaluation Framework for AI Systems in \"the Wild\"","abstract":"Generative AI (GenAI) models have become vital across industries, yet current evaluation methods have not adapted to their widespread use. Traditional evaluations often rely on benchmarks and fixed datasets, frequently failing to reflect real-world performance, which creates a gap between lab-tested outcomes and practical applications. This white paper proposes a comprehensive framework for how we should evaluate real-world GenAI systems, emphasizing diverse, evolving inputs and holistic, dynamic, and ongoing assessment approaches. The paper offers guidance for practitioners on how to design evaluation methods that accurately reflect real-time capabilities, and provides policymakers with recommendations for crafting GenAI policies focused on societal impacts, rather than fixed performance numbers or parameter sizes. We advocate for holistic frameworks that integrate performance, fairness, and ethics and the use of continuous, outcome-oriented methods that combine human and automated assessments while also being transparent to foster trust among stakeholders. Implementing these strategies ensures GenAI models are not only technically proficient but also ethically responsible and impactful.","authors":["Sarah Jabbour","Trenton Chang","Anindya Das Antar","Joseph Peper","Insu Jang","Jiachen Liu","Jae-Won Chung","Shiqi He","Michael Wellman","Bryan Goodman","Elizabeth Bondi-Kelly","Kevin Samy","Rada Mihalcea","Mosharaf Chowdhury","David Jurgens","Lu Wang"],"url":"https://arxiv.org/abs/2504.16778"}
{"created":"2025-04-29","title":"From Diverse Origins to a DEI Crisis: The Pushback Against Equity, Diversity, and Inclusion in Software Engineering","abstract":"Background: Diversity, equity, and inclusion are rooted in the very origins of software engineering, shaped by the contributions from many individuals from underrepresented groups to the field. Yet today, DEI efforts in the industry face growing resistance. As companies retreat from visible commitments, and pushback initiatives started only a few years ago. Aims: This study explores how the DEI backlash is unfolding in the software industry by investigating institutional changes, lived experiences, and the strategies used to sustain DEI practices. Method: We conducted an exploratory case study using 59 publicly available Reddit posts authored by self-identified software professionals. Data were analyzed using reflexive thematic analysis. Results: Our findings show that software companies are responding to the DEI backlash in varied ways, including re-structuring programs, scaling back investments, or quietly continuing efforts under new labels. Professionals reported a wide range of emotional responses, from anxiety and frustration to relief and happiness, shaped by identity, role, and organizational culture. Yet, despite the backlash, multiple forms of resistance and adaptation have emerged to protect inclusive practices in software engineering. Conclusions: The DEI backlash is reshaping DEI in software engineering. While public messaging may soften or disappear, core DEI values persist in adapted forms. This study offers a new perspective into how inclusion is evolving under pressure and highlights the resilience of DEI in software environments.","authors":["Ronnie de Souza Santos","Ann Barcomb","Mairieli Wessel","Cleyton Magalhaes"],"url":"https://arxiv.org/abs/2504.16821"}
{"created":"2025-04-29","title":"Do Large Language Models know who did what to whom?","abstract":"Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.","authors":["Joseph M. Denning","Xiaohan Hannah Guo","Bryor Snefjella","Idan A. Blank"],"url":"https://arxiv.org/abs/2504.16884"}
{"created":"2025-04-29","title":"Analyzing Value Functions of States in Parametric Markov Chains","abstract":"Parametric Markov chains (pMC) are used to model probabilistic systems with unknown or partially known probabilities. Although (universal) pMC verification for reachability properties is known to be coETR-complete, there have been efforts to approach it using potentially easier-to-check properties such as asking whether the pMC is monotonic in certain parameters. In this paper, we first reduce monotonicity to asking whether the reachability probability from a given state is never less than that of another given state. Recent results for the latter property imply an efficient algorithm to collapse same-value equivalence classes, which in turn preserves verification results and monotonicity. We implement our algorithm to collapse \"trivial\" equivalence classes in the pMC and show empirical evidence for the following: First, the collapse gives reductions in size for some existing benchmarks and significant reductions on some custom benchmarks; Second, the collapse speeds up existing algorithms to check monotonicity and parameter lifting, and hence can be used as a fast pre-processing step in practice.","authors":["Kasper Engelen","Guillermo A. P\\'erez","Shrisha Rao"],"url":"https://arxiv.org/abs/2504.17020"}
{"created":"2025-04-29","title":"Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation","abstract":"The generation of high-quality synthetic data presents significant challenges in machine learning research, particularly regarding statistical fidelity and uncertainty quantification. Existing generative models produce compelling synthetic samples but lack rigorous statistical guarantees about their relation to the underlying data distribution, limiting their applicability in critical domains requiring robust error bounds. We address this fundamental limitation by presenting a novel framework that incorporates conformal prediction methodologies into Generative Adversarial Networks (GANs). By integrating multiple conformal prediction paradigms including Inductive Conformal Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction, and Venn-Abers Predictors, we establish distribution-free uncertainty quantification in generated samples. This approach, termed Conformalized GAN (cGAN), demonstrates enhanced calibration properties while maintaining the generative power of traditional GANs, producing synthetic data with provable statistical guarantees. We provide rigorous mathematical proofs establishing finite-sample validity guarantees and asymptotic efficiency properties, enabling the reliable application of synthetic data in high-stakes domains including healthcare, finance, and autonomous systems.","authors":["Rahul Vishwakarma","Shrey Dharmendra Modi","Vishwanath Seshagiri"],"url":"https://arxiv.org/abs/2504.17058"}
{"created":"2025-04-29","title":"Steering the CensorShip: Uncovering Representation Vectors for LLM \"Thought\" Control","abstract":"Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this \"censorship\" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through \"thought suppression\". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector. Our code is publicly available at: https://github.com/hannahxchen/llm-censorship-steering","authors":["Hannah Cyberey","David Evans"],"url":"https://arxiv.org/abs/2504.17130"}
{"created":"2025-04-29","title":"Latent Video Dataset Distillation","abstract":"Dataset distillation has demonstrated remarkable effectiveness in high-compression scenarios for image datasets. While video datasets inherently contain greater redundancy, existing video dataset distillation methods primarily focus on compression in the pixel space, overlooking advances in the latent space that have been widely adopted in modern text-to-image and text-to-video models. In this work, we bridge this gap by introducing a novel video dataset distillation approach that operates in the latent space using a state-of-the-art variational encoder. Furthermore, we employ a diversity-aware data selection strategy to select both representative and diverse samples. Additionally, we introduce a simple, training-free method to further compress the distilled latent dataset. By combining these techniques, our approach achieves a new state-of-the-art performance in dataset distillation, outperforming prior methods on all datasets, e.g. on HMDB51 IPC 1, we achieve a 2.6% performance increase; on MiniUCF IPC 5, we achieve a 7.8% performance increase. Our code is available at https://github.com/liningresearch/Latent_Video_Dataset_Distillation.","authors":["Ning Li","Antai Andy Liu","Jingran Zhang","Justin Cui"],"url":"https://arxiv.org/abs/2504.17132"}
{"created":"2025-04-29","title":"Augmenting Captions with Emotional Cues: An AR Interface for Real-Time Accessible Communication","abstract":"This paper introduces an augmented reality (AR) captioning framework designed to support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by integrating non-verbal emotional cues into live transcriptions. Unlike conventional captioning systems that offer only plain text, our system fuses real-time speech recognition with affective and visual signal interpretation, including facial movements, gestures, and vocal tone, to produce emotionally enriched captions. These enhanced captions are rendered in an AR interface developed with Unity and provide contextual annotations such as speaker tone markers (e.g., \"concerned\") and gesture indicators (e.g., \"nods\"). The system leverages live camera and microphone input, processed through AI models to detect multimodal cues. Findings from preliminary evaluations suggest that this AR-based captioning approach significantly enhances comprehension and reduces cognitive effort compared to standard captions. Our work emphasizes the potential of immersive environments for inclusive, emotion-aware educational accessibility.","authors":["Sunday David Ubur"],"url":"https://arxiv.org/abs/2504.17171"}
{"created":"2025-04-29","title":"Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning","abstract":"Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins. Code is available at: https://github.com/going-doer/Paper2Code.","authors":["Minju Seo","Jinheon Baek","Seongyun Lee","Sung Ju Hwang"],"url":"https://arxiv.org/abs/2504.17192"}
{"created":"2025-04-29","title":"MASR: Self-Reflective Reasoning through Multimodal Hierarchical Attention Focusing for Agent-based Video Understanding","abstract":"Even in the era of rapid advances in large models, video understanding remains a highly challenging task. Compared to texts or images, videos commonly contain more information with redundancy, requiring large models to properly allocate attention at a global level for comprehensive and accurate understanding. To address this, we propose a Multimodal hierarchical Attention focusing Self-reflective Reasoning (MASR) framework for agent-based video understanding. The key innovation lies in its ability to detect and prioritize segments of videos that are highly relevant to the query. Firstly, MASR realizes Multimodal Coarse-to-fine Relevance Sensing (MCRS) which enhances the correlation between the acquired contextual information and the query. Secondly, MASR employs Dilated Temporal Expansion (DTE) to mitigate the risk of missing crucial details when extracting semantic information from the focused frames selected through MCRS. By iteratively applying MCRS and DTE in the self-reflective reasoning process, MASR is able to adaptively adjust the attention to extract highly query-relevant context and therefore improve the response accuracy. In the EgoSchema dataset, MASR achieves a remarkable 5% performance gain over previous leading approaches. In the Next-QA and IntentQA datasets, it outperforms the state-of-the-art standards by 0.2% and 0.3% respectively. In the Video-MME dataset that contains long-term videos, MASR also performs better than other agent-based methods.","authors":["Shiwen Cao","Zhaoxing Zhang","Junming Jiao","Juyi Qiao","Guowen Song","Rong Shen","Xiangbing Meng"],"url":"https://arxiv.org/abs/2504.17213"}
{"created":"2025-04-29","title":"A Lightweight Method for Generating Multi-Tier JIT Compilation Virtual Machine in a Meta-Tracing Compiler Framework","abstract":"Meta-compiler frameworks, such as RPython and Graal/Truffle, generate high-performance virtual machines (VMs) from interpreter definitions. Although they generate VMs with high-quality just-in-time (JIT) compilers, they still lack an important feature that dedicated VMs (i.e., VMs that are developed for specific languages) have, namely \\emph{multi-tier compilation}. Multi-tier compilation uses light-weight compilers at early stages and highly-optimizing compilers at later stages in order to balance between compilation overheads and code quality.","authors":["Yusuke Izawa","Hidehiko Masuhara","Carl Friedrich Bolz-Tereick"],"url":"https://arxiv.org/abs/2504.17460"}
{"created":"2025-04-29","title":"Combining GCN Structural Learning with LLM Chemical Knowledge for Enhanced Virtual Screening","abstract":"Virtual screening plays a critical role in modern drug discovery by enabling the identification of promising candidate molecules for experimental validation. Traditional machine learning methods such, as Support Vector Machines (SVM) and XGBoost, rely on predefined molecular representations, often leading to information loss and potential bias. In contrast, deep learning approaches-particularly Graph Convolutional Networks (GCNs)-offer a more expressive and unbiased alternative by operating directly on molecular graphs. Meanwhile, Large Language Models (LLMs) have recently demonstrated state-of-the-art performance in drug design, thanks to their capacity to capture complex chemical patterns from large-scale data via attention mechanisms.","authors":["Radia Berreziga","Mohammed Brahimi","Khairedine Kraim","Hamid Azzoune"],"url":"https://arxiv.org/abs/2504.17497"}
{"created":"2025-04-29","title":"TileLang: A Composable Tiled Programming Model for AI Systems","abstract":"Modern AI workloads rely heavily on optimized computing kernels for both training and inference. These AI kernels follow well-defined data-flow patterns, such as moving tiles between DRAM and SRAM and performing a sequence of computations on those tiles. However, writing high-performance kernels remains complex despite the clarity of these patterns. Achieving peak performance requires careful, hardware-centric optimizations to fully leverage modern accelerators. While domain-specific compilers attempt to reduce the burden of writing high-performance kernels, they often struggle with usability and expressiveness gaps. In this paper, we present TileLang, a generalized tiled programming model for more efficient AI Kernel programming. TileLang decouples scheduling space (thread binding, layout, tensorize and pipeline) from dataflow, and encapsulated them as a set of customization annotations and primitives. This approach allows users to focus on the kernel's data-flow itself, while leaving most other optimizations to compilers. We conduct comprehensive experiments on commonly-used devices, across numerous experiments, our evaluation shows that TileLang can achieve state-of-the-art performance in key kernels, demonstrating that its unified block-and-thread paradigm and transparent scheduling capabilities deliver both the power and flexibility demanded by modern AI system development.","authors":["Lei Wang","Yu Cheng","Yining Shi","Zhengju Tang","Zhiwen Mo","Wenhao Xie","Lingxiao Ma","Yuqing Xia","Jilong Xue","Fan Yang","Zhi Yang"],"url":"https://arxiv.org/abs/2504.17577"}
{"created":"2025-04-29","title":"Online metric TSP","abstract":"In the online metric traveling salesperson problem, $n$ points of a metric space arrive one by one and have to be placed (immediately and irrevocably) into empty cells of a size-$n$ array. The goal is to minimize the sum of distances between consecutive points in the array. This problem was introduced by Abrahamsen, Bercea, Beretta, Klausen, and Kozma [ESA'24] as a generalization of the online sorting problem, which was introduced by Aamand, Abrahamsen, Beretta, and Kleist [SODA'23] as a tool in their study of online geometric packing problems.","authors":["Christian Bertram"],"url":"https://arxiv.org/abs/2504.17716"}
{"created":"2025-04-29","title":"Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN","abstract":"In the field of image recognition, spiking neural networks (SNNs) have achieved performance comparable to conventional artificial neural networks (ANNs). In such applications, SNNs essentially function as traditional neural networks with quantized activation values. This article focuses on an another alternative perspective,viewing SNNs as binary-activated recurrent neural networks (RNNs) for sequential modeling tasks.From this viewpoint, current SNN architectures face several fundamental challenges in sequence modeling: (1) Traditional models lack effective memory mechanisms for long-range sequence modeling; (2) The biological-inspired components in SNNs (such as reset mechanisms and refractory period applications) remain theoretically under-explored for sequence tasks; (3) The RNN-like computational paradigm in SNNs prevents parallel training across different timesteps.To address these challenges, this study conducts a systematic analysis of the fundamental mechanisms underlying reset operations and refractory periods in binary-activated RNN-based SNN sequence models. We re-examine whether such biological mechanisms are strictly necessary for generating sparse spiking patterns, provide new theoretical explanations and insights, and ultimately propose the fixed-refractory-period SNN architecture for sequence modeling.","authors":["Enqi Zhang"],"url":"https://arxiv.org/abs/2504.17751"}
{"created":"2025-04-29","title":"Step1X-Edit: A Practical Framework for General Image Editing","abstract":"In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.","authors":["Shiyu Liu","Yucheng Han","Peng Xing","Fukun Yin","Rui Wang","Wei Cheng","Jiaqi Liao","Yingming Wang","Honghao Fu","Chunrui Han","Guopeng Li","Yuang Peng","Quan Sun","Jingwei Wu","Yan Cai","Zheng Ge","Ranchen Ming","Lei Xia","Xianfang Zeng","Yibo Zhu","Binxing Jiao","Xiangyu Zhang","Gang Yu","Daxin Jiang"],"url":"https://arxiv.org/abs/2504.17761"}
{"created":"2025-04-29","title":"Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control","abstract":"Learning-based methods, such as imitation learning (IL) and reinforcement learning (RL), can produce excel control policies over challenging agile robot tasks, such as sports robot. However, no existing work has harmonized learning-based policy with model-based methods to reduce training complexity and ensure the safety and stability for agile badminton robot control. In this paper, we introduce Hamlet, a novel hybrid control system for agile badminton robots. Specifically, we propose a model-based strategy for chassis locomotion which provides a base for arm policy. We introduce a physics-informed \"IL+RL\" training framework for learning-based arm policy. In this train framework, a model-based strategy with privileged information is used to guide arm policy training during both IL and RL phases. In addition, we train the critic model during IL phase to alleviate the performance drop issue when transitioning from IL to RL. We present results on our self-engineered badminton robot, achieving 94.5% success rate against the serving machine and 90.7% success rate against human players. Our system can be easily generalized to other agile mobile manipulation tasks such as agile catching and table tennis. Our project website: https://dreamstarring.github.io/HAMLET/.","authors":["Haochen Wang","Zhiwei Shi","Chengxi Zhu","Yafei Qiao","Cheng Zhang","Fan Yang","Pengjie Ren","Lan Lu","Dong Xuan"],"url":"https://arxiv.org/abs/2504.17771"}
{"created":"2025-04-29","title":"Token-Shuffle: Towards High-Resolution Image Generation with Autoregressive Models","abstract":"Autoregressive (AR) models, long dominant in language generation, are increasingly applied to image synthesis but are often considered less competitive than Diffusion-based models. A primary limitation is the substantial number of image tokens required for AR models, which constrains both training and inference efficiency, as well as image resolution. To address this, we present Token-Shuffle, a novel yet simple method that reduces the number of image tokens in Transformer. Our key insight is the dimensional redundancy of visual vocabularies in Multimodal Large Language Models (MLLMs), where low-dimensional visual codes from visual encoder are directly mapped to high-dimensional language vocabularies. Leveraging this, we consider two key operations: token-shuffle, which merges spatially local tokens along channel dimension to decrease the input token number, and token-unshuffle, which untangles the inferred tokens after Transformer blocks to restore the spatial arrangement for output. Jointly training with textual prompts, our strategy requires no additional pretrained text-encoder and enables MLLMs to support extremely high-resolution image synthesis in a unified next-token prediction way while maintaining efficient training and inference. For the first time, we push the boundary of AR text-to-image generation to a resolution of 2048x2048 with gratifying generation performance. In GenAI-benchmark, our 2.7B model achieves 0.77 overall score on hard prompts, outperforming AR models LlamaGen by 0.18 and diffusion models LDM by 0.15. Exhaustive large-scale human evaluations also demonstrate our prominent image generation ability in terms of text-alignment, visual flaw, and visual appearance. We hope that Token-Shuffle can serve as a foundational design for efficient high-resolution image generation within MLLMs.","authors":["Xu Ma","Peize Sun","Haoyu Ma","Hao Tang","Chih-Yao Ma","Jialiang Wang","Kunpeng Li","Xiaoliang Dai","Yujun Shi","Xuan Ju","Yushi Hu","Artsiom Sanakoyeu","Felix Juefei-Xu","Ji Hou","Junjiao Tian","Tao Xu","Tingbo Hou","Yen-Cheng Liu","Zecheng He","Zijian He","Matt Feiszli","Peizhao Zhang","Peter Vajda","Sam Tsai","Yun Fu"],"url":"https://arxiv.org/abs/2504.17789"}
{"created":"2025-04-29","title":"Precise High-Dimensional Asymptotics for Quantifying Heterogeneous Transfers","abstract":"The problem of learning one task with samples from another task is central to transfer learning (TL). In this paper, we examine a fundamental question: When does combining the data samples from a source task and a target task perform better than single-task learning with the target task alone? This question is motivated by an intriguing phenomenon known as negative transfer often observed in the TL literature. Precise quantification of TL effects -- even within simple statistical models -- has remained elusive in the statistical learning literature. A critical challenge is that to compare TL to single-task learning, we would need to compare the risks between two different estimators in a very precise way. In particular, the comparative advantage of one estimator over another would depend on the specific distribution shifts between the two tasks. This paper applies recent developments in the random matrix theory literature to tackle this challenge in a high-dimensional linear regression setting with two tasks. We provide precise high-dimensional asymptotics for the bias and variance of hard parameter sharing (HPS) estimators in the proportional limit, when the sample sizes of both tasks increase proportionally with dimension at fixed ratios. The precise asymptotics are expressed as a function of the sample sizes of both tasks, the covariate shift between their feature population covariate matrices, and the model shift. We provide illustrative examples of our results in a random-effects model to determine positive and negative transfers. For example, we can identify a phase transition in the high-dimensional linear regression setting from positive transfer to negative transfer under a model shift between the source and target tasks. The finding regarding phase transition can be extended to a multiple-task learning setting where the feature covariates are shared across all tasks.","authors":["Fan Yang","Hongyang R. Zhang","Sen Wu","Christopher R\\'e","Weijie J. Su"],"url":"https://arxiv.org/abs/2010.11750"}
{"created":"2025-04-29","title":"SONC Optimization and Exact Nonnegativity Certificates via Second-Order Cone Programming","abstract":"The second-order cone (SOC) is a class of simple convex cones and optimizing over them can be done more efficiently than with semidefinite programming. It is interesting both in theory and in practice to investigate which convex cones admit a representation using SOCs, given that they have a strong expressive ability. In this paper, we prove constructively that the cone of sums of nonnegative circuits (SONC) admits a SOC representation. Based on this, we give a new algorithm for unconstrained polynomial optimization via SOC programming. We also provide a hybrid numeric-symbolic scheme which combines the numerical procedure with a rounding-projection algorithm to obtain exact nonnegativity certificates. Numerical experiments demonstrate the efficiency of our algorithm for polynomials with fairly large degree and number of variables.","authors":["Victor Magron","Jie Wang"],"url":"https://arxiv.org/abs/2012.07903"}
{"created":"2025-04-29","title":"Characterizing Trust and Resilience in Distributed Consensus for Cyberphysical Systems","abstract":"This work considers the problem of resilient consensus where stochastic values of trust between agents are available. Specifically, we derive a unified mathematical framework to characterize convergence, deviation of the consensus from the true consensus value, and expected convergence rate, when there exists additional information of trust between agents. We show that under certain conditions on the stochastic trust values and consensus protocol: 1) almost sure convergence to a common limit value is possible even when malicious agents constitute more than half of the network connectivity, 2) the deviation of the converged limit, from the case where there is no attack, i.e., the true consensus value, can be bounded with probability that approaches 1 exponentially, and 3) correct classification of malicious and legitimate agents can be attained in finite time almost surely. Further, the expected convergence rate decays exponentially as a function of the quality of the trust observations between agents.","authors":["Michal Yemini","Angelia Nedi\\'c","Andrea Goldsmith","Stephanie Gil"],"url":"https://arxiv.org/abs/2103.05464"}
{"created":"2025-04-29","title":"Unifying Summary Statistic Selection for Approximate Bayesian Computation","abstract":"Extracting low-dimensional summary statistics from large datasets is essential for efficient (likelihood-free) inference. We characterize different classes of summaries and demonstrate their importance for correctly analysing dimensionality reduction algorithms. We demonstrate that minimizing the expected posterior entropy (EPE) under the prior predictive distribution of the model subsumes many existing methods. They are equivalent to or are special or limiting cases of minimizing the EPE. We offer a unifying framework for obtaining informative summaries, provide concrete recommendations for practitioners, and propose a practical method to obtain high-fidelity summaries whose utility we demonstrate for both benchmark and practical examples.","authors":["Till Hoffmann","Jukka-Pekka Onnela"],"url":"https://arxiv.org/abs/2206.02340"}
{"created":"2025-04-29","title":"The Cheeger Inequality and Coboundary Expansion: Beyond Constant Coefficients","abstract":"The Cheeger constant of a graph, or equivalently its coboundary expansion, quantifies the expansion of the graph. This notion assumes an implicit choice of a coefficient group, namely, $\\mathbb{F}_2$. In this paper, we study Cheeger-type inequalities for graphs endowed with a generalized coefficient group, called a sheaf; this is motivated by applications to cosystolic expansion and locally testable codes. We prove that a graph is a good spectral expander if and only if it has good coboundary expansion relative to any (resp. some) constant sheaf, or equivalently, relative to any `ordinary' coefficient group. We moreover show that sheaves that are close to being constant in a well-defined sense are also good coboundary expanders, provided that their underlying graph is an expander, thus giving the first example of good coboundary expansion in non-cosntant sheaves on sparse graphs. By contrast, we observe that for general sheaves on graphs, it is impossible to relate the expansion of the graph and the coboundary expansion of the sheaf.","authors":["Uriya A. First","Tali Kaufman"],"url":"https://arxiv.org/abs/2208.01776"}
{"created":"2025-04-29","title":"A Strong Duality Result for Constrained POMDPs with Multiple Cooperative Agents","abstract":"The work studies the problem of decentralized constrained POMDPs in a team-setting where multiple nonstrategic agents have asymmetric information. Using an extension of Sion's Minimax theorem for functions with positive infinity and results on weak-convergence of measures, strong duality is established for the setting of infinite-horizon expected total discounted costs when the observations lie in a countable space, the actions are chosen from a finite space, the constraint costs are bounded, and the objective cost is bounded from below.","authors":["Nouman Khan","Vijay Subramanian"],"url":"https://arxiv.org/abs/2303.14932"}
{"created":"2025-04-29","title":"Causal Q-Aggregation for CATE Model Selection","abstract":"Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\\frac{\\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error terms related to products of errors in the nuisance functions. Crucially, our regret rate does not require that any of the candidate CATE models be close to the truth. We validate our new method on many semi-synthetic datasets and also provide extensions of our work to CATE model selection with instrumental variables and unobserved confounding.","authors":["Hui Lan","Vasilis Syrgkanis"],"url":"https://arxiv.org/abs/2310.16945"}
{"created":"2025-04-29","title":"Dependency-Aware Compilation for Surface Code Quantum Architectures","abstract":"Practical applications of quantum computing depend on fault-tolerant devices with error correction. Today, the most promising approach is a class of error-correcting codes called surface codes. We study the problem of compiling quantum circuits for quantum computers implementing surface codes. Optimal or near-optimal compilation is critical for both efficiency and correctness. The compilation problem requires (1) mapping circuit qubits to the device qubits and (2) routing execution paths between interacting qubits. We solve this problem efficiently and near-optimally with a novel algorithm that exploits the dependency structure of circuit operations to formulate discrete optimization problems that can be approximated via simulated annealing, a classic and simple algorithm. Our extensive evaluation shows that our approach is powerful and flexible for compiling realistic workloads.","authors":["Abtin Molavi","Amanda Xu","Swamit Tannu","Aws Albarghouthi"],"url":"https://arxiv.org/abs/2311.18042"}
{"created":"2025-04-29","title":"An extension of May's Theorem to three alternatives: axiomatizing Minimax voting","abstract":"May's Theorem [K. O. May, Econometrica 20 (1952) 680-684] characterizes majority voting on two alternatives as the unique preferential voting method satisfying several simple axioms. Here we show that by adding some desirable axioms to May's axioms, we can uniquely determine how to vote on three alternatives (setting aside tiebreaking). In particular, we add two axioms stating that the voting method should mitigate spoiler effects and avoid the so-called strong no show paradox. We prove a theorem stating that any preferential voting method satisfying our enlarged set of axioms, which includes some weak homogeneity and preservation axioms, must choose from among the Minimax winners in all three-alternative elections. When applied to more than three alternatives, our axioms also distinguish Minimax from other known voting methods that coincide with or refine Minimax for three alternatives.","authors":["Wesley H. Holliday","Eric Pacuit"],"url":"https://arxiv.org/abs/2312.14256"}
{"created":"2025-04-29","title":"Sampling and estimation on manifolds using the Langevin diffusion","abstract":"Error bounds are derived for sampling and estimation using a discretization of an intrinsically defined Langevin diffusion with invariant measure $\\text{d}\\mu_\\phi \\propto e^{-\\phi} \\mathrm{dvol}_g $ on a compact Riemannian manifold. Two estimators of linear functionals of $\\mu_\\phi $ based on the discretized Markov process are considered: a time-averaging estimator based on a single trajectory and an ensemble-averaging estimator based on multiple independent trajectories. Imposing no restrictions beyond a nominal level of smoothness on $\\phi$, first-order error bounds, in discretization step size, on the bias and variance/mean-square error of both estimators are derived. The order of error matches the optimal rate in Euclidean and flat spaces, and leads to a first-order bound on distance between the invariant measure $\\mu_\\phi$ and a stationary measure of the discretized Markov process. This order is preserved even upon using retractions when exponential maps are unavailable in closed form, thus enhancing practicality of the proposed algorithms. Generality of the proof techniques, which exploit links between two partial differential equations and the semigroup of operators corresponding to the Langevin diffusion, renders them amenable for the study of a more general class of sampling algorithms related to the Langevin diffusion. Conditions for extending analysis to the case of non-compact manifolds are discussed. Numerical illustrations with distributions, log-concave and otherwise, on the manifolds of positive and negative curvature elucidate on the derived bounds and demonstrate practical utility of the sampling algorithm.","authors":["Karthik Bharath","Alexander Lewis","Akash Sharma","Michael V Tretyakov"],"url":"https://arxiv.org/abs/2312.14882"}
{"created":"2025-04-29","title":"An $\\ell^1$-Plug-and-Play Approach for MPI Using a Zero Shot Denoiser with Evaluation on the 3D Open MPI Dataset","abstract":"Objective: Magnetic particle imaging (MPI) is an emerging medical imaging modality which has gained increasing interest in recent years. Among the benefits of MPI are its high temporal resolution, and that the technique does not expose the specimen to any kind of ionizing radiation. It is based on the non-linear response of magnetic nanoparticles to an applied magnetic field. From the electric signal measured in receive coils, the particle concentration has to be reconstructed. Due to the ill-posedness of the reconstruction problem, various regularization methods have been proposed for reconstruction ranging from early stopping methods, via classical Tikhonov regularization and iterative methods to modern machine learning approaches. In this work, we contribute to the latter class: we propose a plug-and-play approach based on a generic zero-shot denoiser with an $\\ell^1$-prior.","authors":["Vladyslav Gapyak","Corinna Rentschler","Thomas M\\\"arz","Andreas Weinmann"],"url":"https://arxiv.org/abs/2401.00275"}
{"created":"2025-04-29","title":"On the $O(\\frac{\\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\\ell_1$ Norm","abstract":"Although adaptive gradient methods have been extensively used in deep learning, their convergence rates proved in the literature are all slower than that of SGD, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\\frac{1}{T}\\sum_{k=1}^T E\\left[\\|\\nabla f(x^k)\\|_1\\right]\\leq O(\\frac{\\sqrt{d}C}{T^{1/4}})$ measured by $\\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable, $T$ is the iteration number, and $C$ is a constant identical to that appeared in the optimal convergence rate of SGD. Our convergence rate matches the lower bound with respect to all the coefficients except the dimension $d$. Since $\\|x\\|_2\\ll\\|x\\|_1\\leq\\sqrt{d}\\|x\\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\\frac{1}{T}\\sum_{k=1}^T E\\left[\\|\\nabla f(x^k)\\|_2\\right]\\leq O(\\frac{C}{T^{1/4}})$ rate of SGD in the ideal case of $\\|\\nabla f(x)\\|_1=\\varTheta(\\sqrt{d}\\|\\nabla f(x)\\|_2)$.","authors":["Huan Li","Yiming Dong","Zhouchen Lin"],"url":"https://arxiv.org/abs/2402.00389"}
{"created":"2025-04-29","title":"GeoFlood: Computational model for overland flooding","abstract":"This paper presents GeoFlood, a new open-source software package for solving the shallow-water equations (SWE) on a quadtree hierarchy of mapped, logically Cartesian grids managed by the parallel, adaptive library ForestClaw (Calhoun and Burstedde, 2017). The GeoFlood model is validated using standard benchmark tests from Neelz and Pender (2013) as well as the historical Malpasset dam failure. The benchmark test results are compared against those obtained from GeoClaw (Clawpack Development Team, 2020) and the software package HEC-RAS (Hydraulic Engineering Center River Analysis System, Army Corps of Engineers) (Brunner, 2018). The Malpasset outburst flood results are compared with those presented in George (2011) (obtained from the GeoClaw software), model results from Hervouet and Petitjean (1999), and empirical data. The comparisons validate GeoFlood's capabilities for idealized benchmarks compared to other commonly used models as well as its ability to efficiently simulate highly dynamic floods in complex terrain, consistent with historical field data. Because it is massively parallel and scalable, GeoFlood may be a valuable tool for efficiently computing large-scale flooding problems at very high resolutions.","authors":["Brian Kyanjo","Donna Calhoun","David L. George"],"url":"https://arxiv.org/abs/2403.15435"}
{"created":"2025-04-29","title":"A Weight-aware-based Multi-source Unsupervised Domain Adaptation Method for Human Motion Intention Recognition","abstract":"Accurate recognition of human motion intention (HMI) is beneficial for exoskeleton robots to improve the wearing comfort level and achieve natural human-robot interaction. A classifier trained on labeled source subjects (domains) performs poorly on unlabeled target subject since the difference in individual motor characteristics. The unsupervised domain adaptation (UDA) method has become an effective way to this problem. However, the labeled data are collected from multiple source subjects that might be different not only from the target subject but also from each other. The current UDA methods for HMI recognition ignore the difference between each source subject, which reduces the classification accuracy. Therefore, this paper considers the differences between source subjects and develops a novel theory and algorithm for UDA to recognize HMI, where the margin disparity discrepancy (MDD) is extended to multi-source UDA theory and a novel weight-aware-based multi-source UDA algorithm (WMDD) is proposed. The source domain weight, which can be adjusted adaptively by the MDD between each source subject and target subject, is incorporated into UDA to measure the differences between source subjects. The developed multi-source UDA theory is theoretical and the generalization error on target subject is guaranteed. The theory can be transformed into an optimization problem for UDA, successfully bridging the gap between theory and algorithm. Moreover, a lightweight network is employed to guarantee the real-time of classification and the adversarial learning between feature generator and ensemble classifiers is utilized to further improve the generalization ability. The extensive experiments verify theoretical analysis and show that WMDD outperforms previous UDA methods on HMI recognition tasks.","authors":["Xiao-Yin Liu","Guotao Li","Xiao-Hu Zhou","Xu Liang","Zeng-Guang Hou"],"url":"https://arxiv.org/abs/2404.15366"}
{"created":"2025-04-29","title":"The dynamics of leadership and success in software development teams","abstract":"From science to industry, teamwork plays a crucial role in knowledge production and innovation. Most studies consider teams as static groups of individuals, thereby failing to capture how the micro-dynamics of collaborative processes and organizational changes determine team success. Here, we leverage fine-grained temporal data on software development teams from three software ecosystems -- Rust, JavaScript, and Python -- to gain insights into the dynamics of online collaborative projects. Our analysis reveals an uneven workload distribution in teams, with stronger heterogeneity correlated with higher success, and the early emergence of a lead developer carrying out the majority of work. Moreover, we find that a sizeable fraction of projects experience a change of lead developer, with such a transition being more likely in projects led by inexperienced users. Finally, we show that leadership change is associated with faster success growth. Our work contributes to a deeper understanding of the link between team evolution and success in collaborative processes.","authors":["Lorenzo Betti","Luca Gallo","Johannes Wachs","Federico Battiston"],"url":"https://arxiv.org/abs/2404.18833"}
{"created":"2025-04-29","title":"Guided Multi-objective Generative AI to Enhance Structure-based Drug Design","abstract":"Generative AI has the potential to revolutionize drug discovery. Yet, despite recent advances in deep learning, existing models cannot generate molecules that satisfy all desired physicochemical properties. Herein, we describe IDOLpro, a generative chemistry AI combining diffusion with multi-objective optimization for structure-based drug design. Differentiable scoring functions guide the latent variables of the diffusion model to explore uncharted chemical space and generate novel ligands in silico, optimizing a plurality of target physicochemical properties. We demonstrate our platform's effectiveness by generating ligands with optimized binding affinity and synthetic accessibility on two benchmark sets. IDOLpro produces ligands with binding affinities over 10%-20% better than the next best state-of-the-art method on each test set, producing more drug-like molecules with generally better synthetic accessibility scores than other methods. We do a head-to-head comparison of IDOLpro against a classic virtual screen of a large database of drug-like molecules. We show that IDOLpro can generate molecules for a range of important disease-related targets with better binding affinity and synthetic accessibility than any molecule found in the virtual screen while being over 100x faster and less expensive to run. On a test set of experimental complexes, IDOLpro is the first to produce molecules with better binding affinities than experimentally observed ligands. IDOLpro can accommodate other scoring functions (e.g. ADME-Tox) to accelerate hit-finding, hit-to-lead, and lead optimization for drug discovery.","authors":["Amit Kadan","Kevin Ryczko","Erika Lloyd","Adrian Roitberg","Takeshi Yamazaki"],"url":"https://arxiv.org/abs/2405.11785"}
{"created":"2025-04-29","title":"Noise-tolerant learnability of shallow quantum circuits from statistics and the cost of quantum pseudorandomness","abstract":"In this work, we study the learnability of quantum circuits in the near term. We demonstrate the natural robustness of quantum statistical queries for learning quantum processes, motivating their use as a theoretical tool for near-term learning problems. We adapt a learning algorithm for constant-depth quantum circuits to the quantum statistical query setting, and show that such circuits can be learned in our setting with only a linear overhead in the query complexity. We prove average-case quantum statistical query lower bounds for learning, within diamond distance, random quantum circuits with depth at least logarithmic and at most linear in the system size. Finally, we prove that pseudorandom unitaries (PRUs) cannot be constructed using circuits of constant depth by constructing an efficient distinguisher using existing learning algorithms. To show the correctness of our distinguisher, we prove a new variation of the quantum no free lunch theorem.","authors":["Chirag Wadhwa","Mina Doosti"],"url":"https://arxiv.org/abs/2405.12085"}
{"created":"2025-04-29","title":"Building a stable classifier with the inflated argmax","abstract":"We propose a new framework for algorithmic stability in the context of multiclass classification. In practice, classification algorithms often operate by first assigning a continuous score (for instance, an estimated probability) to each possible label, then taking the maximizer -- i.e., selecting the class that has the highest score. A drawback of this type of approach is that it is inherently unstable, meaning that it is very sensitive to slight perturbations of the training data, since taking the maximizer is discontinuous. Motivated by this challenge, we propose a pipeline for constructing stable classifiers from data, using bagging (i.e., resampling and averaging) to produce stable continuous scores, and then using a stable relaxation of argmax, which we call the \"inflated argmax,\" to convert these scores to a set of candidate labels. The resulting stability guarantee places no distributional assumptions on the data, does not depend on the number of classes or dimensionality of the covariates, and holds for any base classifier. Using a common benchmark data set, we demonstrate that the inflated argmax provides necessary protection against unstable classifiers, without loss of accuracy.","authors":["Jake A. Soloff","Rina Foygel Barber","Rebecca Willett"],"url":"https://arxiv.org/abs/2405.14064"}
{"created":"2025-04-29","title":"SEMF: Supervised Expectation-Maximization Framework for Predicting Intervals","abstract":"This work introduces the Supervised Expectation-Maximization Framework (SEMF), a versatile and model-agnostic approach for generating prediction intervals with any ML model. SEMF extends the Expectation-Maximization algorithm, traditionally used in unsupervised learning, to a supervised context, leveraging latent variable modeling for uncertainty estimation. Through extensive empirical evaluation of diverse simulated distributions and 11 real-world tabular datasets, SEMF consistently produces narrower prediction intervals while maintaining the desired coverage probability, outperforming traditional quantile regression methods. Furthermore, without using the quantile (pinball) loss, SEMF allows point predictors, including gradient-boosted trees and neural networks, to be calibrated with conformal quantile regression. The results indicate that SEMF enhances uncertainty quantification under diverse data distributions and is particularly effective for models that otherwise struggle with inherent uncertainty representation.","authors":["Ilia Azizi","Marc-Olivier Boldi","Val\\'erie Chavez-Demoulin"],"url":"https://arxiv.org/abs/2405.18176"}
{"created":"2025-04-29","title":"A Compounded Burr Probability Distribution for Fitting Heavy-Tailed Data with Applications to Biological Networks","abstract":"Complex biological networks, encompassing metabolic pathways, gene regulatory systems, and protein-protein interaction networks, often exhibit scale-free structures characterized by heavy-tailed degree distributions. However, empirical studies reveal significant deviations from ideal power law behavior, underscoring the need for more flexible and accurate probabilistic models. In this work, we propose the Compounded Burr (CBurr) distribution, a novel four parameter family derived by compounding the Burr distribution with a discrete mixing process. This model is specifically designed to capture both the body and tail behavior of real-world network degree distributions with applications to biological networks. We rigorously derive its statistical properties, including moments, hazard and risk functions, and tail behavior, and develop an efficient maximum likelihood estimation framework. The CBurr model demonstrates broad applicability to networks with complex connectivity patterns, particularly in biological, social, and technological domains. Extensive experiments on large-scale biological network datasets show that CBurr consistently outperforms classical power-law, log-normal, and other heavy-tailed models across the full degree spectrum. By providing a statistically grounded and interpretable framework, the CBurr model enhances our ability to characterize the structural heterogeneity of biological networks.","authors":["Tanujit Chakraborty","Swarup Chattopadhyay","Suchismita Das","Shraddha M. Naik","Chittaranjan Hens"],"url":"https://arxiv.org/abs/2407.04465"}
{"created":"2025-04-29","title":"Application of Machine Learning and Convex Limiting to Subgrid Flux Modeling in the Shallow-Water Equations","abstract":"We propose a combination of machine learning and flux limiting for property-preserving subgrid scale modeling in the context of flux-limited finite volume methods for the one-dimensional shallow-water equations. The numerical fluxes of a conservative target scheme are fitted to the coarse-mesh averages of a monotone fine-grid discretization using a neural network to parametrize the subgrid scale components. To ensure positivity preservation and the validity of local maximum principles, we use a flux limiter that constrains the intermediate states of an equivalent fluctuation form to stay in a convex admissible set. The results of our numerical studies confirm that the proposed combination of machine learning with monolithic convex limiting produces meaningful closures even in scenarios for which the network was not trained.","authors":["Ilya Timofeyev","Alexey Schwarzmann","Dmitri Kuzmin"],"url":"https://arxiv.org/abs/2407.17214"}
{"created":"2025-04-29","title":"Mechanistic Modeling of Lipid Nanoparticle Formation for the Delivery of Nucleic Acid Therapeutics","abstract":"Nucleic acids such as mRNA have emerged as a promising therapeutic modality with the capability of addressing a wide range of diseases. Lipid nanoparticles (LNPs) as a delivery platform for nucleic acids were used in the COVID-19 vaccines and have received much attention. While modern manufacturing processes which involve rapidly mixing an organic stream containing the lipids with an aqueous stream containing the nucleic acids are conceptually straightforward, detailed understanding of LNP formation and structure is still limited and scale-up can be challenging. Mathematical and computational methods are a promising avenue for deepening scientific understanding of the LNP formation process and facilitating improved process development and control. This article describes strategies for the mechanistic modeling of LNP formation, starting with strategies to estimate and predict important physicochemical properties of the various species such as diffusivities and solubilities. Subsequently, a framework is outlined for constructing mechanistic models of reactor- and particle-scale processes. Insights gained from the various models are mapped back to product quality attributes and process insights. Lastly, the use of the models to guide development of advanced process control and optimization strategies is discussed.","authors":["Pavan K. Inguva","Saikat Mukherjee","Pierre J. Walker","Vico Tenberg","Cedric Devos","Sunkyu Shin","Yanchen Wu","Srimanta Santra","Jie Wang","Shalini Singh","Mona A. Kanso","Shin Hyuk Kim","Bernhardt L. Trout","Martin Z. Bazant","Allan S. Myerson","Richard D. Braatz"],"url":"https://arxiv.org/abs/2408.08577"}
{"created":"2025-04-29","title":"Bounds On MLDR Codes over ${\\mathbb Z}_{p^t}$","abstract":"Upper bounds on the minimum Lee distance of codes that are linear over ${\\mathbb Z}_q$, $q=p^t$, $p$ prime are discussed. The bounds are Singleton like, depending on the length, rank, and alphabet size of the code. Codes meeting such bounds are referred to as Maximum Lee Distance with respect to Rank (MLDR) Codes. We present some new bounds on MLDR codes, using combinatorial arguments. In the context of MLDR codes, our work provides improvements over existing bounds in the literature","authors":["Tim L. Alderson"],"url":"https://arxiv.org/abs/2408.11107"}
{"created":"2025-04-29","title":"Adaptive Sample Aggregation In Transfer Learning","abstract":"Transfer Learning aims to optimally aggregate samples from a target distribution, with related samples from a so-called source distribution to improve target risk. Multiple procedures have been proposed over the last two decades to address this problem, each driven by one of a multitude of possible divergence measures between source and target distributions. A first question asked in this work is whether there exist unified algorithmic approaches that automatically adapt to many of these divergence measures simultaneously.","authors":["Steve Hanneke","Samory Kpotufe"],"url":"https://arxiv.org/abs/2408.16189"}
{"created":"2025-04-29","title":"A Novel Massive Random Access in Cell-Free Massive MIMO Systems for High-Speed Mobility with OTFS Modulation","abstract":"In the research of next-generation wireless communication technologies, orthogonal time frequency space (OTFS) modulation is emerging as a promising technique for high-speed mobile environments due to its superior efficiency and robustness in doubly selective channels. Additionally, the cell-free architecture, which eliminates the issues associated with cell boundaries, offers broader coverage for radio access networks. By combining cell-free network architecture with OTFS modulation, the system may meet the demands of massive random access required by machine-type communication devices in high-speed scenarios. This paper explores a massive random access scheme based on OTFS modulation within a cell-free architecture. A transceiver model for uplink OTFS signals involving multiple access points (APs) is developed, where channel estimation with fractional channel parameters is approximated as a block sparse matrix recovery problem. Building on existing superimposed and embedded preamble schemes, a hybrid preamble scheme is proposed. This scheme leverages superimposed and embedded preambles to respectively achieve rough and accurate active user equipment (UEs) detection (AUD), as well as precise channel estimation, under the condition of supporting a large number of access UEs. Moreover, this study introduces a generalized approximate message passing and pattern coupling sparse Bayesian learning with Laplacian prior (GAMP-PCSBL-La) algorithm, which effectively captures block sparse features after discrete cosine transform (DCT), delivering precise estimation results with reduced computational complexity. Simulation results demonstrate that the proposed scheme is effective and provides superior performance compared to other existing schemes.","authors":["Yanfeng Hu","Dongming Wang","Xinjiang Xia","Jiamin Li","Pengcheng Zhu","Xiaohu You"],"url":"https://arxiv.org/abs/2409.01111"}
{"created":"2025-04-29","title":"Quantum Kernel Methods under Scrutiny: A Benchmarking Study","abstract":"Since the entry of kernel theory in the field of quantum machine learning, quantum kernel methods (QKMs) have gained increasing attention with regard to both probing promising applications and delivering intriguing research insights. Benchmarking these methods is crucial to gain robust insights and to understand their practical utility. In this work, we present a comprehensive large-scale study examining QKMs based on fidelity quantum kernels (FQKs) and projected quantum kernels (PQKs) across a manifold of design choices. Our investigation encompasses both classification and regression tasks for five dataset families and 64 datasets, systematically comparing the use of FQKs and PQKs quantum support vector machines and kernel ridge regression. This resulted in over 20,000 models that were trained and optimized using a state-of-the-art hyperparameter search to ensure robust and comprehensive insights. We delve into the importance of hyperparameters on model performance scores and support our findings through rigorous correlation analyses. Additionally, we provide an in-depth analysis addressing the design freedom of PQKs and explore the underlying principles responsible for learning. Our goal is not to identify the best-performing model for a specific task but to uncover the mechanisms that lead to effective QKMs and reveal universal patterns.","authors":["Jan Schnabel","Marco Roth"],"url":"https://arxiv.org/abs/2409.04406"}
{"created":"2025-04-29","title":"Performance of Quantum Approximate Optimization with Quantum Error Detection","abstract":"Quantum algorithms must be scaled up to tackle real-world applications. Doing so requires overcoming the noise present on today's hardware. The quantum approximate optimization algorithm (QAOA) is a promising candidate for scaling up, due to its modest resource requirements and documented asymptotic speedup over state-of-the-art classical algorithms for some problems. However, achieving better-than-classical performance with QAOA is believed to require fault tolerance. In this paper, we demonstrate a partially fault-tolerant implementation of QAOA using the $[[k+2,k,2]]$ ``Iceberg'' error detection code. We observe that encoding the circuit with the Iceberg code improves the algorithmic performance as compared to the unencoded circuit for problems with up to $20$ logical qubits on a trapped-ion quantum computer. Additionally, we propose and calibrate a model for predicting the code performance. We use this model to characterize the limits of the Iceberg code and extrapolate its performance to future hardware with improved error rates. In particular, we show how our model can be used to determine the necessary conditions for QAOA to outperform the Goemans-Williamson algorithm on future hardware. To the best of our knowledge, our results demonstrate the largest universal quantum computing algorithm protected by partially fault-tolerant quantum error detection on practical applications to date, paving the way towards solving real-world applications with quantum computers.","authors":["Zichang He","David Amaro","Ruslan Shaydulin","Marco Pistoia"],"url":"https://arxiv.org/abs/2409.12104"}
{"created":"2025-04-29","title":"Devil is in Details: Locality-Aware 3D Abdominal CT Volume Generation for Self-Supervised Organ Segmentation","abstract":"In the realm of medical image analysis, self-supervised learning (SSL) techniques have emerged to alleviate labeling demands, while still facing the challenge of training data scarcity owing to escalating resource requirements and privacy constraints. Numerous efforts employ generative models to generate high-fidelity, unlabeled 3D volumes across diverse modalities and anatomical regions. However, the intricate and indistinguishable anatomical structures within the abdomen pose a unique challenge to abdominal CT volume generation compared to other anatomical regions. To address the overlooked challenge, we introduce the Locality-Aware Diffusion (Lad), a novel method tailored for exquisite 3D abdominal CT volume generation. We design a locality loss to refine crucial anatomical regions and devise a condition extractor to integrate abdominal priori into generation, thereby enabling the generation of large quantities of high-quality abdominal CT volumes essential for SSL tasks without the need for additional data such as labels or radiology reports. Volumes generated through our method demonstrate remarkable fidelity in reproducing abdominal structures, achieving a decrease in FID score from 0.0034 to 0.0002 on AbdomenCT-1K dataset, closely mirroring authentic data and surpassing current methods. Extensive experiments demonstrate the effectiveness of our method in self-supervised organ segmentation tasks, resulting in an improvement in mean Dice scores on two abdominal datasets effectively. These results underscore the potential of synthetic data to advance self-supervised learning in medical image analysis.","authors":["Yuran Wang","Zhijing Wan","Yansheng Qiu","Zheng Wang"],"url":"https://arxiv.org/abs/2409.20332"}
{"created":"2025-04-29","title":"Sample-Efficient Quantum State Tomography for Structured Quantum States in One Dimension","abstract":"While quantum state tomography (QST) remains the gold standard for benchmarking and verifying quantum devices, it requires an exponentially large number of measurements and classical computational resources for generic quantum many-body systems, making it impractical even for intermediate-size quantum devices. Fortunately, many physical quantum states often exhibit certain low-dimensional structures that enable the development of efficient QST. A notable example is the class of states represented by matrix product operators (MPOs) with a finite matrix/bond dimension, which include most physical states in one dimension and where the number of independent parameters describing the states only grows linearly with the number of qubits. Whether a sample efficient quantum state tomography protocol, where the number of required state copies scales only linearly as the number of parameters describing the state, exists for a generic MPO state still remains an important open question.","authors":["Zhen Qin","Casey Jameson","Alireza Goldar","Michael B. Wakin","Zhexuan Gong","Zhihui Zhu"],"url":"https://arxiv.org/abs/2410.02583"}
{"created":"2025-04-29","title":"Hierarchical mixtures of Unigram models for short text clustering: The role of Beta-Liouville priors","abstract":"This paper presents a variant of the Multinomial mixture model tailored to the unsupervised classification of short text data. While the Multinomial probability vector is traditionally assigned a Dirichlet prior distribution, this work explores an alternative formulation based on the Beta-Liouville distribution, which offers a more flexible correlation structure than the Dirichlet. We examine the theoretical properties of the Beta-Liouville distribution, with particular focus on its conjugacy with the Multinomial likelihood. This property enables the derivation of update equations for a CAVI (Coordinate Ascent Variational Inference) algorithm, facilitating approximate posterior inference of the model parameters. In addition, we introduce a stochastic variant of the CAVI algorithm to enhance scalability. The paper concludes with empirical examples demonstrating effective strategies for selecting the Beta-Liouville hyperparameters.","authors":["Massimo Bilancia","Samuele Magro"],"url":"https://arxiv.org/abs/2410.21862"}
{"created":"2025-04-29","title":"Causal-discovery-based root-cause analysis and its application in time-series prediction error diagnosis","abstract":"Recent rapid advancements of machine learning have greatly enhanced the accuracy of prediction models, but most models remain \"black boxes\", making prediction error diagnosis challenging, especially with outliers. This lack of transparency hinders trust and reliability in industrial applications. Heuristic attribution methods, while helpful, often fail to capture true causal relationships, leading to inaccurate error attributions. Various root-cause analysis methods have been developed using Shapley values, yet they typically require predefined causal graphs, limiting their applicability for prediction errors in machine learning models. To address these limitations, we introduce the Causal-Discovery-based Root-Cause Analysis (CD-RCA) method that estimates causal relationships between the prediction error and the explanatory variables, without needing a pre-defined causal graph. By simulating synthetic error data, CD-RCA can identify variable contributions to outliers in prediction errors by Shapley values. Extensive experiments show CD-RCA outperforms current heuristic attribution methods.","authors":["Hiroshi Yokoyama","Ryusei Shingaki","Kaneharu Nishino","Shohei Shimizu","Thong Pham"],"url":"https://arxiv.org/abs/2411.06990"}
{"created":"2025-04-29","title":"Emergence of Collective Accuracy in Socially Connected Networks","abstract":"We analyze the accuracy of collective decision-making in socially connected populations, where agents update binary choices through local interactions on a network. Each agent receives a private signal that is biased -- even marginally -- toward the correct alternative, and social influence mediates the aggregation of these signals. We show analytically that, in the large-population limit, the probability of a correct majority converges to a nontrivial expression involving the regularized incomplete beta function. Remarkably, this collective accuracy surpasses that of any individual agent whenever private signals are better than random, revealing that network-mediated influence can enhance, rather than impair, group performance. Our findings may inform the design of resilient decision-making systems in social, biological, and engineered networks, where accuracy must emerge from interdependent and noisy agents.","authors":["Dan Braha","Marcus A. M. de Aguiar"],"url":"https://arxiv.org/abs/2411.08625"}
{"created":"2025-04-29","title":"Learning Modality-Aware Representations: Adaptive Group-wise Interaction Network for Multimodal MRI Synthesis","abstract":"Multimodal MR image synthesis aims to generate missing modality images by effectively fusing and mapping from a subset of available MRI modalities. Most existing methods adopt an image-to-image translation paradigm, treating multiple modalities as input channels. However, these approaches often yield sub-optimal results due to the inherent difficulty in achieving precise feature- or semantic-level alignment across modalities. To address these challenges, we propose an Adaptive Group-wise Interaction Network (AGI-Net) that explicitly models both inter-modality and intra-modality relationships for multimodal MR image synthesis. Specifically, feature channels are first partitioned into predefined groups, after which an adaptive rolling mechanism is applied to conventional convolutional kernels to better capture feature and semantic correspondences between different modalities. In parallel, a cross-group attention module is introduced to enable effective feature fusion across groups, thereby enhancing the network's representational capacity. We validate the proposed AGI-Net on the publicly available IXI and BraTS2023 datasets. Experimental results demonstrate that AGI-Net achieves state-of-the-art performance in multimodal MR image synthesis tasks, confirming the effectiveness of its modality-aware interaction design. We release the relevant code at: https://github.com/zunzhumu/Adaptive-Group-wise-Interaction-Network-for-Multimodal-MRI-Synthesis.git.","authors":["Tao Song","Yicheng Wu","Minhao Hu","Xiangde Luo","Linda Wei","Guotai Wang","Yi Guo","Feng Xu","Shaoting Zhang"],"url":"https://arxiv.org/abs/2411.14684"}
{"created":"2025-04-29","title":"Linear Supervision for Nonlinear, High-Dimensional Neural Control and Differential Games","abstract":"As the dimension of a system increases, traditional methods for control and differential games rapidly become intractable, making the design of safe autonomous agents challenging in complex or team settings. Deep-learning approaches avoid discretization and yield numerous successes in robotics and autonomy, but at a higher dimensional limit, accuracy falls as sampling becomes less efficient. We propose using rapidly generated linear solutions to the partial differential equation (PDE) arising in the problem to accelerate and improve learned value functions for guidance in high-dimensional, nonlinear problems. We define two programs that combine supervision of the linear solution with a standard PDE loss. We demonstrate that these programs offer improvements in speed and accuracy in both a 50-D differential game problem and a 10-D quadrotor control problem.","authors":["William Sharpless","Zeyuan Feng","Somil Bansal","Sylvia Herbert"],"url":"https://arxiv.org/abs/2412.02033"}
{"created":"2025-04-29","title":"Stochastic LQR Design With Disturbance Preview","abstract":"This paper considers the discrete-time, stochastic LQR problem with $p$ steps of disturbance preview information where $p$ is finite. We first derive the solution for this problem on a finite horizon with linear, time-varying dynamics and time-varying costs. Next, we derive the solution on the infinite horizon with linear, time-invariant dynamics and time-invariant costs. Our proofs rely on the well-known principle of optimality. We provide an independent proof for the principle of optimality that relies only on nested information structure. Finally, we show that the finite preview controller converges to the optimal noncausal controller as the preview horizon $p$ tends to infinity. We also provide a simple example to illustrate both the finite and infinite horizon results.","authors":["Jietian Liu","Laurent Lessard","Peter Seiler"],"url":"https://arxiv.org/abs/2412.06662"}
{"created":"2025-04-29","title":"GeoConformal prediction: a model-agnostic framework of measuring the uncertainty of spatial prediction","abstract":"Spatial prediction is a fundamental task in geography. In recent years, with advances in geospatial artificial intelligence (GeoAI), numerous models have been developed to improve the accuracy of geographic variable predictions. Beyond achieving higher accuracy, it is equally important to obtain predictions with uncertainty measures to enhance model credibility and support responsible spatial prediction. Although geostatistic methods like Kriging offer some level of uncertainty assessment, such as Kriging variance, these measurements are not always accurate and lack general applicability to other spatial models. To address this issue, we propose a model-agnostic uncertainty assessment method called GeoConformal Prediction, which incorporates geographical weighting into conformal prediction. We applied it to two classic spatial prediction cases, spatial regression and spatial interpolation, to evaluate its reliability. First, in the spatial regression case, we used XGBoost to predict housing prices, followed by GeoConformal to calculate uncertainty. Our results show that GeoConformal achieved a coverage rate of 93.67%, while Bootstrap methods only reached a maximum coverage of 81.00% after 2000 runs. Next, we applied GeoConformal to spatial interpolation models. We found that the uncertainty obtained from GeoConformal aligned closely with the variance in Kriging. Finally, using GeoConformal, we analyzed the sources of uncertainty in spatial prediction. We found that explicitly including local features in AI models can significantly reduce prediction uncertainty, especially in areas with strong local dependence. Our findings suggest that GeoConformal holds potential not only for geographic knowledge discovery but also for guiding the design of future GeoAI models, paving the way for more reliable and interpretable spatial prediction frameworks.","authors":["Xiayin Lou","Peng Luo","Liqiu Meng"],"url":"https://arxiv.org/abs/2412.08661"}
{"created":"2025-04-29","title":"Self-Consistent Nested Diffusion Bridge for Accelerated MRI Reconstruction","abstract":"Accelerated MRI reconstruction plays a vital role in reducing scan time while preserving image quality. While most existing methods rely on complex-valued image-space or k-space data, these formats are often inaccessible in clinical practice due to proprietary reconstruction pipelines, leaving only magnitude images stored in DICOM files. To address this gap, we focus on the underexplored task of magnitude-image-based MRI reconstruction. Recent advancements in diffusion models, particularly denoising diffusion probabilistic models (DDPMs), have demonstrated strong capabilities in modeling image priors. However, their task-agnostic denoising nature limits performance in source-to-target image translation tasks, such as MRI reconstruction. In this work, we propose a novel Self-Consistent Nested Diffusion Bridge (SC-NDB) framework that models accelerated MRI reconstruction as a bi-directional image translation process between under-sampled and fully-sampled magnitude MRI images. SC-NDB introduces a nested diffusion architecture with a self-consistency constraint and reverse bridge diffusion pathways to improve intermediate prediction fidelity and better capture the explicit priors of source images. Furthermore, we incorporate a Contour Decomposition Embedding Module (CDEM) to inject structural and textural knowledge by leveraging Laplacian pyramids and directional filter banks. Extensive experiments on the fastMRI and IXI datasets demonstrate that our method achieves state-of-the-art performance compared to both magnitude-based and non-magnitude-based diffusion models, confirming the effectiveness and clinical relevance of SC-NDB.","authors":["Tao Song","Yicheng Wu","Minhao Hu","Xiangde Luo","Guoting Luo","Guotai Wang","Yi Guo","Feng Xu","Shaoting Zhang"],"url":"https://arxiv.org/abs/2412.09998"}
{"created":"2025-04-29","title":"From Point to probabilistic gradient boosting for claim frequency and severity prediction","abstract":"Gradient boosting for decision tree algorithms are increasingly used in actuarial applications as they show superior predictive performance over traditional generalised linear models. Many enhancements to the first gradient boosting machine algorithm exist. We present in a unified notation, and contrast, all the existing point and probabilistic gradient boosting for decision tree algorithms: GBM, XGBoost, DART, LightGBM, CatBoost, EGBM, PGBM, XGBoostLSS, cyclic GBM, and NGBoost. In this comprehensive numerical study, we compare their performance on five publicly available datasets for claim frequency and severity, of various sizes and comprising different numbers of (high cardinality) categorical variables. We explain how varying exposure-to-risk can be handled with boosting in frequency models. We compare the algorithms on the basis of computational efficiency, predictive performance, and model adequacy. LightGBM and XGBoostLSS win in terms of computational efficiency. CatBoost sometimes improves predictive performance, especially in the presence of high cardinality categorical variables, common in actuarial science. The fully interpretable EGBM achieves competitive predictive performance compared to the black box algorithms considered. We find that there is no trade-off between model adequacy and predictive accuracy: both are achievable simultaneously.","authors":["Dominik Chevalier","Marie-Pier C\\^ot\\'e"],"url":"https://arxiv.org/abs/2412.14916"}
{"created":"2025-04-29","title":"Improving Acoustic Scene Classification in Low-Resource Conditions","abstract":"Acoustic Scene Classification (ASC) identifies an environment based on an audio signal. This paper explores ASC in low-resource conditions and proposes a novel model, DS-FlexiNet, which combines depthwise separable convolutions from MobileNetV2 with ResNet-inspired residual connections for a balance of efficiency and accuracy. To address hardware limitations and device heterogeneity, DS-FlexiNet employs Quantization Aware Training (QAT) for model compression and data augmentation methods like Auto Device Impulse Response (ADIR) and Freq-MixStyle (FMS) to improve cross-device generalization. Knowledge Distillation (KD) from twelve teacher models further enhances performance on unseen devices. The architecture includes a custom Residual Normalization layer to handle domain differences across devices, and depthwise separable convolutions reduce computational overhead without sacrificing feature representation. Experimental results show that DS-FlexiNet excels in both adaptability and performance under resource-constrained conditions.","authors":["Zhi Chen","Yun-Fei Shao","Yong Ma","Mingsheng Wei","Le Zhang","Wei-Qiang Zhang"],"url":"https://arxiv.org/abs/2412.20722"}
{"created":"2025-04-29","title":"Automatic Double Reinforcement Learning in Semiparametric Markov Decision Processes with Applications to Long-Term Causal Inference","abstract":"Estimating long-term causal effects from short-term data is essential for decision-making in healthcare, economics, and industry, where long-term follow-up is often infeasible. Markov Decision Processes (MDPs) offer a principled framework for modeling outcomes as sequences of states, actions, and rewards over time. We introduce a semiparametric extension of Double Reinforcement Learning (DRL) for statistically efficient, model-robust inference on linear functionals of the Q-function, such as policy values, in infinite-horizon, time-homogeneous MDPs. By imposing semiparametric structure on the Q-function, our method relaxes the strong state overlap assumptions required by fully nonparametric approaches, improving efficiency and stability. To address computational and robustness challenges of minimax nuisance estimation, we develop a novel debiased plug-in estimator based on isotonic Bellman calibration, which integrates fitted Q-iteration with an isotonic regression step. This procedure leverages the Q-function as a data-driven dimension reduction, debiases all linear functionals of interest simultaneously, and enables nonparametric inference without explicit nuisance function estimation. Bellman calibration generalizes isotonic calibration to MDPs and may be of independent interest for prediction in reinforcement learning. Finally, we show that model selection for the Q-function incurs only second-order bias and extend the adaptive debiased machine learning (ADML) framework to MDPs for data-driven learning of semiparametric structure.","authors":["Lars van der Laan","David Hubbard","Allen Tran","Nathan Kallus","Aur\\'elien Bibaut"],"url":"https://arxiv.org/abs/2501.06926"}
{"created":"2025-04-29","title":"Perception-Guided EEG Analysis: A Deep Learning Approach Inspired by Level of Detail (LOD) Theory","abstract":"Objective: This study explores a novel deep learning approach for EEG analysis and perceptual state guidance, inspired by Level of Detail (LOD) theory. The goal is to improve perceptual state identification accuracy and advance personalized psychological therapy. Methods: Portable EEG devices and music rhythm signals were used for data collection. LOD theory was applied to dynamically adjust EEG signal processing, extracting core perceptual features. A Unity-based software system integrated EEG data with audio materials. The deep learning model combined a CNN for feature extraction and classification, and a DQN for reinforcement learning to optimize rhythm adjustments. Results: The CNN achieved 94.05% accuracy in perceptual state classification. The DQN guided subjects to target states with a 92.45% success rate, averaging 13.2 rhythm cycles. However, only 50% of users reported psychological alignment with the target state, indicating room for improvement. Discussion: The results validate the potential of LOD-based EEG biofeedback. Limitations include dataset source, label subjectivity, and reward function optimization. Future work will expand to diverse subjects, incorporate varied musical elements, and refine reward functions for better generalization and personalization.","authors":["BG Tong"],"url":"https://arxiv.org/abs/2501.10428"}
{"created":"2025-04-29","title":"Tight relations and equivalences between smooth relative entropies","abstract":"The precise one-shot characterisation of operational tasks in classical and quantum information theory relies on different forms of smooth entropic quantities. A particularly important connection is between the hypothesis testing relative entropy and the smoothed max-relative entropy, which together govern many operational settings. We first strengthen this connection into a type of equivalence: we show that the hypothesis testing relative entropy is equivalent to a variant of the smooth max-relative entropy based on the information spectrum divergence, which can be alternatively understood as a measured smooth max-relative entropy. Furthermore, we improve a fundamental lemma due to Datta and Renner that connects the different variants of the smoothed max-relative entropy, introducing a modified proof technique based on matrix geometric means and a tightened gentle measurement lemma. We use the unveiled connections and tools to strictly improve on previously known one-shot bounds and duality relations between the smooth max-relative entropy and the hypothesis testing relative entropy, sharpening also bounds that connect the max-relative entropy with R\\'enyi divergences.","authors":["Bartosz Regula","Ludovico Lami","Nilanjana Datta"],"url":"https://arxiv.org/abs/2501.12447"}
{"created":"2025-04-29","title":"Efficient Mitigation of Error Floors in Quantum Error Correction using Non-Binary Low-Density Parity-Check Codes","abstract":"In this paper, we propose an efficient method to reduce error floors in quantum error correction using non-binary low-density parity-check (LDPC) codes. We identify and classify cycle structures in the parity-check matrix where estimated noise becomes trapped, and develop tailored decoding methods for each cycle type. For Type-I cycles, we propose a method to make the difference between estimated and true noise degenerate. Type-II cycles are shown to be uncorrectable, while for Type-III cycles, we utilize the fact that cycles in non-binary LDPC codes do not necessarily correspond to codewords, allowing us to estimate the true noise. Our method significantly improves decoding performance and reduces error floors.","authors":["Kenta Kasai"],"url":"https://arxiv.org/abs/2501.13923"}
{"created":"2025-04-29","title":"Investigating the Feasibility of Patch-based Inference for Generalized Diffusion Priors in Inverse Problems for Medical Images","abstract":"Plug-and-play approaches to solving inverse problems such as restoration and super-resolution have recently benefited from Diffusion-based generative priors for natural as well as medical images. However, solutions often use the standard albeit computationally intensive route of training and inferring with the whole image on the diffusion prior. While patch-based approaches to evaluating diffusion priors in plug-and-play methods have received some interest, they remain an open area of study. In this work, we explore the feasibility of the usage of patches for training and inference of a diffusion prior on MRI images. We explore the minor adaptation necessary for artifact avoidance, the performance and the efficiency of memory usage of patch-based methods as well as the adaptability of whole image training to patch-based evaluation - evaluating across multiple plug-and-play methods, tasks and datasets.","authors":["Saikat Roy","Mahmoud Mostapha","Radu Miron","Matt Holbrook","Mariappan Nadar"],"url":"https://arxiv.org/abs/2501.15309"}
{"created":"2025-04-29","title":"Generalizing Egocentric Temporal Neighborhoods to probe for spatial correlations in temporal networks and infer their topology","abstract":"Motifs are thought to be some fundamental components of social face-to-face interaction temporal networks. However, the motifs previously considered are either limited to a handful of nodes and edges, or do not include triangles, which are thought to be of critical relevance to understand the dynamics of social systems. Thus, we introduce a new class of motifs, that include these triangles, are not limited in their number of nodes or edges, and yet can be mined efficiently in any temporal network. Referring to these motifs as the edge-centered motifs, we show analytically how they subsume the Egocentric Temporal Neighborhoods motifs of the literature. We also confirm in empirical data that the edge-centered motifs bring relevant information with respect to the Egocentric motifs by using a principle of maximum entropy. Then, we show how mining for the edge-centered motifs in a network can be used to probe for spatial correlations in the underlying dynamics that have produced that network. We deduce an approximate formula for the distribution of the edge-centered motifs in empirical networks of social face-to-face interactions. In the last section of this paper, we explore how the statistics of the edge-centered motifs can be used to infer the complete topology of the network they were sampled from. This leads to the needs of mathematical development, that we inaugurate here under the name of graph tiling theory.","authors":["Didier Le Bail"],"url":"https://arxiv.org/abs/2501.16070"}
{"created":"2025-04-29","title":"Representation Number of Word-Representable Split Graphs","abstract":"A split graph is a graph whose vertex set can be partitioned into a clique and an independent set. The word-representability of split graphs was studied in a series of papers in the literature, and the class of word-representable split graphs was characterized through semi-transitive orientation. Nonetheless, the representation number of this class of graphs is still not known. In general, determining the representation number of a word-representable graph is an NP-complete problem. In this work, through an algorithmic procedure, we show that the representation number of the class of word-representable split graphs is at most three. Further, we characterize the class of word-representable split graphs as well as the class of split comparability graphs which have representation number exactly three.","authors":["Tithi Dwary","Khyodeno Mozhui","K. V. Krishna"],"url":"https://arxiv.org/abs/2502.00872"}
{"created":"2025-04-29","title":"On the Surprising Robustness of Sequential Convex Optimization for Contact-Implicit Motion Planning","abstract":"Contact-implicit motion planning-embedding contact sequencing as implicit complementarity constraints-holds the promise of leveraging continuous optimization to discover new contact patterns online. Nevertheless, the resulting optimization, being an instance of Mathematical Programming with Complementary Constraints, fails the classical constraint qualifications that are crucial for the convergence of popular numerical solvers. We present robust contact-implicit motion planning with sequential convex programming (CRISP), a solver that departs from the usual primal-dual algorithmic framework but instead only focuses on the primal problem. CRISP solves a convex quadratic program with an adaptive trust region radius at each iteration, and its convergence is evaluated by a merit function using weighted penalty. We (i) provide sufficient conditions on CRISP's convergence to first-order stationary points of the merit function; (ii) release a high-performance C++ implementation of CRISP with a generic nonlinear programming interface; and (iii) demonstrate CRISP's surprising robustness in solving contact-implicit planning with naive initialization. In fact, CRISP solves several contact-implicit problems with all-zero initialization.","authors":["Yulin Li","Haoyu Han","Shucheng Kang","Jun Ma","Heng Yang"],"url":"https://arxiv.org/abs/2502.01055"}
{"created":"2025-04-29","title":"Quantum Circuit Design using a Progressive Widening Enhanced Monte Carlo Tree Search","abstract":"The performance of Variational Quantum Algorithms (VQAs) strongly depends on the choice of the parameterized quantum circuit to optimize. One of the biggest challenges in VQAs is designing quantum circuits tailored to the particular problem. This article proposes a gradient-free Monte Carlo Tree Search (MCTS) technique to automate the process of quantum circuit design. Our proposed technique introduces a novel formulation of the action space based on a sampling scheme and a progressive widening technique to explore the space dynamically. When testing our MCTS approach on the domain of random quantum circuits, MCTS approximates unstructured circuits under different values of stabilizer R\\'enyi entropy. It turns out that MCTS manages to approximate the benchmark quantum states independently from their degree of nonstabilizerness. Next, our technique exhibits robustness across various application domains, including quantum chemistry and systems of linear equations. Compared to previous MCTS research, our technique reduces the number of quantum circuit evaluations by a factor of 10 up to 100 while achieving equal or better results. In addition, the resulting quantum circuits exhibit up to three times fewer CNOT gates, which is important for implementation on noisy quantum hardware.","authors":["Vincenzo Lipardi","Domenica Dibenedetto","Georgios Stamoulis","Mark H. M. Winands"],"url":"https://arxiv.org/abs/2502.03962"}
{"created":"2025-04-29","title":"A Meta-learner for Heterogeneous Effects in Difference-in-Differences","abstract":"We address the problem of estimating heterogeneous treatment effects in panel data, adopting the popular Difference-in-Differences (DiD) framework under the conditional parallel trends assumption. We propose a novel doubly robust meta-learner for the Conditional Average Treatment Effect on the Treated (CATT), reducing the estimation to a convex risk minimization problem involving a set of auxiliary models. Our framework allows for the flexible estimation of the CATT, when conditioning on any subset of variables of interest using generic machine learning. Leveraging Neyman orthogonality, our proposed approach is robust to estimation errors in the auxiliary models. As a generalization to our main result, we develop a meta-learning approach for the estimation of general conditional functionals under covariate shift. We also provide an extension to the instrumented DiD setting with non-compliance. Empirical results demonstrate the superiority of our approach over existing baselines.","authors":["Hui Lan","Haoge Chang","Eleanor Dillon","Vasilis Syrgkanis"],"url":"https://arxiv.org/abs/2502.04699"}
{"created":"2025-04-29","title":"Flat-Top Beamforming with Efficient Array-Fed RIS","abstract":"Flat-top beam designs are essential for uniform power distribution over a wide angular sector for applications such as 5G/6G networks, beaconing, satellite communications, radar systems, etc. Low sidelobe levels with steep transitions allow negligible cross sector illumination. Active array designs requiring amplitude taper suffer from poor power amplifier utilization. Phase only designs, e.g., Zadoff-Chu or generalized step chirp polyphase sequence methods, often require large active antenna arrays which in turns increases the hardware complexity and reduces the energy efficiency. In our recently proposed novel array-fed reflective intelligent surface (RIS) architecture, the small ($2 \\times 2$) active array has uniform (principal eigenmode) amplitude weighting. We now present a pragmatic flat-top pattern design method for practical array (RIS) sizes, which outperforms current state-of-the-art in terms of design superiority, energy efficiency, and deployment feasibility. This novel design holds promise for advancing sustainable wireless technologies in next-generation communication systems, including applications such as beaconing, broadcast signaling, and hierarchical beamforming, while mitigating the environmental impact of high-energy antenna arrays.","authors":["Krishan Kumar Tiwari","Giuseppe Caire"],"url":"https://arxiv.org/abs/2502.08490"}
{"created":"2025-04-29","title":"Low-Rank Thinning","abstract":"The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.","authors":["Annabelle Michael Carrell","Albert Gong","Abhishek Shetty","Raaz Dwivedi","Lester Mackey"],"url":"https://arxiv.org/abs/2502.12063"}
{"created":"2025-04-29","title":"MVCNet: Multi-View Contrastive Network for Motor Imagery Classification","abstract":"Electroencephalography (EEG)-based brain-computer interfaces (BCIs) enable neural interaction by decoding brain activity for external communication. Motor imagery (MI) decoding has received significant attention due to its intuitive mechanism. However, most existing models rely on single-stream architectures and overlook the multi-view nature of EEG signals, leading to limited performance and generalization. We propose a multi-view contrastive network (MVCNet), a dual-branch architecture that parallelly integrates CNN and Transformer models to capture both local spatial-temporal features and global temporal dependencies. To enhance the informativeness of training data, MVCNet incorporates a unified augmentation pipeline across time, frequency, and spatial domains. Two contrastive modules are further introduced: a cross-view contrastive module that enforces consistency of original and augmented views, and a cross-model contrastive module that aligns features extracted from both branches. Final representations are fused and jointly optimized by contrastive and classification losses. Experiments on five public MI datasets across three scenarios demonstrate that MVCNet consistently outperforms seven state-of-the-art MI decoding networks, highlighting its effectiveness and generalization ability. MVCNet provides a robust solution for MI decoding by integrating multi-view information and dual-branch modeling, contributing to the development of more reliable BCI systems.","authors":["Ziwei Wang","Siyang Li","Xiaoqing Chen","Wei Li","Dongrui Wu"],"url":"https://arxiv.org/abs/2502.17482"}
{"created":"2025-04-29","title":"Detecting Long QT Syndrome and First-Degree Atrioventricular Block using Single-Lead AI-ECG: A Multi-Center Real-World Study","abstract":"Home-based single-lead AI-ECG devices have enabled continuous, real-world cardiac monitoring. However, the accuracy of parameter calculations from single-lead AI-ECG algorithm remains to be fully validated, which is critical for conditions such as Long QT Syndrome (LQTS) and First-Degree Atrioventricular Block (AVBI). In this multicenter study, we assessed FeatureDB, an ECG measurements computation algorithm, in the context of single-lead monitoring using three annotated datasets: PTB-XL+ (n=21,354), CSE (n=105), and HeartVoice-ECG-lite (n=369). FeatureDB showed strong correlation with standard ECG machines (12SL and Uni-G) in key measurements (PR, QRS, QT, QTc), and high agreement confirmed by Bland-Altman analysis. In detecting LQTS (AUC=0.786) and AVBI (AUC=0.684), FeatureDB demonstrated diagnostic performance comparable to commercial ECG systems (12SL: 0.859/0.716; Uni-G: 0.817/0.605), significantly outperforming ECGDeli (0.501/0.569). Notably, FeatureDB can operate locally on resource-limited devices, facilitating use in low-connectivity settings. These findings confirm the clinical reliability of FeatureDB for single-lead ECG diagnostics and highlight its potential to bridge traditional ECG diagnostics with wearable technology for scalable cardiovascular monitoring and early intervention.","authors":["Sumei Fan","Deyun Zhang","Yue Wang","Shijia Geng","Kun Lu","Meng Sang","Weilun Xu","Haixue Wang","Qinghao Zhao","Chuandong Cheng","Peng Wang","Shenda Hong"],"url":"https://arxiv.org/abs/2502.17499"}
{"created":"2025-04-29","title":"Distributed and Localized Covariance Control of Coupled Systems: A System Level Approach","abstract":"This work is concerned with the finite-horizon optimal covariance steering of networked systems governed by discrete-time stochastic linear dynamics. In contrast with existing work that has only considered systems with dynamically decoupled agents, we consider a dynamically coupled system composed of interconnected subsystems subject to local communication constraints. In particular, we propose a distributed algorithm to compute the localized optimal feedback control policy for each individual subsystem, which depends only on the local state histories of its neighboring subsystems. Utilizing the system-level synthesis (SLS) framework, we first recast the localized covariance steering problem as a convex SLS problem with locality constraints. Subsequently, exploiting its partially separable structure, we decompose the latter problem into smaller subproblems, introducing a transformation to deal with nonseparable instances. Finally, we employ a variation of the consensus alternating direction method of multipliers (ADMM) to distribute computation across subsystems on account of their local information and communication constraints. We demonstrate the effectiveness of our proposed algorithm on a power system with 36 interconnected subsystems.","authors":["Ahmed Khalil","Yoonjae Lee","Efstathios Bakolas"],"url":"https://arxiv.org/abs/2503.02094"}
{"created":"2025-04-29","title":"Quantum-Enhanced LLM Efficient Fine Tuning","abstract":"Low-Rank Adaptation (LoRA) enables efficient fine-tuning of pre-trained language models through low-rank matrix approximation, achieving effectiveness in many scenarios. However, its representation capacity is constrained in complex tasks or high-rank dependency settings, potentially limiting model adaptability. To overcome the expressive bottleneck in classical low-rank approximation for fine-tuning large language models (LLMs), we propose Quantum Tensor Hybrid Adaptation (QTHA), a parameter-efficient fine-tuning method that integrates a quantum neural network (QNN) with a tensor network. QTHA explores quantum tensor hybrid fine-tuning within low-rank spaces by decomposing pre-trained weights into quantum neural network and tensor network representations, leveraging quantum state superposition to overcome classical rank limitations. Experiments demonstrate that QTHA achieves performance comparable to or surpassing LoRA in parameter-efficient fine-tuning. Compared to LoRA, QTHA reduces trainable parameters by 76% while reducing training loss by up to 17% and improving test set performance by up to 17% within the same training steps. This research not only enables lightweight adaptation of quantum resources to the billion-parameter models but also validates the feasibility of quantum hardware optimization driven by LLM tasks. It establishes the first engineering-ready foundation for future quantum-enhanced Artificial General Intelligence (AGI) systems.","authors":["Xiaofei Kong","Lei Li","Zhaoyun Chen","Cheng Xue","Xiaofan Xu","Huanyu Liu","Yuchun Wu","Yuan Fang","Han Fang","Kejiang Chen","Yang Yang","Menghan Dou","Guoping Guo"],"url":"https://arxiv.org/abs/2503.12790"}
{"created":"2025-04-29","title":"Conditional Electrocardiogram Generation Using Hierarchical Variational Autoencoders","abstract":"Cardiovascular diseases (CVDs) are disorders impacting the heart and circulatory system. These disorders are the foremost and continuously escalating cause of mortality worldwide. One of the main tasks when working with CVDs is analyzing and identifying pathologies on a 12-lead electrocardiogram (ECG) with a standard 10-second duration. Using machine learning (ML) in automatic ECG analysis increases CVD diagnostics' availability, speed, and accuracy. However, the most significant difficulty in developing ML models is obtaining a sufficient training dataset. Due to the limitations of medical data usage, such as expensiveness, errors, the ambiguity of labels, imbalance of classes, and privacy issues, utilizing synthetic samples depending on specific pathologies bypasses these restrictions and improves algorithm quality. Existing solutions for the conditional generation of ECG signals are mainly built on Generative Adversarial Networks (GANs), and only a few papers consider the architectures based on Variational Autoencoders (VAEs), showing comparable results in recent works. This paper proposes the publicly available conditional Nouveau VAE model for ECG signal generation (cNVAE-ECG), which produces high-resolution ECGs with multiple pathologies. We provide an extensive comparison of the proposed model on various practical downstream tasks, including transfer learning scenarios showing an area under the receiver operating characteristic (AUROC) increase up to 2% surpassing GAN-like competitors.","authors":["Ivan Sviridov","Konstantin Egorov"],"url":"https://arxiv.org/abs/2503.13469"}
{"created":"2025-04-29","title":"Survival Analysis with Machine Learning for Predicting Li-ion Battery Remaining Useful Life","abstract":"Battery degradation significantly impacts the reliability and efficiency of energy storage systems, particularly in electric vehicles and industrial applications. Predicting the remaining useful life (RUL) of lithium-ion batteries is crucial for optimizing maintenance schedules, reducing costs, and improving safety. Traditional RUL prediction methods often struggle with nonlinear degradation patterns and uncertainty quantification. To address these challenges, we propose a hybrid survival analysis framework integrating survival data reconstruction, survival model learning, and survival probability estimation. Our approach transforms battery voltage time series into time-to-failure data using path signatures. The multiple Cox-based survival models and machine-learning-based methods, such as DeepHit and MTLR, are learned to predict battery failure-free probabilities over time. Experiments conducted on the Toyota battery and NASA battery datasets demonstrate the effectiveness of our approach, achieving high time-dependent AUC and concordance index (C-Index) while maintaining a low integrated Brier score. The data and source codes for this work are available to the public at https://github.com/thinkxca/rul.","authors":["Jingyuan Xue","Longfei Wei","Fang Sheng","Jianfei Zhang"],"url":"https://arxiv.org/abs/2503.13558"}
{"created":"2025-04-29","title":"Performance Evaluation of Variational Quantum Eigensolver and Quantum Dynamics Algorithms on the Advection-Diffusion Equation","abstract":"We investigate the potential of near-term quantum algorithms for solving partial differential equations (PDEs), focusing on a linear one-dimensional advection-diffusion equation as a test case. This study benchmarks a ground-state algorithm, Variational Quantum Eigensolver (VQE), against three leading quantum dynamics algorithms, Trotterization, Variational Quantum Imaginary Time Evolution (VarQTE), and Adaptive Variational Quantum Dynamics Simulation (AVQDS), applied to the same PDE on small quantum hardware. While Trotterization is fully quantum, VarQTE and AVQDS are variational algorithms that reduce circuit depth for noisy intermediate-scale quantum (NISQ) devices. However, hardware results from these dynamics methods show sizable errors due to noise and limited shot statistics. To establish a noise-free performance baseline, we implement the VQE-based solver on a noiseless statevector simulator. Our results show VQE can reach final-time infidelities as low as ${O}(10^{-9})$ with $N=4$ qubits and moderate circuit depths, outperforming hardware-deployed dynamics methods that show infidelities $\\gtrsim 10^{-1}$. By comparing noiseless VQE to shot-based and hardware-run algorithms, we assess their accuracy and resource demands, providing a baseline for future quantum PDE solvers. We conclude with a discussion of limitations and potential extensions to higher-dimensional, nonlinear PDEs relevant to engineering and finance.","authors":["A. Bar{\\i}\\c{s} \\\"Ozg\\\"uler"],"url":"https://arxiv.org/abs/2503.24045"}
{"created":"2025-04-29","title":"Vertex-Based Localization of Tur\\'{a}n's Theorem","abstract":"For a simple graph $G$, let $n$ and $m$ denote the number of vertices and edges in $G$, respectively. Tur\\'{a}n's theorem states that in a simple $K_{r+1}$ free graph, $m \\leq \\frac{n^2(r-1)}{2r}$. In this paper, we generalize this result as follows: For each $v \\in V(G)$, let $c(v)$ be the order of the largest clique that contains $v$. We show that \\[ m \\leq \\frac{n}{2}\\sum_{v\\in V(G)}\\frac{c(v)-1}{c(v)}\\] Furthermore, we characterize the class of extremal graphs that attain equality in this bound.","authors":["Rajat Adak (Indian Institute of Science","Bangalore)","L. Sunil Chandran (Indian Institute of Science","Bangalore)"],"url":"https://arxiv.org/abs/2504.02806"}
{"created":"2025-04-29","title":"Traveling wave profiles for a semi-discrete Burgers equation","abstract":"We look for traveling waves of the semi-discrete conservation law $4\\dot u_j +u_{j+1}^2-u_{j-1}^2 = 0$, using variational principles related to concepts of ``hidden convexity'' appearing in recent studies of various PDE (partial differential equations). We analyze and numerically compute with two variational formulations related to dual convex optimization problems constrained by either the differential-difference equation (DDE) or nonlinear integral equation (NIE) that wave profiles should satisfy. We prove existence theorems conditional on the existence of extrema that satisfy a strict convexity criterion, and numerically exhibit a variety of localized, periodic and non-periodic wave phenomena.","authors":["Uditnarayan Kouskiya","Robert L. Pego","Amit Acharya"],"url":"https://arxiv.org/abs/2504.12171"}
{"created":"2025-04-29","title":"Set families: restricted distances via restricted intersections","abstract":"Denote by $f_D(n)$ the maximum size of a set family $\\mathcal{F}$ on $[n] = \\{1, \\dots, n\\}$ with distance set $D$. That is, $|A \\bigtriangleup B| \\in D$ holds for every pair of distinct sets $A, B \\in \\mathcal{F}$. Kleitman's celebrated discrete isodiametric inequality states that $f_D(n)$ is maximized at Hamming balls of radius $d/2$ when $D = \\{1, \\dots, d\\}$. We study the generalization where $D$ is a set of arithmetic progression and determine $f_D(n)$ asymptotically for all homogeneous $D$. In the special case when $D$ is an interval, our result confirms a conjecture of Huang, Klurman, and Pohoata. Moreover, we demonstrate a dichotomy in the growth of $f_D(n)$, showing linear growth in $n$ when $D$ is a non-homogeneous arithmetic progression. Different from previous combinatorial and spectral approaches, we deduce our results by converting the restricted distance problems to restricted intersection problems.","authors":["Zichao Dong","Jun Gao","Hong Liu","Minghui Ouyang","Qiang Zhou"],"url":"https://arxiv.org/abs/2504.12296"}
{"created":"2025-04-29","title":"Addressing the Minor-Embedding Problem in Quantum Annealing and Evaluating State-of-the-Art Algorithm Performance","abstract":"This study addresses the minor-embedding problem, which involves mapping the variables of an Ising model onto a quantum annealing processor. The primary motivation stems from the observed performance disparity of quantum annealers when solving problems suited to the processor's architecture versus those with non-hardware-native topologies. Our research has two main objectives: i) to analyze the impact of embedding quality on the performance of D-Wave Systems quantum annealers, and ii) to evaluate the quality of the embeddings generated by Minorminer, an algorithm provided by D-Wave and widely recognized as the standard minor-embedding technique in the literature. Regarding the first objective, our experiments reveal a clear correlation between the average chain length of embeddings and the relative errors of the solutions sampled. This underscores the critical influence of embedding quality on quantum annealing performance. For the second objective, we focus on the Minorminer technique, assessing its capacity to embed problems, the quality of the embeddings produced, and the robustness of the results. We also compare its performance with Clique Embedding, another algorithm developed by D-Wave, which is deterministic and designed to embed fully connected Ising models into quantum annealing processors, serving as a worst-case scenario. The results demonstrate that there is significant room for improvement for Minorminer, as it has not consistently outperformed the worst-case scenario.","authors":["Aitor Gomez-Tejedor","Eneko Osaba","Esther Villar-Rodriguez"],"url":"https://arxiv.org/abs/2504.13376"}
{"created":"2025-04-29","title":"Adaptive Non-local Observable on Quantum Neural Networks","abstract":"Conventional Variational Quantum Circuits (VQCs) for Quantum Machine Learning typically rely on a fixed Hermitian observable, often built from Pauli operators. Inspired by the Heisenberg picture, we propose an adaptive non-local measurement framework that substantially increases the model complexity of the quantum circuits. Our introduction of dynamical Hermitian observables with evolving parameters shows that optimizing VQC rotations corresponds to tracing a trajectory in the observable space. This viewpoint reveals that standard VQCs are merely a special case of the Heisenberg representation.","authors":["Hsin-Yi Lin","Huan-Hsin Tseng","Samuel Yen-Chi Chen","Shinjae Yoo"],"url":"https://arxiv.org/abs/2504.13414"}
{"created":"2025-04-29","title":"Ordered Yao graphs: maximum degree, edge numbers, and clique numbers","abstract":"For a positive integer $k$ and an ordered set of $n$ points in the plane, define its k-sector ordered Yao graphs as follows. Divide the plane around each point into $k$ equal sectors and draw an edge from each point to its closest predecessor in each of the $k$ sectors. We analyze several natural parameters of these graphs. Our main results are as follows:","authors":["P\\'eter \\'Agoston","Adrian Dumitrescu","Arsenii Sagdeev","Karamjeet Singh","Ji Zeng"],"url":"https://arxiv.org/abs/2504.13819"}
{"created":"2025-04-29","title":"PINN-MEP: Continuous Neural Representations for Minimum-Energy Path Discovery in Molecular Systems","abstract":"Characterizing conformational transitions in physical systems remains a fundamental challenge in the computational sciences. Traditional sampling methods like molecular dynamics (MD) or MCMC often struggle with the high-dimensional nature of molecular systems and the high energy barriers of transitions between stable states. While these transitions are rare events in simulation timescales, they often represent the most biologically significant processes - for example, the conformational change of an ion channel protein from its closed to open state, which controls cellular ion flow and is crucial for neural signaling. Such transitions in real systems may take milliseconds to seconds but could require months or years of continuous simulation to observe even once. We present a method that reformulates transition path generation as a continuous optimization problem solved through physics-informed neural networks (PINNs) inspired by string methods for minimum-energy path (MEP) generation. By representing transition paths as implicit neural functions and leveraging automatic differentiation with differentiable molecular dynamics force fields, our method enables the efficient discovery of physically realistic transition pathways without requiring expensive path sampling. We demonstrate our method's effectiveness on two proteins, including an explicitly hydrated bovine pancreatic trypsin inhibitor (BPTI) system with over 8,300 atoms.","authors":["Magnus Petersen","Roberto Covino"],"url":"https://arxiv.org/abs/2504.16381"}
{"created":"2025-04-29","title":"Likelihood-Free Variational Autoencoders","abstract":"Variational Autoencoders (VAEs) typically rely on a probabilistic decoder with a predefined likelihood, most commonly an isotropic Gaussian, to model the data conditional on latent variables. While convenient for optimization, this choice often leads to likelihood misspecification, resulting in blurry reconstructions and poor data fidelity, especially for high-dimensional data such as images. In this work, we propose EnVAE, a novel likelihood-free generative framework that has a deterministic decoder and employs the energy score--a proper scoring rule--to build the reconstruction loss. This enables likelihood-free inference without requiring explicit parametric density functions. To address the computational inefficiency of the energy score, we introduce a fast variant, FEnVAE, based on the local smoothness of the decoder and the sharpness of the posterior distribution of latent variables. This yields an efficient single-sample training objective that integrates seamlessly into existing VAE pipelines with minimal overhead. Empirical results on standard benchmarks demonstrate that EnVAE achieves superior reconstruction and generation quality compared to likelihood-based baselines. Our framework offers a general, scalable, and statistically principled alternative for flexible and nonparametric distribution learning in generative modeling.","authors":["Chen Xu","Qiang Wang","Lijun Sun"],"url":"https://arxiv.org/abs/2504.17622"}
{"created":"2025-04-29","title":"UNILoc: Unified Localization Combining Model-Based Geometry and Unsupervised Learning","abstract":"Accurate mobile device localization is critical for emerging 5G/6G applications such as autonomous vehicles and augmented reality. In this paper, we propose a unified localization method that integrates model-based and machine learning (ML)-based methods to reap their respective advantages by exploiting available map information. In order to avoid supervised learning, we generate training labels automatically via optimal transport (OT) by fusing geometric estimates with building layouts. Ray-tracing based simulations are carried out to demonstrate that the proposed method significantly improves positioning accuracy for both line-of-sight (LoS) users (compared to ML-based methods) and non-line-of-sight (NLoS) users (compared to model-based methods). Remarkably, the unified method is able to achieve competitive overall performance with the fully-supervised fingerprinting, while eliminating the need for cumbersome labeled data measurement and collection.","authors":["Yuhao Zhang","Guangjin Pan","Musa Furkan Keskin","Ossi Kaltiokallio","Mikko Valkama","Henk Wymeersch"],"url":"https://arxiv.org/abs/2504.17676"}
{"created":"2025-04-29","title":"Quantum Error Correction with Girth-16 Non-Binary LDPC Codes via Affine Permutation Construction","abstract":"We propose a method for constructing quantum error-correcting codes based on non-binary low-density parity-check codes with Tanner graph girth 16. While conventional constructions using circulant permutation matrices are limited to girth 12, our method employs affine permutation matrices and a randomized sequential selection procedure to eliminate short cycles and achieve girth 16.","authors":["Kenta Kasai"],"url":"https://arxiv.org/abs/2504.17790"}
