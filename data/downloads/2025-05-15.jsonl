{"created":"2025-05-15","title":"Post-Quantum Cryptography: An Analysis of Code-Based and Lattice-Based Cryptosystems","abstract":"Most modern cryptographic systems, such as RSA and the Diffie-Hellman Key Exchange, rely on \"trapdoor\" mathematical functions that are presumed to be computationally difficult with existing tools. However, quantum computers will be able to break these systems using Shor's Algorithm, necessitating the development of quantum-resistant alternatives. We first examine the McEliece cryptosystem, a code-based scheme believed to be secure against quantum attacks due to the hardness of decoding arbitrary linear codes. We then explore NTRU, a lattice-based system grounded in the difficulty of solving the Shortest Vector Problem. Finally, we establish connections between the structural foundations and security of the two systems.","authors":["Alexander Meyer"],"url":"https://arxiv.org/abs/2505.08791"}
{"created":"2025-05-15","title":"A Preliminary Framework for Intersectionality in ML Pipelines","abstract":"Machine learning (ML) has become a go-to solution for improving how we use, experience, and interact with technology (and the world around us). Unfortunately, studies have repeatedly shown that machine learning technologies may not provide adequate support for societal identities and experiences. Intersectionality is a sociological framework that provides a mechanism for explicitly considering complex social identities, focusing on social justice and power. While the framework of intersectionality can support the development of technologies that acknowledge and support all members of society, it has been adopted and adapted in ways that are not always true to its foundations, thereby weakening its potential for impact. To support the appropriate adoption and use of intersectionality for more equitable technological outcomes, we amplify the foundational intersectionality scholarship--Crenshaw, Combahee, and Collins (three C's), to create a socially relevant preliminary framework in developing machine-learning solutions. We use this framework to evaluate and report on the (mis)alignments of intersectionality application in machine learning literature.","authors":["Michelle Nashla Turcios","Alicia E. Boyd","Angela D. R. Smith","Brittany Johnson"],"url":"https://arxiv.org/abs/2505.08792"}
{"created":"2025-05-15","title":"Onboard Optimization and Learning: A Survey","abstract":"Onboard learning is a transformative approach in edge AI, enabling real-time data processing, decision-making, and adaptive model training directly on resource-constrained devices without relying on centralized servers. This paradigm is crucial for applications demanding low latency, enhanced privacy, and energy efficiency. However, onboard learning faces challenges such as limited computational resources, high inference costs, and security vulnerabilities. This survey explores a comprehensive range of methodologies that address these challenges, focusing on techniques that optimize model efficiency, accelerate inference, and support collaborative learning across distributed devices. Approaches for reducing model complexity, improving inference speed, and ensuring privacy-preserving computation are examined alongside emerging strategies that enhance scalability and adaptability in dynamic environments. By bridging advancements in hardware-software co-design, model compression, and decentralized learning, this survey provides insights into the current state of onboard learning to enable robust, efficient, and secure AI deployment at the edge.","authors":["Monirul Islam Pavel","Siyi Hu","Mahardhika Pratama","Ryszard Kowalczyk"],"url":"https://arxiv.org/abs/2505.08793"}
{"created":"2025-05-15","title":"The Geometry of Meaning: Perfect Spacetime Representations of Hierarchical Structures","abstract":"We show that there is a fast algorithm that embeds hierarchical structures in three-dimensional Minkowski spacetime. The correlation of data ends up purely encoded in the causal structure. Our model relies solely on oriented token pairs -- local hierarchical signals -- with no access to global symbolic structure. We apply our method to the corpus of \\textit{WordNet}. We provide a perfect embedding of the mammal sub-tree including ambiguities (more than one hierarchy per node) in such a way that the hierarchical structures get completely codified in the geometry and exactly reproduce the ground-truth. We extend this to a perfect embedding of the maximal unambiguous subset of the \\textit{WordNet} with 82{,}115 noun tokens and a single hierarchy per token. We introduce a novel retrieval mechanism in which causality, not distance, governs hierarchical access. Our results seem to indicate that all discrete data has a perfect geometrical representation that is three-dimensional. The resulting embeddings are nearly conformally invariant, indicating deep connections with general relativity and field theory. These results suggest that concepts, categories, and their interrelations, namely hierarchical meaning itself, is geometric.","authors":["Andres Anabalon","Hugo Garces","Julio Oliva","Jose Cifuentes"],"url":"https://arxiv.org/abs/2505.08795"}
{"created":"2025-05-15","title":"Visibility and Influence in Digital Social Relations: Towards a New Symbolic Capital?","abstract":"This study explores the dynamics of visibility and influence in digital social relations, examining their implications for the emergence of a new symbolic capital. Using a mixedmethods design, the research combined semi-structured interviews with 20 digitally active individuals and quantitative social media data analysis to identify key predictors of digital symbolic capital. Findings reveal that visibility is influenced by content quality, network size, and engagement strategies, while influence depends on credibility, authority, and trust. The study identifies a new form of symbolic capital based on online visibility, influence, and reputation, distinct from traditional forms. The research discusses the ethical implications of these dynamics and suggests future research directions, emphasizing the need to update social theories to account for digital transformations.","authors":["F. Annaki","S. Ouassou","S. Igamane"],"url":"https://arxiv.org/abs/2505.08797"}
{"created":"2025-05-15","title":"Measuring Security in 5G and Future Networks","abstract":"In today's increasingly interconnected and fast-paced digital ecosystem, mobile networks, such as 5G and future generations such as 6G, play a pivotal role and must be considered as critical infrastructures. Ensuring their security is paramount to safeguard both individual users and the industries that depend on these networks. An essential condition for maintaining and improving the security posture of a system is the ability to effectively measure and monitor its security state. In this work we address the need for an objective measurement of the security state of 5G and future networks. We introduce a state machine model designed to capture the security life cycle of network functions and the transitions between different states within the life cycle. Such a model can be computed locally at each node, or hierarchically, by aggregating measurements into security domains or the whole network. We identify three essential security metrics -- attack surface exposure, impact of system vulnerabilities, and effectiveness of applied security controls -- that collectively form the basis for calculating the overall security score. With this approach, it is possible to provide a holistic understanding of the security posture, laying the foundation for effective security management in the expected dynamic threat landscape of 6G networks. Through practical examples, we illustrate the real-world application of our proposed methodology, offering valuable insights for developing risk management and informed decision-making strategies in 5G and 6G security operations and laying the foundation for effective security management in the expected dynamic threat landscape of 6G networks.","authors":["Loay Abdelrazek","Rim ElMalki","Filippo Rebecchi","Daniel Cho"],"url":"https://arxiv.org/abs/2505.08799"}
{"created":"2025-05-15","title":"Graph-based Online Monitoring of Train Driver States via Facial and Skeletal Features","abstract":"Driver fatigue poses a significant challenge to railway safety, with traditional systems like the dead-man switch offering limited and basic alertness checks. This study presents an online behavior-based monitoring system utilizing a customised Directed-Graph Neural Network (DGNN) to classify train driver's states into three categories: alert, not alert, and pathological. To optimize input representations for the model, an ablation study was performed, comparing three feature configurations: skeletal-only, facial-only, and a combination of both. Experimental results show that combining facial and skeletal features yields the highest accuracy (80.88%) in the three-class model, outperforming models using only facial or skeletal features. Furthermore, this combination achieves over 99% accuracy in the binary alertness classification. Additionally, we introduced a novel dataset that, for the first time, incorporates simulated pathological conditions into train driver monitoring, broadening the scope for assessing risks related to fatigue and health. This work represents a step forward in enhancing railway safety through advanced online monitoring using vision-based technologies.","authors":["Olivia Nocentini","Marta Lagomarsino","Gokhan Solak","Younggeol Cho","Qiyi Tong","Marta Lorenzini","Arash Ajoudani"],"url":"https://arxiv.org/abs/2505.08800"}
{"created":"2025-05-15","title":"OptiGait-LGBM: An Efficient Approach of Gait-based Person Re-identification in Non-Overlapping Regions","abstract":"Gait recognition, known for its ability to identify individuals from a distance, has gained significant attention in recent times due to its non-intrusive verification. While video-based gait identification systems perform well on large public datasets, their performance drops when applied to real-world, unconstrained gait data due to various factors. Among these, uncontrolled outdoor environments, non-overlapping camera views, varying illumination, and computational efficiency are core challenges in gait-based authentication. Currently, no dataset addresses all these challenges simultaneously. In this paper, we propose an OptiGait-LGBM model capable of recognizing person re-identification under these constraints using a skeletal model approach, which helps mitigate inconsistencies in a person's appearance. The model constructs a dataset from landmark positions, minimizing memory usage by using non-sequential data. A benchmark dataset, RUET-GAIT, is introduced to represent uncontrolled gait sequences in complex outdoor environments. The process involves extracting skeletal joint landmarks, generating numerical datasets, and developing an OptiGait-LGBM gait classification model. Our aim is to address the aforementioned challenges with minimal computational cost compared to existing methods. A comparative analysis with ensemble techniques such as Random Forest and CatBoost demonstrates that the proposed approach outperforms them in terms of accuracy, memory usage, and training time. This method provides a novel, low-cost, and memory-efficient video-based gait recognition solution for real-world scenarios.","authors":["Md. Sakib Hassan Chowdhury","Md. Hafiz Ahamed","Bishowjit Paul","Sarafat Hussain Abhi","Abu Bakar Siddique","Md. Robius Sany"],"url":"https://arxiv.org/abs/2505.08801"}
{"created":"2025-05-15","title":"Multi-modal Synthetic Data Training and Model Collapse: Insights from VLMs and Diffusion Models","abstract":"Recent research has highlighted the risk of generative model collapse, where performance progressively degrades when continually trained on self-generated data. However, existing exploration on model collapse is limited to single, unimodal models, limiting our understanding in more realistic scenarios, such as diverse multi-modal AI agents interacting autonomously through synthetic data and continually evolving. We expand the synthetic data training and model collapse study to multi-modal vision-language generative systems, such as vision-language models (VLMs) and text-to-image diffusion models, as well as recursive generate-train loops with multiple models. We find that model collapse, previously observed in single-modality generative models, exhibits distinct characteristics in the multi-modal context, such as improved vision-language alignment and increased variance in VLM image-captioning task. Additionally, we find that general approaches such as increased decoding budgets, greater model diversity, and relabeling with frozen models can effectively mitigate model collapse. Our findings provide initial insights and practical guidelines for reducing the risk of model collapse in self-improving multi-agent AI systems and curating robust multi-modal synthetic datasets.","authors":["Zizhao Hu","Mohammad Rostami","Jesse Thomason"],"url":"https://arxiv.org/abs/2505.08803"}
{"created":"2025-05-15","title":"TokenProber: Jailbreaking Text-to-image Models via Fine-grained Word Impact Analysis","abstract":"Text-to-image (T2I) models have significantly advanced in producing high-quality images. However, such models have the ability to generate images containing not-safe-for-work (NSFW) content, such as pornography, violence, political content, and discrimination. To mitigate the risk of generating NSFW content, refusal mechanisms, i.e., safety checkers, have been developed to check potential NSFW content. Adversarial prompting techniques have been developed to evaluate the robustness of the refusal mechanisms. The key challenge remains to subtly modify the prompt in a way that preserves its sensitive nature while bypassing the refusal mechanisms. In this paper, we introduce TokenProber, a method designed for sensitivity-aware differential testing, aimed at evaluating the robustness of the refusal mechanisms in T2I models by generating adversarial prompts. Our approach is based on the key observation that adversarial prompts often succeed by exploiting discrepancies in how T2I models and safety checkers interpret sensitive content. Thus, we conduct a fine-grained analysis of the impact of specific words within prompts, distinguishing between dirty words that are essential for NSFW content generation and discrepant words that highlight the different sensitivity assessments between T2I models and safety checkers. Through the sensitivity-aware mutation, TokenProber generates adversarial prompts, striking a balance between maintaining NSFW content generation and evading detection. Our evaluation of TokenProber against 5 safety checkers on 3 popular T2I models, using 324 NSFW prompts, demonstrates its superior effectiveness in bypassing safety filters compared to existing methods (e.g., 54%+ increase on average), highlighting TokenProber's ability to uncover robustness issues in the existing refusal mechanisms.","authors":["Longtian Wang","Xiaofei Xie","Tianlin Li","Yuhan Zhi","Chao Shen"],"url":"https://arxiv.org/abs/2505.08804"}
{"created":"2025-05-15","title":"Security of Internet of Agents: Attacks and Countermeasures","abstract":"With the rise of large language and vision-language models, AI agents have evolved into autonomous, interactive systems capable of perception, reasoning, and decision-making. As they proliferate across virtual and physical domains, the Internet of Agents (IoA) has emerged as a key infrastructure for enabling scalable and secure coordination among heterogeneous agents. This survey offers a comprehensive examination of the security and privacy landscape in IoA systems. We begin by outlining the IoA architecture and its distinct vulnerabilities compared to traditional networks, focusing on four critical aspects: identity authentication threats, cross-agent trust issues, embodied security, and privacy risks. We then review existing and emerging defense mechanisms and highlight persistent challenges. Finally, we identify open research directions to advance the development of resilient and privacy-preserving IoA ecosystems.","authors":["Yuntao Wang","Yanghe Pan","Shaolong Guo","Zhou Su"],"url":"https://arxiv.org/abs/2505.08807"}
{"created":"2025-05-15","title":"SparseMeXT Unlocking the Potential of Sparse Representations for HD Map Construction","abstract":"Recent advancements in high-definition \\emph{HD} map construction have demonstrated the effectiveness of dense representations, which heavily rely on computationally intensive bird's-eye view \\emph{BEV} features. While sparse representations offer a more efficient alternative by avoiding dense BEV processing, existing methods often lag behind due to the lack of tailored designs. These limitations have hindered the competitiveness of sparse representations in online HD map construction. In this work, we systematically revisit and enhance sparse representation techniques, identifying key architectural and algorithmic improvements that bridge the gap with--and ultimately surpass--dense approaches. We introduce a dedicated network architecture optimized for sparse map feature extraction, a sparse-dense segmentation auxiliary task to better leverage geometric and semantic cues, and a denoising module guided by physical priors to refine predictions. Through these enhancements, our method achieves state-of-the-art performance on the nuScenes dataset, significantly advancing HD map construction and centerline detection. Specifically, SparseMeXt-Tiny reaches a mean average precision \\emph{mAP} of 55.5% at 32 frames per second \\emph{fps}, while SparseMeXt-Base attains 65.2% mAP. Scaling the backbone and decoder further, SparseMeXt-Large achieves an mAP of 68.9% at over 20 fps, establishing a new benchmark for sparse representations in HD map construction. These results underscore the untapped potential of sparse methods, challenging the conventional reliance on dense representations and redefining efficiency-performance trade-offs in the field.","authors":["Anqing Jiang","Jinhao Chai","Yu Gao","Yiru Wang","Yuwen Heng","Zhigang Sun","Hao Sun","Zezhong Zhao","Li Sun","Jian Zhou","Lijuan Zhu","Shugong Xu","Hao Zhao"],"url":"https://arxiv.org/abs/2505.08808"}
{"created":"2025-05-15","title":"MixBridge: Heterogeneous Image-to-Image Backdoor Attack through Mixture of Schr\\\"odinger Bridges","abstract":"This paper focuses on implanting multiple heterogeneous backdoor triggers in bridge-based diffusion models designed for complex and arbitrary input distributions. Existing backdoor formulations mainly address single-attack scenarios and are limited to Gaussian noise input models. To fill this gap, we propose MixBridge, a novel diffusion Schr\\\"odinger bridge (DSB) framework to cater to arbitrary input distributions (taking I2I tasks as special cases). Beyond this trait, we demonstrate that backdoor triggers can be injected into MixBridge by directly training with poisoned image pairs. This eliminates the need for the cumbersome modifications to stochastic differential equations required in previous studies, providing a flexible tool to study backdoor behavior for bridge models. However, a key question arises: can a single DSB model train multiple backdoor triggers? Unfortunately, our theory shows that when attempting this, the model ends up following the geometric mean of benign and backdoored distributions, leading to performance conflict across backdoor tasks. To overcome this, we propose a Divide-and-Merge strategy to mix different bridges, where models are independently pre-trained for each specific objective (Divide) and then integrated into a unified model (Merge). In addition, a Weight Reallocation Scheme (WRS) is also designed to enhance the stealthiness of MixBridge. Empirical studies across diverse generation tasks speak to the efficacy of MixBridge.","authors":["Shixi Qin","Zhiyong Yang","Shilong Bao","Shi Wang","Qianqian Xu","Qingming Huang"],"url":"https://arxiv.org/abs/2505.08809"}
{"created":"2025-05-15","title":"Machine Learning-Based Detection of DDoS Attacks in VANETs for Emergency Vehicle Communication","abstract":"Vehicular Ad Hoc Networks (VANETs) play a key role in Intelligent Transportation Systems (ITS), particularly in enabling real-time communication for emergency vehicles. However, Distributed Denial of Service (DDoS) attacks, which interfere with safety-critical communication channels, can severely impair their reliability. This study introduces a robust and scalable framework to detect DDoS attacks in highway-based VANET environments. A synthetic dataset was constructed using Network Simulator 3 (NS-3) in conjunction with the Simulation of Urban Mobility (SUMO) and further enriched with real-world mobility traces from Germany's A81 highway, extracted via OpenStreetMap (OSM). Three traffic categories were simulated: DDoS, VoIP, and TCP-based video streaming (VideoTCP). The data preprocessing pipeline included normalization, signal-to-noise ratio (SNR) feature engineering, missing value imputation, and class balancing using the Synthetic Minority Over-sampling Technique (SMOTE). Feature importance was assessed using SHapley Additive exPlanations (SHAP). Eleven classifiers were benchmarked, among them XGBoost (XGB), CatBoost (CB), AdaBoost (AB), GradientBoosting (GB), and an Artificial Neural Network (ANN). XGB and CB achieved the best performance, each attaining an F1-score of 96%. These results highlight the robustness of the proposed framework and its potential for real-time deployment in VANETs to secure critical emergency communications.","authors":["Bappa Muktar","Vincent Fono","Adama Nouboukpo"],"url":"https://arxiv.org/abs/2505.08810"}
{"created":"2025-05-15","title":"TUGS: Physics-based Compact Representation of Underwater Scenes by Tensorized Gaussian","abstract":"Underwater 3D scene reconstruction is crucial for undewater robotic perception and navigation. However, the task is significantly challenged by the complex interplay between light propagation, water medium, and object surfaces, with existing methods unable to model their interactions accurately. Additionally, expensive training and rendering costs limit their practical application in underwater robotic systems. Therefore, we propose Tensorized Underwater Gaussian Splatting (TUGS), which can effectively solve the modeling challenges of the complex interactions between object geometries and water media while achieving significant parameter reduction. TUGS employs lightweight tensorized higher-order Gaussians with a physics-based underwater Adaptive Medium Estimation (AME) module, enabling accurate simulation of both light attenuation and backscatter effects in underwater environments. Compared to other NeRF-based and GS-based methods designed for underwater, TUGS is able to render high-quality underwater images with faster rendering speeds and less memory usage. Extensive experiments on real-world underwater datasets have demonstrated that TUGS can efficiently achieve superior reconstruction quality using a limited number of parameters, making it particularly suitable for memory-constrained underwater UAV applications","authors":["Shijie Lian","Ziyi Zhang","Laurence Tianruo Yang and","Mengyu Ren","Debin Liu","Hua Li"],"url":"https://arxiv.org/abs/2505.08811"}
{"created":"2025-05-15","title":"Towards Understanding Deep Learning Model in Image Recognition via Coverage Test","abstract":"Deep neural networks (DNNs) play a crucial role in the field of artificial intelligence, and their security-related testing has been a prominent research focus. By inputting test cases, the behavior of models is examined for anomalies, and coverage metrics are utilized to determine the extent of neurons covered by these test cases. With the widespread application and advancement of DNNs, different types of neural behaviors have garnered attention, leading to the emergence of various coverage metrics for neural networks. However, there is currently a lack of empirical research on these coverage metrics, specifically in analyzing the relationships and patterns between model depth, configuration information, and neural network coverage. This paper aims to investigate the relationships and patterns of four coverage metrics: primary functionality, boundary, hierarchy, and structural coverage. A series of empirical experiments were conducted, selecting LeNet, VGG, and ResNet as different DNN architectures, along with 10 models of varying depths ranging from 5 to 54 layers, to compare and study the relationships between different depths, configuration information, and various neural network coverage metrics. Additionally, an investigation was carried out on the relationships between modified decision/condition coverage and dataset size. Finally, three potential future directions are proposed to further contribute to the security testing of DNN Models.","authors":["Wenkai Li","Xiaoqi Li","Yingjie Mao","Yishun Wang"],"url":"https://arxiv.org/abs/2505.08814"}
{"created":"2025-05-15","title":"Self-Supervised Transformer-based Contrastive Learning for Intrusion Detection Systems","abstract":"As the digital landscape becomes more interconnected, the frequency and severity of zero-day attacks, have significantly increased, leading to an urgent need for innovative Intrusion Detection Systems (IDS). Machine Learning-based IDS that learn from the network traffic characteristics and can discern attack patterns from benign traffic offer an advanced solution to traditional signature-based IDS. However, they heavily rely on labeled datasets, and their ability to generalize when encountering unseen traffic patterns remains a challenge. This paper proposes a novel self-supervised contrastive learning approach based on transformer encoders, specifically tailored for generalizable intrusion detection on raw packet sequences. Our proposed learning scheme employs a packet-level data augmentation strategy combined with a transformer-based architecture to extract and generate meaningful representations of traffic flows. Unlike traditional methods reliant on handcrafted statistical features (NetFlow), our approach automatically learns comprehensive packet sequence representations, significantly enhancing performance in anomaly identification tasks and supervised learning for intrusion detection. Our transformer-based framework exhibits better performance in comparison to existing NetFlow self-supervised methods. Specifically, we achieve up to a 3% higher AUC in anomaly detection for intra-dataset evaluation and up to 20% higher AUC scores in inter-dataset evaluation. Moreover, our model provides a strong baseline for supervised intrusion detection with limited labeled data, exhibiting an improvement over self-supervised NetFlow models of up to 1.5% AUC when pretrained and evaluated on the same dataset. Additionally, we show the adaptability of our pretrained model when fine-tuned across different datasets, demonstrating strong performance even when lacking benign data from the target domain.","authors":["Ippokratis Koukoulis","Ilias Syrigos","Thanasis Korakis"],"url":"https://arxiv.org/abs/2505.08816"}
{"created":"2025-05-15","title":"Towards SFW sampling for diffusion models via external conditioning","abstract":"Score-based generative models (SBM), also known as diffusion models, are the de facto state of the art for image synthesis. Despite their unparalleled performance, SBMs have recently been in the spotlight for being tricked into creating not-safe-for-work (NSFW) content, such as violent images and non-consensual nudity. Current approaches that prevent unsafe generation are based on the models' own knowledge, and the majority of them require fine-tuning. This article explores the use of external sources for ensuring safe outputs in SBMs. Our safe-for-work (SFW) sampler implements a Conditional Trajectory Correction step that guides the samples away from undesired regions in the ambient space using multimodal models as the source of conditioning. Furthermore, using Contrastive Language Image Pre-training (CLIP), our method admits user-defined NSFW classes, which can vary in different settings. Our experiments on the text-to-image SBM Stable Diffusion validate that the proposed SFW sampler effectively reduces the generation of explicit content while being competitive with other fine-tuning-based approaches, as assessed via independent NSFW detectors. Moreover, we evaluate the impact of the SFW sampler on image quality and show that the proposed correction scheme comes at a minor cost with negligible effect on samples not needing correction. Our study confirms the suitability of the SFW sampler towards aligned SBM models and the potential of using model-agnostic conditioning for the prevention of unwanted images.","authors":["Camilo Carvajal Reyes","Joaqu\\'in Fontbona","Felipe Tobar"],"url":"https://arxiv.org/abs/2505.08817"}
{"created":"2025-05-15","title":"Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare","abstract":"The intricate and multifaceted nature of vision language model (VLM) development, adaptation, and application necessitates the establishment of clear and standardized reporting protocols, particularly within the high-stakes context of healthcare. Defining these reporting standards is inherently challenging due to the diverse nature of studies involving VLMs, which vary significantly from the development of all new VLMs or finetuning for domain alignment to off-the-shelf use of VLM for targeted diagnosis and prediction tasks. In this position paper, we argue that traditional machine learning reporting standards and evaluation guidelines must be restructured to accommodate multiphase VLM studies; it also has to be organized for intuitive understanding of developers while maintaining rigorous standards for reproducibility. To facilitate community adoption, we propose a categorization framework for VLM studies and outline corresponding reporting standards that comprehensively address performance evaluation, data reporting protocols, and recommendations for manuscript composition. These guidelines are organized according to the proposed categorization scheme. Lastly, we present a checklist that consolidates reporting standards, offering a standardized tool to ensure consistency and quality in the publication of VLM-related research.","authors":["Amara Tariq","Rimita Lahiri","Charles Kahn","Imon Banerjee"],"url":"https://arxiv.org/abs/2505.08818"}
{"created":"2025-05-15","title":"The Geography of Transportation Cybersecurity: Visitor Flows, Industry Clusters, and Spatial Dynamics","abstract":"The rapid evolution of the transportation cybersecurity ecosystem, encompassing cybersecurity, automotive, and transportation and logistics sectors, will lead to the formation of distinct spatial clusters and visitor flow patterns across the US. This study examines the spatiotemporal dynamics of visitor flows, analyzing how socioeconomic factors shape industry clustering and workforce distribution within these evolving sectors. To model and predict visitor flow patterns, we develop a BiTransGCN framework, integrating an attention-based Transformer architecture with a Graph Convolutional Network backbone. By integrating AI-enabled forecasting techniques with spatial analysis, this study improves our ability to track, interpret, and anticipate changes in industry clustering and mobility trends, thereby supporting strategic planning for a secure and resilient transportation network. It offers a data-driven foundation for economic planning, workforce development, and targeted investments in the transportation cybersecurity ecosystem.","authors":["Yuhao Wang (Jack)","Kailai Wang (Jack)","Songhua Hu (Jack)","Yunpeng (Jack)","Zhang","Gino Lim","Pengyu Zhu"],"url":"https://arxiv.org/abs/2505.08822"}
{"created":"2025-05-15","title":"An Extra RMSNorm is All You Need for Fine Tuning to 1.58 Bits","abstract":"Large language models (LLMs) have transformed natural-language processing, yet their scale makes real-world deployment costly. Post-training quantization reduces memory and computation but often degrades accuracy, while quantization-aware training can recover performance at the cost of extra training. Pushing quantization to the ternary (2-bit) regime yields even larger savings but is notoriously unstable. Building on recent work showing that a bias-free, RMS-normalized Transformer with straight-through estimation can reach 1.58-bit precision, we demonstrate that simply inserting RMS normalization before every linear projection and applying a gradual, layer-wise quantization schedule stably fine-tunes full-precision checkpoints into ternary LLMs. Our approach matches or surpasses more elaborate knowledge-distillation pipelines on standard language-modeling benchmarks without adding model complexity. These results indicate that careful normalization alone can close much of the accuracy gap between ternary and full-precision LLMs, making ultra-low-bit inference practical.","authors":["Cody Steinmetz","Gavin Childress","Aaron Herbst","Gavin Jones","Jasdeep Singh","Eli Vang","Keagan Weinstock"],"url":"https://arxiv.org/abs/2505.08823"}
{"created":"2025-05-15","title":"Multi-source Plume Tracing via Multi-Agent Reinforcement Learning","abstract":"Industrial catastrophes like the Bhopal disaster (1984) and the Aliso Canyon gas leak (2015) demonstrate the urgent need for rapid and reliable plume tracing algorithms to protect public health and the environment. Traditional methods, such as gradient-based or biologically inspired approaches, often fail in realistic, turbulent conditions. To address these challenges, we present a Multi-Agent Reinforcement Learning (MARL) algorithm designed for localizing multiple airborne pollution sources using a swarm of small uncrewed aerial systems (sUAS). Our method models the problem as a Partially Observable Markov Game (POMG), employing a Long Short-Term Memory (LSTM)-based Action-specific Double Deep Recurrent Q-Network (ADDRQN) that uses full sequences of historical action-observation pairs, effectively approximating latent states. Unlike prior work, we use a general-purpose simulation environment based on the Gaussian Plume Model (GPM), incorporating realistic elements such as a three-dimensional environment, sensor noise, multiple interacting agents, and multiple plume sources. The incorporation of action histories as part of the inputs further enhances the adaptability of our model in complex, partially observable environments. Extensive simulations show that our algorithm significantly outperforms conventional approaches. Specifically, our model allows agents to explore only 1.29\\% of the environment to successfully locate pollution sources.","authors":["Pedro Antonio Alarcon Granadeno","Theodore Chambers","Jane Cleland-Huang"],"url":"https://arxiv.org/abs/2505.08825"}
{"created":"2025-05-15","title":"Self Rewarding Self Improving","abstract":"We demonstrate that large language models can effectively self-improve through self-judging without requiring reference solutions, leveraging the inherent asymmetry between generating and verifying solutions. Our experiments on Countdown puzzles and MIT Integration Bee problems show that models can provide reliable reward signals without ground truth answers, enabling reinforcement learning in domains previously not possible. By implementing self-judging, we achieve significant performance gains maintaining alignment with formal verification. When combined with synthetic question generation, we establish a complete self-improvement loop where models generate practice problems, solve them, and evaluate their own performance-achieving an 8% improvement with Qwen 2.5 7B over baseline and surpassing GPT-4o performance on integration tasks. Our findings demonstrate that LLM judges can provide effective reward signals for training models, unlocking many reinforcement learning environments previously limited by the difficulty of creating programmatic rewards. This suggests a potential paradigm shift toward AI systems that continuously improve through self-directed learning rather than human-guided training, potentially accelerating progress in domains with scarce training data or complex evaluation requirements.","authors":["Toby Simonds","Kevin Lopez","Akira Yoshiyama","Dominique Garmier"],"url":"https://arxiv.org/abs/2505.08827"}
{"created":"2025-05-15","title":"Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence","abstract":"As human-AI collaboration becomes increasingly prevalent in educational contexts, understanding and measuring the extent and nature of such interactions pose significant challenges. This research investigates the use of authorship verification (AV) techniques not as a punitive measure, but as a means to quantify AI assistance in academic writing, with a focus on promoting transparency, interpretability, and student development. Building on prior work, we structured our investigation into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Using three datasets - including a public dataset (PAN-14) and two from University of Melbourne students from various courses - we expanded the data to include LLM-generated texts, totalling 1,889 documents and 540 authorship problems from 506 students. We developed an adapted Feature Vector Difference AV methodology to construct robust academic writing profiles for students, designed to capture meaningful, individual characteristics of their writing. The method's effectiveness was evaluated across multiple scenarios, including distinguishing between student-authored and LLM-generated texts and testing resilience against LLMs' attempts to mimic student writing styles. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels while providing educators with a transparent tool to support academic integrity investigations. This work advances AV technology, offering actionable insights into the dynamics of academic writing in an AI-driven era.","authors":["Eduardo Araujo Oliveira","Madhavi Mohoni","Sonsoles L\\'opez-Pernas","Mohammed Saqr"],"url":"https://arxiv.org/abs/2505.08828"}
{"created":"2025-05-15","title":"Aggregating Concepts of Fairness and Accuracy in Predictive Systems","abstract":"An algorithm that outputs predictions about the state of the world will almost always be designed with the implicit or explicit goal of outputting accurate predictions (i.e., predictions that are likely to be true). In addition, the rise of increasingly powerful predictive algorithms brought about by the recent revolution in artificial intelligence has led to an emphasis on building predictive algorithms that are fair, in the sense that their predictions do not systematically evince bias or bring about harm to certain individuals or groups. This state of affairs presents two conceptual challenges. First, the goals of accuracy and fairness can sometimes be in tension, and there are no obvious normative guidelines for managing the trade-offs between these two desiderata when they arise. Second, there are many distinct ways of measuring both the accuracy and fairness of a predictive algorithm; here too, there are no obvious guidelines on how to aggregate our preferences for predictive algorithms that satisfy disparate measures of fairness and accuracy to various extents. The goal of this paper is to address these challenges by arguing that there are good reasons for using a linear combination of accuracy and fairness metrics to measure the all-things-considered value of a predictive algorithm for agents who care about both accuracy and fairness. My argument depends crucially on a classic result in the preference aggregation literature due to Harsanyi. After making this formal argument, I apply my result to an analysis of accuracy-fairness trade-offs using the COMPAS dataset compiled by Angwin et al.","authors":["David Kinney"],"url":"https://arxiv.org/abs/2505.08829"}
{"created":"2025-05-15","title":"Federated Large Language Models: Feasibility, Robustness, Security and Future Directions","abstract":"The integration of Large Language Models (LLMs) and Federated Learning (FL) presents a promising solution for joint training on distributed data while preserving privacy and addressing data silo issues. However, this emerging field, known as Federated Large Language Models (FLLM), faces significant challenges, including communication and computation overheads, heterogeneity, privacy and security concerns. Current research has primarily focused on the feasibility of FLLM, but future trends are expected to emphasize enhancing system robustness and security. This paper provides a comprehensive review of the latest advancements in FLLM, examining challenges from four critical perspectives: feasibility, robustness, security, and future directions. We present an exhaustive survey of existing studies on FLLM feasibility, introduce methods to enhance robustness in the face of resource, data, and task heterogeneity, and analyze novel risks associated with this integration, including privacy threats and security challenges. We also review the latest developments in defense mechanisms and explore promising future research directions, such as few-shot learning, machine unlearning, and IP protection. This survey highlights the pressing need for further research to enhance system robustness and security while addressing the unique challenges posed by the integration of FL and LLM.","authors":["Wenhao Jiang","Yuchuan Luo","Guilin Deng","Silong Chen","Xu Yang","Shihong Wu","Xinwen Gao","Lin Liu","Shaojing Fu"],"url":"https://arxiv.org/abs/2505.08830"}
{"created":"2025-05-15","title":"Generative AI for Urban Planning: Synthesizing Satellite Imagery via Diffusion Models","abstract":"Generative AI offers new opportunities for automating urban planning by creating site-specific urban layouts and enabling flexible design exploration. However, existing approaches often struggle to produce realistic and practical designs at scale. Therefore, we adapt a state-of-the-art Stable Diffusion model, extended with ControlNet, to generate high-fidelity satellite imagery conditioned on land use descriptions, infrastructure, and natural environments. To overcome data availability limitations, we spatially link satellite imagery with structured land use and constraint information from OpenStreetMap. Using data from three major U.S. cities, we demonstrate that the proposed diffusion model generates realistic and diverse urban landscapes by varying land-use configurations, road networks, and water bodies, facilitating cross-city learning and design diversity. We also systematically evaluate the impacts of varying language prompts and control imagery on the quality of satellite imagery generation. Our model achieves high FID and KID scores and demonstrates robustness across diverse urban contexts. Qualitative assessments from urban planners and the general public show that generated images align closely with design descriptions and constraints, and are often preferred over real images. This work establishes a benchmark for controlled urban imagery generation and highlights the potential of generative AI as a tool for enhancing planning workflows and public engagement.","authors":["Qingyi Wang","Yuebing Liang","Yunhan Zheng","Kaiyuan Xu","Jinhua Zhao","Shenhao Wang"],"url":"https://arxiv.org/abs/2505.08833"}
{"created":"2025-05-15","title":"Crowd Scene Analysis using Deep Learning Techniques","abstract":"Our research is focused on two main applications of crowd scene analysis crowd counting and anomaly detection In recent years a large number of researches have been presented in the domain of crowd counting We addressed two main challenges in this domain 1 Deep learning models are datahungry paradigms and always need a large amount of annotated data for the training of algorithm It is timeconsuming and costly task to annotate such large amount of data Selfsupervised training is proposed to deal with this challenge 2 MCNN consists of multicolumns of CNN with different sizes of filters by presenting a novel approach based on a combination of selfsupervised training and MultiColumn CNN This enables the model to learn features at different levels and makes it effective in dealing with challenges of occluded scenes nonuniform density complex backgrounds and scale invariation The proposed model was evaluated on publicly available data sets such as ShanghaiTech and UCFQNRF by means of MAE and MSE A spatiotemporal model based on VGG19 is proposed for crowd anomaly detection addressing challenges like lighting environmental conditions unexpected objects and scalability The model extracts spatial and temporal features allowing it to be generalized to realworld scenes Spatial features are learned using CNN while temporal features are learned using LSTM blocks The model works on binary classification and can detect normal or abnormal behavior The models performance is improved by replacing fully connected layers with dense residual blocks Experiments on the Hockey Fight dataset and SCVD dataset show our models outperform other stateoftheart approaches","authors":["Muhammad Junaid Asif"],"url":"https://arxiv.org/abs/2505.08834"}
{"created":"2025-05-15","title":"Robustness Analysis against Adversarial Patch Attacks in Fully Unmanned Stores","abstract":"The advent of convenient and efficient fully unmanned stores equipped with artificial intelligence-based automated checkout systems marks a new era in retail. However, these systems have inherent artificial intelligence security vulnerabilities, which are exploited via adversarial patch attacks, particularly in physical environments. This study demonstrated that adversarial patches can severely disrupt object detection models used in unmanned stores, leading to issues such as theft, inventory discrepancies, and interference. We investigated three types of adversarial patch attacks -- Hiding, Creating, and Altering attacks -- and highlighted their effectiveness. We also introduce the novel color histogram similarity loss function by leveraging attacker knowledge of the color information of a target class object. Besides the traditional confusion-matrix-based attack success rate, we introduce a new bounding-boxes-based metric to analyze the practical impact of these attacks. Starting with attacks on object detection models trained on snack and fruit datasets in a digital environment, we evaluated the effectiveness of adversarial patches in a physical testbed that mimicked a real unmanned store with RGB cameras and realistic conditions. Furthermore, we assessed the robustness of these attacks in black-box scenarios, demonstrating that shadow attacks can enhance success rates of attacks even without direct access to model parameters. Our study underscores the necessity for robust defense strategies to protect unmanned stores from adversarial threats. Highlighting the limitations of the current defense mechanisms in real-time detection systems and discussing various proactive measures, we provide insights into improving the robustness of object detection models and fortifying unmanned retail environments against these attacks.","authors":["Hyunsik Na","Wonho Lee","Seungdeok Roh","Sohee Park","Daeseon Choi"],"url":"https://arxiv.org/abs/2505.08835"}
{"created":"2025-05-15","title":"Adaptive Security Policy Management in Cloud Environments Using Reinforcement Learning","abstract":"The security of cloud environments, such as Amazon Web Services (AWS), is complex and dynamic. Static security policies have become inadequate as threats evolve and cloud resources exhibit elasticity [1]. This paper addresses the limitations of static policies by proposing a security policy management framework that uses reinforcement learning (RL) to adapt dynamically. Specifically, we employ deep reinforcement learning algorithms, including deep Q Networks and proximal policy optimization, enabling the learning and continuous adjustment of controls such as firewall rules and Identity and Access Management (IAM) policies. The proposed RL based solution leverages cloud telemetry data (AWS Cloud Trail logs, network traffic data, threat intelligence feeds) to continuously refine security policies, maximizing threat mitigation, and compliance while minimizing resource impact. Experimental results demonstrate that our adaptive RL based framework significantly outperforms static policies, achieving higher intrusion detection rates (92% compared to 82% for static policies) and substantially reducing incident detection and response times by 58%. In addition, it maintains high conformity with security requirements and efficient resource usage. These findings validate the effectiveness of adaptive reinforcement learning approaches in improving cloud security policy management.","authors":["Muhammad Saqib","Dipkumar Mehta","Fnu Yashu","Shubham Malhotra"],"url":"https://arxiv.org/abs/2505.08837"}
{"created":"2025-05-15","title":"Lightweight Hybrid Block-Stream Cryptographic Algorithm for the Internet of Things","abstract":"In this thesis, a novel lightweight hybrid encryption algorithm named SEPAR is proposed, featuring a 16-bit block length and a 128-bit initialization vector. The algorithm is designed specifically for application in Internet of Things (IoT) technology devices. The design concept of this algorithm is based on the integration of a pseudo-random permutation function and a pseudo-random generator function. This intelligent combination not only enhances the algorithm's resistance against cryptographic attacks but also improves its processing speed. The security analyses conducted on the algorithm, along with the results of NIST statistical tests, confirm its robustness against most common and advanced cryptographic attacks, including linear and differential attacks. The proposed algorithm has been implemented on various software platform architectures. The software implementation was carried out on three platforms: 8-bit, 16-bit, and 32-bit architectures. A comparative analysis with the BORON algorithm on a 32-bit ARM processor indicates a performance improvement of 42.25%. Furthermore, implementation results on 8-bit and 16-bit microcontrollers demonstrate performance improvements of 87.91% and 98.01% respectively, compared to the PRESENT cipher.","authors":["Arsalan Vahi","Mirkamal Mirnia"],"url":"https://arxiv.org/abs/2505.08840"}
{"created":"2025-05-15","title":"Will AI Take My Job? Evolving Perceptions of Automation and Labor Risk in Latin America","abstract":"As artificial intelligence and robotics increasingly reshape the global labor market, understanding public perceptions of these technologies becomes critical. We examine how these perceptions have evolved across Latin America, using survey data from the 2017, 2018, 2020, and 2023 waves of the Latinobar\\'ometro. Drawing on responses from over 48,000 individuals across 16 countries, we analyze fear of job loss due to artificial intelligence and robotics. Using statistical modeling and latent class analysis, we identify key structural and ideological predictors of concern, with education level and political orientation emerging as the most consistent drivers. Our findings reveal substantial temporal and cross-country variation, with a notable peak in fear during 2018 and distinct attitudinal profiles emerging from latent segmentation. These results offer new insights into the social and structural dimensions of AI anxiety in emerging economies and contribute to a broader understanding of public attitudes toward automation beyond the Global North.","authors":["Andrea Cremaschi","Dae-Jin Lee","Manuele Leonelli"],"url":"https://arxiv.org/abs/2505.08841"}
{"created":"2025-05-15","title":"LibVulnWatch: A Deep Assessment Agent System and Leaderboard for Uncovering Hidden Vulnerabilities in Open-Source AI Libraries","abstract":"Open-source AI libraries are foundational to modern AI systems but pose significant, underexamined risks across security, licensing, maintenance, supply chain integrity, and regulatory compliance. We present LibVulnWatch, a graph-based agentic assessment framework that performs deep, source-grounded evaluations of these libraries. Built on LangGraph, the system coordinates a directed acyclic graph of specialized agents to extract, verify, and quantify risk using evidence from trusted sources such as repositories, documentation, and vulnerability databases. LibVulnWatch generates reproducible, governance-aligned scores across five critical domains, publishing them to a public leaderboard for longitudinal ecosystem monitoring. Applied to 20 widely used libraries, including ML frameworks, LLM inference engines, and agent orchestration tools, our system covers up to 88% of OpenSSF Scorecard checks while uncovering up to 19 additional risks per library. These include critical Remote Code Execution (RCE) vulnerabilities, absent Software Bills of Materials (SBOMs), licensing constraints, undocumented telemetry, and widespread gaps in regulatory documentation and auditability. By translating high-level governance principles into practical, verifiable metrics, LibVulnWatch advances technical AI governance with a scalable, transparent mechanism for continuous supply chain risk assessment and informed library selection.","authors":["Zekun Wu","Seonglae Cho","Umar Mohammed","Cristian Munoz","Kleyton Costa","Xin Guan","Theo King","Ze Wang","Emre Kazim","Adriano Koshiyama"],"url":"https://arxiv.org/abs/2505.08842"}
{"created":"2025-05-15","title":"Evaluating Simplification Algorithms for Interpretability of Time Series Classification","abstract":"In this work, we introduce metrics to evaluate the use of simplified time series in the context of interpretability of a TSC - a Time Series Classifier. Such simplifications are important because time series data, in contrast to text and image data, are not intuitively understandable to humans. These metrics are related to the complexity of the simplifications - how many segments they contain - and to their loyalty - how likely they are to maintain the classification of the original time series. We employ these metrics to evaluate four distinct simplification algorithms, across several TSC algorithms and across datasets of varying characteristics, from seasonal or stationary to short or long. Our findings suggest that using simplifications for interpretability of TSC is much better than using the original time series, particularly when the time series are seasonal, non-stationary and/or with low entropy.","authors":["Felix Marti-Perez","Brigt H{\\aa}vardstun","C\\`esar Ferri","Carlos Monserrat","Jan Arne Telle"],"url":"https://arxiv.org/abs/2505.08846"}
{"created":"2025-05-15","title":"On the interplay of Explainability, Privacy and Predictive Performance with Explanation-assisted Model Extraction","abstract":"Machine Learning as a Service (MLaaS) has gained important attraction as a means for deploying powerful predictive models, offering ease of use that enables organizations to leverage advanced analytics without substantial investments in specialized infrastructure or expertise. However, MLaaS platforms must be safeguarded against security and privacy attacks, such as model extraction (MEA) attacks. The increasing integration of explainable AI (XAI) within MLaaS has introduced an additional privacy challenge, as attackers can exploit model explanations particularly counterfactual explanations (CFs) to facilitate MEA. In this paper, we investigate the trade offs among model performance, privacy, and explainability when employing Differential Privacy (DP), a promising technique for mitigating CF facilitated MEA. We evaluate two distinct DP strategies: implemented during the classification model training and at the explainer during CF generation.","authors":["Fatima Ezzeddine","Rinad Akel","Ihab Sbeity","Silvia Giordano","Marc Langheinrich","Omran Ayoub"],"url":"https://arxiv.org/abs/2505.08847"}
{"created":"2025-05-15","title":"Improved Algorithms for Differentially Private Language Model Alignment","abstract":"Language model alignment is crucial for ensuring that large language models (LLMs) align with human preferences, yet it often involves sensitive user data, raising significant privacy concerns. While prior work has integrated differential privacy (DP) with alignment techniques, their performance remains limited. In this paper, we propose novel algorithms for privacy-preserving alignment and rigorously analyze their effectiveness across varying privacy budgets and models. Our framework can be deployed on two celebrated alignment techniques, namely direct preference optimization (DPO) and reinforcement learning from human feedback (RLHF). Through systematic experiments on large-scale language models, we demonstrate that our approach achieves state-of-the-art performance. Notably, one of our algorithms, DP-AdamW, combined with DPO, surpasses existing methods, improving alignment quality by up to 15% under moderate privacy budgets ({\\epsilon}=2-5). We further investigate the interplay between privacy guarantees, alignment efficacy, and computational demands, providing practical guidelines for optimizing these trade-offs.","authors":["Keyu Chen","Hao Tang","Qinglin Liu","Yizhao Xu"],"url":"https://arxiv.org/abs/2505.08849"}
{"created":"2025-05-15","title":"Efficiently Manipulating Clutter via Learning and Search-Based Reasoning","abstract":"This thesis presents novel algorithms to advance robotic object rearrangement, a critical task for autonomous systems in applications like warehouse automation and household assistance. Addressing challenges of high-dimensional planning, complex object interactions, and computational demands, our work integrates deep learning for interaction prediction, tree search for action sequencing, and parallelized computation for efficiency. Key contributions include the Deep Interaction Prediction Network (DIPN) for accurate push motion forecasting (over 90% accuracy), its synergistic integration with Monte Carlo Tree Search (MCTS) for effective non-prehensile object retrieval (100% completion in specific challenging scenarios), and the Parallel MCTS with Batched Simulations (PMBS) framework, which achieves substantial planning speed-up while maintaining or improving solution quality. The research further explores combining diverse manipulation primitives, validated extensively through simulated and real-world experiments.","authors":["Baichuan Huang"],"url":"https://arxiv.org/abs/2505.08853"}
{"created":"2025-05-15","title":"Generative AI for Autonomous Driving: Frontiers and Opportunities","abstract":"Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.","authors":["Yuping Wang","Shuo Xing","Cui Can","Renjie Li","Hongyuan Hua","Kexin Tian","Zhaobin Mo","Xiangbo Gao","Keshu Wu","Sulong Zhou","Hengxu You","Juntong Peng","Junge Zhang","Zehao Wang","Rui Song","Mingxuan Yan","Walter Zimmer","Xingcheng Zhou","Peiran Li","Zhaohan Lu","Chia-Ju Chen","Yue Huang","Ryan A. Rossi","Lichao Sun","Hongkai Yu","Zhiwen Fan","Frank Hao Yang","Yuhao Kang","Ross Greer","Chenxi Liu","Eun Hak Lee","Xuan Di","Xinyue Ye","Liu Ren","Alois Knoll","Xiaopeng Li","Shuiwang Ji","Masayoshi Tomizuka","Marco Pavone","Tianbao Yang","Jing Du","Ming-Hsuan Yang","Hua Wei","Ziran Wang","Yang Zhou","Jiachen Li","Zhengzhong Tu"],"url":"https://arxiv.org/abs/2505.08854"}
{"created":"2025-05-15","title":"Real-time Capable Learning-based Visual Tool Pose Correction via Differentiable Simulation","abstract":"Autonomy in Minimally Invasive Robotic Surgery (MIRS) has the potential to reduce surgeon cognitive and task load, thereby increasing procedural efficiency. However, implementing accurate autonomous control can be difficult due to poor end-effector proprioception, a limitation of their cable-driven mechanisms. Although the robot may have joint encoders for the end-effector pose calculation, various non-idealities make the entire kinematics chain inaccurate. Modern vision-based pose estimation methods lack real-time capability or can be hard to train and generalize. In this work, we demonstrate a real-time capable, vision transformer-based pose estimation approach that is trained using end-to-end differentiable kinematics and rendering in simulation. We demonstrate the potential of this method to correct for noisy pose estimates in simulation, with the longer term goal of verifying the sim-to-real transferability of our approach.","authors":["Shuyuan Yang","Zonghe Chua"],"url":"https://arxiv.org/abs/2505.08875"}
{"created":"2025-05-15","title":"Optimized Couplings for Watermarking Large Language Models","abstract":"Large-language models (LLMs) are now able to produce text that is, in many cases, seemingly indistinguishable from human-generated content. This has fueled the development of watermarks that imprint a ``signal'' in LLM-generated text with minimal perturbation of an LLM's output. This paper provides an analysis of text watermarking in a one-shot setting. Through the lens of hypothesis testing with side information, we formulate and analyze the fundamental trade-off between watermark detection power and distortion in generated textual quality. We argue that a key component in watermark design is generating a coupling between the side information shared with the watermark detector and a random partition of the LLM vocabulary. Our analysis identifies the optimal coupling and randomization strategy under the worst-case LLM next-token distribution that satisfies a min-entropy constraint. We provide a closed-form expression of the resulting detection rate under the proposed scheme and quantify the cost in a max-min sense. Finally, we provide an array of numerical results, comparing the proposed scheme with the theoretical optimum and existing schemes, in both synthetic data and LLM watermarking. Our code is available at https://github.com/Carol-Long/CC_Watermark","authors":["Dor Tsur","Carol Xuan Long","Claudio Mayrink Verdun","Hsiang Hsu","Haim Permuter","Flavio P. Calmon"],"url":"https://arxiv.org/abs/2505.08878"}
{"created":"2025-05-15","title":"Intelligent Road Anomaly Detection with Real-time Notification System for Enhanced Road Safety","abstract":"This study aims to improve transportation safety, especially traffic safety. Road damage anomalies such as potholes and cracks have emerged as a significant and recurring cause for accidents. To tackle this problem and improve road safety, a comprehensive system has been developed to detect potholes, cracks (e.g. alligator, transverse, longitudinal), classify their sizes, and transmit this data to the cloud for appropriate action by authorities. The system also broadcasts warning signals to nearby vehicles warning them if a severe anomaly is detected on the road. Moreover, the system can count road anomalies in real-time. It is emulated through the utilization of Raspberry Pi, a camera module, deep learning model, laptop, and cloud service. Deploying this innovative solution aims to proactively enhance road safety by notifying relevant authorities and drivers about the presence of potholes and cracks to take actions, thereby mitigating potential accidents arising from this prevalent road hazard leading to safer road conditions for the whole community.","authors":["Ali Almakhluk","Uthman Baroudi","Yasser El-Alfy"],"url":"https://arxiv.org/abs/2505.08882"}
{"created":"2025-05-15","title":"Jacobian-Free Newton-Krylov with a globalization method for solving groundwater flow models of multi-layer aquifer systems","abstract":"A Jacobian free Newton Krylov (JFNK) method with a globalization scheme is introduced to solve large and complex nonlinear systems of equations that arise in groundwater flow models of multi-layer aquifer systems. We explore the advantages of the JFNK method relative to the Newton-Krylov (NK) method and identify the circumstances in which the JFNK method demonstrates computing efficiency. We perform the validation and efficiency of the JFNK method on various test cases involving an unconfined single-layer aquifer and a two-layer aquifer with both confined and unconfined conditions. The results are validated by the NK method. The JFNK method is incorporated in Integrated Water Flow Model (IWFM), an integrated hydrologic model developed and maintained by California Department of Water Resources. We examine the determinacy of the JFNK's adaptability on practical models such as the California Central Valley Groundwater-Surface Water Simulation Model (C2VSim).","authors":["Raghav Singhal","Emin Can Dogrul","Zhaojun Bai"],"url":"https://arxiv.org/abs/2505.08884"}
{"created":"2025-05-15","title":"Optimizing Neuro-Fuzzy and Colonial Competition Algorithms for Skin Cancer Diagnosis in Dermatoscopic Images","abstract":"The rising incidence of skin cancer, coupled with limited public awareness and a shortfall in clinical expertise, underscores an urgent need for advanced diagnostic aids. Artificial Intelligence (AI) has emerged as a promising tool in this domain, particularly for distinguishing malignant from benign skin lesions. Leveraging publicly available datasets of skin lesions, researchers have been developing AI-based diagnostic solutions. However, the integration of such computer systems in clinical settings is still nascent. This study aims to bridge this gap by employing a fusion of image processing techniques and machine learning algorithms, specifically neuro-fuzzy and colonial competition approaches. Applied to dermoscopic images from the ISIC database, our method achieved a notable accuracy of 94% on a dataset of 560 images. These results underscore the potential of our approach in aiding clinicians in the early detection of melanoma, thereby contributing significantly to skin cancer diagnostics.","authors":["Hamideh Khaleghpour","Brett McKinney"],"url":"https://arxiv.org/abs/2505.08886"}
{"created":"2025-05-15","title":"IntrinsicEdit: Precise generative image manipulation in intrinsic space","abstract":"Generative diffusion models have advanced image editing with high-quality results and intuitive interfaces such as prompts and semantic drawing. However, these interfaces lack precise control, and the associated methods typically specialize on a single editing task. We introduce a versatile, generative workflow that operates in an intrinsic-image latent space, enabling semantic, local manipulation with pixel precision for a range of editing operations. Building atop the RGB-X diffusion framework, we address key challenges of identity preservation and intrinsic-channel entanglement. By incorporating exact diffusion inversion and disentangled channel manipulation, we enable precise, efficient editing with automatic resolution of global illumination effects -- all without additional data collection or model fine-tuning. We demonstrate state-of-the-art performance across a variety of tasks on complex images, including color and texture adjustments, object insertion and removal, global relighting, and their combinations.","authors":["Linjie Lyu","Valentin Deschaintre","Yannick Hold-Geoffroy","Milo\\v{s} Ha\\v{s}an","Jae Shin Yoon","Thomas Leimk\\\"uhler","Christian Theobalt","Iliyan Georgiev"],"url":"https://arxiv.org/abs/2505.08889"}
{"created":"2025-05-15","title":"Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives","abstract":"Motivation is an important factor underlying successful learning. Previous research has demonstrated the positive effects that static interactive narrative games can have on motivation. Concurrently, advances in AI have made dynamic and adaptive approaches to interactive narrative increasingly accessible. However, limited work has explored the impact that dynamic narratives can have on learner motivation. In this paper, we compare two versions of Academical, a choice-based educational interactive narrative game about research ethics. One version employs a traditional hand-authored branching plot (i.e., static narrative) while the other dynamically sequences plots during play (i.e., dynamic narrative). Results highlight the importance of responsive content and a variety of choices for player engagement, while also illustrating the challenge of balancing pedagogical goals with the dynamic aspects of narrative. We also discuss design implications that arise from these findings. Ultimately, this work provides initial steps to illuminate the emerging potential of AI-driven dynamic narrative in educational games.","authors":["Daeun Hwang","Samuel Shields","Alex Calderwood","Shi Johnson-Bey","Michael Mateas","Noah Wardrip-Fruin","Edward F. Melcer"],"url":"https://arxiv.org/abs/2505.08891"}
{"created":"2025-05-15","title":"WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp","abstract":"Recent advances in generative AI, such as ChatGPT, have transformed access to information in education, knowledge-seeking, and everyday decision-making. However, in many developing regions, access remains a challenge due to the persistent digital divide. To help bridge this gap, we developed WaLLM - a custom AI chatbot over WhatsApp, a widely used communication platform in developing regions. Beyond answering queries, WaLLM offers several features to enhance user engagement: a daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Our service has been operational for over 6 months, amassing over 14.7K queries from approximately 100 users. In this paper, we present WaLLM's design and a systematic analysis of logs to understand user interactions. Our results show that 55% of user queries seek factual information. \"Health and well-being\" was the most popular topic (28%), including queries about nutrition and disease, suggesting users view WaLLM as a reliable source. Two-thirds of users' activity occurred within 24 hours of the daily top question. Users who accessed the \"Leaderboard\" interacted with WaLLM 3x as those who did not. We conclude by discussing implications for culture-based customization, user interface design, and appropriate calibration of users' trust in AI systems for developing regions.","authors":["Hiba Eltigani","Rukhshan Haroon","Asli Kocak","Abdullah Bin Faisal","Noah Martin","Fahad Dogar"],"url":"https://arxiv.org/abs/2505.08894"}
{"created":"2025-05-15","title":"Deep reinforcement learning-based longitudinal control strategy for automated vehicles at signalised intersections","abstract":"Developing an autonomous vehicle control strategy for signalised intersections (SI) is one of the challenging tasks due to its inherently complex decision-making process. This study proposes a Deep Reinforcement Learning (DRL) based longitudinal vehicle control strategy at SI. A comprehensive reward function has been formulated with a particular focus on (i) distance headway-based efficiency reward, (ii) decision-making criteria during amber light, and (iii) asymmetric acceleration/ deceleration response, along with the traditional safety and comfort criteria. This reward function has been incorporated with two popular DRL algorithms, Deep Deterministic Policy Gradient (DDPG) and Soft-Actor Critic (SAC), which can handle the continuous action space of acceleration/deceleration. The proposed models have been trained on the combination of real-world leader vehicle (LV) trajectories and simulated trajectories generated using the Ornstein-Uhlenbeck (OU) process. The overall performance of the proposed models has been tested using Cumulative Distribution Function (CDF) plots and compared with the real-world trajectory data. The results show that the RL models successfully maintain lower distance headway (i.e., higher efficiency) and jerk compared to human-driven vehicles without compromising safety. Further, to assess the robustness of the proposed models, we evaluated the model performance on diverse safety-critical scenarios, in terms of car-following and traffic signal compliance. Both DDPG and SAC models successfully handled the critical scenarios, while the DDPG model showed smoother action profiles compared to the SAC model. Overall, the results confirm that DRL-based longitudinal vehicle control strategy at SI can help to improve traffic safety, efficiency, and comfort.","authors":["Pankaj Kumar","Aditya Mishra","Pranamesh Chakraborty","Subrahmanya Swamy Peruru"],"url":"https://arxiv.org/abs/2505.08896"}
{"created":"2025-05-15","title":"Performance Gains of LLMs With Humans in a World of LLMs Versus Humans","abstract":"Currently, a considerable research effort is devoted to comparing LLMs to a group of human experts, where the term \"expert\" is often ill-defined or variable, at best, in a state of constantly updating LLM releases. Without proper safeguards in place, LLMs will threaten to cause harm to the established structure of safe delivery of patient care which has been carefully developed throughout history to keep the safety of the patient at the forefront. A key driver of LLM innovation is founded on community research efforts which, if continuing to operate under \"humans versus LLMs\" principles, will expedite this trend. Therefore, research efforts moving forward must focus on effectively characterizing the safe use of LLMs in clinical settings that persist across the rapid development of novel LLM models. In this communication, we demonstrate that rather than comparing LLMs to humans, there is a need to develop strategies enabling efficient work of humans with LLMs in an almost symbiotic manner.","authors":["Lucas McCullum","Pelagie Ami Agassi","Leo Anthony Celi","Daniel K. Ebner","Chrystinne Oliveira Fernandes","Rachel S. Hicklen","Mkliwa Koumbia","Lisa Soleymani Lehmann","David Restrepo"],"url":"https://arxiv.org/abs/2505.08902"}
{"created":"2025-05-15","title":"Assessing and Advancing Benchmarks for Evaluating Large Language Models in Software Engineering Tasks","abstract":"Large language models (LLMs) are gaining increasing popularity in software engineering (SE) due to their unprecedented performance across various applications. These models are increasingly being utilized for a range of SE tasks, including requirements engineering and design, code analysis and generation, software maintenance, and quality assurance. As LLMs become more integral to SE, evaluating their effectiveness is crucial for understanding their potential in this field. In recent years, substantial efforts have been made to assess LLM performance in various SE tasks, resulting in the creation of several benchmarks tailored to this purpose. This paper offers a thorough review of 191 benchmarks, addressing three main aspects: what benchmarks are available, how benchmarks are constructed, and the future outlook for these benchmarks. We begin by examining SE tasks such as requirements engineering and design, coding assistant, software testing, AIOPs, software maintenance, and quality management. We then analyze the benchmarks and their development processes, highlighting the limitations of existing benchmarks. Additionally, we discuss the successes and failures of LLMs in different software tasks and explore future opportunities and challenges for SE-related benchmarks. We aim to provide a comprehensive overview of benchmark research in SE and offer insights to support the creation of more effective evaluation tools.","authors":["Xing Hu","Feifei Niu","Junkai Chen","Xin Zhou","Junwei Zhang","Junda He","Xin Xia","David Lo"],"url":"https://arxiv.org/abs/2505.08903"}
{"created":"2025-05-15","title":"FareShare: A Tool for Labor Organizers to Estimate Lost Wages and Contest Arbitrary AI and Algorithmic Deactivations","abstract":"What happens when a rideshare driver is suddenly locked out of the platform connecting them to riders, wages, and daily work? Deactivation-the abrupt removal of gig workers' platform access-typically occurs through arbitrary AI and algorithmic decisions with little explanation or recourse. This represents one of the most severe forms of algorithmic control and often devastates workers' financial stability. Recent U.S. state policies now mandate appeals processes and recovering compensation during the period of wrongful deactivation based on past earnings. Yet, labor organizers still lack effective tools to support these complex, error-prone workflows. We designed FareShare, a computational tool automating lost wage estimation for deactivated drivers, through a 6 month partnership with the State of Washington's largest rideshare labor union. Over the following 3 months, our field deployment of FareShare registered 178 account signups. We observed that the tool could reduce lost wage calculation time by over 95%, eliminate manual data entry errors, and enable legal teams to generate arbitration-ready reports more efficiently. Beyond these gains, the deployment also surfaced important socio-technical challenges around trust, consent, and tool adoption in high-stakes labor contexts.","authors":["Varun Nagaraj Rao","Samantha Dalal","Andrew Schwartz","Amna Liaqat","Dana Calacci","Andr\\'es Monroy-Hern\\'andez"],"url":"https://arxiv.org/abs/2505.08904"}
{"created":"2025-05-15","title":"Grounding Synthetic Data Evaluations of Language Models in Unsupervised Document Corpora","abstract":"Language Models (LMs) continue to advance, improving response quality and coherence. Given Internet-scale training datasets, LMs have likely encountered much of what users might ask them to generate in some form during their training. A plethora of evaluation benchmarks have been constructed to assess model quality, response appropriateness, and reasoning capabilities. However, the human effort required for benchmark construction is limited and being rapidly outpaced by the size and scope of the models under evaluation. Additionally, having humans build a benchmark for every possible domain of interest is impractical. Therefore, we propose a methodology for automating the construction of fact-based synthetic data model evaluations grounded in document populations. This work leverages those very same LMs to evaluate domain-specific knowledge automatically, using only grounding documents (e.g., a textbook) as input. This synthetic data benchmarking approach corresponds well with human curated questions with a Spearman ranking correlation of 0.96 and a benchmark evaluation Pearson accuracy correlation of 0.79. This novel tool supports generating both multiple choice and open-ended synthetic data questions to gain diagnostic insight of LM capability. We apply this methodology to evaluate model performance on a recent relevant arXiv preprint, discovering a surprisingly strong performance from Gemma3 models.","authors":["Michael Majurski","Cynthia Matuszek"],"url":"https://arxiv.org/abs/2505.08905"}
{"created":"2025-05-15","title":"Comparing Parallel Functional Array Languages: Programming and Performance","abstract":"Parallel functional array languages are an emerging class of programming languages that promise to combine low-effort parallel programming with good performance and performance portability. We systematically compare the designs and implementations of five different functional array languages: Accelerate, APL, DaCe, Futhark, and SaC. We demonstrate the expressiveness of functional array programming by means of four challenging benchmarks, namely N-body simulation, MultiGrid, Quickhull, and Flash Attention. These benchmarks represent a range of application domains and parallel computational models. We argue that the functional array code is much shorter and more comprehensible than the hand-optimized baseline implementations because it omits architecture-specific aspects. Instead, the language implementations generate both multicore and GPU executables from a single source code base. Hence, we further argue that functional array code could more easily be ported to, and optimized for, new parallel architectures than conventional implementations of numerical kernels. We demonstrate this potential by reporting the performance of the five parallel functional array languages on a total of 39 instances of the four benchmarks on both a 32-core AMD EPYC 7313 multicore system and on an NVIDIA A30 GPU. We explore in-depth why each language performs well or not so well on each benchmark and architecture. We argue that the results demonstrate that mature functional array languages have the potential to deliver performance competitive with the best available conventional techniques.","authors":["David van Balen","Tiziano De Matteis","Clemens Grelck","Troels Henriksen","Aaron W. Hsu","Gabriele K. Keller","Thomas Koopman","Trevor L. McDonell","Cosmin Oancea","Sven-Bodo Scholz","Artjoms Sinkarovs","Tom Smeding","Phil Trinder","Ivo Gabe de Wolff","Alexandros Nikolaos Ziogas"],"url":"https://arxiv.org/abs/2505.08906"}
{"created":"2025-05-15","title":"Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Inverse Problems","abstract":"Plug-and-play (PnP) methods with deep denoisers have shown impressive results in imaging problems. They typically require strong convexity or smoothness of the fidelity term and a (residual) non-expansive denoiser for convergence. These assumptions, however, are violated in Poisson inverse problems, and non-expansiveness can hinder denoising performance. To address these challenges, we propose a cocoercive conservative (CoCo) denoiser, which may be (residual) expansive, leading to improved denoising. By leveraging the generalized Helmholtz decomposition, we introduce a novel training strategy that combines Hamiltonian regularization to promote conservativeness and spectral regularization to ensure cocoerciveness. We prove that CoCo denoiser is a proximal operator of a weakly convex function, enabling a restoration model with an implicit weakly convex prior. The global convergence of PnP methods to a stationary point of this restoration model is established. Extensive experimental results demonstrate that our approach outperforms closely related methods in both visual quality and quantitative metrics.","authors":["Deliang Wei","Peng Chen","Haobo Xu","Jiale Yao","Fang Li","Tieyong Zeng"],"url":"https://arxiv.org/abs/2505.08909"}
{"created":"2025-05-15","title":"Behind Maya: Building a Multilingual Vision Language Model","abstract":"In recent times, we have seen a rapid development of large Vision-Language Models (VLMs). They have shown impressive results on academic benchmarks, primarily in widely spoken languages but lack performance on low-resource languages and varied cultural contexts. To address these limitations, we introduce Maya, an open-source Multilingual VLM. Our contributions are: 1) a multilingual image-text pretraining dataset in eight languages, based on the LLaVA pretraining dataset; and 2) a multilingual image-text model supporting these languages, enhancing cultural and linguistic comprehension in vision-language tasks. Code available at https://github.com/nahidalam/maya.","authors":["Nahid Alam","Karthik Reddy Kanjula","Surya Guthikonda","Timothy Chung","Bala Krishna S Vegesna","Abhipsha Das","Anthony Susevski","Ryan Sze-Yin Chan","S M Iftekhar Uddin","Shayekh Bin Islam","Roshan Santhosh","Snegha A","Drishti Sharma","Chen Liu","Isha Chaturvedi","Genta Indra Winata","Ashvanth. S","Snehanshu Mukherjee","Alham Fikri Aji"],"url":"https://arxiv.org/abs/2505.08910"}
{"created":"2025-05-15","title":"An Analytical Characterization of Sloppiness in Neural Networks: Insights from Linear Models","abstract":"Recent experiments have shown that training trajectories of multiple deep neural networks with different architectures, optimization algorithms, hyper-parameter settings, and regularization methods evolve on a remarkably low-dimensional \"hyper-ribbon-like\" manifold in the space of probability distributions. Inspired by the similarities in the training trajectories of deep networks and linear networks, we analytically characterize this phenomenon for the latter. We show, using tools in dynamical systems theory, that the geometry of this low-dimensional manifold is controlled by (i) the decay rate of the eigenvalues of the input correlation matrix of the training data, (ii) the relative scale of the ground-truth output to the weights at the beginning of training, and (iii) the number of steps of gradient descent. By analytically computing and bounding the contributions of these quantities, we characterize phase boundaries of the region where hyper-ribbons are to be expected. We also extend our analysis to kernel machines and linear models that are trained with stochastic gradient descent.","authors":["Jialin Mao","Itay Griniasty","Yan Sun","Mark K. Transtrum","James P. Sethna","Pratik Chaudhari"],"url":"https://arxiv.org/abs/2505.08915"}
{"created":"2025-05-15","title":"A New Tractable Description Logic under Categorical Semantics","abstract":"Biomedical ontologies contain numerous concept or role names involving negative knowledge such as lacks_part, absence_of. Such a representation with labels rather than logical constructors would not allow a reasoner to interpret lacks_part as a kind of negation of has_part. It is known that adding negation to the tractable Description Logic (DL) EL allowing for conjunction, existential restriction and concept inclusion makes it intractable since the obtained logic includes implicitly disjunction and universal restriction which interact with other constructors. In this paper, we propose a new extension of EL with a weakened negation allowing to represent negative knowledge while retaining tractability. To this end, we introduce categorical semantics of all logical constructors of the DL SH including EL with disjunction, negation, universal restriction, role inclusion and transitive roles. The categorical semantics of a logical constructor is usually described as a set of categorical properties referring to several objects without using set membership. To restore tractability, we have to weaken semantics of disjunction and universal restriction by identifying \\emph{independent} categorical properties that are responsible for intractability, and dropping them from the set of categorical properties. We show that the logic resulting from weakening semantics is more expressive than EL with the bottom concept, transitive roles and role inclusion.","authors":["Chan Le Duc","Ludovic Brieulle"],"url":"https://arxiv.org/abs/2505.08916"}
{"created":"2025-05-15","title":"Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions","abstract":"High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical treatment planning for lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to assess the reconstruction comprehensively. Further, due to the absence of publicly available shape datasets to benchmark reconstruction algorithms, we developed a shape dataset named Lung3D, including the 3D models of 800 labeled pulmonary segments and the corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/M3DV/ImPulSe.","authors":["Kangxian Xie","Yufei Zhu","Kaiming Kuang","Li Zhang","Hongwei Bran Li","Mingchen Gao","Jiancheng Yang"],"url":"https://arxiv.org/abs/2505.08919"}
{"created":"2025-05-15","title":"Predictive Digital Twins with Quantified Uncertainty for Patient-Specific Decision Making in Oncology","abstract":"Quantifying the uncertainty in predictive models is critical for establishing trust and enabling risk-informed decision making for personalized medicine. In contrast to one-size-fits-all approaches that seek to mitigate risk at the population level, digital twins enable personalized modeling thereby potentially improving individual patient outcomes. Realizing digital twins in biomedicine requires scalable and efficient methods to integrate patient data with mechanistic models of disease progression. This study develops an end-to-end data-to-decisions methodology that combines longitudinal non-invasive imaging data with mechanistic models to estimate and predict spatiotemporal tumor progression accounting for patient-specific anatomy. Through the solution of a statistical inverse problem, imaging data inform the spatially varying parameters of a reaction-diffusion model of tumor progression. An efficient parallel implementation of the forward model coupled with a scalable approximation of the Bayesian posterior distribution enables rigorous, but tractable, quantification of uncertainty due to the sparse, noisy measurements. The methodology is verified on a virtual patient with synthetic data to control for model inadequacy, noise level, and the frequency of data collection. The application to decision-making is illustrated by evaluating the importance of imaging frequency and formulating an optimal experimental design question. The clinical relevance is demonstrated through a model validation study on a cohort of patients with publicly available longitudinal imaging data.","authors":["Graham Pash","Umberto Villa","David A. Hormuth II","Thomas E. Yankeelov","Karen Willcox"],"url":"https://arxiv.org/abs/2505.08927"}
{"created":"2025-05-15","title":"Parameter-Efficient Fine-Tuning of Vision Foundation Model for Forest Floor Segmentation from UAV Imagery","abstract":"Unmanned Aerial Vehicles (UAVs) are increasingly used for reforestation and forest monitoring, including seed dispersal in hard-to-reach terrains. However, a detailed understanding of the forest floor remains a challenge due to high natural variability, quickly changing environmental parameters, and ambiguous annotations due to unclear definitions. To address this issue, we adapt the Segment Anything Model (SAM), a vision foundation model with strong generalization capabilities, to segment forest floor objects such as tree stumps, vegetation, and woody debris. To this end, we employ parameter-efficient fine-tuning (PEFT) to fine-tune a small subset of additional model parameters while keeping the original weights fixed. We adjust SAM's mask decoder to generate masks corresponding to our dataset categories, allowing for automatic segmentation without manual prompting. Our results show that the adapter-based PEFT method achieves the highest mean intersection over union (mIoU), while Low-rank Adaptation (LoRA), with fewer parameters, offers a lightweight alternative for resource-constrained UAV platforms.","authors":["Mohammad Wasil","Ahmad Drak","Brennan Penfold","Ludovico Scarton","Maximilian Johenneken","Alexander Asteroth","Sebastian Houben"],"url":"https://arxiv.org/abs/2505.08932"}
{"created":"2025-05-15","title":"A Framework for Analysis of DEC Approximations to Hodge-Laplacian Problems using Generalized Whitney Forms","abstract":"We provide a framework for interpreting Discrete Exterior Calculus (DEC) numerical schemes in terms of Finite Element Exterior Calculus (FEEC). We demonstrate the equivalence of cochains on primal and dual meshes with Whitney and generalized Whitney forms which allows us to analyze DEC approximations using tools from FEEC. We demonstrate the applicability of our framework by rigorously proving convergence with rates for the Hodge-Laplacian problem in full $k$-form generality on well-centered meshes on contractible domains. We also provide numerical results illustrating optimality of our derived convergence rates. Moreover, we demonstrate how superconvergence phenomena can be explained in our framework with corresponding numerical results.","authors":["Johnny Guzm\\'an","Pratyush Potu"],"url":"https://arxiv.org/abs/2505.08934"}
{"created":"2025-05-15","title":"ATLAHS: An Application-centric Network Simulator Toolchain for AI, HPC, and Distributed Storage","abstract":"Network simulators play a crucial role in evaluating the performance of large-scale systems. However, existing simulators rely heavily on synthetic microbenchmarks or narrowly focus on specific domains, limiting their ability to provide comprehensive performance insights. In this work, we introduce ATLAHS, a flexible, extensible, and open-source toolchain designed to trace real-world applications and accurately simulate their workloads. ATLAHS leverages the GOAL format to model communication and computation patterns in AI, HPC, and distributed storage applications. It supports multiple network simulation backends and handles multi-job and multi-tenant scenarios. Through extensive validation, we demonstrate that ATLAHS achieves high accuracy in simulating realistic workloads (consistently less than 5% error), while significantly outperforming AstraSim, the current state-of-the-art AI systems simulator, in terms of simulation runtime and trace size efficiency. We further illustrate ATLAHS's utility via detailed case studies, highlighting the impact of congestion control algorithms on the performance of distributed storage systems, as well as the influence of job-placement strategies on application runtimes.","authors":["Siyuan Shen","Tommaso Bonato","Zhiyi Hu","Pasquale Jordan","Tiancheng Chen","Torsten Hoefler"],"url":"https://arxiv.org/abs/2505.08936"}
{"created":"2025-05-15","title":"Reduced Order Modeling for First Order Hyperbolic Systems with Application to Multiparameter Acoustic Waveform Inversion","abstract":"Waveform inversion seeks to estimate an inaccessible heterogeneous medium from data gathered by sensors that emit probing signals and measure the generated waves. It is an inverse problem for a second order wave equation or a first order hyperbolic system, with the sensor excitation modeled as a forcing term and the heterogeneous medium described by unknown, spatially variable coefficients. The traditional ``full waveform inversion\" (FWI) formulation estimates the unknown coefficients via minimization of the nonlinear, least squares data fitting objective function. For typical band-limited and high frequency data, this objective function has spurious local minima near and far from the true coefficients. Thus, FWI implemented with gradient based optimization algorithms may fail, even for good initial guesses. Recently, it was shown that it is possible to obtain a better behaved objective function for wave speed estimation, using data driven reduced order models (ROMs) that capture the propagation of pressure waves, governed by the classic second order wave equation. Here we introduce ROMs for vectorial waves, satisfying a general first order hyperbolic system. They are defined via Galerkin projection on the space spanned by the wave snapshots, evaluated on a uniform time grid with appropriately chosen time step. Our ROMs are data driven: They are computed in an efficient and non-iterative manner, from the sensor measurements, without knowledge of the medium and the snapshots. The ROM computation applies to any linear waves in lossless and non-dispersive media. For the inverse problem we focus attention on acoustic waves in a medium with unknown variable wave speed and density. We show that these can be determined via minimization of an objective function that uses a ROM based approximation of the vectorial wave field inside the inaccessible medium.","authors":["Liliana Borcea","Josselin Garnier","Alexander V. Mamonov","J\\\"orn Zimmerling"],"url":"https://arxiv.org/abs/2505.08937"}
{"created":"2025-05-15","title":"Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work","abstract":"As generative AI tools become integrated into design workflows, students increasingly engage with these tools not just as aids, but as collaborators. This study analyzes reflections from 33 student teams in an HCI design course to examine the kinds of judgments students make when using AI tools. We found both established forms of design judgment (e.g., instrumental, appreciative, quality) and emergent types: agency-distribution judgment and reliability judgment. These new forms capture how students negotiate creative responsibility with AI and assess the trustworthiness of its outputs. Our findings suggest that generative AI introduces new layers of complexity into design reasoning, prompting students to reflect not only on what AI produces, but also on how and when to rely on it. By foregrounding these judgments, we offer a conceptual lens for understanding how students engage in co-creative sensemaking with AI in design contexts.","authors":["Suchismita Naik","Prakash Shukla","Ike Obi","Jessica Backus","Nancy Rasche","Paul Parsons"],"url":"https://arxiv.org/abs/2505.08939"}
{"created":"2025-05-15","title":"NeurIPS 2024 Ariel Data Challenge: Characterisation of Exoplanetary Atmospheres Using a Data-Centric Approach","abstract":"The characterization of exoplanetary atmospheres through spectral analysis is a complex challenge. The NeurIPS 2024 Ariel Data Challenge, in collaboration with the European Space Agency's (ESA) Ariel mission, provided an opportunity to explore machine learning techniques for extracting atmospheric compositions from simulated spectral data. In this work, we focus on a data-centric business approach, prioritizing generalization over competition-specific optimization. We briefly outline multiple experimental axes, including feature extraction, signal transformation, and heteroskedastic uncertainty modeling. Our experiments demonstrate that uncertainty estimation plays a crucial role in the Gaussian Log-Likelihood (GLL) score, impacting performance by several percentage points. Despite improving the GLL score by 11%, our results highlight the inherent limitations of tabular modeling and feature engineering for this task, as well as the constraints of a business-driven approach within a Kaggle-style competition framework. Our findings emphasize the trade-offs between model simplicity, interpretability, and generalization in astrophysical data analysis.","authors":["Jeremie Blanchard","Lisa Casino","Jordan Gierschendorf"],"url":"https://arxiv.org/abs/2505.08940"}
{"created":"2025-05-15","title":"ForeCite: Adapting Pre-Trained Language Models to Predict Future Citation Rates of Academic Papers","abstract":"Predicting the future citation rates of academic papers is an important step toward the automation of research evaluation and the acceleration of scientific progress. We present $\\textbf{ForeCite}$, a simple but powerful framework to append pre-trained causal language models with a linear head for average monthly citation rate prediction. Adapting transformers for regression tasks, ForeCite achieves a test correlation of $\\rho = 0.826$ on a curated dataset of 900K+ biomedical papers published between 2000 and 2024, a 27-point improvement over the previous state-of-the-art. Comprehensive scaling-law analysis reveals consistent gains across model sizes and data volumes, while temporal holdout experiments confirm practical robustness. Gradient-based saliency heatmaps suggest a potentially undue reliance on titles and abstract texts. These results establish a new state-of-the-art in forecasting the long-term influence of academic research and lay the groundwork for the automated, high-fidelity evaluation of scientific contributions.","authors":["Gavin Hull","Alex Bihlo"],"url":"https://arxiv.org/abs/2505.08941"}
{"created":"2025-05-15","title":"Toward Cost-Efficient Serving of Mixture-of-Experts with Asynchrony","abstract":"Mixture-of-Experts (MoE) architectures offer the promise of larger model capacity without the prohibitive costs of fully dense designs. However, in real-world inference serving, load skew across experts often leads to suboptimal device utilization and excessive synchronization overheads. This paper introduces Asynchronous Expert Parallelism (AEP), a new paradigm that decouples layer execution from barrier-style synchronization. By dynamically queuing tokens at each layer (referred to as $\\mu$-queuing) and adaptively re-batching them on demand, GPUs avoid waiting for straggling experts and instead continuously process whichever layer is ready. This asynchronous approach mitigates two major inefficiencies in traditional expert-parallel systems: (1) idle GPU time while waiting for the hottest expert, and (2) small-batch executions on colder experts that waste memory bandwidth.","authors":["Shaoyu Wang","Guangrong He","Geon-Woo Kim","Yanqi Zhou","Seo Jin Park"],"url":"https://arxiv.org/abs/2505.08944"}
{"created":"2025-05-15","title":"Multi-step manipulation task and motion planning guided by video demonstration","abstract":"This work aims to leverage instructional video to solve complex multi-step task-and-motion planning tasks in robotics. Towards this goal, we propose an extension of the well-established Rapidly-Exploring Random Tree (RRT) planner, which simultaneously grows multiple trees around grasp and release states extracted from the guiding video. Our key novelty lies in combining contact states and 3D object poses extracted from the guiding video with a traditional planning algorithm that allows us to solve tasks with sequential dependencies, for example, if an object needs to be placed at a specific location to be grasped later. We also investigate the generalization capabilities of our approach to go beyond the scene depicted in the instructional video. To demonstrate the benefits of the proposed video-guided planning approach, we design a new benchmark with three challenging tasks: (I) 3D re-arrangement of multiple objects between a table and a shelf, (ii) multi-step transfer of an object through a tunnel, and (iii) transferring objects using a tray similar to a waiter transfers dishes. We demonstrate the effectiveness of our planning algorithm on several robots, including the Franka Emika Panda and the KUKA KMR iiwa. For a seamless transfer of the obtained plans to the real robot, we develop a trajectory refinement approach formulated as an optimal control problem (OCP).","authors":["Kateryna Zorina","David Kovar","Mederic Fourmy","Florent Lamiraux","Nicolas Mansard","Justin Carpentier","Josef Sivic","Vladimir Petrik"],"url":"https://arxiv.org/abs/2505.08949"}
{"created":"2025-05-15","title":"Even Faster Algorithm for the Chamfer Distance","abstract":"For two d-dimensional point sets A, B of size up to n, the Chamfer distance from A to B is defined as CH(A,B) = \\sum_{a \\in A} \\min_{b \\in B} \\|a-b\\|. The Chamfer distance is a widely used measure for quantifying dissimilarity between sets of points, used in many machine learning and computer vision applications. A recent work of Bakshi et al, NeuriPS'23, gave the first near-linear time (1+eps)-approximate algorithm, with a running time of O(ndlog(n)/eps^2). In this paper we improve the running time further, to O(nd(loglog(n)+log(1/eps))/eps^2). When eps is a constant, this reduces the gap between the upper bound and the trivial Omega(dn) lower bound significantly, from O(log n) to O(loglog n).","authors":["Ying Feng","Piotr Indyk"],"url":"https://arxiv.org/abs/2505.08957"}
{"created":"2025-05-15","title":"Adaptive Entanglement Generation for Quantum Routing","abstract":"Entanglement generation in long-distance quantum networks is a difficult process due to resource limitations and the probabilistic nature of entanglement swapping. To maximize success probability, existing quantum routing algorithms employ computationally expensive solutions (e.g., linear programming) to determine which links to entangle and use for end-to-end entanglement generation. Such optimization methods, however, cannot meet the delay requirements of real-world quantum networks, necessitating swift yet efficient real-time optimization models. In this paper, we propose reinforcement learning (RL)-based models to determine which links to entangle and proactively swap to meet connection requests. We show that the proposed RL-based approach is 20x faster compared to linear programming. Moreover, we show that one can take advantage of the longevity of entanglements to (i) cache entangled links for future use and (ii) proactively swap entanglement on high-demand path segments, thereby increasing the likelihood of request success. Through comprehensive simulations, we demonstrate that caching unused entanglements leads to a 10-15% improvement in the performance of state-of-the-art quantum routing algorithms. Complementing caching with proactive entanglement swapping further enhances the request success rate by up to 52.55%.","authors":["Tasdiqul Islam","Md Arifuzzaman","Engin Arslan"],"url":"https://arxiv.org/abs/2505.08958"}
{"created":"2025-05-15","title":"Differentiable Channel Selection in Self-Attention For Person Re-Identification","abstract":"In this paper, we propose a novel attention module termed the Differentiable Channel Selection Attention module, or the DCS-Attention module. In contrast with conventional self-attention, the DCS-Attention module features selection of informative channels in the computation of the attention weights. The selection of the feature channels is performed in a differentiable manner, enabling seamless integration with DNN training. Our DCS-Attention is compatible with either fixed neural network backbones or learnable backbones with Differentiable Neural Architecture Search (DNAS), leading to DCS with Fixed Backbone (DCS-FB) and DCS-DNAS, respectively. Importantly, our DCS-Attention is motivated by the principle of Information Bottleneck (IB), and a novel variational upper bound for the IB loss, which can be optimized by SGD, is derived and incorporated into the training loss of the networks with the DCS-Attention modules. In this manner, a neural network with DCS-Attention modules is capable of selecting the most informative channels for feature extraction so that it enjoys state-of-the-art performance for the Re-ID task. Extensive experiments on multiple person Re-ID benchmarks using both DCS-FB and DCS-DNAS show that DCS-Attention significantly enhances the prediction accuracy of DNNs for person Re-ID, which demonstrates the effectiveness of DCS-Attention in learning discriminative features critical to identifying person identities. The code of our work is available at https://github.com/Statistical-Deep-Learning/DCS-Attention.","authors":["Yancheng Wang","Nebojsa Jojic","Yingzhen Yang"],"url":"https://arxiv.org/abs/2505.08961"}
{"created":"2025-05-15","title":"GPML: Graph Processing for Machine Learning","abstract":"The dramatic increase of complex, multi-step, and rapidly evolving attacks in dynamic networks involves advanced cyber-threat detectors. The GPML (Graph Processing for Machine Learning) library addresses this need by transforming raw network traffic traces into graph representations, enabling advanced insights into network behaviors. The library provides tools to detect anomalies in interaction and community shifts in dynamic networks. GPML supports community and spectral metrics extraction, enhancing both real-time detection and historical forensics analysis. This library supports modern cybersecurity challenges with a robust, graph-based approach.","authors":["Majed Jaber","Julien Michel","Nicolas Boutry","Pierre Parrend"],"url":"https://arxiv.org/abs/2505.08964"}
{"created":"2025-05-15","title":"To Stay or to Bypass: Unraveling Mainline Vehicles' Aggregate Strategic Decision-Making at Highway Weaving Ramps","abstract":"The weaving ramp scenario is a critical bottleneck in highway networks due to conflicting flows and complex interactions among merging, exiting, and through vehicles. In this work, we propose a game-theoretic model to capture and predict the aggregate lane choice behavior of mainline through vehicles as they approach the weaving zone. Faced with potential conflicts from merging and exiting vehicles, mainline vehicles can either bypass the conflict zone by changing to an adjacent lane or stay steadfast in their current lane. Our model effectively captures these strategic choices using a small set of parameters, requiring only limited traffic measurements for calibration. The model's validity is demonstrated through SUMO simulations, achieving high predictive accuracy. The simplicity and flexibility of the proposed framework make it a practical tool for analyzing bottleneck weaving scenarios and informing traffic management strategies.","authors":["Haohui He","Kexin Wang","Ruolin Li"],"url":"https://arxiv.org/abs/2505.08965"}
{"created":"2025-05-15","title":"Convergence and Stability of Discrete Exterior Calculus for the Hodge Laplace Problem in Two Dimensions","abstract":"We prove convergence and stability of the discrete exterior calculus (DEC) solutions for the Hodge-Laplace problems in two dimensions for families of meshes that are non-degenerate Delaunay and shape regular. We do this by relating the DEC solutions to the lowest order finite element exterior calculus (FEEC) solutions. A Poincar\\'e inequality and a discrete inf-sup condition for DEC are part of this proof. We also prove that under appropriate geometric conditions on the mesh the DEC and FEEC norms are equivalent. Only one side of the norm equivalence is needed for proving stability and convergence and this allows us to relax the conditions on the meshes.","authors":["Chengbin Zhu","Snorre H. Christiansen","Kaibo Hu","Anil N. Hirani"],"url":"https://arxiv.org/abs/2505.08966"}
{"created":"2025-05-15","title":"On a Modified Random Genetic Drift Model: Derivation and a Structure-Preserving Operator-Splitting Discretization","abstract":"One of the fundamental mathematical models for studying random genetic drift is the Kimura equation, derived as the large-population limit of the discrete Wright-Fisher model. However, due to the degeneracy of the diffusion coefficient, it is impossible to impose a suitable boundary condition that ensures the Kimura equation admits a classical solution while preserving biological significance. In this work, we propose a modified model for random genetic drift that admits classical solutions by modifying the domain of the Kimura equation from $(0, 1)$ to $(\\delta, 1 - \\delta)$ with $\\delta$ being a small parameter, which allows us to impose a Robin-type boundary condition. By introducing two additional variables for the probabilities in the boundary region, we effectively capture the conservation of mass and the fixation dynamics in the original model. To numerically investigate the modified model, we develop a hybrid Eulerian-Lagrangian operator splitting scheme. The scheme first solves the flow map equation in the bulk region using a Lagrangian approach with a no-flux boundary condition, followed by handling the boundary dynamics in Eulerian coordinates. This hybrid scheme ensures mass conservation, maintains positivity, and preserves the first moment. Various numerical tests demonstrate the efficiency, accuracy, and structure-preserving properties of the proposed scheme. Numerical results demonstrate the key qualitative features of the original Kimura equation, including the fixation behavior and the correct stationary distribution in the small-$\\delta$ limit.","authors":["Chi-An Chen","Chun Liu","Yiwei Wang"],"url":"https://arxiv.org/abs/2505.08969"}
{"created":"2025-05-15","title":"Approximation of viscous transport and conservative equations with one sided Lipschitz velocity fields","abstract":"The aim of this work is to investigate semi-Lagrangian approximation schemes on unstructured grids for viscous transport and conservative equations with measurable coefficients that satisfy a one-sided Lipschitz condition. To establish the convergence of the schemes, we exploit the characterization of the solution for these equations expressed in terms of measurable time-dependent viscosity solution and, respectively, duality solution. We supplement our theoretical analysis with various numerical examples to illustrate the features of the schemes.","authors":["Fabio Camilli","Adriano Festa","Luciano Marzufero"],"url":"https://arxiv.org/abs/2505.08970"}
{"created":"2025-05-15","title":"Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training","abstract":"In standard large vision-language models (LVLMs) pre-training, the model typically maximizes the joint probability of the caption conditioned on the image via next-token prediction (NTP); however, since only a small subset of caption tokens directly relates to the visual content, this naive NTP unintentionally fits the model to noise and increases the risk of hallucination. We present PRIOR, a simple vision-language pre-training approach that addresses this issue by prioritizing image-related tokens through differential weighting in the NTP loss, drawing from the importance sampling framework. PRIOR introduces a reference model-a text-only large language model (LLM) trained on the captions without image inputs, to weight each token based on its probability for LVLMs training. Intuitively, tokens that are directly related to the visual inputs are harder to predict without the image and thus receive lower probabilities from the text-only reference LLM. During training, we implement a token-specific re-weighting term based on the importance scores to adjust each token's loss. We implement PRIOR in two distinct settings: LVLMs with visual encoders and LVLMs without visual encoders. We observe 19% and 8% average relative improvement, respectively, on several vision-language benchmarks compared to NTP. In addition, PRIOR exhibits superior scaling properties, as demonstrated by significantly higher scaling coefficients, indicating greater potential for performance gains compared to NTP given increasing compute and data.","authors":["Yangyi Chen","Hao Peng","Tong Zhang","Heng Ji"],"url":"https://arxiv.org/abs/2505.08971"}
{"created":"2025-05-15","title":"SaFARi: State-Space Models for Frame-Agnostic Representation","abstract":"State-Space Models (SSMs) have re-emerged as a powerful tool for online function approximation, and as the backbone of machine learning models for long-range dependent data. However, to date, only a few polynomial bases have been explored for this purpose, and the state-of-the-art implementations were built upon the best of a few limited options. In this paper, we present a generalized method for building an SSM with any frame or basis, rather than being restricted to polynomials. This framework encompasses the approach known as HiPPO, but also permits an infinite diversity of other possible \"species\" within the SSM architecture. We dub this approach SaFARi: SSMs for Frame-Agnostic Representation.","authors":["Hossein Babaei","Mel White","Sina Alemohammad","Richard G. Baraniuk"],"url":"https://arxiv.org/abs/2505.08977"}
{"created":"2025-05-15","title":"Inference Attacks for X-Vector Speaker Anonymization","abstract":"We revisit the privacy-utility tradeoff of x-vector speaker anonymization. Existing approaches quantify privacy through training complex speaker verification or identification models that are later used as attacks. Instead, we propose a novel inference attack for de-anonymization. Our attack is simple and ML-free yet we show experimentally that it outperforms existing approaches.","authors":["Luke Bauer","Wenxuan Bao","Malvika Jadhav","Vincent Bindschaedler"],"url":"https://arxiv.org/abs/2505.08978"}
{"created":"2025-05-15","title":"ITERA-LLM: Boosting Sub-8-Bit Large Language Model Inference via Iterative Tensor Decomposition","abstract":"Recent advancements in Large Language Models (LLMs) have demonstrated impressive capabilities as their scale expands to billions of parameters. Deploying these large-scale models on resource-constrained platforms presents significant challenges, with post-training fixed-point quantization often used as a model compression technique. However, quantization-only methods typically lead to significant accuracy degradation in LLMs when precision falls below 8 bits. This paper addresses this challenge through a software-hardware co-design framework, ITERA-LLM, which integrates sub-8-bit quantization with SVD-based iterative low-rank tensor decomposition for error compensation, leading to higher compression ratios and reduced computational complexity. The proposed approach is complemented by a hardware-aware Design Space Exploration (DSE) process that optimizes accuracy, latency, and resource utilization, tailoring the configuration to the specific requirements of the targeted LLM. Our results show that ITERA-LLM achieves linear layer latency reduction of up to 41.1%, compared to quantization-only baseline approach while maintaining similar model accuracy.","authors":["Keran Zheng","Yinting Huang","Zhewen Yu","Christos-Savvas Bouganis"],"url":"https://arxiv.org/abs/2505.08981"}
{"created":"2025-05-15","title":"Model-free Online Learning for the Kalman Filter: Forgetting Factor and Logarithmic Regret","abstract":"We consider the problem of online prediction for an unknown, non-explosive linear stochastic system. With a known system model, the optimal predictor is the celebrated Kalman filter. In the case of unknown systems, existing approaches based on recursive least squares and its variants may suffer from degraded performance due to the highly imbalanced nature of the regression model. This imbalance can easily lead to overfitting and thus degrade prediction accuracy. We tackle this problem by injecting an inductive bias into the regression model via {exponential forgetting}. While exponential forgetting is a common wisdom in online learning, it is typically used for re-weighting data. In contrast, our approach focuses on balancing the regression model. This achieves a better trade-off between {regression} and {regularization errors}, and simultaneously reduces the {accumulation error}. With new proof techniques, we also provide a sharper logarithmic regret bound of $O(\\log^3 N)$, where $N$ is the number of observations.","authors":["Jiachen Qian","Yang Zheng"],"url":"https://arxiv.org/abs/2505.08982"}
{"created":"2025-05-15","title":"Position-Normal Manifold for Efficient Glint Rendering on High-Resolution Normal Maps","abstract":"Detailed microstructures on specular objects often exhibit intriguing glinty patterns under high-frequency lighting, which is challenging to render using a conventional normal-mapped BRDF. In this paper, we present a manifold-based formulation of the glint normal distribution functions (NDF) that precisely captures the surface normal distributions over queried footprints. The manifold-based formulation transfers the integration for the glint NDF construction to a problem of mesh intersections. Compared to previous works that rely on complex numerical approximations, our integral solution is exact and much simpler to compute, which also allows an easy adaptation of a mesh clustering hierarchy to accelerate the NDF evaluation of large footprints. Our performance and quality analysis shows that our NDF formulation achieves similar glinty appearance compared to the baselines but is an order of magnitude faster. Within this framework, we further present a novel derivation of analytical shadow-masking for normal-mapped diffuse surfaces -- a component that is often ignored in previous works.","authors":["Liwen Wu","Fujun Luan","Milo\\v{s} Ha\\v{s}an","Ravi Ramamoorthi"],"url":"https://arxiv.org/abs/2505.08985"}
{"created":"2025-05-15","title":"ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation","abstract":"Automated poultry processing lines still rely on humans to lift slippery, easily bruised carcasses onto a shackle conveyor. Deformability, anatomical variance, and strict hygiene rules make conventional suction and scripted motions unreliable. We present ChicGrasp, an end--to--end hardware--software co-design for this task. An independently actuated dual-jaw pneumatic gripper clamps both chicken legs, while a conditional diffusion-policy controller, trained from only 50 multi--view teleoperation demonstrations (RGB + proprioception), plans 5 DoF end--effector motion, which includes jaw commands in one shot. On individually presented raw broiler carcasses, our system achieves a 40.6\\% grasp--and--lift success rate and completes the pick to shackle cycle in 38 s, whereas state--of--the--art implicit behaviour cloning (IBC) and LSTM-GMM baselines fail entirely. All CAD, code, and datasets will be open-source. ChicGrasp shows that imitation learning can bridge the gap between rigid hardware and variable bio--products, offering a reproducible benchmark and a public dataset for researchers in agricultural engineering and robot learning.","authors":["Amirreza Davar","Zhengtong Xu","Siavash Mahmoudi","Pouya Sohrabipour","Chaitanya Pallerla","Yu She","Wan Shou","Philip Crandall","Dongyi Wang"],"url":"https://arxiv.org/abs/2505.08986"}
{"created":"2025-05-15","title":"Generalization in Monitored Markov Decision Processes (Mon-MDPs)","abstract":"Reinforcement learning (RL) typically models the interaction between the agent and environment as a Markov decision process (MDP), where the rewards that guide the agent's behavior are always observable. However, in many real-world scenarios, rewards are not always observable, which can be modeled as a monitored Markov decision process (Mon-MDP). Prior work on Mon-MDPs have been limited to simple, tabular cases, restricting their applicability to real-world problems. This work explores Mon-MDPs using function approximation (FA) and investigates the challenges involved. We show that combining function approximation with a learned reward model enables agents to generalize from monitored states with observable rewards, to unmonitored environment states with unobservable rewards. Therefore, we demonstrate that such generalization with a reward model achieves near-optimal policies in environments formally defined as unsolvable. However, we identify a critical limitation of such function approximation, where agents incorrectly extrapolate rewards due to overgeneralization, resulting in undesirable behaviors. To mitigate overgeneralization, we propose a cautious police optimization method leveraging reward uncertainty. This work serves as a step towards bridging this gap between Mon-MDP theory and real-world applications.","authors":["Montaser Mohammedalamen","Michael Bowling"],"url":"https://arxiv.org/abs/2505.08988"}
{"created":"2025-05-15","title":"Toward Accessible and Safe Live Streaming Using Distributed Content Filtering with MoQ","abstract":"Live video streaming is increasingly popular on social media platforms. With the growth of live streaming comes an increased need for robust content moderation to remove dangerous, illegal, or otherwise objectionable content. Whereas video on demand distribution enables offline content analysis, live streaming imposes restrictions on latency for both analysis and distribution. In this paper, we present extensions to the in-progress Media Over QUIC Transport protocol that enable real-time content moderation in one-to-many video live streams. Importantly, our solution removes only the video segments that contain objectionable content, allowing playback resumption as soon as the stream conforms to content policies again. Content analysis tasks may be transparently distributed to arbitrary client devices. We implement and evaluate our system in the context of light strobe removal for photosensitive viewers, finding that streaming clients experience an increased latency of only one group-of-pictures duration.","authors":["Andrew C. Freeman"],"url":"https://arxiv.org/abs/2505.08990"}
{"created":"2025-05-15","title":"Dataflow & Tiling Strategies in Edge-AI FPGA Accelerators: A Comprehensive Literature Review","abstract":"Edge-AI applications demand high-throughput, low-latency inference on FPGAs under tight resource and power constraints. This survey provides a comprehensive review of two key architectural decisions for FPGA-based neural network accelerators: (i) the dataflow (the order and manner in which data is moved and reused on chip), and (ii) the tiling/blocking strategy (how large tensors are partitioned to fit on-chip). We first present a broadened taxonomy of canonical dataflow styles: Weight-Stationary, Output-Stationary, Row-Stationary, and No-Local-Reuse, including formal definitions, pseudocode/diagrams, and real FPGA examples. We then discuss analytical frameworks (MAESTRO, Timeloop) and compare them with a concise feature table, illustrating how they model reuse, performance, and hardware costs. Next, we detail multi-level tiling and loop unrolling/pipelining strategies for FPGAs, clarifying how each memory tier (registers, LUTRAM, BRAM, HBM) can be exploited. Our four case studies - FINN, FINN-R, FlightLLM, and SSR - highlight distinct dataflows (from binary streaming to hybrid sparse transformations) and tiling patterns. We include a unified comparison matrix covering platform, precision, throughput, resource utilization, and energy efficiency, plus small block diagrams for each design. We conclude by examining design automation trade-offs among HLS, DSL, and hand-coded RTL, offering a \"lessons learned\" summary box, and charting future research directions in partial reconfiguration, hybrid dataflows, and domain-specific compiler flows for next-generation edge AI FPGA accelerators.","authors":["Zhaoqin Li"],"url":"https://arxiv.org/abs/2505.08992"}
{"created":"2025-05-15","title":"Enhancing Aerial Combat Tactics through Hierarchical Multi-Agent Reinforcement Learning","abstract":"This work presents a Hierarchical Multi-Agent Reinforcement Learning framework for analyzing simulated air combat scenarios involving heterogeneous agents. The objective is to identify effective Courses of Action that lead to mission success within preset simulations, thereby enabling the exploration of real-world defense scenarios at low cost and in a safe-to-fail setting. Applying deep Reinforcement Learning in this context poses specific challenges, such as complex flight dynamics, the exponential size of the state and action spaces in multi-agent systems, and the capability to integrate real-time control of individual units with look-ahead planning. To address these challenges, the decision-making process is split into two levels of abstraction: low-level policies control individual units, while a high-level commander policy issues macro commands aligned with the overall mission targets. This hierarchical structure facilitates the training process by exploiting policy symmetries of individual agents and by separating control from command tasks. The low-level policies are trained for individual combat control in a curriculum of increasing complexity. The high-level commander is then trained on mission targets given pre-trained control policies. The empirical validation confirms the advantages of the proposed framework.","authors":["Ardian Selmonaj","Oleg Szehr","Giacomo Del Rio","Alessandro Antonucci","Adrian Schneider","Michael R\\\"uegsegger"],"url":"https://arxiv.org/abs/2505.08995"}
{"created":"2025-05-15","title":"A suite of LMs comprehend puzzle statements as well as humans","abstract":"Recent claims suggest that large language models (LMs) underperform humans in comprehending minimally complex English statements (Dentella et al., 2024). Here, we revisit those findings and argue that human performance was overestimated, while LLM abilities were underestimated. Using the same stimuli, we report a preregistered study comparing human responses in two conditions: one allowed rereading (replicating the original study), and one that restricted rereading (a more naturalistic comprehension test). Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect accuracy. Results further show that both humans and models are disproportionately challenged by queries involving potentially reciprocal actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than model-specific deficits. Additional analyses using Llama-2-70B log probabilities, a recoding of open-ended model responses, and grammaticality ratings of other sentences reveal systematic underestimation of model performance. We find that GPT-4o can align with either naive or expert grammaticality judgments, depending on prompt framing. These findings underscore the need for more careful experimental design and coding practices in LLM evaluation, and they challenge the assumption that current models are inherently weaker than humans at language comprehension.","authors":["Adele E Goldberg","Supantho Rakshit","Jennifer Hu","Kyle Mahowald"],"url":"https://arxiv.org/abs/2505.08996"}
{"created":"2025-05-15","title":"Neural BRDF Importance Sampling by Reparameterization","abstract":"Neural bidirectional reflectance distribution functions (BRDFs) have emerged as popular material representations for enhancing realism in physically-based rendering. Yet their importance sampling remains a significant challenge. In this paper, we introduce a reparameterization-based formulation of neural BRDF importance sampling that seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples. The reparameterization-based formulation transfers the distribution learning task to a problem of identifying BRDF integral substitutions. In contrast to previous methods that rely on invertible networks and multi-step inference to reconstruct BRDF distributions, our model removes these constraints, which offers greater flexibility and efficiency. Our variance and performance analysis demonstrates that our reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.","authors":["Liwen Wu","Sai Bi","Zexiang Xu","Hao Tan","Kai Zhang","Fujun Luan","Haolin Lu","Ravi Ramamoorthi"],"url":"https://arxiv.org/abs/2505.08998"}
{"created":"2025-05-15","title":"Towards Adaptive Meta-Gradient Adversarial Examples for Visual Tracking","abstract":"In recent years, visual tracking methods based on convolutional neural networks and Transformers have achieved remarkable performance and have been successfully applied in fields such as autonomous driving. However, the numerous security issues exposed by deep learning models have gradually affected the reliable application of visual tracking methods in real-world scenarios. Therefore, how to reveal the security vulnerabilities of existing visual trackers through effective adversarial attacks has become a critical problem that needs to be addressed. To this end, we propose an adaptive meta-gradient adversarial attack (AMGA) method for visual tracking. This method integrates multi-model ensembles and meta-learning strategies, combining momentum mechanisms and Gaussian smoothing, which can significantly enhance the transferability and attack effectiveness of adversarial examples. AMGA randomly selects models from a large model repository, constructs diverse tracking scenarios, and iteratively performs both white- and black-box adversarial attacks in each scenario, optimizing the gradient directions of each model. This paradigm minimizes the gap between white- and black-box adversarial attacks, thus achieving excellent attack performance in black-box scenarios. Extensive experimental results on large-scale datasets such as OTB2015, LaSOT, and GOT-10k demonstrate that AMGA significantly improves the attack performance, transferability, and deception of adversarial examples. Codes and data are available at https://github.com/pgao-lab/AMGA.","authors":["Wei-Long Tian","Peng Gao","Xiao Liu","Long Xu","Hamido Fujita","Hanan Aljuai","Mao-Li Wang"],"url":"https://arxiv.org/abs/2505.08999"}
{"created":"2025-05-15","title":"Cyclic system for an algebraic theory of alternating parity automata","abstract":"$\\omega$-regular languages are a natural extension of the regular languages to the setting of infinite words. Likewise, they are recognised by a host of automata models, one of the most important being Alternating Parity Automata (APAs), a generalisation of B\\\"uchi automata that symmetrises both the transitions (with universal as well as existential branching) and the acceptance condition (by a parity condition).","authors":["Anupam Das","Abhishek De"],"url":"https://arxiv.org/abs/2505.09000"}
{"created":"2025-05-15","title":"SAFE-SiP: Secure Authentication Framework for System-in-Package Using Multi-party Computation","abstract":"The emergence of chiplet-based heterogeneous integration is transforming the semiconductor, AI, and high-performance computing industries by enabling modular designs and improved scalability. However, assembling chiplets from multiple vendors after fabrication introduces a complex supply chain that raises serious security concerns, including counterfeiting, overproduction, and unauthorized access. Current solutions often depend on dedicated security chiplets or changes to the timing flow, which assume a trusted SiP integrator. This assumption can expose chiplet signatures to other vendors and create new attack surfaces. This work addresses those vulnerabilities using Multi-party Computation (MPC), which enables zero-trust authentication without disclosing sensitive information to any party. We present SAFE-SiP, a scalable authentication framework that garbles chiplet signatures and uses MPC for verifying integrity, effectively blocking unauthorized access and adversarial inference. SAFE-SiP removes the need for a dedicated security chiplet and ensures secure authentication, even in untrusted integration scenarios. We evaluated SAFE-SiP on five RISC-V-based System-in-Package (SiP) designs. Experimental results show that SAFE-SiP incurs minimal power overhead, an average area overhead of only 3.05%, and maintains a computational complexity of 2^192, offering a highly efficient and scalable security solution.","authors":["Ishraq Tashdid","Tasnuva Farheen","Sazadur Rahman"],"url":"https://arxiv.org/abs/2505.09002"}
{"created":"2025-05-15","title":"Continual Reinforcement Learning via Autoencoder-Driven Task and New Environment Recognition","abstract":"Continual learning for reinforcement learning agents remains a significant challenge, particularly in preserving and leveraging existing information without an external signal to indicate changes in tasks or environments. In this study, we explore the effectiveness of autoencoders in detecting new tasks and matching observed environments to previously encountered ones. Our approach integrates policy optimization with familiarity autoencoders within an end-to-end continual learning system. This system can recognize and learn new tasks or environments while preserving knowledge from earlier experiences and can selectively retrieve relevant knowledge when re-encountering a known environment. Initial results demonstrate successful continual learning without external signals to indicate task changes or reencounters, showing promise for this methodology.","authors":["Zeki Doruk Erden","Donia Gasmi","Boi Faltings"],"url":"https://arxiv.org/abs/2505.09003"}
{"created":"2025-05-15","title":"For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies","abstract":"It remains debated how well any LM understands natural language or generates reliable metalinguistic judgments. Moreover, relatively little work has demonstrated that LMs can represent and respect subtle relationships between form and function proposed by linguists. We here focus on a particular such relationship established in recent work: English speakers' judgments about the information structure of canonical sentences predicts independently collected acceptability ratings on corresponding 'long distance dependency' [LDD] constructions, across a wide array of base constructions and multiple types of LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on the same tasks used with humans and new extensions.Results reveal reliable metalinguistic skill on the information structure and acceptability tasks, replicating a striking interaction between the two, despite the zero-shot, explicit nature of the tasks, and little to no chance of contamination [Studies 1a, 1b]. Study 2 manipulates the information structure of base sentences and confirms a causal relationship: increasing the prominence of a constituent in a context sentence increases the subsequent acceptability ratings on an LDD construction. The findings suggest a tight relationship between natural and GPT-4 generated English, and between information structure and syntax, which begs for further exploration.","authors":["Nicole Cuneo","Eleanor Graves","Supantho Rakshit","Adele E. Goldberg"],"url":"https://arxiv.org/abs/2505.09005"}
{"created":"2025-05-15","title":"Vendi Information Gain: An Alternative To Mutual Information For Science And Machine Learning","abstract":"In his 1948 seminal paper A Mathematical Theory of Communication that birthed information theory, Claude Shannon introduced mutual information (MI), which he called \"rate of transmission\", as a way to quantify information gain (IG) and defined it as the difference between the marginal and conditional entropy of a random variable. While MI has become a standard tool in science and engineering, it has several shortcomings. First, MI is often intractable - it requires a density over samples with tractable Shannon entropy - and existing techniques for approximating it often fail, especially in high dimensions. Moreover, in settings where MI is tractable, its symmetry and insensitivity to sample similarity are undesirable. In this paper, we propose the Vendi Information Gain (VIG), a novel alternative to MI that leverages the Vendi scores, a flexible family of similarity-based diversity metrics. We call the logarithm of the VS the Vendi entropy and define VIG as the difference between the marginal and conditional Vendi entropy of a variable. Being based on the VS, VIG accounts for similarity. Furthermore, VIG generalizes MI and recovers it under the assumption that the samples are completely dissimilar. Importantly, VIG only requires samples and not a probability distribution over them. Finally, it is asymmetric, a desideratum for a good measure of IG that MI fails to meet. VIG extends information theory to settings where MI completely fails. For example, we use VIG to describe a novel, unified framework for active data acquisition, a popular paradigm of modern data-driven science. We demonstrate the advantages of VIG over MI in diverse applications, including in cognitive science to model human response times to external stimuli and in epidemiology to learn epidemic processes and identify disease hotspots in different countries via level-set estimation.","authors":["Quan Nguyen","Adji Bousso Dieng"],"url":"https://arxiv.org/abs/2505.09007"}
{"created":"2025-05-15","title":"Fully Dynamic Euclidean Bi-Chromatic Matching in Sublinear Update Time","abstract":"We consider the Euclidean bi-chromatic matching problem in the dynamic setting, where the goal is to efficiently process point insertions and deletions while maintaining a high-quality solution. Computing the minimum cost bi-chromatic matching is one of the core problems in geometric optimization that has found many applications, most notably in estimating Wasserstein distance between two distributions. In this work, we present the first fully dynamic algorithm for Euclidean bi-chromatic matching with sub-linear update time. For any fixed $\\varepsilon > 0$, our algorithm achieves $O(1/\\varepsilon)$-approximation and handles updates in $O(n^{\\varepsilon})$ time. Our experiments show that our algorithm enables effective monitoring of the distributional drift in the Wasserstein distance on real and synthetic data sets, while outperforming the runtime of baseline approximations by orders of magnitudes.","authors":["Gramoz Goranci","Peter Kiss","Neel Patel","Martin P. Seybold","Eva Szilagyi","Da Wei Zheng"],"url":"https://arxiv.org/abs/2505.09010"}
{"created":"2025-05-15","title":"Signal-based AI-driven software solution for automated quantification of metastatic bone disease and treatment response assessment using Whole-Body Diffusion-Weighted MRI (WB-DWI) biomarkers in Advanced Prostate Cancer","abstract":"We developed an AI-driven software solution to quantify metastatic bone disease from WB-DWI scans. Core technologies include: (i) a weakly-supervised Residual U-Net model generating a skeleton probability map to isolate bone; (ii) a statistical framework for WB-DWI intensity normalisation, obtaining a signal-normalised b=900s/mm^2 (b900) image; and (iii) a shallow convolutional neural network that processes outputs from (i) and (ii) to generate a mask of suspected bone lesions, characterised by higher b900 signal intensity due to restricted water diffusion. This mask is applied to the gADC map to extract TDV and gADC statistics. We tested the tool using expert-defined metastatic bone disease delineations on 66 datasets, assessed repeatability of imaging biomarkers (N=10), and compared software-based response assessment with a construct reference standard based on clinical, laboratory and imaging assessments (N=118). Dice score between manual and automated delineations was 0.6 for lesions within pelvis and spine, with an average surface distance of 2mm. Relative differences for log-transformed TDV (log-TDV) and median gADC were below 9% and 5%, respectively. Repeatability analysis showed coefficients of variation of 4.57% for log-TDV and 3.54% for median gADC, with intraclass correlation coefficients above 0.9. The software achieved 80.5% accuracy, 84.3% sensitivity, and 85.7% specificity in assessing response to treatment compared to the construct reference standard. Computation time generating a mask averaged 90 seconds per scan. Our software enables reproducible TDV and gADC quantification from WB-DWI scans for monitoring metastatic bone disease response, thus providing potentially useful measurements for clinical decision-making in APC patients.","authors":["Antonio Candito","Matthew D Blackledge","Richard Holbrey","Nuria Porta","Ana Ribeiro","Fabio Zugni","Luca D'Erme","Francesca Castagnoli","Alina Dragan","Ricardo Donners","Christina Messiou","Nina Tunariu","Dow-Mu Koh"],"url":"https://arxiv.org/abs/2505.09011"}
{"created":"2025-05-15","title":"Deep Reinforcement Learning for Power Grid Multi-Stage Cascading Failure Mitigation","abstract":"Cascading failures in power grids can lead to grid collapse, causing severe disruptions to social operations and economic activities. In certain cases, multi-stage cascading failures can occur. However, existing cascading-failure-mitigation strategies are usually single-stage-based, overlooking the complexity of the multi-stage scenario. This paper treats the multi-stage cascading failure problem as a reinforcement learning task and develops a simulation environment. The reinforcement learning agent is then trained via the deterministic policy gradient algorithm to achieve continuous actions. Finally, the effectiveness of the proposed approach is validated on the IEEE 14-bus and IEEE 118-bus systems.","authors":["Bo Meng","Chenghao Xu","Yongli Zhu"],"url":"https://arxiv.org/abs/2505.09012"}
{"created":"2025-05-15","title":"Resource Allocation with Multi-Team Collaboration Based on Hamilton's Rule","abstract":"This paper presents a multi-team collaboration strategy based on Hamilton's rule from ecology that facilitates resource allocation among multiple teams, where agents are considered as shared resource among all teams that must be allocated appropriately. We construct an algorithmic framework that allows teams to make bids for agents that consider the costs and benefits of transferring agents while also considering relative mission importance for each team. This framework is applied to a multi-team coverage control mission to demonstrate its effectiveness. It is shown that the necessary criteria of a mission evaluation function are met by framing it as a function of the locational coverage cost of each team with respect to agent gain and loss, and these results are illustrated through simulations.","authors":["Riwa Karam","Ruoyu Lin","Brooks A. Butler","Magnus Egerstedt"],"url":"https://arxiv.org/abs/2505.09016"}
{"created":"2025-05-15","title":"DyGSSM: Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update","abstract":"Most of the dynamic graph representation learning methods involve dividing a dynamic graph into discrete snapshots to capture the evolving behavior of nodes over time. Existing methods primarily capture only local or global structures of each node within a snapshot using message-passing and random walk-based methods. Then, they utilize sequence-based models (e.g., transformers) to encode the temporal evolution of node embeddings, and meta-learning techniques to update the model parameters. However, these approaches have two limitations. First, they neglect the extraction of global and local information simultaneously in each snapshot. Second, they fail to consider the model's performance in the current snapshot during parameter updates, resulting in a lack of temporal dependency management. Recently, HiPPO (High-order Polynomial Projection Operators) algorithm has gained attention for their ability to optimize and preserve sequence history in State Space Model (SSM). To address the aforementioned limitations in dynamic graph representation learning, we propose a novel method called Multi-view Dynamic Graph Embeddings with State Space Model Gradient Update (DyGSSM). Our approach combines Graph Convolution Networks (GCN) for local feature extraction and random walk with Gated Recurrent Unit (GRU) for global feature extraction in each snapshot. We then integrate the local and global features using a cross-attention mechanism. Additionally, we incorporate an SSM based on HiPPO algorithm to account for long-term dependencies when updating model parameters, ensuring that model performance in each snapshot informs subsequent updates. Experiments on five public datasets show that our method outperforms existing baseline and state-of-the-art (SOTA) methods in 17 out of 20 cases.","authors":["Bizhan Alipour Pijan","Serdar Bozdag"],"url":"https://arxiv.org/abs/2505.09017"}
{"created":"2025-05-15","title":"Multimodal Fusion of Glucose Monitoring and Food Imagery for Caloric Content Prediction","abstract":"Effective dietary monitoring is critical for managing Type 2 diabetes, yet accurately estimating caloric intake remains a major challenge. While continuous glucose monitors (CGMs) offer valuable physiological data, they often fall short in capturing the full nutritional profile of meals due to inter-individual and meal-specific variability. In this work, we introduce a multimodal deep learning framework that jointly leverages CGM time-series data, Demographic/Microbiome, and pre-meal food images to enhance caloric estimation. Our model utilizes attention based encoding and a convolutional feature extraction for meal imagery, multi-layer perceptrons for CGM and Microbiome data followed by a late fusion strategy for joint reasoning. We evaluate our approach on a curated dataset of over 40 participants, incorporating synchronized CGM, Demographic and Microbiome data and meal photographs with standardized caloric labels. Our model achieves a Root Mean Squared Relative Error (RMSRE) of 0.2544, outperforming the baselines models by over 50%. These findings demonstrate the potential of multimodal sensing to improve automated dietary assessment tools for chronic disease management.","authors":["Adarsh Kumar"],"url":"https://arxiv.org/abs/2505.09018"}
{"created":"2025-05-15","title":"JcvPCA and JsvCRP : a set of metrics to evaluate changes in joint coordination strategies","abstract":"Characterizing changes in inter-joint coordination presents significant challenges, as it necessitates the examination of relationships between multiple degrees of freedom during movements and their temporal evolution. Existing metrics are inadequate in providing physiologically coherent results that document both the temporal and spatial aspects of inter-joint coordination. In this article, we introduce two novel metrics to enhance the analysis of inter-joint coordination. The first metric, Joint Contribution Variation based on Principal Component Analysis (JcvPCA), evaluates the variation in each joint's contribution during series of movements. The second metric, Joint Synchronization Variation based on Continuous Relative Phase (JsvCRP), measures the variation in temporal synchronization among joints between two movement datasets. We begin by presenting each metric and explaining their derivation. We then demonstrate the application of these metrics using simulated and experimental datasets involving identical movement tasks performed with distinct coordination strategies. The results show that these metrics can successfully differentiate between unique coordination strategies, providing meaningful insights into joint collaboration during movement. These metrics hold significant potential for fields such as ergonomics and clinical rehabilitation, where a precise understanding of the evolution of inter-joint coordination strategies is crucial. Potential applications include evaluating the effects of upper limb exoskeletons in industrial settings or monitoring the progress of patients undergoing neurological rehabilitation.","authors":["Oc\\'eane Dubois","Agn\\`es Roby-Brami","Ross Parry","Nathana\\\"el Jarrass\\'e"],"url":"https://arxiv.org/abs/2505.09020"}
{"created":"2025-05-15","title":"AI-Mediated Code Comment Improvement","abstract":"This paper describes an approach to improve code comments along different quality axes by rewriting those comments with customized Artificial Intelligence (AI)-based tools. We conduct an empirical study followed by grounded theory qualitative analysis to determine the quality axes to improve. Then we propose a procedure using a Large Language Model (LLM) to rewrite existing code comments along the quality axes. We implement our procedure using GPT-4o, then distil the results into a smaller model capable of being run in-house, so users can maintain data custody. We evaluate both our approach using GPT-4o and the distilled model versions. We show in an evaluation how our procedure improves code comments along the quality axes. We release all data and source code in an online repository for reproducibility.","authors":["Maria Dhakal","Chia-Yi Su","Robert Wallace","Chris Fakhimi","Aakash Bansal","Toby Li","Yu Huang","Collin McMillan"],"url":"https://arxiv.org/abs/2505.09021"}
{"created":"2025-05-15","title":"Block-Biased Mamba for Long-Range Sequence Processing","abstract":"Mamba extends earlier state space models (SSMs) by introducing input-dependent dynamics, and has demonstrated strong empirical performance across a range of domains, including language modeling, computer vision, and foundation models. However, a surprising weakness remains: despite being built on architectures designed for long-range dependencies, Mamba performs poorly on long-range sequential tasks. Understanding and addressing this gap is important for improving Mamba's universality and versatility. In this work, we analyze Mamba's limitations through three perspectives: expressiveness, inductive bias, and training stability. Our theoretical results show how Mamba falls short in each of these aspects compared to earlier SSMs such as S4D. To address these issues, we propose $\\text{B}_2\\text{S}_6$, a simple extension of Mamba's S6 unit that combines block-wise selective dynamics with a channel-specific bias. We prove that these changes equip the model with a better-suited inductive bias and improve its expressiveness and stability. Empirically, $\\text{B}_2\\text{S}_6$ outperforms S4 and S4D on Long-Range Arena (LRA) tasks while maintaining Mamba's performance on language modeling benchmarks.","authors":["Annan Yu","N. Benjamin Erichson"],"url":"https://arxiv.org/abs/2505.09022"}
{"created":"2025-05-15","title":"Automated Meta Prompt Engineering for Alignment with the Theory of Mind","abstract":"We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment.","authors":["Aaron Baughman","Rahul Agarwal","Eduardo Morales","Gozde Akay"],"url":"https://arxiv.org/abs/2505.09024"}
{"created":"2025-05-15","title":"Tests as Prompt: A Test-Driven-Development Benchmark for LLM Code Generation","abstract":"We introduce WebApp1K, a novel benchmark for evaluating large language models (LLMs) in test-driven development (TDD) tasks, where test cases serve as both prompt and verification for code generation. Unlike traditional approaches relying on natural language prompts, our benchmark emphasizes the ability of LLMs to interpret and implement functionality directly from test cases, reflecting real-world software development practices. Comprising 1000 diverse challenges across 20 application domains, the benchmark evaluates LLMs on their ability to generate compact, functional code under the constraints of context length and multi-feature complexity. Our findings highlight instruction following and in-context learning as critical capabilities for TDD success, surpassing the importance of general coding proficiency or pretraining knowledge. Through comprehensive evaluation of 19 frontier models, we reveal performance bottlenecks, such as instruction loss in long prompts, and provide a detailed error analysis spanning multiple root causes. This work underscores the practical value of TDD-specific benchmarks and lays the foundation for advancing LLM capabilities in rigorous, application-driven coding scenarios.","authors":["Yi Cui"],"url":"https://arxiv.org/abs/2505.09027"}
{"created":"2025-05-15","title":"Monte Carlo Beam Search for Actor-Critic Reinforcement Learning in Continuous Control","abstract":"Actor-critic methods, like Twin Delayed Deep Deterministic Policy Gradient (TD3), depend on basic noise-based exploration, which can result in less than optimal policy convergence. In this study, we introduce Monte Carlo Beam Search (MCBS), a new hybrid method that combines beam search and Monte Carlo rollouts with TD3 to improve exploration and action selection. MCBS produces several candidate actions around the policy's output and assesses them through short-horizon rollouts, enabling the agent to make better-informed choices. We test MCBS across various continuous-control benchmarks, including HalfCheetah-v4, Walker2d-v5, and Swimmer-v5, showing enhanced sample efficiency and performance compared to standard TD3 and other baseline methods like SAC, PPO, and A2C. Our findings emphasize MCBS's capability to enhance policy learning through structured look-ahead search while ensuring computational efficiency. Additionally, we offer a detailed analysis of crucial hyperparameters, such as beam width and rollout depth, and explore adaptive strategies to optimize MCBS for complex control tasks. Our method shows a higher convergence rate across different environments compared to TD3, SAC, PPO, and A2C. For instance, we achieved 90% of the maximum achievable reward within around 200 thousand timesteps compared to 400 thousand timesteps for the second-best method.","authors":["Hazim Alzorgan","Abolfazl Razi"],"url":"https://arxiv.org/abs/2505.09029"}
{"created":"2025-05-15","title":"Improving the Reliability of LLMs: Combining CoT, RAG, Self-Consistency, and Self-Verification","abstract":"Hallucination, where large language models (LLMs) generate confident but incorrect or irrelevant information, remains a key limitation in their application to complex, open-ended tasks. Chain-of-thought (CoT) prompting has emerged as a promising method for improving multistep reasoning by guiding models through intermediate steps. However, CoT alone does not fully address the hallucination problem. In this work, we investigate how combining CoT with retrieval-augmented generation (RAG), as well as applying self-consistency and self-verification strategies, can reduce hallucinations and improve factual accuracy. By incorporating external knowledge sources during reasoning and enabling models to verify or revise their own outputs, we aim to generate more accurate and coherent responses. We present a comparative evaluation of baseline LLMs against CoT, CoT+RAG, self-consistency, and self-verification techniques. Our results highlight the effectiveness of each method and identify the most robust approach for minimizing hallucinations while preserving fluency and reasoning depth.","authors":["Adarsh Kumar","Hwiyoon Kim","Jawahar Sai Nathani","Neil Roy"],"url":"https://arxiv.org/abs/2505.09031"}
{"created":"2025-05-15","title":"FocusE: A semantic extension of FocusST","abstract":"To analyse and verify the safety and security properties of interactive systems, a formal specification might be necessary. There are many types of formal languages and frameworks. The decision regarding what type of formal specification should be applied in each particular case depends on many factors. One of the approaches to specify interactive systems formally is to present them as a composition of components processing data and control streams. In this short paper, we present FocusE, a formal approach for modelling event-based streams. The proposed approach is based on a formal language FocusST, and can be seen as its semantic extension.","authors":["Maria Spichkova"],"url":"https://arxiv.org/abs/2505.09032"}
{"created":"2025-05-15","title":"Item Level Exploration Traffic Allocation in Large-scale Recommendation Systems","abstract":"This paper contributes to addressing the item cold start problem in large-scale recommender systems, focusing on how to efficiently gain initial visibility for newly ingested content. We propose an exploration system designed to efficiently allocate impressions to these fresh items. Our approach leverages a learned probabilistic model to predict an item's discoverability, which then informs a scalable and adaptive traffic allocation strategy. This system intelligently distributes exploration budgets, optimizing for the long-term benefit of the recommendation platform. The impact is a demonstrably more efficient cold-start process, leading to a significant increase in the discoverability of new content and ultimately enriching the item corpus available for exploitation, as evidenced by its successful deployment in a large-scale production environment.","authors":["Dong Wang","Junyi Jiao","Arnab Bhadury","Yaping Zhang","Mingyan Gao"],"url":"https://arxiv.org/abs/2505.09033"}
{"created":"2025-05-15","title":"Multiparty Selective Disclosure using Attribute-Based Encryption","abstract":"This study proposes a mechanism for encrypting SD-JWT (Selective Disclosure JSON Web Token) Disclosures using Attribute-Based Encryption (ABE) to enable flexible access control on the basis of the Verifier's attributes. By integrating Ciphertext-Policy ABE (CP-ABE) into the existing SD-JWT framework, the Holder can assign decryption policies to Disclosures, ensuring information is selectively disclosed. The mechanism's feasibility was evaluated in a virtualized environment by measuring the processing times for SD-JWT generation, encryption, and decryption with varying Disclosure counts (5, 10, 20). Results showed that SD-JWT generation is lightweight, while encryption and decryption times increase linearly with the number of Disclosures. This approach is suitable for privacy-sensitive applications like healthcare, finance, and supply chain tracking but requires optimization for real-time use cases such as IoT. Future research should focus on improving ABE efficiency and addressing scalability challenges.","authors":["Shigenori Ohashi"],"url":"https://arxiv.org/abs/2505.09034"}
{"created":"2025-05-15","title":"Unencrypted Flying Objects: Security Lessons from University Small Satellite Developers and Their Code","abstract":"Satellites face a multitude of security risks that set them apart from hardware on Earth. Small satellites may face additional challenges, as they are often developed on a budget and by amateur organizations or universities that do not consider security. We explore the security practices and preferences of small satellite teams, particularly university satellite teams, to understand what barriers exist to building satellites securely. We interviewed 8 university satellite club leaders across 4 clubs in the U.S. and perform a code audit of 3 of these clubs' code repositories. We find that security practices vary widely across teams, but all teams studied had vulnerabilities available to an unprivileged, ground-based attacker. Participants foresee many risks of unsecured small satellites and indicate security shortcomings in industry and government. Lastly, we identify a set of considerations for how to build future small satellites securely, in amateur organizations and beyond.","authors":["Rachel McAmis","Gregor Haas","Mattea Sim","David Kohlbrenner","Tadayoshi Kohno"],"url":"https://arxiv.org/abs/2505.09038"}
{"created":"2025-05-15","title":"Atomic Consistency Preference Optimization for Long-Form Question Answering","abstract":"Large Language Models (LLMs) frequently produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated factual and non-factual pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness, which may not always be accessible. To address this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals, i.e., the agreement of individual facts across multiple stochastic responses, to identify high- and low-quality data pairs for model alignment. By eliminating the need for costly GPT calls, ACPO provides a scalable and efficient approach to improving factoid question-answering. Despite being self-supervised, empirical results demonstrate that ACPO outperforms FactAlign, a strong supervised alignment baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its effectiveness in enhancing factual reliability without relying on external models or knowledge bases.","authors":["Jingfeng Chen","Raghuveer Thirukovalluru","Junlin Wang","Kaiwei Luo","Bhuwan Dhingra"],"url":"https://arxiv.org/abs/2505.09039"}
{"created":"2025-05-15","title":"RT-cache: Efficient Robot Trajectory Retrieval System","abstract":"This paper introduces RT-cache, a novel trajectorymemory pipeline that accelerates real-world robot inference by leveraging big-data retrieval and learning from experience. While modern Vision-Language-Action (VLA) models can handle diverse robotic tasks, they often incur high per-step inference costs, resulting in significant latency, sometimes minutes per task. In contrast, RT-cache stores a large-scale Memory of previously successful robot trajectories and retrieves relevant multistep motion snippets, drastically reducing inference overhead. By integrating a Memory Builder with a Trajectory Retrieval, we develop an efficient retrieval process that remains tractable even for extremely large datasets. RT-cache flexibly accumulates real-world experiences and replays them whenever the current scene matches past states, adapting quickly to new or unseen environments with only a few additional samples. Experiments on the Open-X Embodiment Dataset and other real-world data demonstrate that RT-cache completes tasks both faster and more successfully than a baseline lacking retrieval, suggesting a practical, data-driven solution for real-time manipulation.","authors":["Owen Kwon","Abraham George","Alison Bartsch","Amir Barati Farimani"],"url":"https://arxiv.org/abs/2505.09040"}
{"created":"2025-05-15","title":"Approximating the Directed Hausdorff Distance","abstract":"The Hausdorff distance is a metric commonly used to compute the set similarity of geometric sets.","authors":["Oliver A. Chubet","Parth M. Parikh","Donald R. Sheehy","Siddharth S. Sheth"],"url":"https://arxiv.org/abs/2505.09046"}
{"created":"2025-05-15","title":"Positioning Monocular Optical See Through Head Worn Displays in Glasses for Everyday Wear","abstract":"Head-worn displays for everyday wear in the form of regular eyeglasses are technically feasible with recent advances in waveguide technology. One major design decision is determining where in the user's visual field to position the display. Centering the display in the principal point of gaze (PPOG) allows the user to switch attentional focus between the virtual and real images quickly, and best performance often occurs when the display is centered in PPOG or is centered vertically below PPOG. However, these positions are often undesirable in that they are considered interruptive or are associated with negative social perceptions by users. Offsetting the virtual image may be preferred when tasks involve driving, walking, or social interaction. This paper consolidates findings from recent studies on monocular optical see-through HWDs (OST-HWDs), focusing on potential for interruption, comfort, performance, and social perception. For text-based tasks, which serve as a proxy for many monocular OST-HWD tasks, we recommend a 15{\\deg} horizontal field of view (FOV) with the virtual image in the right lens vertically centered but offset to +8.7{\\deg} to +23.7{\\deg} toward the ear. Glanceable content can be offset up to +30{\\deg} for short interactions.","authors":["Parth Arora","Ethan Kimmel","Katherine Huang","Tyler Kwok","Yukun Song","Sofia Vempala","Georgianna Lin","Ozan Cakmakci","Thad Starner"],"url":"https://arxiv.org/abs/2505.09047"}
{"created":"2025-05-15","title":"Modeling Interdependent Cybersecurity Threats Using Bayesian Networks: A Case Study on In-Vehicle Infotainment Systems","abstract":"Cybersecurity threats are increasingly marked by interdependence, uncertainty, and evolving complexity challenges that traditional assessment methods such as CVSS, STRIDE, and attack trees fail to adequately capture. This paper reviews the application of Bayesian Networks (BNs) in cybersecurity risk modeling, highlighting their capacity to represent probabilistic dependencies, integrate diverse threat indicators, and support reasoning under uncertainty. A structured case study is presented in which a STRIDE-based attack tree for an automotive In-Vehicle Infotainment (IVI) system is transformed into a Bayesian Network. Logical relationships are encoded using Conditional Probability Tables (CPTs), and threat likelihoods are derived from normalized DREAD scores. The model enables not only probabilistic inference of system compromise likelihood but also supports causal analysis using do-calculus and local sensitivity analysis to identify high-impact vulnerabilities. These analyses provide insight into the most influential nodes within the threat propagation chain, informing targeted mitigation strategies. While demonstrating the potential of BNs for dynamic and context-aware risk assessment, the study also outlines limitations related to scalability, reliance on expert input, static structure assumptions, and limited temporal modeling. The paper concludes by advocating for future enhancements through Dynamic Bayesian Networks, structure learning, and adaptive inference to better support real-time cybersecurity decision-making in complex environments.","authors":["Sangita Sridar"],"url":"https://arxiv.org/abs/2505.09048"}
{"created":"2025-05-15","title":"EcoSphere: A Decision-Support Tool for Automated Carbon Emission and Cost Optimization in Sustainable Urban Development","abstract":"The construction industry is a major contributor to global greenhouse gas emissions, with embodied carbon being a key component. This study develops EcoSphere, an innovative software designed to evaluate and balance embodied and operational carbon emissions with construction and environmental costs in urban planning. Using high-resolution data from the National Structure Inventory, combined with computer vision and natural language processing applied to Google Street View and satellite imagery, EcoSphere categorizes buildings by structural and material characteristics with a bottom-up approach, creating a baseline emissions dataset. By simulating policy scenarios and mitigation strategies, EcoSphere provides policymakers and non-experts with actionable insights for sustainable development in cities and provide them with a vision of the environmental and financial results of their decisions. Case studies in Chicago and Indianapolis showcase how EcoSphere aids in assessing policy impacts on carbon emissions and costs, supporting data-driven progress toward carbon neutrality.","authors":["Siavash Ghorbany","Ming Hu","Siyuan Yao","Matthew Sisk","Chaoli Wang"],"url":"https://arxiv.org/abs/2505.09054"}
{"created":"2025-05-15","title":"A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias","abstract":"Large Language Models (LLMs) represent a major step toward artificial general intelligence, significantly advancing our ability to interact with technology. While LLMs perform well on Natural Language Processing tasks -- such as translation, generation, code writing, and summarization -- questions remain about their output similarity, variability, and ethical implications. For instance, how similar are texts generated by the same model? How does this compare across different models? And which models best uphold ethical standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like generation, explanation, and rewriting. This resulted in approximately 3 million texts from 12 LLMs, including proprietary and open-source systems from OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs from the same LLM are more similar to each other than to human-written texts; (2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4 produces more varied responses; (3) LLM writing styles differ significantly, with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for distinctiveness; (4) differences in vocabulary and tone underscore the linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate greater gender balance and reduced bias. These results offer new insights into the behavior and diversity of LLM outputs, helping guide future development and ethical evaluation.","authors":["Brandon Smith","Mohamed Reda Bouadjenek","Tahsin Alamgir Kheya","Phillip Dawson","Sunil Aryal"],"url":"https://arxiv.org/abs/2505.09056"}
{"created":"2025-05-15","title":"Leveraging Offline Data from Similar Systems for Online Linear Quadratic Control","abstract":"``Sim2real gap\", in which the system learned in simulations is not the exact representation of the real system, can lead to loss of stability and performance when controllers learned using data from the simulated system are used on the real system. In this work, we address this challenge in the linear quadratic regulator (LQR) setting. Specifically, we consider an LQR problem for a system with unknown system matrices. Along with the state-action pairs from the system to be controlled, a trajectory of length $S$ of state-action pairs from a different unknown system is available. Our proposed algorithm is constructed upon Thompson sampling and utilizes the mean as well as the uncertainty of the dynamics of the system from which the trajectory of length $S$ is obtained. We establish that the algorithm achieves $\\tilde{\\mathcal{O}}({f(S,M_{\\delta})\\sqrt{T/S}})$ Bayes regret after $T$ time steps, where $M_{\\delta}$ characterizes the \\emph{dissimilarity} between the two systems and $f(S,M_{\\delta})$ is a function of $S$ and $M_{\\delta}$. When $M_{\\delta}$ is sufficiently small, the proposed algorithm achieves $\\tilde{\\mathcal{O}}({\\sqrt{T/S}})$ Bayes regret and outperforms a naive strategy which does not utilize the available trajectory.","authors":["Shivam Bajaj","Prateek Jaiswal","Vijay Gupta"],"url":"https://arxiv.org/abs/2505.09057"}
{"created":"2025-05-15","title":"Reach-Avoid-Stabilize Using Admissible Control Sets","abstract":"Hamilton-Jacobi Reachability (HJR) analysis has been successfully used in many robotics and control tasks, and is especially effective in computing reach-avoid sets and control laws that enable an agent to reach a goal while satisfying state constraints. However, the original HJR formulation provides no guarantees of safety after a) the prescribed time horizon, or b) goal satisfaction. The reach-avoid-stabilize (RAS) problem has therefore gained a lot of focus: find the set of initial states (the RAS set), such that the trajectory can reach the target, and stabilize to some point of interest (POI) while avoiding obstacles. Solving RAS problems using HJR usually requires defining a new value function, whose zero sub-level set is the RAS set. The existing methods do not consider the problem when there are a series of targets to reach and/or obstacles to avoid. We propose a method that uses the idea of admissible control sets; we guarantee that the system will reach each target while avoiding obstacles as prescribed by the given time series. Moreover, we guarantee that the trajectory ultimately stabilizes to the POI. The proposed method provides an under-approximation of the RAS set, guaranteeing safety. Numerical examples are provided to validate the theory.","authors":["Zheng Gong","Boyang Li","Sylvia Herbert"],"url":"https://arxiv.org/abs/2505.09058"}
{"created":"2025-05-15","title":"Evaluating Mutation-based Fault Localization for Quantum Programs","abstract":"Quantum computers leverage the principles of quantum mechanics to execute operations. They require quantum programs that define operations on quantum bits (qubits), the fundamental units of computation. Unlike traditional software development, the process of creating and debugging quantum programs requires specialized knowledge of quantum computation, making the development process more challenging. In this paper, we apply and evaluate mutation-based fault localization (MBFL) for quantum programs with the aim of enhancing debugging efficiency. We use quantum mutation operations, which are specifically designed for quantum programs, to identify faults. Our evaluation involves 23 real-world faults and 305 artificially induced faults in quantum programs developed with Qiskit(R). The results show that real-world faults are more challenging for MBFL than artificial faults. In fact, the median EXAM score, which represents the percentage of the code examined before locating the faulty statement (lower is better), is 1.2% for artificial benchmark and 19.4% for the real-world benchmark in the worst-case scenario. Our study highlights the potential and limitations of MBFL for quantum programs, considering different fault types and mutation operation types. Finally, we discuss future directions for improving MBFL in the context of quantum programming.","authors":["Yuta Ishimoto","Masanari Kondo","Naoyasu Ubayashi","Yasutaka Kamei","Ryota Katsube","Naoto Sato","Hideto Ogawa"],"url":"https://arxiv.org/abs/2505.09059"}
{"created":"2025-05-15","title":"A federated Kaczmarz algorithm","abstract":"In this paper, we propose a federated algorithm for solving large linear systems that is inspired by the classic randomized Kaczmarz algorithm. We provide convergence guarantees of the proposed method, and as a corollary of our analysis, we provide a new proof for the convergence of the classic randomized Kaczmarz method. We demonstrate experimentally the behavior of our method when applied to related problems. For underdetermined systems, we demonstrate that our algorithm can be used for sparse approximation. For inconsistent systems, we demonstrate that our algorithm converges to a horizon of the least squares solution. Finally, we apply our algorithm to real data and show that it is consistent with the selection of Lasso, while still offering the computational advantages of the Kaczmarz framework and thresholding-based algorithms in the federated setting.","authors":["Halyun Jeong","Deanna Needell","Chi-Hao Wu"],"url":"https://arxiv.org/abs/2505.09061"}
{"created":"2025-05-15","title":"Variational Prefix Tuning for Diverse and Accurate Code Summarization Using Pre-trained Language Models","abstract":"Recent advancements in source code summarization have leveraged transformer-based pre-trained models, including Large Language Models of Code (LLMCs), to automate and improve the generation of code summaries. However, existing methods often focus on generating a single high-quality summary for a given source code, neglecting scenarios where the generated summary might be inadequate and alternative options are needed. In this paper, we introduce Variational Prefix Tuning (VPT), a novel approach that enhances pre-trained models' ability to generate diverse yet accurate sets of summaries, allowing the user to choose the most suitable one for the given source code. Our method integrates a Conditional Variational Autoencoder (CVAE) framework as a modular component into pre-trained models, enabling us to model the distribution of observed target summaries and sample continuous embeddings to be used as prefixes to steer the generation of diverse outputs during decoding. Importantly, we construct our method in a parameter-efficient manner, eliminating the need for expensive model retraining, especially when using LLMCs. Furthermore, we employ a bi-criteria reranking method to select a subset of generated summaries, optimizing both the diversity and the accuracy of the options presented to users. We present extensive experimental evaluations using widely used datasets and current state-of-the-art pre-trained code summarization models to demonstrate the effectiveness of our approach and its adaptability across models.","authors":["Junda Zhao","Yuliang Song","Eldan Cohen"],"url":"https://arxiv.org/abs/2505.09062"}
{"created":"2025-05-15","title":"Single-shot prediction of parametric partial differential equations","abstract":"We introduce Flexi-VAE, a data-driven framework for efficient single-shot forecasting of nonlinear parametric partial differential equations (PDEs), eliminating the need for iterative time-stepping while maintaining high accuracy and stability. Flexi-VAE incorporates a neural propagator that advances latent representations forward in time, aligning latent evolution with physical state reconstruction in a variational autoencoder setting. We evaluate two propagation strategies, the Direct Concatenation Propagator (DCP) and the Positional Encoding Propagator (PEP), and demonstrate, through representation-theoretic analysis, that DCP offers superior long-term generalization by fostering disentangled and physically meaningful latent spaces. Geometric diagnostics, including Jacobian spectral analysis, reveal that propagated latent states reside in regions of lower decoder sensitivity and more stable local geometry than those derived via direct encoding, enhancing robustness for long-horizon predictions. We validate Flexi-VAE on canonical PDE benchmarks, the 1D viscous Burgers equation and the 2D advection-diffusion equation, achieving accurate forecasts across wide parametric ranges. The model delivers over 50x CPU and 90x GPU speedups compared to autoencoder-LSTM baselines for large temporal shifts. These results position Flexi-VAE as a scalable and interpretable surrogate modeling tool for accelerating high-fidelity simulations in computational fluid dynamics (CFD) and other parametric PDE-driven applications, with extensibility to higher-dimensional and more complex systems.","authors":["Khalid Rafiq","Wenjing Liao","Aditya G. Nair"],"url":"https://arxiv.org/abs/2505.09063"}
{"created":"2025-05-15","title":"Vertex-based auxiliary space multigrid method and its application to linear elasticity equations","abstract":"In this paper, a vertex-based auxiliary space multigrid(V-ASMG) method as a preconditioner of the PCG method is proposed for solving the large sparse linear equations derived from the linear elasticity equations. The main key of such V-ASMG method lies in an auxiliary region-tree structure based on the geometrically regular subdivision. The computational complexity of building such a region-tree is $\\mathcal{O}\\left(q N\\log_2 N\\right)$, where $N$ is the number of the given original grid vertices and $q$ is the power of the ratio of the maximum distance $d_{max}$ to minimum distance $d_{min}$ between the given original grid vertices. The process of constructing the auxiliary region-tree is similar to the method in [17], but the selection of the representative points is changed. To be more specific, instead of choosing the barycenters, the correspondence between each grid layer is constructed based on the position relationship of the grid vertices. There are two advantages for this approach: the first is its simplicity, there is no need to deal with hanging points when building the auxiliary region-tree, and it is possible to construct the restriction/prolongation operator directly by using the bilinear interpolation function, and it is easy to be generalized to other problems as well, due to all the information we need is only the grid vertices; the second is its strong convergence, the corresponding relative residual can quickly converge to the given tolerance(It is taken to be $10^{-6}$ in this paper), thus obtaining the desired numerical solution. Two- and three-dimensional numerical experiments are given to verify the strong convergence of the proposed V-ASMG method as a preconditioner of the PCG method.","authors":["Jiayin Li","Jinbiao Wu","Wenqian Zhang","Jiawen Liu"],"url":"https://arxiv.org/abs/2505.09064"}
{"created":"2025-05-15","title":"Display Content, Display Methods and Evaluation Methods of the HCI in Explainable Recommender Systems: A Survey","abstract":"Explainable Recommender Systems (XRS) aim to provide users with understandable reasons for the recommendations generated by these systems, representing a crucial research direction in artificial intelligence (AI). Recent research has increasingly focused on the algorithms, display, and evaluation methodologies of XRS. While current research and reviews primarily emphasize the algorithmic aspects, with fewer studies addressing the Human-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews lack a unified taxonomy for XRS and there is insufficient attention given to the emerging area of short video recommendations. In this study, we synthesize existing literature and surveys on XRS, presenting a unified framework for its research and development. The main contributions are as follows: 1) We adopt a lifecycle perspective to systematically summarize the technologies and methods used in XRS, addressing challenges posed by the diversity and complexity of algorithmic models and explanation techniques. 2) For the first time, we highlight the application of multimedia, particularly video-based explanations, along with its potential, technical pathways, and challenges in XRS. 3) We provide a structured overview of evaluation methods from both qualitative and quantitative dimensions. These findings provide valuable insights for the systematic design, progress, and testing of XRS.","authors":["Weiqing Li","Yue Xu","Yuefeng Li","Yinghui Huang"],"url":"https://arxiv.org/abs/2505.09065"}
{"created":"2025-05-15","title":"S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment","abstract":"This paper introduces S-DAT (Synthetic-Divergent Association Task), a scalable, multilingual framework for automated assessment of divergent thinking (DT) -a core component of human creativity. Traditional creativity assessments are often labor-intensive, language-specific, and reliant on subjective human ratings, limiting their scalability and cross-cultural applicability. In contrast, S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance -- a language-agnostic proxy for DT. We evaluate S-DAT across eleven diverse languages, including English, Spanish, German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating robust and consistent scoring across linguistic contexts. Unlike prior DAT approaches, the S-DAT shows convergent validity with other DT measures and correct discriminant validity with convergent thinking. This cross-linguistic flexibility allows for more inclusive, global-scale creativity research, addressing key limitations of earlier approaches. S-DAT provides a powerful tool for fairer, more comprehensive evaluation of cognitive flexibility in diverse populations and can be freely assessed online: https://sdat.iol.zib.de/.","authors":["Jennifer Haase","Paul H. P. Hanel","Sebastian Pokutta"],"url":"https://arxiv.org/abs/2505.09068"}
{"created":"2025-05-15","title":"A Novel 6-axis Force/Torque Sensor Using Inductance Sensors","abstract":"This paper presents a novel six-axis force/torque (F/T) sensor based on inductive sensing technology. Unlike conventional strain gauge-based sensors that require direct contact and external amplification, the proposed sensor utilizes non-contact inductive measurements to estimate force via displacement of a conductive target. A compact, fully integrated architecture is achieved by incorporating a CAN-FD based signal processing module directly onto the PCB, enabling high-speed data acquisition at up to 4~kHz without external DAQ systems. The sensing mechanism is modeled and calibrated through a rational function fitting approach, which demonstrated superior performance in terms of root mean square error (RMSE), coefficient of determination ($R^2$), and linearity error compared to other nonlinear models. Static and repeatability experiments validate the sensor's accuracy, achieving a resolution of 0.03~N and quantization levels exceeding 55,000 steps, surpassing that of commercial sensors. The sensor also exhibits low crosstalk, high sensitivity, and robust noise characteristics. Its performance and structure make it suitable for precision robotic applications, especially in scenarios where compactness, non-contact operation, and integrated processing are essential.","authors":["Hyun-Bin Kim","Kyung-Soo Kim"],"url":"https://arxiv.org/abs/2505.09069"}
{"created":"2025-05-15","title":"2D-3D Attention and Entropy for Pose Robust 2D Facial Recognition","abstract":"Despite recent advances in facial recognition, there remains a fundamental issue concerning degradations in performance due to substantial perspective (pose) differences between enrollment and query (probe) imagery. Therefore, we propose a novel domain adaptive framework to facilitate improved performances across large discrepancies in pose by enabling image-based (2D) representations to infer properties of inherently pose invariant point cloud (3D) representations. Specifically, our proposed framework achieves better pose invariance by using (1) a shared (joint) attention mapping to emphasize common patterns that are most correlated between 2D facial images and 3D facial data and (2) a joint entropy regularizing loss to promote better consistency$\\unicode{x2014}$enhancing correlations among the intersecting 2D and 3D representations$\\unicode{x2014}$by leveraging both attention maps. This framework is evaluated on FaceScape and ARL-VTF datasets, where it outperforms competitive methods by achieving profile (90$\\unicode{x00b0}$$\\unicode{x002b}$) TAR @ 1$\\unicode{x0025}$ FAR improvements of at least 7.1$\\unicode{x0025}$ and 1.57$\\unicode{x0025}$, respectively.","authors":["J. Brennan Peace","Shuowen Hu","Benjamin S. Riggan"],"url":"https://arxiv.org/abs/2505.09073"}
{"created":"2025-05-15","title":"Deployable and Generalizable Motion Prediction: Taxonomy, Open Challenges and Future Directions","abstract":"Motion prediction, the anticipation of future agent states or scene evolution, is rooted in human cognition, bridging perception and decision-making. It enables intelligent systems, such as robots and self-driving cars, to act safely in dynamic, human-involved environments, and informs broader time-series reasoning challenges. With advances in methods, representations, and datasets, the field has seen rapid progress, reflected in quickly evolving benchmark results. Yet, when state-of-the-art methods are deployed in the real world, they often struggle to generalize to open-world conditions and fall short of deployment standards. This reveals a gap between research benchmarks, which are often idealized or ill-posed, and real-world complexity.","authors":["Letian Wang","Marc-Antoine Lavoie","Sandro Papais","Barza Nisar","Yuxiao Chen","Wenhao Ding","Boris Ivanovic","Hao Shao","Abulikemu Abuduweili","Evan Cook","Yang Zhou","Peter Karkus","Jiachen Li","Changliu Liu","Marco Pavone","Steven Waslander"],"url":"https://arxiv.org/abs/2505.09074"}
{"created":"2025-05-15","title":"AdaFortiTran: An Adaptive Transformer Model for Robust OFDM Channel Estimation","abstract":"Deep learning models for channel estimation in Orthogonal Frequency Division Multiplexing (OFDM) systems often suffer from performance degradation under fast-fading channels and low-SNR scenarios. To address these limitations, we introduce the Adaptive Fortified Transformer (AdaFortiTran), a novel model specifically designed to enhance channel estimation in challenging environments. Our approach employs convolutional layers that exploit locality bias to capture strong correlations between neighboring channel elements, combined with a transformer encoder that applies the global Attention mechanism to channel patches. This approach effectively models both long-range dependencies and spectro-temporal interactions within single OFDM frames. We further augment the model's adaptability by integrating nonlinear representations of available channel statistics SNR, delay spread, and Doppler shift as priors. A residual connection is employed to merge global features from the transformer with local features from early convolutional processing, followed by final convolutional layers to refine the hierarchical channel representation. Despite its compact architecture, AdaFortiTran achieves up to 6 dB reduction in mean squared error (MSE) compared to state-of-the-art models. Tested across a wide range of Doppler shifts (200-1000 Hz), SNRs (0 to 25 dB), and delay spreads (50-300 ns), it demonstrates superior robustness in high-mobility environments.","authors":["Berkay Guler","Hamid Jafarkhani"],"url":"https://arxiv.org/abs/2505.09076"}
{"created":"2025-05-15","title":"SALM: A Multi-Agent Framework for Language Model-Driven Social Network Simulation","abstract":"Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.","authors":["Gaurav Koley"],"url":"https://arxiv.org/abs/2505.09081"}
{"created":"2025-05-15","title":"CEC-Zero: Chinese Error Correction Solution Based on LLM","abstract":"Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.","authors":["Sophie Zhang","Zhiming Lin"],"url":"https://arxiv.org/abs/2505.09082"}
{"created":"2025-05-15","title":"Human-like Cognitive Generalization for Large Models via Brain-in-the-loop Supervision","abstract":"Recent advancements in deep neural networks (DNNs), particularly large-scale language models, have demonstrated remarkable capabilities in image and natural language understanding. Although scaling up model parameters with increasing volume of training data has progressively improved DNN capabilities, achieving complex cognitive abilities - such as understanding abstract concepts, reasoning, and adapting to novel scenarios, which are intrinsic to human cognition - remains a major challenge. In this study, we show that brain-in-the-loop supervised learning, utilizing a small set of brain signals, can effectively transfer human conceptual structures to DNNs, significantly enhancing their comprehension of abstract and even unseen concepts. Experimental results further indicate that the enhanced cognitive capabilities lead to substantial performance gains in challenging tasks, including few-shot/zero-shot learning and out-of-distribution recognition, while also yielding highly interpretable concept representations. These findings highlight that human-in-the-loop supervision can effectively augment the complex cognitive abilities of large models, offering a promising pathway toward developing more human-like cognitive abilities in artificial systems.","authors":["Jiaxuan Chen","Yu Qi","Yueming Wang","Gang Pan"],"url":"https://arxiv.org/abs/2505.09085"}
{"created":"2025-05-15","title":"Generating time-consistent dynamics with discriminator-guided image diffusion models","abstract":"Realistic temporal dynamics are crucial for many video generation, processing and modelling applications, e.g. in computational fluid dynamics, weather prediction, or long-term climate simulations. Video diffusion models (VDMs) are the current state-of-the-art method for generating highly realistic dynamics. However, training VDMs from scratch can be challenging and requires large computational resources, limiting their wider application. Here, we propose a time-consistency discriminator that enables pretrained image diffusion models to generate realistic spatiotemporal dynamics. The discriminator guides the sampling inference process and does not require extensions or finetuning of the image diffusion model. We compare our approach against a VDM trained from scratch on an idealized turbulence simulation and a real-world global precipitation dataset. Our approach performs equally well in terms of temporal consistency, shows improved uncertainty calibration and lower biases compared to the VDM, and achieves stable centennial-scale climate simulations at daily time steps.","authors":["Philipp Hess","Maximilian Gelbrecht","Christof Sch\\\"otz","Michael Aich","Yu Huang","Shangshang Yang","Niklas Boers"],"url":"https://arxiv.org/abs/2505.09089"}
{"created":"2025-05-15","title":"DPN-GAN: Inducing Periodic Activations in Generative Adversarial Networks for High-Fidelity Audio Synthesis","abstract":"In recent years, generative adversarial networks (GANs) have made significant progress in generating audio sequences. However, these models typically rely on bandwidth-limited mel-spectrograms, which constrain the resolution of generated audio sequences, and lead to mode collapse during conditional generation. To address this issue, we propose Deformable Periodic Network based GAN (DPN-GAN), a novel GAN architecture that incorporates a kernel-based periodic ReLU activation function to induce periodic bias in audio generation. This innovative approach enhances the model's ability to capture and reproduce intricate audio patterns. In particular, our proposed model features a DPN module for multi-resolution generation utilizing deformable convolution operations, allowing for adaptive receptive fields that improve the quality and fidelity of the synthetic audio. Additionally, we enhance the discriminator network using deformable convolution to better distinguish between real and generated samples, further refining the audio quality. We trained two versions of the model: DPN-GAN small (38.67M parameters) and DPN-GAN large (124M parameters). For evaluation, we use five different datasets, covering both speech synthesis and music generation tasks, to demonstrate the efficiency of the DPN-GAN. The experimental results demonstrate that DPN-GAN delivers superior performance on both out-of-distribution and noisy data, showcasing its robustness and adaptability. Trained across various datasets, DPN-GAN outperforms state-of-the-art GAN architectures on standard evaluation metrics, and exhibits increased robustness in synthesized audio.","authors":["Zeeshan Ahmad","Shudi Bao","Meng Chen"],"url":"https://arxiv.org/abs/2505.09091"}
{"created":"2025-05-15","title":"OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions","abstract":"Lane Keeping Assist (LKA) is widely adopted in modern vehicles, yet its real-world performance remains underexplored due to proprietary systems and limited data access. This paper presents OpenLKA, the first open, large-scale dataset for LKA evaluation and improvement. It includes 400 hours of driving data from 50+ production vehicle models, collected through extensive road testing in Tampa, Florida and global contributions from the Comma.ai driving community. The dataset spans a wide range of challenging scenarios, including complex road geometries, degraded lane markings, adverse weather, lighting conditions and surrounding traffic. The dataset is multimodal, comprising: i) full CAN bus streams, decoded using custom reverse-engineered DBC files to extract key LKA events (e.g., system disengagements, lane detection failures); ii) synchronized high-resolution dash-cam video; iii) real-time outputs from Openpilot, providing accurate estimates of road curvature and lane positioning; iv) enhanced scene annotations generated by Vision Language Models, describing lane visibility, pavement quality, weather, lighting, and traffic conditions. By integrating vehicle-internal signals with high-fidelity perception and rich semantic context, OpenLKA provides a comprehensive platform for benchmarking the real-world performance of production LKA systems, identifying safety-critical operational scenarios, and assessing the readiness of current road infrastructure for autonomous driving. The dataset is publicly available at: https://github.com/OpenLKA/OpenLKA.","authors":["Yuhang Wang","Abdulaziz Alhuraish","Shengming Yuan","Hao Zhou"],"url":"https://arxiv.org/abs/2505.09092"}
{"created":"2025-05-15","title":"PLanet: Formalizing Experimental Design","abstract":"Carefully constructed experimental designs are essential for drawing valid, generalizable conclusions from scientific studies. Unfortunately, experimental design plans can be difficult to specify, communicate clearly, and relate to alternatives. In response, we introduce a grammar of experimental design that provides composable operators for constructing assignment procedures (e.g., Latin square). We implement this grammar in PLanet, a domain-specific language (DSL) that constructs assignment plans in three stages: experimental unit specification, trial-order construction, and order-to-unit mapping. We evaluate PLanet's expressivity by taking a purposive sample of recent CHI and UIST publications, representing their experiments as programs in PLanet, and identifying ambiguities and alternatives. In our evaluation, PLanet could express 11 out of 12 experiments found in sampled papers. Additionally, we found that PLanet constructs helped make complex design choices explicit when the researchers omit technical language describing their study designs.","authors":["London Bielicke","Anna Zhang","Shruti Tyagi","Emery Berger","Adam Chlipala","Eunice Jun"],"url":"https://arxiv.org/abs/2505.09094"}
{"created":"2025-05-15","title":"Statistical Mean Estimation with Coded Relayed Observations","abstract":"We consider a problem of statistical mean estimation in which the samples are not observed directly, but are instead observed by a relay (``teacher'') that transmits information through a memoryless channel to the decoder (``student''), who then produces the final estimate. We consider the minimax estimation error in the large deviations regime, and establish achievable error exponents that are tight in broad regimes of the estimation accuracy and channel quality. In contrast, two natural baseline methods are shown to yield strictly suboptimal error exponents. We initially focus on Bernoulli sources and binary symmetric channels, and then generalize to sub-Gaussian and heavy-tailed settings along with arbitrary discrete memoryless channels.","authors":["Yan Hao Ling","Zhouhao Yang","Jonathan Scarlett"],"url":"https://arxiv.org/abs/2505.09098"}
{"created":"2025-05-15","title":"Imitation Learning for Adaptive Control of a Virtual Soft Exoglove","abstract":"The use of wearable robots has been widely adopted in rehabilitation training for patients with hand motor impairments. However, the uniqueness of patients' muscle loss is often overlooked. Leveraging reinforcement learning and a biologically accurate musculoskeletal model in simulation, we propose a customized wearable robotic controller that is able to address specific muscle deficits and to provide compensation for hand-object manipulation tasks. Video data of a same subject performing human grasping tasks is used to train a manipulation model through learning from demonstration. This manipulation model is subsequently fine-tuned to perform object-specific interaction tasks. The muscle forces in the musculoskeletal manipulation model are then weakened to simulate neurological motor impairments, which are later compensated by the actuation of a virtual wearable robotics glove. Results shows that integrating the virtual wearable robotic glove provides shared assistance to support the hand manipulator with weakened muscle forces. The learned exoglove controller achieved an average of 90.5\\% of the original manipulation proficiency.","authors":["Shirui Lyu","Vittorio Caggiano","Matteo Leonetti","Dario Farina","Letizia Gionfrida"],"url":"https://arxiv.org/abs/2505.09099"}
{"created":"2025-05-15","title":"VGC-RIO: A Tightly Integrated Radar-Inertial Odometry with Spatial Weighted Doppler Velocity and Local Geometric Constrained RCS Histograms","abstract":"Recent advances in 4D radar-inertial odometry","authors":["Jianguang Xiang","Xiaofeng He","Zizhuo Chen","Lilian Zhang","Xincan Luo","Jun Mao"],"url":"https://arxiv.org/abs/2505.09103"}
{"created":"2025-05-15","title":"Argus: Federated Non-convex Bilevel Learning over 6G Space-Air-Ground Integrated Network","abstract":"The space-air-ground integrated network (SAGIN) has recently emerged as a core element in the 6G networks. However, traditional centralized and synchronous optimization algorithms are unsuitable for SAGIN due to infrastructureless and time-varying environments. This paper aims to develop a novel Asynchronous algorithm a.k.a. Argus for tackling non-convex and non-smooth decentralized federated bilevel learning over SAGIN. The proposed algorithm allows networked agents (e.g. autonomous aerial vehicles) to tackle bilevel learning problems in time-varying networks asynchronously, thereby averting stragglers from impeding the overall training speed. We provide a theoretical analysis of the iteration complexity, communication complexity, and computational complexity of Argus. Its effectiveness is further demonstrated through numerical experiments.","authors":["Ya Liu","Kai Yang","Yu Zhu","Keying Yang","Haibo Zhao"],"url":"https://arxiv.org/abs/2505.09106"}
{"created":"2025-05-15","title":"Air-Ground Collaboration for Language-Specified Missions in Unknown Environments","abstract":"As autonomous robotic systems become increasingly mature, users will want to specify missions at the level of intent rather than in low-level detail. Language is an expressive and intuitive medium for such mission specification. However, realizing language-guided robotic teams requires overcoming significant technical hurdles. Interpreting and realizing language-specified missions requires advanced semantic reasoning. Successful heterogeneous robots must effectively coordinate actions and share information across varying viewpoints. Additionally, communication between robots is typically intermittent, necessitating robust strategies that leverage communication opportunities to maintain coordination and achieve mission objectives. In this work, we present a first-of-its-kind system where an unmanned aerial vehicle (UAV) and an unmanned ground vehicle (UGV) are able to collaboratively accomplish missions specified in natural language while reacting to changes in specification on the fly. We leverage a Large Language Model (LLM)-enabled planner to reason over semantic-metric maps that are built online and opportunistically shared between an aerial and a ground robot. We consider task-driven navigation in urban and rural areas. Our system must infer mission-relevant semantics and actively acquire information via semantic mapping. In both ground and air-ground teaming experiments, we demonstrate our system on seven different natural-language specifications at up to kilometer-scale navigation.","authors":["Fernando Cladera","Zachary Ravichandran","Jason Hughes","Varun Murali","Carlos Nieto-Granda","M. Ani Hsieh","George J. Pappas","Camillo J. Taylor","Vijay Kumar"],"url":"https://arxiv.org/abs/2505.09108"}
{"created":"2025-05-15","title":"FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis","abstract":"Due to the deformability of garments, generating a large amount of high-quality data for robotic garment manipulation tasks is highly challenging. In this paper, we present a synthetic garment dataset that can be used for robotic garment folding. We begin by constructing geometric garment templates based on keypoints and applying generative models to generate realistic texture patterns. Leveraging these keypoint annotations, we generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. To improve robustness, we propose KG-DAgger, which uses a keypoint-based strategy to generate demonstration data for recovering from failures. KG-DAgger significantly improves the model performance, boosting the real-world success rate by 25\\%. After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75\\% success rate in the real world. Experiments in both simulation and real-world settings validate the effectiveness of our proposed framework.","authors":["Yuxing Chen","Bowen Xiao","He Wang"],"url":"https://arxiv.org/abs/2505.09109"}
{"created":"2025-05-15","title":"Toward Malicious Clients Detection in Federated Learning","abstract":"Federated learning (FL) enables multiple clients to collaboratively train a global machine learning model without sharing their raw data. However, the decentralized nature of FL introduces vulnerabilities, particularly to poisoning attacks, where malicious clients manipulate their local models to disrupt the training process. While Byzantine-robust aggregation rules have been developed to mitigate such attacks, they remain inadequate against more advanced threats. In response, recent advancements have focused on FL detection techniques to identify potentially malicious participants. Unfortunately, these methods often misclassify numerous benign clients as threats or rely on unrealistic assumptions about the server's capabilities. In this paper, we propose a novel algorithm, SafeFL, specifically designed to accurately identify malicious clients in FL. The SafeFL approach involves the server collecting a series of global models to generate a synthetic dataset, which is then used to distinguish between malicious and benign models based on their behavior. Extensive testing demonstrates that SafeFL outperforms existing methods, offering superior efficiency and accuracy in detecting malicious clients.","authors":["Zhihao Dou","Jiaqi Wang","Wei Sun","Zhuqing Liu","Minghong Fang"],"url":"https://arxiv.org/abs/2505.09110"}
{"created":"2025-05-15","title":"Sequential Treatment Effect Estimation with Unmeasured Confounders","abstract":"This paper studies the cumulative causal effects of sequential treatments in the presence of unmeasured confounders. It is a critical issue in sequential decision-making scenarios where treatment decisions and outcomes dynamically evolve over time. Advanced causal methods apply transformer as a backbone to model such time sequences, which shows superiority in capturing long time dependence and periodic patterns via attention mechanism. However, even they control the observed confounding, these estimators still suffer from unmeasured confounders, which influence both treatment assignments and outcomes. How to adjust the latent confounding bias in sequential treatment effect estimation remains an open challenge. Therefore, we propose a novel Decomposing Sequential Instrumental Variable framework for CounterFactual Regression (DSIV-CFR), relying on a common negative control assumption. Specifically, an instrumental variable (IV) is a special negative control exposure, while the previous outcome serves as a negative control outcome. This allows us to recover the IVs latent in observation variables and estimate sequential treatment effects via a generalized moment condition. We conducted experiments on 4 datasets and achieved significant performance in one- and multi-step prediction, supported by which we can identify optimal treatments for dynamic systems.","authors":["Yingrong Wang","Anpeng Wu","Baohong Li","Ziyang Xiao","Ruoxuan Xiong","Qing Han","Kun Kuang"],"url":"https://arxiv.org/abs/2505.09113"}
{"created":"2025-05-15","title":"Beyond the Known: Decision Making with Counterfactual Reasoning Decision Transformer","abstract":"Decision Transformers (DT) play a crucial role in modern reinforcement learning, leveraging offline datasets to achieve impressive results across various domains. However, DT requires high-quality, comprehensive data to perform optimally. In real-world applications, the lack of training data and the scarcity of optimal behaviours make training on offline datasets challenging, as suboptimal data can hinder performance. To address this, we propose the Counterfactual Reasoning Decision Transformer (CRDT), a novel framework inspired by counterfactual reasoning. CRDT enhances DT ability to reason beyond known data by generating and utilizing counterfactual experiences, enabling improved decision-making in unseen scenarios. Experiments across Atari and D4RL benchmarks, including scenarios with limited data and altered dynamics, demonstrate that CRDT outperforms conventional DT approaches. Additionally, reasoning counterfactually allows the DT agent to obtain stitching abilities, combining suboptimal trajectories, without architectural modifications. These results highlight the potential of counterfactual reasoning to enhance reinforcement learning agents' performance and generalization capabilities.","authors":["Minh Hoang Nguyen","Linh Le Pham Van","Thommen George Karimpanal","Sunil Gupta","Hung Le"],"url":"https://arxiv.org/abs/2505.09114"}
{"created":"2025-05-15","title":"PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence","abstract":"Advance Care Planning (ACP) allows individuals to specify their preferred end-of-life life-sustaining treatments before they become incapacitated by injury or terminal illness (e.g., coma, cancer, dementia). While online ACP offers high accessibility, it lacks key benefits of clinical consultations, including personalized value exploration, immediate clarification of decision consequences. To bridge this gap, we conducted two formative studies: 1) shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and social workers (18 patients total), and 2) interviewed 14 users of ACP websites. Building on these insights, we designed PreCare in collaboration with 6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed to guide users through exploring personal values, gaining ACP knowledge, and supporting informed decision-making. A usability study (n=12) showed that PreCare achieved a System Usability Scale (SUS) rating of excellent. A comparative evaluation (n=12) showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.","authors":["Yu Lun Hsu (National Taiwan University)","Yun-Rung Chou (National Taiwan University)","Chiao-Ju Chang (National Taiwan University)","Yu-Cheng Chang (National Taiwan University)","Zer-Wei Lee (National Taiwan University)","Rokas Gipi\\v{s}kis (Vilnius University)","Rachel Li (University of California","Berkeley)","Chih-Yuan Shih (National Taiwan University Hospital)","Jen-Kuei Peng (National Taiwan University Hospital)","Hsien-Liang Huang (National Taiwan University Hospital)","Jaw-Shiun Tsai (National Taiwan University Hospital)","Mike Y. Chen"],"url":"https://arxiv.org/abs/2505.09115"}
{"created":"2025-05-15","title":"A Method for Assisting Novices Creating Class Diagrams Based on the Instructor's Class Layout","abstract":"Nowadays, modeling exercises on software development objects are conducted in higher education institutions for information technology. Not only are there many defects such as missing elements in the models created by learners during the exercises, but the layout of elements in the class diagrams often differs significantly from the correct answers created by the instructors. In this paper, we focus on the above problem and propose a method to provide effective support to learners during modeling exercises by automatically converting the layout of the learner's class diagram to that of the instructor, in addition to indicating the correctness of the artifacts to the learners during the exercises. The proposed method was implemented and evaluated as a tool, and the results indicate that the automatic layout conversion was an effective feedback to the learners.","authors":["Yuta Saito","Takehiro Kokubu","Takafumi Tanaka","Atsuo Hazeyama","Hiroaki Hashiura"],"url":"https://arxiv.org/abs/2505.09116"}
{"created":"2025-05-15","title":"Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning","abstract":"Traditional scene graphs primarily focus on spatial relationships, limiting vision-language models' (VLMs) ability to reason about complex interactions in visual scenes. This paper addresses two key challenges: (1) conventional detection-to-construction methods produce unfocused, contextually irrelevant relationship sets, and (2) existing approaches fail to form persistent memories for generalizing interaction reasoning to new scenes. We propose Interaction-augmented Scene Graph Reasoning (ISGR), a framework that enhances VLMs' interactional reasoning through three complementary components. First, our dual-stream graph constructor combines SAM-powered spatial relation extraction with interaction-aware captioning to generate functionally salient scene graphs with spatial grounding. Second, we employ targeted interaction queries to activate VLMs' latent knowledge of object functionalities, converting passive recognition into active reasoning about how objects work together. Finally, we introduce a lone-term memory reinforcement learning strategy with a specialized interaction-focused reward function that transforms transient patterns into long-term reasoning heuristics. Extensive experiments demonstrate that our approach significantly outperforms baseline methods on interaction-heavy reasoning benchmarks, with particularly strong improvements on complex scene understanding tasks. The source code can be accessed at https://github.com/open_upon_acceptance.","authors":["Dayong Liang","Changmeng Zheng","Zhiyuan Wen","Yi Cai","Xiao-Yong Wei","Qing Li"],"url":"https://arxiv.org/abs/2505.09118"}
{"created":"2025-05-15","title":"Model Identification Adaptive Control with $\\rho$-POMDP Planning","abstract":"Accurate system modeling is crucial for safe, effective control, as misidentification can lead to accumulated errors, especially under partial observability. We address this problem by formulating informative input design (IID) and model identification adaptive control (MIAC) as belief space planning problems, modeled as partially observable Markov decision processes with belief-dependent rewards ($\\rho$-POMDPs). We treat system parameters as hidden state variables that must be localized while simultaneously controlling the system. We solve this problem with an adapted belief-space iterative Linear Quadratic Regulator (BiLQR). We demonstrate it on fully and partially observable tasks for cart-pole and steady aircraft flight domains. Our method outperforms baselines such as regression, filtering, and local optimal control methods, even under instantaneous disturbances to system parameters.","authors":["Michelle Ho","Arec Jamgochian","Mykel J. Kochenderfer"],"url":"https://arxiv.org/abs/2505.09119"}
{"created":"2025-05-15","title":"Promoting SAM for Camouflaged Object Detection via Selective Key Point-based Guidance","abstract":"Big model has emerged as a new research paradigm that can be applied to various down-stream tasks with only minor effort for domain adaption. Correspondingly, this study tackles Camouflaged Object Detection (COD) leveraging the Segment Anything Model (SAM). The previous studies declared that SAM is not workable for COD but this study reveals that SAM works if promoted properly, for which we devise a new framework to render point promotions: First, we develop the Promotion Point Targeting Network (PPT-net) to leverage multi-scale features in predicting the probabilities of camouflaged objects' presences at given candidate points over the image. Then, we develop a key point selection (KPS) algorithm to deploy both positive and negative point promotions contrastively to SAM to guide the segmentation. It is the first work to facilitate big model for COD and achieves plausible results experimentally over the existing methods on 3 data sets under 6 metrics. This study demonstrates an off-the-shelf methodology for COD by leveraging SAM, which gains advantage over designing professional models from scratch, not only in performance, but also in turning the problem to a less challenging task, that is, seeking informative but not exactly precise promotions.","authors":["Guoying Liang","Su Yang"],"url":"https://arxiv.org/abs/2505.09123"}
{"created":"2025-05-15","title":"Quasi-3D beam theory based on equilibrium stress definition and mixed element model for accurate analysis of functionally graded beams","abstract":"This paper presents a novel quasi-3D theory and the corresponding mixed beam element model to achieve accurate solutions for functionally graded beams. The key innovations include the development of equilibrium-based stress expressions, the modified cross-sectional stiffness matrix, and the mixed beam element model based on semi-analytical definition of internal force fields. In contrast to the conventional quasi-3D theory where stress expressions are derived from constitutive equations and geometric relations, the stress expressions in this study are derived from the differential equilibrium equations among stresses, ensuring strict adherence of stress solutions to equilibrium conditions. To incorporate the influence of equilibrium-derived stress distributions, the modified cross-sectional stiffness matrix is derived, enhancing the theoretical and practical feasibility of the beam model. For beam element construction, the mixed variational principle of two-field variables is employed, with generalized internal forces and generalized displacements regarded as two independent fields. Especially, semi-analytical internal force fields, which partially satisfy the differential equilibrium equations, are introduced to improve the element performance. Numerical examples are conducted to verify the accuracy and effectiveness of the proposed theory and beam element.","authors":["Wenxiong Li","Zhiwei Liu","Suiyin Chen","Gengying Li"],"url":"https://arxiv.org/abs/2505.09127"}
{"created":"2025-05-15","title":"WSCIF: A Weakly-Supervised Color Intelligence Framework for Tactical Anomaly Detection in Surveillance Keyframes","abstract":"The deployment of traditional deep learning models in high-risk security tasks in an unlabeled, data-non-exploitable video intelligence environment faces significant challenges. In this paper, we propose a lightweight anomaly detection framework based on color features for surveillance video clips in a high sensitivity tactical mission, aiming to quickly identify and interpret potential threat events under resource-constrained and data-sensitive conditions. The method fuses unsupervised KMeans clustering with RGB channel histogram modeling to achieve composite detection of structural anomalies and color mutation signals in key frames. The experiment takes an operation surveillance video occurring in an African country as a research sample, and successfully identifies multiple highly anomalous frames related to high-energy light sources, target presence, and reflective interference under the condition of no access to the original data. The results show that this method can be effectively used for tactical assassination warning, suspicious object screening and environmental drastic change monitoring with strong deployability and tactical interpretation value. The study emphasizes the importance of color features as low semantic battlefield signal carriers, and its battlefield intelligent perception capability will be further extended by combining graph neural networks and temporal modeling in the future.","authors":["Wei Meng"],"url":"https://arxiv.org/abs/2505.09129"}
{"created":"2025-05-15","title":"Fair Clustering via Alignment","abstract":"Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute. While recently developed fair clustering algorithms optimize clustering objectives under specific fairness constraints, their inherent complexity or approximation often results in suboptimal clustering utility or numerical instability in practice. To resolve these limitations, we propose a new fair clustering algorithm based on a novel decomposition of the fair K-means clustering objective function. The proposed algorithm, called Fair Clustering via Alignment (FCA), operates by alternately (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space. A key advantage of FCA is that it theoretically guarantees approximately optimal clustering utility for any given fairness level without complex constraints, thereby enabling high-utility fair clustering in practice. Experiments show that FCA outperforms existing methods by (i) attaining a superior trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness without numerical instability.","authors":["Kunwoong Kim","Jihu Lee","Sangchul Park","Yongdai Kim"],"url":"https://arxiv.org/abs/2505.09131"}
{"created":"2025-05-15","title":"Initial Algebra Correspondence under Reachability Conditions","abstract":"Suitable reachability conditions can make two different fixed point semantics of a transition system coincide. For instance, the total and partial expected reward semantics on Markov chains (MCs) coincide whenever the MC at hand is almost surely reachable. In this paper, we present a unifying framework for such reachability conditions that ensures the correspondence of two different semantics. Our categorical framework naturally induces an abstract reachability condition via a suitable adjunction, which allows us to prove coincidences of fixed points, and more generally of initial algebras. We demonstrate the generality of our approach by instantiating several examples, including the almost surely reachability condition for MCs, and the unambiguity condition of automata. We further study a canonical construction of our instance for Markov decision processes by pointwise Kan extensions.","authors":["Mayuko Kori","Kazuki Watanabe","Jurriaan Rot"],"url":"https://arxiv.org/abs/2505.09132"}
{"created":"2025-05-15","title":"Scaling Gaussian Process Regression with Full Derivative Observations","abstract":"We present a scalable Gaussian Process (GP) method that can fit and predict full derivative observations called DSoftKI. It extends SoftKI, a method that approximates a kernel via softmax interpolation from learned interpolation point locations, to the setting with derivatives. DSoftKI enhances SoftKI's interpolation scheme to incorporate the directional orientation of interpolation points relative to the data. This enables the construction of a scalable approximate kernel, including its first and second-order derivatives, through interpolation. We evaluate DSoftKI on a synthetic function benchmark and high-dimensional molecular force field prediction (100-1000 dimensions), demonstrating that DSoftKI is accurate and can scale to larger datasets with full derivative observations than previously possible.","authors":["Daniel Huang"],"url":"https://arxiv.org/abs/2505.09134"}
{"created":"2025-05-15","title":"Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models","abstract":"Vision-language models (VLMs) offer flexible object detection through natural language prompts but suffer from performance variability depending on prompt phrasing. In this paper, we introduce a method for automated prompt refinement using a novel metric called the Contrastive Class Alignment Score (CCAS), which ranks prompts based on their semantic alignment with a target object class while penalizing similarity to confounding classes. Our method generates diverse prompt candidates via a large language model and filters them through CCAS, computed using prompt embeddings from a sentence transformer. We evaluate our approach on challenging object categories, demonstrating that our automatic selection of high-precision prompts improves object detection accuracy without the need for additional model training or labeled data. This scalable and model-agnostic pipeline offers a principled alternative to manual prompt engineering for VLM-based detection systems.","authors":["Lucas Choi","Ross Greer"],"url":"https://arxiv.org/abs/2505.09139"}
{"created":"2025-05-15","title":"TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation","abstract":"Recent advancements in Diffusion Transformer (DiT) models have significantly improved 3D point cloud generation. However, existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries. To address this limitation, we propose TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure for 3D point cloud generation. Specifically, we design the bottleneck structure utilizing Perceiver Resampler, which not only offers a mode to integrate topological information extracted through persistent homology into feature learning, but also adaptively filters out redundant local features to improve training efficiency. Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning. Videos and code are available at https://github.com/Zechao-Guan/TopoDiT-3D.","authors":["Zechao Guan","Feng Yan","Shuai Du","Lin Ma","Qingshan Liu"],"url":"https://arxiv.org/abs/2505.09140"}
{"created":"2025-05-15","title":"ELIS: Efficient LLM Iterative Scheduling System with Response Length Predictor","abstract":"We propose ELIS, a serving system for Large Language Models (LLMs) featuring an Iterative Shortest Remaining Time First (ISRTF) scheduler designed to efficiently manage inference tasks with the shortest remaining tokens. Current LLM serving systems often employ a first-come-first-served scheduling strategy, which can lead to the \"head-of-line blocking\" problem. To overcome this limitation, it is necessary to predict LLM inference times and apply a shortest job first scheduling strategy. However, due to the auto-regressive nature of LLMs, predicting the inference latency is challenging. ELIS addresses this challenge by training a response length predictor for LLMs using the BGE model, an encoder-based state-of-the-art model. Additionally, we have devised the ISRTF scheduling strategy, an optimization of shortest remaining time first tailored to existing LLM iteration batching. To evaluate our work in an industrial setting, we simulate streams of requests based on our study of real-world user LLM serving trace records. Furthermore, we implemented ELIS as a cloud-native scheduler system on Kubernetes to evaluate its performance in production environments. Our experimental results demonstrate that ISRTF reduces the average job completion time by up to 19.6%.","authors":["Seungbeom Choi","Jeonghoe Goo","Eunjoo Jeon","Mingyu Yang","Minsung Jang"],"url":"https://arxiv.org/abs/2505.09142"}
{"created":"2025-05-15","title":"Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation","abstract":"We present Latent Theory of Mind (LatentToM), a decentralized diffusion policy architecture for collaborative robot manipulation. Our policy allows multiple manipulators with their own perception and computation to collaborate with each other towards a common task goal with or without explicit communication. Our key innovation lies in allowing each agent to maintain two latent representations: an ego embedding specific to the robot, and a consensus embedding trained to be common to both robots, despite their different sensor streams and poses. We further let each robot train a decoder to infer the other robot's ego embedding from their consensus embedding, akin to theory of mind in latent space. Training occurs centrally, with all the policies' consensus encoders supervised by a loss inspired by sheaf theory, a mathematical theory for clustering data on a topological manifold. Specifically, we introduce a first-order cohomology loss to enforce sheaf-consistent alignment of the consensus embeddings. To preserve the expressiveness of the consensus embedding, we further propose structural constraints based on theory of mind and a directional consensus mechanism. Execution can be fully distributed, requiring no explicit communication between policies. In which case, the information is exchanged implicitly through each robot's sensor stream by observing the actions of the other robots and their effects on the scene. Alternatively, execution can leverage direct communication to share the robots' consensus embeddings, where the embeddings are shared once during each inference step and are aligned using the sheaf Laplacian. In our hardware experiments, LatentToM outperforms a naive decentralized diffusion baseline, and shows comparable performance with a state-of-the-art centralized diffusion policy for bi-manual manipulation. Project website: https://stanfordmsl.github.io/LatentToM/.","authors":["Chengyang He","Gadiel Sznaier Camps","Xu Liu","Mac Schwager","Guillaume Sartoretti"],"url":"https://arxiv.org/abs/2505.09144"}
{"created":"2025-05-15","title":"Robot-Assisted Drone Recovery on a Wavy Surface Using Error-State Kalman Filter and Receding Horizon Model Predictive Control","abstract":"Recovering a drone on a disturbed water surface remains a significant challenge in maritime robotics. In this paper, we propose a unified framework for Robot-Assisted Drone Recovery on a Wavy Surface that addresses two major tasks: Firstly, accurate prediction of a moving drone's position under wave-induced disturbances using an Error-State Kalman Filter (ESKF), and secondly, effective motion planning for a manipulator via Receding Horizon Control (RHC). Specifically, the ESKF predicts the drone's future position 0.5s ahead, while the manipulator plans a capture trajectory in real time, thus overcoming not only wave-induced base motions but also limited torque constraints. We provide a system design that comprises a manipulator subsystem and a UAV subsystem. On the UAV side, we detail how position control and suspended payload strategies are implemented. On the manipulator side, we show how an RHC scheme outperforms traditional low-level control algorithms. Simulation and real-world experiments - using wave-disturbed motion data - demonstrate that our approach achieves a high success rate - above 95% and outperforms conventional baseline methods by up to 10% in efficiency and 20% in precision. The results underscore the feasibility and robustness of our system, which achieves state-of-the-art (SOTA) performance and offers a practical solution for maritime drone operations.","authors":["Yimou Wu","Mingyang Liang","Ruoyu Xu"],"url":"https://arxiv.org/abs/2505.09145"}
{"created":"2025-05-15","title":"Spatial public goods games with queueing and reputation","abstract":"In real-world social and economic systems, the provisioning of public goods generally entails continuous interactions among individuals, with decisions to cooperate or defect being influenced by dynamic factors such as timing, resource availability, and the duration of engagement. However, the traditional public goods game ignores the asynchrony of the strategy adopted by players in the game. To address this problem, we propose a spatial public goods game that integrates an M/M/1 queueing system to simulate the dynamic flow of player interactions. We use a birth-death process to characterize the stochastic dynamics of this queueing system, with players arriving following a Poisson process and service times being exponentially distributed under a first-come-first-served basis with finite queue capacity. We also incorporate reputation so that players who have cooperated in the past are more likely to be chosen for future interactions. Our research shows that a high arrival rate, low service rate, and the reputation mechanism jointly facilitate the emergence of cooperative individuals in the network, which thus provides an interesting and new perspective for the provisioning of public goods.","authors":["Gui Zhang","Xiaojin Xiong","Bin Pin","Minyu Feng","Matja\\v{z} Perc"],"url":"https://arxiv.org/abs/2505.09154"}
{"created":"2025-05-15","title":"AMSnet 2.0: A Large AMS Database with AI Segmentation for Net Detection","abstract":"Current multimodal large language models (MLLMs) struggle to understand circuit schematics due to their limited recognition capabilities. This could be attributed to the lack of high-quality schematic-netlist training data. Existing work such as AMSnet applies schematic parsing to generate netlists. However, these methods rely on hard-coded heuristics and are difficult to apply to complex or noisy schematics in this paper. We therefore propose a novel net detection mechanism based on segmentation with high robustness. The proposed method also recovers positional information, allowing digital reconstruction of schematics. We then expand AMSnet dataset with schematic images from various sources and create AMSnet 2.0. AMSnet 2.0 contains 2,686 circuits with schematic images, Spectre-formatted netlists, OpenAccess digital schematics, and positional information for circuit components and nets, whereas AMSnet only includes 792 circuits with SPICE netlists but no digital schematics.","authors":["Yichen Shi","Zhuofu Tao","Yuhao Gao","Li Huang","Hongyang Wang","Zhiping Yu","Ting-Jung Lin","Lei He"],"url":"https://arxiv.org/abs/2505.09155"}
{"created":"2025-05-15","title":"A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning","abstract":"Current applications of self-supervised learning to wireless channel representation often borrow paradigms developed for text and image processing, without fully addressing the unique characteristics and constraints of wireless communications. Aiming to fill this gap, we first propose WiMAE (Wireless Masked Autoencoder), a transformer-based encoder-decoder foundation model pretrained on a realistic open-source multi-antenna wireless channel dataset. Building upon this foundation, we develop ContraWiMAE, which enhances WiMAE by incorporating a contrastive learning objective alongside the reconstruction task in a unified multi-task framework. By warm-starting from pretrained WiMAE weights and generating positive pairs via noise injection, the contrastive component enables the model to capture both structural and discriminative features, enhancing representation quality beyond what reconstruction alone can achieve. Through extensive evaluation on unseen scenarios, we demonstrate the effectiveness of both approaches across multiple downstream tasks, with ContraWiMAE showing further improvements in linear separability and adaptability in diverse wireless environments. Comparative evaluations against a state-of-the-art wireless channel foundation model confirm the superior performance and data efficiency of our models, highlighting their potential as powerful baselines for future research in self-supervised wireless channel representation learning.","authors":["Berkay Guler","Giovanni Geraci","Hamid Jafarkhani"],"url":"https://arxiv.org/abs/2505.09160"}
{"created":"2025-05-15","title":"Theoretical and Experimental Assessment of Large Beam Codebook at mmWave Devices: How Much is Enough?","abstract":"Modern millimeter wave (mmWave) transceivers come with a large number of antennas, each of which can support thousands of phase shifter configurations. This capability enables beam sweeping with fine angular resolution, but results in large codebook sizes that can span more than six orders of magnitude. On the other hand, the mobility of user terminals and their randomly changing orientations require constantly adjusting the beam direction. A key focus of recent research has been on the design of beam sweeping codebooks that balance a trade-off between the achievable gain and the beam search time, governed by the codebook size. In this paper, we investigate the extent to which a large codebook can be reduced to fewer steering vectors while covering the entire angular space and maintaining performance close to the maximum array gain. We derive a closed-form expression for the angular coverage range of a steering vector, subject to maintaining a gain loss within \\(\\gamma\\) dB (e.g., 2\\, dB) with respect to the maximum gain achieved by an infinitely large codebook. We demonstrate, both theoretically and experimentally, that a large beam-steering codebooks (such as the \\(1024^{16}\\) set considered in our experiment) can be reduced to just a few steering vectors. This framework serves as a proof that only a few steering vectors are sufficient to achieve near-maximum gain, challenging the common belief that a large codebook with fine angular resolution is essential to fully reap the benefits of an antenna array.","authors":["Bora Bozkurt","Hasan Atalay Gunel","Mohaned Chraiti","Ibrahim Hokelek","Ali Gorcin","Ali Ghrayeb"],"url":"https://arxiv.org/abs/2505.09162"}
{"created":"2025-05-15","title":"Adaptive Migration Decision for Multi-Tenant Memory Systems","abstract":"Tiered memory systems consisting of fast small memory and slow large memory have emerged to provide high capacity memory in a cost-effective way. The effectiveness of tiered memory systems relies on how many memory accesses can be absorbed by the fast first-tier memory by page migration. The recent studies proposed several different ways of detecting hot pages and migrating them efficiently. However, our investigation shows that page migration is not always beneficial as it has the associated cost of detecting and migrating hot pages. When an application is unfriendly to migration, it is often better not to migrate pages at all. Based on the observation on migration friendliness, this paper proposes a migration control framework for multi-tenant tiered memory systems. First, it proposes a detection mechanism for migration friendliness, using per-page ping-pong status. Ping-pong pages which are promoted and demoted repeatedly in a short period of time tells migration effectiveness. Based on their change behaviors, migration is stopped or continued. After the page migration is stopped, the second mechanism detects changes of memory access patterns in a low cost way to determine whether migration needs to be resumed. Finally, as each application has a different behavior, our framework provides per-process migration control to selectively stop and start migration depending on application characteristics. We implement the framework in the Linux kernel. The evaluation with a commercial CXL-based tiered memory system shows that it effectively controls migration in single and multi-tenant environments.","authors":["Hyungjun Cho","Igjae Kim","Kwanghoon Choi","Hongjin Kim","Wonjae Lee","Junhyeok Im","Jinin So","Jaehyuk Huh"],"url":"https://arxiv.org/abs/2505.09164"}
{"created":"2025-05-15","title":"BusOut is NP-complete","abstract":"This study examines the computational complexity of the decision problem modeled on the smartphone game Bus Out. The objective of the game is to load all the passengers in a queue onto appropriate buses using a limited number of bus parking spots by selecting and dispatching the buses on a map. We show that the problem is NP-complete, even for highly restricted instances. We also show that it is hard to approximate the minimum number of parking spots needed to solve a given instance.","authors":["Takehiro Ishibashi","Ryo Yoshinaka","Ayumi Shinohara"],"url":"https://arxiv.org/abs/2505.09165"}
{"created":"2025-05-15","title":"An Initial Exploration of Default Images in Text-to-Image Generation","abstract":"In the creative practice of text-to-image generation (TTI), images are generated from text prompts. However, TTI models are trained to always yield an output, even if the prompt contains unknown terms. In this case, the model may generate what we call \"default images\": images that closely resemble each other across many unrelated prompts. We argue studying default images is valuable for designing better solutions for TTI and prompt engineering. In this paper, we provide the first investigation into default images on Midjourney, a popular image generator. We describe our systematic approach to create input prompts triggering default images, and present the results of our initial experiments and several small-scale ablation studies. We also report on a survey study investigating how default images affect user satisfaction. Our work lays the foundation for understanding default images in TTI and highlights challenges and future research directions.","authors":["Hannu Simonen","Atte Kiviniemi","Jonas Oppenlaender"],"url":"https://arxiv.org/abs/2505.09166"}
{"created":"2025-05-15","title":"DRRNet: Macro-Micro Feature Fusion and Dual Reverse Refinement for Camouflaged Object Detection","abstract":"The core challenge in Camouflage Object Detection (COD) lies in the indistinguishable similarity between targets and backgrounds in terms of color, texture, and shape. This causes existing methods to either lose edge details (such as hair-like fine structures) due to over-reliance on global semantic information or be disturbed by similar backgrounds (such as vegetation patterns) when relying solely on local features. We propose DRRNet, a four-stage architecture characterized by a \"context-detail-fusion-refinement\" pipeline to address these issues. Specifically, we introduce an Omni-Context Feature Extraction Module to capture global camouflage patterns and a Local Detail Extraction Module to supplement microstructural information for the full-scene context module. We then design a module for forming dual representations of scene understanding and structural awareness, which fuses panoramic features and local features across various scales. In the decoder, we also introduce a reverse refinement module that leverages spatial edge priors and frequency-domain noise suppression to perform a two-stage inverse refinement of the output. By applying two successive rounds of inverse refinement, the model effectively suppresses background interference and enhances the continuity of object boundaries. Experimental results demonstrate that DRRNet significantly outperforms state-of-the-art methods on benchmark datasets. Our code is available at https://github.com/jerrySunning/DRRNet.","authors":["Jianlin Sun","Xiaolin Fang","Juwei Guan","Dongdong Gui","Teqi Wang","Tongxin Zhu"],"url":"https://arxiv.org/abs/2505.09168"}
{"created":"2025-05-15","title":"Automated SAR ADC Sizing Using Analytical Equations","abstract":"Conventional analog and mixed-signal (AMS) circuit designs heavily rely on manual effort, which is time-consuming and labor-intensive. This paper presents a fully automated design methodology for Successive Approximation Register (SAR) Analog-to-Digital Converters (ADCs) from performance specifications to complete transistor sizing. To tackle the high-dimensional sizing problem, we propose a dual optimization scheme. The system-level optimization iteratively partitions the overall requirements and analytically maps them to subcircuit design specifications, while local optimization loops determines the subcircuits' design parameters. The dependency graph-based framework serializes the simulations for verification, knowledge-based calculations, and transistor sizing optimization in topological order, which eliminates the need for human intervention. We demonstrate the effectiveness of the proposed methodology through two case studies with varying performance specifications, achieving high SNDR and low power consumption while meeting all the specified design constraints.","authors":["Zhongyi Li","Zhuofu Tao","Yanze Zhou","Yichen Shi","Zhiping Yu","Ting-Jung Lin","Lei He"],"url":"https://arxiv.org/abs/2505.09172"}
{"created":"2025-05-15","title":"Quotient Complex Transformer (QCformer) for Perovskite Data Analysis","abstract":"The discovery of novel functional materials is crucial in addressing the challenges of sustainable energy generation and climate change. Hybrid organic-inorganic perovskites (HOIPs) have gained attention for their exceptional optoelectronic properties in photovoltaics. Recently, geometric deep learning, particularly graph neural networks (GNNs), has shown strong potential in predicting material properties and guiding material design. However, traditional GNNs often struggle to capture the periodic structures and higher-order interactions prevalent in such systems. To address these limitations, we propose a novel representation based on quotient complexes (QCs) and introduce the Quotient Complex Transformer (QCformer) for material property prediction. A material structure is modeled as a quotient complex, which encodes both pairwise and many-body interactions via simplices of varying dimensions and captures material periodicity through a quotient operation. Our model leverages higher-order features defined on simplices and processes them using a simplex-based Transformer module. We pretrain QCformer on benchmark datasets such as the Materials Project and JARVIS, and fine-tune it on HOIP datasets. The results show that QCformer outperforms state-of-the-art models in HOIP property prediction, demonstrating its effectiveness. The quotient complex representation and QCformer model together contribute a powerful new tool for predictive modeling of perovskite materials.","authors":["Xinyu You","Xiang Liu","Chuan-Shen Hu","Kelin Xia","Tze Chien Sum"],"url":"https://arxiv.org/abs/2505.09174"}
{"created":"2025-05-15","title":"Optimizing Urban Critical Green Space Development Using Machine Learning","abstract":"This paper presents a novel framework for prioritizing urban green space development in Tehran using diverse socio-economic, environmental, and sensitivity indices. The indices were derived from various sources including Google Earth Engine, air pollution measurements, municipal reports and the Weather Research & Forecasting (WRF) model. The WRF model was used to estimate the air temperature at a 1 km resolution due to insufficient meteorological stations, yielding RMSE and MAE values of 0.96{\\deg}C and 0.92{\\deg}C, respectively. After data preparation, several machine learning models were used for binary vegetation cover classification including XGBoost, LightGBM, Random Forest (RF) and Extra Trees. RF achieved the highest performance, exceeding 94% in Overall Accuracy, Recall, and F1-score. Then, the probability of areas lacking vegetation cover was assessed using socio-economic, environmental and sensitivity indices. This resulted in the RF generating an urban green space development prioritization map. Feature Importance Analysis revealed that the most significant indices were nightly land surface temperature (LST) and sensitive population. Finally, the framework performance was validated through microclimate simulation to assess the critical areas after and before the green space development by green roofs. The simulation demonstrated reducing air temperature by up to 0.67{\\deg}C after utilizing the green roof technology in critical areas. As a result, this framework provides a valuable tool for urban planners to develop green spaces.","authors":["Mohammad Ganjirad","Mahmoud Reza Delavar","Hossein Bagheri","Mohammad Mehdi Azizi"],"url":"https://arxiv.org/abs/2505.09175"}
{"created":"2025-05-15","title":"UniCAD: Efficient and Extendable Architecture for Multi-Task Computer-Aided Diagnosis System","abstract":"The growing complexity and scale of visual model pre-training have made developing and deploying multi-task computer-aided diagnosis (CAD) systems increasingly challenging and resource-intensive. Furthermore, the medical imaging community lacks an open-source CAD platform to enable the rapid creation of efficient and extendable diagnostic models. To address these issues, we propose UniCAD, a unified architecture that leverages the robust capabilities of pre-trained vision foundation models to seamlessly handle both 2D and 3D medical images while requiring only minimal task-specific parameters. UniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation strategy is employed to adapt a pre-trained visual model to the medical image domain, achieving performance on par with fully fine-tuned counterparts while introducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular architecture that combines a frozen foundation model with multiple plug-and-play experts, enabling diverse tasks and seamless functionality expansion. Building on this unified CAD architecture, we establish an open-source platform where researchers can share and access lightweight CAD experts, fostering a more equitable and efficient research ecosystem. Comprehensive experiments across 12 diverse medical datasets demonstrate that UniCAD consistently outperforms existing methods in both accuracy and deployment efficiency. The source code and project page are available at https://mii-laboratory.github.io/UniCAD/.","authors":["Yitao Zhu","Yuan Yin","Zhenrong Shen","Zihao Zhao","Haiyu Song","Sheng Wang","Dinggang Shen","Qian Wang"],"url":"https://arxiv.org/abs/2505.09178"}
{"created":"2025-05-15","title":"Zero-shot Quantization: A Comprehensive Survey","abstract":"Network quantization has proven to be a powerful approach to reduce the memory and computational demands of deep learning models for deployment on resource-constrained devices. However, traditional quantization methods often rely on access to training data, which is impractical in many real-world scenarios due to privacy, security, or regulatory constraints. Zero-shot Quantization (ZSQ) emerges as a promising solution, achieving quantization without requiring any real data. In this paper, we provide a comprehensive overview of ZSQ methods and their recent advancements. First, we provide a formal definition of the ZSQ problem and highlight the key challenges. Then, we categorize the existing ZSQ methods into classes based on data generation strategies, and analyze their motivations, core ideas, and key takeaways. Lastly, we suggest future research directions to address the remaining limitations and advance the field of ZSQ. To the best of our knowledge, this paper is the first in-depth survey on ZSQ.","authors":["Minjun Kim","Jaehyeon Choi","Jongkeun Lee","Wonjin Cho","U Kang"],"url":"https://arxiv.org/abs/2505.09188"}
{"created":"2025-05-15","title":"Some Computational Tools for Solving a Selection of Problems in Control Theory","abstract":"This paper demonstrates how certified computational tools can be used to address various problems in control theory. In particular, we introduce PACE.jl, a Julia package that implements symbolic elimination techniques, including (among others) discriminant varieties and Rational Univariate Representation, while also supporting multi-precision interval computations. We showcase its applications to key control theory problems, including identification, stability analysis, and optimization, for both parameter-dependent and parameter-free systems.","authors":["Alexander Demin","Christina Katsamaki","Fabrice Rouillier"],"url":"https://arxiv.org/abs/2505.09191"}
{"created":"2025-05-15","title":"PDE: Gene Effect Inspired Parameter Dynamic Evolution for Low-light Image Enhancement","abstract":"Low-light image enhancement (LLIE) is a fundamental task in computational photography, aiming to improve illumination, reduce noise, and enhance image quality. While recent advancements focus on designing increasingly complex neural network models, we observe a peculiar phenomenon: resetting certain parameters to random values unexpectedly improves enhancement performance for some images. Drawing inspiration from biological genes, we term this phenomenon the gene effect. The gene effect limits enhancement performance, as even random parameters can sometimes outperform learned ones, preventing models from fully utilizing their capacity. In this paper, we investigate the reason and propose a solution. Based on our observations, we attribute the gene effect to static parameters, analogous to how fixed genetic configurations become maladaptive when environments change. Inspired by biological evolution, where adaptation to new environments relies on gene mutation and recombination, we propose parameter dynamic evolution (PDE) to adapt to different images and mitigate the gene effect. PDE employs a parameter orthogonal generation technique and the corresponding generated parameters to simulate gene recombination and gene mutation, separately. Experiments validate the effectiveness of our techniques. The code will be released to the public.","authors":["Tong Li","Lizhi Wang","Hansen Feng","Lin Zhu","Hua Huang"],"url":"https://arxiv.org/abs/2505.09196"}
{"created":"2025-05-15","title":"SHACL-DS: A SHACL extension to validate RDF dataset","abstract":"The Shapes Constraint Language (SHACL) provides a powerful mechanism for validating RDF data against shape constraints, but is inherently designed for single-graph validation. This limitation makes SHACL unsuitable for natively validating RDF datasets comprising multiple named graphs. To address this gap, developers must build solutions on top of SHACL, applying a shapes graph to each RDF dataset or combinations thereof using bespoke code. However, these approaches may lead to information loss, such as the named graph from which the data originates. This paper introduces SHACL-DS, an extension to SHACL that enables validation of RDF datasets. The extension adds a layer on top of SHACL, and the only disruptive change is the execution of SPARQL queries in, e.g., SPARQL-based constraints. The contributions are a SHACL-DS specification, a prototype implementation, and a set of test cases illustrating its use and providing future developers guidance in building SHACL-DS engines. This work lays the foundation for integrating dataset-level features into SHACL and encourages further exploration of advanced RDF dataset validation techniques.","authors":["Christophe Debruyne","Davan Chiem Dao"],"url":"https://arxiv.org/abs/2505.09198"}
{"created":"2025-05-15","title":"HMamba: Hyperbolic Mamba for Sequential Recommendation","abstract":"Sequential recommendation systems have become a cornerstone of personalized services, adept at modeling the temporal evolution of user preferences by capturing dynamic interaction sequences. Existing approaches predominantly rely on traditional models, including RNNs and Transformers. Despite their success in local pattern recognition, Transformer-based methods suffer from quadratic computational complexity and a tendency toward superficial attention patterns, limiting their ability to infer enduring preference hierarchies in sequential recommendation data. Recent advances in Mamba-based sequential models introduce linear-time efficiency but remain constrained by Euclidean geometry, failing to leverage the intrinsic hyperbolic structure of recommendation data. To bridge this gap, we propose Hyperbolic Mamba, a novel architecture that unifies the efficiency of Mamba's selective state space mechanism with hyperbolic geometry's hierarchical representational power. Our framework introduces (1) a hyperbolic selective state space that maintains curvature-aware sequence modeling and (2) stabilized Riemannian operations to enable scalable training. Experiments across four benchmarks demonstrate that Hyperbolic Mamba achieves 3-11% improvement while retaining Mamba's linear-time efficiency, enabling real-world deployment. This work establishes a new paradigm for efficient, hierarchy-aware sequential modeling.","authors":["Qianru Zhang","Honggang Wen","Wei Yuan","Crystal Chen","Menglin Yang","Siu-Ming Yiu","Hongzhi Yin"],"url":"https://arxiv.org/abs/2505.09205"}
{"created":"2025-05-15","title":"Educational impacts of generative artificial intelligence on learning and performance of engineering students in China","abstract":"With the rapid advancement of generative artificial intelligence(AI), its potential applications in higher education have attracted significant attention. This study investigated how 148 students from diverse engineering disciplines and regions across China used generative AI, focusing on its impact on their learning experience and the opportunities and challenges it poses in engineering education. Based on the surveyed data, we explored four key areas: the frequency and application scenarios of AI use among engineering students, its impact on students' learning and performance, commonly encountered challenges in using generative AI, and future prospects for its adoption in engineering education. The results showed that more than half of the participants reported a positive impact of generative AI on their learning efficiency, initiative, and creativity, with nearly half believing it also enhanced their independent thinking. However, despite acknowledging improved study efficiency, many felt their actual academic performance remained largely unchanged and expressed concerns about the accuracy and domain-specific reliability of generative AI. Our findings provide a first-hand insight into the current benefits and challenges generative AI brings to students, particularly Chinese engineering students, while offering several recommendations, especially from the students' perspective, for effectively integrating generative AI into engineering education.","authors":["Lei Fan","Kunyang Deng","Fangxue Liu"],"url":"https://arxiv.org/abs/2505.09208"}
{"created":"2025-05-15","title":"Towards Efficient Verification of Parallel Applications with Mc SimGrid","abstract":"Assessing the correctness of distributed and parallel applications is notoriously difficult due to the complexity of the concurrent behaviors and the difficulty to reproduce bugs. In this context, Dynamic Partial Order Reduction (DPOR) techniques have proved successful in exploiting concurrency to verify applications without exploring all their behaviors. However, they may lack of efficiency when tracking non-systematic bugs of real size applications. In this paper, we suggest two adaptations of the Optimal Dynamic Partial Order Reduction (ODPOR) algorithm with a particular focus on bug finding and explanation. The first adaptation is an out-of-order version called RFS ODPOR which avoids being stuck in uninteresting large parts of the state space. Once a bug is found, the second adaptation takes advantage of ODPOR principles to efficiently find the origins of the bug.","authors":["Matthieu Laurent (MAGELLAN","DEVINE)","Thierry J\\'eron (DEVINE)","Martin Quinson (MAGELLAN)"],"url":"https://arxiv.org/abs/2505.09209"}
{"created":"2025-05-15","title":"The Larger the Merrier? Efficient Large AI Model Inference in Wireless Edge Networks","abstract":"The growing demand for large artificial intelligence model (LAIM) services is driving a paradigm shift from traditional cloud-based inference to edge-based inference for low-latency, privacy-preserving applications. In particular, edge-device co-inference, which partitions LAIMs between edge devices and servers, has emerged as a promising strategy for resource-efficient LAIM execution in wireless networks. In this paper, we investigate a pruning-aware LAIM co-inference scheme, where a pre-trained LAIM is pruned and partitioned into on-device and on-server sub-models for deployment. For analysis, we first prove that the LAIM output distortion is upper bounded by its parameter distortion. Then, we derive a lower bound on parameter distortion via rate-distortion theory, analytically capturing the relationship between pruning ratio and co-inference performance. Next, based on the analytical results, we formulate an LAIM co-inference distortion bound minimization problem by jointly optimizing the pruning ratio, transmit power, and computation frequency under system latency, energy, and available resource constraints. Moreover, we propose an efficient algorithm to tackle the considered highly non-convex problem. Finally, extensive simulations demonstrate the effectiveness of the proposed design. In particular, model parameter distortion is shown to provide a reliable bound on output distortion. Also, the proposed joint pruning ratio and resource management design achieves superior performance in balancing trade-offs among inference performance, system latency, and energy consumption compared with benchmark schemes, such as fully on-device and on-server inference. Moreover, the split point is shown to play a critical role in system performance optimization under heterogeneous and resource-limited edge environments.","authors":["Zhonghao Lyu","Ming Xiao","Jie Xu","Mikael Skoglund","Marco Di Renzo"],"url":"https://arxiv.org/abs/2505.09214"}
{"created":"2025-05-15","title":"Birch SGD: A Tree Graph Framework for Local and Asynchronous SGD Methods","abstract":"We propose a new unifying framework, Birch SGD, for analyzing and designing distributed SGD methods. The central idea is to represent each method as a weighted directed tree, referred to as a computation tree. Leveraging this representation, we introduce a general theoretical result that reduces convergence analysis to studying the geometry of these trees. This perspective yields a purely graph-based interpretation of optimization dynamics, offering a new and intuitive foundation for method development. Using Birch SGD, we design eight new methods and analyze them alongside previously known ones, with at least six of the new methods shown to have optimal computational time complexity. Our research leads to two key insights: (i) all methods share the same \"iteration rate\" of $O\\left(\\frac{(R + 1) L \\Delta}{\\varepsilon} + \\frac{\\sigma^2 L \\Delta}{\\varepsilon^2}\\right)$, where $R$ the maximum \"tree distance\" along the main branch of a tree; and (ii) different methods exhibit different trade-offs-for example, some update iterates more frequently, improving practical performance, while others are more communication-efficient or focus on other aspects. Birch SGD serves as a unifying framework for navigating these trade-offs. We believe these results provide a unified foundation for understanding, analyzing, and designing efficient asynchronous and parallel optimization methods.","authors":["Alexander Tyurin","Danil Sivtsov"],"url":"https://arxiv.org/abs/2505.09218"}
{"created":"2025-05-15","title":"Securing P4 Programs by Information Flow Control","abstract":"Software-Defined Networking (SDN) has transformed network architectures by decoupling the control and data-planes, enabling fine-grained control over packet processing and forwarding. P4, a language designed for programming data-plane devices, allows developers to define custom packet processing behaviors directly on programmable network devices. This provides greater control over packet forwarding, inspection, and modification. However, the increased flexibility provided by P4 also brings significant security challenges, particularly in managing sensitive data and preventing information leakage within the data-plane.","authors":["Anoud Alshnakat","Amir M. Ahmadian","Musard Balliu","Roberto Guanciale","Mads Dam"],"url":"https://arxiv.org/abs/2505.09221"}
{"created":"2025-05-15","title":"QUIC Steps: Evaluating Pacing Strategies in QUIC Implementations","abstract":"Pacing is a key mechanism in modern transport protocols, used to regulate packet transmission timing to minimize traffic burstiness, lower latency, and reduce packet loss. Standardized in 2021, QUIC is a UDP-based protocol designed to improve upon the TCP / TLS stack. While the QUIC protocol recommends pacing, and congestion control algorithms like BBR rely on it, the user-space nature of QUIC introduces unique challenges. These challenges include coarse-grained timers, system call overhead, and OS scheduling delays, all of which complicate precise packet pacing. This paper investigates how pacing is implemented differently across QUIC stacks, including quiche, picoquic, and ngtcp2, and evaluates the impact of system-level features like GSO and Linux qdiscs on pacing. Using a custom measurement framework and a passive optical fiber tap, we establish a baseline with default settings and systematically explore the effects of qdiscs, hardware offloading using the ETF qdisc, and GSO on pacing precision and network performance. We also extend and evaluate a kernel patch to enable pacing of individual packets within GSO buffers, combining batching efficiency with precise pacing. Kernel-assisted and purely user-space pacing approaches are compared. We show that pacing with only user-space timers can work well, as demonstrated by picoquic with BBR. With quiche, we identify FQ as a qdisc well-suited for pacing QUIC traffic, as it is relatively easy to use and offers precise pacing based on packet timestamps. Our findings provide new insights into the trade-offs involved in implementing pacing in QUIC and highlight potential optimizations for real-world applications like video streaming and video calls.","authors":["Marcel Kempf","Simon Tietz","Benedikt Jaeger","Johannes Sp\\\"ath","Georg Carle","Johannes Zirngibl"],"url":"https://arxiv.org/abs/2505.09222"}
{"created":"2025-05-15","title":"Ethical Aspects of the Use of Social Robots in Elderly Care -- A Systematic Qualitative Review","abstract":"Background: The use of social robotics in elderly care is increasingly discussed as one way of meeting emerging care needs due to scarce resources. While many potential benefits are associated with robotic care technologies, there is a variety of ethical challenges. To support steps towards a responsible implementation and use, this review develops an overview on ethical aspects of the use of social robots in elderly care from a decision-makers' perspective.","authors":["Marianne Leineweber","Clara Victoria Keusgen","Marc Bubeck","Joschka Haltaufderheide","Robert Ranisch","Corinna Klingler"],"url":"https://arxiv.org/abs/2505.09224"}
{"created":"2025-05-15","title":"A Standardized Benchmark Set of Clustering Problem Instances for Comparing Black-Box Optimizers","abstract":"One key challenge in optimization is the selection of a suitable set of benchmark problems. A common goal is to find functions which are representative of a class of real-world optimization problems in order to ensure findings on the benchmarks will translate to relevant problem domains. While some problem characteristics are well-covered by popular benchmarking suites, others are often overlooked. One example of such a problem characteristic is permutation invariance, where the search space consists of a set of symmetrical search regions. This type of problem occurs e.g. when a set of solutions has to be found, but the ordering within this set does not matter. The data clustering problem, often seen in machine learning contexts, is a clear example of such an optimization landscape, and has thus been proposed as a base from which optimization benchmarks can be created. In addition to the symmetry aspect, these clustering problems also contain potential regions of neutrality, which can provide an additional challenge to optimization algorithms. In this paper, we present a standardized benchmark suite for the evaluation of continuous black-box optimization algorithms, based on data clustering problems. To gain insight into the diversity of the benchmark set, both internally and in comparison to existing suites, we perform a benchmarking study of a set of modular CMA-ES configurations, as well as an analysis using exploratory landscape analysis. Our benchmark set is open-source and integrated with the IOHprofiler benchmarking framework to encourage its use in future research.","authors":["Diederick Vermetten","Catalin-Viorel Dinu","Marcus Gallagher"],"url":"https://arxiv.org/abs/2505.09233"}
{"created":"2025-05-15","title":"Equilibrio de carga para transformadores de distribuci\\'on el\\'ectrica mejorando la calidad de servicio en fin de l\\'inea","abstract":"The distribution of electrical energy faces global challenges, such as increasing demand, the integration of distributed generation, high energy losses, and the need to improve service quality. In particular, load imbalance-where loads are not evenly distributed across the circuit phase-can reduce efficiency, shorten equipment lifespan, and increase susceptibility to service interruptions. While methods that involve shifting loads from one phase to another can be costly, they are effective when smart meters are available and implemented efficiently. This work proposes the use of genetic algorithms to optimally identify which loads should be reassigned in order to improve both phase balance and voltage quality at the end nodes of the network while minimizing the number of required changes. The algorithm was evaluated through simulations using PandaPower, a power flow analysis tool, modeling simple networks based on real-world characteristics of the electrical system in Tucum\\'an.","authors":["Juan M. Bord\\'on","Victor A. Jimenez","Adrian Will"],"url":"https://arxiv.org/abs/2505.09235"}
{"created":"2025-05-15","title":"Approximate Cartesian Tree Matching with One Difference","abstract":"Cartesian tree pattern matching consists of finding all the factors of a text that have the same Cartesian tree than a given pattern. There already exist theoretical and practical solutions for the exact case. In this paper, we propose the first algorithms for solving approximate Cartesian tree pattern matching with one difference given a pattern of length m and a text of length n. We present a generic algorithm that find all the factors of the text that have the same Cartesian tree of the pattern with one difference, using different notions of differences. We show that this algorithm has a O(nM) worst-case complexity and that, for several random models, the algorithm has a linear average-case complexity. We also present an automaton based algorithm, adapting [PALP19], that can be generalized to deal with more than one difference.","authors":["Bastien Auvray","Julien David","Samah Ghazawi","Richard Groult","Gad M. Landau","Thierry Lecroq"],"url":"https://arxiv.org/abs/2505.09236"}
{"created":"2025-05-15","title":"Stable and Convexified Information Bottleneck Optimization via Symbolic Continuation and Entropy-Regularized Trajectories","abstract":"The Information Bottleneck (IB) method frequently suffers from unstable optimization, characterized by abrupt representation shifts near critical points of the IB trade-off parameter, beta. In this paper, I introduce a novel approach to achieve stable and convex IB optimization through symbolic continuation and entropy-regularized trajectories. I analytically prove convexity and uniqueness of the IB solution path when an entropy regularization term is included, and demonstrate how this stabilizes representation learning across a wide range of \\b{eta} values. Additionally, I provide extensive sensitivity analyses around critical points (beta) with statistically robust uncertainty quantification (95% confidence intervals). The open-source implementation, experimental results, and reproducibility framework included in this work offer a clear path for practical deployment and future extension of my proposed method.","authors":["Faruk Alpay"],"url":"https://arxiv.org/abs/2505.09239"}
{"created":"2025-05-15","title":"The Cost of Skeletal Call-by-Need, Smoothly","abstract":"Skeletal call-by-need is an optimization of call-by-need evaluation also known as \"fully lazy sharing\": when the duplication of a value has to take place, it is first split into \"skeleton\", which is then duplicated, and \"flesh\" which is instead kept shared. Here, we provide two cost analyses of skeletal call-by-need. Firstly, we provide a family of terms showing that skeletal call-by-need can be asymptotically exponentially faster than call-by-need in both time and space; it is the first such evidence, to our knowledge. Secondly, we prove that skeletal call-by-need can be implemented efficiently, that is, with bi-linear overhead. This result is obtained by providing a new smooth presentation of ideas by Shivers and Wand for the reconstruction of skeletons, which is then smoothly plugged into the study of an abstract machine following the distillation technique by Accattoli et al.","authors":["Beniamino Accattoli","Francesco Magliocca","Lo\\\"ic Peyrot","Claudio Sacerdoti Coen"],"url":"https://arxiv.org/abs/2505.09242"}
{"created":"2025-05-15","title":"On verification and constraint generation for families of similar hybrid automata","abstract":"In this paper we give an overview of results on the analysis of parametric linear hybrid automata, and of systems of similar linear hybrid automata: We present possibilities of describing systems with a parametric (i.e. not explicitly specified) number of similar components which can be connected to other systems, such that some parts in the description might be underspecified (i.e. parametric). We consider global safety properties for such systems, expressed by universally quantified formulae, using quantification over variables ranging over the component systems. We analyze possibilities of using methods for hierarchical reasoning and symbol elimination for determining relationships on (some of) the parameters used in the description of these systems under which the global safety properties are guaranteed to be inductive invariants. We discuss an implementation and illustrate its use on several examples.","authors":["Viorica Sofronie-Stokkermans","Philipp Marohn"],"url":"https://arxiv.org/abs/2505.09244"}
{"created":"2025-05-15","title":"Focus, Merge, Rank: Improved Question Answering Based on Semi-structured Knowledge Bases","abstract":"In many real-world settings, machine learning models and interactive systems have access to both structured knowledge, e.g., knowledge graphs or tables, and unstructured content, e.g., natural language documents. However, most rely on either. Semi-Structured Knowledge Bases (SKBs) bridge this gap by linking unstructured content to nodes within structured data, thereby enabling new strategies for knowledge access and use. In this work, we present FocusedRetriever, a modular SKB-based framework for multi-hop question answering. It integrates components (VSS-based entity search, LLM-based generation of Cypher queries and pairwise re-ranking) in a way that enables it to outperform state-of-the-art methods across all three STaRK benchmark test sets, covering diverse domains and multiple performance metrics. The average first-hit rate exceeds that of the second-best method by 25.7%. FocusedRetriever leverages (1) the capacity of Large Language Models (LLMs) to extract relational facts and entity attributes from unstructured text, (2) node set joins to filter answer candidates based on these extracted triplets and constraints, (3) vector similarity search to retrieve and rank relevant unstructured content, and (4) the contextual capabilities of LLMs to finally rank the top-k answers. For generality, we only incorporate base LLMs in FocusedRetriever in our evaluation. However, our analysis of intermediate results highlights several opportunities for further upgrades including finetuning. The source code is publicly available at https://github.com/kramerlab/FocusedRetriever .","authors":["Derian Boer","Stephen Roth","Stefan Kramer"],"url":"https://arxiv.org/abs/2505.09246"}
{"created":"2025-05-15","title":"Structural Parameterization of Steiner Tree Packing","abstract":"Steiner Tree Packing (STP) is a notoriously hard problem in classical complexity theory, which is of practical relevance to VLSI circuit design. Previous research has approached this problem by providing heuristic or approximate algorithms. In this paper, we show the first FPT algorithms for STP parameterized by structural parameters of the input graph. In particular, we show that STP is fixed-parameter tractable by the tree-cut width as well as the fracture number of the input graph.","authors":["Niko Hastrich","Kirill Simonov"],"url":"https://arxiv.org/abs/2505.09250"}
{"created":"2025-05-15","title":"A Surrogate Model for the Forward Design of Multi-layered Metasurface-based Radar Absorbing Structures","abstract":"Metasurface-based radar absorbing structures (RAS) are highly preferred for applications like stealth technology, electromagnetic (EM) shielding, etc. due to their capability to achieve frequency selective absorption characteristics with minimal thickness and reduced weight penalty. However, the conventional approach for the EM design and optimization of these structures relies on forward simulations, using full wave simulation tools, to predict the electromagnetic (EM) response of candidate meta atoms. This process is computationally intensive, extremely time consuming and requires exploration of large design spaces. To overcome this challenge, we propose a surrogate model that significantly accelerates the prediction of EM responses of multi-layered metasurface-based RAS. A convolutional neural network (CNN) based architecture with Huber loss function has been employed to estimate the reflection characteristics of the RAS model. The proposed model achieved a cosine similarity of 99.9% and a mean square error of 0.001 within 1000 epochs of training. The efficiency of the model has been established via full wave simulations as well as experiment where it demonstrated significant reduction in computational time while maintaining high predictive accuracy.","authors":["Vineetha Joy","Aditya Anand","Nidhi","Anshuman Kumar","Amit Sethi","Hema Singh"],"url":"https://arxiv.org/abs/2505.09251"}
{"created":"2025-05-15","title":"Zero-Shot Multi-modal Large Language Model v.s. Supervised Deep Learning: A Comparative Study on CT-Based Intracranial Hemorrhage Subtyping","abstract":"Introduction: Timely identification of intracranial hemorrhage (ICH) subtypes on non-contrast computed tomography is critical for prognosis prediction and therapeutic decision-making, yet remains challenging due to low contrast and blurring boundaries. This study evaluates the performance of zero-shot multi-modal large language models (MLLMs) compared to traditional deep learning methods in ICH binary classification and subtyping. Methods: We utilized a dataset provided by RSNA, comprising 192 NCCT volumes. The study compares various MLLMs, including GPT-4o, Gemini 2.0 Flash, and Claude 3.5 Sonnet V2, with conventional deep learning models, including ResNet50 and Vision Transformer. Carefully crafted prompts were used to guide MLLMs in tasks such as ICH presence, subtype classification, localization, and volume estimation. Results: The results indicate that in the ICH binary classification task, traditional deep learning models outperform MLLMs comprehensively. For subtype classification, MLLMs also exhibit inferior performance compared to traditional deep learning models, with Gemini 2.0 Flash achieving an macro-averaged precision of 0.41 and a macro-averaged F1 score of 0.31. Conclusion: While MLLMs excel in interactive capabilities, their overall accuracy in ICH subtyping is inferior to deep networks. However, MLLMs enhance interpretability through language interactions, indicating potential in medical imaging analysis. Future efforts will focus on model refinement and developing more precise MLLMs to improve performance in three-dimensional medical image processing.","authors":["Yinuo Wang","Yue Zeng","Kai Chen","Cai Meng","Chao Pan","Zhouping Tang"],"url":"https://arxiv.org/abs/2505.09252"}
{"created":"2025-05-15","title":"Moving towards informative and actionable social media research","abstract":"Social media is nearly ubiquitous in modern life, and concerns have been raised about its putative societal impacts, ranging from undermining mental health and exacerbating polarization to fomenting violence and disrupting democracy. Despite extensive research, consensus on these effects remains elusive, with observational studies often highlighting concerns while randomized controlled trials (RCTs) yield conflicting or null findings. This review examines how the complexity inherent in social systems can account for such discrepancies, emphasizing that emergent societal and long-term outcomes cannot be readily inferred from individual-level effects. In complex systems, such as social networks, feedback loops, hysteresis, multi-scale dynamics, and non-linearity limit the utility of approaches for assessing causality that are otherwise robust in simpler contexts. Revisiting large-scale experiments, we explore how null or conflicting findings may reflect these complexities rather than a true absence of effects. Even in cases where the methods are appropriate, assessing the net impacts of social media provides little actionable insight given that eliminating social media is not a realistic option for whole populations. We argue that progress will require a complexity-minded approach focused on specific design choices of online platforms that triangulates experimental, observational and theoretical methods.","authors":["Joseph B. Bak-Coleman","Stephan Lewandowsky","Philipp Lorenz-Spreen","Arvind Narayanan","Amy Orben","Lisa Oswald"],"url":"https://arxiv.org/abs/2505.09254"}
{"created":"2025-05-15","title":"Data-driven Internal Model Control for Output Regulation","abstract":"Output regulation is a fundamental problem in control theory, extensively studied since the 1970s. Traditionally, research has primarily addressed scenarios where the system model is explicitly known, leaving the problem in the absence of a system model less explored. Leveraging the recent advancements in Willems et al.'s fundamental lemma, data-driven control has emerged as a powerful tool for stabilizing unknown systems. This paper tackles the output regulation problem for unknown single and multi-agent systems (MASs) using noisy data. Previous approaches have attempted to solve data-based output regulation equations (OREs), which are inadequate for achieving zero tracking error with noisy data. To circumvent the need for solving data-based OREs, we propose an internal model-based data-driven controller that reformulates the output regulation problem into a stabilization problem. This method is first applied to linear time-invariant (LTI) systems, demonstrating exact solution capabilities, i.e., zero tracking error, through solving a straightforward data-based linear matrix inequality (LMI). Furthermore, we extend our approach to solve the $k$th-order output regulation problem for nonlinear systems. Extensions to both linear and nonlinear MASs are discussed. Finally, numerical tests validate the effectiveness and correctness of the proposed controllers.","authors":["Wenjie Liu","Yifei Li","Jian Sun","Gang Wang","Keyou You","Lihua Xie","Jie Chen"],"url":"https://arxiv.org/abs/2505.09255"}
{"created":"2025-05-15","title":"Test-Time Augmentation for Pose-invariant Face Recognition","abstract":"The goal of this paper is to enhance face recognition performance by augmenting head poses during the testing phase. Existing methods often rely on training on frontalised images or learning pose-invariant representations, yet both approaches typically require re-training and testing for each dataset, involving a substantial amount of effort. In contrast, this study proposes Pose-TTA, a novel approach that aligns faces at inference time without additional training. To achieve this, we employ a portrait animator that transfers the source image identity into the pose of a driving image. Instead of frontalising a side-profile face -- which can introduce distortion -- Pose-TTA generates matching side-profile images for comparison, thereby reducing identity information loss. Furthermore, we propose a weighted feature aggregation strategy to address any distortions or biases arising from the synthetic data, thus enhancing the reliability of the augmented images. Extensive experiments on diverse datasets and with various pre-trained face recognition models demonstrate that Pose-TTA consistently improves inference performance. Moreover, our method is straightforward to integrate into existing face recognition pipelines, as it requires no retraining or fine-tuning of the underlying recognition models.","authors":["Jaemin Jung","Youngjoon Jang","Joon Son Chung"],"url":"https://arxiv.org/abs/2505.09256"}
{"created":"2025-05-15","title":"Efficient Graph Embedding at Scale: Optimizing CPU-GPU-SSD Integration","abstract":"Graph embeddings provide continuous vector representations of nodes in a graph, which are widely applicable in community detection, recommendations, and various scientific fields. However, existing graph embedding systems either face scalability challenges due to the high cost of RAM and multiple GPUs, or rely on disk storage at the expense of I/O efficiency. In this paper, we propose Legend, a lightweight heterogeneous system for graph embedding that systematically redefines data management across CPU, GPU, and NVMe SSD resources. Legend is built on a foundation of efficient data placement and retrieval strategies tailored to the unique strengths of each hardware. Key innovations include a prefetch-friendly embedding loading strategy, enabling GPUs to directly prefetch data from SSDs with minimal I/O overhead, and a high-throughput GPU-SSD direct access driver optimized for graph embedding tasks. Furthermore, we propose a customized parallel execution strategy to maximize GPU utilization, ensuring efficient handling of billion-scale datasets. Extensive experiments demonstrate that Legend achieves up to 4.8x speedup compared to state-of-the-art systems. Moreover, Legend exhibits comparable performance on a single GPU to that of the state-of-the-art system using 4 GPUs on the billion-scale dataset.","authors":["Zhonggen Li","Xiangyu Ke","Yifan Zhu","Yunjun Gao","Feifei Li"],"url":"https://arxiv.org/abs/2505.09258"}
{"created":"2025-05-15","title":"Interplay Between AI and Space-Air-Ground Integrated Network: The Road Ahead","abstract":"Space-air-ground integrated network (SAGIN) is envisioned as a key network architecture for achieving ubiquitous coverage in the next-generation communication system. Concurrently, artificial intelligence (AI) plays a pivotal role in managing the complex control of SAGIN, thereby enhancing its automation and flexibility. Despite this, there remains a significant research gap concerning the interaction between AI and SAGIN. In this context, we first present a promising approach for developing a generalized AI model capable of executing multiple tasks simultaneously in SAGIN. Subsequently, we propose a framework that leverages software-defined networking (SDN) and AI technologies to manage the resources and services across the entire SAGIN. Particularly, we demonstrate the real-world applicability of our proposed framework through a comprehensive case study. These works pave the way for the deep integration of SAGIN and AI in future wireless networks.","authors":["Chenyu Wu","Xi Wang","Yi Hu","Shuai Han","Dusit Niyato"],"url":"https://arxiv.org/abs/2505.09259"}
{"created":"2025-05-15","title":"Instantiating Standards: Enabling Standard-Driven Text TTP Extraction with Evolvable Memory","abstract":"Extracting MITRE ATT\\&amp;CK Tactics, Techniques, and Procedures (TTPs) from natural language threat reports is crucial yet challenging. Existing methods primarily focus on performance metrics using data-driven approaches, often neglecting mechanisms to ensure faithful adherence to the official standard. This deficiency compromises reliability and consistency of TTP assignments, creating intelligence silos and contradictory threat assessments across organizations. To address this, we introduce a novel framework that converts abstract standard definitions into actionable, contextualized knowledge. Our method utilizes Large Language Model (LLM) to generate, update, and apply this knowledge. This framework populates an evolvable memory with dual-layer situational knowledge instances derived from labeled examples and official definitions. The first layer identifies situational contexts (e.g., \"Communication with C2 using encoded subdomains\"), while the second layer captures distinctive features that differentiate similar techniques (e.g., distinguishing T1132 \"Data Encoding\" from T1071 \"Application Layer Protocol\" based on whether the focus is on encoding methods or protocol usage). This structured approach provides a transparent basis for explainable TTP assignments and enhanced human oversight, while also helping to standardize other TTP extraction systems. Experiments show our framework (using Qwen2.5-32B) boosts Technique F1 scores by 11\\% over GPT-4o. Qualitative analysis confirms superior standardization, enhanced transparency, and improved explainability in real-world threat intelligence scenarios. To the best of our knowledge, this is the first work that uses the LLM to generate, update, and apply the a new knowledge for TTP extraction.","authors":["Cheng Meng","ZhengWei Jiang","QiuYun Wang","XinYi Li","ChunYan Ma","FangMing Dong","FangLi Ren","BaoXu Liu"],"url":"https://arxiv.org/abs/2505.09261"}
{"created":"2025-05-15","title":"Few-Shot Anomaly-Driven Generation for Anomaly Classification and Segmentation","abstract":"Anomaly detection is a practical and challenging task due to the scarcity of anomaly samples in industrial inspection. Some existing anomaly detection methods address this issue by synthesizing anomalies with noise or external data. However, there is always a large semantic gap between synthetic and real-world anomalies, resulting in weak performance in anomaly detection. To solve the problem, we propose a few-shot Anomaly-driven Generation (AnoGen) method, which guides the diffusion model to generate realistic and diverse anomalies with only a few real anomalies, thereby benefiting training anomaly detection models. Specifically, our work is divided into three stages. In the first stage, we learn the anomaly distribution based on a few given real anomalies and inject the learned knowledge into an embedding. In the second stage, we use the embedding and given bounding boxes to guide the diffusion model to generate realistic and diverse anomalies on specific objects (or textures). In the final stage, we propose a weakly-supervised anomaly detection method to train a more powerful model with generated anomalies. Our method builds upon DRAEM and DesTSeg as the foundation model and conducts experiments on the commonly used industrial anomaly detection dataset, MVTec. The experiments demonstrate that our generated anomalies effectively improve the model performance of both anomaly classification and segmentation tasks simultaneously, \\eg, DRAEM and DseTSeg achieved a 5.8\\% and 1.5\\% improvement in AU-PR metric on segmentation task, respectively. The code and generated anomalous data are available at https://github.com/gaobb/AnoGen.","authors":["Guan Gui","Bin-Bin Gao","Jun Liu","Chengjie Wang","Yunsheng Wu"],"url":"https://arxiv.org/abs/2505.09263"}
{"created":"2025-05-15","title":"Learning to Detect Multi-class Anomalies with Just One Normal Image Prompt","abstract":"Unsupervised reconstruction networks using self-attention transformers have achieved state-of-the-art performance for multi-class (unified) anomaly detection with a single model. However, these self-attention reconstruction models primarily operate on target features, which may result in perfect reconstruction for both normal and anomaly features due to high consistency with context, leading to failure in detecting anomalies. Additionally, these models often produce inaccurate anomaly segmentation due to performing reconstruction in a low spatial resolution latent space. To enable reconstruction models enjoying high efficiency while enhancing their generalization for unified anomaly detection, we propose a simple yet effective method that reconstructs normal features and restores anomaly features with just One Normal Image Prompt (OneNIP). In contrast to previous work, OneNIP allows for the first time to reconstruct or restore anomalies with just one normal image prompt, effectively boosting unified anomaly detection performance. Furthermore, we propose a supervised refiner that regresses reconstruction errors by using both real normal and synthesized anomalous images, which significantly improves pixel-level anomaly segmentation. OneNIP outperforms previous methods on three industry anomaly detection benchmarks: MVTec, BTAD, and VisA. The code and pre-trained models are available at https://github.com/gaobb/OneNIP.","authors":["Bin-Bin Gao"],"url":"https://arxiv.org/abs/2505.09264"}
{"created":"2025-05-15","title":"MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning","abstract":"Zero- and few-shot visual anomaly segmentation relies on powerful vision-language models that detect unseen anomalies using manually designed textual prompts. However, visual representations are inherently independent of language. In this paper, we explore the potential of a pure visual foundation model as an alternative to widely used vision-language models for universal visual anomaly segmentation. We present a novel paradigm that unifies anomaly segmentation into change segmentation. This paradigm enables us to leverage large-scale synthetic image pairs, featuring object-level and local region changes, derived from existing image datasets, which are independent of target anomaly datasets. We propose a one-prompt Meta-learning framework for Universal Anomaly Segmentation (MetaUAS) that is trained on this synthetic dataset and then generalizes well to segment any novel or unseen visual anomalies in the real world. To handle geometrical variations between prompt and query images, we propose a soft feature alignment module that bridges paired-image change perception and single-image semantic segmentation. This is the first work to achieve universal anomaly segmentation using a pure vision model without relying on special anomaly detection datasets and pre-trained visual-language models. Our method effectively and efficiently segments any anomalies with only one normal image prompt and enjoys training-free without guidance from language. Our MetaUAS significantly outperforms previous zero-shot, few-shot, and even full-shot anomaly segmentation methods. The code and pre-trained models are available at https://github.com/gaobb/MetaUAS.","authors":["Bin-Bin Gao"],"url":"https://arxiv.org/abs/2505.09265"}
{"created":"2025-05-15","title":"How an unintended Side Effect of a Research Project led to Boosting the Power of UML","abstract":"This paper describes the design, implementation and use of a new UML modeling tool that represents a significant advance over conventional tools. Among other things, it allows the integration of class diagrams and object diagrams as well as the execution of objects. This not only enables new software architectures characterized by the integration of software with corresponding object models, but is also ideal for use in teaching, as it provides students with a particularly stimulating learning experience. A special feature of the project is that it has emerged from a long-standing international research project, which is aimed at a comprehensive multi-level architecture. The project is therefore an example of how research can lead to valuable results that arise as a side effect of other work.","authors":["Ulrich Frank","Pierre Maier"],"url":"https://arxiv.org/abs/2505.09269"}
{"created":"2025-05-15","title":"Recent Advances in Medical Imaging Segmentation: A Survey","abstract":"Medical imaging is a cornerstone of modern healthcare, driving advancements in diagnosis, treatment planning, and patient care. Among its various tasks, segmentation remains one of the most challenging problem due to factors such as data accessibility, annotation complexity, structural variability, variation in medical imaging modalities, and privacy constraints. Despite recent progress, achieving robust generalization and domain adaptation remains a significant hurdle, particularly given the resource-intensive nature of some proposed models and their reliance on domain expertise. This survey explores cutting-edge advancements in medical image segmentation, focusing on methodologies such as Generative AI, Few-Shot Learning, Foundation Models, and Universal Models. These approaches offer promising solutions to longstanding challenges. We provide a comprehensive overview of the theoretical foundations, state-of-the-art techniques, and recent applications of these methods. Finally, we discuss inherent limitations, unresolved issues, and future research directions aimed at enhancing the practicality and accessibility of segmentation models in medical imaging. We are maintaining a \\href{https://github.com/faresbougourzi/Awesome-DL-for-Medical-Imaging-Segmentation}{GitHub Repository} to continue tracking and updating innovations in this field.","authors":["Fares Bougourzi","Abdenour Hadid"],"url":"https://arxiv.org/abs/2505.09274"}
{"created":"2025-05-15","title":"Privacy-Preserving Runtime Verification","abstract":"Runtime verification offers scalable solutions to improve the safety and reliability of systems. However, systems that require verification or monitoring by a third party to ensure compliance with a specification might contain sensitive information, causing privacy concerns when usual runtime verification approaches are used. Privacy is compromised if protected information about the system, or sensitive data that is processed by the system, is revealed. In addition, revealing the specification being monitored may undermine the essence of third-party verification.","authors":["Thomas A. Henzinger","Mahyar Karimi","K. S. Thejaswini"],"url":"https://arxiv.org/abs/2505.09276"}
{"created":"2025-05-15","title":"A drone that learns to efficiently find objects in agricultural fields: from simulation to the real world","abstract":"Drones are promising for data collection in precision agriculture, however, they are limited by their battery capacity. Efficient path planners are therefore required. This paper presents a drone path planner trained using Reinforcement Learning (RL) on an abstract simulation that uses object detections and uncertain prior knowledge. The RL agent controls the flight direction and can terminate the flight. By using the agent in combination with the drone's flight controller and a detection network to process camera images, it is possible to evaluate the performance of the agent on real-world data. In simulation, the agent yielded on average a 78% shorter flight path compared to a full coverage planner, at the cost of a 14% lower recall. On real-world data, the agent showed a 72% shorter flight path compared to a full coverage planner, however, at the cost of a 25% lower recall. The lower performance on real-world data was attributed to the real-world object distribution and the lower accuracy of prior knowledge, and shows potential for improvement. Overall, we concluded that for applications where it is not crucial to find all objects, such as weed detection, the learned-based path planner is suitable and efficient.","authors":["Rick van Essen","Gert Kootstra"],"url":"https://arxiv.org/abs/2505.09278"}
{"created":"2025-05-15","title":"Phase Transitions in Decision Problems Over Odd-Sized Alphabets","abstract":"In [A. Jackson, Explaining the ubiquity of phase transitions in decision problems (2025), arXiv:2501.14569], I established that phase transitions are always present in a large subset of decision problems over even-sized alphabets, explaining -- in part -- why phase transitions are seen so often in decision problems. However, decision problems over odd-sized alphabets were not discussed. Here, I correct that oversight, showing that a similar subset of decision problems over odd-sized alphabets also always exhibit phase transitions.","authors":["Andrew Jackson"],"url":"https://arxiv.org/abs/2505.09282"}
{"created":"2025-05-15","title":"A Note on Semantic Diffusion","abstract":"This paper provides an in-depth examination of the concept of semantic diffusion as a complementary instrument to large language models (LLMs) for design applications. Conventional LLMs and diffusion models fail to induce a convergent, iterative refinement process: each invocation of the diffusion mechanism spawns a new stochastic cycle, so successive outputs do not relate to prior ones and convergence toward a desired design is not guaranteed. The proposed hybrid framework - \"LLM + semantic diffusion\" - resolves this limitation by enforcing an approximately convergent search procedure, thereby formally addressing the problem of localized design refinement.","authors":["Alexander P. Ryjov","Alina A. Egorova"],"url":"https://arxiv.org/abs/2505.09283"}
{"created":"2025-05-15","title":"Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations","abstract":"Modeling and reconstructing multidimensional physical dynamics from sparse and off-grid observations presents a fundamental challenge in scientific research. Recently, diffusion-based generative modeling shows promising potential for physical simulation. However, current approaches typically operate on on-grid data with preset spatiotemporal resolution, but struggle with the sparsely observed and continuous nature of real-world physical dynamics. To fill the gaps, we present SDIFT, Sequential DIffusion in Functional Tucker space, a novel framework that generates full-field evolution of physical dynamics from irregular sparse observations. SDIFT leverages the functional Tucker model as the latent space representer with proven universal approximation property, and represents observations as latent functions and Tucker core sequences. We then construct a sequential diffusion model with temporally augmented UNet in the functional Tucker space, denoising noise drawn from a Gaussian process to generate the sequence of core tensors.","authors":["Panqi Chen","Yifan Sun","Lei Cheng","Yang Yang","Weichang Li","Yang Liu","Weiqing Liu","Jiang Bian","Shikai Fang"],"url":"https://arxiv.org/abs/2505.09284"}
{"created":"2025-05-15","title":"A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data","abstract":"Effectively analyzing online review data is essential across industries. However, many existing studies are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets. To address these limitations, we propose a multilingual, scalable, and unsupervised framework for cross-domain aspect detection. This framework is designed for multi-aspect labeling of multilingual and multi-domain review data. In this study, we apply automatic labeling to Korean and English review datasets spanning various domains and assess the quality of the generated labels through extensive experiments. Aspect category candidates are first extracted through clustering, and each review is then represented as an aspect-aware embedding vector using negative sampling. To evaluate the framework, we conduct multi-aspect labeling and fine-tune several pretrained language models to measure the effectiveness of the automatically generated labels. Results show that these models achieve high performance, demonstrating that the labels are suitable for training. Furthermore, comparisons with publicly available large language models highlight the framework's superior consistency and scalability when processing large-scale data. A human evaluation also confirms that the quality of the automatic labels is comparable to those created manually. This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments. Future research will explore automatic review summarization and the integration of artificial intelligence agents to further improve the efficiency and depth of review analysis.","authors":["Jiin Park","Misuk Kim"],"url":"https://arxiv.org/abs/2505.09286"}
{"created":"2025-05-15","title":"Ranking-Based At-Risk Student Prediction Using Federated Learning and Differential Features","abstract":"Digital textbooks are widely used in various educational contexts, such as university courses and online lectures. Such textbooks yield learning log data that have been used in numerous educational data mining (EDM) studies for student behavior analysis and performance prediction. However, these studies have faced challenges in integrating confidential data, such as academic records and learning logs, across schools due to privacy concerns. Consequently, analyses are often conducted with data limited to a single school, which makes developing high-performing and generalizable models difficult. This study proposes a method that combines federated learning and differential features to address these issues. Federated learning enables model training without centralizing data, thereby preserving student privacy. Differential features, which utilize relative values instead of absolute values, enhance model performance and generalizability. To evaluate the proposed method, a model for predicting at-risk students was trained using data from 1,136 students across 12 courses conducted over 4 years, and validated on hold-out test data from 5 other courses. Experimental results demonstrated that the proposed method addresses privacy concerns while achieving performance comparable to that of models trained via centralized learning in terms of Top-n precision, nDCG, and PR-AUC. Furthermore, using differential features improved prediction performance across all evaluation datasets compared to non-differential approaches. The trained models were also applicable for early prediction, achieving high performance in detecting at-risk students in earlier stages of the semester within the validation datasets.","authors":["Shunsuke Yoneda","Valdemar \\v{S}v\\'abensk\\'y","Gen Li","Daisuke Deguchi","Atsushi Shimada"],"url":"https://arxiv.org/abs/2505.09287"}
{"created":"2025-05-15","title":"Reproducibility Study of \"Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\"","abstract":"This study evaluates and extends the findings made by Piatti et al., who introduced GovSim, a simulation framework designed to assess the cooperative decision-making capabilities of large language models (LLMs) in resource-sharing scenarios. By replicating key experiments, we validate claims regarding the performance of large models, such as GPT-4-turbo, compared to smaller models. The impact of the universalization principle is also examined, with results showing that large models can achieve sustainable cooperation, with or without the principle, while smaller models fail without it. In addition, we provide multiple extensions to explore the applicability of the framework to new settings. We evaluate additional models, such as DeepSeek-V3 and GPT-4o-mini, to test whether cooperative behavior generalizes across different architectures and model sizes. Furthermore, we introduce new settings: we create a heterogeneous multi-agent environment, study a scenario using Japanese instructions, and explore an \"inverse environment\" where agents must cooperate to mitigate harmful resource distributions. Our results confirm that the benchmark can be applied to new models, scenarios, and languages, offering valuable insights into the adaptability of LLMs in complex cooperative tasks. Moreover, the experiment involving heterogeneous multi-agent systems demonstrates that high-performing models can influence lower-performing ones to adopt similar behaviors. This finding has significant implications for other agent-based applications, potentially enabling more efficient use of computational resources and contributing to the development of more effective cooperative AI systems.","authors":["Pedro M. P. Curvo","Mara Dragomir","Salvador Torpes","Mohammadmahdi Rahimi"],"url":"https://arxiv.org/abs/2505.09289"}
{"created":"2025-05-15","title":"On the Learning with Augmented Class via Forests","abstract":"Decision trees and forests have achieved successes in various real applications, most working with all testing classes known in training data. In this work, we focus on learning with augmented class via forests, where an augmented class may appear in testing data yet not in training data. We incorporate information of augmented class into trees' splitting, i.e., a new splitting criterion, called augmented Gini impurity, is introduced to exploit some unlabeled data from testing distribution. We then develop the approach named Learning with Augmented Class via Forests (LACForest), which constructs shallow forests based on the augmented Gini impurity and then splits forests with pseudo-labeled augmented instances for better performance. We also develop deep neural forests with a novel optimization objective based on our augmented Gini impurity, so as to utilize the representation power of neural networks for forests. Theoretically, we present the convergence analysis for augmented Gini impurity, and finally conduct experiments to verify the effectiveness of our approaches. The code is available at https://github.com/nju-xuf/LACForest/.","authors":["Fan Xu","Wuyang Chen","Wei Gao"],"url":"https://arxiv.org/abs/2505.09294"}
{"created":"2025-05-15","title":"Toward Fair Federated Learning under Demographic Disparities and Data Imbalance","abstract":"Ensuring fairness is critical when applying artificial intelligence to high-stakes domains such as healthcare, where predictive models trained on imbalanced and demographically skewed data risk exacerbating existing disparities. Federated learning (FL) enables privacy-preserving collaboration across institutions, but remains vulnerable to both algorithmic bias and subgroup imbalance - particularly when multiple sensitive attributes intersect. We propose FedIDA (Fed erated Learning for Imbalance and D isparity A wareness), a framework-agnostic method that combines fairness-aware regularization with group-conditional oversampling. FedIDA supports multiple sensitive attributes and heterogeneous data distributions without altering the convergence behavior of the underlying FL algorithm. We provide theoretical analysis establishing fairness improvement bounds using Lipschitz continuity and concentration inequalities, and show that FedIDA reduces the variance of fairness metrics across test sets. Empirical results on both benchmark and real-world clinical datasets confirm that FedIDA consistently improves fairness while maintaining competitive predictive performance, demonstrating its effectiveness for equitable and privacy-preserving modeling in healthcare. The source code is available on GitHub.","authors":["Qiming Wu","Siqi Li","Doudou Zhou","Nan Liu"],"url":"https://arxiv.org/abs/2505.09295"}
{"created":"2025-05-15","title":"Adaptive Noise Resilient Keyword Spotting Using One-Shot Learning","abstract":"Keyword spotting (KWS) is a key component of smart devices, enabling efficient and intuitive audio interaction. However, standard KWS systems deployed on embedded devices often suffer performance degradation under real-world operating conditions. Resilient KWS systems address this issue by enabling dynamic adaptation, with applications such as adding or replacing keywords, adjusting to specific users, and improving noise robustness. However, deploying resilient, standalone KWS systems with low latency on resource-constrained devices remains challenging due to limited memory and computational resources. This study proposes a low computational approach for continuous noise adaptation of pretrained neural networks used for KWS classification, requiring only 1-shot learning and one epoch. The proposed method was assessed using two pretrained models and three real-world noise sources at signal-to-noise ratios (SNRs) ranging from 24 to -3 dB. The adapted models consistently outperformed the pretrained models across all scenarios, especially at SNR $\\leq$ 18 dB, achieving accuracy improvements of 4.9% to 46.0%. These results highlight the efficacy of the proposed methodology while being lightweight enough for deployment on resource-constrained devices.","authors":["Luciano Sebastian Martinez-Rau","Quynh Nguyen Phuong Vu","Yuxuan Zhang","Bengt Oelmann","Sebastian Bader"],"url":"https://arxiv.org/abs/2505.09304"}
{"created":"2025-05-15","title":"Embodied Intelligent Industrial Robotics: Concepts and Techniques","abstract":"In recent years, embodied intelligent robotics (EIR) has made significant progress in multi-modal perception, autonomous decision-making, and physical interaction. Some robots have already been tested in general-purpose scenarios such as homes and shopping malls. We aim to advance the research and application of embodied intelligence in industrial scenes. However, current EIR lacks a deep understanding of industrial environment semantics and the normative constraints between industrial operating objects. To address this gap, this paper first reviews the history of industrial robotics and the mainstream EIR frameworks. We then introduce the concept of the embodied intelligent industrial robotics (EIIR) and propose a knowledge-driven EIIR technology framework for industrial environments. The framework includes four main modules: world model, high-level task planner, low-level skill controller, and simulator. We also review the current development of technologies related to each module and highlight recent progress in adapting them to industrial applications. Finally, we summarize the key challenges EIIR faces in industrial scenarios and suggest future research directions. We believe that EIIR technology will shape the next generation of industrial robotics. Industrial systems based on embodied intelligent industrial robots offer strong potential for enabling intelligent manufacturing. We will continue to track and summarize new research in this area and hope this review will serve as a valuable reference for scholars and engineers interested in industrial embodied intelligence. Together, we can help drive the rapid advancement and application of this technology. The associated project can be found at https://github.com/jackeyzengl/Embodied_Intelligent_Industrial_Robotics_Paper_List.","authors":["Chaoran Zhang","Chenhao Zhang","Zhaobo Xu","Qinghongbing Xie","Pingfa Feng","Long Zeng"],"url":"https://arxiv.org/abs/2505.09305"}
{"created":"2025-05-15","title":"Predicting butterfly species presence from satellite imagery using soft contrastive regularisation","abstract":"The growing demand for scalable biodiversity monitoring methods has fuelled interest in remote sensing data, due to its widespread availability and extensive coverage. Traditionally, the application of remote sensing to biodiversity research has focused on mapping and monitoring habitats, but with increasing availability of large-scale citizen-science wildlife observation data, recent methods have started to explore predicting multi-species presence directly from satellite images. This paper presents a new data set for predicting butterfly species presence from satellite data in the United Kingdom. We experimentally optimise a Resnet-based model to predict multi-species presence from 4-band satellite images, and find that this model especially outperforms the mean rate baseline for locations with high species biodiversity. To improve performance, we develop a soft, supervised contrastive regularisation loss that is tailored to probabilistic labels (such as species-presence data), and demonstrate that this improves prediction accuracy. In summary, our new data set and contrastive regularisation method contribute to the open challenge of accurately predicting species biodiversity from remote sensing data, which is key for efficient biodiversity monitoring.","authors":["Thijs L van der Plas","Stephen Law","Michael JO Pocock"],"url":"https://arxiv.org/abs/2505.09306"}
{"created":"2025-05-15","title":"Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model","abstract":"The Unconstrained Feature Model (UFM) is a mathematical framework that enables closed-form approximations for minimal training loss and related performance measures in deep neural networks (DNNs). This paper leverages the UFM to provide qualitative insights into neural multivariate regression, a critical task in imitation learning, robotics, and reinforcement learning. Specifically, we address two key questions: (1) How do multi-task models compare to multiple single-task models in terms of training performance? (2) Can whitening and normalizing regression targets improve training performance? The UFM theory predicts that multi-task models achieve strictly smaller training MSE than multiple single-task models when the same or stronger regularization is applied to the latter, and our empirical results confirm these findings. Regarding whitening and normalizing regression targets, the UFM theory predicts that they reduce training MSE when the average variance across the target dimensions is less than one, and our empirical results once again confirm these findings. These findings highlight the UFM as a powerful framework for deriving actionable insights into DNN design and data pre-processing strategies.","authors":["George Andriopoulos","Soyuj Jung Basnet","Juan Guevara","Li Guo","Keith Ross"],"url":"https://arxiv.org/abs/2505.09308"}
{"created":"2025-05-15","title":"Detecting Sybil Addresses in Blockchain Airdrops: A Subgraph-based Feature Propagation and Fusion Approach","abstract":"Sybil attacks pose a significant security threat to blockchain ecosystems, particularly in token airdrop events. This paper proposes a novel sybil address identification method based on subgraph feature extraction lightGBM. The method first constructs a two-layer deep transaction subgraph for each address, then extracts key event operation features according to the lifecycle of sybil addresses, including the time of first transaction, first gas acquisition, participation in airdrop activities, and last transaction. These temporal features effectively capture the consistency of sybil address behavior operations. Additionally, the method extracts amount and network structure features, comprehensively describing address behavior patterns and network topology through feature propagation and fusion. Experiments conducted on a dataset containing 193,701 addresses (including 23,240 sybil addresses) show that this method outperforms existing approaches in terms of precision, recall, F1 score, and AUC, with all metrics exceeding 0.9. The methods and results of this study can be further applied to broader blockchain security areas such as transaction manipulation identification and token liquidity risk assessment, contributing to the construction of a more secure and fair blockchain ecosystem.","authors":["Qiangqiang Liu","Qian Huang","Frank Fan","Haishan Wu","Xueyan Tang"],"url":"https://arxiv.org/abs/2505.09313"}
{"created":"2025-05-15","title":"TransDiffuser: End-to-end Trajectory Generation with Decorrelated Multi-modal Representation for Autonomous Driving","abstract":"In recent years, diffusion model has shown its potential across diverse domains from vision generation to language modeling. Transferring its capabilities to modern autonomous driving systems has also emerged as a promising direction.In this work, we propose TransDiffuser, an encoder-decoder based generative trajectory planning model for end-to-end autonomous driving. The encoded scene information serves as the multi-modal conditional input of the denoising decoder. To tackle the mode collapse dilemma in generating high-quality diverse trajectories, we introduce a simple yet effective multi-modal representation decorrelation optimization mechanism during the training process.TransDiffuser achieves PDMS of 94.85 on the NAVSIM benchmark, surpassing previous state-of-the-art methods without any anchor-based prior trajectories.","authors":["Xuefeng Jiang","Yuan Ma","Pengxiang Li","Leimeng Xu","Xin Wen","Kun Zhan","Zhongpu Xia","Peng Jia","XianPeng Lang","Sheng Sun"],"url":"https://arxiv.org/abs/2505.09315"}
{"created":"2025-05-15","title":"Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging","abstract":"Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents.","authors":["Hongjin Qian","Zheng Liu"],"url":"https://arxiv.org/abs/2505.09316"}
{"created":"2025-05-15","title":"Statistical Modeling and Uncertainty Estimation of LLM Inference Systems","abstract":"Large Language Model (LLM) inference systems present significant challenges in statistical performance characterization due to dynamic workload variations, diverse hardware architectures, and complex interactions between model size, batch processing, and throughput requirements. Accurate statistical characterization enables better workload scheduling, adaptive resource provisioning, and cost-aware inference optimization, making it crucial for improving efficiency in large-scale AI deployments. Traditional analytical models provide explainability but cannot cover the vast diversity of real-world workloads, making it impossible to benchmark every scenario in advance. Machine learning (ML) approaches effectively predict performance for non-benchmarked cases but struggle when extrapolating beyond their observed training space. To address these limitations for LLM inference systems, we propose an Analytical with Learning Augmentation (ALA) framework that bridges analytical modeling with \\ml for robust statistical prediction and uncertainty estimation in LLM inference workloads. Our method employs an analytical throughput model with parameters estimated for benchmarked workloads, then extends to unobserved configurations using \\ml predictions. We enhance this with simulated annealing to exploit subsets of the workload data point combinations and develop an error predictor. Finally, we quantify uncertainty based on vector space similarity between new and observed workloads to ensure robust generalization. Through extensive experimentation on diverse LLM inference workloads, we demonstrate that our framework achieves low median errors while maintaining adaptability to new inference scenarios.","authors":["Kaustabha Ray","Nelson Mimura Gonzalez","Bruno Wassermann","Rachel Tzoref-Brill","Dean H. Lorenz"],"url":"https://arxiv.org/abs/2505.09319"}
{"created":"2025-05-15","title":"Online Bin Packing with Item Size Estimates","abstract":"Imagine yourself moving to another place, and therefore, you need to pack all of your belongings into moving boxes with some capacity. In the classical bin packing model, you would try to minimize the number of boxes, knowing the exact size of each item you want to pack. In the online bin packing problem, you need to start packing the first item into a box, without knowing what other stuff is upcoming.","authors":["Matthias Gehnen","Andreas Usdenski"],"url":"https://arxiv.org/abs/2505.09321"}
{"created":"2025-05-15","title":"Neural Video Compression using 2D Gaussian Splatting","abstract":"The computer vision and image processing research community has been involved in standardizing video data communications for the past many decades, leading to standards such as AVC, HEVC, VVC, AV1, AV2, etc. However, recent groundbreaking works have focused on employing deep learning-based techniques to replace the traditional video codec pipeline to a greater affect. Neural video codecs (NVC) create an end-to-end ML-based solution that does not rely on any handcrafted features (motion or edge-based) and have the ability to learn content-aware compression strategies, offering better adaptability and higher compression efficiency than traditional methods. This holds a great potential not only for hardware design, but also for various video streaming platforms and applications, especially video conferencing applications such as MS-Teams or Zoom that have found extensive usage in classrooms and workplaces. However, their high computational demands currently limit their use in real-time applications like video conferencing. To address this, we propose a region-of-interest (ROI) based neural video compression model that leverages 2D Gaussian Splatting. Unlike traditional codecs, 2D Gaussian Splatting is capable of real-time decoding and can be optimized using fewer data points, requiring only thousands of Gaussians for decent quality outputs as opposed to millions in 3D scenes. In this work, we designed a video pipeline that speeds up the encoding time of the previous Gaussian splatting-based image codec by 88% by using a content-aware initialization strategy paired with a novel Gaussian inter-frame redundancy-reduction mechanism, enabling Gaussian splatting to be used for a video-codec solution, the first of its kind solution in this neural video codec space.","authors":["Lakshya Gupta","Imran N. Junejo"],"url":"https://arxiv.org/abs/2505.09324"}
{"created":"2025-05-15","title":"SingNet: Towards a Large-Scale, Diverse, and In-the-Wild Singing Voice Dataset","abstract":"The lack of a publicly-available large-scale and diverse dataset has long been a significant bottleneck for singing voice applications like Singing Voice Synthesis (SVS) and Singing Voice Conversion (SVC). To tackle this problem, we present SingNet, an extensive, diverse, and in-the-wild singing voice dataset. Specifically, we propose a data processing pipeline to extract ready-to-use training data from sample packs and songs on the internet, forming 3000 hours of singing voices in various languages and styles. Furthermore, to facilitate the use and demonstrate the effectiveness of SingNet, we pre-train and open-source various state-of-the-art (SOTA) models on Wav2vec2, BigVGAN, and NSF-HiFiGAN based on our collected singing voice data. We also conduct benchmark experiments on Automatic Lyric Transcription (ALT), Neural Vocoder, and Singing Voice Conversion (SVC). Audio demos are available at: https://singnet-dataset.github.io/.","authors":["Yicheng Gu","Chaoren Wang","Junan Zhang","Xueyao Zhang","Zihao Fang","Haorui He","Zhizheng Wu"],"url":"https://arxiv.org/abs/2505.09325"}
{"created":"2025-05-15","title":"BioVFM-21M: Benchmarking and Scaling Self-Supervised Vision Foundation Models for Biomedical Image Analysis","abstract":"Scaling up model and data size have demonstrated impressive performance improvement over a wide range of tasks. Despite extensive studies on scaling behaviors for general-purpose tasks, medical images exhibit substantial differences from natural data. It remains unclear the key factors in developing medical vision foundation models at scale due to the absence of an extensive understanding of scaling behavior in the medical domain. In this paper, we explored the scaling behavior across model sizes, training algorithms, data sizes, and imaging modalities in developing scalable medical vision foundation models by self-supervised learning. To support scalable pretraining, we introduce BioVFM-21M, a large-scale biomedical image dataset encompassing a wide range of biomedical image modalities and anatomies. We observed that scaling up does provide benefits but varies across tasks. Additional analysis reveals several factors correlated with scaling benefits. Finally, we propose BioVFM, a large-scale medical vision foundation model pretrained on 21 million biomedical images, which outperforms the previous state-of-the-art foundation models across 12 medical benchmarks. Our results highlight that while scaling up is beneficial for pursuing better performance, task characteristics, data diversity, pretraining methods, and computational efficiency remain critical considerations for developing scalable medical foundation models.","authors":["Jiarun Liu","Hong-Yu Zhou","Weijian Huang","Hao Yang","Dongning Song","Tao Tan","Yong Liang","Shanshan Wang"],"url":"https://arxiv.org/abs/2505.09329"}
{"created":"2025-05-15","title":"MUST: Multi-Scale Structural-Temporal Link Prediction Model for UAV Ad Hoc Networks","abstract":"Link prediction in unmanned aerial vehicle (UAV) ad hoc networks (UANETs) aims to predict the potential formation of future links between UAVs. In adversarial environments where the route information of UAVs is unavailable, predicting future links must rely solely on the observed historical topological information of UANETs. However, the highly dynamic and sparse nature of UANET topologies presents substantial challenges in effectively capturing meaningful structural and temporal patterns for accurate link prediction. Most existing link prediction methods focus on temporal dynamics at a single structural scale while neglecting the effects of sparsity, resulting in insufficient information capture and limited applicability to UANETs. In this paper, we propose a multi-scale structural-temporal link prediction model (MUST) for UANETs. Specifically, we first employ graph attention networks (GATs) to capture structural features at multiple levels, including the individual UAV level, the UAV community level, and the overall network level. Then, we use long short-term memory (LSTM) networks to learn the temporal dynamics of these multi-scale structural features. Additionally, we address the impact of sparsity by introducing a sophisticated loss function during model optimization. We validate the performance of MUST using several UANET datasets generated through simulations. Extensive experimental results demonstrate that MUST achieves state-of-the-art link prediction performance in highly dynamic and sparse UANETs.","authors":["Cunlai Pu","Fangrui Wu","Rajput Ramiz Sharafat","Guangzhao Dai","Xiangbo Shu"],"url":"https://arxiv.org/abs/2505.09331"}
{"created":"2025-05-15","title":"Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition","abstract":"In this paper, we introduce MultiviewVLM, a vision-language model designed for unsupervised contrastive multiview representation learning of facial emotions from 3D/4D data. Our architecture integrates pseudo-labels derived from generated textual prompts to guide implicit alignment of emotional semantics. To capture shared information across multi-views, we propose a joint embedding space that aligns multiview representations without requiring explicit supervision. We further enhance the discriminability of our model through a novel multiview contrastive learning strategy that leverages stable positive-negative pair sampling. A gradient-friendly loss function is introduced to promote smoother and more stable convergence, and the model is optimized for distributed training to ensure scalability. Extensive experiments demonstrate that MultiviewVLM outperforms existing state-of-the-art methods and can be easily adapted to various real-world applications with minimal modifications.","authors":["Muzammil Behzad"],"url":"https://arxiv.org/abs/2505.09336"}
{"created":"2025-05-15","title":"Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs","abstract":"We observe a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by ``irrelevant'' contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. We find statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors.","authors":["Jingcheng Niu","Xingdi Yuan","Tong Wang","Hamidreza Saghir","Amir H. Abdi"],"url":"https://arxiv.org/abs/2505.09338"}
{"created":"2025-05-15","title":"RAG-Enabled Intent Reasoning for Application-Network Interaction","abstract":"Intent-based network (IBN) is a promising solution to automate network operation and management. IBN aims to offer human-tailored network interaction, allowing the network to communicate in a way that aligns with the network users' language, rather than requiring the network users to understand the technical language of the network/devices. Nowadays, different applications interact with the network, each with its own specialized needs and domain language. Creating semantic languages (i.e., ontology-based languages) and associating them with each application to facilitate intent translation lacks technical expertise and is neither practical nor scalable. To tackle the aforementioned problem, we propose a context-aware AI framework that utilizes machine reasoning (MR), retrieval augmented generation (RAG), and generative AI technologies to interpret intents from different applications and generate structured network intents. The proposed framework allows for generalized/domain-specific intent expression and overcomes the drawbacks of large language models (LLMs) and vanilla-RAG framework. The experimental results show that our proposed intent-RAG framework outperforms the LLM and vanilla-RAG framework in intent translation.","authors":["Salwa Mostafa","Mohamed K. Abdel-Aziz","Mohammed S. Elbamby","Mehdi Bennis"],"url":"https://arxiv.org/abs/2505.09339"}
{"created":"2025-05-15","title":"Access Controls Will Solve the Dual-Use Dilemma","abstract":"AI safety systems face a dual-use dilemma. Since the same request can be either harmless or harmful depending on who made it and why, if the system makes decisions based solely on the request's content, it will refuse some legitimate queries and let pass harmful ones. To address this, we propose a conceptual access control framework, based on verified user credentials (such as institutional affiliation) and classifiers that assign model outputs to risk categories (such as advanced virology). The system permits responses only when the user's verified credentials match the category's requirements. For implementation of the model output classifiers, we introduce a theoretical approach utilizing small, gated expert modules integrated into the generator model, trained with gradient routing, that enable efficient risk detection without the capability gap problems of external monitors. While open questions remain about the verification mechanisms, risk categories, and the technical implementation, our framework makes the first step toward enabling granular governance of AI capabilities: verified users gain access to specialized knowledge without arbitrary restrictions, while adversaries are blocked from it. This contextual approach reconciles model utility with robust safety, addressing the dual-use dilemma.","authors":["Ev\\v{z}en Wybitul"],"url":"https://arxiv.org/abs/2505.09341"}
{"created":"2025-05-15","title":"Evaluating the Robustness of Adversarial Defenses in Malware Detection Systems","abstract":"Machine learning is a key tool for Android malware detection, effectively identifying malicious patterns in apps. However, ML-based detectors are vulnerable to evasion attacks, where small, crafted changes bypass detection. Despite progress in adversarial defenses, the lack of comprehensive evaluation frameworks in binary-constrained domains limits understanding of their robustness. We introduce two key contributions. First, Prioritized Binary Rounding, a technique to convert continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Second, the sigma-binary attack, a novel adversarial method for binary domains, designed to achieve attack goals with minimal feature changes. Experiments on the Malscan dataset show that sigma-binary outperforms existing attacks and exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant brittleness, with attack success rates exceeding 90% using fewer than 10 feature modifications and reaching 100% with just 20. Adversarially trained defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small budgets but remains vulnerable to unrestricted perturbations, with attack success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates strong robustness against state-of-the-art gradient-based adversarial attacks by maintaining an attack success rate below 16.55%, the sigma-binary attack significantly outperforms these methods, achieving a 94.56% success rate under unrestricted perturbations. These findings highlight the critical need for precise method like sigma-binary to expose hidden vulnerabilities in existing defenses and support the development of more resilient malware detection systems.","authors":["Mostafa Jafari","Alireza Shameli-Sendi"],"url":"https://arxiv.org/abs/2505.09342"}
{"created":"2025-05-15","title":"Insights into DeepSeek-V3: Scaling Challenges and Reflections on Hardware for AI Architectures","abstract":"The rapid scaling of large language models (LLMs) has unveiled critical limitations in current hardware architectures, including constraints in memory capacity, computational efficiency, and interconnection bandwidth. DeepSeek-V3, trained on 2,048 NVIDIA H800 GPUs, demonstrates how hardware-aware model co-design can effectively address these challenges, enabling cost-efficient training and inference at scale. This paper presents an in-depth analysis of the DeepSeek-V3/R1 model architecture and its AI infrastructure, highlighting key innovations such as Multi-head Latent Attention (MLA) for enhanced memory efficiency, Mixture of Experts (MoE) architectures for optimized computation-communication trade-offs, FP8 mixed-precision training to unlock the full potential of hardware capabilities, and a Multi-Plane Network Topology to minimize cluster-level network overhead. Building on the hardware bottlenecks encountered during DeepSeek-V3's development, we engage in a broader discussion with academic and industry peers on potential future hardware directions, including precise low-precision computation units, scale-up and scale-out convergence, and innovations in low-latency communication fabrics. These insights underscore the critical role of hardware and model co-design in meeting the escalating demands of AI workloads, offering a practical blueprint for innovation in next-generation AI systems.","authors":["Chenggang Zhao","Chengqi Deng","Chong Ruan","Damai Dai","Huazuo Gao","Jiashi Li","Liyue Zhang","Panpan Huang","Shangyan Zhou","Shirong Ma","Wenfeng Liang","Ying He","Yuqing Wang","Yuxuan Liu","Y. X. Wei"],"url":"https://arxiv.org/abs/2505.09343"}
{"created":"2025-05-15","title":"GreenFactory: Ensembling Zero-Cost Proxies to Estimate Performance of Neural Networks","abstract":"Determining the performance of a Deep Neural Network during Neural Architecture Search processes is essential for identifying optimal architectures and hyperparameters. Traditionally, this process requires training and evaluation of each network, which is time-consuming and resource-intensive. Zero-cost proxies estimate performance without training, serving as an alternative to traditional training. However, recent proxies often lack generalization across diverse scenarios and provide only relative rankings rather than predicted accuracies. To address these limitations, we propose GreenFactory, an ensemble of zero-cost proxies that leverages a random forest regressor to combine multiple predictors' strengths and directly predict model test accuracy. We evaluate GreenFactory on NATS-Bench, achieving robust results across multiple datasets. Specifically, GreenFactory achieves high Kendall correlations on NATS-Bench-SSS, indicating substantial agreement between its predicted scores and actual performance: 0.907 for CIFAR-10, 0.945 for CIFAR-100, and 0.920 for ImageNet-16-120. Similarly, on NATS-Bench-TSS, we achieve correlations of 0.921 for CIFAR-10, 0.929 for CIFAR-100, and 0.908 for ImageNet-16-120, showcasing its reliability in both search spaces.","authors":["Gabriel Cort\\^es","Nuno Louren\\c{c}o","Paolo Romano","Penousal Machado"],"url":"https://arxiv.org/abs/2505.09344"}
{"created":"2025-05-15","title":"Procedural Low-Poly Terrain Generation with Terracing for Computer Games","abstract":"In computer games, traditional procedural terrain generation relies on a grid of vertices, with each point representing terrain elevation. For each square in the grid, two triangles are created by connecting fixed vertex indices, resulting in a continuous 3D surface. While this method is efficient for modelling smooth terrain, the grid-like structure lacks the distinct, chaotic appearance of low-poly objects and is not suitable to be used for our purposes. The technique presented in this paper aims to solve the following problem: Generate random, low-poly looking terraced terrain with different biomes and add vegetation to create an interesting environment.","authors":["Richard Tivolt"],"url":"https://arxiv.org/abs/2505.09350"}
{"created":"2025-05-15","title":"Coordinated Multi-Valve Disturbance-Rejection Pressure Control for High-Altitude Test Stands via Exterior Penalty Functions","abstract":"High altitude simulation test benches for aero engines employ multi chamber, multi valve intake systems that demand effective decoupling and strong disturbance rejection during transient tests. This paper proposes a coordinated active disturbance rejection control (ADRC) scheme based on an external penalty function. The chamber pressure safety limit is reformulated as an inequality constrained optimization problem, and an exponential penalty together with a gradient based algorithm is designed for dynamic constraint relaxation, with global convergence rigorously proven. A coordination term is then integrated into a distributed ADRC framework to yield a multi valve coordinated LADRC controller, whose asymptotic stability is established via Lyapunov theory. Hardware in the loop simulations using MATLAB/Simulink and a PLC demonstrate that, under $\\pm$3 kPa pressure constraints, chamber V2's maximum error is 1.782 kPa (77.1\\% lower than PID control), and under a 180 kg/s^2 flow rate disturbance, valve oscillations decrease from $\\pm$27\\% to $\\pm$5\\% (an 81.5\\% reduction). These results confirm the proposed method's superior disturbance rejection and decoupling performance.","authors":["Zhang Louyue","Li Xin","Zhai Chao","Shi Duoqi","Zhang Hehong","Dan Zhihong","Wang Xi","Liu Jiashuai","Xiao Gaoxi"],"url":"https://arxiv.org/abs/2505.09352"}
{"created":"2025-05-15","title":"Deterministic Suffix-reading Automata","abstract":"We introduce deterministic suffix-reading automata (DSA), a new automaton model over finite words. Transitions in a DSA are labeled with words. From a state, a DSA triggers an outgoing transition on seeing a word ending with the transition's label. Therefore, rather than moving along an input word letter by letter, a DSA can jump along blocks of letters, with each block ending in a suitable suffix. This feature allows DSAs to recognize regular languages more concisely, compared to DFAs. In this work, we focus on questions around finding a minimal DSA for a regular language. The number of states is not a faithful measure of the size of a DSA, since the transition-labels contain strings of arbitrary length. Hence, we consider total-size (number of states + number of edges + total length of transition-labels) as the size measure of DSAs.","authors":["R Keerthan","B Srivathsan","R Venkatesh","Sagar Verma"],"url":"https://arxiv.org/abs/2505.09353"}
{"created":"2025-05-15","title":"Exploiting the Potential Supervision Information of Clean Samples in Partial Label Learning","abstract":"Diminishing the impact of false-positive labels is critical for conducting disambiguation in partial label learning. However, the existing disambiguation strategies mainly focus on exploiting the characteristics of individual partial label instances while neglecting the strong supervision information of clean samples randomly lying in the datasets. In this work, we show that clean samples can be collected to offer guidance and enhance the confidence of the most possible candidates. Motivated by the manner of the differentiable count loss strat- egy and the K-Nearest-Neighbor algorithm, we proposed a new calibration strategy called CleanSE. Specifically, we attribute the most reliable candidates with higher significance under the assumption that for each clean sample, if its label is one of the candidates of its nearest neighbor in the representation space, it is more likely to be the ground truth of its neighbor. Moreover, clean samples offer help in characterizing the sample distributions by restricting the label counts of each label to a specific interval. Extensive experiments on 3 synthetic benchmarks and 5 real-world PLL datasets showed this calibration strategy can be applied to most of the state-of-the-art PLL methods as well as enhance their performance.","authors":["Guangtai Wang","Chi-Man Vong","Jintao Huang"],"url":"https://arxiv.org/abs/2505.09354"}
{"created":"2025-05-15","title":"APR-Transformer: Initial Pose Estimation for Localization in Complex Environments through Absolute Pose Regression","abstract":"Precise initialization plays a critical role in the performance of localization algorithms, especially in the context of robotics, autonomous driving, and computer vision. Poor localization accuracy is often a consequence of inaccurate initial poses, particularly noticeable in GNSS-denied environments where GPS signals are primarily relied upon for initialization. Recent advances in leveraging deep neural networks for pose regression have led to significant improvements in both accuracy and robustness, especially in estimating complex spatial relationships and orientations. In this paper, we introduce APR-Transformer, a model architecture inspired by state-of-the-art methods, which predicts absolute pose (3D position and 3D orientation) using either image or LiDAR data. We demonstrate that our proposed method achieves state-of-the-art performance on established benchmark datasets such as the Radar Oxford Robot-Car and DeepLoc datasets. Furthermore, we extend our experiments to include our custom complex APR-BeIntelli dataset. Additionally, we validate the reliability of our approach in GNSS-denied environments by deploying the model in real-time on an autonomous test vehicle. This showcases the practical feasibility and effectiveness of our approach. The source code is available at:https://github.com/GT-ARC/APR-Transformer.","authors":["Srinivas Ravuri (Technische Universit\\\"at Berlin","Berlin","Germany)","Yuan Xu (Technische Universit\\\"at Berlin","Berlin","Germany)","Martin Ludwig Zehetner (Forschungszentrum Informatik","Berlin","Germany)","Ketan Motlag (Technische Universit\\\"at Berlin","Berlin","Germany)","Sahin Albayrak (Technische Universit\\\"at Berlin","Berlin","Germany)"],"url":"https://arxiv.org/abs/2505.09356"}
{"created":"2025-05-15","title":"Marigold: Affordable Adaptation of Diffusion-Based Image Generators for Image Analysis","abstract":"The success of deep learning in computer vision over the past decade has hinged on large labeled datasets and strong pretrained models. In data-scarce settings, the quality of these pretrained models becomes crucial for effective transfer learning. Image classification and self-supervised learning have traditionally been the primary methods for pretraining CNNs and transformer-based architectures. Recently, the rise of text-to-image generative models, particularly those using denoising diffusion in a latent space, has introduced a new class of foundational models trained on massive, captioned image datasets. These models' ability to generate realistic images of unseen content suggests they possess a deep understanding of the visual world. In this work, we present Marigold, a family of conditional generative models and a fine-tuning protocol that extracts the knowledge from pretrained latent diffusion models like Stable Diffusion and adapts them for dense image analysis tasks, including monocular depth estimation, surface normals prediction, and intrinsic decomposition. Marigold requires minimal modification of the pre-trained latent diffusion model's architecture, trains with small synthetic datasets on a single GPU over a few days, and demonstrates state-of-the-art zero-shot generalization. Project page: https://marigoldcomputervision.github.io","authors":["Bingxin Ke","Kevin Qu","Tianfu Wang","Nando Metzger","Shengyu Huang","Bo Li","Anton Obukhov","Konrad Schindler"],"url":"https://arxiv.org/abs/2505.09358"}
{"created":"2025-05-15","title":"Improved Corner Cutting Constraints for Mixed-Integer Motion Planning of a Differential Drive Micro-Mobility Vehicle","abstract":"This paper addresses the problem of motion planning for differential drive micro-mobility platforms. This class of vehicle is designed to perform small-distance transportation of passengers and goods in structured environments. Our approach leverages mixed-integer linear programming (MILP) to compute global optimal collision-free trajectories taking into account the kinematics and dynamics of the vehicle. We propose novel constraints for intersample collision avoidance and demonstrate its effectiveness using pick-up and delivery missions and statistical analysis of Monte Carlo simulations. The results show that the novel formulation provides the best trajectories in terms of time expenditure and control effort when compared to two state-of-the-art approaches.","authors":["Angelo Caregnato-Neto","Janito Vaqueiro Ferreira"],"url":"https://arxiv.org/abs/2505.09359"}
{"created":"2025-05-15","title":"Efficient Mixed Precision Quantization in Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have become essential for handling large-scale graph applications. However, the computational demands of GNNs necessitate the development of efficient methods to accelerate inference. Mixed precision quantization emerges as a promising solution to enhance the efficiency of GNN architectures without compromising prediction performance. Compared to conventional deep learning architectures, GNN layers contain a wider set of components that can be quantized, including message passing functions, aggregation functions, update functions, the inputs, learnable parameters, and outputs of these functions. In this paper, we introduce a theorem for efficient quantized message passing to aggregate integer messages. It guarantees numerical equality of the aggregated messages using integer values with respect to those obtained with full (FP32) precision. Based on this theorem, we introduce the Mixed Precision Quantization for GNN (MixQ-GNN) framework, which flexibly selects effective integer bit-widths for all components within GNN layers. Our approach systematically navigates the wide set of possible bit-width combinations, addressing the challenge of optimizing efficiency while aiming at maintaining comparable prediction performance. MixQ-GNN integrates with existing GNN quantization methods, utilizing their graph structure advantages to achieve higher prediction performance. On average, MixQ-GNN achieved reductions in bit operations of 5.5x for node classification and 5.1x for graph classification compared to architectures represented in FP32 precision.","authors":["Samir Moustafa","Nils M. Kriege","Wilfried N. Gansterer"],"url":"https://arxiv.org/abs/2505.09361"}
{"created":"2025-05-15","title":"eqsat: An Equality Saturation Dialect for Non-destructive Rewriting","abstract":"With recent algorithmic improvements and easy-to-use libraries, equality saturation is being picked up for hardware design, program synthesis, theorem proving, program optimization, and more. Existing work on using equality saturation for program optimization makes use of external equality saturation libraries such as egg, typically generating a single optimized expression. In the context of a compiler, such an approach uses equality saturation to replace a small number of passes. In this work, we propose an alternative approach that represents equality saturation natively in the compiler's intermediate representation, facilitating the application of constructive compiler passes that maintain the e-graph state throughout the compilation flow. We take LLVM's MLIR framework and propose a new MLIR dialect named eqsat that represents e-graphs in MLIR code. This not only provides opportunities to rethink e-matching and extraction techniques by orchestrating existing MLIR passes, such as common subexpression elimination, but also avoids translation overhead between the chosen e-graph library and MLIR. Our eqsat intermediate representation (IR) allows programmers to apply equality saturation on arbitrary domain-specific IRs using the same flow as other compiler transformations in MLIR.","authors":["Jules Merckx","Alexandre Lopoukhine","Samuel Coward","Jianyi Cheng","Bjorn De Sutter","Tobias Grosser"],"url":"https://arxiv.org/abs/2505.09363"}
{"created":"2025-05-15","title":"Diffusion Recommender Models and the Illusion of Progress: A Concerning Study of Reproducibility and a Conceptual Mismatch","abstract":"Countless new machine learning models are published every year and are reported to significantly advance the state-of-the-art in \\emph{top-n} recommendation. However, earlier reproducibility studies indicate that progress in this area may be quite limited. Specifically, various widespread methodological issues, e.g., comparisons with untuned baseline models, have led to an \\emph{illusion of progress}. In this work, our goal is to examine whether these problems persist in today's research. To this end, we aim to reproduce the latest advancements reported from applying modern Denoising Diffusion Probabilistic Models to recommender systems, focusing on four models published at the top-ranked SIGIR conference in 2023 and 2024. Our findings are concerning, revealing persistent methodological problems. Alarmingly, through experiments, we find that the latest recommendation techniques based on diffusion models, despite their computational complexity and substantial carbon footprint, are consistently outperformed by simpler existing models. Furthermore, we identify key mismatches between the characteristics of diffusion models and those of the traditional \\emph{top-n} recommendation task, raising doubts about their suitability for recommendation. We also note that, in the papers we analyze, the generative capabilities of these models are constrained to a minimum. Overall, our results and continued methodological issues call for greater scientific rigor and a disruptive change in the research and publication culture in this area.","authors":["Michael Benigni","Maurizio Ferrari Dacrema","Dietmar Jannach"],"url":"https://arxiv.org/abs/2505.09364"}
{"created":"2025-05-15","title":"Personalized Control for Lower Limb Prosthesis Using Kolmogorov-Arnold Networks","abstract":"Objective: This paper investigates the potential of learnable activation functions in Kolmogorov-Arnold Networks (KANs) for personalized control in a lower-limb prosthesis. In addition, user-specific vs. pooled training data is evaluated to improve machine learning (ML) and Deep Learning (DL) performance for turn intent prediction.","authors":["SeyedMojtaba Mohasel","Alireza Afzal Aghaei","Corey Pew"],"url":"https://arxiv.org/abs/2505.09366"}
{"created":"2025-05-15","title":"RobustSpring: Benchmarking Robustness to Image Corruptions for Optical Flow, Scene Flow and Stereo","abstract":"Standard benchmarks for optical flow, scene flow, and stereo vision algorithms generally focus on model accuracy rather than robustness to image corruptions like noise or rain. Hence, the resilience of models to such real-world perturbations is largely unquantified. To address this, we present RobustSpring, a comprehensive dataset and benchmark for evaluating robustness to image corruptions for optical flow, scene flow, and stereo models. RobustSpring applies 20 different image corruptions, including noise, blur, color changes, quality degradations, and weather distortions, in a time-, stereo-, and depth-consistent manner to the high-resolution Spring dataset, creating a suite of 20,000 corrupted images that reflect challenging conditions. RobustSpring enables comparisons of model robustness via a new corruption robustness metric. Integration with the Spring benchmark enables public two-axis evaluations of both accuracy and robustness. We benchmark a curated selection of initial models, observing that accurate models are not necessarily robust and that robustness varies widely by corruption type. RobustSpring is a new computer vision benchmark that treats robustness as a first-class citizen to foster models that combine accuracy with resilience. It will be available at https://spring-benchmark.org.","authors":["Jenny Schmalfuss","Victor Oei","Lukas Mehl","Madlen Bartsch","Shashank Agnihotri","Margret Keuper","Andr\\'es Bruhn"],"url":"https://arxiv.org/abs/2505.09368"}
{"created":"2025-05-15","title":"A Dynamic Working Set Method for Compressed Sensing","abstract":"We propose a dynamic working set method (DWS) for the problem $\\min_{\\mathtt{x} \\in \\mathbb{R}^n} \\frac{1}{2}\\|\\mathtt{Ax}-\\mathtt{b}\\|^2 + \\eta\\|\\mathtt{x}\\|_1$ that arises from compressed sensing. DWS manages the working set while iteratively calling a regression solver to generate progressively better solutions. Our experiments show that DWS is more efficient than other state-of-the-art software in the context of compressed sensing. Scale space such that $\\|b\\|=1$. Let $s$ be the number of non-zeros in the unknown signal. We prove that for any given $\\varepsilon > 0$, DWS reaches a solution with an additive error $\\varepsilon/\\eta^2$ such that each call of the solver uses only $O(\\frac{1}{\\varepsilon}s\\log s \\log\\frac{1}{\\varepsilon})$ variables, and each intermediate solution has $O(\\frac{1}{\\varepsilon}s\\log s\\log\\frac{1}{\\varepsilon})$ non-zero coordinates.","authors":["Siu-Wing Cheng","Man Ting Wong"],"url":"https://arxiv.org/abs/2505.09370"}
{"created":"2025-05-15","title":"MAKE: Multi-Aspect Knowledge-Enhanced Vision-Language Pretraining for Zero-shot Dermatological Assessment","abstract":"Dermatological diagnosis represents a complex multimodal challenge that requires integrating visual features with specialized clinical knowledge. While vision-language pretraining (VLP) has advanced medical AI, its effectiveness in dermatology is limited by text length constraints and the lack of structured texts. In this paper, we introduce MAKE, a Multi-Aspect Knowledge-Enhanced vision-language pretraining framework for zero-shot dermatological tasks. Recognizing that comprehensive dermatological descriptions require multiple knowledge aspects that exceed standard text constraints, our framework introduces: (1) a multi-aspect contrastive learning strategy that decomposes clinical narratives into knowledge-enhanced sub-texts through large language models, (2) a fine-grained alignment mechanism that connects subcaptions with diagnostically relevant image features, and (3) a diagnosis-guided weighting scheme that adaptively prioritizes different sub-captions based on clinical significance prior. Through pretraining on 403,563 dermatological image-text pairs collected from education resources, MAKE significantly outperforms state-of-the-art VLP models on eight datasets across zero-shot skin disease classification, concept annotation, and cross-modal retrieval tasks. Our code will be made publicly available at https: //github.com/SiyuanYan1/MAKE.","authors":["Siyuan Yan","Xieji Li","Ming Hu","Yiwen Jiang","Zhen Yu","Zongyuan Ge"],"url":"https://arxiv.org/abs/2505.09372"}
{"created":"2025-05-15","title":"Optimization of the initial post-buckling response of trusses and frames by an asymptotic approach","abstract":"Asymptotic post-buckling theory is applied to sizing and topology optimization of trusses and frames, exploring its potential and current computational difficulties. We show that a designs' post-buckling response can be controlled by including the lowest two asymptotic coefficients, representing the initial post-buckling slope and curvature, in the optimization formulation. This also reduces the imperfection sensitivity of the optimized design. The asymptotic expansion can further be used to approximate the structural nonlinear response, and then to optimize for a given measure of the nonlinear mechanical performance such as, for example, end-compliance or complementary work. Examples of linear and nonlinear compliance minimization of trusses and frames show the effective use of the asymptotic method for including post-buckling constraints in structural optimization.","authors":["Federico Ferrari","Ole Sigmund"],"url":"https://arxiv.org/abs/2505.09373"}
{"created":"2025-05-15","title":"DNS Query Forgery: A Client-Side Defense Against Mobile App Traffic Profiling","abstract":"Mobile applications continuously generate DNS queries that can reveal sensitive user behavioral patterns even when communications are encrypted. This paper presents a privacy enhancement framework based on query forgery to protect users against profiling attempts that leverage these background communications. We first mathematically model user profiles as probability distributions over interest categories derived from mobile application traffic. We then evaluate three query forgery strategies -- uniform sampling, TrackMeNot-based generation, and an optimized approach that minimizes Kullback-Leibler divergence -- to quantify their effectiveness in obfuscating user profiles. Then we create a synthetic dataset comprising 1,000 user traces constructed from real mobile application traffic and we extract the user profiles based on DNS traffic. Our evaluation reveals that a 50\\% privacy improvement is achievable with less than 20\\% traffic overhead when using our approach, while achieving 100\\% privacy protection requires approximately 40-60\\% additional traffic. We further propose a modular system architecture for practical implementation of our protection mechanisms on mobile devices. This work offers a client-side privacy solution that operates without third-party trust requirements, empowering individual users to defend against traffic analysis without compromising application functionality.","authors":["Andrea Jimenez-Berenguel","C\\'esar Gil","Carlos Garcia-Rubio","Jordi Forn\\'e","Celeste Campo"],"url":"https://arxiv.org/abs/2505.09374"}
{"created":"2025-05-15","title":"Strategies to Measure Energy Consumption Using RAPL During Workflow Execution on Commodity Clusters","abstract":"In science, problems in many fields can be solved by processing datasets using a series of computationally expensive algorithms, sometimes referred to as workflows. Traditionally, the configurations of these workflows are optimized to achieve a short runtime for the given task and dataset on a given (often distributed) infrastructure. However, recently more attention has been drawn to energy-efficient computing, due to the negative impact of energy-inefficient computing on the environment and energy costs. To be able to assess the energy-efficiency of a given workflow configuration, reliable and accurate methods to measure the energy consumption of a system are required. One approach is the usage of built-in hardware energy counters, such as Intel RAPL. Unfortunately, effectively using RAPL for energy measurement within a workflow on a managed cluster with the typical deep software infrastructure stack can be difficult, for instance because of limited privileges and the need for communication between nodes. In this paper, we describe three ways to implement RAPL energy measurement on a Kubernetes cluster while executing scientific workflows utilizing the Nextflow workflow engine. We compare them by utilizing a set of eight criteria that should be fulfilled for accurate measurement, such as the ability to react to workflow faults, portability, and added overhead. We highlight advantages and drawbacks of each method and discuss challenges and pitfalls, as well as ways to avoid them. We also empirically evaluate all methods, and find that approaches using a shell script and a Nextflow plugin are both effective and easy to implement. Additionally, we find that measuring the energy consumption of a single task is straight forward when only one task runs at a time, but concurrent task executions on the same node require approximating per-task energy usage using metrics such as CPU utilization.","authors":["Philipp Thamm"],"url":"https://arxiv.org/abs/2505.09375"}
{"created":"2025-05-15","title":"AfforDance: Personalized AR Dance Learning System with Visual Affordance","abstract":"We propose AfforDance, an augmented reality (AR)-based dance learning system that generates personalized learning content and enhances learning through visual affordances. Our system converts user-selected dance videos into interactive learning experiences by integrating 3D reference avatars, audio synchronization, and adaptive visual cues that guide movement execution. This work contributes to personalized dance education by offering an adaptable, user-centered learning interface.","authors":["Hyunyoung Han","Jongwon Jang","Kitaeg Shim","Sang Ho Yoon"],"url":"https://arxiv.org/abs/2505.09376"}
{"created":"2025-05-15","title":"Strategic Jenga Play via Graph Based Dynamics Modeling","abstract":"Controlled manipulation of multiple objects whose dynamics are closely linked is a challenging problem within contact-rich manipulation, requiring an understanding of how the movement of one will impact the others. Using the Jenga game as a testbed to explore this problem, we graph-based modeling to tackle two different aspects of the task: 1) block selection and 2) block extraction. For block selection, we construct graphs of the Jenga tower and attempt to classify, based on the tower's structure, whether removing a given block will cause the tower to collapse. For block extraction, we train a dynamics model that predicts how all the blocks in the tower will move at each timestep in an extraction trajectory, which we then use in a sampling-based model predictive control loop to safely pull blocks out of the tower with a general-purpose parallel-jaw gripper. We train and evaluate our methods in simulation, demonstrating promising results towards block selection and block extraction on a challenging set of full-sized Jenga towers, even at advanced stages of the game.","authors":["Kavya Puthuveetil","Xinyi Zhang","Kazuto Yokoyama","Tetsuya Narita"],"url":"https://arxiv.org/abs/2505.09377"}
{"created":"2025-05-15","title":"Text-driven Motion Generation: Overview, Challenges and Directions","abstract":"Text-driven motion generation offers a powerful and intuitive way to create human movements directly from natural language. By removing the need for predefined motion inputs, it provides a flexible and accessible approach to controlling animated characters. This makes it especially useful in areas like virtual reality, gaming, human-computer interaction, and robotics. In this review, we first revisit the traditional perspective on motion synthesis, where models focused on predicting future poses from observed initial sequences, often conditioned on action labels. We then provide a comprehensive and structured survey of modern text-to-motion generation approaches, categorizing them from two complementary perspectives: (i) architectural, dividing methods into VAE-based, diffusion-based, and hybrid models; and (ii) motion representation, distinguishing between discrete and continuous motion generation strategies. In addition, we explore the most widely used datasets, evaluation methods, and recent benchmarks that have shaped progress in this area. With this survey, we aim to capture where the field currently stands, bring attention to its key challenges and limitations, and highlight promising directions for future exploration. We hope this work offers a valuable starting point for researchers and practitioners working to push the boundaries of language-driven human motion synthesis.","authors":["Ali Rida Sahili","Najett Neji","Hedi Tabia"],"url":"https://arxiv.org/abs/2505.09379"}
{"created":"2025-05-15","title":"Examining Deployment and Refinement of the VIOLA-AI Intracranial Hemorrhage Model Using an Interactive NeoMedSys Platform","abstract":"Background: There are many challenges and opportunities in the clinical deployment of AI tools in radiology. The current study describes a radiology software platform called NeoMedSys that can enable efficient deployment and refinements of AI models. We evaluated the feasibility and effectiveness of running NeoMedSys for three months in real-world clinical settings and focused on improvement performance of an in-house developed AI model (VIOLA-AI) designed for intracranial hemorrhage (ICH) detection.","authors":["Qinghui Liu","Jon Nesvold","Hanna Raaum","Elakkyen Murugesu","Martin R{\\o}vang","Bradley J Maclntosh","Atle Bj{\\o}rnerud","Karoline Skogen"],"url":"https://arxiv.org/abs/2505.09380"}
{"created":"2025-05-15","title":"The Voice Timbre Attribute Detection 2025 Challenge Evaluation Plan","abstract":"Voice timbre refers to the unique quality or character of a person's voice that distinguishes it from others as perceived by human hearing. The Voice Timbre Attribute Detection (VtaD) 2025 challenge focuses on explaining the voice timbre attribute in a comparative manner. In this challenge, the human impression of voice timbre is verbalized with a set of sensory descriptors, including bright, coarse, soft, magnetic, and so on. The timbre is explained from the comparison between two voices in their intensity within a specific descriptor dimension. The VtaD 2025 challenge starts in May and culminates in a special proposal at the NCMMSC2025 conference in October 2025 in Zhenjiang, China.","authors":["Zhengyan Sheng","Jinghao He","Liping Chen","Kong Aik Lee","Zhen-Hua Ling"],"url":"https://arxiv.org/abs/2505.09382"}
{"created":"2025-05-15","title":"CANTXSec: A Deterministic Intrusion Detection and Prevention System for CAN Bus Monitoring ECU Activations","abstract":"Despite being a legacy protocol with various known security issues, Controller Area Network (CAN) still represents the de-facto standard for communications within vehicles, ships, and industrial control systems. Many research works have designed Intrusion Detection Systems (IDSs) to identify attacks by training machine learning classifiers on bus traffic or its properties. Actions to take after detection are, on the other hand, less investigated, and prevention mechanisms usually include protocol modification (e.g., adding authentication). An effective solution has yet to be implemented on a large scale in the wild. The reasons are related to the effort to handle sporadic false positives, the inevitable delay introduced by authentication, and the closed-source automobile environment that does not easily permit modifying Electronic Control Units (ECUs) software.","authors":["Denis Donadel","Kavya Balasubramanian","Alessandro Brighente","Bhaskar Ramasubramanian","Mauro Conti","Radha Poovendran"],"url":"https://arxiv.org/abs/2505.09384"}
{"created":"2025-05-15","title":"FedSaaS: Class-Consistency Federated Semantic Segmentation via Global Prototype Supervision and Local Adversarial Harmonization","abstract":"Federated semantic segmentation enables pixel-level classification in images through collaborative learning while maintaining data privacy. However, existing research commonly overlooks the fine-grained class relationships within the semantic space when addressing heterogeneous problems, particularly domain shift. This oversight results in ambiguities between class representation. To overcome this challenge, we propose a novel federated segmentation framework that strikes class consistency, termed FedSaaS. Specifically, we introduce class exemplars as a criterion for both local- and global-level class representations. On the server side, the uploaded class exemplars are leveraged to model class prototypes, which supervise global branch of clients, ensuring alignment with global-level representation. On the client side, we incorporate an adversarial mechanism to harmonize contributions of global and local branches, leading to consistent output. Moreover, multilevel contrastive losses are employed on both sides to enforce consistency between two-level representations in the same semantic space. Extensive experiments on several driving scene segmentation datasets demonstrate that our framework outperforms state-of-the-art methods, significantly improving average segmentation accuracy and effectively addressing the class-consistency representation problem.","authors":["Xiaoyang Yu","Xiaoming Wu","Xin Wang","Dongrun Li","Ming Yang","Peng Cheng"],"url":"https://arxiv.org/abs/2505.09385"}
{"created":"2025-05-15","title":"Instant AoI Optimization through Relay Location Selection in Disaster Multi-hop Communication","abstract":"Meteorological disasters such as typhoons, forest fires, and floods can damage the communication infrastructures, which will further disable the communication capabilities of cellular networks. The multi-hop wireless communication based on IoT devices (e.g., rescue robots, UAVs, and mobile devices) becomes an available and rapidly deployable communication approach for search and rescue operations. However, Age of Information (AoI), an emerging network performance metric, has not been comprehensively investigated in this multi-hop model. In this paper, we first construct a UAV-relayed wireless network model and formulate the end-to-end instant AoI. Then we derive the optimal location of the relay UAV to achieve the minimum instant AoI by mathematical analysis. Simulations show that the derived relay location can always guarantee the optimal AoI and outperform other schemes.","authors":["Yang Gao","Zezhi Zeng"],"url":"https://arxiv.org/abs/2505.09386"}
{"created":"2025-05-15","title":"Qwen3 Technical Report","abstract":"In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.","authors":["An Yang","Anfeng Li","Baosong Yang","Beichen Zhang","Binyuan Hui","Bo Zheng","Bowen Yu","Chang Gao","Chengen Huang","Chenxu Lv","Chujie Zheng","Dayiheng Liu","Fan Zhou","Fei Huang","Feng Hu","Hao Ge","Haoran Wei","Huan Lin","Jialong Tang","Jian Yang","Jianhong Tu","Jianwei Zhang","Jianxin Yang","Jiaxi Yang","Jing Zhou","Jingren Zhou","Junyang Lin","Kai Dang","Keqin Bao","Kexin Yang","Le Yu","Lianghao Deng","Mei Li","Mingfeng Xue","Mingze Li","Pei Zhang","Peng Wang","Qin Zhu","Rui Men","Ruize Gao","Shixuan Liu","Shuang Luo","Tianhao Li","Tianyi Tang","Wenbiao Yin","Xingzhang Ren","Xinyu Wang","Xinyu Zhang","Xuancheng Ren","Yang Fan","Yang Su","Yichang Zhang","Yinger Zhang","Yu Wan","Yuqiong Liu","Zekun Wang","Zeyu Cui","Zhenru Zhang","Zhipeng Zhou","Zihan Qiu"],"url":"https://arxiv.org/abs/2505.09388"}
{"created":"2025-05-15","title":"Mitigating Configuration Differences Between Development and Production Environments: A Catalog of Strategies","abstract":"Context: The Configuration Management of the development and production environments is an important aspect of IT operations. However, managing the configuration differences between these two environments can be challenging, leading to inconsistent behavior, unexpected errors, and increased downtime. Objective: In this study, we sought to investigate the strategies software companies employ to mitigate the configuration differences between the development and production environments. Our goal is to provide a comprehensive understanding of these strategies used to contribute to reducing the risk of configuration-related issues. Method: To achieve this goal, we interviewed 17 participants and leveraged the Thematic Analysis methodology to analyze the interview data. These participants shed some light on the current practices, processes, challenges, or issues they have encountered. Results: Based on the interviews, we systematically formulated and structured a catalog of eight strategies that explain how software producing companies mitigate these configuration differences. These strategies vary from 1) creating detailed configuration management plans, 2) using automation tools, and 3) developing processes to test and validate changes through containers and virtualization technologies. Conclusion: By implementing these strategies, companies can improve their ability to respond quickly and effectively to changes in the production environment. In addition, they can also ensure compliance with industry standards and regulations.","authors":["Marcos Nazario","Rodrigo Bonifacio","Gustavo Pinto"],"url":"https://arxiv.org/abs/2505.09392"}
{"created":"2025-05-15","title":"UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units","abstract":"Sparse wearable inertial measurement units (IMUs) have gained popularity for estimating 3D human motion. However, challenges such as pose ambiguity, data drift, and limited adaptability to diverse bodies persist. To address these issues, we propose UMotion, an uncertainty-driven, online fusing-all state estimation framework for 3D human shape and pose estimation, supported by six integrated, body-worn ultra-wideband (UWB) distance sensors with IMUs. UWB sensors measure inter-node distances to infer spatial relationships, aiding in resolving pose ambiguities and body shape variations when combined with anthropometric data. Unfortunately, IMUs are prone to drift, and UWB sensors are affected by body occlusions. Consequently, we develop a tightly coupled Unscented Kalman Filter (UKF) framework that fuses uncertainties from sensor data and estimated human motion based on individual body shape. The UKF iteratively refines IMU and UWB measurements by aligning them with uncertain human motion constraints in real-time, producing optimal estimates for each. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of UMotion in stabilizing sensor data and the improvement over state of the art in pose accuracy.","authors":["Huakun Liu","Hiroki Ota","Xin Wei","Yutaro Hirao","Monica Perusquia-Hernandez","Hideaki Uchiyama","Kiyoshi Kiyokawa"],"url":"https://arxiv.org/abs/2505.09393"}
{"created":"2025-05-15","title":"Index Modulated Affine Frequency Division Multiplexing With Spread Spectrum","abstract":"The recently proposed affine frequency division multiplexing (AFDM) is a new transmission waveform that has shown excellent performance in high-mobility environments, making it a sensible option for the next-generation wireless networks. In this paper, we investigate an energy-efficient index modulation scheme for AFDM by leveraging spread spectrum, referred to as IM-AFDM-SS, to combat the interference caused by the doubly dispersive channels. Specifically, the information bits are conveyed by the transmitted symbols as well as the indices of the selected spreading codes in our proposed IM-AFDM-SS scheme. To avoid extensive computations, we also develop a lowcomplexity maximal ratio combining (MRC) detector algorithm, which recovers the spreading codes first and demodulates the symbols afterwards. Moreover, an upper bound on the bit error rate (BER) of the proposed IM-AFDM-SS system with maximumlikelihood (ML) detection is derived. Numerical results demonstrate the superiority of the proposed IM-AFDM-SS system over the classical AFDM spread spectrum (AFDM-SS) and the existing index modulated AFDM (IM-AFDM) systems.","authors":["Mi Qian","Fei Ji","Yao Ge","Miaowen Wen","Yong Liang Guan"],"url":"https://arxiv.org/abs/2505.09394"}
{"created":"2025-05-15","title":"The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners","abstract":"The rapid rise of large language models (LLMs) has shifted artificial intelligence (AI) research toward agentic systems, motivating the use of weaker and more flexible notions of agency. However, this shift raises key questions about the extent to which LLM-based agents replicate human strategic reasoning, particularly in game-theoretic settings. In this context, we examine the role of agentic sophistication in shaping artificial reasoners' performance by evaluating three agent designs: a simple game-theoretic model, an unstructured LLM-as-agent model, and an LLM integrated into a traditional agentic framework. Using guessing games as a testbed, we benchmarked these agents against human participants across general reasoning patterns and individual role-based objectives. Furthermore, we introduced obfuscated game scenarios to assess agents' ability to generalise beyond training distributions. Our analysis, covering over 2000 reasoning samples across 25 agent configurations, shows that human-inspired cognitive structures can enhance LLM agents' alignment with human strategic behaviour. Still, the relationship between agentic design complexity and human-likeness is non-linear, highlighting a critical dependence on underlying LLM capabilities and suggesting limits to simple architectural augmentation.","authors":["Vince Trencsenyi","Agnieszka Mensfelt","Kostas Stathis"],"url":"https://arxiv.org/abs/2505.09396"}
{"created":"2025-05-15","title":"Utilization of Skin Color Change for Image-based Tactile Sensing","abstract":"Measurement of pressure distribution applied to a fingertip is crucial for the teleoperation of robots and human computer interface. Previous studies have acquired pressure distribution by affixing a sensor array to the fingertip or by optically recording the deformation of an object. However, these existing methods inhibit the fingertip from directly contacting the texture, and the pressure applied to the fingertip is measured indirectly. In this study, we propose a method to measure pressure distribution by directly touching a transparent object, focusing on the change in skin color induced by the applied pressure, caused by blood flow. We evaluated the relationship between pressure and skin color change when local pressure is applied, and found a correlation between the pressure and the color change. However, the contact area and the color change area did not align perfectly. We further explored the factor causing the spatial non-uniformity of the color change, by accounting for the stress distribution using finite element analysis. These results suggest that the proposed measurement method can be utilized to measure the internal stress distribution, and it is anticipated to serve as a simple sensor in the field of human computer interface.","authors":["Seitaro Kaneko","Hiroki Ishizuka","Hidenori Yoshimura","Hiroyuki Kajimoto"],"url":"https://arxiv.org/abs/2505.09402"}
{"created":"2025-05-15","title":"Wormhole Detection Based on Z-Score And Neighbor Table Comparison","abstract":"Wormhole attacks can cause serious disruptions to the network topology in disaster rescue opportunity networks.","authors":["Zezhi Zeng"],"url":"https://arxiv.org/abs/2505.09405"}
{"created":"2025-05-15","title":"FreeDriveRF: Monocular RGB Dynamic NeRF without Poses for Autonomous Driving via Point-Level Dynamic-Static Decoupling","abstract":"Dynamic scene reconstruction for autonomous driving enables vehicles to perceive and interpret complex scene changes more precisely. Dynamic Neural Radiance Fields (NeRFs) have recently shown promising capability in scene modeling. However, many existing methods rely heavily on accurate poses inputs and multi-sensor data, leading to increased system complexity. To address this, we propose FreeDriveRF, which reconstructs dynamic driving scenes using only sequential RGB images without requiring poses inputs. We innovatively decouple dynamic and static parts at the early sampling level using semantic supervision, mitigating image blurring and artifacts. To overcome the challenges posed by object motion and occlusion in monocular camera, we introduce a warped ray-guided dynamic object rendering consistency loss, utilizing optical flow to better constrain the dynamic modeling process. Additionally, we incorporate estimated dynamic flow to constrain the pose optimization process, improving the stability and accuracy of unbounded scene reconstruction. Extensive experiments conducted on the KITTI and Waymo datasets demonstrate the superior performance of our method in dynamic scene modeling for autonomous driving.","authors":["Yue Wen","Liang Song","Yijia Liu","Siting Zhu","Yanzi Miao","Lijun Han","Hesheng Wang"],"url":"https://arxiv.org/abs/2505.09406"}
{"created":"2025-05-15","title":"Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits","abstract":"Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.","authors":["Subrit Dikshit","Ritu Tiwari","Priyank Jain"],"url":"https://arxiv.org/abs/2505.09407"}
{"created":"2025-05-15","title":"Counterfactual Strategies for Markov Decision Processes","abstract":"Counterfactuals are widely used in AI to explain how minimal changes to a model's input can lead to a different output. However, established methods for computing counterfactuals typically focus on one-step decision-making, and are not directly applicable to sequential decision-making tasks. This paper fills this gap by introducing counterfactual strategies for Markov Decision Processes (MDPs). During MDP execution, a strategy decides which of the enabled actions (with known probabilistic effects) to execute next. Given an initial strategy that reaches an undesired outcome with a probability above some limit, we identify minimal changes to the initial strategy to reduce that probability below the limit. We encode such counterfactual strategies as solutions to non-linear optimization problems, and further extend our encoding to synthesize diverse counterfactual strategies. We evaluate our approach on four real-world datasets and demonstrate its practical viability in sophisticated sequential decision-making tasks.","authors":["Paul Kobialka","Lina Gerlach","Francesco Leofante","Erika \\'Abrah\\'am","Silvia Lizeth Tapia Tarifa","Einar Broch Johnsen"],"url":"https://arxiv.org/abs/2505.09412"}
{"created":"2025-05-15","title":"Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians","abstract":"Current learning-based methods predict NeRF or 3D Gaussians from point clouds to achieve photo-realistic rendering but still depend on categorical priors, dense point clouds, or additional refinements. Hence, we introduce a novel point cloud rendering method by predicting 2D Gaussians from point clouds. Our method incorporates two identical modules with an entire-patch architecture enabling the network to be generalized to multiple datasets. The module normalizes and initializes the Gaussians utilizing the point cloud information including normals, colors and distances. Then, splitting decoders are employed to refine the initial Gaussians by duplicating them and predicting more accurate results, making our methodology effectively accommodate sparse point clouds as well. Once trained, our approach exhibits direct generalization to point clouds across different categories. The predicted Gaussians are employed directly for rendering without additional refinement on the rendered images, retaining the benefits of 2D Gaussians. We conduct extensive experiments on various datasets, and the results demonstrate the superiority and generalization of our method, which achieves SOTA performance. The code is available at https://github.com/murcherful/GauPCRender}{https://github.com/murcherful/GauPCRender.","authors":["Ma Changfeng","Bi Ran","Guo Jie","Wang Chongjun","Guo Yanwen"],"url":"https://arxiv.org/abs/2505.09413"}
{"created":"2025-05-15","title":"FACTors: A New Dataset for Studying the Fact-checking Ecosystem","abstract":"Our fight against false information is spearheaded by fact-checkers. They investigate the veracity of claims and document their findings as fact-checking reports. With the rapid increase in the amount of false information circulating online, the use of automation in fact-checking processes aims to strengthen this ecosystem by enhancing scalability. Datasets containing fact-checked claims play a key role in developing such automated solutions. However, to the best of our knowledge, there is no fact-checking dataset at the ecosystem level, covering claims from a sufficiently long period of time and sourced from a wide range of actors reflecting the entire ecosystem that admittedly follows widely-accepted codes and principles of fact-checking. We present a new dataset FACTors, the first to fill this gap by presenting ecosystem-level data on fact-checking. It contains 118,112 claims from 117,993 fact-checking reports in English (co-)authored by 1,953 individuals and published during the period of 1995-2025 by 39 fact-checking organisations that are active signatories of the IFCN (International Fact-Checking Network) and/or EFCSN (European Fact-Checking Standards Network). It contains 7,327 overlapping claims investigated by multiple fact-checking organisations, corresponding to 2,977 unique claims. It allows to conduct new ecosystem-level studies of the fact-checkers (organisations and individuals). To demonstrate the usefulness of FACTors, we present three example applications, including a first-of-its-kind statistical analysis of the fact-checking ecosystem, examining the political inclinations of the fact-checking organisations, and attempting to assign a credibility score to each organisation based on the findings of the statistical analysis and political leanings. Our methods for constructing FACTors are generic and can be used to maintain a live dataset that can be updated dynamically.","authors":["Enes Altuncu","Can Ba\\c{s}kent","Sanjay Bhattacherjee","Shujun Li","Dwaipayan Roy"],"url":"https://arxiv.org/abs/2505.09414"}
{"created":"2025-05-15","title":"FaceShield: Explainable Face Anti-Spoofing with Multimodal Large Language Models","abstract":"Face anti-spoofing (FAS) is crucial for protecting facial recognition systems from presentation attacks. Previous methods approached this task as a classification problem, lacking interpretability and reasoning behind the predicted results. Recently, multimodal large language models (MLLMs) have shown strong capabilities in perception, reasoning, and decision-making in visual tasks. However, there is currently no universal and comprehensive MLLM and dataset specifically designed for FAS task. To address this gap, we propose FaceShield, a MLLM for FAS, along with the corresponding pre-training and supervised fine-tuning (SFT) datasets, FaceShield-pre10K and FaceShield-sft45K. FaceShield is capable of determining the authenticity of faces, identifying types of spoofing attacks, providing reasoning for its judgments, and detecting attack areas. Specifically, we employ spoof-aware vision perception (SAVP) that incorporates both the original image and auxiliary information based on prior knowledge. We then use an prompt-guided vision token masking (PVTM) strategy to random mask vision tokens, thereby improving the model's generalization ability. We conducted extensive experiments on three benchmark datasets, demonstrating that FaceShield significantly outperforms previous deep learning models and general MLLMs on four FAS tasks, i.e., coarse-grained classification, fine-grained classification, reasoning, and attack localization. Our instruction datasets, protocols, and codes will be released soon.","authors":["Hongyang Wang","Yichen Shi","Zhuofu Tao","Yuhao Gao","Liepiao Zhang","Xun Lin","Jun Feng","Xiaochen Yuan","Zitong Yu","Xiaochun Cao"],"url":"https://arxiv.org/abs/2505.09415"}
{"created":"2025-05-15","title":"Non-expansive Fuzzy ALC","abstract":"Fuzzy description logics serve the representation of vague knowledge, typically letting concepts take truth degrees in the unit interval. Expressiveness, logical properties, and complexity vary strongly with the choice of propositional base. The Lukasiewicz propositional base is generally perceived to have preferable logical properties but often entails high complexity or even undecidability. Contrastingly, the less expressive Zadeh propositional base comes with low complexity but entails essentially no change in logical behaviour compared to the classical case. To strike a balance between these poles, we propose non-expansive fuzzy ALC, in which the Zadeh base is extended with Lukasiewicz connectives where one side is restricted to be a rational constant, that is, with constant shift operators. This allows, for instance, modelling dampened inheritance of properties along roles. We present an unlabelled tableau method for non-expansive fuzzy ALC, which allows reasoning over general TBoxes in EXPTIME like in two-valued ALC.","authors":["Stefan Gebhart","Lutz Schr\\\"oder","Paul Wild"],"url":"https://arxiv.org/abs/2505.09416"}
{"created":"2025-05-15","title":"MoRAL: Motion-aware Multi-Frame 4D Radar and LiDAR Fusion for Robust 3D Object Detection","abstract":"Reliable autonomous driving systems require accurate detection of traffic participants. To this end, multi-modal fusion has emerged as an effective strategy. In particular, 4D radar and LiDAR fusion methods based on multi-frame radar point clouds have demonstrated the effectiveness in bridging the point density gap. However, they often neglect radar point clouds' inter-frame misalignment caused by object movement during accumulation and do not fully exploit the object dynamic information from 4D radar. In this paper, we propose MoRAL, a motion-aware multi-frame 4D radar and LiDAR fusion framework for robust 3D object detection. First, a Motion-aware Radar Encoder (MRE) is designed to compensate for inter-frame radar misalignment from moving objects. Later, a Motion Attention Gated Fusion (MAGF) module integrate radar motion features to guide LiDAR features to focus on dynamic foreground objects. Extensive evaluations on the View-of-Delft (VoD) dataset demonstrate that MoRAL outperforms existing methods, achieving the highest mAP of 73.30% in the entire area and 88.68% in the driving corridor. Notably, our method also achieves the best AP of 69.67% for pedestrians in the entire area and 96.25% for cyclists in the driving corridor.","authors":["Xiangyuan Peng","Yu Wang","Miao Tang","Bierzynski Kay","Lorenzo Servadei","Robert Wille"],"url":"https://arxiv.org/abs/2505.09422"}
{"created":"2025-05-15","title":"Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion","abstract":"Recent studies have proved that imitation learning shows strong potential in the field of robotic manipulation. However, existing methods still struggle with precision manipulation task and rely on inefficient image/point cloud observations. In this paper, we explore to introduce SE(3) object pose into imitation learning and propose the pose-guided efficient imitation learning methods for robotic precise insertion task. First, we propose a precise insertion diffusion policy which utilizes the relative SE(3) pose as the observation-action pair. The policy models the source object SE(3) pose trajectory relative to the target object. Second, we explore to introduce the RGBD data to the pose-guided diffusion policy. Specifically, we design a goal-conditioned RGBD encoder to capture the discrepancy between the current state and the goal state. In addition, a pose-guided residual gated fusion method is proposed, which takes pose features as the backbone, and the RGBD features selectively compensate for pose feature deficiencies through an adaptive gating mechanism. Our methods are evaluated on 6 robotic precise insertion tasks, demonstrating competitive performance with only 7-10 demonstrations. Experiments demonstrate that the proposed methods can successfully complete precision insertion tasks with a clearance of about 0.01 mm. Experimental results highlight its superior efficiency and generalization capability compared to existing baselines. Code will be available at https://github.com/sunhan1997/PoseInsert.","authors":["Han Sun","Yizhao Wang","Zhenning Zhou","Shuai Wang","Haibo Yang","Jingyuan Sun","Qixin Cao"],"url":"https://arxiv.org/abs/2505.09424"}
{"created":"2025-05-15","title":"SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation","abstract":"Large Language Models (LLMs) show growing promise in autonomous driving by reasoning over complex traffic scenarios to generate path plans. However, their tendencies toward overconfidence, and hallucinations raise critical safety concerns. We introduce SafePath, a modular framework that augments LLM-based path planning with formal safety guarantees using conformal prediction. SafePath operates in three stages. In the first stage, we use an LLM that generates a set of diverse candidate paths, exploring possible trajectories based on agent behaviors and environmental cues. In the second stage, SafePath filters out high-risk trajectories while guaranteeing that at least one safe option is included with a user-defined probability, through a multiple-choice question-answering formulation that integrates conformal prediction. In the final stage, our approach selects the path with the lowest expected collision risk when uncertainty is low or delegates control to a human when uncertainty is high. We theoretically prove that SafePath guarantees a safe trajectory with a user-defined probability, and we show how its human delegation rate can be tuned to balance autonomy and safety. Extensive experiments on nuScenes and Highway-env show that SafePath reduces planning uncertainty by 77\\% and collision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven path planning more safer.","authors":["Achref Doula","Max M\\\"uhl\\\"auser","Alejandro Sanchez Guinea"],"url":"https://arxiv.org/abs/2505.09427"}
{"created":"2025-05-15","title":"Linear Search with Probabilistic Detection and Variable Speeds","abstract":"We present results on new variants of the famous linear search (or cow-path) problem that involves an agent searching for a target with unknown position on the infinite line. We consider the variant where the agent can move either at speed $1$ or at a slower speed $v \\in [0, 1)$. When traveling at the slower speed $v$, the agent is guaranteed to detect the target upon passing through its location. When traveling at speed $1$, however, the agent, upon passing through the target's location, detects it with probability $p \\in [0, 1]$. We present algorithms and provide upper bounds for the competitive ratios for three cases separately: when $p=0$, $v=0$, and when $p,v \\in (0,1)$. We also prove that the provided algorithm for the $p=0$ case is optimal.","authors":["Jared Coleman","Oscar Morales-Ponce"],"url":"https://arxiv.org/abs/2505.09429"}
{"created":"2025-05-15","title":"Train a Multi-Task Diffusion Policy on RLBench-18 in One Day with One GPU","abstract":"We present a method for training multi-task vision-language robotic diffusion policies that reduces training time and memory usage by an order of magnitude. This improvement arises from a previously underexplored distinction between action diffusion and the image diffusion techniques that inspired it: image generation targets are high-dimensional, while robot actions lie in a much lower-dimensional space. Meanwhile, the vision-language conditions for action generation remain high-dimensional. Our approach, Mini-Diffuser, exploits this asymmetry by introducing Level-2 minibatching, which pairs multiple noised action samples with each vision-language condition, instead of the conventional one-to-one sampling strategy. To support this batching scheme, we introduce architectural adaptations to the diffusion transformer that prevent information leakage across samples while maintaining full conditioning access. In RLBench simulations, Mini-Diffuser achieves 95\\% of the performance of state-of-the-art multi-task diffusion policies, while using only 5\\% of the training time and 7\\% of the memory. Real-world experiments further validate that Mini-Diffuser preserves the key strengths of diffusion-based policies, including the ability to model multimodal action distributions and produce behavior conditioned on diverse perceptual inputs. Code available at github.com/utomm/mini-diffuse-actor.","authors":["Yutong Hu","Pinhao Song","Kehan Wen","Renaud Detry"],"url":"https://arxiv.org/abs/2505.09430"}
{"created":"2025-05-15","title":"Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenche-Young Losses","abstract":"Surrogate regret bounds bridge the gap between the convergence rates of surrogate and target losses, with linear bounds favorable for their lossless regret transfer. While convex smooth surrogate losses are appealing in particular due to the efficient estimation and optimization, the existence of a trade-off between the smoothness and linear regret bound has been believed in the community. That being said, the better optimization and estimation properties of convex smooth surrogate losses may inevitably deteriorate after undergoing the regret transfer onto a target loss. We overcome this dilemma for arbitrary discrete target losses by constructing a convex smooth surrogate loss, which entails a linear surrogate regret bound composed with a tailored prediction link. The construction is based on Fenchel-Young losses generated by the convolutional negentropy, which are equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk. Consequently, the infimal convolution enables us to derive a smooth loss while maintaining the surrogate regret bound linear. We additionally benefit from the infimal convolution to have a consistent estimator of the underlying class probability. Our results are overall a novel demonstration of how convex analysis penetrates into optimization and statistical efficiency in risk minimization.","authors":["Yuzhou Cao","Han Bao","Lei Feng","Bo An"],"url":"https://arxiv.org/abs/2505.09432"}
{"created":"2025-05-15","title":"Efficient LiDAR Reflectance Compression via Scanning Serialization","abstract":"Reflectance attributes in LiDAR point clouds provide essential information for downstream tasks but remain underexplored in neural compression methods. To address this, we introduce SerLiC, a serialization-based neural compression framework to fully exploit the intrinsic characteristics of LiDAR reflectance. SerLiC first transforms 3D LiDAR point clouds into 1D sequences via scan-order serialization, offering a device-centric perspective for reflectance analysis. Each point is then tokenized into a contextual representation comprising its sensor scanning index, radial distance, and prior reflectance, for effective dependencies exploration. For efficient sequential modeling, Mamba is incorporated with a dual parallelization scheme, enabling simultaneous autoregressive dependency capture and fast processing. Extensive experiments demonstrate that SerLiC attains over 2x volume reduction against the original reflectance data, outperforming the state-of-the-art method by up to 22% reduction of compressed bits while using only 2% of its parameters. Moreover, a lightweight version of SerLiC achieves > 10 fps (frames per second) with just 111K parameters, which is attractive for real-world applications.","authors":["Jiahao Zhu","Kang You","Dandan Ding","Zhan Ma"],"url":"https://arxiv.org/abs/2505.09433"}
{"created":"2025-05-15","title":"Decentralized Nonlinear Model Predictive Control-Based Flock Navigation with Real-Time Obstacle Avoidance in Unknown Obstructed Environments","abstract":"This work extends our prior work on the distributed nonlinear model predictive control (NMPC) for navigating a robot fleet following a certain flocking behavior in unknown obstructed environments with a more realistic local obstacle avoidance strategy. More specifically, we integrate the local obstacle avoidance constraint using point clouds into the NMPC framework. Here, each agent relies on data from its local sensor to perceive and respond to nearby obstacles. A point cloud processing technique is presented for both two-dimensional and three-dimensional point clouds to minimize the computational burden during the optimization. The process consists of directional filtering and down-sampling that significantly reduce the number of data points. The algorithm's performance is validated through realistic 3D simulations in Gazebo, and its practical feasibility is further explored via hardware-in-the-loop (HIL) simulations on embedded platforms.","authors":["Nuthasith Gerdpratoom","Kaoru Yamamoto"],"url":"https://arxiv.org/abs/2505.09434"}
{"created":"2025-05-15","title":"Endo-CLIP: Progressive Self-Supervised Pre-training on Raw Colonoscopy Records","abstract":"Pre-training on image-text colonoscopy records offers substantial potential for improving endoscopic image analysis, but faces challenges including non-informative background images, complex medical terminology, and ambiguous multi-lesion descriptions. We introduce Endo-CLIP, a novel self-supervised framework that enhances Contrastive Language-Image Pre-training (CLIP) for this domain. Endo-CLIP's three-stage framework--cleansing, attunement, and unification--addresses these challenges by (1) removing background frames, (2) leveraging large language models to extract clinical attributes for fine-grained contrastive learning, and (3) employing patient-level cross-attention to resolve multi-polyp ambiguities. Extensive experiments demonstrate that Endo-CLIP significantly outperforms state-of-the-art pre-training methods in zero-shot and few-shot polyp detection and classification, paving the way for more accurate and clinically relevant endoscopic analysis.","authors":["Yili He","Yan Zhu","Peiyao Fu","Ruijie Yang","Tianyi Chen","Zhihua Wang","Quanlin Li","Pinghong Zhou","Xian Yang","Shuo Wang"],"url":"https://arxiv.org/abs/2505.09435"}
{"created":"2025-05-15","title":"CXMArena: Unified Dataset to benchmark performance in realistic CXM Scenarios","abstract":"Large Language Models (LLMs) hold immense potential for revolutionizing Customer Experience Management (CXM), particularly in contact center operations. However, evaluating their practical utility in complex operational environments is hindered by data scarcity (due to privacy concerns) and the limitations of current benchmarks. Existing benchmarks often lack realism, failing to incorporate deep knowledge base (KB) integration, real-world noise, or critical operational tasks beyond conversational fluency. To bridge this gap, we introduce CXMArena, a novel, large-scale synthetic benchmark dataset specifically designed for evaluating AI in operational CXM contexts. Given the diversity in possible contact center features, we have developed a scalable LLM-powered pipeline that simulates the brand's CXM entities that form the foundation of our datasets-such as knowledge articles including product specifications, issue taxonomies, and contact center conversations. The entities closely represent real-world distribution because of controlled noise injection (informed by domain experts) and rigorous automated validation. Building on this, we release CXMArena, which provides dedicated benchmarks targeting five important operational tasks: Knowledge Base Refinement, Intent Prediction, Agent Quality Adherence, Article Search, and Multi-turn RAG with Integrated Tools. Our baseline experiments underscore the benchmark's difficulty: even state of the art embedding and generation models achieve only 68% accuracy on article search, while standard embedding methods yield a low F1 score of 0.3 for knowledge base refinement, highlighting significant challenges for current models necessitating complex pipelines and solutions over conventional techniques.","authors":["Raghav Garg","Kapil Sharma","Karan Gupta"],"url":"https://arxiv.org/abs/2505.09436"}
{"created":"2025-05-15","title":"Dimensioning and Optimization of Reliability Coverage in Local 6G Networks","abstract":"Enabling vertical use cases for the sixth generation (6G) wireless networks, such as automated manufacturing, immersive extended reality (XR), and self-driving fleets, will require network designs that meet reliability and latency targets in well-defined service areas. In order to establish a quantifiable design objective, we introduce the novel concept of reliability coverage, defined as the percentage area covered by communication services operating under well-defined reliability and performance targets. Reliability coverage allows us to unify the different network design tasks occurring at different time scales, namely resource orchestration and allocation, resulting in a single framework for dimensioning and optimization in local 6G networks. The two time scales, when considered together, yield remarkably consistent results and allow us to observe how stringent reliability/latency requirements translate into the increased wireless network resource demands.","authors":["Jacek Kibi{\\l}da","Dian Echevarr\\'ia P\\'erez","Andr\\'e Gomes","Onel L. Alcaraz L\\'opez","Arthur S. de Sena","Nurul Huda Mahmood","Hirley Alves"],"url":"https://arxiv.org/abs/2505.09440"}
{"created":"2025-05-15","title":"MrTrack: Register Mamba for Needle Tracking with Rapid Reciprocating Motion during Ultrasound-Guided Aspiration Biopsy","abstract":"Ultrasound-guided fine needle aspiration (FNA) biopsy is a common minimally invasive diagnostic procedure. However, an aspiration needle tracker addressing rapid reciprocating motion is still missing. MrTrack, an aspiration needle tracker with a mamba-based register mechanism, is proposed. MrTrack leverages a Mamba-based register extractor to sequentially distill global context from each historical search map, storing these temporal cues in a register bank. The Mamba-based register retriever then retrieves temporal prompts from the register bank to provide external cues when current vision features are temporarily unusable due to rapid reciprocating motion and imaging degradation. A self-supervised register diversify loss is proposed to encourage feature diversity and dimension independence within the learned register, mitigating feature collapse. Comprehensive experiments conducted on both motorized and manual aspiration datasets demonstrate that MrTrack not only outperforms state-of-the-art trackers in accuracy and robustness but also achieves superior inference efficiency.","authors":["Yuelin Zhang","Qingpeng Ding","Long Lei","Yongxuan Feng","Raymond Shing-Yan Tang","Shing Shin Cheng"],"url":"https://arxiv.org/abs/2505.09450"}
{"created":"2025-05-15","title":"SEGA-DCIM: Design Space Exploration-Guided Automatic Digital CIM Compiler with Multiple Precision Support","abstract":"Digital computing-in-memory (DCIM) has been a popular solution for addressing the memory wall problem in recent years. However, the DCIM design still heavily relies on manual efforts, and the optimization of DCIM is often based on human experience. These disadvantages limit the time to market while increasing the design difficulty of DCIMs. This work proposes a design space exploration-guided automatic DCIM compiler (SEGA-DCIM) with multiple precision support, including integer and floating-point data precision operations. SEGA-DCIM can automatically generate netlists and layouts of DCIM designs by leveraging a template-based method. With a multi-objective genetic algorithm (MOGA)-based design space explorer, SEGA-DCIM can easily select appropriate DCIM designs for a specific application considering the trade-offs among area, power, and delay. As demonstrated by the experimental results, SEGA-DCIM offers solutions with wide design space, including integer and floating-point precision designs, while maintaining competitive performance compared to state-of-the-art (SOTA) DCIMs.","authors":["Haikang Diao","Haoyi Zhang","Jiahao Song","Haoyang Luo","Yibo Lin","Runsheng Wang","Yuan Wang","Xiyuan Tang"],"url":"https://arxiv.org/abs/2505.09451"}
{"created":"2025-05-15","title":"Beyond Pixels: Leveraging the Language of Soccer to Improve Spatio-Temporal Action Detection in Broadcast Videos","abstract":"State-of-the-art spatio-temporal action detection (STAD) methods show promising results for extracting soccer events from broadcast videos. However, when operated in the high-recall, low-precision regime required for exhaustive event coverage in soccer analytics, their lack of contextual understanding becomes apparent: many false positives could be resolved by considering a broader sequence of actions and game-state information. In this work, we address this limitation by reasoning at the game level and improving STAD through the addition of a denoising sequence transduction task. Sequences of noisy, context-free player-centric predictions are processed alongside clean game state information using a Transformer-based encoder-decoder model. By modeling extended temporal context and reasoning jointly over team-level dynamics, our method leverages the \"language of soccer\" - its tactical regularities and inter-player dependencies - to generate \"denoised\" sequences of actions. This approach improves both precision and recall in low-confidence regimes, enabling more reliable event extraction from broadcast video and complementing existing pixel-based methods.","authors":["Jeremie Ochin","Raphael Chekroun","Bogdan Stanciulescu","Sotiris Manitsaris"],"url":"https://arxiv.org/abs/2505.09455"}
{"created":"2025-05-15","title":"Variational Rank Reduction Autoencoder","abstract":"Deterministic Rank Reduction Autoencoders (RRAEs) enforce by construction a regularization on the latent space by applying a truncated SVD. While this regularization makes Autoencoders more powerful, using them for generative purposes is counter-intuitive due to their deterministic nature. On the other hand, Variational Autoencoders (VAEs) are well known for their generative abilities by learning a probabilistic latent space. In this paper, we present Variational Rank Reduction Autoencoders (VRRAEs), a model that leverages the advantages of both RRAEs and VAEs. Our claims and results show that when carefully sampling the latent space of RRAEs and further regularizing with the Kullback-Leibler (KL) divergence (similarly to VAEs), VRRAEs outperform RRAEs and VAEs. Additionally, we show that the regularization induced by the SVD not only makes VRRAEs better generators than VAEs, but also reduces the possibility of posterior collapse. Our results include a synthetic dataset of a small size that showcases the robustness of VRRAEs against collapse, and three real-world datasets; the MNIST, CelebA, and CIFAR-10, over which VRRAEs are shown to outperform both VAEs and RRAEs on many random generation and interpolation tasks based on the FID score.","authors":["Jad Mounayer","Alicia Tierz","Jerome Tomezyk","Chady Ghnatios","Francisco Chinesta"],"url":"https://arxiv.org/abs/2505.09458"}
{"created":"2025-05-15","title":"ARM SVE Unleashed: Performance and Insights Across HPC Applications on Nvidia Grace","abstract":"Vector architectures are essential for boosting computing throughput. ARM provides SVE as the next-generation length-agnostic vector extension beyond traditional fixed-length SIMD. This work provides a first study of the maturity and readiness of exploiting ARM and SVE in HPC. Using selected performance hardware events on the ARM Grace processor and analytical models, we derive new metrics to quantify the effectiveness of exploiting SVE vectorization to reduce executed instructions and improve performance speedup. We further propose an adapted roofline model that combines vector length and data elements to identify potential performance bottlenecks. Finally, we propose a decision tree for classifying the SVE-boosted performance in applications.","authors":["Ruimin Shi","Gabin Schieffer","Maya Gokhale","Pei-Hung Lin","Hiren Patel","Ivy Peng"],"url":"https://arxiv.org/abs/2505.09462"}
{"created":"2025-05-15","title":"A 2D Semantic-Aware Position Encoding for Vision Transformers","abstract":"Vision transformers have demonstrated significant advantages in computer vision tasks due to their ability to capture long-range dependencies and contextual relationships through self-attention. However, existing position encoding techniques, which are largely borrowed from natural language processing, fail to effectively capture semantic-aware positional relationships between image patches. Traditional approaches like absolute position encoding and relative position encoding primarily focus on 1D linear position relationship, often neglecting the semantic similarity between distant yet contextually related patches. These limitations hinder model generalization, translation equivariance, and the ability to effectively handle repetitive or structured patterns in images. In this paper, we propose 2-Dimensional Semantic-Aware Position Encoding ($\\text{SaPE}^2$), a novel position encoding method with semantic awareness that dynamically adapts position representations by leveraging local content instead of fixed linear position relationship or spatial coordinates. Our method enhances the model's ability to generalize across varying image resolutions and scales, improves translation equivariance, and better aggregates features for visually similar but spatially distant patches. By integrating $\\text{SaPE}^2$ into vision transformers, we bridge the gap between position encoding and perceptual similarity, thereby improving performance on computer vision tasks.","authors":["Xi Chen","Shiyang Zhou","Muqi Huang","Jiaxu Feng","Yun Xiong","Kun Zhou","Biao Yang","Yuhui Zhang","Huishuai Bao","Sijia Peng","Chuan Li","Feng Shi"],"url":"https://arxiv.org/abs/2505.09466"}
{"created":"2025-05-15","title":"Primal-dual splitting methods for phase-field surfactant model with moving contact lines","abstract":"Surfactants have important effects on the dynamics of droplets on solid surfaces, which has inspired many industrial applications. Phase-field surfactant model with moving contact lines (PFS-MCL) has been employed to investigate the complex droplet dynamics with surfactants, while its numerical simulation remains challenging due to the coupling of gradient flows with respect to transport distances involving nonlinear and degenerate mobilities. We propose a novel structure-preserving variational scheme for PFS-MCL model with the dynamic boundary condition based on the minimizing movement scheme and optimal transport theory for Wasserstein gradient flows. The proposed scheme consists of a series of convex minimization problems and can be efficiently solved by our proposed primal-dual splitting method and its accelerated versions. By respecting the underlying PDE's variational structure with respect to the transport distance, the proposed scheme is proved to inherits the desirable properties including original energy dissipation, bound-preserving, and mass conservation. Through a suite of numerical simulations, we validate the performance of the proposed scheme and investigate the effects of surfactants on the droplet dynamics.","authors":["Wei Wu","Zhen Zhang","Chaozhen Wei"],"url":"https://arxiv.org/abs/2505.09469"}
{"created":"2025-05-15","title":"Streaming Multi-agent Pathfinding","abstract":"The task of the multi-agent pathfinding (MAPF) problem is to navigate a team of agents from their start point to the goal points. However, this setup is unsuitable in the assembly line scenario, which is periodic with a long working hour. To address this issue, the study formalizes the streaming MAPF (S-MAPF) problem, which assumes that the agents in the same agent stream have a periodic start time and share the same action sequence. The proposed solution, Agent Stream Conflict-Based Search (ASCBS), is designed to tackle this problem by incorporating a cyclic vertex/edge constraint to handle conflicts. Additionally, this work explores the potential usage of the disjoint splitting strategy within ASCBS. Experimental results indicate that ASCBS surpasses traditional MAPF solvers in terms of runtime for scenarios with prolonged working hours.","authors":["Mingkai Tang","Lu Gan","Kaichen Zhang"],"url":"https://arxiv.org/abs/2505.09472"}
{"created":"2025-05-15","title":"Function-Correcting $b$-symbol Codes for Locally $(\\lambda, \\rho,b)$-Functions","abstract":"The family of functions plays a central role in the design and effectiveness of function-correcting codes. By focusing on a well-defined family of functions, function-correcting codes can be constructed with minimal length while still ensuring full error detection or correction within that family. In this paper, we explore locally ($\\lambda,\\rho$)-functions and develop function-correcting codes using these functions for $b$-symbol read channels. We establish the recurrence relation between the optimal redundancy of $(f,t)$ -function-correcting codes for the $(b+1)$-read and $b$-read channels. We establish an upper bound on the redundancy of general locally ($\\lambda,\\rho$, $b$)-function-correcting codes by linking it to the minimum achievable length of $b$-symbol error-correcting codes and traditional Hamming-metric codes, given a fixed number of codewords and a specified minimum distance. Specifically, we present explicit upper bounds for the classes of ($4,2t,b$)-local functions and ($2^b,2t,b$)-local functions. Additionally, for the case where $b=1$, we show that a ($3,2t,1$)-local function achieves the optimal redundancy of $3t$ under certain conditions. Moreover, we explicitly investigate locality and redundancy for the weight distribution function.","authors":["Gyanendra K. Verma","Anamika Singh","Abhay Kumar Singh"],"url":"https://arxiv.org/abs/2505.09473"}
{"created":"2025-05-15","title":"aUToPath: Unified Planning and Control for Autonomous Vehicles in Urban Environments Using Hybrid Lattice and Free-Space Search","abstract":"This paper presents aUToPath, a unified online framework for global path-planning and control to address the challenge of autonomous navigation in cluttered urban environments. A key component of our framework is a novel hybrid planner that combines pre-computed lattice maps with dynamic free-space sampling to efficiently generate optimal driveable corridors in cluttered scenarios. Our system also features sequential convex programming (SCP)-based model predictive control (MPC) to refine the corridors into smooth, dynamically consistent trajectories. A single optimization problem is used to both generate a trajectory and its corresponding control commands; this addresses limitations of decoupled approaches by guaranteeing a safe and feasible path. Simulation results of the novel planner on randomly generated obstacle-rich scenarios demonstrate the success rate of a free-space Adaptively Informed Trees* (AIT*)-based planner, and runtimes comparable to a lattice-based planner. Real-world experiments of the full system on a Chevrolet Bolt EUV further validate performance in dense obstacle fields, demonstrating no violations of traffic, kinematic, or vehicle constraints, and a 100% success rate across eight trials.","authors":["Tanmay P. Patel","Connor Wilson","Ellina R. Zhang","Morgan Tran","Chang Keun Paik","Steven L. Waslander","Timothy D. Barfoot"],"url":"https://arxiv.org/abs/2505.09475"}
{"created":"2025-05-15","title":"Deploying Foundation Model-Enabled Air and Ground Robots in the Field: Challenges and Opportunities","abstract":"The integration of foundation models (FMs) into robotics has enabled robots to understand natural language and reason about the semantics in their environments. However, existing FM-enabled robots primary operate in closed-world settings, where the robot is given a full prior map or has a full view of its workspace. This paper addresses the deployment of FM-enabled robots in the field, where missions often require a robot to operate in large-scale and unstructured environments. To effectively accomplish these missions, robots must actively explore their environments, navigate obstacle-cluttered terrain, handle unexpected sensor inputs, and operate with compute constraints. We discuss recent deployments of SPINE, our LLM-enabled autonomy framework, in field robotic settings. To the best of our knowledge, we present the first demonstration of large-scale LLM-enabled robot planning in unstructured environments with several kilometers of missions. SPINE is agnostic to a particular LLM, which allows us to distill small language models capable of running onboard size, weight and power (SWaP) limited platforms. Via preliminary model distillation work, we then present the first language-driven UAV planner using on-device language models. We conclude our paper by proposing several promising directions for future research.","authors":["Zachary Ravichandran","Fernando Cladera","Jason Hughes","Varun Murali","M. Ani Hsieh","George J. Pappas","Camillo J. Taylor","Vijay Kumar"],"url":"https://arxiv.org/abs/2505.09477"}
{"created":"2025-05-15","title":"Card Sorting Simulator: Augmenting Design of Logical Information Architectures with Large Language Models","abstract":"Card sorting is a common ideation technique that elicits information on users' mental organization of content and functionality by having them sort items into categories. For more robust card sorting research, digital card sorting tools could benefit from providing quick automated feedback. Our objective of this research is to advance toward an instrument that applies artificial intelligence (AI) to augment card sorting. For this purpose, we develop the Card Sorting Simulator, a prototype tool that leverages Large Language Models (LLMs) to generate informative categorizations of cards. To illuminate how aligned the simulation is with card sorting by actual participants, and to inform the instrument's design decisions, we conducted a generalizability-focused comparative study. We obtained 28 pre-existing card sorting studies from real practitioners, comprising 1,399 participants, along with diverse contents and origins. With this dataset, we conducted a comprehensive and nuanced analysis of the agreement between actual card sorting results (clusterings of cards) and synthetic clusterings across a multitude of LLMs and prompt designs. Mutual information scores indicate a good degree of agreement to real result clustering, although similarity matrices also demonstrate inconsistencies from mental models, which can be attributed to their top-down nature. Furthermore, the number of cards or complexity of their labels impact the accuracy of its simulation. These findings bolster the case for AI augmentation in card sorting research as a source of meaningful preliminary feedback and highlight the need for further study for the development and validation of intelligent user research tools.","authors":["Eduard Kuric","Peter Demcak","Matus Krajcovic"],"url":"https://arxiv.org/abs/2505.09478"}
{"created":"2025-05-15","title":"Denoising and Alignment: Rethinking Domain Generalization for Multimodal Face Anti-Spoofing","abstract":"Face Anti-Spoofing (FAS) is essential for the security of facial recognition systems in diverse scenarios such as payment processing and surveillance. Current multimodal FAS methods often struggle with effective generalization, mainly due to modality-specific biases and domain shifts. To address these challenges, we introduce the \\textbf{M}ulti\\textbf{m}odal \\textbf{D}enoising and \\textbf{A}lignment (\\textbf{MMDA}) framework. By leveraging the zero-shot generalization capability of CLIP, the MMDA framework effectively suppresses noise in multimodal data through denoising and alignment mechanisms, thereby significantly enhancing the generalization performance of cross-modal alignment. The \\textbf{M}odality-\\textbf{D}omain Joint \\textbf{D}ifferential \\textbf{A}ttention (\\textbf{MD2A}) module in MMDA concurrently mitigates the impacts of domain and modality noise by refining the attention mechanism based on extracted common noise features. Furthermore, the \\textbf{R}epresentation \\textbf{S}pace \\textbf{S}oft (\\textbf{RS2}) Alignment strategy utilizes the pre-trained CLIP model to align multi-domain multimodal data into a generalized representation space in a flexible manner, preserving intricate representations and enhancing the model's adaptability to various unseen conditions. We also design a \\textbf{U}-shaped \\textbf{D}ual \\textbf{S}pace \\textbf{A}daptation (\\textbf{U-DSA}) module to enhance the adaptability of representations while maintaining generalization performance. These improvements not only enhance the framework's generalization capabilities but also boost its ability to represent complex representations. Our experimental results on four benchmark datasets under different evaluation protocols demonstrate that the MMDA framework outperforms existing state-of-the-art methods in terms of cross-domain generalization and multimodal detection accuracy. The code will be released soon.","authors":["Yingjie Ma","Xun Lin","Zitong Yu","Xin Liu","Xiaochen Yuan","Weicheng Xie","Linlin Shen"],"url":"https://arxiv.org/abs/2505.09484"}
{"created":"2025-05-15","title":"Preserving Plasticity in Continual Learning with Adaptive Linearity Injection","abstract":"Loss of plasticity in deep neural networks is the gradual reduction in a model's capacity to incrementally learn and has been identified as a key obstacle to learning in non-stationary problem settings. Recent work has shown that deep linear networks tend to be resilient towards loss of plasticity. Motivated by this observation, we propose Adaptive Linearization (AdaLin), a general approach that dynamically adapts each neuron's activation function to mitigate plasticity loss. Unlike prior methods that rely on regularization or periodic resets, AdaLin equips every neuron with a learnable parameter and a gating mechanism that injects linearity into the activation function based on its gradient flow. This adaptive modulation ensures sufficient gradient signal and sustains continual learning without introducing additional hyperparameters or requiring explicit task boundaries. When used with conventional activation functions like ReLU, Tanh, and GeLU, we demonstrate that AdaLin can significantly improve performance on standard benchmarks, including Random Label and Permuted MNIST, Random Label and Shuffled CIFAR-10, and Class-Split CIFAR-100. Furthermore, its efficacy is shown in more complex scenarios, such as class-incremental learning on CIFAR-100 with a ResNet-18 backbone, and in mitigating plasticity loss in off-policy reinforcement learning agents. We perform a systematic set of ablations that show that neuron-level adaptation is crucial for good performance and analyze a number of metrics in the network that might be correlated to loss of plasticity.","authors":["Seyed Roozbeh Razavi Rohani","Khashayar Khajavi","Wesley Chung","Mo Chen","Sharan Vaswani"],"url":"https://arxiv.org/abs/2505.09486"}
{"created":"2025-05-15","title":"Radon Exposure Dataset","abstract":"Exposure to elevated radon levels in the home is one of the leading causes of lung cancer in the world. The following study describes the creation of a comprehensive, state-level dataset designed to enable the modeling and prediction of household radon concentrations at Zip Code Tabulation Area (ZCTA) and sub-kilometer scales. Details include the data collection and processing involved in compiling physical and demographic factors for Pennsylvania and Utah. Attempting to mitigate this risk requires identifying the underlying geological causes and the populations that might be at risk. This work focuses on identifying at-risk populations throughout Pennsylvania and Utah, where radon levels are some of the highest in the country. The resulting dataset harmonizes geological and demographic factors from various sources and spatial resolutions, including temperature, geochemistry, and soil characteristics. Demographic variables such as the household heating fuel used, the age of building, and the housing type provide further insight into which populations could be most susceptible in areas with potentially high radon levels. This dataset also serves as a foundational resource for two other studies conducted by the authors. The resolution of the data provides a novel approach to predicting potential radon exposure, and the data processing conducted for these states can be scaled up to larger spatial resolutions (e.g., the Contiguous United States [CONUS]) and allow for a broad reclassification of radon exposure potential in the United States.","authors":["Dakotah Maguire","Jeremy Logan","Heechan Lee","Heidi Hanson"],"url":"https://arxiv.org/abs/2505.09489"}
{"created":"2025-05-15","title":"Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput","abstract":"In this paper, we introduce Flash-VL 2B, a novel approach to optimizing Vision-Language Models (VLMs) for real-time applications, targeting ultra-low latency and high throughput without sacrificing accuracy. Leveraging advanced architectural enhancements and efficient computational strategies, Flash-VL 2B is designed to maximize throughput by reducing processing time while maintaining competitive performance across multiple vision-language benchmarks. Our approach includes tailored architectural choices, token compression mechanisms, data curation, training schemes, and a novel image processing technique called implicit semantic stitching that effectively balances computational load and model performance. Through extensive evaluations on 11 standard VLM benchmarks, we demonstrate that Flash-VL 2B achieves state-of-the-art results in both speed and accuracy, making it a promising solution for deployment in resource-constrained environments and large-scale real-time applications.","authors":["Bo Zhang","Shuo Li","Runhe Tian","Yang Yang","Jixin Tang","Jinhao Zhou","Lin Ma"],"url":"https://arxiv.org/abs/2505.09498"}
{"created":"2025-05-15","title":"Layered Unlearning for Adversarial Relearning","abstract":"Our goal is to understand how post-training methods, such as fine-tuning, alignment, and unlearning, modify language model behavior and representations. We are particularly interested in the brittle nature of these modifications that makes them easy to bypass through prompt engineering or relearning. Recent results suggest that post-training induces shallow context-dependent ``circuits'' that suppress specific response patterns. This could be one explanation for the brittleness of post-training. To test this hypothesis, we design an unlearning algorithm, Layered Unlearning (LU), that creates distinct inhibitory mechanisms for a growing subset of the data. By unlearning the first $i$ folds while retaining the remaining $k - i$ at the $i$th of $k$ stages, LU limits the ability of relearning on a subset of data to recover the full dataset. We evaluate LU through a combination of synthetic and large language model (LLM) experiments. We find that LU improves robustness to adversarial relearning for several different unlearning methods. Our results contribute to the state-of-the-art of machine unlearning and provide insight into the effect of post-training updates.","authors":["Timothy Qian","Vinith Suriyakumar","Ashia Wilson","Dylan Hadfield-Menell"],"url":"https://arxiv.org/abs/2505.09500"}
{"created":"2025-05-15","title":"Scaling Up: Revisiting Mining Android Sandboxes at Scale for Malware Classification","abstract":"The widespread use of smartphones in daily life has raised concerns about privacy and security among researchers and practitioners. Privacy issues are generally highly prevalent in mobile applications, particularly targeting the Android platform, the most popular mobile operating system. For this reason, several techniques have been proposed to identify malicious behavior in Android applications, including the Mining Android Sandbox approach (MAS approach), which aims to identify malicious behavior in repackaged Android applications (apps). However, previous empirical studies evaluated the MAS approach using a small dataset consisting of only 102 pairs of original and repackaged apps. This limitation raises questions about the external validity of their findings and whether the MAS approach can be generalized to larger datasets. To address these concerns, this paper presents the results of a replication study focused on evaluating the performance of the MAS approach regarding its capabilities of correctly classifying malware from different families. Unlike previous studies, our research employs a dataset that is an order of magnitude larger, comprising 4,076 pairs of apps covering a more diverse range of Android malware families. Surprisingly, our findings indicate a poor performance of the MAS approach for identifying malware, with the F1-score decreasing from 0.90 for the small dataset used in the previous studies to 0.54 in our more extensive dataset. Upon closer examination, we discovered that certain malware families partially account for the low accuracy of the MAS approach, which fails to classify a repackaged version of an app as malware correctly. Our findings highlight the limitations of the MAS approach, particularly when scaled, and underscore the importance of complementing it with other techniques to detect a broader range of malware effectively.","authors":["Francisco Costa","Ismael Medeiros","Leandro Oliveira","Jo\\~ao Cal\\'assio","Rodrigo Bonif\\'acio","Krishna Narasimhan","Mira Mezini","M\\'arcio Ribeiro"],"url":"https://arxiv.org/abs/2505.09501"}
{"created":"2025-05-15","title":"Towards Fair In-Context Learning with Tabular Foundation Models","abstract":"Tabular foundational models have exhibited strong in-context learning (ICL) capabilities on structured data, allowing them to make accurate predictions on test sets without parameter updates, using training examples as context. This emerging approach positions itself as a competitive alternative to traditional gradient-boosted tree methods. However, while biases in conventional machine learning models are well documented, it remains unclear how these biases manifest in tabular ICL. The paper investigates the fairness implications of tabular ICL and explores three preprocessing strategies--correlation removal, group-balanced demonstration selection, and uncertainty-based demonstration selection--to address bias. Comprehensive experiments indicate that uncertainty-based demonstration selection consistently enhances group fairness of in-context predictions. The source code for reproducing the results of this work can be found at https://github.com/patrikken/Fair-TabICL.","authors":["Patrik Kenfack","Samira Ebrahimi Kaho","Ulrich A\\\"ivodji"],"url":"https://arxiv.org/abs/2505.09503"}
{"created":"2025-05-15","title":"Partnership through Play: Investigating How Long-Distance Couples Use Digital Games to Facilitate Intimacy","abstract":"Long-distance relationships (LDRs) have become more common in the last few decades, primarily among young adults pursuing educational or employment opportunities. A common way for couples in LDRs to spend time together is by playing multiplayer video games, which are often a shared hobby and therefore a preferred joint activity. However, games are relatively understudied in the context of relational maintenance for LDRs. In this work, we used a mixed-methods approach to collect data on the experiences of 13 couples in LDRs who frequently play games together. We investigated different values around various game mechanics and modalities and found significant differences in couple play styles, and also detail how couples appropriate game mechanics to express affection to each other virtually. We also created prototypes and design implications based on couples' needs surrounding the lack of physical sensation and memorabilia storage in most popular games.","authors":["Nisha Devasia","Adrian Rodriguez","Logan Tuttle","Julie Kientz"],"url":"https://arxiv.org/abs/2505.09509"}
{"created":"2025-05-15","title":"Design of a Formation Control System to Assist Human Operators in Flying a Swarm of Robotic Blimps","abstract":"Formation control is essential for swarm robotics, enabling coordinated behavior in complex environments. In this paper, we introduce a novel formation control system for an indoor blimp swarm using a specialized leader-follower approach enhanced with a dynamic leader-switching mechanism. This strategy allows any blimp to take on the leader role, distributing maneuvering demands across the swarm and enhancing overall formation stability. Only the leader blimp is manually controlled by a human operator, while follower blimps use onboard monocular cameras and a laser altimeter for relative position and altitude estimation. A leader-switching scheme is proposed to assist the human operator to maintain stability of the swarm, especially when a sharp turn is performed. Experimental results confirm that the leader-switching mechanism effectively maintains stable formations and adapts to dynamic indoor environments while assisting human operator.","authors":["Tianfu Wu","Jiaqi Fu","Wugang Meng","Sungjin Cho","Huanzhe Zhan","Fumin Zhang"],"url":"https://arxiv.org/abs/2505.09511"}
{"created":"2025-05-15","title":"Risk-aware Markov Decision Processes Using Cumulative Prospect Theory","abstract":"Cumulative prospect theory (CPT) is the first theory for decision-making under uncertainty that combines full theoretical soundness and empirically realistic features [P.P. Wakker - Prospect theory: For risk and ambiguity, Page 2]. While CPT was originally considered in one-shot settings for risk-aware decision-making, we consider CPT in sequential decision-making. The most fundamental and well-studied models for sequential decision-making are Markov chains (MCs), and their generalization Markov decision processes (MDPs). The complexity theoretic study of MCs and MDPs with CPT is a fundamental problem that has not been addressed in the literature.","authors":["Thomas Brihaye","Krishnendu Chatterjee","Stefanie Mohr","Maximilian Weininger"],"url":"https://arxiv.org/abs/2505.09514"}
{"created":"2025-05-15","title":"Regulation without calibration","abstract":"This article revisits the importance of the internal model principle in the literature of regulation and synchronization. Trajectory regulation, the task of regulating continuous-time signals generated by differential equations, is contrasted with event regulation, the task of only regulating discrete events associated with the trajectories. In trajectory regulation, the internal model principle requires an exact internal generator of the continuous-time trajectories, which translates into unrealistic calibration requirements. Event regulation is envisioned as a way to relieve calibration of the continuous behavior while ensuring reliability of the discrete events.","authors":["Rodolphe Sepulchre","Alessandro Cecconi","Michelangelo Bin","Lorenzo Marconi"],"url":"https://arxiv.org/abs/2505.09515"}
{"created":"2025-05-15","title":"\\textsc{rfPG}: Robust Finite-Memory Policy Gradients for Hidden-Model POMDPs","abstract":"Partially observable Markov decision processes (POMDPs) model specific environments in sequential decision-making under uncertainty. Critically, optimal policies for POMDPs may not be robust against perturbations in the environment. Hidden-model POMDPs (HM-POMDPs) capture sets of different environment models, that is, POMDPs with a shared action and observation space. The intuition is that the true model is hidden among a set of potential models, and it is unknown which model will be the environment at execution time. A policy is robust for a given HM-POMDP if it achieves sufficient performance for each of its POMDPs. We compute such robust policies by combining two orthogonal techniques: (1) a deductive formal verification technique that supports tractable robust policy evaluation by computing a worst-case POMDP within the HM-POMDP and (2) subgradient ascent to optimize the candidate policy for a worst-case POMDP. The empirical evaluation shows that, compared to various baselines, our approach (1) produces policies that are more robust and generalize better to unseen POMDPs and (2) scales to HM-POMDPs that consist of over a hundred thousand environments.","authors":["Maris F. L. Galesloot","Roman Andriushchenko","Milan \\v{C}e\\v{s}ka","Sebastian Junges","Nils Jansen"],"url":"https://arxiv.org/abs/2505.09518"}
{"created":"2025-05-15","title":"PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning","abstract":"Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating router into prompt tuning (PT) increases training efficiency yet does not improve performance universally; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets demonstrate that PT-MoE achieves state-of-the-art performance in both question answering (QA) and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA. Our analysis reveals that while PT methods generally excel in QA tasks and LoRA-based methods in math datasets, the integration of matrix decomposition and MoE in PT-MoE yields complementary benefits: decomposition enables efficient parameter sharing across experts while MoE provides dynamic adaptation, collectively enabling PT-MoE to demonstrate cross-task consistency and generalization abilities. These findings, along with ablation studies on routing mechanisms and architectural components, provide insights for future PEFT methods.","authors":["Zongqian Li","Yixuan Su","Nigel Collier"],"url":"https://arxiv.org/abs/2505.09519"}
{"created":"2025-05-15","title":"An Asymptotically Optimal Approximation Algorithm for Multiobjective Submodular Maximization at Scale","abstract":"Maximizing a single submodular set function subject to a cardinality constraint is a well-studied and central topic in combinatorial optimization. However, finding a set that maximizes multiple functions at the same time is much less understood, even though it is a formulation which naturally occurs in robust maximization or problems with fairness considerations such as fair influence maximization or fair allocation.","authors":["Fabian Spaeh","Fabian Spaeh"],"url":"https://arxiv.org/abs/2505.09525"}
{"created":"2025-05-15","title":"Evaluation Metrics for Misinformation Warning Interventions: Challenges and Prospects","abstract":"Misinformation has become a widespread issue in the 21st century, impacting numerous areas of society and underscoring the need for effective intervention strategies. Among these strategies, user-centered interventions, such as warning systems, have shown promise in reducing the spread of misinformation. Many studies have used various metrics to evaluate the effectiveness of these warning interventions. However, no systematic review has thoroughly examined these metrics in all studies. This paper provides a comprehensive review of existing metrics for assessing the effectiveness of misinformation warnings, categorizing them into four main groups: behavioral impact, trust and credulity, usability, and cognitive and psychological effects. Through this review, we identify critical challenges in measuring the effectiveness of misinformation warnings, including inconsistent use of cognitive and attitudinal metrics, the lack of standardized metrics for affective and emotional impact, variations in user trust, and the need for more inclusive warning designs. We present an overview of these metrics and propose areas for future research.","authors":["Hussaini Zubairu","Abdelrahaman Abdou","Ashraf Matrawy"],"url":"https://arxiv.org/abs/2505.09526"}
{"created":"2025-05-15","title":"Conformal Bounds on Full-Reference Image Quality for Imaging Inverse Problems","abstract":"In imaging inverse problems, we would like to know how close the recovered image is to the true image in terms of full-reference image quality (FRIQ) metrics like PSNR, SSIM, LPIPS, etc. This is especially important in safety-critical applications like medical imaging, where knowing that, say, the SSIM was poor could potentially avoid a costly misdiagnosis. But since we don't know the true image, computing FRIQ is non-trivial. In this work, we combine conformal prediction with approximate posterior sampling to construct bounds on FRIQ that are guaranteed to hold up to a user-specified error probability. We demonstrate our approach on image denoising and accelerated magnetic resonance imaging (MRI) problems. Code is available at https://github.com/jwen307/quality_uq.","authors":["Jeffrey Wen","Rizwan Ahmad","Philip Schniter"],"url":"https://arxiv.org/abs/2505.09528"}
{"created":"2025-05-15","title":"Contactless Cardiac Pulse Monitoring Using Event Cameras","abstract":"Time event cameras are a novel technology for recording scene information at extremely low latency and with low power consumption. Event cameras output a stream of events that encapsulate pixel-level light intensity changes within the scene, capturing information with a higher dynamic range and temporal resolution than traditional cameras. This study investigates the contact-free reconstruction of an individual's cardiac pulse signal from time event recording of their face using a supervised convolutional neural network (CNN) model. An end-to-end model is trained to extract the cardiac signal from a two-dimensional representation of the event stream, with model performance evaluated based on the accuracy of the calculated heart rate. The experimental results confirm that physiological cardiac information in the facial region is effectively preserved within the event stream, showcasing the potential of this novel sensor for remote heart rate monitoring. The model trained on event frames achieves a root mean square error (RMSE) of 3.32 beats per minute (bpm) compared to the RMSE of 2.92 bpm achieved by the baseline model trained on standard camera frames. Furthermore, models trained on event frames generated at 60 and 120 FPS outperformed the 30 FPS standard camera results, achieving an RMSE of 2.54 and 2.13 bpm, respectively.","authors":["Mohamed Moustafa","Joseph Lemley","Peter Corcoran"],"url":"https://arxiv.org/abs/2505.09529"}
{"created":"2025-05-15","title":"Optimizing the Decoding Probability and Coverage Ratio of Composite DNA","abstract":"This paper studies two problems that are motivated by the novel recent approach of composite DNA that takes advantage of the DNA synthesis property which generates a huge number of copies for every synthesized strand. Under this paradigm, every composite symbols does not store a single nucleotide but a mixture of the four DNA nucleotides. The first problem studies the expected number of strand reads in order to decode a composite strand or a group of composite strands. In the second problem, our goal is study how to carefully choose a fixed number of mixtures of the DNA nucleotides such that the decoding probability by the maximum likelihood decoder is maximized.","authors":["Tomer Cohen","Eitan Yaakobi"],"url":"https://arxiv.org/abs/2505.09533"}
{"created":"2025-05-15","title":"GlobalMood: A cross-cultural benchmark for music emotion recognition","abstract":"Human annotations of mood in music are essential for music generation and recommender systems. However, existing datasets predominantly focus on Western songs with mood terms derived from English, which may limit generalizability across diverse linguistic and cultural backgrounds. To address this, we introduce `GlobalMood', a novel cross-cultural benchmark dataset comprising 1,180 songs sampled from 59 countries, with large-scale annotations collected from 2,519 individuals across five culturally and linguistically distinct locations: U.S., France, Mexico, S. Korea, and Egypt. Rather than imposing predefined mood categories, we implement a bottom-up, participant-driven approach to organically elicit culturally specific music-related mood terms. We then recruit another pool of human participants to collect 988,925 ratings for these culture-specific descriptors. Our analysis confirms the presence of a valence-arousal structure shared across cultures, yet also reveals significant divergences in how certain mood terms, despite being dictionary equivalents, are perceived cross-culturally. State-of-the-art multimodal models benefit substantially from fine-tuning on our cross-culturally balanced dataset, as evidenced by improved alignment with human evaluations - particularly in non-English contexts. More broadly, our findings inform the ongoing debate on the universality versus cultural specificity of emotional descriptors, and our methodology can contribute to other multimodal and cross-lingual research.","authors":["Harin Lee","Elif \\c{C}elen","Peter Harrison","Manuel Anglada-Tort","Pol van Rijn","Minsu Park","Marc Sch\\\"onwiesner","Nori Jacoby"],"url":"https://arxiv.org/abs/2505.09539"}
{"created":"2025-05-15","title":"The Silent Scientist: When Software Research Fails to Reach Its Audience","abstract":"If software research were a performance, it would be a thoughtful theater play -- full of rich content but confined to the traditional stage of academic publishing. Meanwhile, its potential audience is immersed in engaging on-demand experiences, leaving the theater half-empty, and the research findings lost in the wings. As long as this remains the case, discussions about research relevance and impact lack meaningful context.","authors":["Marvin Wyrich","Christof Tinnes","Sebastian Baltes","Sven Apel"],"url":"https://arxiv.org/abs/2505.09541"}
{"created":"2025-05-15","title":"Distilling Realizable Students from Unrealizable Teachers","abstract":"We study policy distillation under privileged information, where a student policy with only partial observations must learn from a teacher with full-state access. A key challenge is information asymmetry: the student cannot directly access the teacher's state space, leading to distributional shifts and policy degradation. Existing approaches either modify the teacher to produce realizable but sub-optimal demonstrations or rely on the student to explore missing information independently, both of which are inefficient. Our key insight is that the student should strategically interact with the teacher --querying only when necessary and resetting from recovery states --to stay on a recoverable path within its own observation space. We introduce two methods: (i) an imitation learning approach that adaptively determines when the student should query the teacher for corrections, and (ii) a reinforcement learning approach that selects where to initialize training for efficient exploration. We validate our methods in both simulated and real-world robotic tasks, demonstrating significant improvements over standard teacher-student baselines in training efficiency and final performance. The project website is available at : https://portal-cornell.github.io/CritiQ_ReTRy/","authors":["Yujin Kim","Nathaniel Chin","Arnav Vasudev","Sanjiban Choudhury"],"url":"https://arxiv.org/abs/2505.09546"}
{"created":"2025-05-15","title":"Learning Long-Context Diffusion Policies via Past-Token Prediction","abstract":"Reasoning over long sequences of observations and actions is essential for many robotic tasks. Yet, learning effective long-context policies from demonstrations remains challenging. As context length increases, training becomes increasingly expensive due to rising memory demands, and policy performance often degrades as a result of spurious correlations. Recent methods typically sidestep these issues by truncating context length, discarding historical information that may be critical for subsequent decisions. In this paper, we propose an alternative approach that explicitly regularizes the retention of past information. We first revisit the copycat problem in imitation learning and identify an opposite challenge in recent diffusion policies: rather than over-relying on prior actions, they often fail to capture essential dependencies between past and future actions. To address this, we introduce Past-Token Prediction (PTP), an auxiliary task in which the policy learns to predict past action tokens alongside future ones. This regularization significantly improves temporal modeling in the policy head, with minimal reliance on visual representations. Building on this observation, we further introduce a multistage training strategy: pre-train the visual encoder with short contexts, and fine-tune the policy head using cached long-context embeddings. This strategy preserves the benefits of PTP while greatly reducing memory and computational overhead. Finally, we extend PTP into a self-verification mechanism at test time, enabling the policy to score and select candidates consistent with past actions during inference. Experiments across four real-world and six simulated tasks demonstrate that our proposed method improves the performance of long-context diffusion policies by 3x and accelerates policy training by more than 10x.","authors":["Marcel Torne","Andy Tang","Yuejiang Liu","Chelsea Finn"],"url":"https://arxiv.org/abs/2505.09561"}
{"created":"2025-05-15","title":"Camera-Only 3D Panoptic Scene Completion for Autonomous Driving through Differentiable Object Shapes","abstract":"Autonomous vehicles need a complete map of their surroundings to plan and act. This has sparked research into the tasks of 3D occupancy prediction, 3D scene completion, and 3D panoptic scene completion, which predict a dense map of the ego vehicle's surroundings as a voxel grid. Scene completion extends occupancy prediction by predicting occluded regions of the voxel grid, and panoptic scene completion further extends this task by also distinguishing object instances within the same class; both aspects are crucial for path planning and decision-making. However, 3D panoptic scene completion is currently underexplored. This work introduces a novel framework for 3D panoptic scene completion that extends existing 3D semantic scene completion models. We propose an Object Module and Panoptic Module that can easily be integrated with 3D occupancy and scene completion methods presented in the literature. Our approach leverages the available annotations in occupancy benchmarks, allowing individual object shapes to be learned as a differentiable problem. The code is available at https://github.com/nicolamarinello/OffsetOcc .","authors":["Nicola Marinello","Simen Cassiman","Jonas Heylen","Marc Proesmans","Luc Van Gool"],"url":"https://arxiv.org/abs/2505.09562"}
{"created":"2025-05-15","title":"Using Foundation Models as Pseudo-Label Generators for Pre-Clinical 4D Cardiac CT Segmentation","abstract":"Cardiac image segmentation is an important step in many cardiac image analysis and modeling tasks such as motion tracking or simulations of cardiac mechanics. While deep learning has greatly advanced segmentation in clinical settings, there is limited work on pre-clinical imaging, notably in porcine models, which are often used due to their anatomical and physiological similarity to humans. However, differences between species create a domain shift that complicates direct model transfer from human to pig data.","authors":["Anne-Marie Rickmann","Stephanie L. Thorn","Shawn S. Ahn","Supum Lee","Selen Uman","Taras Lysyy","Rachel Burns","Nicole Guerrera","Francis G. Spinale","Jason A. Burdick","Albert J. Sinusas","James S. Duncan"],"url":"https://arxiv.org/abs/2505.09564"}
{"created":"2025-05-15","title":"BLIP3-o: A Family of Fully Open Unified Multimodal Models-Architecture, Training and Dataset","abstract":"Unifying image understanding and generation has gained growing attention in recent research on multimodal models. Although design choices for image understanding have been extensively studied, the optimal model architecture and training recipe for a unified framework with image generation remain underexplored. Motivated by the strong potential of autoregressive and diffusion models for high-quality generation and scalability, we conduct a comprehensive study of their use in unified multimodal settings, with emphasis on image representations, modeling objectives, and training strategies. Grounded in these investigations, we introduce a novel approach that employs a diffusion transformer to generate semantically rich CLIP image features, in contrast to conventional VAE-based representations. This design yields both higher training efficiency and improved generative quality. Furthermore, we demonstrate that a sequential pretraining strategy for unified models-first training on image understanding and subsequently on image generation-offers practical advantages by preserving image understanding capability while developing strong image generation ability. Finally, we carefully curate a high-quality instruction-tuning dataset BLIP3o-60k for image generation by prompting GPT-4o with a diverse set of captions covering various scenes, objects, human gestures, and more. Building on our innovative model design, training recipe, and datasets, we develop BLIP3-o, a suite of state-of-the-art unified multimodal models. BLIP3-o achieves superior performance across most of the popular benchmarks spanning both image understanding and generation tasks. To facilitate future research, we fully open-source our models, including code, model weights, training scripts, and pretraining and instruction tuning datasets.","authors":["Jiuhai Chen","Zhiyang Xu","Xichen Pan","Yushi Hu","Can Qin","Tom Goldstein","Lifu Huang","Tianyi Zhou","Saining Xie","Silvio Savarese","Le Xue","Caiming Xiong","Ran Xu"],"url":"https://arxiv.org/abs/2505.09568"}
{"created":"2025-05-15","title":"MIGRATION-BENCH: Repository-Level Code Migration Benchmark from Java 8","abstract":"With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on problem-solving and issue-resolution tasks. In contrast, we introduce a new coding benchmark MIGRATION-BENCH with a distinct focus: code migration. MIGRATION-BENCH aims to serve as a comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), MIGRATION-BENCH includes a full dataset and its subset selected with $5,102$ and $300$ repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.00% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience and https://github.com/amazon-science/self_debug respectively.","authors":["Linbo Liu","Xinle Liu","Qiang Zhou","Lin Chen","Yihan Liu","Hoan Nguyen","Behrooz Omidvar-Tehrani","Xi Shen","Jun Huan","Omer Tripp","Anoop Deoras"],"url":"https://arxiv.org/abs/2505.09569"}
{"created":"2025-05-15","title":"Don't Forget your Inverse DDIM for Image Editing","abstract":"The field of text-to-image generation has undergone significant advancements with the introduction of diffusion models. Nevertheless, the challenge of editing real images persists, as most methods are either computationally intensive or produce poor reconstructions. This paper introduces SAGE (Self-Attention Guidance for image Editing) - a novel technique leveraging pre-trained diffusion models for image editing. SAGE builds upon the DDIM algorithm and incorporates a novel guidance mechanism utilizing the self-attention layers of the diffusion U-Net. This mechanism computes a reconstruction objective based on attention maps generated during the inverse DDIM process, enabling efficient reconstruction of unedited regions without the need to precisely reconstruct the entire input image. Thus, SAGE directly addresses the key challenges in image editing. The superiority of SAGE over other methods is demonstrated through quantitative and qualitative evaluations and confirmed by a statistically validated comprehensive user study, in which all 47 surveyed users preferred SAGE over competing methods. Additionally, SAGE ranks as the top-performing method in seven out of 10 quantitative analyses and secures second and third places in the remaining three.","authors":["Guillermo Gomez-Trenado","Pablo Mesejo","Oscar Cord\\'on","St\\'ephane Lathuili\\`ere"],"url":"https://arxiv.org/abs/2505.09571"}
{"created":"2025-05-15","title":"SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures","abstract":"We study gradient flows for loss landscapes of fully connected feed forward neural networks with commonly used continuously differentiable activation functions such as the logistic, hyperbolic tangent, softplus or GELU function. We prove that the gradient flow either converges to a critical point or diverges to infinity while the loss converges to an asymptotic critical value. Moreover, we prove the existence of a threshold $\\varepsilon>0$ such that the loss value of any gradient flow initialized at most $\\varepsilon$ above the optimal level converges to it. For polynomial target functions and sufficiently big architecture and data set, we prove that the optimal loss value is zero and can only be realized asymptotically. From this setting, we deduce our main result that any gradient flow with sufficiently good initialization diverges to infinity. Our proof heavily relies on the geometry of o-minimal structures. We confirm these theoretical findings with numerical experiments and extend our investigation to real-world scenarios, where we observe an analogous behavior.","authors":["Julian Kranz","Davide Gallon","Steffen Dereich","Arnulf Jentzen"],"url":"https://arxiv.org/abs/2505.09572"}
{"created":"2025-05-15","title":"Ethics and Persuasion in Reinforcement Learning from Human Feedback: A Procedural Rhetorical Approach","abstract":"Since 2022, versions of generative AI chatbots such as ChatGPT and Claude have been trained using a specialized technique called Reinforcement Learning from Human Feedback (RLHF) to fine-tune language model output using feedback from human annotators. As a result, the integration of RLHF has greatly enhanced the outputs of these large language models (LLMs) and made the interactions and responses appear more \"human-like\" than those of previous versions using only supervised learning. The increasing convergence of human and machine-written text has potentially severe ethical, sociotechnical, and pedagogical implications relating to transparency, trust, bias, and interpersonal relations. To highlight these implications, this paper presents a rhetorical analysis of some of the central procedures and processes currently being reshaped by RLHF-enhanced generative AI chatbots: upholding language conventions, information seeking practices, and expectations for social relationships. Rhetorical investigations of generative AI and LLMs have, to this point, focused largely on the persuasiveness of the content generated. Using Ian Bogost's concept of procedural rhetoric, this paper shifts the site of rhetorical investigation from content analysis to the underlying mechanisms of persuasion built into RLHF-enhanced LLMs. In doing so, this theoretical investigation opens a new direction for further inquiry in AI ethics that considers how procedures rerouted through AI-driven technologies might reinforce hegemonic language use, perpetuate biases, decontextualize learning, and encroach upon human relationships. It will therefore be of interest to educators, researchers, scholars, and the growing number of users of generative AI chatbots.","authors":["Shannon Lodoen","Alexi Orchard"],"url":"https://arxiv.org/abs/2505.09576"}
{"created":"2025-05-15","title":"VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation","abstract":"While vision-language models have advanced significantly, their application in language-conditioned robotic manipulation is still underexplored, especially for contact-rich tasks that extend beyond visually dominant pick-and-place scenarios. To bridge this gap, we introduce Vision-Tactile-Language-Action model, a novel framework that enables robust policy generation in contact-intensive scenarios by effectively integrating visual and tactile inputs through cross-modal language grounding. A low-cost, multi-modal dataset has been constructed in a simulation environment, containing vision-tactile-action-instruction pairs specifically designed for the fingertip insertion task. Furthermore, we introduce Direct Preference Optimization (DPO) to offer regression-like supervision for the VTLA model, effectively bridging the gap between classification-based next token prediction loss and continuous robotic tasks. Experimental results show that the VTLA model outperforms traditional imitation learning methods (e.g., diffusion policies) and existing multi-modal baselines (TLA/VLA), achieving over 90% success rates on unseen peg shapes. Finally, we conduct real-world peg-in-hole experiments to demonstrate the exceptional Sim2Real performance of the proposed VTLA model. For supplementary videos and results, please visit our project website: https://sites.google.com/view/vtla","authors":["Chaofan Zhang","Peng Hao","Xiaoge Cao","Xiaoshuai Hao","Shaowei Cui","Shuo Wang"],"url":"https://arxiv.org/abs/2505.09577"}
{"created":"2025-05-15","title":"Second-order invariant-domain preserving approximation to the multi-species Euler equations","abstract":"This work is concerned with constructing a second-order, invariant-domain preserving approximation of the compressible multi-species Euler equations where each species is modeled by an ideal gas equation of state. We give the full solution to the Riemann problem and derive its maximum wave speed. The maximum wave speed is used in constructing a first-order invariant-domain preserving approximation. We then extend the methodology to second-order accuracy and detail a convex limiting technique which is used for preserving the invariant domain. Finally, the numerical method is verified with analytical solutions and then validated with several benchmarks and laboratory experiments.","authors":["Bennett Clayton","Tarik Dzanic","Eric J. Tovar"],"url":"https://arxiv.org/abs/2505.09581"}
{"created":"2025-05-15","title":"Beyond Likes: How Normative Feedback Complements Engagement Signals on Social Media","abstract":"Many online platforms incorporate engagement signals--such as likes and upvotes--into their content ranking systems and interface design. These signals are designed to boost user engagement. However, they can unintentionally elevate content that is less inclusive and may not support normatively desirable behavior. This issue becomes especially concerning when toxic content correlates strongly with popularity indicators such as likes and upvotes. In this study, we propose structured prosocial feedback as a complementary signal to likes and upvotes--one that highlights content quality based on normative criteria to help address the limitations of conventional engagement signals. We begin by designing and implementing a machine learning feedback system powered by a large language model (LLM), which evaluates user comments based on principles of positive psychology, such as individual well-being, constructive social media use, and character strengths. We then conduct a pre-registered user study to examine how existing peer-based and the new expert-based feedback interact to shape users' selection of comments in a social media setting. Results show that peer feedback increases conformity to popularity cues, while expert feedback shifts preferences toward normatively higher-quality content. Moreover, incorporating expert feedback alongside peer evaluations improves alignment with expert assessments and contributes to a less toxic community environment. This illustrates the added value of normative cues--such as expert scores generated by LLMs using psychological rubrics--and underscores the potential benefits of incorporating such signals into platform feedback systems to foster healthier online environments.","authors":["Yuchen Wu","Mingduo Zhao","John Canny"],"url":"https://arxiv.org/abs/2505.09583"}
{"created":"2025-05-15","title":"Rhomboid Tiling for Geometric Graph Deep Learning","abstract":"Graph Neural Networks (GNNs) have proven effective for learning from graph-structured data through their neighborhood-based message passing framework. Many hierarchical graph clustering pooling methods modify this framework by introducing clustering-based strategies, enabling the construction of more expressive and powerful models. However, all of these message passing framework heavily rely on the connectivity structure of graphs, limiting their ability to capture the rich geometric features inherent in geometric graphs. To address this, we propose Rhomboid Tiling (RT) clustering, a novel clustering method based on the rhomboid tiling structure, which performs clustering by leveraging the complex geometric information of the data and effectively extracts its higher-order geometric structures. Moreover, we design RTPool, a hierarchical graph clustering pooling model based on RT clustering for graph classification tasks. The proposed model demonstrates superior performance, outperforming 21 state-of-the-art competitors on all the 7 benchmark datasets.","authors":["Yipeng Zhang","Longlong Li","Kelin Xia"],"url":"https://arxiv.org/abs/2505.09586"}
{"created":"2025-05-15","title":"Distance-aware Self-adaptive Graph Convolution for Fine-grained Hierarchical Recommendation","abstract":"Graph Convolutional Networks (GCNs) are widely used to improve recommendation accuracy and performance by effectively learning the representations of user and item nodes. However, two major challenges remain: (1) the lack of further optimization in the graph representation structure and (2) insufficient attention given to the varying contributions of different convolutional layers.This paper proposes SAGCN, a distance-based adaptive hierarchical aggregation method that refines the aggregation process through differentiated representation metrics. SAGCN introduces a detailed approach to multilayer information aggregation and representation space optimization, enabling the model to learn hierarchical embedding weights based on the distance between hierarchical representations. This innovation allows for more precise cross-layer information aggregation, improves the model's ability to capture hierarchical embeddings, and optimizes the representation space structure. Additionally, the objective loss function is refined to better align with recommendation tasks.Extensive experiments conducted on four real-world datasets demonstrate significant improvements, including over a 5% increase on Yelp and a 5.58% increase in Recall@10 on the ML_1M dataset.","authors":["Tao Huang","Yihong Chen","Wei Fan","Wei Zhou","Junhao Wen"],"url":"https://arxiv.org/abs/2505.09590"}
{"created":"2025-05-15","title":"Variational Visual Question Answering","abstract":"Despite remarkable progress in multimodal models for Visual Question Answering (VQA), there remain major reliability concerns because the models can often be overconfident and miscalibrated, especially in out-of-distribution (OOD) settings. Plenty has been done to address such issues for unimodal models, but little work exists for multimodal cases. Here, we address unreliability in multimodal models by proposing a Variational VQA approach. Specifically, instead of fine-tuning vision-language models by using AdamW, we employ a recently proposed variational algorithm called IVON, which yields a posterior distribution over model parameters. Through extensive experiments, we show that our approach improves calibration and abstentions without sacrificing the accuracy of AdamW. For instance, compared to AdamW fine-tuning, we reduce Expected Calibration Error by more than 50% compared to the AdamW baseline and raise Coverage by 4% vs. SOTA (for a fixed risk of 1%). In the presence of distribution shifts, the performance gain is even higher, achieving 8% Coverage (@ 1% risk) improvement vs. SOTA when 50% of test cases are OOD. Overall, we present variational learning as a viable option to enhance the reliability of multimodal models.","authors":["Tobias Jan Wieczorek","Nathalie Daun","Mohammad Emtiyaz Khan","Marcus Rohrbach"],"url":"https://arxiv.org/abs/2505.09591"}
{"created":"2025-05-15","title":"Online Isolation Forest","abstract":"The anomaly detection literature is abundant with offline methods, which require repeated access to data in memory, and impose impractical assumptions when applied to a streaming context. Existing online anomaly detection methods also generally fail to address these constraints, resorting to periodic retraining to adapt to the online context. We propose Online-iForest, a novel method explicitly designed for streaming conditions that seamlessly tracks the data generating process as it evolves over time. Experimental validation on real-world datasets demonstrated that Online-iForest is on par with online alternatives and closely rivals state-of-the-art offline anomaly detection techniques that undergo periodic retraining. Notably, Online-iForest consistently outperforms all competitors in terms of efficiency, making it a promising solution in applications where fast identification of anomalies is of primary importance such as cybersecurity, fraud and fault detection.","authors":["Filippo Leveni","Guilherme Weigert Cassales","Bernhard Pfahringer","Albert Bifet","Giacomo Boracchi"],"url":"https://arxiv.org/abs/2505.09593"}
{"created":"2025-05-15","title":"WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models","abstract":"Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.","authors":["Abdullah Mushtaq","Imran Taj","Rafay Naeem","Ibrahim Ghaznavi","Junaid Qadir"],"url":"https://arxiv.org/abs/2505.09595"}
{"created":"2025-05-15","title":"MDTP -- An Adaptive Multi-Source Data Transfer Protocol","abstract":"Scientific data volume is growing in size, and as a direct result, the need for faster transfers is also increasing. The scientific community has sought to leverage parallel transfer methods using multi-threaded and multi-source download models to reduce download times. In multi-source transfers, a client downloads data from multiple replicated servers in parallel. Tools such as Aria2 and BitTorrent support such multi-source transfers and have shown improved transfer times.","authors":["Sepideh Abdollah","Craig Partridge","Susmit Shannigrahi"],"url":"https://arxiv.org/abs/2505.09597"}
{"created":"2025-05-15","title":"How Hungry is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference","abstract":"As large language models (LLMs) spread across industries, understanding their environmental footprint at the inference level is no longer optional; it is essential. However, most existing studies exclude proprietary models, overlook infrastructural variability and overhead, or focus solely on training, even as inference increasingly dominates AI's environmental impact. To bridge this gap, this paper introduces a novel infrastructure-aware benchmarking framework for quantifying the environmental footprint of LLM inference across 30 state-of-the-art models as deployed in commercial data centers. Our framework combines public API performance data with region-specific environmental multipliers and statistical inference of hardware configurations. We additionally utilize cross-efficiency Data Envelopment Analysis (DEA) to rank models by performance relative to environmental cost. Our results show that o3 and DeepSeek-R1 emerge as the most energy-intensive models, consuming over 33 Wh per long prompt, more than 70 times the consumption of GPT-4.1 nano, and that Claude-3.7 Sonnet ranks highest in eco-efficiency. While a single short GPT-4o query consumes 0.43 Wh, scaling this to 700 million queries/day results in substantial annual environmental impacts. These include electricity use comparable to 35,000 U.S. homes, freshwater evaporation matching the annual drinking needs of 1.2 million people, and carbon emissions requiring a Chicago-sized forest to offset. These findings illustrate a growing paradox: although individual queries are efficient, their global scale drives disproportionate resource consumption. Our study provides a standardized, empirically grounded methodology for benchmarking the sustainability of LLM deployments, laying a foundation for future environmental accountability in AI development and sustainability standards.","authors":["Nidhal Jegham","Marwen Abdelatti","Lassad Elmoubarki","Abdeltawab Hendawi"],"url":"https://arxiv.org/abs/2505.09598"}
{"created":"2025-05-15","title":"Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware","abstract":"Scaling robot learning requires vast and diverse datasets. Yet the prevailing data collection paradigm-human teleoperation-remains costly and constrained by manual effort and physical robot access. We introduce Real2Render2Real (R2R2R), a novel approach for generating robot training data without relying on object dynamics simulation or teleoperation of robot hardware. The input is a smartphone-captured scan of one or more objects and a single video of a human demonstration. R2R2R renders thousands of high visual fidelity robot-agnostic demonstrations by reconstructing detailed 3D object geometry and appearance, and tracking 6-DoF object motion. R2R2R uses 3D Gaussian Splatting (3DGS) to enable flexible asset generation and trajectory synthesis for both rigid and articulated objects, converting these representations to meshes to maintain compatibility with scalable rendering engines like IsaacLab but with collision modeling off. Robot demonstration data generated by R2R2R integrates directly with models that operate on robot proprioceptive states and image observations, such as vision-language-action models (VLA) and imitation learning policies. Physical experiments suggest that models trained on R2R2R data from a single human demonstration can match the performance of models trained on 150 human teleoperation demonstrations. Project page: https://real2render2real.com","authors":["Justin Yu","Letian Fu","Huang Huang","Karim El-Refai","Rares Andrei Ambrus","Richard Cheng","Muhammad Zubair Irshad","Ken Goldberg"],"url":"https://arxiv.org/abs/2505.09601"}
{"created":"2025-05-15","title":"Adversarial Suffix Filtering: a Defense Pipeline for LLMs","abstract":"Large Language Models (LLMs) are increasingly embedded in autonomous systems and public-facing environments, yet they remain susceptible to jailbreak vulnerabilities that may undermine their security and trustworthiness. Adversarial suffixes are considered to be the current state-of-the-art jailbreak, consistently outperforming simpler methods and frequently succeeding even in black-box settings. Existing defenses rely on access to the internal architecture of models limiting diverse deployment, increase memory and computation footprints dramatically, or can be bypassed with simple prompt engineering methods. We introduce $\\textbf{Adversarial Suffix Filtering}$ (ASF), a lightweight novel model-agnostic defensive pipeline designed to protect LLMs against adversarial suffix attacks. ASF functions as an input preprocessor and sanitizer that detects and filters adversarially crafted suffixes in prompts, effectively neutralizing malicious injections. We demonstrate that ASF provides comprehensive defense capabilities across both black-box and white-box attack settings, reducing the attack efficacy of state-of-the-art adversarial suffix generation methods to below 4%, while only minimally affecting the target model's capabilities in non-adversarial scenarios.","authors":["David Khachaturov","Robert Mullins"],"url":"https://arxiv.org/abs/2505.09602"}
{"created":"2025-05-15","title":"DataMIL: Selecting Data for Robot Imitation Learning with Datamodels","abstract":"Recently, the robotics community has amassed ever larger and more diverse datasets to train generalist robot policies. However, while these policies achieve strong mean performance across a variety of tasks, they often underperform on individual, specialized tasks and require further tuning on newly acquired task-specific data. Combining task-specific data with carefully curated subsets of large prior datasets via co-training can produce better specialized policies, but selecting data naively may actually harm downstream performance. To address this, we introduce DataMIL, a policy-driven data selection framework built on the datamodels paradigm that reasons about data selection in an end-to-end manner, using the policy itself to identify which data points will most improve performance. Unlike standard practices that filter data using human notions of quality (e.g., based on semantic or visual similarity), DataMIL directly optimizes data selection for task success, allowing us to select data that enhance the policy while dropping data that degrade it. To avoid performing expensive rollouts in the environment during selection, we use a novel surrogate loss function on task-specific data, allowing us to use DataMIL in the real world without degrading performance. We validate our approach on a suite of more than 60 simulation and real-world manipulation tasks - most notably showing successful data selection from the Open X-Embodiment datasets-demonstrating consistent gains in success rates and superior performance over multiple baselines. Our results underscore the importance of end-to-end, performance-aware data selection for unlocking the potential of large prior datasets in robotics. More information at https://robin-lab.cs.utexas.edu/datamodels4imitation/","authors":["Shivin Dass","Alaa Khaddaj","Logan Engstrom","Aleksander Madry","Andrew Ilyas","Roberto Mart\\'in-Mart\\'in"],"url":"https://arxiv.org/abs/2505.09603"}
{"created":"2025-05-15","title":"The Niche Connectivity Paradox: Multichrome Contagions Overcome Vaccine Hesitancy more effectively than Monochromacy","abstract":"The rise of vaccine hesitancy has caused a resurgence of vaccine-preventable diseases such as measles and pertussis, alongside widespread skepticism and refusals of COVID-19 vaccinations. While categorizing individuals as either supportive of or opposed to vaccines provides a convenient dichotomy of vaccine attitudes, vaccine hesitancy is far more complex and dynamic. It involves wavering individuals whose attitudes fluctuate -- those who may exhibit pro-vaccine attitudes at one time and anti-vaccine attitudes at another. Here, we identify and analyze multichrome contagions as potential targets for intervention by leveraging a dataset of known pro-vax and anti-vax Twitter users ($n =135$ million) and a large COVID-19 Twitter dataset ($n = 3.5$ billion; including close analysis of $1,563,472$ unique individuals). We reconstruct an evolving multiplex sentiment landscape using top co-spreading issues, characterizing them as monochrome and multichrome contagions, based on their conceptual overlap with vaccination. We demonstrate switchers as deliberative: they are more moderate, engage with a wider range of topics, and occupy more central positions in their networks. Further examination of their information consumption shows that their discourse often engages with progressive issues such as climate change, which can serve as avenues for multichrome contagion interventions to promote pro-vaccine attitudes. Using data-driven intervention simulations, we demonstrate a paradox of niche connectivity, where multichrome contagions with fragmented, non-overlapping communities generate the highest levels of diffusion for pro-vaccine attitudes. Our work offers insights into harnessing synergistic hitchhiking effect of multichrome contagions to drive desired attitude and behavior changes in network-based interventions, particularly for overcoming vaccine hesitancy.","authors":["Ho-Chun Herbert Chang","Feng Fu"],"url":"https://arxiv.org/abs/2505.09605"}
{"created":"2025-05-15","title":"LightLab: Controlling Light Sources in Images with Diffusion Models","abstract":"We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for relighting. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.","authors":["Nadav Magar","Amir Hertz","Eric Tabellion","Yael Pritch","Alex Rav-Acha","Ariel Shamir","Yedid Hoshen"],"url":"https://arxiv.org/abs/2505.09608"}
{"created":"2025-05-15","title":"Customizing a Large Language Model for VHDL Design of High-Performance Microprocessors","abstract":"The use of Large Language Models (LLMs) in hardware design has taken off in recent years, principally through its incorporation in tools that increase chip designer productivity. There has been considerable discussion about the use of LLMs in RTL specifications of chip designs, for which the two most popular languages are Verilog and VHDL. LLMs and their use in Verilog design has received significant attention due to the higher popularity of the language, but little attention so far has been given to VHDL despite its continued popularity in the industry. There has also been little discussion about the unique needs of organizations that engage in high-performance processor design, and techniques to deploy AI solutions in these settings. In this paper, we describe our journey in developing a Large Language Model (LLM) specifically for the purpose of explaining VHDL code, a task that has particular importance in an organization with decades of experience and assets in high-performance processor design. We show how we developed test sets specific to our needs and used them for evaluating models as we performed extended pretraining (EPT) of a base LLM. Expert evaluation of the code explanations produced by the EPT model increased to 69% compared to a base model rating of 43%. We further show how we developed an LLM-as-a-judge to gauge models similar to expert evaluators. This led us to deriving and evaluating a host of new models, including an instruction-tuned version of the EPT model with an expected expert evaluator rating of 71%. Our experiments also indicate that with the potential use of newer base models, this rating can be pushed to 85% and beyond. We conclude with a discussion on further improving the quality of hardware design LLMs using exciting new developments in the Generative AI world.","authors":["Nicolas Dupuis","Ravi Nair","Shyam Ramji","Sean McClintock","Nishant Chauhan","Priyanka Nagpal","Bart Blaner","Ken Valk","Leon Stok","Ruchir Puri"],"url":"https://arxiv.org/abs/2505.09610"}
{"created":"2025-05-15","title":"Language Agents Mirror Human Causal Reasoning Biases. How Can We Help Them Think Like Scientists?","abstract":"Language model (LM) agents are increasingly used as autonomous decision-makers who need to actively gather information to guide their decisions. A crucial cognitive skill for such agents is the efficient exploration and understanding of the causal structure of the world -- key to robust, scientifically grounded reasoning. Yet, it remains unclear whether LMs possess this capability or exhibit systematic biases leading to erroneous conclusions. In this work, we examine LMs' ability to explore and infer causal relationships, using the well-established \"Blicket Test\" paradigm from developmental psychology. We find that LMs reliably infer the common, intuitive disjunctive causal relationships but systematically struggle with the unusual, yet equally (or sometimes even more) evidenced conjunctive ones. This \"disjunctive bias\" persists across model families, sizes, and prompting strategies, and performance further declines as task complexity increases. Interestingly, an analogous bias appears in human adults, suggesting that LMs may have inherited deep-seated reasoning heuristics from their training data. To this end, we quantify similarities between LMs and humans, finding that LMs exhibit adult-like inference profiles (but not children-like). Finally, we propose a test-time sampling method which explicitly samples and eliminates hypotheses about causal relationships from the LM. This scalable approach significantly reduces the disjunctive bias and moves LMs closer to the goal of scientific, causally rigorous reasoning.","authors":["Anthony GX-Chen","Dongyan Lin","Mandana Samiei","Doina Precup","Blake A. Richards","Rob Fergus","Kenneth Marino"],"url":"https://arxiv.org/abs/2505.09614"}
{"created":"2025-05-15","title":"UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing","abstract":"Audio-Visual Video Parsing (AVVP) entails the challenging task of localizing both uni-modal events (i.e., those occurring exclusively in either the visual or acoustic modality of a video) and multi-modal events (i.e., those occurring in both modalities concurrently). Moreover, the prohibitive cost of annotating training data with the class labels of all these events, along with their start and end times, imposes constraints on the scalability of AVVP techniques unless they can be trained in a weakly-supervised setting, where only modality-agnostic, video-level labels are available in the training data. To this end, recently proposed approaches seek to generate segment-level pseudo-labels to better guide model training. However, the absence of inter-segment dependencies when generating these pseudo-labels and the general bias towards predicting labels that are absent in a segment limit their performance. This work proposes a novel approach towards overcoming these weaknesses called Uncertainty-weighted Weakly-supervised Audio-visual Video Parsing (UWAV). Additionally, our innovative approach factors in the uncertainty associated with these estimated pseudo-labels and incorporates a feature mixup based training regularization for improved training. Empirical results show that UWAV outperforms state-of-the-art methods for the AVVP task on multiple metrics, across two different datasets, attesting to its effectiveness and generalizability.","authors":["Yung-Hsuan Lai","Janek Ebbers","Yu-Chiang Frank Wang","Fran\\c{c}ois Germain","Michael Jeffrey Jones","Moitreya Chatterjee"],"url":"https://arxiv.org/abs/2505.09615"}
{"created":"2025-05-15","title":"A Retrieval-Augmented Generation Framework for Academic Literature Navigation in Data Science","abstract":"In the rapidly evolving field of data science, efficiently navigating the expansive body of academic literature is crucial for informed decision-making and innovation. This paper presents an enhanced Retrieval-Augmented Generation (RAG) application, an artificial intelligence (AI)-based system designed to assist data scientists in accessing precise and contextually relevant academic resources. The AI-powered application integrates advanced techniques, including the GeneRation Of BIbliographic Data (GROBID) technique for extracting bibliographic information, fine-tuned embedding models, semantic chunking, and an abstract-first retrieval method, to significantly improve the relevance and accuracy of the retrieved information. This implementation of AI specifically addresses the challenge of academic literature navigation. A comprehensive evaluation using the Retrieval-Augmented Generation Assessment System (RAGAS) framework demonstrates substantial improvements in key metrics, particularly Context Relevance, underscoring the system's effectiveness in reducing information overload and enhancing decision-making processes. Our findings highlight the potential of this enhanced Retrieval-Augmented Generation system to transform academic exploration within data science, ultimately advancing the workflow of research and innovation in the field.","authors":["Ahmet Yasin Aytar","Kemal Kilic","Kamer Kaya"],"url":"https://arxiv.org/abs/2412.15404"}
{"created":"2025-05-15","title":"CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs","abstract":"With the rapid development of DNN applications, multi-tenant execution, where multiple DNNs are co-located on a single SoC, is becoming a prevailing trend. Although many methods are proposed in prior works to improve multi-tenant performance, the impact of shared cache is not well studied. This paper proposes CaMDN, an architecture-scheduling co-design to enhance cache efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a lightweight architecture is proposed to support model-exclusive, NPU-controlled regions inside shared cache to eliminate unexpected cache contention. Moreover, a cache scheduling method is proposed to improve shared cache utilization. In particular, it includes a cache-aware mapping method for adaptability to the varying available cache capacity and a dynamic allocation algorithm to adjust the usage among co-located DNNs at runtime. Compared to prior works, CaMDN reduces the memory access by 33.4% on average and achieves a model speedup of up to 2.56$\\times$ (1.88$\\times$ on average).","authors":["Tianhao Cai","Liang Wang","Limin Xiao","Meng Han","Zeyu Wang","Lin Sun","Xiaojian Liao"],"url":"https://arxiv.org/abs/2505.06625"}
{"created":"2025-05-15","title":"Equilibrium Propagation for Learning in Lagrangian Dynamical Systems","abstract":"We propose a method for training dynamical systems governed by Lagrangian mechanics using Equilibrium Propagation. Our approach extends Equilibrium Propagation -- initially developed for energy-based models -- to dynamical trajectories by leveraging the principle of action extremization. Training is achieved by gently nudging trajectories toward desired targets and measuring how the variables conjugate to the parameters to be trained respond. This method is particularly suited to systems with periodic boundary conditions or fixed initial and final states, enabling efficient parameter updates without requiring explicit backpropagation through time. In the case of periodic boundary conditions, this approach yields the semiclassical limit of Quantum Equilibrium Propagation. Applications to systems with dissipation are also discussed.","authors":["Serge Massar"],"url":"https://arxiv.org/abs/2505.07363"}
{"created":"2025-05-15","title":"Reconfiguration of List Colourings","abstract":"Given a proper (list) colouring of a graph $G$, a recolouring step changes the colour at a single vertex to another colour (in its list) that is currently unused on its neighbours, hence maintaining a proper colouring. Suppose that each vertex $v$ has its own private list $L(v)$ of allowed colours such that $|L(v)|\\ge \\mbox{deg}(v)+1$. We prove that if $G$ is connected and its maximum degree $\\Delta$ is at least $3$, then for any two proper $L$-colourings in which at least one vertex can be recoloured, one can be transformed to the other by a sequence of $O(|V(G)|^2)$ recolouring steps. We also show that reducing the list-size of a single vertex $w$ to $\\mbox{deg}(w)$ can lead to situations where the space of proper $L$-colourings is `shattered'. Our results can be interpreted as showing a sharp phase transition in the Glauber dynamics of proper $L$-colourings of graphs. This constitutes a `local' strengthening and generalisation of a result of Feghali, Johnson, and Paulusma, which considered the situation where the lists are all identical to $\\{1,\\ldots,\\Delta+1\\}$.","authors":["Stijn Cambie","Wouter Cames van Batenburg","Daniel W. Cranston","Jan van den Heuvel","Ross J. Kang"],"url":"https://arxiv.org/abs/2505.08020"}
{"created":"2025-05-15","title":"Short Wins Long: Short Codes with Language Model Semantic Correction Outperform Long Codes","abstract":"This paper presents a novel semantic-enhanced decoding scheme for transmitting natural language sentences with multiple short block codes over noisy wireless channels. After ASCII source coding, the natural language sentence message is divided into segments, where each is encoded with short block channel codes independently before transmission. At the receiver, each short block of codewords is decoded in parallel, followed by a semantic error correction (SEC) model to reconstruct corrupted segments semantically. We design and train the SEC model based on Bidirectional and Auto-Regressive Transformers (BART). Simulations demonstrate that the proposed scheme can significantly outperform encoding the sentence with one conventional long LDPC code, in terms of block error rate (BLER), semantic metrics, and decoding latency. Finally, we proposed a semantic hybrid automatic repeat request (HARQ) scheme to further enhance the error performance, which selectively requests retransmission depends on semantic uncertainty.","authors":["Jiafu Hao","Chentao Yue","Hao Chang","Branka Vucetic","Yonghui Li"],"url":"https://arxiv.org/abs/2505.08536"}
{"created":"2025-05-15","title":"How to Incorporate External Fields in Analog Ising Machines","abstract":"Ising machines (IMs) are specialized devices designed to efficiently solve combinatorial optimization problems (COPs). They consist of artificial spins that evolve towards a low-energy configuration representing a problem's solution. Most realistic COPs require both spin-spin couplings and external fields. In IMs with analog spins, these interactions scale differently with the continuous spin amplitudes, leading to imbalances that affect performance. Various techniques have been proposed to mitigate this issue, but their performance has not been benchmarked. We address this gap through a numerical analysis. We evaluate the time-to-solution of these methods across three distinct problem classes with up to 500 spins. Our results show that the most effective way to incorporate external fields is through an approach where the spin interactions are proportional to the spin signs, rather than their continuous amplitudes.","authors":["Robbe De Prins","Jacob Lamers","Peter Bienstman","Guy Van der Sande","Guy Verschaffelt","Thomas Van Vaerenbergh"],"url":"https://arxiv.org/abs/2505.08796"}
{"created":"2025-05-15","title":"In-Context Learning for Label-Efficient Cancer Image Classification in Oncology","abstract":"The application of AI in oncology has been limited by its reliance on large, annotated datasets and the need for retraining models for domain-specific diagnostic tasks. Taking heed of these limitations, we investigated in-context learning as a pragmatic alternative to model retraining by allowing models to adapt to new diagnostic tasks using only a few labeled examples at inference, without the need for retraining. Using four vision-language models (VLMs)-Paligemma, CLIP, ALIGN and GPT-4o, we evaluated the performance across three oncology datasets: MHIST, PatchCamelyon and HAM10000. To the best of our knowledge, this is the first study to compare the performance of multiple VLMs on different oncology classification tasks. Without any parameter updates, all models showed significant gains with few-shot prompting, with GPT-4o reaching an F1 score of 0.81 in binary classification and 0.60 in multi-class classification settings. While these results remain below the ceiling of fully fine-tuned systems, they highlight the potential of ICL to approximate task-specific behavior using only a handful of examples, reflecting how clinicians often reason from prior cases. Notably, open-source models like Paligemma and CLIP demonstrated competitive gains despite their smaller size, suggesting feasibility for deployment in computing constrained clinical environments. Overall, these findings highlight the potential of ICL as a practical solution in oncology, particularly for rare cancers and resource-limited contexts where fine-tuning is infeasible and annotated data is difficult to obtain.","authors":["Mobina Shrestha","Bishwas Mandal","Vishal Mandal","Asis Shrestha"],"url":"https://arxiv.org/abs/2505.08798"}
{"created":"2025-05-15","title":"Thoughts on Objectives of Sparse and Hierarchical Masked Image Model","abstract":"Masked image modeling is one of the most poplular objectives of training. Recently, the SparK model has been proposed with superior performance among self-supervised learning models. This paper proposes a new mask pattern for this SparK model, proposing it as the Mesh Mask-ed SparK model. We report the effect of the mask pattern used for image masking in pre-training on performance.","authors":["Asahi Miyazaki","Tsuyoshi Okita"],"url":"https://arxiv.org/abs/2505.08819"}
{"created":"2025-05-15","title":"A Comparative Study of Transformer-Based Models for Multi-Horizon Blood Glucose Prediction","abstract":"Accurate blood glucose prediction can enable novel interventions for type 1 diabetes treatment, including personalized insulin and dietary adjustments. Although recent advances in transformer-based architectures have demonstrated the power of attention mechanisms in complex multivariate time series prediction, their potential for blood glucose (BG) prediction remains underexplored. We present a comparative analysis of transformer models for multi-horizon BG prediction, examining forecasts up to 4 hours and input history up to 1 week. The publicly available DCLP3 dataset (n=112) was split (80%-10%-10%) for training, validation, and testing, and the OhioT1DM dataset (n=12) served as an external test set. We trained networks with point-wise, patch-wise, series-wise, and hybrid embeddings, using CGM, insulin, and meal data. For short-term blood glucose prediction, Crossformer, a patch-wise transformer architecture, achieved a superior 30-minute prediction of RMSE (15.6 mg / dL on OhioT1DM). For longer-term predictions (1h, 2h, and 4h), PatchTST, another path-wise transformer, prevailed with the lowest RMSE (24.6 mg/dL, 36.1 mg/dL, and 46.5 mg/dL on OhioT1DM). In general, models that used tokenization through patches demonstrated improved accuracy with larger input sizes, with the best results obtained with a one-week history. These findings highlight the promise of transformer-based architectures for BG prediction by capturing and leveraging seasonal patterns in multivariate time-series data to improve accuracy.","authors":["Meryem Altin Karagoz","Marc D. Breton","Anas El Fathi"],"url":"https://arxiv.org/abs/2505.08821"}
{"created":"2025-05-15","title":"Ultrasound Report Generation with Multimodal Large Language Models for Standardized Texts","abstract":"Ultrasound (US) report generation is a challenging task due to the variability of US images, operator dependence, and the need for standardized text. Unlike X-ray and CT, US imaging lacks consistent datasets, making automation difficult. In this study, we propose a unified framework for multi-organ and multilingual US report generation, integrating fragment-based multilingual training and leveraging the standardized nature of US reports. By aligning modular text fragments with diverse imaging data and curating a bilingual English-Chinese dataset, the method achieves consistent and clinically accurate text generation across organ sites and languages. Fine-tuning with selective unfreezing of the vision transformer (ViT) further improves text-image alignment. Compared to the previous state-of-the-art KMVE method, our approach achieves relative gains of about 2\\% in BLEU scores, approximately 3\\% in ROUGE-L, and about 15\\% in CIDEr, while significantly reducing errors such as missing or incorrect content. By unifying multi-organ and multi-language report generation into a single, scalable framework, this work demonstrates strong potential for real-world clinical workflows.","authors":["Peixuan Ge","Tongkun Su","Faqin Lv","Baoliang Zhao","Peng Zhang","Chi Hong Wong","Liang Yao","Yu Sun","Zenan Wang","Pak Kin Wong","Ying Hu"],"url":"https://arxiv.org/abs/2505.08838"}
{"created":"2025-05-15","title":"Total Variation-Based Image Decomposition and Denoising for Microscopy Images","abstract":"Experimentally acquired microscopy images are unavoidably affected by the presence of noise and other unwanted signals, which degrade their quality and might hide relevant features. With the recent increase in image acquisition rate, modern denoising and restoration solutions become necessary. This study focuses on image decomposition and denoising of microscopy images through a workflow based on total variation (TV), addressing images obtained from various microscopy techniques, including atomic force microscopy (AFM), scanning tunneling microscopy (STM), and scanning electron microscopy (SEM). Our approach consists in restoring an image by extracting its unwanted signal components and subtracting them from the raw one, or by denoising it. We evaluate the performance of TV-$L^1$, Huber-ROF, and TGV-$L^1$ in achieving this goal in distinct study cases. Huber-ROF proved to be the most flexible one, while TGV-$L^1$ is the most suitable for denoising. Our results suggest a wider applicability of this method in microscopy, restricted not only to STM, AFM, and SEM images. The Python code used for this study is publicly available as part of AiSurf. It is designed to be integrated into experimental workflows for image acquisition or can be used to denoise previously acquired images.","authors":["Marco Corrias","Giada Franceschi","Michele Riva","Alberto Tampieri","Karin F\\\"ottinger","Ulrike Diebold","Thomas Pock","Cesare Franchini"],"url":"https://arxiv.org/abs/2505.08843"}
{"created":"2025-05-15","title":"CellTypeAgent: Trustworthy cell type annotation with Large Language Models","abstract":"Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis. We present a trustworthy large language model (LLM)-agent, CellTypeAgent, which integrates LLMs with verification from relevant databases. CellTypeAgent achieves higher accuracy than existing methods while mitigating hallucinations. We evaluated CellTypeAgent across nine real datasets involving 303 cell types from 36 tissues. This combined approach holds promise for more efficient and reliable cell type annotation.","authors":["Jiawen Chen","Jianghao Zhang","Huaxiu Yao","Yun Li"],"url":"https://arxiv.org/abs/2505.08844"}
{"created":"2025-05-15","title":"Validation of Conformal Prediction in Cervical Atypia Classification","abstract":"Deep learning based cervical cancer classification can potentially increase access to screening in low-resource regions. However, deep learning models are often overconfident and do not reliably reflect diagnostic uncertainty. Moreover, they are typically optimized to generate maximum-likelihood predictions, which fail to convey uncertainty or ambiguity in their results. Such challenges can be addressed using conformal prediction, a model-agnostic framework for generating prediction sets that contain likely classes for trained deep-learning models. The size of these prediction sets indicates model uncertainty, contracting as model confidence increases. However, existing conformal prediction evaluation primarily focuses on whether the prediction set includes or covers the true class, often overlooking the presence of extraneous classes. We argue that prediction sets should be truthful and valuable to end users, ensuring that the listed likely classes align with human expectations rather than being overly relaxed and including false positives or unlikely classes. In this study, we comprehensively validate conformal prediction sets using expert annotation sets collected from multiple annotators. We evaluate three conformal prediction approaches applied to three deep-learning models trained for cervical atypia classification. Our expert annotation-based analysis reveals that conventional coverage-based evaluations overestimate performance and that current conformal prediction methods often produce prediction sets that are not well aligned with human labels. Additionally, we explore the capabilities of the conformal prediction methods in identifying ambiguous and out-of-distribution data.","authors":["Misgina Tsighe Hagos","Antti Suutala","Dmitrii Bychkov","Hakan K\\\"uc\\\"ukel","Joar von Bahr","Milda Poceviciute","Johan Lundin","Nina Linder","Claes Lundstr\\\"om"],"url":"https://arxiv.org/abs/2505.08845"}
{"created":"2025-05-15","title":"Bounding Neyman-Pearson Region with $f$-Divergences","abstract":"The Neyman-Pearson region of a simple binary hypothesis testing is the set of points whose coordinates represent the false positive rate and false negative rate of some test. The lower boundary of this region is given by the Neyman-Pearson lemma, and is up to a coordinate change, equivalent to the optimal ROC curve. We establish a novel lower bound for the boundary in terms of any $f$-divergence. Since the bound generated by hockey-stick $f$-divergences characterizes the Neyman-Pearson boundary, this bound is best possible. In the case of KL divergence, this bound improves Pinsker's inequality. Furthermore, we obtain a closed-form refined upper bound for the Neyman-Pearson boundary in terms of the Chernoff $\\alpha$-coefficient. Finally, we present methods for constructing pairs of distributions that can approximately or exactly realize any given Neyman-Pearson boundary.","authors":["Andrew Mullhaupt","Cheng Peng"],"url":"https://arxiv.org/abs/2505.08899"}
{"created":"2025-05-15","title":"Statistical Decision Theory with Counterfactual Loss","abstract":"Classical statistical decision theory evaluates treatment choices based solely on observed outcomes. However, by ignoring counterfactual outcomes, it cannot assess the quality of decisions relative to feasible alternatives. For example, the quality of a physician's decision may depend not only on patient survival, but also on whether a less invasive treatment could have produced a similar result. To address this limitation, we extend standard decision theory to incorporate counterfactual losses--criteria that evaluate decisions using all potential outcomes. The central challenge in this generalization is identification: because only one potential outcome is observed for each unit, the associated risk under a counterfactual loss is generally not identifiable. We show that under the assumption of strong ignorability, a counterfactual risk is identifiable if and only if the counterfactual loss function is additive in the potential outcomes. Moreover, we demonstrate that additive counterfactual losses can yield treatment recommendations that differ from those based on standard loss functions, provided that the decision problem involves more than two treatment options.","authors":["Benedikt Koch","Kosuke Imai"],"url":"https://arxiv.org/abs/2505.08908"}
{"created":"2025-05-15","title":"When Recall Fails, Discord Remembers: A Quantum Analogue of Kuhn's Theorem","abstract":"A behavioral quantum strategy is shown to replicate the payoff of a classical mixed strategy in an extensive-form game with imperfect recall, using only local measurements on a separable quantum state with zero entanglement and nonzero discord. Classical behavioral strategies, constrained by imperfect recall, cannot achieve this coordination. The result suggests a quantum analogue to Kuhn's classical equivalence: discord enables behavioral-style strategies to functionally substitute for strategic memory and recover coordination lost in the classical setting. This highlights quantum discord as a minimal and robust resource for extending bounded rationality beyond classical limits.","authors":["Faisal Shah Khan"],"url":"https://arxiv.org/abs/2505.08917"}
{"created":"2025-05-15","title":"When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T primate genomes","abstract":"The emergence of telomere-to-telomere (T2T) genome assemblies has opened new avenues for comparative genomics, yet effective tokenization strategies for genomic sequences remain underexplored. In this pilot study, we apply Byte Pair Encoding (BPE) to nine T2T primate genomes including three human assemblies by training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are shared across all assemblies, while nearly 991,854 tokens are unique to a single genome, indicating a rapid decline in shared vocabulary with increasing assembly comparisons. Moreover, phylogenetic trees derived from token overlap failed to recapitulate established primate relationships, a discrepancy attributed to the disproportionate influence of species-specific high-copy repetitive elements. These findings underscore the dual nature of BPE tokenization: while it effectively compresses repetitive sequences, its sensitivity to high-copy elements limits its utility as a universal tool for comparative genomics. We discuss potential hybrid strategies and repeat-masking approaches to refine genomic tokenization, emphasizing the need for domain-specific adaptations in the development of large-scale genomic language models. The dnaBPE tool used in this study is open-source and available at https://github.com/aglabx/dnaBPE.","authors":["Marina Popova","Iaroslav Chelombitko","Aleksey Komissarov"],"url":"https://arxiv.org/abs/2505.08918"}
{"created":"2025-05-15","title":"Packaging HEP Heterogeneous Mini-apps for Portable Benchmarking and Facility Evaluation on Modern HPCs","abstract":"High Energy Physics (HEP) experiments are making increasing use of GPUs and GPU dominated High Performance Computer facilities. Both the software and hardware of these systems are rapidly evolving, creating challenges for experiments to make informed decisions as to where they wish to devote resources. In its first phase, the High Energy Physics Center for Computational Excellence (HEP-CCE) produced portable versions of a number of heterogeneous HEP mini-apps, such as \\ptor, FastCaloSim, Patatrack and the WireCell Toolkit, that exercise a broad range of GPU characteristics, enabling cross platform and facility benchmarking and evaluation. However, these mini-apps still require a significant amount of manual intervention to deploy on a new facility.","authors":["Mohammad Atif","Pengfei Ding","Ka Hei Martin Kwok","Charles Leggett"],"url":"https://arxiv.org/abs/2505.08933"}
{"created":"2025-05-15","title":"Sensitivity and Hamming graphs","abstract":"For any $m\\geq 3$ we show that the Hamming graph $H(n,m)$ admits an imbalanced partition into $m$ sets, each inducing a subgraph of low maximum degree. This improves previous results by Tandya and by Potechin and Tsang, and disproves the Strong $m$-ary Sensitivity Conjecture of Asensio, Garc\\'ia-Marco, and Knauer. On the other hand, we prove their weaker $m$-ary Sensitivity Conjecture by showing that the sensitivity of any $m$-ary function is bounded from below by a polynomial expression in its degree.","authors":["Sara Asensio","Yuval Filmus","Ignacio Garc\\'ia-Marco","Kolja Knauer"],"url":"https://arxiv.org/abs/2505.08951"}
{"created":"2025-05-15","title":"Geometric lower bounds for the steady-state occupancy of processing networks with limited connectivity","abstract":"We consider processing networks where multiple dispatchers are connected to single-server queues by a bipartite compatibility graph, modeling constraints that are common in data centers and cloud networks due to geographic reasons or data locality issues. We prove lower bounds for the steady-state occupancy, i.e., the complementary cumulative distribution function of the empirical queue length distribution. The lower bounds are geometric with ratios given by two flexibility metrics: the average degree of the dispatchers and a novel metric that averages the minimum degree over the compatible dispatchers across the servers. Using these lower bounds, we establish that the asymptotic performance of a growing processing network cannot match that of the classic Power-of-$d$ or JSQ policies unless the flexibility metrics approach infinity in the large-scale limit.","authors":["Diego Goldsztajn","Andres Ferragut"],"url":"https://arxiv.org/abs/2505.08974"}
{"created":"2025-05-15","title":"Lower Bounds on the MMSE of Adversarially Inferring Sensitive Features","abstract":"We propose an adversarial evaluation framework for sensitive feature inference based on minimum mean-squared error (MMSE) estimation with a finite sample size and linear predictive models. Our approach establishes theoretical lower bounds on the true MMSE of inferring sensitive features from noisy observations of other correlated features. These bounds are expressed in terms of the empirical MMSE under a restricted hypothesis class and a non-negative error term. The error term captures both the estimation error due to finite number of samples and the approximation error from using a restricted hypothesis class. For linear predictive models, we derive closed-form bounds, which are order optimal in terms of the noise variance, on the approximation error for several classes of relationships between the sensitive and non-sensitive features, including linear mappings, binary symmetric channels, and class-conditional multi-variate Gaussian distributions. We also present a new lower bound that relies on the MSE computed on a hold-out validation dataset of the MMSE estimator learned on finite-samples and a restricted hypothesis class. Through empirical evaluation, we demonstrate that our framework serves as an effective tool for MMSE-based adversarial evaluation of sensitive feature inference that balances theoretical guarantees with practical efficiency.","authors":["Monica Welfert","Nathan Stromberg","Mario Diaz","Lalitha Sankar"],"url":"https://arxiv.org/abs/2505.09004"}
{"created":"2025-05-15","title":"Probabilistic Wind Power Forecasting via Non-Stationary Gaussian Processes","abstract":"Accurate probabilistic forecasting of wind power is essential for maintaining grid stability and enabling efficient integration of renewable energy sources. Gaussian Process (GP) models offer a principled framework for quantifying uncertainty; however, conventional approaches rely on stationary kernels, which are inadequate for modeling the inherently non-stationary nature of wind speed and power output. We propose a non-stationary GP framework that incorporates the generalized spectral mixture (GSM) kernel, enabling the model to capture time-varying patterns and heteroscedastic behaviors in wind speed and wind power data. We evaluate the performance of the proposed model on real-world SCADA data across short\\mbox{-,} medium-, and long-term forecasting horizons. Compared to standard radial basis function and spectral mixture kernels, the GSM-based model outperforms, particularly in short-term forecasts. These results highlight the necessity of modeling non-stationarity in wind power forecasting and demonstrate the practical value of non-stationary GP models in operational settings.","authors":["Domniki Ladopoulou","Dat Minh Hong","Petros Dellaportas"],"url":"https://arxiv.org/abs/2505.09026"}
{"created":"2025-05-15","title":"Aging-Aware Battery Control via Convex Optimization","abstract":"We consider the task of controlling a battery while balancing two competing objectives that evolve over different time scales. The short-term objective, such as arbitrage or load smoothing, improves with more battery cycling, while the long-term objective is to maximize battery lifetime, which discourages cycling. Using a semi-empirical aging model, we formulate this problem as a convex optimization problem. We use model predictive control (MPC) with a convex approximation of aging dynamics to optimally manage the trade-off between performance and degradation. Through simulations, we quantify this trade-off in both economic and smoothing applications.","authors":["Obidike Nnorom Jr.","Giray Ogut","Stephen Boyd","Philip Levis"],"url":"https://arxiv.org/abs/2505.09030"}
{"created":"2025-05-15","title":"The Adaptive Complexity of Finding a Stationary Point","abstract":"In large-scale applications, such as machine learning, it is desirable to design non-convex optimization algorithms with a high degree of parallelization. In this work, we study the adaptive complexity of finding a stationary point, which is the minimal number of sequential rounds required to achieve stationarity given polynomially many queries executed in parallel at each round.","authors":["Huanjian Zhou","Andi Han","Akiko Takeda","Masashi Sugiyama"],"url":"https://arxiv.org/abs/2505.09045"}
{"created":"2025-05-15","title":"Solving Reach- and Stabilize-Avoid Problems Using Discounted Reachability","abstract":"In this article, we consider the infinite-horizon reach-avoid (RA) and stabilize-avoid (SA) zero-sum game problems for general nonlinear continuous-time systems, where the goal is to find the set of states that can be controlled to reach or stabilize to a target set, without violating constraints even under the worst-case disturbance. Based on the Hamilton-Jacobi reachability method, we address the RA problem by designing a new Lipschitz continuous RA value function, whose zero sublevel set exactly characterizes the RA set. We establish that the associated Bellman backup operator is contractive and that the RA value function is the unique viscosity solution of a Hamilton-Jacobi variational inequality. Finally, we develop a two-step framework for the SA problem by integrating our RA strategies with a recently proposed Robust Control Lyapunov-Value Function, thereby ensuring both target reachability and long-term stability. We numerically verify our RA and SA frameworks on a 3D Dubins car system to demonstrate the efficacy of the proposed approach.","authors":["Boyang Li","Zheng Gong","Sylvia Herbert"],"url":"https://arxiv.org/abs/2505.09067"}
{"created":"2025-05-15","title":"Risk Bounds For Distributional Regression","abstract":"This work examines risk bounds for nonparametric distributional regression estimators. For convex-constrained distributional regression, general upper bounds are established for the continuous ranked probability score (CRPS) and the worst-case mean squared error (MSE) across the domain. These theoretical results are applied to isotonic and trend filtering distributional regression, yielding convergence rates consistent with those for mean estimation. Furthermore, a general upper bound is derived for distributional regression under non-convex constraints, with a specific application to neural network-based estimators. Comprehensive experiments on both simulated and real data validate the theoretical contributions, demonstrating their practical effectiveness.","authors":["Carlos Misael Madrid Padilla","Oscar Hernan Madrid Padilla","Sabyasachi Chatterjee"],"url":"https://arxiv.org/abs/2505.09075"}
{"created":"2025-05-15","title":"Ornithologist: Towards Trustworthy \"Reasoning\" about Central Bank Communications","abstract":"I develop Ornithologist, a weakly-supervised textual classification system and measure the hawkishness and dovishness of central bank text. Ornithologist uses ``taxonomy-guided reasoning'', guiding a large language model with human-authored decision trees. This increases the transparency and explainability of the system and makes it accessible to non-experts. It also reduces hallucination risk. Since it requires less supervision than traditional classification systems, it can more easily be applied to other problems or sources of text (e.g. news) without much modification. Ornithologist measurements of hawkishness and dovishness of RBA communication carry information about the future of the cash rate path and of market expectations.","authors":["Dominic Zaun Eu Jones"],"url":"https://arxiv.org/abs/2505.09083"}
{"created":"2025-05-15","title":"A Comparative Review of RNA Language Models","abstract":"Given usefulness of protein language models (LMs) in structure and functional inference, RNA LMs have received increased attentions in the last few years. However, these RNA models are often not compared against the same standard. Here, we divided RNA LMs into three classes (pretrained on multiple RNA types (especially noncoding RNAs), specific-purpose RNAs, and LMs that unify RNA with DNA or proteins or both) and compared 13 RNA LMs along with 3 DNA and 1 protein LMs as controls in zero-shot prediction of RNA secondary structure and functional classification. Results shows that the models doing well on secondary structure prediction often perform worse in function classification or vice versa, suggesting that more balanced unsupervised training is needed.","authors":["He Wang","Yikun Zhang","Jie Chen","Jian Zhan","Yaoqi Zhou"],"url":"https://arxiv.org/abs/2505.09087"}
{"created":"2025-05-15","title":"Derivative-free optimization is competitive for aerodynamic design optimization in moderate dimensions","abstract":"Aerodynamic design optimization is an important problem in aircraft design that depends on the interplay between a numerical optimizer and a high-fidelity flow physics solver. Derivative-based, first and (quasi) second order, optimization techniques are the de facto choice, particularly given the availability of the adjoint method and its ability to efficiently compute gradients at the cost of just one solution of the forward problem. However, implementation of the adjoint method requires careful mathematical treatment, and its sensitivity to changes in mesh quality limits widespread applicability. Derivative-free approaches are often overlooked for large scale optimization, citing their lack of scalability in higher dimensions and/or the lack of practical interest in globally optimal solutions that they often target. However, breaking free from an adjoint solver can be paradigm-shifting in broadening the applicability of aerodynamic design optimization. We provide a systematic benchmarking of a select sample of widely used derivative-based and derivative-free optimization algorithms on the design optimization of three canonical aerodynamic bodies, namely, the NACA0012 and RAE2822 airfoils, and the ONERAM6 wing. Our results demonstrate that derivative-free methods are competitive with derivative-based methods, while outperforming them consistently in the high-dimensional setting. These findings highlight the practical competitiveness of modern derivative-free strategies, offering a scalable and robust alternative for aerodynamic design optimization when adjoint-based gradients are unavailable or unreliable.","authors":["Punya Plaban","Peter Bachman","Ashwin Renganathan"],"url":"https://arxiv.org/abs/2505.09088"}
{"created":"2025-05-15","title":"Architecture of Tianyu Software: Relative Photometry as a Case Study","abstract":"Tianyu telescope, an one-meter robotic optical survey instrument to be constructed in Lenghu, Qinghai, China, is designed for detecting transiting exoplanets, variable stars and transients. It requires a highly automated, optimally distributed, easily extendable, and highly flexible software to enable the data processing for the raw data at rates exceeding 500MB/s. In this work, we introduce the architecture of the Tianyu pipeline and use relative photometry as a case to demonstrate its high scalability and efficiency. This pipeline is tested on the data collected from Muguang observatory and Xinglong observatory. The pipeline demonstrates high scalability, with most processing stages increasing in throughput as the number of consumers grows. Compared to a single consumer, the median throughput of image calibration, alignment, and flux extraction increases by 41%, 257%, and 107% respectively when using 5 consumers, while image stacking exhibits limited scalability due to I/O constraints. In our tests, the pipeline was able to detect two transiting sources. Besides, the pipeline captures variability in the light curves of nine known and two previously unknown variable sources in the testing data. Meanwhile, the differential photometric precision of the light curves is near the theoretical limitation. These results indicate that this pipeline is suitable for detecting transiting exoplanets and variable stars. This work builds the fundation for further development of Tianyu software. Code of this work is available at https://github.com/ruiyicheng/Tianyu_pipeline.","authors":["Yicheng Rui","Yifan Xuan","Shuyue Zheng","Kexin Li","Kaiming Cui","Kai Xiao","Jie Zheng","Jun Kai Ng","Hongxuan Jiang","Fabo Feng","Qinghui Sun"],"url":"https://arxiv.org/abs/2505.09107"}
{"created":"2025-05-15","title":"Bridging Theory and Experiment in Materials Discovery: Machine-Learning-Assisted Prediction of Synthesizable Structures","abstract":"Even though thermodynamic energy-based crystal structure prediction (CSP) has revolutionized materials discovery, the energy-driven CSP approaches often struggle to identify experimentally realizable metastable materials synthesized through kinetically controlled pathways, creating a critical gap between theoretical predictions and experimental synthesis. Here, we propose a synthesizability-driven CSP framework that integrates symmetry-guided structure derivation with a Wyckoff encode-based machine-learning model, allowing for the efficient localization of subspaces likely to yield highly synthesizable structures. Within the identified promising subspaces, a structure-based synthesizability evaluation model, fine-tuned using recently synthesized structures to enhance predictive accuracy, is employed in conjunction with ab initio calculations to systematically identify synthesizable candidates. The framework successfully reproduces 13 experimentally known XSe (X = Sc, Ti, Mn, Fe, Ni, Cu, Zn) structures, demonstrating its effectiveness in predicting synthesizable structures. Notably, 92,310 structures are filtered from the 554,054 candidates predicted by GNoME, exhibiting great potential for promising synthesizability. Additionally, eight thermodynamically favorable Hf-X-O (X = Ti, V, and Mn) structures have been identified, among which three HfV$_2$O$_7$ candidates exhibit high synthesizability, presenting viable candidates for experimental realization and potentially associated with experimentally observed temperature-induced phase transitions. This work establishes a data-driven paradigm for machine-learning-assisted inorganic materials synthesis, highlighting its potential to bridge the gap between computational predictions and experimental realization while unlocking new opportunities for the targeted discovery of novel functional materials.","authors":["Yu Xin","Peng Liu","Zhuohang Xie","Wenhui Mi","Pengyue Gao","Hong Jian Zhao","Jian Lv","Yanchao Wang","Yanming Ma"],"url":"https://arxiv.org/abs/2505.09161"}
{"created":"2025-05-15","title":"Online Learning of Neural Networks","abstract":"We study online learning of feedforward neural networks with the sign activation function that implement functions from the unit ball in $\\mathbb{R}^d$ to a finite label set $\\{1, \\ldots, Y\\}$.","authors":["Amit Daniely","Idan Mehalel","Elchanan Mossel"],"url":"https://arxiv.org/abs/2505.09167"}
{"created":"2025-05-15","title":"BiECVC: Gated Diversification of Bidirectional Contexts for Learned Video Compression","abstract":"Recent forward prediction-based learned video compression (LVC) methods have achieved impressive results, even surpassing VVC reference software VTM under the Low Delay B (LDB) configuration. In contrast, learned bidirectional video compression (BVC) remains underexplored and still lags behind its forward-only counterparts. This performance gap is mainly due to the limited ability to extract diverse and accurate contexts: most existing BVCs primarily exploit temporal motion while neglecting non-local correlations across frames. Moreover, they lack the adaptability to dynamically suppress harmful contexts arising from fast motion or occlusion. To tackle these challenges, we propose BiECVC, a BVC framework that incorporates diversified local and non-local context modeling along with adaptive context gating. For local context enhancement, BiECVC reuses high-quality features from lower layers and aligns them using decoded motion vectors without introducing extra motion overhead.To model non-local dependencies efficiently, we adopt a linear attention mechanism that balances performance and complexity. To further mitigate the impact of inaccurate context prediction, we introduce Bidirectional Context Gating, inspired by data-dependent decay in recent autoregressive language models, to dynamically filter contextual information based on conditional coding results. Extensive experiments demonstrate that BiECVC achieves state-of-the-art performance, reducing the bit-rate by 13.4% and 15.7% compared to VTM 13.2 under the Random Access (RA) configuration with intra periods of 32 and 64, respectively. To our knowledge, BiECVC is the first learned video codec to surpass VTM 13.2 RA across all standard test datasets. Code will be available at https://github.com/JiangWeibeta/ECVC.","authors":["Wei Jiang","Junru Li","Kai Zhang","Li Zhang"],"url":"https://arxiv.org/abs/2505.09193"}
{"created":"2025-05-15","title":"InvDesFlow-AL: Active Learning-based Workflow for Inverse Design of Functional Materials","abstract":"Developing inverse design methods for functional materials with specific properties is critical to advancing fields like renewable energy, catalysis, energy storage, and carbon capture. Generative models based on diffusion principles can directly produce new materials that meet performance constraints, thereby significantly accelerating the material design process. However, existing methods for generating and predicting crystal structures often remain limited by low success rates. In this work, we propose a novel inverse material design generative framework called InvDesFlow-AL, which is based on active learning strategies. This framework can iteratively optimize the material generation process to gradually guide it towards desired performance characteristics. In terms of crystal structure prediction, the InvDesFlow-AL model achieves an RMSE of 0.0423 {\\AA}, representing an 32.96% improvement in performance compared to exsisting generative models. Additionally, InvDesFlow-AL has been successfully validated in the design of low-formation-energy and low-Ehull materials. It can systematically generate materials with progressively lower formation energies while continuously expanding the exploration across diverse chemical spaces. These results fully demonstrate the effectiveness of the proposed active learning-driven generative model in accelerating material discovery and inverse design. To further prove the effectiveness of this method, we took the search for BCS superconductors under ambient pressure as an example explored by InvDesFlow-AL. As a result, we successfully identified Li\\(_2\\)AuH\\(_6\\) as a conventional BCS superconductor with an ultra-high transition temperature of 140 K. This discovery provides strong empirical support for the application of inverse design in materials science.","authors":["Xiao-Qi Han","Peng-Jie Guo","Ze-Feng Gao","Hao Sun","Zhong-Yi Lu"],"url":"https://arxiv.org/abs/2505.09203"}
{"created":"2025-05-15","title":"Injectivity of boundary integral operator in direct-indirect mixed Burton-Miller equation for wave scattering problems with transmissive circular inclusion","abstract":"This study proves that the injectivity condition for the integral operator of the direct-indirect mixed Burton-Miller (BM) boundary integral equation (BIE) for Helmholtz transmission problems is identical to that for the ordinary BM BIE for Helmholtz transmission problems with a transmissive circular inclusion. Although some numerical methods based on the direct-indirect mixed BM BIE can be computed faster than the ordinary BM BIE, its well-posedness has been unclear. This study resolves a part of the well-posedness, namely the injectivity of the integral operator with a transmissive circular inclusion.","authors":["Yasuhiro Matsumoto","Kei Matsushima"],"url":"https://arxiv.org/abs/2505.09217"}
{"created":"2025-05-15","title":"Optimal Transport-Based Domain Adaptation for Rotated Linear Regression","abstract":"Optimal Transport (OT) has proven effective for domain adaptation (DA) by aligning distributions across domains with differing statistical properties. Building on the approach of Courty et al. (2016), who mapped source data to the target domain for improved model transfer, we focus on a supervised DA problem involving linear regression models under rotational shifts. This ongoing work considers cases where source and target domains are related by a rotation-common in applications like sensor calibration or image orientation. We show that in $\\mathbb{R}^2$ , when using a p-norm cost with $p $\\ge$ 2$, the optimal transport map recovers the underlying rotation. Based on this, we propose an algorithm that combines K-means clustering, OT, and singular value decomposition (SVD) to estimate the rotation angle and adapt the regression model. This method is particularly effective when the target domain is sparsely sampled, leveraging abundant source data for improved generalization. Our contributions offer both theoretical and practical insights into OT-based model adaptation under geometric transformations.","authors":["Brian Britos (AMU)","Mathias Bourel (UDELAR)"],"url":"https://arxiv.org/abs/2505.09229"}
{"created":"2025-05-15","title":"A Hybrid Quantum-Classical Particle-in-Cell Method for Plasma Simulations","abstract":"We present a hybrid quantum-classical electrostatic Particle-in-Cell (PIC) method, where the electrostatic field Poisson solver is implemented on a quantum computer simulator using a hybrid classical-quantum Neural Network (HNN) using data-driven and physics-informed learning approaches. The HNN is trained on classical PIC simulation results and executed via a PennyLane quantum simulator. The remaining computational steps, including particle motion and field interpolation, are performed on a classical system. To evaluate the accuracy and computational cost of this hybrid approach, we test the hybrid quantum-classical electrostatic PIC against the two-stream instability, a standard benchmark in plasma physics. Our results show that the quantum Poisson solver achieves comparable accuracy to classical methods. It also provides insights into the feasibility of using quantum computing and HNNs for plasma simulations. We also discuss the computational overhead associated with current quantum computer simulators, showing the challenges and potential advantages of hybrid quantum-classical numerical methods.","authors":["Pratibha Raghupati Hegde","Paolo Marcandelli","Yuanchun He","Luca Pennati","Jeremy J. Williams","Ivy Peng","Stefano Markidis"],"url":"https://arxiv.org/abs/2505.09260"}
{"created":"2025-05-15","title":"EDBench: Large-Scale Electron Density Data for Molecular Modeling","abstract":"Existing molecular machine learning force fields (MLFFs) generally focus on the learning of atoms, molecules, and simple quantum chemical properties (such as energy and force), but ignore the importance of electron density (ED) $\\rho(r)$ in accurately understanding molecular force fields (MFFs). ED describes the probability of finding electrons at specific locations around atoms or molecules, which uniquely determines all ground state properties (such as energy, molecular structure, etc.) of interactive multi-particle systems according to the Hohenberg-Kohn theorem. However, the calculation of ED relies on the time-consuming first-principles density functional theory (DFT) which leads to the lack of large-scale ED data and limits its application in MLFFs. In this paper, we introduce EDBench, a large-scale, high-quality dataset of ED designed to advance learning-based research at the electronic scale. Built upon the PCQM4Mv2, EDBench provides accurate ED data, covering 3.3 million molecules. To comprehensively evaluate the ability of models to understand and utilize electronic information, we design a suite of ED-centric benchmark tasks spanning prediction, retrieval, and generation. Our evaluation on several state-of-the-art methods demonstrates that learning from EDBench is not only feasible but also achieves high accuracy. Moreover, we show that learning-based method can efficiently calculate ED with comparable precision while significantly reducing the computational cost relative to traditional DFT calculations. All data and benchmarks from EDBench will be freely available, laying a robust foundation for ED-driven drug discovery and materials science.","authors":["Hongxin Xiang","Ke Li","Mingquan Liu","Zhixiang Cheng","Bin Yao","Wenjie Du","Jun Xia","Li Zeng","Xin Jin","Xiangxiang Zeng"],"url":"https://arxiv.org/abs/2505.09262"}
{"created":"2025-05-15","title":"Enhanced Photonic Chip Design via Interpretable Machine Learning Techniques","abstract":"Photonic chip design has seen significant advancements with the adoption of inverse design methodologies, offering flexibility and efficiency in optimizing device performance. However, the black-box nature of the optimization approaches, such as those used in inverse design in order to minimize a loss function or maximize coupling efficiency, poses challenges in understanding the outputs. This challenge is prevalent in machine learning-based optimization methods, which can suffer from the same lack of transparency. To this end, interpretability techniques address the opacity of optimization models. In this work, we apply interpretability techniques from machine learning, with the aim of gaining understanding of inverse design optimization used in designing photonic components, specifically two-mode multiplexers. We base our methodology on the widespread interpretability technique known as local interpretable model-agnostic explanations, or LIME. As a result, LIME-informed insights point us to more effective initial conditions, directly improving device performance. This demonstrates that interpretability methods can do more than explain models -- they can actively guide and enhance the inverse-designed photonic components. Our results demonstrate the ability of interpretable techniques to reveal underlying patterns in the inverse design process, leading to the development of better-performing components.","authors":["Lirand\\\"e Pira","Airin Antony","Nayanthara Prathap","Daniel Peace","Jacquiline Romero"],"url":"https://arxiv.org/abs/2505.09266"}
{"created":"2025-05-15","title":"Rough sets semantics for the three-valued extension of first-order Priest's da Costa logic","abstract":"We provide a rough sets semantics for the three-valued extension of first-order Priest's da Costa logic, which we studied in a previous paper. The semantics follows the usual pattern of the semantics for first-order classical logic.","authors":["Jos\\'e Luis Castiglioni","Rodolfo C. Ertola-Biraben"],"url":"https://arxiv.org/abs/2505.09302"}
{"created":"2025-05-15","title":"Q-space Guided Collaborative Attention Translation Network for Flexible Diffusion-Weighted Images Synthesis","abstract":"This study, we propose a novel Q-space Guided Collaborative Attention Translation Networks (Q-CATN) for multi-shell, high-angular resolution DWI (MS-HARDI) synthesis from flexible q-space sampling, leveraging the commonly acquired structural MRI data. Q-CATN employs a collaborative attention mechanism to effectively extract complementary information from multiple modalities and dynamically adjust its internal representations based on flexible q-space information, eliminating the need for fixed sampling schemes. Additionally, we introduce a range of task-specific constraints to preserve anatomical fidelity in DWI, enabling Q-CATN to accurately learn the intrinsic relationships between directional DWI signal distributions and q-space. Extensive experiments on the Human Connectome Project (HCP) dataset demonstrate that Q-CATN outperforms existing methods, including 1D-qDL, 2D-qDL, MESC-SD, and QGAN, in estimating parameter maps and fiber tracts both quantitatively and qualitatively, while preserving fine-grained details. Notably, its ability to accommodate flexible q-space sampling highlights its potential as a promising toolkit for clinical and research applications. Our code is available at https://github.com/Idea89560041/Q-CATN.","authors":["Pengli Zhu","Yingji Fu","Nanguang Chen","Anqi Qiu"],"url":"https://arxiv.org/abs/2505.09323"}
{"created":"2025-05-15","title":"Accelerating Machine Learning Systems via Category Theory: Applications to Spherical Attention for Gene Regulatory Networks","abstract":"How do we enable artificial intelligence models to improve themselves? This is central to exponentially improving generalized artificial intelligence models, which can improve their own architecture to handle new problem domains in an efficient manner that leverages the latest hardware. However, current automated compilation methods are poor, and efficient algorithms require years of human development. In this paper, we use neural circuit diagrams, based in category theory, to prove a general theorem related to deep learning algorithms, guide the development of a novel attention algorithm catered to the domain of gene regulatory networks, and produce a corresponding efficient kernel. The algorithm we propose, spherical attention, shows that neural circuit diagrams enable a principled and systematic method for reasoning about deep learning architectures and providing high-performance code. By replacing SoftMax with an $L^2$ norm as suggested by diagrams, it overcomes the special function unit bottleneck of standard attention while retaining the streaming property essential to high-performance. Our diagrammatically derived \\textit{FlashSign} kernel achieves comparable performance to the state-of-the-art, fine-tuned FlashAttention algorithm on an A100, and $3.6\\times$ the performance of PyTorch. Overall, this investigation shows neural circuit diagrams' suitability as a high-level framework for the automated development of efficient, novel artificial intelligence architectures.","authors":["Vincent Abbott","Kotaro Kamiya","Gerard Glowacki","Yu Atsumi","Gioele Zardini","Yoshihiro Maruyama"],"url":"https://arxiv.org/abs/2505.09326"}
{"created":"2025-05-15","title":"Adaptive control for multi-scale stochastic dynamical systems with stochastic next generation reservoir computing","abstract":"The rapid advancement of neuroscience and machine learning has established data-driven stochastic dynamical system modeling as a powerful tool for understanding and controlling high-dimensional, spatio-temporal processes. We introduce the stochastic next-generation reservoir computing (NG-RC) controller, a framework that integrates the computational efficiency of NG-RC with stochastic analysis to enable robust event-triggered control in multiscale stochastic systems. The asymptotic stability of the controller is rigorously proven via an extended stochastic LaSalle theorem, providing theoretical guarantees for amplitude regulation in nonlinear stochastic dynamics. Numerical experiments on a stochastic Van-der-Pol system subject to both additive and multiplicative noise validate the algorithm, demonstrating its convergence rate across varying temporal scales and noise intensities. To bridge theoretical insights with real-world applications, we deploy the controller to modulate pathological dynamics reconstructed from epileptic EEG data. This work advances a theoretically guaranteed scalable framework for adaptive control of stochastic systems, with broad potential for data-driven decision making in engineering, neuroscience, and beyond.","authors":["Jiani Cheng","Ting Gao","Jinqiao Duan"],"url":"https://arxiv.org/abs/2505.09327"}
{"created":"2025-05-15","title":"DCSNet: A Lightweight Knowledge Distillation-Based Model with Explainable AI for Lung Cancer Diagnosis from Histopathological Images","abstract":"Lung cancer is a leading cause of cancer-related deaths globally, where early detection and accurate diagnosis are critical for improving survival rates. While deep learning, particularly convolutional neural networks (CNNs), has revolutionized medical image analysis by detecting subtle patterns indicative of early-stage lung cancer, its adoption faces challenges. These models are often computationally expensive and require significant resources, making them unsuitable for resource constrained environments. Additionally, their lack of transparency hinders trust and broader adoption in sensitive fields like healthcare. Knowledge distillation addresses these challenges by transferring knowledge from large, complex models (teachers) to smaller, lightweight models (students). We propose a knowledge distillation-based approach for lung cancer detection, incorporating explainable AI (XAI) techniques to enhance model transparency. Eight CNNs, including ResNet50, EfficientNetB0, EfficientNetB3, and VGG16, are evaluated as teacher models. We developed and trained a lightweight student model, Distilled Custom Student Network (DCSNet) using ResNet50 as the teacher. This approach not only ensures high diagnostic performance in resource-constrained settings but also addresses transparency concerns, facilitating the adoption of AI-driven diagnostic tools in healthcare.","authors":["Sadman Sakib Alif","Nasim Anzum Promise","Fiaz Al Abid","Aniqua Nusrat Zereen"],"url":"https://arxiv.org/abs/2505.09334"}
{"created":"2025-05-15","title":"ARCANE -- Early Detection of Interplanetary Coronal Mass Ejections","abstract":"Interplanetary coronal mass ejections (ICMEs) are major drivers of space weather disturbances, posing risks to both technological infrastructure and human activities. Automatic detection of ICMEs in solar wind in situ data is essential for early warning systems. While several methods have been proposed to identify these structures in time series data, robust real-time detection remains a significant challenge. In this work, we present ARCANE - the first framework explicitly designed for early ICME detection in streaming solar wind data under realistic operational constraints, enabling event identification without requiring observation of the full structure. Our approach evaluates the strengths and limitations of detection models by comparing a machine learning-based method to a threshold-based baseline. The ResUNet++ model, previously validated on science data, significantly outperforms the baseline, particularly in detecting high-impact events, while retaining solid performance on lower-impact cases. Notably, we find that using real-time solar wind (RTSW) data instead of high-resolution science data leads to only minimal performance degradation. Despite the challenges of operational settings, our detection pipeline achieves an F1 score of 0.53, with an average detection delay of 21.5% of the event's duration while only seeing a minimal amount of data. As more data becomes available, the performance increases significantly. These results mark a substantial step forward in automated space weather monitoring and lay the groundwork for enhanced real-time forecasting capabilities.","authors":["H. T. R\\\"udisser","G. Nguyen","J. Le Lou\\\"edec","C. M\\\"ostl"],"url":"https://arxiv.org/abs/2505.09365"}
{"created":"2025-05-15","title":"TensorRL-QAS: Reinforcement learning with tensor networks for scalable quantum architecture search","abstract":"Variational quantum algorithms hold the promise to address meaningful quantum problems already on noisy intermediate-scale quantum hardware, but they face the challenge of designing quantum circuits that both solve the target problem and comply with device limitations. Quantum architecture search (QAS) automates this design process, with reinforcement learning (RL) emerging as a promising approach. Yet, RL-based QAS methods encounter significant scalability issues, as computational and training costs grow rapidly with the number of qubits, circuit depth, and noise, severely impacting performance. To address these challenges, we introduce $\\textit{TensorRL-QAS}$, a scalable framework that combines tensor network (TN) methods with RL for designing quantum circuits. By warm-starting the architecture search with a matrix product state approximation of the target solution, TensorRL-QAS effectively narrows the search space to physically meaningful circuits, accelerating convergence to the desired solution. Tested on several quantum chemistry problems of up to 12-qubit, TensorRL-QAS achieves up to a 10-fold reduction in CNOT count and circuit depth compared to baseline methods, while maintaining or surpassing chemical accuracy. It reduces function evaluations by up to 100-fold, accelerates training episodes by up to $98\\%$, and achieves up to $50\\%$ success probability for 10-qubit systems-far exceeding the $<1\\%$ rates of baseline approaches. Robustness and versatility are demonstrated both in the noiseless and noisy scenarios, where we report a simulation of up to 8-qubit. These advancements establish TensorRL-QAS as a promising candidate for a scalable and efficient quantum circuit discovery protocol on near-term quantum hardware.","authors":["Akash Kundu","Stefano Mangini"],"url":"https://arxiv.org/abs/2505.09371"}
{"created":"2025-05-15","title":"Quantum-Enhanced Parameter-Efficient Learning for Typhoon Trajectory Forecasting","abstract":"Typhoon trajectory forecasting is essential for disaster preparedness but remains computationally demanding due to the complexity of atmospheric dynamics and the resource requirements of deep learning models. Quantum-Train (QT), a hybrid quantum-classical framework that leverages quantum neural networks (QNNs) to generate trainable parameters exclusively during training, eliminating the need for quantum hardware at inference time. Building on QT's success across multiple domains, including image classification, reinforcement learning, flood prediction, and large language model (LLM) fine-tuning, we introduce Quantum Parameter Adaptation (QPA) for efficient typhoon forecasting model learning. Integrated with an Attention-based Multi-ConvGRU model, QPA enables parameter-efficient training while maintaining predictive accuracy. This work represents the first application of quantum machine learning (QML) to large-scale typhoon trajectory prediction, offering a scalable and energy-efficient approach to climate modeling. Our results demonstrate that QPA significantly reduces the number of trainable parameters while preserving performance, making high-performance forecasting more accessible and sustainable through hybrid quantum-classical learning.","authors":["Chen-Yu Liu","Kuan-Cheng Chen","Yi-Chien Chen","Samuel Yen-Chi Chen","Wei-Hao Huang","Wei-Jia Huang","Yen-Jui Chang"],"url":"https://arxiv.org/abs/2505.09395"}
{"created":"2025-05-15","title":"Independent Component Analysis by Robust Distance Correlation","abstract":"Independent component analysis (ICA) is a powerful tool for decomposing a multivariate signal or distribution into fully independent sources, not just uncorrelated ones. Unfortunately, most approaches to ICA are not robust against outliers. Here we propose a robust ICA method called RICA, which estimates the components by minimizing a robust measure of dependence between multivariate random variables. The dependence measure used is the distance correlation (dCor). In order to make it more robust we first apply a new transformation called the bowl transform, which is bounded, one-to-one, continuous, and maps far outliers to points close to the origin. This preserves the crucial property that a zero dCor implies independence. RICA estimates the independent sources sequentially, by looking for the component that has the smallest dCor with the remainder. RICA is strongly consistent and has the usual parametric rate of convergence. Its robustness is investigated by a simulation study, in which it generally outperforms its competitors. The method is illustrated on three applications, including the well-known cocktail party problem.","authors":["Sarah Leyder","Jakob Raymaekers","Peter J. Rousseeuw","Tom Van Deuren","Tim Verdonck"],"url":"https://arxiv.org/abs/2505.09425"}
{"created":"2025-05-15","title":"Evaluating GPT- and Reasoning-based Large Language Models on Physics Olympiad Problems: Surpassing Human Performance and Implications for Educational Assessment","abstract":"Large language models (LLMs) are now widely accessible, reaching learners at all educational levels. This development has raised concerns that their use may circumvent essential learning processes and compromise the integrity of established assessment formats. In physics education, where problem solving plays a central role in instruction and assessment, it is therefore essential to understand the physics-specific problem-solving capabilities of LLMs. Such understanding is key to informing responsible and pedagogically sound approaches to integrating LLMs into instruction and assessment. This study therefore compares the problem-solving performance of a general-purpose LLM (GPT-4o, using varying prompting techniques) and a reasoning-optimized model (o1-preview) with that of participants of the German Physics Olympiad, based on a set of well-defined Olympiad problems. In addition to evaluating the correctness of the generated solutions, the study analyzes characteristic strengths and limitations of LLM-generated solutions. The findings of this study indicate that both tested LLMs (GPT-4o and o1-preview) demonstrate advanced problem-solving capabilities on Olympiad-type physics problems, on average outperforming the human participants. Prompting techniques had little effect on GPT-4o's performance, while o1-preview almost consistently outperformed both GPT-4o and the human benchmark. Based on these findings, the study discusses implications for the design of summative and formative assessment in physics education, including how to uphold assessment integrity and support students in critically engaging with LLMs.","authors":["Paul Tschisgale","Holger Maus","Fabian Kieser","Ben Kroehs","Stefan Petersen","Peter Wulff"],"url":"https://arxiv.org/abs/2505.09438"}
{"created":"2025-05-15","title":"Omni-R1: Do You Really Need Audio to Fine-Tune Your Audio LLM?","abstract":"We propose Omni-R1 which fine-tunes a recent multi-modal LLM, Qwen2.5-Omni, on an audio question answering dataset with the reinforcement learning method GRPO. This leads to new State-of-the-Art performance on the recent MMAU benchmark. Omni-R1 achieves the highest accuracies on the sounds, music, speech, and overall average categories, both on the Test-mini and Test-full splits. To understand the performance improvement, we tested models both with and without audio and found that much of the performance improvement from GRPO could be attributed to better text-based reasoning. We also made a surprising discovery that fine-tuning without audio on a text-only dataset was effective at improving the audio-based performance.","authors":["Andrew Rouditchenko","Saurabhchand Bhati","Edson Araujo","Samuel Thomas","Hilde Kuehne","Rogerio Feris","James Glass"],"url":"https://arxiv.org/abs/2505.09439"}
{"created":"2025-05-15","title":"Quantum state-agnostic work extraction (almost) without dissipation","abstract":"We investigate work extraction protocols designed to transfer the maximum possible energy to a battery using sequential access to $N$ copies of an unknown pure qubit state. The core challenge is designing interactions to optimally balance two competing goals: charging of the battery optimally using the qubit in hand, and acquiring more information by qubit to improve energy harvesting in subsequent rounds. Here, we leverage exploration-exploitation trade-off in reinforcement learning to develop adaptive strategies achieving energy dissipation that scales only poly-logarithmically in $N$. This represents an exponential improvement over current protocols based on full state tomography.","authors":["Josep Lumbreras","Ruo Cheng Huang","Yanglin Hu","Mile Gu","Marco Tomamichel"],"url":"https://arxiv.org/abs/2505.09456"}
{"created":"2025-05-15","title":"Fairness-aware Bayes optimal functional classification","abstract":"Algorithmic fairness has become a central topic in machine learning, and mitigating disparities across different subpopulations has emerged as a rapidly growing research area. In this paper, we systematically study the classification of functional data under fairness constraints, ensuring the disparity level of the classifier is controlled below a pre-specified threshold. We propose a unified framework for fairness-aware functional classification, tackling an infinite-dimensional functional space, addressing key challenges from the absence of density ratios and intractability of posterior probabilities, and discussing unique phenomena in functional classification. We further design a post-processing algorithm, Fair Functional Linear Discriminant Analysis classifier (Fair-FLDA), which targets at homoscedastic Gaussian processes and achieves fairness via group-wise thresholding. Under weak structural assumptions on eigenspace, theoretical guarantees on fairness and excess risk controls are established. As a byproduct, our results cover the excess risk control of the standard FLDA as a special case, which, to the best of our knowledge, is first time seen. Our theoretical findings are complemented by extensive numerical experiments on synthetic and real datasets, highlighting the practicality of our designed algorithm.","authors":["Xiaoyu Hu","Gengyu Xue","Zhenhua Lin","Yi Yu"],"url":"https://arxiv.org/abs/2505.09471"}
{"created":"2025-05-15","title":"Reinforcement Learning for Individual Optimal Policy from Heterogeneous Data","abstract":"Offline reinforcement learning (RL) aims to find optimal policies in dynamic environments in order to maximize the expected total rewards by leveraging pre-collected data. Learning from heterogeneous data is one of the fundamental challenges in offline RL. Traditional methods focus on learning an optimal policy for all individuals with pre-collected data from a single episode or homogeneous batch episodes, and thus, may result in a suboptimal policy for a heterogeneous population. In this paper, we propose an individualized offline policy optimization framework for heterogeneous time-stationary Markov decision processes (MDPs). The proposed heterogeneous model with individual latent variables enables us to efficiently estimate the individual Q-functions, and our Penalized Pessimistic Personalized Policy Learning (P4L) algorithm guarantees a fast rate on the average regret under a weak partial coverage assumption on behavior policies. In addition, our simulation studies and a real data application demonstrate the superior numerical performance of the proposed method compared with existing methods.","authors":["Rui Miao","Babak Shahbaba","Annie Qu"],"url":"https://arxiv.org/abs/2505.09496"}
{"created":"2025-05-15","title":"Deep-SITAR: A SITAR-Based Deep Learning Framework for Growth Curve Modeling via Autoencoders","abstract":"Several approaches have been developed to capture the complexity and nonlinearity of human growth. One widely used is the Super Imposition by Translation and Rotation (SITAR) model, which has become popular in studies of adolescent growth. SITAR is a shape-invariant mixed-effects model that represents the shared growth pattern of a population using a natural cubic spline mean curve while incorporating three subject-specific random effects -- timing, size, and growth intensity -- to account for variations among individuals. In this work, we introduce a supervised deep learning framework based on an autoencoder architecture that integrates a deep neural network (neural network) with a B-spline model to estimate the SITAR model. In this approach, the encoder estimates the random effects for each individual, while the decoder performs a fitting based on B-splines similar to the classic SITAR model. We refer to this method as the Deep-SITAR model. This innovative approach enables the prediction of the random effects of new individuals entering a population without requiring a full model re-estimation. As a result, Deep-SITAR offers a powerful approach to predicting growth trajectories, combining the flexibility and efficiency of deep learning with the interpretability of traditional mixed-effects models.","authors":["Mar\\'ia Alejandra Hern\\'andez","Oscar Rodriguez","Dae-Jin Lee"],"url":"https://arxiv.org/abs/2505.09506"}
{"created":"2025-05-15","title":"Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios","abstract":"Cluster analysis, or clustering, plays a crucial role across numerous scientific and engineering domains. Despite the wealth of clustering methods proposed over the past decades, each method is typically designed for specific scenarios and presents certain limitations in practical applications. In this paper, we propose depth-based local center clustering (DLCC). This novel method makes use of data depth, which is known to produce a center-outward ordering of sample points in a multivariate space. However, data depth typically fails to capture the multimodal characteristics of {data}, something of the utmost importance in the context of clustering. To overcome this, DLCC makes use of a local version of data depth that is based on subsets of {data}. From this, local centers can be identified as well as clusters of varying shapes. Furthermore, we propose a new internal metric based on density-based clustering to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a flexible clustering approach that seems to overcome some limitations of traditional clustering methods, thereby enhancing data analysis capabilities across a wide range of application scenarios.","authors":["Siyi Wang","Alexandre Leblanc","Paul D. McNicholas"],"url":"https://arxiv.org/abs/2505.09516"}
{"created":"2025-05-15","title":"Spec2VolCAMU-Net: A Spectrogram-to-Volume Model for EEG-to-fMRI Reconstruction based on Multi-directional Time-Frequency Convolutional Attention Encoder and Vision-Mamba U-Net","abstract":"High-resolution functional magnetic resonance imaging (fMRI) is essential for mapping human brain activity; however, it remains costly and logistically challenging. If comparable volumes could be generated directly from widely available scalp electroencephalography (EEG), advanced neuroimaging would become significantly more accessible. Existing EEG-to-fMRI generators rely on plain CNNs that fail to capture cross-channel time-frequency cues or on heavy transformer/GAN decoders that strain memory and stability. We propose Spec2VolCAMU-Net, a lightweight spectrogram-to-volume generator that confronts these issues via a Multi-directional Time-Frequency Convolutional Attention Encoder, stacking temporal, spectral and joint convolutions with self-attention, and a Vision-Mamba U-Net decoder whose linear-time state-space blocks enable efficient long-range spatial modelling. Trained end-to-end with a hybrid SSI-MSE loss, Spec2VolCAMU-Net achieves state-of-the-art fidelity on three public benchmarks, recording SSIMs of 0.693 on NODDI, 0.725 on Oddball and 0.788 on CN-EPFL, representing improvements of 14.5%, 14.9%, and 16.9% respectively over previous best SSIM scores. Furthermore, it achieves competitive PSNR scores, particularly excelling on the CN-EPFL dataset with a 4.6% improvement over the previous best PSNR, thus striking a better balance in reconstruction quality. The proposed model is lightweight and efficient, making it suitable for real-time applications in clinical and research settings. The code is available at https://github.com/hdy6438/Spec2VolCAMU-Net.","authors":["Dongyi He","Shiyang Li","Bin Jiang","He Yan"],"url":"https://arxiv.org/abs/2505.09521"}
{"created":"2025-05-15","title":"Scalable Computations for Generalized Mixed Effects Models with Crossed Random Effects Using Krylov Subspace Methods","abstract":"Mixed effects models are widely used for modeling data with hierarchically grouped structures and high-cardinality categorical predictor variables. However, for high-dimensional crossed random effects, current standard computations relying on Cholesky decompositions can become prohibitively slow. In this work, we present novel Krylov subspace-based methods that address several existing computational bottlenecks. Among other things, we theoretically analyze and empirically evaluate various preconditioners for the conjugate gradient and stochastic Lanczos quadrature methods, derive new convergence results, and develop computationally efficient methods for calculating predictive variances. Extensive experiments using simulated and real-world data sets show that our proposed methods scale much better than Cholesky-based computations, for instance, achieving a runtime reduction of approximately two orders of magnitudes for both estimation and prediction. Moreover, our software implementation is up to 10'000 times faster and more stable than state-of-the-art implementations such as lme4 and glmmTMB when using default settings. Our methods are implemented in the free C++ software library GPBoost with high-level Python and R packages.","authors":["Pascal K\\\"undig","Fabio Sigrist"],"url":"https://arxiv.org/abs/2505.09552"}
{"created":"2025-05-15","title":"WavReward: Spoken Dialogue Models With Generalist Reward Evaluators","abstract":"End-to-end spoken dialogue models such as GPT-4o-audio have recently garnered significant attention in the speech domain. However, the evaluation of spoken dialogue models' conversational performance has largely been overlooked. This is primarily due to the intelligent chatbots convey a wealth of non-textual information which cannot be easily measured using text-based language models like ChatGPT. To address this gap, we propose WavReward, a reward feedback model based on audio language models that can evaluate both the IQ and EQ of spoken dialogue systems with speech input. Specifically, 1) based on audio language models, WavReward incorporates the deep reasoning process and the nonlinear reward mechanism for post-training. By utilizing multi-sample feedback via the reinforcement learning algorithm, we construct a specialized evaluator tailored to spoken dialogue models. 2) We introduce ChatReward-30K, a preference dataset used to train WavReward. ChatReward-30K includes both comprehension and generation aspects of spoken dialogue models. These scenarios span various tasks, such as text-based chats, nine acoustic attributes of instruction chats, and implicit chats. WavReward outperforms previous state-of-the-art evaluation models across multiple spoken dialogue scenarios, achieving a substantial improvement about Qwen2.5-Omni in objective accuracy from 55.1$\\%$ to 91.5$\\%$. In subjective A/B testing, WavReward also leads by a margin of 83$\\%$. Comprehensive ablation studies confirm the necessity of each component of WavReward. All data and code will be publicly at https://github.com/jishengpeng/WavReward after the paper is accepted.","authors":["Shengpeng Ji","Tianle Liang","Yangzhuo Li","Jialong Zuo","Minghui Fang","Jinzheng He","Yifu Chen","Zhengqing Liu","Ziyue Jiang","Xize Cheng","Siqi Zheng","Jin Xu","Junyang Lin","Zhou Zhao"],"url":"https://arxiv.org/abs/2505.09558"}
{"created":"2025-05-15","title":"Improved Sample Upper and Lower Bounds for Trace Estimation of Quantum State Powers","abstract":"As often emerges in various basic quantum properties such as entropy, the trace of quantum state powers $\\operatorname{tr}(\\rho^q)$ has attracted a lot of attention. The recent work of Liu and Wang (SODA 2025) showed that $\\operatorname{tr}(\\rho^q)$ can be estimated to within additive error $\\varepsilon$ with a dimension-independent sample complexity of $\\widetilde O(1/\\varepsilon^{3+\\frac{2}{q-1}})$ for any constant $q > 1$, where only an $\\Omega(1/\\varepsilon)$ lower bound was given. In this paper, we significantly improve the sample complexity of estimating $\\operatorname{tr}(\\rho^q)$ in both the upper and lower bounds. In particular:","authors":["Kean Chen","Qisheng Wang"],"url":"https://arxiv.org/abs/2505.09563"}
{"created":"2025-05-15","title":"Meta-learning Slice-to-Volume Reconstruction in Fetal Brain MRI using Implicit Neural Representations","abstract":"High-resolution slice-to-volume reconstruction (SVR) from multiple motion-corrupted low-resolution 2D slices constitutes a critical step in image-based diagnostics of moving subjects, such as fetal brain Magnetic Resonance Imaging (MRI). Existing solutions struggle with image artifacts and severe subject motion or require slice pre-alignment to achieve satisfying reconstruction performance. We propose a novel SVR method to enable fast and accurate MRI reconstruction even in cases of severe image and motion corruption. Our approach performs motion correction, outlier handling, and super-resolution reconstruction with all operations being entirely based on implicit neural representations. The model can be initialized with task-specific priors through fully self-supervised meta-learning on either simulated or real-world data. In extensive experiments including over 480 reconstructions of simulated and clinical MRI brain data from different centers, we prove the utility of our method in cases of severe subject motion and image artifacts. Our results demonstrate improvements in reconstruction quality, especially in the presence of severe motion, compared to state-of-the-art methods, and up to 50% reduction in reconstruction time.","authors":["Maik Dannecker","Thomas Sanchez","Meritxell Bach Cuadra","\\\"Ozg\\\"un Turgut","Anthony N. Price","Lucilio Cordero-Grande","Vanessa Kyriakopoulou","Joseph V. Hajnal","Daniel Rueckert"],"url":"https://arxiv.org/abs/2505.09565"}
{"created":"2025-05-15","title":"Adaptively-weighted Nearest Neighbors for Matrix Completion","abstract":"In this technical note, we introduce and analyze AWNN: an adaptively weighted nearest neighbor method for performing matrix completion. Nearest neighbor (NN) methods are widely used in missing data problems across multiple disciplines such as in recommender systems and for performing counterfactual inference in panel data settings. Prior works have shown that in addition to being very intuitive and easy to implement, NN methods enjoy nice theoretical guarantees. However, the performance of majority of the NN methods rely on the appropriate choice of the radii and the weights assigned to each member in the nearest neighbor set and despite several works on nearest neighbor methods in the past two decades, there does not exist a systematic approach of choosing the radii and the weights without relying on methods like cross-validation. AWNN addresses this challenge by judiciously balancing the bias variance trade off inherent in weighted nearest-neighbor regression. We provide theoretical guarantees for the proposed method under minimal assumptions and support the theory via synthetic experiments.","authors":["Tathagata Sadhukhan","Manit Paul","Raaz Dwivedi"],"url":"https://arxiv.org/abs/2505.09612"}
{"created":"2025-05-15","title":"Correct-by-construction requirement decomposition","abstract":"In systems engineering, accurately decomposing requirements is crucial for creating well-defined and manageable system components, particularly in safety-critical domains. Despite the critical need, rigorous, top-down methodologies for effectively breaking down complex requirements into precise, actionable sub-requirements are scarce, especially compared to the wealth of bottom-up verification techniques. Addressing this gap, we introduce a formal decomposition for contract-based design that guarantees the correctness of decomposed requirements if specific conditions are met. Our (semi-)automated methodology augments contract-based design with reachability analysis and constraint programming to systematically identify, verify, and validate sub-requirements representable by continuous bounded sets -- continuous relations between real-valued inputs and outputs. We demonstrate the efficacy and practicality of a correct-by-construction approach through a comprehensive case study on a cruise control system, highlighting how our methodology improves the interpretability, tractability, and verifiability of system requirements.","authors":["Minghui Sun","Georgios Bakirtzis","Hassan Jafarzadeh","Cody Fleming"],"url":"https://arxiv.org/abs/1909.02070"}
{"created":"2025-05-15","title":"Evacuation decisions in response to natural disasters: Insights from a large-scale social media survey","abstract":"Evacuation in response to natural disasters is a complex process involving multiple decision-makers at the personal, household, community, and government levels. Consequently, many disparate factors influence who evacuates, when, and how to respond to a nearby disaster. In this paper, we leverage a novel method of data collection through social media to explore the evacuation response decisions of people in areas affected by the 2019-2020 Australian bushfires. We explore the validity of this data collection method for generating plausible estimates of evacuation and its ability to supplement cell phone location data using survey responses. Ultimately, we identify several key factors influencing household decisions on evacuation, specifically focusing on the phenomenon of household members evacuating or returning from evacuation at different times.","authors":["Paige Maas","Zack Almquist","Eugenia Giraudy","JW Schneider"],"url":"https://arxiv.org/abs/2008.03665"}
{"created":"2025-05-15","title":"Synchronization of strongly connected partial DFAs and prefix codes","abstract":"We study synchronizing partial DFAs, which extend the classical concept of synchronizing complete DFAs and are a special case of synchronizing unambiguous NFAs. A partial DFA is called synchronizing if it has a word (called a \\emph{reset word}) whose action brings a non-empty subset of states to a unique state and is undefined for all other states. The class of strongly connected partial automata is precisely the class of automata recognized prefix codes. While in the general case the problem of checking whether a partial DFA is synchronizing is PSPACE-complete, we show that in the strongly connected case this problem can be efficiently reduced to the same problem for a complete DFA. Using combinatorial, algebraic, and formal languages methods, we develop techniques that relate main synchronization problems for strongly connected partial DFAs to the same problems for complete DFAs. In particular, this includes the \\v{C}ern\\'{y} and the rank conjectures, the problem of finding a reset word, and upper bounds on the length of the shortest reset words of literal automata of finite prefix codes. We conclude that solving fundamental synchronization problems is equally hard in both models, as an essential improvement of the results for one model implies an improvement for the other.","authors":["Mikhail V. Berlinkov","Robert Ferens","Andrew Ryzhikov","Marek Szyku{\\l}a"],"url":"https://arxiv.org/abs/2101.05057"}
{"created":"2025-05-15","title":"Manifesto for Putting 'Chartjunk' in the Trash 2021!","abstract":"In this provocation we ask the visualization research community to join us in removing chartjunk from our research lexicon. We present an etymology of chartjunk, framing its provocative origins as misaligned, and harmful, to the ways the term is currently used by visualization researchers. We call on the community to dissolve chartjunk from the ways we talk about, write about, and think about the graphical devices we design and study. As a step towards this goal we contribute a performance of maintenance through a trio of acts: editing the Wikipedia page on chartjunk, cutting out chartjunk from IEEE papers, and scanning and posting a repository of the pages with chartjunk removed to invite the community to re-imagine how we describe visualizations. This contribution blurs the boundaries between research, activism, and maintenance art, and is intended to inspire the community to join us in taking out the trash.","authors":["Derya Akbaba","Jack Wilburn","Main T. Nance","Miriah Meyer"],"url":"https://arxiv.org/abs/2109.10132"}
{"created":"2025-05-15","title":"Guaranteed Rejection-free Sampling Method Using Past Behaviours for Motion Planning of Autonomous Systems","abstract":"The paper presents a novel learning-based sampling strategy that guarantees rejection-free sampling of the free space under both biased and approximately uniform conditions, leveraging multivariate kernel densities. Historical data from a given autonomous system is leveraged to estimate a non-parametric probabilistic description of the domain, which also describes the free space where feasible solutions of the motion planning problem are likely to be found. The tuning parameters of the kernel density estimator, the bandwidth and the kernel, are used to alter the description of the free space so that no samples can fall outside the originally defined space.The proposed method is demonstrated in two real-life case studies: An autonomous surface vessel (2D) and an autonomous drone (3D). Two planning problems are solved, showing that the proposed approximately uniform sampling scheme is capable of guaranteeing rejection-free samples of the considered workspace. Furthermore, the effectiveness of the proposed method is statistically validated using Monte Carlo simulations.","authors":["Thomas T. Enevoldsen","Roberto Galeazzi"],"url":"https://arxiv.org/abs/2109.14687"}
{"created":"2025-05-15","title":"Design and Operation of Shared Machine Learning Clusters on Campus","abstract":"Amid the rapid advancements in large machine learning (ML) models, universities worldwide are investing substantial funds and efforts into GPU clusters. However, managing a shared GPU cluster poses a pyramid of challenges, from hardware configuration to resource allocation among users.","authors":["Kaiqiang Xu","Decang Sun","Hao Wang","Zhenghang Ren","Xinchen Wan","Xudong Liao","Zilong Wang","Junxue Zhang","Kai Chen"],"url":"https://arxiv.org/abs/2110.01556"}
{"created":"2025-05-15","title":"Learning to Be Cautious","abstract":"A key challenge in the field of reinforcement learning is to develop agents that behave cautiously in novel situations. It is generally impossible to anticipate all situations that an autonomous system may face or what behavior would best avoid bad outcomes. An agent that can learn to be cautious would overcome this challenge by discovering for itself when and how to behave cautiously. In contrast, current approaches typically embed task-specific safety information or explicit cautious behaviors into the system, which is error-prone and imposes extra burdens on practitioners. In this paper, we present both a sequence of tasks where cautious behavior becomes increasingly non-obvious, as well as an algorithm to demonstrate that it is possible for a system to learn to be cautious. The essential features of our algorithm are that it characterizes reward function uncertainty without task-specific safety information and uses this uncertainty to construct a robust policy. Specifically, we construct robust policies with a k-of-N counterfactual regret minimization (CFR) subroutine given learned reward function uncertainty represented by a neural network ensemble. These policies exhibit caution in each of our tasks without any task-specific safety tuning.","authors":["Montaser Mohammedalamen","Dustin Morrill","Alexander Sieusahai","Yash Satsangi","Michael Bowling"],"url":"https://arxiv.org/abs/2110.15907"}
{"created":"2025-05-15","title":"Lower Bounds for Subset Sum in Resolution with Modular Counting","abstract":"In this paper we prove lower bounds for sizes of refutations of unsatisfiable vector Subset Sum instances $\\overrightarrow{a}_1 x_1 + \\dots + \\overrightarrow{a}_n x_n = \\overrightarrow{b}$ in the proof system Res(lin$_{\\mathbb{F}_q}$) where $char(\\mathbb{F}_{q})\\geq 5$. As a basis for the hardness criterion for such instances we choose the property of the matrix $A$ with columns $(\\overrightarrow{a}_1, \\ldots, \\overrightarrow{a}_n)$ to be (the transpose of) the generating matrix for a good error-correcting code $C_{A} := \\{x\\cdot A\\, |\\, x \\in \\mathbb{F}_{q}^k\\}\\subset \\mathbb{F}_{q}^n$ and prove the following lower bounds:","authors":["Fedor Part"],"url":"https://arxiv.org/abs/2202.08214"}
{"created":"2025-05-15","title":"DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects","abstract":"Research in manipulation of deformable objects is typically conducted on a limited range of scenarios, because handling each scenario on hardware takes significant effort. Realistic simulators with support for various types of deformations and interactions have the potential to speed up experimentation with novel tasks and algorithms. However, for highly deformable objects it is challenging to align the output of a simulator with the behavior of real objects. Manual tuning is not intuitive, hence automated methods are needed. We view this alignment problem as a joint perception-inference challenge and demonstrate how to use recent neural network architectures to successfully perform simulation parameter inference from real point clouds. We analyze the performance of various architectures, comparing their data and training requirements. Furthermore, we propose to leverage differentiable point cloud sampling and differentiable simulation to significantly reduce the time to achieve the alignment. We employ an efficient way to propagate gradients from point clouds to simulated meshes and further through to the physical simulation parameters, such as mass and stiffness. Experiments with highly deformable objects show that our method can achieve comparable or better alignment with real object behavior, while reducing the time needed to achieve this by more than an order of magnitude. Videos and supplementary material are available at https://diffcloud.github.io.","authors":["Priya Sundaresan","Rika Antonova","Jeannette Bohg"],"url":"https://arxiv.org/abs/2204.03139"}
{"created":"2025-05-15","title":"3D Cartoon Face Generation with Controllable Expressions from a Single GAN Image","abstract":"In this paper, we investigate an open research task of generating 3D cartoon face shapes from single 2D GAN generated human faces and without 3D supervision, where we can also manipulate the facial expressions of the 3D shapes. To this end, we discover the semantic meanings of StyleGAN latent space, such that we are able to produce face images of various expressions, poses, and lighting conditions by controlling the latent codes. Specifically, we first finetune the pretrained StyleGAN face model on the cartoon datasets. By feeding the same latent codes to face and cartoon generation models, we aim to realize the translation from 2D human face images to cartoon styled avatars. We then discover semantic directions of the GAN latent space, in an attempt to change the facial expressions while preserving the original identity. As we do not have any 3D annotations for cartoon faces, we manipulate the latent codes to generate images with different poses and lighting conditions, such that we can reconstruct the 3D cartoon face shapes. We validate the efficacy of our method on three cartoon datasets qualitatively and quantitatively.","authors":["Hao Wang","Wenhao Shen","Guosheng Lin","Steven C. H. Hoi","Chunyan Miao"],"url":"https://arxiv.org/abs/2207.14425"}
{"created":"2025-05-15","title":"EiHi Net: Out-of-Distribution Generalization Paradigm","abstract":"This paper develops a new EiHi net to solve the out-of-distribution (OoD) generalization problem in deep learning. EiHi net is a model learning paradigm that can be blessed on any visual backbone. This paradigm can change the previous learning method of the deep model, namely find out correlations between inductive sample features and corresponding categories, which suffers from pseudo correlations between indecisive features and labels. We fuse SimCLR and VIC-Reg via explicitly and dynamically establishing the original - positive - negative sample pair as a minimal learning element, the deep model iteratively establishes a relationship close to the causal one between features and labels, while suppressing pseudo correlations. To further validate the proposed model, and strengthen the established causal relationships, we develop a human-in-the-loop strategy, with few guidance samples, to prune the representation space directly. Finally, it is shown that the developed EiHi net makes significant improvements in the most difficult and typical OoD dataset Nico, compared with the current SOTA results, without any domain ($e.g.$ background, irrelevant features) information.","authors":["Qinglai Wei","Beiming Yuan","Diancheng Chen"],"url":"https://arxiv.org/abs/2209.14946"}
{"created":"2025-05-15","title":"Detecting Misuse of Security APIs: A Systematic Review","abstract":"Security Application Programming Interfaces (APIs) are crucial for ensuring software security. However, their misuse introduces vulnerabilities, potentially leading to severe data breaches and substantial financial loss. Complex API design, inadequate documentation, and insufficient security training often lead to unintentional misuse by developers. The software security community has devised and evaluated several approaches to detecting security API misuse to help developers and organizations. This study rigorously reviews the literature on detecting misuse of security APIs to gain a comprehensive understanding of this critical domain. Our goal is to identify and analyze security API misuses, the detection approaches developed, and the evaluation methodologies employed along with the open research avenues to advance the state-of-the-art in this area. Employing the systematic literature review (SLR) methodology, we analyzed 69 research papers. Our review has yielded (a) identification of 6 security API types; (b) classification of 30 distinct misuses; (c) categorization of detection techniques into heuristic-based and ML-based approaches; and (d) identification of 10 performance measures and 9 evaluation benchmarks. The review reveals a lack of coverage of detection approaches in several areas. We recommend that future efforts focus on aligning security API development with developers' needs and advancing standardized evaluation methods for detection technologies.","authors":["Zahra Mousavi","Chadni Islam","M. Ali Babar","Alsharif Abuadbba","Kristen Moore"],"url":"https://arxiv.org/abs/2306.08869"}
{"created":"2025-05-15","title":"Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases","abstract":"The necessity to manage inconsistency in Description Logics Knowledge Bases (KBs) has come to the fore with the increasing importance gained by the Semantic Web, where information comes from different sources that constantly change their content and may contain contradictory descriptions when considered either alone or together. Classical reasoning algorithms do not handle inconsistent KBs, forcing the debugging of the KB in order to remove the inconsistency. In this paper, we exploit an existing probabilistic semantics called DISPONTE to overcome this problem and allow queries also in case of inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE and empirically tested the validity of our proposal. Moreover, we formally compare the presented approach to that of the repair semantics, one of the most established semantics when considering DL reasoning tasks.","authors":["Riccardo Zese","Evelina Lamma","Fabrizio Riguzzi"],"url":"https://arxiv.org/abs/2306.09138"}
{"created":"2025-05-15","title":"Local Differential Privacy-Preserving Spectral Clustering for General Graphs","abstract":"Spectral clustering is a widely used algorithm to find clusters in networks. Several researchers have studied the stability of spectral clustering under local differential privacy with the additional assumption that the underlying networks are generated from the stochastic block model (SBM). However, we argue that this assumption is too restrictive since social networks do not originate from the SBM. Thus, we delve into an analysis for general graphs in this work. Our primary focus is the edge flipping method -- a common technique for protecting local differential privacy. We show that, when the edges of an $n$-vertex graph satisfying some reasonable well-clustering assumptions are flipped with a probability of $O(\\log n/n)$, the clustering outcomes are largely consistent. Empirical tests further corroborate these theoretical findings. Conversely, although clustering outcomes have been stable for non-sparse and well-clustered graphs produced from the SBM, we show that in general, spectral clustering may yield highly erratic results on certain well-clustered graphs when the flipping probability is $\\omega(\\log n/n)$. This indicates that the best privacy budget obtainable for general graphs is $\\Theta(\\log n)$.","authors":["Sayan Mukherjee","Vorapong Suppakitpaisarn"],"url":"https://arxiv.org/abs/2309.06867"}
{"created":"2025-05-15","title":"Multi-Path Bound for DAG Tasks","abstract":"This paper studies the response time bound of a DAG (directed acyclic graph) task. Recently, the idea of using multiple paths to bound the response time of a DAG task, instead of using a single longest path in previous results, was proposed and leads to the so-called multi-path bound. Multi-path bounds can greatly reduce the response time bound and significantly improve the schedulability of DAG tasks. This paper derives a new multi-path bound and proposes an optimal algorithm to compute this bound. We further present a systematic analysis on the dominance and the sustainability of three existing multi-path bounds and the proposed multi-path bound. Our bound theoretically dominates and empirically outperforms all existing multi-path bounds. What's more, the proposed bound is the only multi-path bound that is proved to be self-sustainable.","authors":["Qingqiang He","Nan Guan","Shuai Zhao","Mingsong Lv"],"url":"https://arxiv.org/abs/2310.15471"}
{"created":"2025-05-15","title":"A Call to Arms: AI Should be Critical for Social Media Analysis of Conflict Zones","abstract":"The massive proliferation of social media data represents a transformative opportunity for conflict studies and for tracking the proliferation and use of weaponry, as conflicts are increasingly documented in these online spaces. At the same time, the scale and types of data available are problematic for traditional open-source intelligence. This paper focuses on identifying specific weapon systems and the insignias of the armed groups using them as documented in the Ukraine war, as these tasks are critical to operational intelligence and tracking weapon proliferation, especially given the scale of international military aid given to Ukraine. The large scale of social media makes manual assessment difficult, however, so this paper presents early work that uses computer vision models to support this task. We demonstrate that these models can both identify weapons embedded in images shared in social media and how the resulting collection of military-relevant images and their post times interact with the offline, real-world conflict. Not only can we then track changes in the prevalence of images of tanks, land mines, military trucks, etc., we find correlations among time series data associated with these images and the daily fatalities in this conflict. This work shows substantial opportunity for examining similar online documentation of conflict contexts, and we also point to future avenues where computer vision can be further improved for these open-source intelligence tasks.","authors":["Afia Abedin","Abdul Bais","Cody Buntain","Laura Courchesne","Brian McQuinn","Matthew E. Taylor","Muhib Ullah"],"url":"https://arxiv.org/abs/2311.00810"}
{"created":"2025-05-15","title":"A Quick Primer on Machine Learning in Wireless Communications","abstract":"This is our final issue of the quick primer on the use of Python to build a wireless communications prototype. This prototype simulates multiple-input and multiple-output (MIMO) systems for a single orthogonal frequency division multiplexing (OFDM) symbol. Further, it shows several artificial intelligence (AI) and machine learning (ML) use cases and introduces the deepwireless library for code implementation. The intent of this primer is to empower the reader with the means to efficiently create reproducible simulations related to AI and ML in wireless communications on inexpensive computing devices. This primer has sprung from a draft aligned with the syllabus of a graduate course (EESC 7v86) -- which we created to be first taught in Fall 2022 -- and has since evolved to where it stands today.","authors":["Faris B. Mismar"],"url":"https://arxiv.org/abs/2312.17713"}
{"created":"2025-05-15","title":"Efficient approximation of Earth Mover's Distance Based on Nearest Neighbor Search","abstract":"Earth Mover's Distance (EMD) is an important similarity measure between two distributions, used in computer vision and many other application domains. However, its exact calculation is computationally and memory intensive, which hinders its scalability and applicability for large-scale problems. Various approximate EMD algorithms have been proposed to reduce computational costs, but they suffer lower accuracy and may require additional memory usage or manual parameter tuning. In this paper, we present a novel approach, NNS-EMD, to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve high accuracy, low time complexity, and high memory efficiency. The NNS operation reduces the number of data points compared in each NNS iteration and offers opportunities for parallel processing. We further accelerate NNS-EMD via vectorization on GPU, which is especially beneficial for large datasets. We compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD algorithms on image classification and retrieval tasks. We also apply NNS-EMD to calculate transport mapping and realize color transfer between images. NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and achieves superior accuracy, speedup, and memory efficiency over existing approximate EMD methods.","authors":["Guangyu Meng","Ruyu Zhou","Liu Liu","Peixian Liang","Fang Liu","Danny Chen","Michael Niemier","X. Sharon Hu"],"url":"https://arxiv.org/abs/2401.07378"}
{"created":"2025-05-15","title":"Detecting Multimedia Generated by Large AI Models: A Survey","abstract":"The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.","authors":["Li Lin","Neeraj Gupta","Yue Zhang","Hainan Ren","Chun-Hao Liu","Feng Ding","Xin Wang","Xin Li","Luisa Verdoliva","Shu Hu"],"url":"https://arxiv.org/abs/2402.00045"}
{"created":"2025-05-15","title":"LLM-based NLG Evaluation: Current Status and Challenges","abstract":"Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. Lastly, we discuss several open problems in this area and point out future research directions.","authors":["Mingqi Gao","Xinyu Hu","Jie Ruan","Xiao Pu","Xiaojun Wan"],"url":"https://arxiv.org/abs/2402.01383"}
{"created":"2025-05-15","title":"TREET: TRansfer Entropy Estimation via Transformers","abstract":"Transfer entropy (TE) is an information theoretic measure that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel attention-based approach for estimating TE for stationary processes. The proposed approach employs Donsker-Varadhan representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods on a dedicated estimation benchmark. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma, and use it to estimate the capacity of communication channels with memory, which is a canonical optimization problem in information theory. We further demonstrate how an optimized TREET can be used to estimate underlying densities, providing experimental results. Finally, we apply TREET to feature analysis of patients with Apnea, demonstrating its applicability to real-world physiological data. Our work, applied with state-of-the-art deep learning methods, opens a new door for communication problems which are yet to be solved.","authors":["Omer Luxembourg","Dor Tsur","Haim Permuter"],"url":"https://arxiv.org/abs/2402.06919"}
{"created":"2025-05-15","title":"F$^3$Loc: Fusion and Filtering for Floorplan Localization","abstract":"In this paper we propose an efficient data-driven solution to self-localization within a floorplan. Floorplan data is readily available, long-term persistent and inherently robust to changes in the visual appearance. Our method does not require retraining per map and location or demand a large database of images of the area of interest. We propose a novel probabilistic model consisting of an observation and a novel temporal filtering module. Operating internally with an efficient ray-based representation, the observation module consists of a single and a multiview module to predict horizontal depth from images and fuses their results to benefit from advantages offered by either methodology. Our method operates on conventional consumer hardware and overcomes a common limitation of competing methods that often demand upright images. Our full system meets real-time requirements, while outperforming the state-of-the-art by a significant margin.","authors":["Changan Chen","Rui Wang","Christoph Vogel","Marc Pollefeys"],"url":"https://arxiv.org/abs/2403.03370"}
{"created":"2025-05-15","title":"Edge-Disjoint Spanning Trees on Star-Product Networks","abstract":"A star-product operation may be used to create large graphs from smaller factor graphs. Network topologies based on star-products demonstrate several advantages including low-diameter, high scalability, modularity and others. Many state-of-the-art diameter-2 and -3 topologies~(Slim Fly, Bundlefly, PolarStar etc.) can be represented as star products.","authors":["Kelly Isham","Laura Monroe","Kartik Lakhotia","Aleyah Dawkins","Daniel Hwang","Ales Kubicek"],"url":"https://arxiv.org/abs/2403.12231"}
{"created":"2025-05-15","title":"Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model","abstract":"Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges to the efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs.","authors":["Yanpeng Ye","Jie Ren","Shaozhou Wang","Yuwei Wan","Imran Razzak","Bram Hoex","Haofen Wang","Tong Xie","Wenjie Zhang"],"url":"https://arxiv.org/abs/2404.03080"}
{"created":"2025-05-15","title":"How Do OSS Developers Reuse Architectural Solutions from Q&A Sites: An Empirical Study","abstract":"Developers reuse programming-related knowledge on Q&amp;A sites that functionally matches the programming problems they encounter in their development. Despite extensive research on Q&amp;A sites, being a high-level and important type of development-related knowledge, architectural solutions and their reuse are rarely explored. To fill this gap, we conducted a mixed-methods study that includes a mining study and a survey study. For the mining study, we mined 984 commits and issues from 893 OSS projects on GitHub that explicitly referenced architectural solutions from SO and SWESE. For the survey study, we identified practitioners involved in the reuse of these architectural solutions and surveyed 227 of them to further understand how practitioners reuse architectural solutions from Q&amp;A sites in their OSS development. Our findings: (1) OSS practitioners use architectural solutions from Q&amp;A sites to solve a large variety of architectural problems, wherein Component design issue, Architectural anti-pattern, and Security issue are dominant; (2) Seven categories of architectural solutions from Q&amp;A sites have been reused to solve those problems, among which Architectural refactoring, Use of frameworks, and Architectural tactic are the three most reused architectural solutions; (3) OSS developers often rely on ad hoc ways (e.g., informal, improvised, or unstructured approaches) to incorporate architectural solutions from SO, drawing on personal experience and intuition rather than standardized or systematic practices; (4) Using architectural solutions from SO comes with a variety of challenges, e.g., OSS practitioners complain that they need to spend significant time to adapt such architectural solutions to address design concerns raised in their OSS development, and it is challenging to use architectural solutions that are not tailored to the design context of their OSS projects.","authors":["Musengamana Jean de Dieu","Peng Liang","Mojtaba Shahin"],"url":"https://arxiv.org/abs/2404.05041"}
{"created":"2025-05-15","title":"DACAD: Domain Adaptation Contrastive Learning for Anomaly Detection in Multivariate Time Series","abstract":"In time series anomaly detection (TSAD), the scarcity of labeled data poses a challenge to the development of accurate models. Unsupervised domain adaptation (UDA) offers a solution by leveraging labeled data from a related domain to detect anomalies in an unlabeled target domain. However, existing UDA methods assume consistent anomalous classes across domains. To address this limitation, we propose a novel Domain Adaptation Contrastive learning model for Anomaly Detection in multivariate time series (DACAD), combining UDA with contrastive learning. DACAD utilizes an anomaly injection mechanism that enhances generalization across unseen anomalous classes, improving adaptability and robustness. Additionally, our model employs supervised contrastive loss for the source domain and self-supervised contrastive triplet loss for the target domain, ensuring comprehensive feature representation learning and domain-invariant feature extraction. Finally, an effective Center-based Entropy Classifier (CEC) accurately learns normal boundaries in the source domain. Extensive evaluations on multiple real-world datasets and a synthetic dataset highlight DACAD's superior performance in transferring knowledge across domains and mitigating the challenge of limited labeled data in TSAD.","authors":["Zahra Zamanzadeh Darban","Yiyuan Yang","Geoffrey I. Webb","Charu C. Aggarwal","Qingsong Wen","Shirui Pan","Mahsa Salehi"],"url":"https://arxiv.org/abs/2404.11269"}
{"created":"2025-05-15","title":"Teaching Divisibility and Binomials with Coq","abstract":"The goal of this contribution is to provide worksheets in Coq for students to learn about divisibility and binomials. These basic topics are a good case study as they are widely taught in the early academic years (or before in France). We present here our technical and pedagogical choices, the numerous exercises we developed and a small experiment we conducted on two students. As expected, it required additional Coq material such as other lemmas and dedicated tactics. The worksheets are freely available and flexible in several ways.","authors":["Sylvie Boldo (TOCCATA)","Fran\\c{c}ois Cl\\'ement (SERENA","CERMICS)","David Hamelin (TOCCATA)","Micaela Mayero (LIPN","TOCCATA)","Pierre Rousselin (LAGA","SERENA","CERMICS)"],"url":"https://arxiv.org/abs/2404.12676"}
{"created":"2025-05-15","title":"Systematic interval observer design for linear systems","abstract":"We first develop systematic and comprehensive interval observer designs for linear time-invariant (LTI) systems, under standard assumptions of observability and interval bounds on the initial condition and uncertainties. Traditionally, such designs rely on specific transformations into Metzler (in continuous time) or non-negative (in discrete time) forms, which may impose limitations. We demonstrate that these can be effectively replaced by an LTI transformation that is straightforward to compute offline. Subsequently, we extend the framework to time-varying systems, overcoming the limitations of conventional approaches that offer no guarantees. Our method utilizes dynamic transformations into higher-dimensional target systems, for which interval observers can always be constructed. These transformations become left-invertible after a finite time, provided the system is observable and the target dynamics are sufficiently high-dimensional and fast, thereby enabling the recovery of interval bounds in the original coordinates. Academic examples are provided to illustrate the proposed methodology.","authors":["Thach Ngoc Dinh","Gia Quoc Bao Tran"],"url":"https://arxiv.org/abs/2405.06445"}
{"created":"2025-05-15","title":"Coded Downlink Massive Random Access and a Finite de Finetti Theorem","abstract":"This paper considers a massive connectivity setting in which a base-station (BS) aims to communicate sources $(X_1,\\cdots,X_k)$ to a randomly activated subset of $k$ users, among a large pool of $n$ users, via a common message in the downlink. Although the identities of the $k$ active users are assumed to be known at the BS, each active user only knows whether itself is active and does not know the identities of the other active users. A naive coding strategy is to transmit the sources alongside the identities of the users for which the source information is intended. This requires $H(X_1,\\cdots,X_k) + k\\log(n)$ bits, because the cost of specifying the identity of one out of $n$ users is $\\log(n)$ bits. For large $n$, this overhead can be significant. This paper shows that it is possible to develop coding techniques that eliminate the dependency of the overhead on $n$, if the source distribution follows certain symmetry. Specifically, if the source distribution is independently and identically distributed (i.i.d.) then the overhead can be reduced to at most $O(\\log(k))$ bits, and in case of uniform i.i.d. sources, the overhead can be further reduced to $O(1)$ bits. For sources that follow a more general exchangeable distribution, the overhead is at most $O(k)$ bits, and in case of finite-alphabet exchangeable sources, the overhead can be further reduced to $O(\\log(k))$ bits. The downlink massive random access problem is closely connected to the study of finite exchangeable sequences. The proposed coding strategy allows bounds on the Kullback-Leibler (KL) divergence between finite exchangeable distributions and i.i.d. mixture distributions to be developed and gives a new KL divergence version of the finite de Finetti theorem, which is scaling optimal.","authors":["Ryan Song","Kareem M. Attiah","Wei Yu"],"url":"https://arxiv.org/abs/2405.08301"}
{"created":"2025-05-15","title":"Enumerating Graphlets with Amortized Time Complexity Independent of Graph Size","abstract":"Graphlets of order $k$ in a graph $G$ are connected subgraphs induced by $k$ nodes (called $k$-graphlets) or by $k$ edges (called edge $k$-graphlets). They are among the interesting subgraphs in network analysis to get insights on both the local and global structure of a network. While several algorithms exist for discovering and enumerating graphlets, the cost per solution of such algorithms typically depends on the size of the graph $G$, or its maximum degree. In real networks, even the latter can be in the order of millions, whereas $k$ is typically required to be a small value. In this paper we provide the first algorithm to list all graphlets of order $k$ in a graph $G=(V,E)$ with an amortized cost per solution depending \\emph{solely} on the order $k$, contrarily to previous approaches where the cost depends \\emph{also} on the size of $G$ or its maximum degree. Specifically, we show that it is possible to list $k$-graphlets in $O(k^2)$ time per solution, and to list edge $k$-graphlets in $O(k)$ time per solution. Furthermore we show that, if the input graph has bounded degree, then the cost per solution for listing $k$-graphlets is reduced to $O(k)$. Whenever $k = O(1)$, as it is often the case in practical settings, these algorithms are the first to achieve constant time per solution.","authors":["Alessio Conte","Roberto Grossi","Yasuaki Kobayashi","Kazuhiro Kurita","Davide Rucci","Takeaki Uno","Kunihiro Wasa"],"url":"https://arxiv.org/abs/2405.13613"}
{"created":"2025-05-15","title":"A General Graph Spectral Wavelet Convolution via Chebyshev Order Decomposition","abstract":"Spectral graph convolution, an important tool of data filtering on graphs, relies on two essential decisions: selecting spectral bases for signal transformation and parameterizing the kernel for frequency analysis. While recent techniques mainly focus on standard Fourier transform and vector-valued spectral functions, they fall short in flexibility to model signal distributions over large spatial ranges, and capacity of spectral function. In this paper, we present a novel wavelet-based graph convolution network, namely WaveGC, which integrates multi-resolution spectral bases and a matrix-valued filter kernel. Theoretically, we establish that WaveGC can effectively capture and decouple short-range and long-range information, providing superior filtering flexibility, surpassing existing graph wavelet neural networks. To instantiate WaveGC, we introduce a novel technique for learning general graph wavelets by separately combining odd and even terms of Chebyshev polynomials. This approach strictly satisfies wavelet admissibility criteria. Our numerical experiments showcase the consistent improvements in both short-range and long-range tasks. This underscores the effectiveness of the proposed model in handling different scenarios. Our code is available at https://github.com/liun-online/WaveGC.","authors":["Nian Liu","Xiaoxin He","Thomas Laurent","Francesco Di Giovanni","Michael M. Bronstein","Xavier Bresson"],"url":"https://arxiv.org/abs/2405.13806"}
{"created":"2025-05-15","title":"Piecewise rational rotation-minimizing motions via data stream interpolation","abstract":"When a moving frame defined along a space curve is required to keep an axis aligned with the tangent direction of motion, the use of rotation-minimizing frames (RMF) avoids unnecessary rotations in the normal plane. The construction of rigid body motions using a specific subset of quintic curves with rational RMFs (RRMFs) is here considered. In particular, a novel geometric characterization of such subset enables the design of a local algorithm to interpolate an assigned stream of positions, together with an initial frame orientation. To achieve this, the translational part of the motion is described by a parametric $G^1$ spline curve whose segments are quintic RRMFs, with a globally continuous piecewise rational rotation-minimizing frame. A selection of numerical experiments illustrates the performances of the proposed method on synthetic and arbitrary data streams.","authors":["Carlotta Giannelli","Lorenzo Sacco","Alessandra Sestini","Zbyn\\v{e}k \\v{S}\\'ir"],"url":"https://arxiv.org/abs/2405.14229"}
{"created":"2025-05-15","title":"Cognitive Insights and Stable Coalition Matching for Fostering Multi-Agent Cooperation","abstract":"Cognitive abilities, such as Theory of Mind (ToM), play a vital role in facilitating cooperation in human social interactions. However, our study reveals that agents with higher ToM abilities may not necessarily exhibit better cooperative behavior compared to those with lower ToM abilities. To address this challenge, we propose a novel matching coalition mechanism that leverages the strengths of agents with different ToM levels by explicitly considering belief alignment and specialized abilities when forming coalitions. Our proposed matching algorithm seeks to find stable coalitions that maximize the potential for cooperative behavior and ensure long-term viability. By incorporating cognitive insights into the design of multi-agent systems, our work demonstrates the potential of leveraging ToM to create more sophisticated and human-like coordination strategies that foster cooperation and improve overall system performance.","authors":["Jiaqi Shao","Tianjun Yuan","Tao Lin","Bing Luo"],"url":"https://arxiv.org/abs/2405.18044"}
{"created":"2025-05-15","title":"Comparing Quantum Annealing and Spiking Neuromorphic Computing for Sampling Binary Sparse Coding QUBO Problems","abstract":"We consider the problem of computing a sparse binary representation of an image. To be precise, given an image and an overcomplete, non-orthonormal basis, we aim to find a sparse binary vector indicating the minimal set of basis vectors that when added together best reconstruct the given input. We formulate this problem with an $L_2$ loss on the reconstruction error, and an $L_0$ (or, equivalently, an $L_1$) loss on the binary vector enforcing sparsity. This yields a quadratic unconstrained binary optimization problem (QUBO), whose optimal solution(s) in general is NP-hard to find. The contribution of this work is twofold. First, we solve the sparse representation QUBOs by solving them both on a D-Wave quantum annealer with Pegasus chip connectivity via minor embedding, as well as on the Intel Loihi 2 spiking neuromorphic processor using a stochastic Non-equilibrium Boltzmann Machine (NEBM). Second, we deploy Quantum Evolution Monte Carlo with Reverse Annealing and iterated warm starting on Loihi 2 to evolve the solution quality from the respective machines. The solutions are benchmarked against simulated annealing, a classical heuristic, and the optimal solutions are computed using CPLEX. Iterated reverse quantum annealing performs similarly to simulated annealing, although simulated annealing is always able to sample the optimal solution whereas quantum annealing was not always able to. The Loihi 2 solutions that are sampled are on average more sparse than the solutions from any of the other methods. We demonstrate that both quantum annealing and neuromorphic computing are suitable for binary sparse coding QUBOs, and that Loihi 2 outperforms a D-Wave quantum annealer standard linear-schedule anneal, while iterated reverse quantum annealing performs much better than both unmodified linear-schedule quantum annealing and iterated warm starting on Loihi 2.","authors":["Kyle Henke","Elijah Pelofske","Garrett Kenyon","Georg Hahn"],"url":"https://arxiv.org/abs/2405.20525"}
{"created":"2025-05-15","title":"Conformance Testing of Relational DBMS Against SQL Specifications","abstract":"A Relational Database Management System (RDBMS) is one of the fundamental software that supports a wide range of applications, making it critical to identify bugs within these systems. There has been active research on testing RDBMS, most of which employ crash or use metamorphic relations as the oracle. Although existing approaches can detect bugs in RDBMS, they are far from comprehensively evaluating the RDBMS's correctness (i.e., with respect to the semantics of SQL). In this work, we propose a method to test the semantic conformance of RDBMS i.e., whether its behavior respects the intended semantics of SQL. Specifically, we have formally defined the semantics of SQL and implemented them in Prolog. Then, the Prolog implementation serves as the reference RDBMS, enabling differential testing on existing RDBMS. We applied our approach to four widely-used and thoroughly tested RDBMSs, i.e., MySQL, TiDB, SQLite, and DuckDB. In total, our approach uncovered 19 bugs and 11 inconsistencies, which are all related to violating the SQL specification or missing/unclear specification, thereby demonstrating the effectiveness and applicability of our approach.","authors":["Shuang Liu","Chenglin Tian","Jun Sun","Ruifeng Wang","Wei Lu","Yongxin Zhao","Yinxing Xue","Junjie Wang","Xiaoyong Du"],"url":"https://arxiv.org/abs/2406.09469"}
{"created":"2025-05-15","title":"String Partition for Building Long Burrows-Wheeler Transforms","abstract":"Constructing the Burrows-Wheeler transform (BWT) for long strings poses significant challenges regarding construction time and memory usage. We use a prefix of the suffix array to partition a long string into shorter substrings, thereby enabling the use of multi-string BWT construction algorithms to process these partitions fast. We provide an implementation, partDNA, for DNA sequences. Through comparison with state-of-the-art BWT construction algorithms, we show that partDNA with IBB offers a novel trade-off for construction time and memory usage for BWT construction on real genome datasets. Beyond this, the proposed partitioning strategy is applicable to strings of any alphabet.","authors":["Enno Adler","Stefan B\\\"ottcher","Rita Hartel"],"url":"https://arxiv.org/abs/2406.10610"}
{"created":"2025-05-15","title":"The Susceptibility Paradox in Online Social Influence","abstract":"Understanding susceptibility to online influence is crucial for mitigating the spread of misinformation and protecting vulnerable audiences. This paper investigates susceptibility to influence within social networks, focusing on the differential effects of influence-driven versus spontaneous behaviors on user content adoption. Our analysis reveals that influence-driven adoption exhibits high homophily, indicating that individuals prone to influence often connect with similarly susceptible peers, thereby reinforcing peer influence dynamics, whereas spontaneous adoption shows significant but lower homophily. Additionally, we extend the Generalized Friendship Paradox to influence-driven behaviors, demonstrating that users' friends are generally more susceptible to influence than the users themselves, de facto establishing the notion of Susceptibility Paradox in online social influence. This pattern does not hold for spontaneous behaviors, where friends exhibit fewer spontaneous adoptions. We find that susceptibility to influence can be predicted using friends' susceptibility alone, while predicting spontaneous adoption requires additional features, such as user metadata. These findings highlight the complex interplay between user engagement and characteristics in spontaneous content adoption. Our results provide new insights into social influence mechanisms and offer implications for designing more effective moderation strategies to protect vulnerable audiences.","authors":["Luca Luceri","Jinyi Ye","Julie Jiang","Emilio Ferrara"],"url":"https://arxiv.org/abs/2406.11553"}
{"created":"2025-05-15","title":"Accelerated Stochastic Min-Max Optimization Based on Bias-corrected Momentum","abstract":"Lower-bound analyses for nonconvex strongly-concave minimax optimization problems have shown that stochastic first-order algorithms require at least $\\mathcal{O}(\\varepsilon^{-4})$ oracle complexity to find an $\\varepsilon$-stationary point. Some works indicate that this complexity can be improved to $\\mathcal{O}(\\varepsilon^{-3})$ when the loss gradient is Lipschitz continuous. The question of achieving enhanced convergence rates under distinct conditions, remains unresolved. In this work, we address this question for optimization problems that are nonconvex in the minimization variable and strongly concave or Polyak-Lojasiewicz (PL) in the maximization variable. We introduce novel bias-corrected momentum algorithms utilizing efficient Hessian-vector products. We establish convergence conditions and demonstrate a lower iteration complexity of $\\mathcal{O}(\\varepsilon^{-3})$ for the proposed algorithms. The effectiveness of the method is validated through applications to robust logistic regression using real-world datasets.","authors":["Haoyuan Cai","Sulaiman A. Alghunaim","Ali H. Sayed"],"url":"https://arxiv.org/abs/2406.13041"}
{"created":"2025-05-15","title":"Public Constitutional AI","abstract":"We are increasingly subjected to the power of AI authorities. As AI decisions become inescapable, entering domains such as healthcare, education, and law, we must confront a vital question: how can we ensure AI systems have the legitimacy necessary for effective governance? This essay argues that to secure AI legitimacy, we need methods that engage the public in designing and constraining AI systems, ensuring these technologies reflect the community's shared values. Constitutional AI, proposed by Anthropic, represents a step towards this goal, offering a model for democratic control of AI. However, while Constitutional AI's commitment to hardcoding explicit principles into AI models enhances transparency and accountability, it falls short in two crucial aspects: addressing the opacity of individual AI decisions and fostering genuine democratic legitimacy. To overcome these limitations, this essay proposes \"Public Constitutional AI.\" This approach envisions a participatory process where diverse stakeholders, including ordinary citizens, deliberate on the principles guiding AI development. The resulting \"AI Constitution\" would carry the legitimacy of popular authorship, grounding AI governance in the public will. Furthermore, the essay proposes \"AI Courts\" to develop \"AI case law,\" providing concrete examples for operationalizing constitutional principles in AI training. This evolving combination of constitutional principles and case law aims to make AI governance more responsive to public values. By grounding AI governance in deliberative democratic processes, Public Constitutional AI offers a path to imbue automated authorities with genuine democratic legitimacy, addressing the unique challenges posed by increasingly powerful AI systems while ensuring their alignment with the public interest.","authors":["Gilad Abiri"],"url":"https://arxiv.org/abs/2406.16696"}
{"created":"2025-05-15","title":"Enhancing ZFP: A Statistical Approach to Understanding and Reducing Error Bias in a Lossy Floating-Point Compression Algorithm","abstract":"The amount of data generated and gathered in scientific simulations and data collection applications is continuously growing, putting mounting pressure on storage and bandwidth concerns. A means of reducing such issues is data compression; however, lossless data compression is typically ineffective when applied to floating-point data. Thus, users tend to apply a lossy data compressor, which allows for small deviations from the original data. It is essential to understand how the error from lossy compression impacts the accuracy of the data analytics. Thus, we must analyze not only the compression properties but the error as well. In this paper, we provide a statistical analysis of the error caused by ZFP compression, a state-of-the-art, lossy compression algorithm explicitly designed for floating-point data. We show that the error is indeed biased and propose simple modifications to the algorithm to neutralize the bias and further reduce the resulting error.","authors":["Alyson Fox","Peter Lindstrom"],"url":"https://arxiv.org/abs/2407.01826"}
{"created":"2025-05-15","title":"ALEN: A Dual-Approach for Uniform and Non-Uniform Low-Light Image Enhancement","abstract":"Low-light image enhancement is an important task in computer vision, essential for improving the visibility and quality of images captured in non-optimal lighting conditions. Inadequate illumination can lead to significant information loss and poor image quality, impacting various applications such as surveillance. photography, or even autonomous driving. In this regard, automated methods have been developed to automatically adjust illumination in the image for a better visual perception. Current enhancement techniques often use specific datasets to enhance low-light images, but still present challenges when adapting to diverse real-world conditions, where illumination degradation may be localized to specific regions. To address this challenge, the Adaptive Light Enhancement Network (ALEN) is introduced, whose main approach is the use of a classification mechanism to determine whether local or global illumination enhancement is required. Subsequently, estimator networks adjust illumination based on this classification and simultaneously enhance color fidelity. ALEN integrates the Light Classification Network (LCNet) for illuminance categorization, complemented by the Single-Channel Network (SCNet), and Multi-Channel Network (MCNet) for precise estimation of illumination and color, respectively. Extensive experiments on publicly available datasets for low-light conditions were carried out to underscore ALEN's robust generalization capabilities, demonstrating superior performance in both quantitative metrics and qualitative assessments when compared to recent state-of-the-art methods. The ALEN not only enhances image quality in terms of visual perception but also represents an advancement in high-level vision tasks, such as semantic segmentation, as presented in this work. The code of this method is available at https://github.com/xingyumex/ALEN","authors":["Ezequiel Perez-Zarate","Oscar Ramos-Soto","Chunxiao Liu","Diego Oliva","Marco Perez-Cisneros"],"url":"https://arxiv.org/abs/2407.19708"}
{"created":"2025-05-15","title":"Joint Antenna Position and Beamforming Optimization with Self-Interference Mitigation in MA-ISAC System","abstract":"Movable antennas (MAs) have demonstrated significant potential in enhancing the performance of integrated sensing and communication (ISAC) systems. However, the application in the integrated and cost-effective full-duplex (FD) monostatic systems remains underexplored. To address this research gap, we develop an MA-ISAC model within a monostatic framework, where the self-interference channel is modeled in the near field and characterized by antenna position vectors. This model allows us to investigate the use of MAs with the goal of maximizing the weighted sum of communication capacity and sensing mutual information. The resulting optimization problem is non-convex making it challenging to solve optimally. To overcome this, we employ fractional programming (FP) to propose an alternating optimization (AO) algorithm that jointly optimizes the beamforming and antenna positions for both transceivers. Specifically, closed-form solutions for the transmit and receive beamforming matrices are derived using the Karush-Kuhn-Tucker (KKT) conditions, and a novel coarse-to-fine grained search (CFGS) approach is employed to determine the high-quality sub-optimal antenna positions. Numerical results demonstrate that with strong self-interference cancellation (SIC) capabilities, MAs significantly enhance the overall performance and reliability of the ISAC system when utilizing our proposed algorithm, compared to conventional fixed-position antenna designs.","authors":["Size Peng","Cixiao Zhang","Yin Xu","Qingqing Wu","Lipeng Zhu","Xiaowu Ou","Dazhi He"],"url":"https://arxiv.org/abs/2408.00413"}
{"created":"2025-05-15","title":"Deep-MacroFin: Informed Equilibrium Neural Network for Continuous Time Economic Models","abstract":"In this paper, we present Deep-MacroFin, a comprehensive framework designed to solve partial differential equations, with a particular focus on models in continuous time economics. This framework leverages deep learning methodologies, including Multi-Layer Perceptrons and the newly developed Kolmogorov-Arnold Networks. It is optimized using economic information encapsulated by Hamilton-Jacobi-Bellman (HJB) equations and coupled algebraic equations. The application of neural networks holds the promise of accurately resolving high-dimensional problems with fewer computational demands and limitations compared to other numerical methods. This framework can be readily adapted for systems of partial differential equations in high dimensions. Importantly, it offers a more efficient (5$\\times$ less CUDA memory and 40$\\times$ fewer FLOPs in 100D problems) and user-friendly implementation than existing libraries. We also incorporate a time-stepping scheme to enhance training stability for nonlinear HJB equations, enabling the solution of 50D economic models.","authors":["Yuntao Wu","Jiayuan Guo","Goutham Gopalakrishna","Zissis Poulos"],"url":"https://arxiv.org/abs/2408.10368"}
{"created":"2025-05-15","title":"Trefftz Discontinuous Galerkin approximation of an acoustic waveguide","abstract":"We propose a modified Trefftz Discontinuous Galerkin (TDG) method for approximating a time-harmonic acoustic scattering problem in an infinitely elongated waveguide. In the waveguide we suppose there is a bounded, penetrable and possibly absorbing scatterer. The classical TDG is not applicable to this important case. Novel features of our modified TDG method are that it is applicable when the scatterer is absorbing, and it uses a stable treatment of the asymptotic radiation condition for the scattered field. For the modified TDG, we prove $h$ and $p$-convergence in the $L^2$ norm. The theoretical results are verified numerically for a discretization based on plane waves (that may be evanescent in the scatterer).","authors":["Peter Monk","Manuel Pena","Virginia Selgas"],"url":"https://arxiv.org/abs/2408.14833"}
{"created":"2025-05-15","title":"BOP-Distrib: Revisiting 6D Pose Estimation Benchmarks for Better Evaluation under Visual Ambiguities","abstract":"6D pose estimation aims at determining the object pose that best explains the camera observation. The unique solution for non-ambiguous objects can turn into a multi-modal pose distribution for symmetrical objects or when occlusions of symmetry-breaking elements happen, depending on the viewpoint. Currently, 6D pose estimation methods are benchmarked on datasets that consider, for their ground truth annotations, visual ambiguities as only related to global object symmetries, whereas they should be defined per-image to account for the camera viewpoint. We thus first propose an automatic method to re-annotate those datasets with a 6D pose distribution specific to each image, taking into account the object surface visibility in the image to correctly determine the visual ambiguities. Second, given this improved ground truth, we re-evaluate the state-of-the-art single pose methods and show that this greatly modifies the ranking of these methods. Third, as some recent works focus on estimating the complete set of solutions, we derive a precision/recall formulation to evaluate them against our image-wise distribution ground truth, making it the first benchmark for pose distribution methods on real images.","authors":["Boris Meden","Asma Brazi","Fabrice Mayran de Chamisso","Steve Bourgeois","Vincent Lepetit"],"url":"https://arxiv.org/abs/2408.17297"}
{"created":"2025-05-15","title":"One Homography is All You Need: IMM-based Joint Homography and Multiple Object State Estimation","abstract":"A novel online MOT algorithm, IMM Joint Homography State Estimation (IMM-JHSE), is proposed. IMM-JHSE uses an initial homography estimate as the only additional 3D information, whereas other 3D MOT methods use regular 3D measurements. By jointly modelling the homography matrix and its dynamics as part of track state vectors, IMM-JHSE removes the explicit influence of camera motion compensation techniques on predicted track position states, which was prevalent in previous approaches. Expanding upon this, static and dynamic camera motion models are combined using an IMM filter. A simple bounding box motion model is used to predict bounding box positions to incorporate image plane information. In addition to applying an IMM to camera motion, a non-standard IMM approach is applied where bounding-box-based BIoU scores are mixed with ground-plane-based Mahalanobis distances in an IMM-like fashion to perform association only, making IMM-JHSE robust to motion away from the ground plane. Finally, IMM-JHSE makes use of dynamic process and measurement noise estimation techniques. IMM-JHSE improves upon related techniques, including UCMCTrack, OC-SORT, C-BIoU and ByteTrack on the DanceTrack and KITTI-car datasets, increasing HOTA by 2.64 and 2.11, respectively, while offering competitive performance on the MOT17, MOT20 and KITTI-pedestrian datasets. Using publicly available detections, IMM-JHSE outperforms almost all other 2D MOT methods and is outperformed only by 3D MOT methods -- some of which are offline -- on the KITTI-car dataset. Compared to tracking-by-attention methods, IMM-JHSE shows remarkably similar performance on the DanceTrack dataset and outperforms them on the MOT17 dataset. The code is publicly available: https://github.com/Paulkie99/imm-jhse.","authors":["Paul Johannes Claasen","Johan Pieter de Villiers"],"url":"https://arxiv.org/abs/2409.02562"}
{"created":"2025-05-15","title":"Exploring forms of the moist shallow water equations using a new compatible finite element discretisation","abstract":"The moist shallow water equations offer a promising route for advancing understanding of the coupling of physical parametrisations and dynamics in numerical atmospheric models, an issue known as 'physics-dynamics coupling'. Without moist physics, the traditional shallow water equations are a simplified form of the atmospheric equations of motion and so are computationally cheap, but retain many relevant dynamical features of the atmosphere. Introducing physics into the shallow water model in the form of moisture provides a tool to experiment with numerical techniques for physics-dynamics coupling in a simple dynamical model. In this paper, we compare some of the different moist shallow water models by writing them in a general formulation. The general formulation encompasses three existing forms of the moist shallow water equations and also a fourth, previously unexplored formulation. The equations are coupled to a three-state moist physics scheme that interacts with the resolved flow through source terms and produces two-way physics-dynamics feedback. We present a new compatible finite element discretisation of the equations and apply it to the different formulations of the moist shallow water equations in three test cases. The results show that the models capture generation of cloud and rain and physics-dynamics interactions, and demonstrate some differences between moist shallow water formulations and the implications of these different modelling choices.","authors":["Nell Hartney","Thomas M. Bendall","Jemma Shipton"],"url":"https://arxiv.org/abs/2409.07182"}
{"created":"2025-05-15","title":"METDrive: Multi-modal End-to-end Autonomous Driving with Temporal Guidance","abstract":"Multi-modal end-to-end autonomous driving has shown promising advancements in recent work. By embedding more modalities into end-to-end networks, the system's understanding of both static and dynamic aspects of the driving environment is enhanced, thereby improving the safety of autonomous driving. In this paper, we introduce METDrive, an end-to-end system that leverages temporal guidance from the embedded time series features of ego states, including rotation angles, steering, throttle signals, and waypoint vectors. The geometric features derived from perception sensor data and the time series features of ego state data jointly guide the waypoint prediction with the proposed temporal guidance loss function. We evaluated METDrive on the CARLA leaderboard benchmarks, achieving a driving score of 70%, a route completion score of 94%, and an infraction score of 0.78.","authors":["Ziang Guo","Xinhao Lin","Zakhar Yagudin","Artem Lykov","Yong Wang","Yanqiang Li","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2409.12667"}
{"created":"2025-05-15","title":"The Asymptotic Behaviour of Information Leakage Metrics","abstract":"Information theoretic leakage metrics quantify the amount of information about a private random variable $X$ that is leaked through a correlated revealed variable $Y$. They can be used to evaluate the privacy of a system in which an adversary, from whom we want to keep $X$ private, is given access to $Y$. Global information theoretic leakage metrics quantify the overall amount of information leaked upon observing $Y$, whilst their pointwise counterparts define leakage as a function of the particular realisation $Y=y$ that the adversary sees, and thus can be viewed as random variables. We consider an adversary who observes a large number of independent identically distributed realisations of $Y$. We formalise the essential asymptotic behaviour of an information theoretic leakage metric, considering in turn what this means for pointwise and global metrics. With the resulting requirements in mind, we take an axiomatic approach to defining a set of pointwise leakage metrics, as well as a set of global leakage metrics that are constructed from them. The global set encompasses many known measures including mutual information, Sibson mutual information, Arimoto mutual information, maximal leakage, min entropy leakage, $f$-divergence metrics, and g-leakage. We prove that both sets follow the desired asymptotic behaviour. Finally, we derive composition theorems which quantify the rate of privacy degradation as an adversary is given access to a large number of independent observations of $Y$. It is found that, for both pointwise and global metrics, privacy degrades exponentially with increasing observations for the adversary, at a rate governed by the minimum Chernoff information between distinct conditional channel distributions. This extends the work of Wu et al. (2024), who have previously found this to be true for certain known metrics, including some that fall into our more general set.","authors":["Sophie Taylor","Praneeth Kumar Vippathalla","Justin P. Coon"],"url":"https://arxiv.org/abs/2409.13003"}
{"created":"2025-05-15","title":"Efficient Local and Tabu Search Strategies for Large-Scale Quadratic Integer Programming","abstract":"This study investigates the area of general quadratic integer programming (QIP), encompassing both unconstrained (UQIP) and constrained (CQIP) variants. These NP-hard problems have far-reaching applications, yet the non-convex cases have received limited attention in the literature. To address this gap, we introduce a closed-form formula for single-variable changes, establishing novel necessary and sufficient conditions for 1-Opt local improvement in UQIP and CQIP. We develop a simple local and sophisticated tabu search with an oscillation strategy tailored for large-scale problems. Experimental results on instances with up to 8000 variables demonstrate the efficiency of these strategies, producing high-quality solutions within a short time. Our approaches significantly outperform the Gurobi 11.0.2 solver.","authors":["Haibo Wang","Bahram Alidaee"],"url":"https://arxiv.org/abs/2409.14176"}
{"created":"2025-05-15","title":"Least Squares and Marginal Log-Likelihood Model Predictive Control using Normalizing Flows","abstract":"Real-world (bio)chemical processes often exhibit stochastic dynamics with non-trivial correlations and state-dependent fluctuations. Model predictive control (MPC) often must consider these fluctuations to achieve reliable performance. However, most process models simply add stationary noise terms to a deterministic prediction. This work proposes using conditional normalizing flows as discrete-time models to learn stochastic dynamics. Normalizing flows learn the probability density function (PDF) of the states explicitly, given prior states and control inputs. In addition to standard least squares (LSQ) objectives, this work derives a marginal log-likelihood (MLL) objective based on the explicit PDF and Markov chain simulations. In a reactor study, the normalizing flow MPC reduces the setpoint error in open and closed-loop cases to half that of a nominal controller. Furthermore, the chance constraints lead to fewer constraint violations than the nominal controller. The MLL objective yields slightly more stable results than the LSQ, particularly for small scenario sets.","authors":["Eike Cramer"],"url":"https://arxiv.org/abs/2409.17632"}
{"created":"2025-05-15","title":"State-of-the-Art Periorbital Distance Prediction and Disease Classification Using Periorbital Features","abstract":"Periorbital distances are critical markers for diagnosing and monitoring a range of oculoplastic and craniofacial conditions. Manual measurement, however, is subjective and prone to intergrader variability. Automated methods have been developed but remain limited by standardized imaging requirements, small datasets, and a narrow focus on individual measurements. We developed a segmentation pipeline trained on a domain-specific dataset of healthy eyes and compared its performance against the Segment Anything Model (SAM) and the prior benchmark, PeriorbitAI. Segmentation accuracy was evaluated across multiple disease classes and imaging conditions. We further investigated the use of predicted periorbital distances as features for disease classification under in-distribution (ID) and out-of-distribution (OOD) settings, comparing shallow classifiers, CNNs, and fusion models. Our segmentation model achieved state-of-the-art accuracy across all datasets, with error rates within intergrader variability and superior performance relative to SAM and PeriorbitAI. In classification tasks, models trained on periorbital distances matched CNN performance on ID data (77--78\\% accuracy) and substantially outperformed CNNs under OOD conditions (63--68\\% accuracy vs. 14\\%). Fusion models achieved the highest ID accuracy (80\\%) but were sensitive to degraded CNN features under OOD shifts. Segmentation-derived periorbital distances provide robust, explainable features for disease classification and generalize better under domain shift than CNN image classifiers. These results establish a new benchmark for periorbital distance prediction and highlight the potential of anatomy-based AI pipelines for real-world deployment in oculoplastic and craniofacial care.","authors":["George R. Nahass","Sasha Hubschman","Jeffrey C. Peterson","Ghasem Yazdanpanah","Nicholas Tomaras","Madison Cheung","Alex Palacios","Kevin Heinze","Chad A. Purnell","Pete Setabutr","Ann Q. Tran","Darvin Yi"],"url":"https://arxiv.org/abs/2409.18769"}
{"created":"2025-05-15","title":"Feature Extractor or Decision Maker: Rethinking the Role of Visual Encoders in Visuomotor Policies","abstract":"An end-to-end (E2E) visuomotor policy is typically treated as a unified whole, but recent approaches using out-of-domain (OOD) data to pretrain the visual encoder have cleanly separated the visual encoder from the network, with the remainder referred to as the policy. We propose Visual Alignment Testing, an experimental framework designed to evaluate the validity of this functional separation. Our results indicate that in E2E-trained models, visual encoders actively contribute to decision-making resulting from motor data supervision, contradicting the assumed functional separation. In contrast, OOD-pretrained models, where encoders lack this capability, experience an average performance drop of 42\\% in our benchmark results, compared to the state-of-the-art performance achieved by E2E policies. We believe this initial exploration of visual encoders' role can provide a first step towards guiding future pretraining methods to address their decision-making ability, such as developing task-conditioned or context-aware encoders.","authors":["Ruiyu Wang","Zheyu Zhuang","Shutong Jin","Nils Ingelhag","Danica Kragic","Florian T. Pokorny"],"url":"https://arxiv.org/abs/2409.20248"}
{"created":"2025-05-15","title":"Theoretical Insights into Fine-Tuning Attention Mechanism: Generalization and Optimization","abstract":"Large Language Models (LLMs), built on Transformer architectures, exhibit remarkable generalization across a wide range of tasks. However, fine-tuning these models for specific tasks remains resource-intensive due to their extensive parameterization. In this paper, we explore two remarkable phenomena related to the attention mechanism during the fine-tuning of LLMs (where $\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$ denote the weights of the query, key, and value layers, respectively). The first phenomenon, termed \"Unequal Importance of Attention Matrices\", highlights the impact of fine-tuning different weight matrices. It shows that optimizing the $\\mathbf{W}_v$ matrix yields significantly better performance than optimizing the $\\mathbf{W}_k$ matrix. Fine-tuning only the $\\mathbf{W}_q$ and $\\mathbf{W}_v$ matrices is computationally efficient while delivering results comparable to, or even better than fine-tuning all three matrices ($\\mathbf{W}_q$, $\\mathbf{W}_k$, and $\\mathbf{W}_v$). The second phenomenon,\"Attention Matrices with Customized Learning Rate Lead to Better Convergence\", emphasizes the importance of assigning distinct learning rates to these matrices. Specifically, a higher learning rate for the $\\mathbf{W}_v$ matrix compared to $\\mathbf{W}_q$ and $\\mathbf{W}_k$ accelerates convergence and improves performance. Building on these insights, we propose a new strategy that improves fine-tuning efficiency in terms of both storage and time. Experimental results on benchmark datasets validate the effectiveness of this approach, supporting our theoretical findings. Our analysis lays the theoretical groundwork for configuring and improving algorithms in LLMs fine-tuning.","authors":["Xinhao Yao","Hongjin Qian","Xiaolin Hu","Gengze Xu","Wei Liu","Jian Luan","Bin Wang","Yong Liu"],"url":"https://arxiv.org/abs/2410.02247"}
{"created":"2025-05-15","title":"FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering","abstract":"In this paper, we introduce FAMMA, an open-source benchmark for \\underline{f}in\\underline{a}ncial \\underline{m}ultilingual \\underline{m}ultimodal question \\underline{a}nswering (QA). Our benchmark aims to evaluate the abilities of large language models (LLMs) in answering complex reasoning questions that require advanced financial knowledge. The benchmark has two versions: FAMMA-Basic consists of 1,945 questions extracted from university textbooks and exams, along with human-annotated answers and rationales; FAMMA-LivePro consists of 103 novel questions created by human domain experts, with answers and rationales held out from the public for a contamination-free evaluation. These questions cover advanced knowledge of 8 major subfields in finance (e.g., corporate finance, derivatives, and portfolio management). Some are in Chinese or French, while a majority of them are in English. Each question has some non-text data such as charts, diagrams, or tables. Our experiments reveal that FAMMA poses a significant challenge on LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally, we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data, and fine-tuned a series of open-source Qwen models using this reasoning data. We found that training a model on these reasoning trajectories can significantly improve its performance on FAMMA-LivePro. We released our leaderboard, data, code, and trained models at https://famma-bench.github.io/famma/.","authors":["Siqiao Xue","Xiaojing Li","Fan Zhou","Qingyang Dai","Zhixuan Chu","Hongyuan Mei"],"url":"https://arxiv.org/abs/2410.04526"}
{"created":"2025-05-15","title":"Optimal-state Dynamics Estimation for Physics-based Human Motion Capture from Videos","abstract":"Human motion capture from monocular videos has made significant progress in recent years. However, modern approaches often produce temporal artifacts, e.g. in form of jittery motion and struggle to achieve smooth and physically plausible motions. Explicitly integrating physics, in form of internal forces and exterior torques, helps alleviating these artifacts. Current state-of-the-art approaches make use of an automatic PD controller to predict torques and reaction forces in order to re-simulate the input kinematics, i.e. the joint angles of a predefined skeleton. However, due to imperfect physical models, these methods often require simplifying assumptions and extensive preprocessing of the input kinematics to achieve good performance. To this end, we propose a novel method to selectively incorporate the physics models with the kinematics observations in an online setting, inspired by a neural Kalman-filtering approach. We develop a control loop as a meta-PD controller to predict internal joint torques and external reaction forces, followed by a physics-based motion simulation. A recurrent neural network is introduced to realize a Kalman filter that attentively balances the kinematics input and simulated motion, resulting in an optimal-state dynamics prediction. We show that this filtering step is crucial to provide an online supervision that helps balancing the shortcoming of the respective input motions, thus being important for not only capturing accurate global motion trajectories but also producing physically plausible human poses. The proposed approach excels in the physics-based human pose estimation task and demonstrates the physical plausibility of the predictive dynamics, compared to state of the art. The code is available on https://github.com/cuongle1206/OSDCap","authors":["Cuong Le","Viktor Johansson","Manon Kok","Bastian Wandt"],"url":"https://arxiv.org/abs/2410.07795"}
{"created":"2025-05-15","title":"Time Can Invalidate Algorithmic Recourse","abstract":"Algorithmic Recourse (AR) aims to provide users with actionable steps to overturn unfavourable decisions made by machine learning predictors. However, these actions often take time to implement (e.g., getting a degree can take years), and their effects may vary as the world evolves. Thus, it is natural to ask for recourse that remains valid in a dynamic environment. In this paper, we study the robustness of algorithmic recourse over time by casting the problem through the lens of causality. We demonstrate theoretically and empirically that (even robust) causal AR methods can fail over time, except in the -- unlikely -- case that the world is stationary. Even more critically, unless the world is fully deterministic, counterfactual AR cannot be solved optimally. To account for this, we propose a simple yet effective algorithm for temporal AR that explicitly accounts for time under the assumption of having access to an estimator approximating the stochastic process. Our simulations on synthetic and realistic datasets show how considering time produces more resilient solutions to potential trends in the data distribution.","authors":["Giovanni De Toni","Stefano Teso","Bruno Lepri","Andrea Passerini"],"url":"https://arxiv.org/abs/2410.08007"}
{"created":"2025-05-15","title":"Combinatorial Logistic Bandits","abstract":"We introduce a novel framework called combinatorial logistic bandits (CLogB), where in each round, a subset of base arms (called the super arm) is selected, with the outcome of each base arm being binary and its expectation following a logistic parametric model. The feedback is governed by a general arm triggering process. Our study covers CLogB with reward functions satisfying two smoothness conditions, capturing application scenarios such as online content delivery, online learning to rank, and dynamic channel allocation. We first propose a simple yet efficient algorithm, CLogUCB, utilizing a variance-agnostic exploration bonus. Under the 1-norm triggering probability modulated (TPM) smoothness condition, CLogUCB achieves a regret bound of $\\tilde{O}(d\\sqrt{\\kappa KT})$, where $\\tilde{O}$ ignores logarithmic factors, $d$ is the dimension of the feature vector, $\\kappa$ represents the nonlinearity of the logistic model, and $K$ is the maximum number of base arms a super arm can trigger. This result improves on prior work by a factor of $\\tilde{O}(\\sqrt{\\kappa})$. We then enhance CLogUCB with a variance-adaptive version, VA-CLogUCB, which attains a regret bound of $\\tilde{O}(d\\sqrt{KT})$ under the same 1-norm TPM condition, improving another $\\tilde{O}(\\sqrt{\\kappa})$ factor. VA-CLogUCB shows even greater promise under the stronger triggering probability and variance modulated (TPVM) condition, achieving a leading $\\tilde{O}(d\\sqrt{T})$ regret, thus removing the additional dependency on the action-size $K$. Furthermore, we enhance the computational efficiency of VA-CLogUCB by eliminating the nonconvex optimization process when the context feature map is time-invariant while maintaining the tight $\\tilde{O}(d\\sqrt{T})$ regret. Finally, experiments on synthetic and real-world datasets demonstrate the superior performance of our algorithms compared to benchmark algorithms.","authors":["Xutong Liu","Xiangxiang Dai","Xuchuang Wang","Mohammad Hajiesmaili","John C. S. Lui"],"url":"https://arxiv.org/abs/2410.17075"}
{"created":"2025-05-15","title":"Reflecting Topology Consistency and Abnormality via Learnable Attentions for Airway Labeling","abstract":"Accurate airway anatomical labeling is crucial for clinicians to identify and navigate complex bronchial structures during bronchoscopy. Automatic airway anatomical labeling is challenging due to significant individual variability and anatomical variations. Previous methods are prone to generate inconsistent predictions, which is harmful for preoperative planning and intraoperative navigation. This paper aims to address these challenges by proposing a novel method that enhances topological consistency and improves the detection of abnormal airway branches. We propose a novel approach incorporating two modules: the Soft Subtree Consistency (SSC) and the Abnormal Branch Saliency (ABS). The SSC module constructs a soft subtree to capture clinically relevant topological relationships, allowing for flexible feature aggregation within and across subtrees. The ABS module facilitates the interaction between node features and prototypes to distinguish abnormal branches, preventing the erroneous aggregation of features between normal and abnormal nodes. Evaluated on a challenging dataset characterized by severe airway distortion and atrophy, our method achieves superior performance compared to state-of-the-art approaches. Specifically, it attains a 91.4% accuracy at the segmental level and an 83.7% accuracy at the subsegmental level, representing a 1.4% increase in subsegmental accuracy and a 3.1% increase in topological consistency. Notably, the method demonstrates reliable performance in cases with disease-induced airway deformities, ensuring consistent and accurate labeling.","authors":["Chenyu Li","Minghui Zhang","Chuyan Zhang","Yun Gu"],"url":"https://arxiv.org/abs/2410.23854"}
{"created":"2025-05-15","title":"Distributing Intelligence in 6G Programmable Data Planes for Effective In-Network Intrusion Prevention","abstract":"The problem of attacks on new generation network infrastructures is becoming increasingly relevant, given the widening of the attack surface of these networks resulting from the greater number of devices that will access them in the future (sensors, actuators, vehicles, household appliances, etc.). Approaches to the design of intrusion detection systems must evolve and go beyond the traditional concept of perimeter control to build on new paradigms that exploit the typical characteristics of future 5G and 6G networks, such as in-network computing and intelligent programmable data planes. The aim of this research is to propose a disruptive paradigm in which devices in a typical data plane of a future programmable network have anomaly detection capabilities and cooperate in a fully distributed fashion to act as an ML-enabled Intrusion Prevention System ``embedded\" into the network. The reported proof-of-concept experiments demonstrate that the proposed paradigm allows working effectively and with a good level of precision while occupying overall less CPU and RAM resources of the devices involved.","authors":["Mattia G. Spina","Floriano De Rango","Edoardo Scalzo","Francesca Guerriero","Antonio Iera"],"url":"https://arxiv.org/abs/2410.24013"}
{"created":"2025-05-15","title":"SPOT: SE(3) Pose Trajectory Diffusion for Object-Centric Manipulation","abstract":"We introduce SPOT, an object-centric imitation learning framework. The key idea is to capture each task by an object-centric representation, specifically the SE(3) object pose trajectory relative to the target. This approach decouples embodiment actions from sensory inputs, facilitating learning from various demonstration types, including both action-based and action-less human hand demonstrations, as well as cross-embodiment generalization. Additionally, object pose trajectories inherently capture planning constraints from demonstrations without the need for manually-crafted rules. To guide the robot in executing the task, the object trajectory is used to condition a diffusion policy. We systematically evaluate our method on simulation and real-world tasks. In real-world evaluation, using only eight demonstrations shot on an iPhone, our approach completed all tasks while fully complying with task constraints. Project page: https://nvlabs.github.io/object_centric_diffusion","authors":["Cheng-Chun Hsu","Bowen Wen","Jie Xu","Yashraj Narang","Xiaolong Wang","Yuke Zhu","Joydeep Biswas","Stan Birchfield"],"url":"https://arxiv.org/abs/2411.00965"}
{"created":"2025-05-15","title":"What Features in Prompts Jailbreak LLMs? Investigating the Mechanisms Behind Attacks","abstract":"Jailbreaks have been a central focus of research regarding the safety and reliability of large language models (LLMs), yet the mechanisms underlying these attacks remain poorly understood. While previous studies have predominantly relied on linear methods to detect jailbreak attempts and model refusals, we take a different approach by examining both linear and non-linear features in prompts that lead to successful jailbreaks. First, we introduce a novel dataset comprising 10,800 jailbreak attempts spanning 35 diverse attack methods. Leveraging this dataset, we train probes to classify successful from unsuccessful jailbreaks using the latent representations corresponding to prompt tokens. Notably, we find that even when probes achieve high accuracy in predicting the success of jailbreaks, their performance often fails to generalize to unseen attack methods. This reveals that different jailbreaking strategies exploit different non-linear, non-universal features. Next, we demonstrate that non-linear probes provide a powerful tool for steering model behavior. Specifically, we use these probes to guide targeted latent space perturbations, enabling us to effectively modulate the model's robustness against jailbreaks. Overall, our findings challenge the assumption that jailbreaks can be fully understood through linear or simple universal prompt features alone, highlighting the importance of a nuanced understanding of the mechanisms behind LLM vulnerabilities.","authors":["Nathalie Kirch","Constantin Weisser","Severin Field","Helen Yannakoudakis","Stephen Casper"],"url":"https://arxiv.org/abs/2411.03343"}
{"created":"2025-05-15","title":"Think Smart, Act SMARL! Analyzing Probabilistic Logic Shields for Multi-Agent Reinforcement Learning","abstract":"Safe reinforcement learning (RL) is crucial for real-world applications, and multi-agent interactions introduce additional safety challenges. While Probabilistic Logic Shields (PLS) has been a powerful proposal to enforce safety in single-agent RL, their generalizability to multi-agent settings remains unexplored. In this paper, we address this gap by conducting extensive analyses of PLS within decentralized, multi-agent environments, and in doing so, propose Shielded Multi-Agent Reinforcement Learning (SMARL) as a general framework for steering MARL towards norm-compliant outcomes. Our key contributions are: (1) a novel Probabilistic Logic Temporal Difference (PLTD) update for shielded, independent Q-learning, which incorporates probabilistic constraints directly into the value update process; (2) a probabilistic logic policy gradient method for shielded PPO with formal safety guarantees for MARL; and (3) comprehensive evaluation across symmetric and asymmetrically shielded $n$-player game-theoretic benchmarks, demonstrating fewer constraint violations and significantly better cooperation under normative constraints. These results position SMARL as an effective mechanism for equilibrium selection, paving the way toward safer, socially aligned multi-agent systems.","authors":["Satchit Chatterji","Erman Acar"],"url":"https://arxiv.org/abs/2411.04867"}
{"created":"2025-05-15","title":"Is Linear Feedback on Smoothed Dynamics Sufficient for Stabilizing Contact-Rich Plans?","abstract":"Designing planners and controllers for contact-rich manipulation is extremely challenging as contact violates the smoothness conditions that many gradient-based controller synthesis tools assume. Contact smoothing approximates a non-smooth system with a smooth one, allowing one to use these synthesis tools more effectively. However, applying classical control synthesis methods to smoothed contact dynamics remains relatively under-explored. This paper analyzes the efficacy of linear controller synthesis using differential simulators based on contact smoothing. We introduce natural baselines for leveraging contact smoothing to compute (a) open-loop plans robust to uncertain conditions and/or dynamics, and (b) feedback gains to stabilize around open-loop plans. Using robotic bimanual whole-body manipulation as a testbed, we perform extensive empirical experiments on over 300 trajectories and analyze why LQR seems insufficient for stabilizing contact-rich plans. The video summarizing this paper and hardware experiments is found here: https://youtu.be/HLaKi6qbwQg?si=_zCAmBBD6rGSitm9.","authors":["Yuki Shirai","Tong Zhao","H. J. Terry Suh","Huaijiang Zhu","Xinpei Ni","Jiuguang Wang","Max Simchowitz","Tao Pang"],"url":"https://arxiv.org/abs/2411.06542"}
{"created":"2025-05-15","title":"Degree Matrix Comparison for Graph Alignment","abstract":"The graph alignment problem, which considers the optimal node correspondence across networks, has recently gained significant attention due to its wide applications. There are graph alignment methods suited for various network types, but we focus on the unsupervised geometric alignment algorithms. We propose Degree Matrix Comparison (DMC), a very simple degree-based method that has shown to be effective for heterogeneous networks. Through extensive experiments and mathematical proofs, we demonstrate the potential of this method. Remarkably, DMC achieves up to 99% correct node alignment for 90%-overlap networks and 100% accuracy for isomorphic graphs. Additionally, we propose a reduced Greedy DMC with lower time complexity and Weighted DMC that has demonstrated potential for aligning weighted graphs. Positive results from applying Greedy DMC and the Weighted DMC furthermore speaks to the validity and potential of the DMC. The sequence of DMC methods could significantly impact graph alignment, offering reliable solutions for the task.","authors":["Ashley Wang","Peter Chin"],"url":"https://arxiv.org/abs/2411.07475"}
{"created":"2025-05-15","title":"Synthesis with Privacy Against an Observer","abstract":"We study automatic synthesis of systems that interact with their environment and maintain privacy against an observer to the interaction. The system and the environment interact via sets $I$ and $O$ of input and output signals. The input to the synthesis problem contains, in addition to a specification, also a list of secrets, a function $cost: I\\cup O\\rightarrow\\mathbb{N}$, which maps each signal to the cost of hiding it, and a bound $b\\in\\mathbb{N}$ on the budget that the system may use for hiding of signals. The desired output is an $(I/O)$-transducer $T$ and a set $H\\subseteq I\\cup O$ of signals that respects the bound on the budget, thus $\\sum_{s\\in H} cost(s)\\leq b$, such that for every possible interaction of $T$, the generated computation satisfies the specification, yet an observer, from whom the signals in $H$ are hidden, cannot evaluate the secrets.","authors":["Orna Kupferman","Ofer Leshkowitz","Namma Shamash Halevy"],"url":"https://arxiv.org/abs/2411.08635"}
{"created":"2025-05-15","title":"P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs","abstract":"Recent advancements in large language models (LLMs) showcase varied multilingual capabilities across tasks like translation, code generation, and reasoning. Previous assessments often limited their scope to fundamental natural language processing (NLP) or isolated capability-specific tasks. To alleviate this drawback, we aim to present a comprehensive multilingual multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark covering effective fundamental and capability-specialized datasets. Furthermore, P-MMEval delivers consistent language coverage across various datasets and provides parallel samples. Finally, we conduct extensive experiments on representative multilingual model series to compare performances across models and tasks, explore the relationship between multilingual performances and factors such as tasks, model sizes, languages, and prompts, and examine the effectiveness of knowledge transfer from English to other languages. The resulting insights are intended to offer valuable guidance for future research. The dataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.","authors":["Yidan Zhang","Yu Wan","Boyi Deng","Baosong Yang","Haoran Wei","Fei Huang","Bowen Yu","Junyang Lin","Fei Huang","Jingren Zhou"],"url":"https://arxiv.org/abs/2411.09116"}
{"created":"2025-05-15","title":"PSPO*: An Effective Process-supervised Policy Optimization for Reasoning Alignment","abstract":"Process supervision enhances the performance of large language models in reasoning tasks by providing feedback at each step of chain-of-thought reasoning. However, due to the lack of effective process supervision methods, even advanced large language models are prone to logical errors and redundant reasoning. We claim that the effectiveness of process supervision significantly depends on both the accuracy and the length of reasoning chains. Moreover, we identify that these factors exhibit a nonlinear relationship with the overall reward score of the reasoning process. Inspired by these insights, we propose a novel process supervision paradigm, PSPO*, which systematically outlines the workflow from reward model training to policy optimization, and highlights the importance of nonlinear rewards in process supervision. Based on PSPO*, we develop the PSPO-WRS, which considers the number of reasoning steps in determining reward scores and utilizes an adjusted Weibull distribution for nonlinear reward shaping. Experimental results on six mathematical reasoning datasets demonstrate that PSPO-WRS consistently outperforms current mainstream models.","authors":["Jiawei Li","Xinyue Liang","Junlong Zhang","Yizhe Yang","Chong Feng","Yang Gao"],"url":"https://arxiv.org/abs/2411.11681"}
{"created":"2025-05-15","title":"Towards Understanding the Impact of Data Bugs on Deep Learning Models in Software Engineering","abstract":"Deep learning (DL) techniques have achieved significant success in various software engineering tasks (e.g., code completion by Copilot). However, DL systems are prone to bugs from many sources, including training data. Existing literature suggests that bugs in training data are highly prevalent, but little research has focused on understanding their impacts on the models used in software engineering tasks. In this paper, we address this research gap through a comprehensive empirical investigation focused on three types of data prevalent in software engineering tasks: code-based, text-based, and metric-based. Using state-of-the-art baselines, we compare the models trained on clean datasets with those trained on datasets with quality issues and without proper preprocessing. By analysing the gradients, weights, and biases from neural networks under training, we identify the symptoms of data quality and preprocessing issues. Our analysis reveals that quality issues in code data cause biased learning and gradient instability, whereas problems in text data lead to overfitting and poor generalisation of models. On the other hand, quality issues in metric data result in exploding gradients and model overfitting, and inadequate preprocessing exacerbates these effects across all three data types. Finally, we demonstrate the validity and generalizability of our findings using six new datasets. Our research provides a better understanding of the impact and symptoms of data bugs in software engineering datasets. Practitioners and researchers can leverage these findings to develop better monitoring systems and data-cleaning methods to help detect and resolve data bugs in deep learning systems.","authors":["Mehil B Shah","Mohammad Masudur Rahman","Foutse Khomh"],"url":"https://arxiv.org/abs/2411.12137"}
{"created":"2025-05-15","title":"ThreatModeling-LLM: Automating Threat Modeling using Large Language Models for Banking System","abstract":"Threat modeling is a crucial component of cybersecurity, particularly for industries such as banking, where the security of financial data is paramount. Traditional threat modeling approaches require expert intervention and manual effort, often leading to inefficiencies and human error. The advent of Large Language Models (LLMs) offers a promising avenue for automating these processes, enhancing both efficiency and efficacy. However, this transition is not straightforward due to three main challenges: (1) the lack of publicly available, domain-specific datasets, (2) the need for tailored models to handle complex banking system architectures, and (3) the requirement for real-time, adaptive mitigation strategies that align with compliance standards like NIST 800-53. In this paper, we introduce ThreatModeling-LLM, a novel and adaptable framework that automates threat modeling for banking systems using LLMs. ThreatModeling-LLM operates in three stages: 1) dataset creation, 2) prompt engineering and 3) model fine-tuning. We first generate a benchmark dataset using Microsoft Threat Modeling Tool (TMT). Then, we apply Chain of Thought (CoT) and Optimization by PROmpting (OPRO) on the pre-trained LLMs to optimize the initial prompt. Lastly, we fine-tune the LLM using Low-Rank Adaptation (LoRA) based on the benchmark dataset and the optimized prompt to improve the threat identification and mitigation generation capabilities of pre-trained LLMs.","authors":["Tingmin Wu","Shuiqiao Yang","Shigang Liu","David Nguyen","Seung Jang","Alsharif Abuadbba"],"url":"https://arxiv.org/abs/2411.17058"}
{"created":"2025-05-15","title":"Morphological-Symmetry-Equivariant Heterogeneous Graph Neural Network for Robotic Dynamics Learning","abstract":"We present a morphological-symmetry-equivariant heterogeneous graph neural network, namely MS-HGNN, for robotic dynamics learning, that integrates robotic kinematic structures and morphological symmetries into a single graph network. These structural priors are embedded into the learning architecture as constraints, ensuring high generalizability, sample and model efficiency. The proposed MS-HGNN is a versatile and general architecture that is applicable to various multi-body dynamic systems and a wide range of dynamics learning problems. We formally prove the morphological-symmetry-equivariant property of our MS-HGNN and validate its effectiveness across multiple quadruped robot learning problems using both real-world and simulated data. Our code is made publicly available at https://github.com/lunarlab-gatech/MorphSym-HGNN/.","authors":["Fengze Xie","Sizhe Wei","Yue Song","Yisong Yue","Lu Gan"],"url":"https://arxiv.org/abs/2412.01297"}
{"created":"2025-05-15","title":"HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment","abstract":"Mesh quality assessment (MQA) models play a critical role in the design, optimization, and evaluation of mesh operation systems in a wide variety of applications. Current MQA models, whether model-based methods using topology-aware features or projection-based approaches working on rendered 2D projections, often fail to capture the intricate interactions between texture and 3D geometry. We introduce HybridMQA, a first-of-its-kind hybrid full-reference colored MQA framework that integrates model-based and projection-based approaches, capturing complex interactions between textural information and 3D structures for enriched quality representations. Our method employs graph learning to extract detailed 3D representations, which are then projected to 2D using a novel feature rendering process that precisely aligns them with colored projections. This enables the exploration of geometry-texture interactions via cross-attention, producing comprehensive mesh quality representations. Extensive experiments demonstrate HybridMQA's superior performance across diverse datasets, highlighting its ability to effectively leverage geometry-texture interactions for a thorough understanding of mesh quality. Our implementation will be made publicly available.","authors":["Armin Shafiee Sarvestani","Sheyang Tang","Zhou Wang"],"url":"https://arxiv.org/abs/2412.01986"}
{"created":"2025-05-15","title":"Reduction from the partition problem: Dynamic lot sizing problem with polynomial complexity","abstract":"In this note, we polynomially reduce an instance of the partition problem to a dynamic lot sizing problem, and show that solving the latter problem solves the former problem. By solving the dynamic program formulation of the dynamic lot sizing problem, we show that the instance of the partition problem can be solved with pseudo-polynomial time complexity. Numerical results on solving instances of the partition problem are also provided using an implementation of the algorithm that solves the dynamic program. We conclude by discussing polynomial time solvability of the partition problem through further observation on the dynamic program formulation of the dynamic lot sizing problem.","authors":["Chee-Khian Sim"],"url":"https://arxiv.org/abs/2412.05017"}
{"created":"2025-05-15","title":"MCP-MedSAM: A Powerful Lightweight Medical Segment Anything Model Trained with a Single GPU in Just One Day","abstract":"Medical image segmentation involves partitioning medical images into meaningful regions, with a focus on identifying anatomical structures and lesions. It has broad applications in healthcare, and deep learning methods have enabled significant advancements in automating this process. Recently, the introduction of the Segmentation Anything Model (SAM), the first foundation model for segmentation task, has prompted researchers to adapt it for the medical domain to improve performance across various tasks. However, SAM's large model size and high GPU requirements hinder its scalability and development in the medical domain. In this work, we propose MCP-MedSAM, a powerful and lightweight medical SAM model designed to be trainable on a single A100 GPU with 40GB of memory within one day while delivering superior segmentation performance. Recognizing the significant internal differences between modalities and the need for direct segmentation target information within bounding boxes, we introduce two kinds of prompts: the modality prompt and the content prompt. After passing through the prompt encoder, their embedding representations can further improve the segmentation performance by incorporating more relevant information without adding significant training overhead. Additionally, we adopt an effective modality-based data sampling strategy to address data imbalance between modalities, ensuring more balanced performance across all modalities. Our method was trained and evaluated using a large-scale challenge dataset, compared to top-ranking methods on the challenge leaderboard, MCP-MedSAM achieved superior performance while requiring only one day of training on a single GPU. The code is publicly available at \\textcolor{blue}{https://github.com/dong845/MCP-MedSAM}.}","authors":["Donghang Lyu","Ruochen Gao","Marius Staring"],"url":"https://arxiv.org/abs/2412.05888"}
{"created":"2025-05-15","title":"A physics-informed transformer neural operator for learning generalized solutions of initial boundary value problems","abstract":"Initial boundary value problems arise commonly in applications with engineering and natural systems governed by nonlinear partial differential equations (PDEs). Operator learning is an emerging field for solving these equations by using a neural network to learn a map between infinite dimensional input and output function spaces. These neural operators are trained using a combination of data (observations or simulations) and PDE-residuals (physics-loss). A major drawback of existing neural approaches is the requirement to retrain with new initial/boundary conditions, and the necessity for a large amount of simulation data for training. We develop a physics-informed transformer neural operator (named PINTO) that efficiently generalizes to unseen initial and boundary conditions, trained in a simulation-free setting using only physics loss. The main innovation lies in our new iterative kernel integral operator units, implemented using cross-attention, to transform the PDE solution's domain points into an initial/boundary condition-aware representation vector, enabling efficient learning of the solution function for new scenarios. The PINTO architecture is applied to simulate the solutions of important equations used in engineering applications: advection, Burgers, and steady and unsteady Navier-Stokes equations (three flow scenarios). For these five test cases, we show that the relative errors during testing under challenging conditions of unseen initial/boundary conditions are only one-fifth to one-third of other leading physics informed operator learning methods. Moreover, our PINTO model is able to accurately solve the advection and Burgers equations at time steps that are not included in the training collocation points. The code is available at https://github.com/quest-lab-iisc/PINTO","authors":["Sumanth Kumar Boya","Deepak Subramani"],"url":"https://arxiv.org/abs/2412.09009"}
{"created":"2025-05-15","title":"ON as ALC: Active Loop Closing Object Goal Navigation","abstract":"In simultaneous localization and mapping, active loop closing (ALC) is an active vision problem that aims to visually guide a robot to maximize the chances of revisiting previously visited points, thereby resetting the drift errors accumulated in the incrementally built map during travel. However, current mainstream navigation strategies that leverage such incomplete maps as workspace prior knowledge often fail in modern long-term autonomy long-distance travel scenarios where map accumulation errors become significant. To address these limitations of map-based navigation, this paper is the first to explore mapless navigation in the embodied AI field, in particular, to utilize object-goal navigation (commonly abbreviated as ON, ObjNav, or OGN) techniques that efficiently explore target objects without using such a prior map. Specifically, in this work, we start from an off-the-shelf mapless ON planner, extend it to utilize a prior map, and further show that the performance in long-distance ALC (LD-ALC) can be maximized by minimizing ``ALC loss\" and ``ON loss\". This study highlights a simple and effective approach, called ALC-ON (ALCON), to accelerate the progress of challenging long-distance ALC technology by leveraging the growing frontier-guided, data-driven, and LLM-guided ON technologies.","authors":["Daiki Iwata","Kanji Tanaka","Shoya Miyazaki","Kouki Terashima"],"url":"https://arxiv.org/abs/2412.11523"}
{"created":"2025-05-15","title":"A Fully Adaptive Radau Method for the Efficient Solution of Stiff Ordinary Differential Equations at Low Tolerances","abstract":"Radau IIA methods, specifically the adaptive order Radau method in Fortran due to Hairer, are known to be state-of-the-art for the high-accuracy solution of highly stiff ordinary differential equations (ODEs). However, the traditional implementation was specialized to a specific range of tolerance, in particular only supporting 5th, 9th, and 13th order versions of the tableau and only derived in double precision floating point, thus limiting the ability to be truly general purpose for high fidelity scenarios. To alleviate these constraints, we implement an adaptive-time adaptive-order Radau method which can derive the coefficients for the Radau IIA embedded tableau to any order on the fly to any precision. Additionally, our Julia-based implementation includes many modernizations to improve performance, including improvements to the order adaptation scheme and improved linear algebra integrations. In a head-to-head benchmark against the classic Fortran implementation, we demonstrate our implementation is approximately 2x across a range of stiff ODEs. We benchmark our algorithm against several well-reputed numerical integrators for stiff ODEs and find state-of-the-art performance on several test problems, with a 1.5-times speed-up over common numerical integrators for stiff ODEs when low error tolerance is required. The newly implemented method is distributed in open source software for free usage on stiff ODEs.","authors":["Shreyas Ekanathan","Oscar Smith","Christopher Rackauckas"],"url":"https://arxiv.org/abs/2412.14362"}
{"created":"2025-05-15","title":"BridgePure: Limited Protection Leakage Can Break Black-Box Data Protection","abstract":"Availability attacks, or unlearnable examples, are defensive techniques that allow data owners to modify their datasets in ways that prevent unauthorized machine learning models from learning effectively while maintaining the data's intended functionality. It has led to the release of popular black-box tools (e.g., APIs) for users to upload personal data and receive protected counterparts. In this work, we show that such black-box protections can be substantially compromised if a small set of unprotected in-distribution data is available. Specifically, we propose a novel threat model of protection leakage, where an adversary can (1) easily acquire (unprotected, protected) pairs by querying the black-box protections with a small unprotected dataset; and (2) train a diffusion bridge model to build a mapping between unprotected and protected data. This mapping, termed BridgePure, can effectively remove the protection from any previously unseen data within the same distribution. BridgePure demonstrates superior purification performance on classification and style mimicry tasks, exposing critical vulnerabilities in black-box data protection. We suggest that practitioners implement multi-level countermeasures to mitigate such risks.","authors":["Yihan Wang","Yiwei Lu","Xiao-Shan Gao","Gautam Kamath","Yaoliang Yu"],"url":"https://arxiv.org/abs/2412.21061"}
{"created":"2025-05-15","title":"A Bio-Inspired Research Paradigm of Collision Perception Neurons Enabling Neuro-Robotic Integration: The LGMD Case","abstract":"Compared to human vision, locust visual systems excel at rapid and precise collision detection, despite relying on only hundreds of thousands of neurons organized through a few neuropils. This efficiency makes them an attractive model system for developing artificial collision-detecting systems. Specifically, researchers have identified collision-selective neurons in the locust's optic lobe, called lobula giant movement detectors (LGMDs), which respond specifically to approaching objects. Research upon LGMD neurons began in the early 1970s. Initially, due to their large size, these neurons were identified as motion detectors, but their role as looming detectors was recognized over time. Since then, progress in neuroscience, computational modeling of LGMD's visual neural circuits, and LGMD-based robotics have advanced in tandem, each field supporting and driving the others. Today, with a deeper understanding of LGMD neurons, LGMD-based models have significantly improved collision-free navigation in mobile robots including ground and aerial robots. This review highlights recent developments in LGMD research from the perspectives of neuroscience, computational modeling, and robotics. It emphasizes a biologically plausible research paradigm, where insights from neuroscience inform real-world applications, which would in turn validate and advance neuroscience. With strong support from extensive research and growing application demand, this paradigm has reached a mature stage and demonstrates versatility across different areas of neuroscience research, thereby enhancing our understanding of the interconnections between neuroscience, computational modeling, and robotics. Furthermore, this paradigm would shed light upon the modeling and robotic research into other motion-sensitive neurons or neural circuits.","authors":["Ziyan Qin","Jigen Peng","Shigang Yue","Qinbing Fu"],"url":"https://arxiv.org/abs/2501.02982"}
{"created":"2025-05-15","title":"Deontic Temporal Logic for Formal Verification of AI Ethics","abstract":"Ensuring ethical behavior in Artificial Intelligence (AI) systems amidst their increasing ubiquity and influence is a major concern the world over. The use of formal methods in AI ethics is a possible crucial approach for specifying and verifying the ethical behavior of AI systems. This paper proposes a formalization based on deontic logic to define and evaluate the ethical behavior of AI systems, focusing on system-level specifications, contributing to this important goal. It introduces axioms and theorems to capture ethical requirements related to fairness and explainability. The formalization incorporates temporal operators to reason about the ethical behavior of AI systems over time. The authors evaluate the effectiveness of this formalization by assessing the ethics of the real-world COMPAS and loan prediction AI systems. Various ethical properties of the COMPAS and loan prediction systems are encoded using deontic logical formulas, allowing the use of an automated theorem prover to verify whether these systems satisfy the defined properties. The formal verification reveals that both systems fail to fulfill certain key ethical properties related to fairness and non-discrimination, demonstrating the effectiveness of the proposed formalization in identifying potential ethical issues in real-world AI applications.","authors":["Priya T. V.","Shrisha Rao"],"url":"https://arxiv.org/abs/2501.05765"}
{"created":"2025-05-15","title":"Beyond Optimal Fault Tolerance","abstract":"The optimal fault-tolerance achievable by any protocol has been characterized in a wide range of settings. For example, for state machine replication (SMR) protocols operating in the partially synchronous setting, it is possible to simultaneously guarantee consistency against $\\alpha$-bounded adversaries (i.e., adversaries that control less than an $\\alpha$ fraction of the participants) and liveness against $\\beta$-bounded adversaries if and only if $\\alpha + 2\\beta \\leq 1$.","authors":["Andrew Lewis-Pye","Tim Roughgarden"],"url":"https://arxiv.org/abs/2501.06044"}
{"created":"2025-05-15","title":"Equitable Allocations of Mixtures of Goods and Chores","abstract":"Equitable allocation of indivisible items involves partitioning the items among agents such that everyone derives (almost) equal utility. We consider the approximate notion of \\textit{equitability up to one item} (EQ1) and focus on the settings containing mixtures of items (goods and chores), where an agent may derive positive, negative, or zero utility from an item. We first show that -- in stark contrast to the goods-only and chores-only settings -- an EQ1 allocation may not exist even for additive $\\{-1,1\\}$ bivalued instances, and its corresponding decision problem is computationally intractable. We focus on a natural domain of normalized valuations where the value of the entire set of items is constant for all agents. On the algorithmic side, we show that an EQ1 allocation can be computed efficiently for (i) $\\{-1, 0, 1\\}$ normalized valuations, (ii) objective but non-normalized valuations, (iii) two agents with type-normalized valuations. Previously, EQX allocations were known to exist only for 2 agents and objective valuations, while the case of subjective valuations remained computationally intractable even with two agents. We make progress by presenting an efficient algorithm that outputs an EQX allocation for $\\{-1,1\\}$ normalized subjective valuations for any number of agents. We complement our study by providing a comprehensive picture of achieving EQ1 allocations in conjunction with economic efficiency notions such as Pareto optimality and social welfare.","authors":["Hadi Hosseini","Aditi Sethia"],"url":"https://arxiv.org/abs/2501.06799"}
{"created":"2025-05-15","title":"A Comprehensive Social Bias Audit of Contrastive Vision Language Models","abstract":"In the domain of text-to-image generative models, biases inherent in training datasets often propagate into generated content, posing significant ethical challenges, particularly in socially sensitive contexts. We introduce FairCoT, a novel framework that enhances fairness in text-to-image models through Chain-of-Thought (CoT) reasoning within multimodal generative large language models. FairCoT employs iterative CoT refinement to systematically mitigate biases, and dynamically adjusts textual prompts in real time, ensuring diverse and equitable representation in generated images. By integrating iterative reasoning processes, FairCoT addresses the limitations of zero-shot CoT in sensitive scenarios, balancing creativity with ethical responsibility. Experimental evaluations across popular text-to-image systems--including DALL-E and various Stable Diffusion variants--demonstrate that FairCoT significantly enhances fairness and diversity without sacrificing image quality or semantic fidelity. By combining robust reasoning, lightweight deployment, and extensibility to multiple models, FairCoT represents a promising step toward more socially responsible and transparent AI-driven content generation.","authors":["Zahraa Al Sahili","Ioannis Patras","Matthew Purver"],"url":"https://arxiv.org/abs/2501.13223"}
{"created":"2025-05-15","title":"Handling Missing Data in Downstream Tasks With Distribution-Preserving Guarantees","abstract":"Missing feature values are a significant hurdle for downstream machine-learning tasks such as classification. However, imputation methods for classification might be time-consuming for high-dimensional data, and offer few theoretical guarantees on the preservation of the data distribution and imputation quality, especially for not-missing-at-random mechanisms. First, we propose an imputation approach named F3I based on the iterative improvement of a K-nearest neighbor imputation, where neighbor-specific weights are learned through the optimization of a novel concave, differentiable objective function related to the preservation of the data distribution on non-missing values. F3I can then be chained to and jointly trained with any classifier architecture. Second, we provide a theoretical analysis of imputation quality and data distribution preservation by F3I for several types of missing mechanisms. Finally, we demonstrate the superior performance of F3I on several imputation and classification tasks, with applications to drug repurposing and handwritten-digit recognition data.","authors":["Rahul Bordoloi","Cl\\'emence R\\'eda","Saptarshi Bej","Olaf Wolkenhauer"],"url":"https://arxiv.org/abs/2501.13786"}
{"created":"2025-05-15","title":"Improving Network Threat Detection by Knowledge Graph, Large Language Model, and Imbalanced Learning","abstract":"Network threat detection has been challenging due to the complexities of attack activities and the limitation of historical threat data to learn from. To help enhance the existing practices of using analytics, machine learning, and artificial intelligence methods to detect the network threats, we propose an integrated modelling framework, where Knowledge Graph is used to analyze the users' activity patterns, Imbalanced Learning techniques are used to prune and weigh Knowledge Graph, and LLM is used to retrieve and interpret the users' activities from Knowledge Graph. The proposed framework is applied to Agile Threat Detection through Online Sequential Learning. The preliminary results show the improved threat capture rate by 3%-4% and the increased interpretabilities of risk predictions based on the users' activities.","authors":["Lili Zhang","Quanyan Zhu","Herman Ray","Ying Xie"],"url":"https://arxiv.org/abs/2501.16393"}
{"created":"2025-05-15","title":"On the Partitioning of GPU Power among Multi-Instances","abstract":"Efficient power management in cloud data centers is essential for reducing costs, enhancing performance, and minimizing environmental impact. GPUs, critical for tasks like machine learning (ML) and GenAI, are major contributors to power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU utilization by enabling isolated partitions with per-partition resource tracking, facilitating GPU sharing by multiple tenants. However, accurately apportioning GPU power consumption among MIG instances remains challenging due to a lack of hardware support. This paper addresses this challenge by developing software methods to estimate power usage per MIG partition. We analyze NVIDIA GPU utilization metrics and find that light-weight methods with good accuracy can be difficult to construct. We hence explore the use of ML-based power models to enable accurate, partition-level power estimation. Our findings reveal that a single generic offline power model or modeling method is not applicable across diverse workloads, especially with concurrent MIG usage, and that online models constructed using partition-level utilization metrics of workloads under execution can significantly improve accuracy. Using NVIDIA A100 GPUs, we demonstrate this approach for accurate partition-level power estimation for workloads including matrix multiplication and Large Language Model inference, contributing to transparent and fair carbon reporting.","authors":["Tirth Vamja","Kaustabha Ray","Felix George","UmaMaheswari C Devi"],"url":"https://arxiv.org/abs/2501.17752"}
{"created":"2025-05-15","title":"The Core of Approval-Based Committee Elections with Few Seats","abstract":"In an approval-based committee election, the goal is to select a committee consisting of $k$ out of $m$ candidates, based on $n$ voters who each approve an arbitrary number of the candidates. The core of such an election consists of all committees that satisfy a certain stability property which implies proportional representation. In particular, committees in the core cannot be \"objected to\" by a coalition of voters who is underrepresented. The notion of the core was proposed in 2016, but it has remained an open problem whether it is always non-empty. We prove that core committees always exist when $k \\le 8$, for any number of candidates $m$ and any number of voters $n$, by showing that the Proportional Approval Voting (PAV) rule due to Thiele [1895] always satisfies the core when $k \\le 7$ and always selects at least one committee in the core when $k = 8$. We also develop an artificial rule based on recursive application of PAV, and use it to show that the core is non-empty whenever there are $m \\le 15$ candidates, for any committee size $k \\le m$ and any number of voters $n$. These results are obtained with the help of computer search using linear programs.","authors":["Dominik Peters"],"url":"https://arxiv.org/abs/2501.18304"}
{"created":"2025-05-15","title":"Convolutional Fourier Analysis Network (CFAN): A Unified Time-Frequency Approach for ECG Classification","abstract":"Machine learning has revolutionized biomedical signal analysis, particularly in electrocardiogram (ECG) classification. While convolutional neural networks (CNNs) excel at automatic feature extraction, the optimal integration of time- and frequency-domain information remains unresolved. This study introduces the Convolutional Fourier Analysis Network (CFAN), a novel architecture that unifies time-frequency analysis by embedding Fourier principles directly into CNN layers. We evaluate CFAN against four benchmarks - spectrogram-based 2D CNN (SPECT); 1D CNN (CNN1D); Fourier-based 1D CNN (FFT1D); and CNN1D with integrated Fourier Analysis Network (CNN1D-FAN) - across three ECG tasks: arrhythmia classification (MIT-BIH), identity recognition (ECG-ID), and apnea detection (Apnea-ECG). CFAN achieved state-of-the-art performance, surpassing all competing methods with accuracies of 98.95% (MIT-BIH), 96.83% (ECG-ID), and 95.01% (Apnea-ECG). Notably, on ECG-ID and Apnea-ECG, CFAN demonstrated statistically significant improvements over the second-best method (CNN1D-FAN, $p \\leq 0.02$), further validating its superior performance. Key innovations include CONV-FAN blocks that combine sine, cosine and GELU activations in convolutional layers to capture periodic features and joint time-frequency learning without spectrogram conversion. Our results highlight CFAN's potential for broader biomedical and signal classification applications.","authors":["Sam Jeong","Hae Yong Kim"],"url":"https://arxiv.org/abs/2502.00497"}
{"created":"2025-05-15","title":"Learning Traffic Anomalies from Generative Models on Real-Time Observations","abstract":"Accurate detection of traffic anomalies is crucial for effective urban traffic management and congestion mitigation. We use the Spatiotemporal Generative Adversarial Network (STGAN) framework combining Graph Neural Networks and Long Short-Term Memory networks to capture complex spatial and temporal dependencies in traffic data. We apply STGAN to real-time, minute-by-minute observations from 42 traffic cameras across Gothenburg, Sweden, collected over several months in 2020. The images are processed to compute a flow metric representing vehicle density, which serves as input for the model. Training is conducted on data from April to November 2020, and validation is performed on a separate dataset from November 14 to 23, 2020. Our results demonstrate that the model effectively detects traffic anomalies with high precision and low false positive rates. The detected anomalies include camera signal interruptions, visual artifacts, and extreme weather conditions affecting traffic flow.","authors":["Fotis I. Giasemis","Alexandros Sopasakis"],"url":"https://arxiv.org/abs/2502.01391"}
{"created":"2025-05-15","title":"Hybrid Resolver Model Generalization for Fault Condition Modeling: A Promising Tool for Reliability Study","abstract":"Resolvers, like all electromagnetic devices, are constantly under investigation, both operationally and structurally. In this regard, proposing a modeling methodology that can save significant time without compromising accuracy is a big honor. In this study, a generalized hybrid model is suggested that, in addition to the above benefits, has sufficient capability to ease reliability study in the field of resolvers, where a large number of faulty conditions must be investigated under different operating conditions, including changes in angular velocity, voltage, and frequency of excitation; all of which are highlighted in the context of fault coverage. This model also serves as a promising tool for generating large datasets, which is advantageous for fault diagnosis. A resolver with a non-uniform air gap is chosen as a case study to challenge the suggested model, particularly in relation to eccentricity faults. We generalize the suggested model to account for the most common faulty conditions of resolvers: in-turn short circuits in signal and excitation windings, as well as static and dynamic eccentricity faults. The close agreement between the results of the suggested model and those from Time-Stepping Finite Element Analysis (TS-FEA), along with significant time savings in both healthy and faulty conditions, highlights the generality and proficiency of the suggested model. Finally, the case study is prototyped, and we verify the accuracy of the suggested model experimentally.","authors":["MohammadSadegh KhajueeZadeh","Farid Tootoonchian","Ali Pourghoraba"],"url":"https://arxiv.org/abs/2502.02323"}
{"created":"2025-05-15","title":"FAS: Fast ANN-SNN Conversion for Spiking Large Language Models","abstract":"Spiking Large Language Models have been shown as a good alternative to LLMs in various scenarios. Existing methods for creating Spiking LLMs, i.e., direct training and ANN-SNN conversion, often suffer from performance degradation and relatively high computational costs. To address these issues, we propose a novel Fast ANN-SNN conversion strategy (FAS) that transforms LLMs into spiking LLMs in two stages. The first stage employs a full-parameter fine-tuning of pre-trained models, so it does not need any direct training from scratch. The second stage introduces a coarse-to-fine calibration method to reduce conversion errors and improve accuracy. Experiments on both language and vision-language tasks across four different scales of LLMs demonstrate that FAS can achieve state-of-the-art performance yet with significantly reduced inference latency and computational costs. Notably, FAS only takes eight timesteps to achieve an accuracy of 3\\% higher than that of the OPT-7B model, while reducing energy consumption by 96.63\\%. The source code is available at https://github.com/lc783/FAS","authors":["Long Chen","Xiaotian Song","Andy Song","BaDong Chen","Jiancheng Lv","Yanan Sun"],"url":"https://arxiv.org/abs/2502.04405"}
{"created":"2025-05-15","title":"RobotMover: Learning to Move Large Objects From Human Demonstrations","abstract":"Moving large objects, such as furniture or appliances, is a critical capability for robots operating in human environments. This task presents unique challenges, including whole-body coordination to avoid collisions and managing the dynamics of bulky, heavy objects. In this work, we present RobotMover, a learning-based system for large object manipulation that uses human-object interaction demonstrations to train robot control policies. RobotMover formulates the manipulation problem as imitation learning using a simplified spatial representation called the Interaction Chain, which captures essential interaction dynamics in a way that generalizes across different robot bodies. We incorporate this Interaction Chain into a reward function and train policies in simulation using domain randomization to enable zero-shot transfer to real-world robots. The resulting policies allow a Spot robot to manipulate various large objects, including chairs, tables, and standing lamps. Through extensive experiments in both simulation and the real world, we show that RobotMover achieves strong performance in terms of capability, robustness, and controllability, outperforming both learned and teleoperation baselines. The system also supports practical applications by combining learned policies with simple planning modules to perform long-horizon object transport and rearrangement tasks.","authors":["Tianyu Li","Joanne Truong","Jimmy Yang","Alexander Clegg","Akshara Rai","Sehoon Ha","Xavier Puig"],"url":"https://arxiv.org/abs/2502.05271"}
{"created":"2025-05-15","title":"Graph-structured Small Molecule Drug Discovery Through Deep Learning: Progress, Challenges, and Opportunities","abstract":"Due to their excellent drug-like and pharmacokinetic properties, small molecule drugs are widely used to treat various diseases, making them a critical component of drug discovery. In recent years, with the rapid development of deep learning (DL) techniques, DL-based small molecule drug discovery methods have achieved excellent performance in prediction accuracy, speed, and complex molecular relationship modeling compared to traditional machine learning approaches. These advancements enhance drug screening efficiency and optimization and provide more precise and effective solutions for various drug discovery tasks. Contributing to this field's development, this paper aims to systematically summarize and generalize the recent key tasks and representative techniques in graph-structured small molecule drug discovery in recent years. Specifically, we provide an overview of the major tasks in small molecule drug discovery and their interrelationships. Next, we analyze the six core tasks, summarizing the related methods, commonly used datasets, and technological development trends. Finally, we discuss key challenges, such as interpretability and out-of-distribution generalization, and offer our insights into future research directions for small molecule drug discovery.","authors":["Kun Li","Yida Xiong","Hongzhi Zhang","Xiantao Cai","Jia Wu","Bo Du","Wenbin Hu"],"url":"https://arxiv.org/abs/2502.08975"}
{"created":"2025-05-15","title":"Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples","abstract":"The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.","authors":["Chengqian Gao","Haonan Li","Liu Liu","Zeke Xie","Peilin Zhao","Zhiqiang Xu"],"url":"https://arxiv.org/abs/2502.09650"}
{"created":"2025-05-15","title":"PropNet: a White-Box and Human-Like Network for Sentence Representation","abstract":"Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.","authors":["Fei Yang"],"url":"https://arxiv.org/abs/2502.10725"}
{"created":"2025-05-15","title":"Synthetic Politics: Prevalence, Spreaders, and Emotional Reception of AI-Generated Political Images on X","abstract":"Despite widespread concerns about the risks of AI-generated content (AIGC) to the integrity of social media discourse, little is known about its scale and scope, the actors responsible for its dissemination online, and the user responses it elicits. In this work, we measure and characterize the prevalence, spreaders, and emotional reception of AI-generated political images. Analyzing a large-scale dataset from Twitter/X related to the 2024 U.S. Presidential Election, we find that approximately 12% of shared images are detected as AI-generated, and around 10% of users are responsible for sharing 80% of AI-generated images. AIGC superspreaders--defined as the users who not only share a high volume of AI-generated images but also receive substantial engagement through retweets--are more likely to be X Premium subscribers, have a right-leaning orientation, and exhibit automated behavior. Their profiles contain a higher proportion of AI-generated images than non-superspreaders, and some engage in extreme levels of AIGC sharing. Moreover, superspreaders' AI image tweets elicit more positive and less toxic responses than their non-AI image tweets. This study serves as one of the first steps toward understanding the role generative AI plays in shaping online socio-political environments and offers implications for platform governance.","authors":["Zhiyi Chen","Jinyi Ye","Beverlyn Tsai","Emilio Ferrara","Luca Luceri"],"url":"https://arxiv.org/abs/2502.11248"}
{"created":"2025-05-15","title":"Soft Arm-Motor Thrust Characterization for a Pneumatically Actuated Soft Morphing Quadrotor","abstract":"In this work, an experimental characterization of the configuration space of a soft, pneumatically actuated morphing quadrotor is presented, with a focus on precise thrust characterization of its flexible arms, considering the effect of downwash. Unlike traditional quadrotors, the soft drone has pneumatically actuated arms, introducing complex, nonlinear interactions between motor thrust and arm deformation, which make precise control challenging. The silicone arms are actuated using differential pressure to achieve flexibility and thus have a variable workspace compared to their fixed counter-parts. The deflection of the soft arms during compression and expansion is controlled throughout the flight. However, in real time, the downwash from the motor attached at the tip of the soft arm generates a significant and random disturbance on the arm. This disturbance affects both the desired deflection of the arm and the overall stability of the system. To address this factor, an experimental characterization of the effect of downwash on the deflection angle of the arm is conducted.","authors":["Vidya Sumathy","Jakub Haluska","George Nikolakopoulos"],"url":"https://arxiv.org/abs/2502.12716"}
{"created":"2025-05-15","title":"Pushing the Limits of the Reactive Affine Shaker Algorithm to Higher Dimensions","abstract":"Bayesian Optimization (BO) for the minimization of expensive functions of continuous variables uses all the knowledge acquired from previous samples (${\\boldsymbol x}_i$ and $f({\\boldsymbol x}_i)$ values) to build a surrogate model based on Gaussian processes. The surrogate is then exploited to define the next point to sample, through a careful balance of exploration and exploitation. Initially intended for low-dimensional spaces, BO has recently been modified and used also for very large-dimensional spaces (up to about one thousand dimensions).","authors":["Roberto Battiti","Mauro Brunato"],"url":"https://arxiv.org/abs/2502.12877"}
{"created":"2025-05-15","title":"An ocean front detection and tracking algorithm","abstract":"Existing ocean front detection methods--including histogram-based variance analysis, Lyapunov exponent, gradient thresholding, and machine learning--suffer from critical limitations: discontinuous outputs, over-detection, reliance on single-threshold decisions, and lack of open-source implementations. To address these challenges, this paper proposes the Bayesian Front Detection and Tracking framework with Metric Space Analysis (BFDT-MSA). The framework introduces three innovations: (1) a Bayesian decision mechanism that integrates gradient priors and field operators to eliminate manual threshold sensitivity; (2) morphological refinement algorithms for merging fragmented fronts, deleting spurious rings, and thinning frontal zones to pixel-level accuracy; and (3) a novel metric space definition for temporal front tracking, enabling systematic analysis of front evolution. Validated on global SST data (2022--2024), BFDT-MSA reduces over-detection by $73\\%$ compared to histogram-based methods while achieving superior intensity ($0.16^\\circ$C/km), continuity, and spatiotemporal coherence. The open-source release bridges a critical gap in reproducible oceanographic research.","authors":["Yishuo Wang","Feng Zhou","Qicheng Meng","Muping Zhou","Zhijun Hu","Chengqing Zhang","Tianhao Zhao"],"url":"https://arxiv.org/abs/2502.15250"}
{"created":"2025-05-15","title":"Activation Steering in Neural Theorem Provers","abstract":"Large Language Models (LLMs) have shown promise in proving formal theorems using proof assistants like Lean. However, current state of the art language models struggles to predict next step in proofs leading practitioners to use different sampling techniques to improve LLMs capabilities. We observe that the LLM is capable of predicting the correct tactic; however, it faces challenges in ranking it appropriately within the set of candidate tactics, affecting the overall selection process. To overcome this hurdle, we use activation steering to guide LLMs responses to improve the generations at the time of inference. Our results suggest that activation steering offers a promising lightweight alternative to specialized fine-tuning for enhancing theorem proving capabilities in LLMs, particularly valuable in resource-constrained environments.","authors":["Shashank Kirtania"],"url":"https://arxiv.org/abs/2502.15507"}
{"created":"2025-05-15","title":"InductionBench: LLMs Fail in the Simplest Complexity Class","abstract":"Large language models (LLMs) have shown remarkable improvements in reasoning and many existing benchmarks have been addressed by models such as o1 and o3 either fully or partially. However, a majority of these benchmarks emphasize deductive reasoning, including mathematical and coding tasks in which rules such as mathematical axioms or programming syntax are clearly defined, based on which LLMs can plan and apply these rules to arrive at a solution. In contrast, inductive reasoning, where one infers the underlying rules from observed data, remains less explored. Such inductive processes lie at the heart of scientific discovery, as they enable researchers to extract general principles from empirical observations. To assess whether LLMs possess this capacity, we introduce InductionBench, a new benchmark designed to evaluate the inductive reasoning ability of LLMs. Our experimental findings reveal that even the most advanced models available struggle to master the simplest complexity classes within the subregular hierarchy of functions, highlighting a notable deficiency in current LLMs' inductive reasoning capabilities. Coda and data are available https://github.com/Wenyueh/inductive_reasoning_benchmark.","authors":["Wenyue Hua","Tyler Wong","Sun Fei","Liangming Pan","Adam Jardine","William Yang Wang"],"url":"https://arxiv.org/abs/2502.15823"}
{"created":"2025-05-15","title":"An Analytical Emotion Framework of Rumour Threads on Social Media","abstract":"Rumours in online social media pose significant risks to modern society, motivating the need for better understanding of how they develop. We focus specifically on the interface between emotion and rumours in threaded discourses, building on the surprisingly sparse literature on the topic which has largely focused on single aspect of emotions within the original rumour posts themselves, and largely overlooked the comparative differences between rumours and non-rumours. In this work, we take one step further to provide a comprehensive analytical emotion framework with multi-aspect emotion detection, contrasting rumour and non-rumour threads and provide both correlation and causal analysis of emotions. We applied our framework on existing widely-used rumour datasets to further understand the emotion dynamics in online social media threads. Our framework reveals that rumours trigger more negative emotions (e.g., anger, fear, pessimism), while non-rumours evoke more positive ones. Emotions are contagious, rumours spread negativity, non-rumours spread positivity. Causal analysis shows surprise bridges rumours and other emotions; pessimism comes from sadness and fear, while optimism arises from joy and love.","authors":["Rui Xing","Boyang Sun","Kun Zhang","Preslav Nakov","Timothy Baldwin","Jey Han Lau"],"url":"https://arxiv.org/abs/2502.16560"}
{"created":"2025-05-15","title":"Learning Autonomy: Off-Road Navigation Enhanced by Human Input","abstract":"In the area of autonomous driving, navigating off-road terrains presents a unique set of challenges, from unpredictable surfaces like grass and dirt to unexpected obstacles such as bushes and puddles. In this work, we present a novel learning-based local planner that addresses these challenges by directly capturing human driving nuances from real-world demonstrations using only a monocular camera. The key features of our planner are its ability to navigate in challenging off-road environments with various terrain types and its fast learning capabilities. By utilizing minimal human demonstration data (5-10 mins), it quickly learns to navigate in a wide array of off-road conditions. The local planner significantly reduces the real world data required to learn human driving preferences. This allows the planner to apply learned behaviors to real-world scenarios without the need for manual fine-tuning, demonstrating quick adjustment and adaptability in off-road autonomous driving technology.","authors":["Akhil Nagariya","Dimitar Filev","Srikanth Saripalli","Gaurav Pandey"],"url":"https://arxiv.org/abs/2502.18760"}
{"created":"2025-05-15","title":"UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route Generation on a Large Scales","abstract":"The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a cutting-edge advancement in aerial robotics, designed to enhance communication and operational efficiency for unmanned aerial vehicles (UAVs). By integrating advanced planning capabilities, the system addresses the Traveling Salesman Problem (TSP) to optimize flight paths, reducing the total trajectory length by 18.5\\% compared to traditional methods. Additionally, the incorporation of the A* algorithm enables robust obstacle avoidance, ensuring safe and efficient navigation in complex environments. The system leverages satellite imagery processing combined with the Visual Language Model (VLM) and GPT's natural language processing capabilities, allowing users to generate detailed flight plans through simple text commands. This seamless fusion of visual and linguistic analysis empowers precise decision-making and mission planning, making UAV-VLPA* a transformative tool for modern aerial operations. With its unmatched operational efficiency, navigational safety, and user-friendly functionality, UAV-VLPA* sets a new standard in autonomous aerial robotics, paving the way for future innovations in the field.","authors":["Oleg Sautenkov","Aibek Akhmetkazy","Yasheerah Yaqoot","Muhammad Ahsan Mustafa","Grik Tadevosyan","Artem Lykov","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2503.02454"}
{"created":"2025-05-15","title":"PriFFT: Privacy-preserving Federated Fine-tuning of Large Language Models via Hybrid Secret Sharing","abstract":"Fine-tuning large language models (LLMs) raises privacy concerns due to the risk of exposing sensitive training data. Federated learning (FL) mitigates this risk by keeping training samples on local devices, while facing the following problems in privacy-preserving federated fine-tuning. (i) Recent studies show that adversaries can still infer private information in FL. (ii) LLM parameters are shared publicly during federated fine-tuning, while developers are often reluctant to disclose these parameters, posing further security challenges. (iii) Existing works focus on secure inference of LLMs but do not consider privacy-preserving fine-tuning. Inspired by the above problems, we propose PriFFT, a privacy-preserving federated fine-tuning mechanism, to protect both the model parameters and users' privacy. Due to considerable LLM parameters, we present hybrid secret sharing combining arithmetic secret sharing (ASS) and function secret sharing (FSS) to build secure operations and implement secure layers and activation for privacy-preserving fine-tuning. To improve the efficiency of privacy-preserving federated fine-tuning of LLMs, we optimize several secure computation protocols based on FSS, including reciprocal calculation, tensor products, natural exponentiation, softmax, sigmoid, hyperbolic tangent, and dropout. The hybrid secret sharing enables PriFFT to apply our optimized FSS protocols while combining ASS protocols to support complex computation without extra communication. The optimized protocols reduce execution time up to 62.5% and communication overhead up to 70.7% compared to existing protocols. Besides, PriFFT reduces execution time and communication overhead in privacy-preserving fine-tuning up to 59.1%$ and 77.0%$ without accuracy drop compared to the existing secret sharing methods.","authors":["Zhichao You","Xuewen Dong","Ke Cheng","Xutong Mu","Jiaxuan Fu","Shiyang Ma","Qiang Qu","Yulong Shen"],"url":"https://arxiv.org/abs/2503.03146"}
{"created":"2025-05-15","title":"Distributed Certifiably Correct Range-Aided SLAM","abstract":"Reliable simultaneous localization and mapping (SLAM) algorithms are necessary for safety-critical autonomous navigation. In the communication-constrained multi-agent setting, navigation systems increasingly use point-to-point range sensors as they afford measurements with low bandwidth requirements and known data association. The state estimation problem for these systems takes the form of range-aided (RA) SLAM. However, distributed algorithms for solving the RA-SLAM problem lack formal guarantees on the quality of the returned estimate. To this end, we present the first distributed algorithm for RA-SLAM that can efficiently recover certifiably globally optimal solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA), achieves this via the Riemannian Staircase method, where computational procedures developed for distributed certifiably correct pose graph optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's efficacy on real-world multi-agent datasets by achieving absolute trajectory errors comparable to those of a state-of-the-art centralized certifiably correct RA-SLAM algorithm. Additionally, we perform a parametric study on the structure of the RA-SLAM problem using synthetic data, revealing how common parameters affect DCORA's performance.","authors":["Alexander Thoms","Alan Papalia","Jared Velasquez","David M. Rosen","Sriram Narasimhan"],"url":"https://arxiv.org/abs/2503.03192"}
{"created":"2025-05-15","title":"Goal-Oriented Random Access (GORA)","abstract":"We propose Goal-Oriented Random Access (GORA), where transmitters jointly optimize what to send and when to access the shared channel to a common access point, considering the ultimate goal of the information transfer at its final destination. This goal is captured by an objective function, which is expressed as a general (not necessarily monotonic) function of the Age of Information. Our findings reveal that, under certain conditions, it may be desirable for transmitters to delay channel access intentionally and, when accessing the channel, transmit aged samples to reach a specific goal at the receiver.","authors":["Ahsen Topbas","Cagri Ari","Onur Kaya","Elif Uysal"],"url":"https://arxiv.org/abs/2503.03291"}
{"created":"2025-05-15","title":"A Modified Hermite Radial Basis Function for Accurate Interpolation","abstract":"Accurate interpolation of functions and derivatives is crucial in solving partial differential equations (PDEs). The Radial Basis Function (RBF) method has become an extremely popular and robust approach for interpolation on scattered data. Hermite Radial Basis Function (HRBF) methods are an extension of the RBF and improve the overall accuracy by incorporating both function and derivative information. Unfortunately, the use of infinitely smooth kernels, such as the Gaussian, suffers from ill-conditioning at low to moderate shape parameters. This work proposes a Modified HRBF (MHRBF) method that introduces an additional polynomial term to balance kernel behavior, improving accuracy while maintaining or lowering computational cost. The numerical results demonstrate that MHRBF achieves lower errors with fewer unknowns compared to the original HRBF, making it a robust alternative for stable and accurate RBF-based interpolation.","authors":["Amirhossein Fashamiha","David Salac"],"url":"https://arxiv.org/abs/2503.05752"}
{"created":"2025-05-15","title":"An asymptotic preserving scheme satisfying entropy stability for the barotropic Euler system","abstract":"In this paper we study structure-preserving numerical methods for low Mach number barotropic Euler equations. Besides their asymptotic preserving properties that are crucial in order to obtain uniformly consistent and stable approximations of the Euler equations in their singular limit as the Mach number approaches zero, our aim is also to preserve discrete entropy stability. Suitable acoustic/advection splitting approach combined with time implicit-explicit approximations are used to achieve the asymptotic preserving property. The entropy stability of different space discretisation strategies is studied for different values of Mach number and is validated by the numerical experiments.","authors":["Megala Anandan","M\\'aria Luk\\'a\\v{c}ov\\'a-Medvid'ov\\'a","S. V. Raghurama Rao"],"url":"https://arxiv.org/abs/2503.07284"}
{"created":"2025-05-15","title":"Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference","abstract":"Survey research plays a crucial role in studies by capturing consumer preferences and informing policy decisions. Stated preference (SP) surveys help researchers understand how individuals make trade-offs in hypothetical, potentially futuristic, scenarios. However, traditional methods are costly, time-consuming, and affected by respondent fatigue and ethical constraints. Large language models (LLMs) have shown remarkable capabilities in generating human-like responses, prompting interest in their use in survey research. This study investigates LLMs for simulating consumer choices in energy-related SP surveys and explores their integration into data collection and analysis workflows. Test scenarios were designed to assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and aggregated levels, considering prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, model types, integration with traditional choice models, and potential biases. While LLMs achieve accuracy above random guessing, performance remains insufficient for practical simulation use. Cloud-based LLMs do not consistently outperform smaller local models. DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Previous SP choices are the most effective input; longer prompts with more factors reduce accuracy. Mixed logit models can support LLM prompt refinement. Reasoning LLMs show potential in data analysis by indicating factor significance, offering a qualitative complement to statistical models. Despite limitations, pre-trained LLMs offer scalability and require minimal historical data. Future work should refine prompts, further explore CoT reasoning, and investigate fine-tuning techniques.","authors":["Han Wang","Jacek Pawlak","Aruna Sivakumar"],"url":"https://arxiv.org/abs/2503.10652"}
{"created":"2025-05-15","title":"Reinforcement Learning Outperforms Supervised Fine-Tuning: A Case Study on Audio Question Answering","abstract":"Recently, reinforcement learning (RL) has been shown to greatly enhance the reasoning capabilities of large language models (LLMs), and RL-based approaches have been progressively applied to visual multimodal tasks. However, the audio modality has largely been overlooked in these developments. Thus, we conduct a series of RL explorations in audio understanding and reasoning, specifically focusing on the audio question answering (AQA) task. We leverage the group relative policy optimization (GRPO) algorithm to Qwen2-Audio-7B-Instruct, and our experiments demonstrated state-of-the-art performance on the MMAU Test-mini benchmark, achieving an accuracy rate of 64.5%. The main findings in this technical report are as follows: 1) The GRPO algorithm can be effectively applied to large audio language models (LALMs), even when the model has only 8.2B parameters; 2) With only 38k post-training samples, RL significantly outperforms supervised fine-tuning (SFT), indicating that RL-based approaches can be effective without large datasets; 3) The explicit reasoning process has not shown significant benefits for AQA tasks, and how to efficiently utilize deep thinking remains an open question for further research; 4) LALMs still lag far behind humans auditory-language reasoning, suggesting that the RL-based approaches warrant further exploration. Our project is available at https://github.com/xiaomi-research/r1-aqa and https://huggingface.co/mispeech/r1-aqa.","authors":["Gang Li","Jizhong Liu","Heinrich Dinkel","Yadong Niu","Junbo Zhang","Jian Luan"],"url":"https://arxiv.org/abs/2503.11197"}
{"created":"2025-05-15","title":"TuneNSearch: a hybrid transfer learning and local search approach for solving vehicle routing problems","abstract":"This paper introduces TuneNSearch, a hybrid transfer learning and local search approach for addressing different variants of vehicle routing problems (VRP). Recently, multi-task learning has gained much attention for solving VRP variants. However, this adaptability often compromises the performance of the models. To address this challenge, we first pre-train a reinforcement learning model on the multi-depot VRP, followed by a short fine-tuning phase to adapt it to different variants. By leveraging the complexity of the multi-depot VRP, the pre-trained model learns richer node representations and gains more transferable knowledge compared to models trained on simpler routing problems, such as the traveling salesman problem. TuneNSearch employs, in the first stage, a Transformer-based architecture, augmented with a residual edge-graph attention network to capture the impact of edge distances and residual connections between layers. This architecture allows for a more precise capture of graph-structured data, improving the encoding of VRP's features. After inference, our model is also coupled with a second stage composed of a local search algorithm, which yields substantial performance gains with minimal computational overhead added. Results show that TuneNSearch outperforms many existing state-of-the-art models trained for each VRP variant, requiring only one-fifth of the training epochs. Our approach demonstrates strong generalization, achieving high performance across different tasks, distributions and problem sizes, thus addressing a long-standing gap in the literature.","authors":["Arthur Corr\\^ea","Crist\\'ov\\~ao Silva","Liming Xu","Alexandra Brintrup","Samuel Moniz"],"url":"https://arxiv.org/abs/2503.12662"}
{"created":"2025-05-15","title":"Kernel-based error bounds of bilinear Koopman surrogate models for nonlinear data-driven control","abstract":"We derive novel deterministic bounds on the approximation error of data-based bilinear surrogate models for unknown nonlinear systems. The surrogate models are constructed using kernel-based extended dynamic mode decomposition to approximate the Koopman operator in a reproducing kernel Hilbert space. Unlike previous methods that require restrictive assumptions on the invariance of the dictionary, our approach leverages kernel-based dictionaries that allow us to control the projection error via pointwise error bounds, overcoming a significant limitation of existing theoretical guarantees. The derived state- and input-dependent error bounds allow for direct integration into Koopman-based robust controller designs with closed-loop guarantees for the unknown nonlinear system. Numerical examples illustrate the effectiveness of the proposed framework.","authors":["Robin Str\\\"asser","Manuel Schaller","Julian Berberich","Karl Worthmann","Frank Allg\\\"ower"],"url":"https://arxiv.org/abs/2503.13407"}
{"created":"2025-05-15","title":"The Hidden Bloat in Machine Learning Systems","abstract":"Software bloat refers to code and features that is not used by a software during runtime. For Machine Learning (ML) systems, bloat is a major contributor to their technical debt leading to decreased performance and resource wastage. In this work, we present, Negativa-ML, a novel tool to identify and remove bloat in ML frameworks by analyzing their shared libraries. Our approach includes novel techniques to detect and locate unnecessary code within device code - a key area overlooked by existing research, which focuses primarily on host code. We evaluate Negativa-ML using four popular ML frameworks across ten workloads over 300 shared libraries. The results demonstrate that the ML frameworks are highly bloated on both the device and host code side. On average, Negativa-ML reduces the device code size in these frameworks by up to 75% and the host code by up to 72%, resulting in total file size reductions of up to 55%. The device code is a primary source of bloat within ML frameworks. Through debloating, we achieve reductions in peak host memory usage, peak GPU memory usage, and execution time by up to 74.6%, 69.6%, and 44.6%, respectively.","authors":["Huaifeng Zhang","Ahmed Ali-Eldin"],"url":"https://arxiv.org/abs/2503.14226"}
{"created":"2025-05-15","title":"Reinforcement Learning-based Heuristics to Guide Domain-Independent Dynamic Programming","abstract":"Domain-Independent Dynamic Programming (DIDP) is a state-space search paradigm based on dynamic programming for combinatorial optimization. In its current implementation, DIDP guides the search using user-defined dual bounds. Reinforcement learning (RL) is increasingly being applied to combinatorial optimization problems and shares several key structures with DP, being represented by the Bellman equation and state-based transition systems. We propose using reinforcement learning to obtain a heuristic function to guide the search in DIDP. We develop two RL-based guidance approaches: value-based guidance using Deep Q-Networks and policy-based guidance using Proximal Policy Optimization. Our experiments indicate that RL-based guidance significantly outperforms standard DIDP and problem-specific greedy heuristics with the same number of node expansions. Further, despite longer node evaluation times, RL guidance achieves better run-time performance than standard DIDP on three of four benchmark domains.","authors":["Minori Narita","Ryo Kuroiwa","J. Christopher Beck"],"url":"https://arxiv.org/abs/2503.16371"}
{"created":"2025-05-15","title":"ICLR Points: How Many ICLR Publications Is One Paper in Each Area?","abstract":"Scientific publications significantly impact academic-related decisions in computer science, where top-tier conferences are particularly influential. However, efforts required to produce a publication differ drastically across various subfields. While existing citation-based studies compare venues within areas, cross-area comparisons remain challenging due to differing publication volumes and citation practices.","authors":["Zhongtang Luo"],"url":"https://arxiv.org/abs/2503.16623"}
{"created":"2025-05-15","title":"Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark","abstract":"Large Language Models (LLMs) have demonstrated considerable potential in general practice. However, existing benchmarks and evaluation frameworks primarily depend on exam-style or simplified question-answer formats, lacking a competency-based structure aligned with the real-world clinical responsibilities encountered in general practice. Consequently, the extent to which LLMs can reliably fulfill the duties of general practitioners (GPs) remains uncertain. In this work, we propose a novel evaluation framework to assess the capability of LLMs to function as GPs. Based on this framework, we introduce a general practice benchmark (GPBench), whose data are meticulously annotated by domain experts in accordance with routine clinical practice standards. We evaluate ten state-of-the-art LLMs and analyze their competencies. Our findings indicate that current LLMs are not yet ready for deployment in such settings without human oversight, and further optimization specifically tailored to the daily responsibilities of GPs is essential.","authors":["Zheqing Li","Yiying Yang","Jiping Lang","Wenhao Jiang","Yuhang Zhao","Shuang Li","Dingqian Wang","Zhu Lin","Xuanna Li","Yuze Tang","Jiexian Qiu","Xiaolin Lu","Hongji Yu","Shuang Chen","Yuhua Bi","Xiaofei Zeng","Yixian Chen","Junrong Chen","Lin Yao"],"url":"https://arxiv.org/abs/2503.17599"}
{"created":"2025-05-15","title":"Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV Cache Quantization","abstract":"Modern Large Language Model serving system batches multiple requests to achieve high throughput, while batching attention operations is challenging, rendering memory bandwidth a critical bottleneck. The community relies on high-end GPUs with multiple high-bandwidth memory channels. Unfortunately, HBM's high bandwidth often comes at the expense of limited memory capacity, which reduces core utilization and increases costs. Recent advancements enabling longer contexts for LLMs have substantially increased the key-value cache size, further intensifying the pressures on memory capacity. The literature has explored KV cache quantization techniques, which commonly use low bitwidth for most values, selectively using higher bitwidth for outlier values. While this approach helps achieve high accuracy and low bitwidth simultaneously, it comes with the limitation that cost for online outlier detection is excessively high, negating the advantages. We propose Oaken, an acceleration solution that achieves high accuracy and high performance simultaneously through co-designing algorithm and hardware. To effectively find a sweet spot in the accuracy-performance trade-off space of KV cache quantization, Oaken employs an online-offline hybrid approach, setting outlier thresholds offline, which are then used to determine the quantization scale online. To translate the proposed algorithmic technique into tangible performance gains, Oaken also comes with custom quantization engines and memory management units that can be integrated with any LLM accelerators. We built an Oaken accelerator on top of an LLM accelerator, LPU, and conducted a comprehensive evaluation. Our experiments show that for a batch size of 256, Oaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU, incurring a minimal accuracy loss of only 0.54\\% on average, compared to state-of-the-art KV cache quantization techniques.","authors":["Minsu Kim","Seongmin Hong","RyeoWook Ko","Soongyu Choi","Hunjong Lee","Junsoo Kim","Joo-Young Kim","Jongse Park"],"url":"https://arxiv.org/abs/2503.18599"}
{"created":"2025-05-15","title":"AdaWorld: Learning Adaptable World Models with Latent Actions","abstract":"World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.","authors":["Shenyuan Gao","Siyuan Zhou","Yilun Du","Jun Zhang","Chuang Gan"],"url":"https://arxiv.org/abs/2503.18938"}
{"created":"2025-05-15","title":"A Spectrum-based Filter Design for Periodic Control of Systems with Time Delay","abstract":"A fully analytical controller design is proposed to tackle a periodic control problem for stable linear systems with an input delay. Applying the internal model control scheme, the controller design reduces to designing a filter, which is done through the placement of poles and zeros. The zeros are placed to compensate for the harmonics and to achieve the desired degree of properness for the filter. For placing the poles, a quasi-optimal procedure is proposed utilizing the standard LQR method. Given the high-dimensionality of the filter due to targeting a large number of harmonics, the design, as well as controller implementation, is performed over a state-space representation. A thorough experimental case study is included to demonstrate both the practical feasibility and effectiveness of the proposed control design. The experimental validation is performed on a physical system, the goal of which is to reject periodic vibrations acting on a mass-spring-damper setup where the sensor and the actuator are non-collocated.","authors":["Can Kutlu Y\\\"uksel","Tom\\'a\\v{s} Vyhl\\'idal","Jaroslav Bu\\v{s}ek","Martin Hrom\\v{c}\\'ik","Silviu-Iulian Niculescu"],"url":"https://arxiv.org/abs/2503.19863"}
{"created":"2025-05-15","title":"UI-R1: Enhancing Efficient Action Prediction of GUI Agents by Reinforcement Learning","abstract":"The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Despite its success in language models, its application in multi-modal domains, particularly in graphic user interface (GUI) agent tasks, remains under-explored. To address this issue, we propose UI-R1, the first framework to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for GUI action prediction tasks. Specifically, UI-R1 introduces a novel rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). For efficient training, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. Experimental results demonstrate that our proposed UI-R1-3B achieves significant improvements over the base model (i.e. Qwen2.5-VL-3B) on both in-domain (ID) and out-of-domain (OOD) tasks, with average accuracy gains of 22.1% on ScreenSpot, 6.0% on ScreenSpot-Pro, and 12.7% on ANDROIDCONTROL. Furthermore, UI-R1-3B delivers competitive performance compared to larger models (e.g., OS-Atlas-7B) trained via supervised fine-tuning (SFT) on 76K samples. We additionally develop an optimized version, UI-R1-E-3B, which significantly improves both grounding efficiency and accuracy. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain. Code website: https://github.com/lll6gg/UI-R1.","authors":["Zhengxi Lu","Yuxiang Chai","Yaxuan Guo","Xi Yin","Liang Liu","Hao Wang","Han Xiao","Shuai Ren","Guanjing Xiong","Hongsheng Li"],"url":"https://arxiv.org/abs/2503.21620"}
{"created":"2025-05-15","title":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks","abstract":"Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.","authors":["Wenqi Zhang","Mengna Wang","Gangao Liu","Xu Huixin","Yiwei Jiang","Yongliang Shen","Guiyang Hou","Zhe Zheng","Hang Zhang","Xin Li","Weiming Lu","Peng Li","Yueting Zhuang"],"url":"https://arxiv.org/abs/2503.21696"}
{"created":"2025-05-15","title":"Video-R1: Reinforcing Video Reasoning in MLLMs","abstract":"Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for incentivizing video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-CoT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 37.1% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All code, models, and data are released in: https://github.com/tulerfeng/Video-R1.","authors":["Kaituo Feng","Kaixiong Gong","Bohao Li","Zonghao Guo","Yibing Wang","Tianshuo Peng","Junfei Wu","Xiaoying Zhang","Benyou Wang","Xiangyu Yue"],"url":"https://arxiv.org/abs/2503.21776"}
{"created":"2025-05-15","title":"OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching","abstract":"Hallucinations are often inevitable in downstream tasks using large language models (LLMs). To tackle the substantial challenge of addressing hallucinations for LLM-based ontology matching (OM) systems, we introduce a new benchmark dataset OAEI-LLM-T. The dataset evolves from seven TBox datasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of ten different LLMs performing OM tasks. These OM-specific hallucinations are organised into two primary categories and six sub-categories. We showcase the usefulness of the dataset in constructing an LLM leaderboard for OM tasks and for fine-tuning LLMs used in OM tasks.","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang","Jing Jiang"],"url":"https://arxiv.org/abs/2503.21813"}
{"created":"2025-05-15","title":"Is analogy enough to draw novel adjective-noun inferences?","abstract":"Recent work (Ross et al., 2025, 2024) has argued that the ability of humans and LLMs respectively to generalize to novel adjective-noun combinations shows that they each have access to a compositional mechanism to determine the phrase's meaning and derive inferences. We study whether these inferences can instead be derived by analogy to known inferences, without need for composition. We investigate this by (1) building a model of analogical reasoning using similarity over lexical items, and (2) asking human participants to reason by analogy. While we find that this strategy works well for a large proportion of the dataset of Ross et al. (2025), there are novel combinations for which both humans and LLMs derive convergent inferences but which are not well handled by analogy. We thus conclude that the mechanism humans and LLMs use to generalize in these cases cannot be fully reduced to analogy, and likely involves composition.","authors":["Hayley Ross","Kathryn Davidson","Najoung Kim"],"url":"https://arxiv.org/abs/2503.24293"}
{"created":"2025-05-15","title":"GPTAQ: Efficient Finetuning-Free Quantization for Asymmetric Calibration","abstract":"We introduce GPTAQ, a novel finetuning-free quantization method for compressing large-scale transformer architectures. Unlike the previous GPTQ method, which independently calibrates each layer, we always match the quantized layer's output to the exact output in the full-precision model, resulting in a scheme that we call asymmetric calibration. Such a scheme can effectively reduce the quantization error accumulated in previous layers. We analyze this problem using optimal brain compression to derive a close-formed solution. The new solution explicitly minimizes the quantization error as well as the accumulated asymmetry error. Furthermore, we utilize various techniques to parallelize the solution calculation, including channel parallelization, neuron decomposition, and Cholesky reformulation for matrix fusion. As a result, GPTAQ is easy to implement, simply using 20 more lines of code than GPTQ but improving its performance under low-bit quantization. Remarkably, on a single GPU, we quantize a 405B language transformer as well as EVA-02, the rank first vision transformer that achieves 90% pretraining Imagenet accuracy. Code is available at Github.","authors":["Yuhang Li","Ruokai Yin","Donghyun Lee","Shiting Xiao","Priyadarshini Panda"],"url":"https://arxiv.org/abs/2504.02692"}
{"created":"2025-05-15","title":"Deconstructing Jazz Piano Style Using Machine Learning","abstract":"Artistic style has been studied for centuries, and recent advances in machine learning create new possibilities for understanding it computationally. However, ensuring that machine-learning models produce insights aligned with the interests of practitioners and critics remains a significant challenge. Here, we focus on musical style, which benefits from a rich theoretical and mathematical analysis tradition. We train a variety of supervised-learning models to identify 20 iconic jazz musicians across a carefully curated dataset of 84 hours of recordings, and interpret their decision-making processes. Our models include a novel multi-input architecture that enables four musical domains (melody, harmony, rhythm, and dynamics) to be analysed separately. These models enable us to address fundamental questions in music theory and also advance the state-of-the-art in music performer identification (94% accuracy across 20 classes). We release open-source implementations of our models and an accompanying web application for exploring musical styles.","authors":["Huw Cheston","Reuben Bance","Peter M. C. Harrison"],"url":"https://arxiv.org/abs/2504.05009"}
{"created":"2025-05-15","title":"Zerrow: True Zero-Copy Arrow Pipelines in Bauplan","abstract":"Bauplan is a FaaS-based lakehouse specifically built for data pipelines: its execution engine uses Apache Arrow for data passing between the nodes in the DAG. While Arrow is known as the \"zero copy format\", in practice, limited Linux kernel support for shared memory makes it difficult to avoid copying entirely. In this work, we introduce several new techniques to eliminate nearly all copying from pipelines: in particular, we implement a new kernel module that performs de-anonymization, thus eliminating a copy to intermediate data. We conclude by sharing our preliminary evaluation on different workloads types, as well as discussing our plan for future improvements.","authors":["Yifan Dai","Jacopo Tagliabue","Andrea Arpaci-Dusseau","Remzi Arpaci-Dusseau","Tyler R. Caraza-Harter"],"url":"https://arxiv.org/abs/2504.06151"}
{"created":"2025-05-15","title":"Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions","abstract":"Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework.","authors":["Xinyi Wang","Taekyung Kim","Bardh Hoxha","Georgios Fainekos","Dimitra Panagou"],"url":"https://arxiv.org/abs/2504.06513"}
{"created":"2025-05-15","title":"Symbolic Parallel Composition for Multi-language Protocol Verification","abstract":"The implementation of security protocols often combines different languages. This practice, however, poses a challenge to traditional verification techniques, which typically assume a single-language environment and, therefore, are insufficient to handle challenges presented by the interplay of different languages. To address this issue, we establish principles for combining multiple programming languages operating on different atomic types using a symbolic execution semantics. This facilitates the (parallel) composition of labeled transition systems, improving the analysis of complex systems by streamlining communication between diverse programming languages. By treating the Dolev-Yao (DY) model as a symbolic abstraction, our approach eliminates the need for translation between different base types, such as bitstrings and DY terms. Our technique provides a foundation for securing interactions in multi-language environments, enhancing program verification and system analysis in complex, interconnected systems.","authors":["Faezeh Nasrabadi","Robert K\\\"unnemann","Hamed Nemati"],"url":"https://arxiv.org/abs/2504.06833"}
{"created":"2025-05-15","title":"Towards More Efficient, Robust, Instance-adaptive, and Sequential Decision making","abstract":"The primary goal of my Ph.D. study is to develop provably efficient and practical algorithms for data-driven sequential decision-making under uncertainty. My work focuses on reinforcement learning (RL), multi-armed bandits, and their applications, including recommendation systems, computer networks, video analytics, and large language models (LLMs). Sequential decision-making methods, such as bandits and RL, have demonstrated remarkable success - ranging from outperforming human players in complex games like Atari and Go to advancing robotics, recommendation systems, and fine-tuning LLMs. Despite these successes, many established algorithms rely on idealized models that can fail under model misspecifications or adversarial perturbations, particularly in settings where accurate prior knowledge of the underlying model class is unavailable or where malicious users operate within dynamic systems. These challenges are pervasive in real-world applications, where robust and adaptive solutions are critical. Furthermore, while worst-case guarantees provide theoretical reliability, they often fail to capture instance-dependent performance, which can lead to more efficient and practical solutions. Another key challenge lies in generalizing to new, unseen environments, a crucial requirement for deploying these methods in dynamic and unpredictable settings. To address these limitations, my research aims to develop more efficient, robust, instance-adaptive, and generalizable sequential decision-making algorithms for both reinforcement learning and bandits. Towards this end, I focus on developing more efficient, robust, instance-adaptive, and generalizable for both general reinforcement learning (RL) and bandits.","authors":["Zhiyong Wang"],"url":"https://arxiv.org/abs/2504.09192"}
{"created":"2025-05-15","title":"Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling","abstract":"The most widely used generative models map noise and data distributions by matching flows or scores. However, they struggle to incorporate partial observations and additional priors--something energy-based models (EBMs) handle elegantly by simply adding corresponding scalar energy terms. We address this issue by proposing Energy Matching, a framework that endows flow-based approaches with the flexibility of EBMs. Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 and ImageNet generation in terms of fidelity, while retaining simulation-free training of transport-based approaches away from the data manifold. Furthermore, we leverage the method's flexibility to introduce an interaction energy that supports diverse mode exploration, which we demonstrate in a controlled protein-generation setting. Our approach focuses on learning a scalar potential energy--without time-conditioning, auxiliary generators, or additional networks--which marks a significant departure from recent EBM methods. We believe that this simplified framework significantly advances EBMs capabilities and paves the way for their wider adoption in generative modeling across diverse domains.","authors":["Michal Balcerak","Tamaz Amiranashvili","Antonio Terpin","Suprosanna Shit","Sebastian Kaltenbach","Petros Koumoutsakos","Bjoern Menze"],"url":"https://arxiv.org/abs/2504.10612"}
{"created":"2025-05-15","title":"Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition","abstract":"Numerical solvers for PDEs face challenges in balancing computational cost and accuracy, particularly for multiscale and dynamical systems. Neural operators (NOs) can significantly speed up simulations; however, they face challenges such as error accumulation for dynamical systems and limited generalization in multiphysics problems. This work introduces a novel hybrid framework that integrates PI-NO with finite element method (FE) through domain decomposition and leverages numerical analysis for time marching. The core innovation lies in efficient coupling FE and NO subdomains via a Schwarz alternating method: regions with complex, nonlinear, or high-gradient behavior are resolved using a pretrained NO, while the remainder is handled by conventional FE. To address the challenges of dynamic systems, we embed a time-stepping scheme directly into the NO architecture, substantially reducing long-term error propagation. Also, an adaptive subdomain evolution strategy enables the ML resolved region to expand dynamically, capturing emerging fine scale features without remeshing. The framework efficacy has been validated across a range of problems, spanning static, quasi-static, and dynamic regimes (e.g., linear elasticity, hyperelasticity, and elastodynamics), demonstrating accelerated convergence (up to 20% improvement in convergence compared to conventional FE coupling) while preserving solution fidelity with error margins consistently below 1%. Our study shows that our hybrid solver: (1) maintains solution continuity across subdomain interfaces, (2) reduces computational costs by eliminating fine mesh requirements, (3) mitigates error accumulation in time dependent simulations, and (4) enables automatic adaptation to evolving physical phenomena. This work bridges the gap between numerical methods and AI-driven surrogates, offering a scalable pathway for high-fidelity multiscale simulations.","authors":["Wei Wang","Maryam Hakimzadeh","Haihui Ruan","Somdatta Goswami"],"url":"https://arxiv.org/abs/2504.11383"}
{"created":"2025-05-15","title":"Multi-Agent Reinforcement Learning Simulation for Environmental Policy Synthesis","abstract":"Climate policy development faces significant challenges due to deep uncertainty, complex system dynamics, and competing stakeholder interests. Climate simulation methods, such as Earth System Models, have become valuable tools for policy exploration. However, their typical use is for evaluating potential polices, rather than directly synthesizing them. The problem can be inverted to optimize for policy pathways, but the traditional optimization approaches often struggle with non-linear dynamics, heterogeneous agents, and comprehensive uncertainty quantification. We propose a framework for augmenting climate simulations with Multi-Agent Reinforcement Learning (MARL) to address these limitations. We identify key challenges at the interface between climate simulations and the application of MARL in the context of policy synthesis, including reward definition, scalability with increasing agents and state spaces, uncertainty propagation across linked systems, and solution validation. Additionally, we discuss challenges in making MARL-derived solutions interpretable and useful for policy-makers. Our framework provides a foundation for more sophisticated climate policy exploration while acknowledging important limitations and areas for future research.","authors":["James Rudd-Jones","Mirco Musolesi","Mar\\'ia P\\'erez-Ortiz"],"url":"https://arxiv.org/abs/2504.12777"}
{"created":"2025-05-15","title":"DiffOG: Differentiable Policy Trajectory Optimization with Generalizability","abstract":"Imitation learning-based visuomotor policies excel at manipulation tasks but often produce suboptimal action trajectories compared to model-based methods. Directly mapping camera data to actions via neural networks can result in jerky motions and difficulties in meeting critical constraints, compromising safety and robustness in real-world deployment. For tasks that require high robustness or strict adherence to constraints, ensuring trajectory quality is crucial. However, the lack of interpretability in neural networks makes it challenging to generate constraint-compliant actions in a controlled manner. This paper introduces differentiable policy trajectory optimization with generalizability (DiffOG), a learning-based trajectory optimization framework designed to enhance visuomotor policies. By leveraging the proposed differentiable formulation of trajectory optimization with transformer, DiffOG seamlessly integrates policies with a generalizable optimization layer. DiffOG refines action trajectories to be smoother and more constraint-compliant while maintaining alignment with the original demonstration distribution, thus avoiding degradation in policy performance. We evaluated DiffOG across 11 simulated tasks and 2 real-world tasks. The results demonstrate that DiffOG significantly enhances the trajectory quality of visuomotor policies while having minimal impact on policy performance, outperforming trajectory processing baselines such as greedy constraint clipping and penalty-based trajectory optimization. Furthermore, DiffOG achieves superior performance compared to existing constrained visuomotor policy. Please visit the project website for more details: https://zhengtongxu.github.io/diffog-website/.","authors":["Zhengtong Xu","Zichen Miao","Qiang Qiu","Zhe Zhang","Yu She"],"url":"https://arxiv.org/abs/2504.13807"}
{"created":"2025-05-15","title":"PRISM: A Unified Framework for Photorealistic Reconstruction and Intrinsic Scene Modeling","abstract":"We present PRISM, a unified framework that enables multiple image generation and editing tasks in a single foundational model. Starting from a pre-trained text-to-image diffusion model, PRISM proposes an effective fine-tuning strategy to produce RGB images along with intrinsic maps (referred to as X layers) simultaneously. Unlike previous approaches, which infer intrinsic properties individually or require separate models for decomposition and conditional generation, PRISM maintains consistency across modalities by generating all intrinsic layers jointly. It supports diverse tasks, including text-to-RGBX generation, RGB-to-X decomposition, and X-to-RGBX conditional generation. Additionally, PRISM enables both global and local image editing through conditioning on selected intrinsic layers and text prompts. Extensive experiments demonstrate the competitive performance of PRISM both for intrinsic image decomposition and conditional image generation while preserving the base model's text-to-image generation capability.","authors":["Alara Dirik","Tuanfeng Wang","Duygu Ceylan","Stefanos Zafeiriou","Anna Fr\\\"uhst\\\"uck"],"url":"https://arxiv.org/abs/2504.14219"}
{"created":"2025-05-15","title":"Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation","abstract":"Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable multimodal perception capabilities, garnering significant attention. While numerous evaluation studies have emerged, assessing LVLMs both holistically and on specialized tasks, fine-grained image tasks-fundamental to computer vision-remain largely unexplored. To fill this gap, we introduce a comprehensive fine-grained evaluation benchmark, i.e., FG-BMK, comprising 1.01 million questions and 0.33 million images. Our evaluation systematically examines LVLMs from both human-oriented and machine-oriented perspectives, focusing on their semantic recognition and fine-grained feature representation capabilities. Through extensive experiments on twelve representative LVLMs/VLMs, we uncover key findings regarding the influence of training paradigms, modality alignment, perturbation susceptibility, and fine-grained category reasoning on task performance. This work provides critical insights into the limitations of current LVLMs and offers guidance for future data construction and model design in the development of more advanced LVLMs. Our code is open-source and available at https://github.com/SEU-VIPGroup/FG-BMK.","authors":["Hong-Tao Yu","Xiu-Shen Wei","Yuxin Peng","Serge Belongie"],"url":"https://arxiv.org/abs/2504.14988"}
{"created":"2025-05-15","title":"Harden and Catch for Just-in-Time Assured LLM-Based Software Testing: Open Research Challenges","abstract":"Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching 'Just-in-Time' (JiTTest) Challenge, in which tests are generated 'just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025. Author order is alphabetical. The corresponding author is Mark Harman.","authors":["Mark Harman","Peter O'Hearn","Shubho Sengupta"],"url":"https://arxiv.org/abs/2504.16472"}
{"created":"2025-05-15","title":"Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G","abstract":"The Quality of Experience (QoE) is the users satisfaction while streaming a video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube reflects the smooth streaming session without any buffering and quality shift events. One of the most important factors nowadays affecting QoE of YouTube is frequent shifts from higher to lower resolutions and vice versa. These shifts ensure a smooth streaming session; however, it might get a lower mean opinion score. For instance, dropping from 1080p to 480p during a video can preserve continuity but might reduce the viewers enjoyment. Over time, OTT platforms are looking for alternative ways to boost user experience instead of relying on traditional Quality of Service (QoS) metrics such as bandwidth, latency, and throughput. As a result, we look into the relationship between quality shifting in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our findings state that these channel metrics positively correlate with shifts. Thus, in real-time, OTT can only rely on them to predict video streaming sessions into lower- and higher-resolution categories, thus providing more resources to improve user experience. Using traditional Machine Learning (ML) classifiers, we achieved an accuracy of 77-percent, while using only RSRP, RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency networks promise enhanced streaming capabilities, the proposed methodology can be used to improve OTT services.","authors":["Raza Ul Mustafa","Sesha Dassanayake","Noman Ashraf"],"url":"https://arxiv.org/abs/2504.17938"}
{"created":"2025-05-15","title":"CrashFixer: A crash resolution agent for the Linux kernel","abstract":"Code large language models (LLMs) have shown impressive capabilities on a multitude of software engineering tasks. In particular, they have demonstrated remarkable utility in the task of code repair. However, common benchmarks used to evaluate the performance of code LLMs are often limited to small-scale settings. In this work, we build upon kGym, which shares a benchmark for system-level Linux kernel bugs and a platform to run experiments on the Linux kernel.","authors":["Alex Mathai","Chenxi Huang","Suwei Ma","Jihwan Kim","Hailie Mitchell","Aleksandr Nogikh","Petros Maniatis","Franjo Ivan\\v{c}i\\'c","Junfeng Yang","Baishakhi Ray"],"url":"https://arxiv.org/abs/2504.20412"}
{"created":"2025-05-15","title":"Rethinking Time Encoding via Learnable Transformation Functions","abstract":"Effectively modeling time information and incorporating it into applications or models involving chronologically occurring events is crucial. Real-world scenarios often involve diverse and complex time patterns, which pose significant challenges for time encoding methods. While previous methods focus on capturing time patterns, many rely on specific inductive biases, such as using trigonometric functions to model periodicity. This narrow focus on single-pattern modeling makes them less effective in handling the diversity and complexities of real-world time patterns. In this paper, we investigate to improve the existing commonly used time encoding methods and introduce Learnable Transformation-based Generalized Time Encoding (LeTE). We propose using deep function learning techniques to parameterize non-linear transformations in time encoding, making them learnable and capable of modeling generalized time patterns, including diverse and complex temporal dynamics. By enabling learnable transformations, LeTE encompasses previous methods as specific cases and allows seamless integration into a wide range of tasks. Through extensive experiments across diverse domains, we demonstrate the versatility and effectiveness of LeTE.","authors":["Xi Chen","Yateng Tang","Jiarong Xu","Jiawei Zhang","Siwei Zhang","Sijia Peng","Xuehao Zheng","Yun Xiong"],"url":"https://arxiv.org/abs/2505.00887"}
{"created":"2025-05-15","title":"Llama-Nemotron: Efficient Reasoning Models","abstract":"We introduce the Llama-Nemotron series of models, an open family of heterogeneous reasoning models that deliver exceptional reasoning capabilities, inference efficiency, and an open license for enterprise use. The family comes in three sizes -- Nano (8B), Super (49B), and Ultra (253B) -- and performs competitively with state-of-the-art reasoning models such as DeepSeek-R1 while offering superior inference throughput and memory efficiency. In this report, we discuss the training procedure for these models, which entails using neural architecture search from Llama 3 models for accelerated inference, knowledge distillation, and continued pretraining, followed by a reasoning-focused post-training stage consisting of two main parts: supervised fine-tuning and large scale reinforcement learning. Llama-Nemotron models are the first open-source models to support a dynamic reasoning toggle, allowing users to switch between standard chat and reasoning modes during inference. To further support open research and facilitate model development, we provide the following resources: 1. We release the Llama-Nemotron reasoning models -- LN-Nano, LN-Super, and LN-Ultra -- under the commercially permissive NVIDIA Open Model License Agreement. 2. We release the complete post-training dataset: Llama-Nemotron-Post-Training-Dataset. 3. We also release our training codebases: NeMo, NeMo-Aligner, and Megatron-LM.","authors":["Akhiad Bercovich","Itay Levy","Izik Golan","Mohammad Dabbah","Ran El-Yaniv","Omri Puny","Ido Galil","Zach Moshe","Tomer Ronen","Najeeb Nabwani","Ido Shahaf","Oren Tropp","Ehud Karpas","Ran Zilberstein","Jiaqi Zeng","Soumye Singhal","Alexander Bukharin","Yian Zhang","Tugrul Konuk","Gerald Shen","Ameya Sunil Mahabaleshwarkar","Bilal Kartal","Yoshi Suhara","Olivier Delalleau","Zijia Chen","Zhilin Wang","David Mosallanezhad","Adi Renduchintala","Haifeng Qian","Dima Rekesh","Fei Jia","Somshubra Majumdar","Vahid Noroozi","Wasi Uddin Ahmad","Sean Narenthiran","Aleksander Ficek","Mehrzad Samadi","Jocelyn Huang","Siddhartha Jain","Igor Gitman","Ivan Moshkov","Wei Du","Shubham Toshniwal","George Armstrong","Branislav Kisacanin","Matvei Novikov","Daria Gitman","Evelina Bakhturina","Jane Polak Scowcroft","John Kamalu","Dan Su","Kezhi Kong","Markus Kliegl","Rabeeh Karimi","Ying Lin","Sanjeev Satheesh","Jupinder Parmar","Pritam Gundecha","Brandon Norick","Joseph Jennings","Shrimai Prabhumoye","Syeda Nahida Akter","Mostofa Patwary","Abhinav Khattar","Deepak Narayanan","Roger Waleffe","Jimmy Zhang","Bor-Yiing Su","Guyue Huang","Terry Kong","Parth Chadha","Sahil Jain","Christine Harvey","Elad Segal","Jining Huang","Sergey Kashirsky","Robert McQueen","Izzy Putterman","George Lam","Arun Venkatesan","Sherry Wu","Vinh Nguyen","Manoj Kilaru","Andrew Wang","Anna Warno","Abhilash Somasamudramath","Sandip Bhaskar","Maka Dong","Nave Assaf","Shahar Mor","Omer Ullman Argov","Scot Junkin","Oleksandr Romanenko","Pedro Larroy","Monika Katariya","Marco Rovinelli","Viji Balas","Nicholas Edelman","Anahita Bhiwandiwalla","Muthu Subramaniam","Smita Ithape","Karthik Ramamoorthy","Yuting Wu","Suguna Varshini Velury","Omri Almog","Joyjit Daw","Denys Fridman","Erick Galinkin","Michael Evans","Shaona Ghosh","Katherine Luna","Leon Derczynski","Nikki Pope","Eileen Long","Seth Schneider","Guillermo Siman","Tomasz Grzegorzek","Pablo Ribalta","Monika Katariya","Chris Alexiuk","Joey Conway","Trisha Saar","Ann Guan","Krzysztof Pawelec","Shyamala Prayaga","Oleksii Kuchaiev","Boris Ginsburg","Oluwatobi Olabiyi","Kari Briski","Jonathan Cohen","Bryan Catanzaro","Jonah Alben","Yonatan Geifman","Eric Chung"],"url":"https://arxiv.org/abs/2505.00949"}
{"created":"2025-05-15","title":"Don't be lazy: CompleteP enables compute-efficient deep transformers","abstract":"We study compute efficiency of LLM training when using different parameterizations, i.e., rules for adjusting model and optimizer hyperparameters (HPs) as model size changes. Some parameterizations fail to transfer optimal base HPs (such as learning rate) across changes in model depth, requiring practitioners to either re-tune these HPs as they scale up (expensive), or accept sub-optimal training when re-tuning is prohibitive. Even when they achieve HP transfer, we develop theory to show parameterizations may still exist in the lazy learning regime where layers learn only features close to their linearization, preventing effective use of depth and nonlinearity. Finally, we identify and adopt the parameterization we call CompleteP that achieves both depth-wise HP transfer and non-lazy learning in all layers. CompleteP enables a wider range of model width/depth ratios to remain compute-efficient, unlocking shapes better suited for different hardware settings and operational contexts. Moreover, CompleteP enables 12-34% compute efficiency improvements over the prior state-of-the-art.","authors":["Nolan Dey","Bin Claire Zhang","Lorenzo Noci","Mufan Li","Blake Bordelon","Shane Bergsma","Cengiz Pehlevan","Boris Hanin","Joel Hestness"],"url":"https://arxiv.org/abs/2505.01618"}
{"created":"2025-05-15","title":"SafeNav: Safe Path Navigation using Landmark Based Localization in a GPS-denied Environment","abstract":"In battlefield environments, adversaries frequently disrupt GPS signals, requiring alternative localization and navigation methods. Traditional vision-based approaches like Simultaneous Localization and Mapping (SLAM) and Visual Odometry (VO) involve complex sensor fusion and high computational demand, whereas range-free methods like DV-HOP face accuracy and stability challenges in sparse, dynamic networks. This paper proposes LanBLoc-BMM, a navigation approach using landmark-based localization (LanBLoc) combined with a battlefield-specific motion model (BMM) and Extended Kalman Filter (EKF). Its performance is benchmarked against three state-of-the-art visual localization algorithms integrated with BMM and Bayesian filters, evaluated on synthetic and real-imitated trajectory datasets using metrics including Average Displacement Error (ADE), Final Displacement Error (FDE), and a newly introduced Average Weighted Risk Score (AWRS). LanBLoc-BMM (with EKF) demonstrates superior performance in ADE, FDE, and AWRS on real-imitated datasets. Additionally, two safe navigation methods, SafeNav-CHull and SafeNav-Centroid, are introduced by integrating LanBLoc-BMM(EKF) with a novel Risk-Aware RRT* (RAw-RRT*) algorithm for obstacle avoidance and risk exposure minimization. Simulation results in battlefield scenarios indicate SafeNav-Centroid excels in accuracy, risk exposure, and trajectory efficiency, while SafeNav-CHull provides superior computational speed.","authors":["Ganesh Sapkota","Sanjay Madria"],"url":"https://arxiv.org/abs/2505.01956"}
{"created":"2025-05-15","title":"Triple-identity Authentication: The Future of Secure Access","abstract":"In a typical authentication process, the local system verifies the user's identity using a stored hash value generated by a cross-system hash algorithm. This article shifts the research focus from traditional password encryption to the establishment of gatekeeping mechanisms for effective interactions between a system and the outside world. Here, we propose a triple-identity authentication system to achieve this goal. Specifically, this local system opens the inner structure of its hash algorithm to all user credentials, including the login name, login password, and authentication password. When a login credential is entered, the local system hashes it and then creates a unique identifier using intermediate hash elements randomly selected from the open algorithm. Importantly, this locally generated unique identifier (rather than the stored hash produced by the open algorithm) is utilized to verify the user's combined identity, which is generated by combining the entered credential with the International Mobile Equipment Identity and the International Mobile Subscriber Identity. The verification process is implemented at each interaction point: the login name field, the login password field, and the server's authentication point. Thus, within the context of this triple-identity authentication system, we establish a robust gatekeeping mechanism for system interactions, ultimately providing a level of security that is equivalent to multi-factor authentication.","authors":["Suyun Borjigin"],"url":"https://arxiv.org/abs/2505.02004"}
{"created":"2025-05-15","title":"GarmentGS: Point-Cloud Guided Gaussian Splatting for High-Fidelity Non-Watertight 3D Garment Reconstruction","abstract":"Traditional 3D garment creation requires extensive manual operations, resulting in time and labor costs. Recently, 3D Gaussian Splatting has achieved breakthrough progress in 3D scene reconstruction and rendering, attracting widespread attention and opening new pathways for 3D garment reconstruction. However, due to the unstructured and irregular nature of Gaussian primitives, it is difficult to reconstruct high-fidelity, non-watertight 3D garments. In this paper, we present GarmentGS, a dense point cloud-guided method that can reconstruct high-fidelity garment surfaces with high geometric accuracy and generate non-watertight, single-layer meshes. Our method introduces a fast dense point cloud reconstruction module that can complete garment point cloud reconstruction in 10 minutes, compared to traditional methods that require several hours. Furthermore, we use dense point clouds to guide the movement, flattening, and rotation of Gaussian primitives, enabling better distribution on the garment surface to achieve superior rendering effects and geometric accuracy. Through numerical and visual comparisons, our method achieves fast training and real-time rendering while maintaining competitive quality.","authors":["Zhihao Tang","Shenghao Yang","Hongtao Zhang","Mingbo Zhao"],"url":"https://arxiv.org/abs/2505.02126"}
{"created":"2025-05-15","title":"Dexterous Contact-Rich Manipulation via the Contact Trust Region","abstract":"What is a good local description of contact dynamics for contact-rich manipulation, and where can we trust this local description? While many approaches often rely on the Taylor approximation of dynamics with an ellipsoidal trust region, we argue that such approaches are fundamentally inconsistent with the unilateral nature of contact. As a remedy, we present the Contact Trust Region (CTR), which captures the unilateral nature of contact while remaining efficient for computation. With CTR, we first develop a Model-Predictive Control (MPC) algorithm capable of synthesizing local contact-rich plans. Then, we extend this capability to plan globally by stitching together local MPC plans, enabling efficient and dexterous contact-rich manipulation. To verify the performance of our method, we perform comprehensive evaluations, both in high-fidelity simulation and on hardware, on two contact-rich systems: a planar IiwaBimanual system and a 3D AllegroHand system. On both systems, our method offers a significantly lower-compute alternative to existing RL-based approaches to contact-rich manipulation. In particular, our Allegro in-hand manipulation policy, in the form of a roadmap, takes fewer than 10 minutes to build offline on a standard laptop using just its CPU, with online inference taking just a few seconds. Experiment data, video and code are available at ctr.theaiinstitute.com.","authors":["H. J. Terry Suh","Tao Pang","Tong Zhao","Russ Tedrake"],"url":"https://arxiv.org/abs/2505.02291"}
{"created":"2025-05-15","title":"Behavioral Sensing and Intervention Paradigm: A Review of Closed-Loop Approaches for Ingestion Health","abstract":"Ingestive behavior plays a critical role in health, yet many existing interventions remain limited to static guidance or manual self-tracking. With the increasing integration of sensors and perceptual computing, recent systems have begun to support closed-loop interventions that dynamically sense user behavior and provide feedback during or around ingestion episodes. In this survey, we review 136 studies that leverage sensor-enabled or interaction-mediated approaches to influence eating behavior. We propose a behavioral closed-loop paradigm comprising three core components: target behaviors, sensing modalities, and feedback strategies. A taxonomy of sensing and intervention modalities is presented, organized along human- and environment-based dimensions. Our analysis also examines evaluation methods and design trends across different modality-behavior pairings. This review reveals prevailing patterns and critical gaps, offering design insights for future adaptive and context-aware ingestion health interventions.","authors":["Jun Fang","Yanuo Zhou","Ka I Chan","Jiajin Li","Zeyi Sun","Zhengnan Li","Zicong Fu","Hongjing Piao","Haodong Xu","Yuanchun Shi","Yuntao Wang"],"url":"https://arxiv.org/abs/2505.03185"}
{"created":"2025-05-15","title":"Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering","abstract":"As large language models (LLMs) improve in their capacity to serve as personal AI assistants, their ability to output uniquely tailored, personalized responses that align with the soft preferences of their users is essential for enhancing user satisfaction and retention. However, untrained lay users have poor prompt specification abilities and often struggle with conveying their latent preferences to AI assistants. To address this, we leverage activation steering to guide LLMs to align with interpretable preference dimensions during inference. In contrast to memory-based personalization methods that require longer user history, steering is extremely lightweight and can be easily controlled by the user via an linear strength factor. We embed steering into three different interactive chatbot interfaces and conduct a within-subjects user study (n=14) to investigate how end users prefer to personalize their conversations. The results demonstrate the effectiveness of preference-based steering for aligning real-world conversations with hidden user preferences, and highlight further insights on how diverse values around control, usability, and transparency lead users to prefer different interfaces.","authors":["Jessica Y. Bo","Tianyu Xu","Ishan Chatterjee","Katrina Passarella-Ward","Achin Kulshrestha","D Shin"],"url":"https://arxiv.org/abs/2505.04260"}
{"created":"2025-05-15","title":"Reliably Bounding False Positives: A Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction","abstract":"The rapid advancement of large language models has raised significant concerns regarding their potential misuse by malicious actors. As a result, developing effective detectors to mitigate these risks has become a critical priority. However, most existing detection methods focus excessively on detection accuracy, often neglecting the societal risks posed by high false positive rates (FPRs). This paper addresses this issue by leveraging Conformal Prediction (CP), which effectively constrains the upper bound of FPRs. While directly applying CP constrains FPRs, it also leads to a significant reduction in detection performance. To overcome this trade-off, this paper proposes a Zero-Shot Machine-Generated Text Detection Framework via Multiscaled Conformal Prediction (MCP), which both enforces the FPR constraint and improves detection performance. This paper also introduces RealDet, a high-quality dataset that spans a wide range of domains, ensuring realistic calibration and enabling superior detection performance when combined with MCP. Empirical evaluations demonstrate that MCP effectively constrains FPRs, significantly enhances detection performance, and increases robustness against adversarial attacks across multiple detectors and datasets.","authors":["Xiaowei Zhu","Yubing Ren","Yanan Cao","Xixun Lin","Fang Fang","Yangxi Li"],"url":"https://arxiv.org/abs/2505.05084"}
{"created":"2025-05-15","title":"VIMPPI: Enhancing Model Predictive Path Integral Control with Variational Integration for Underactuated Systems","abstract":"This paper presents VIMPPI, a novel control approach for underactuated double pendulum systems developed for the AI Olympics competition. We enhance the Model Predictive Path Integral framework by incorporating variational integration techniques, enabling longer planning horizons without additional computational cost. Operating at 500-700 Hz with control interpolation and disturbance detection mechanisms, VIMPPI substantially outperforms both baseline methods and alternative MPPI implementations","authors":["Igor Alentev","Lev Kozlov","Ivan Domrachev","Simeon Nedelchev","Jee-Hwan Ryu"],"url":"https://arxiv.org/abs/2505.05507"}
{"created":"2025-05-15","title":"Interval reduced-order switched positive observers for uncertain switched positive linear systems","abstract":"In this paper, existence conditions and a design procedure of reduced-order switched positive observers for continuous- and discrete-time switched positive linear systems with uncertainty are established. In the analyzed class, arbitrary switching is permitted, whereas the uncertainty expressed via matrix inequalities concerns both the initial state and system parameters. Positive lower and positive upper interval switched observers are obtained. The proposed observers are of (n - p) order, where n is the dimension of the state vector and p is the rank of the output matrix, i.e., p-dimensional measurement information. Moreover, as a special case, existence conditions and a design procedure of reduced-order positive observers for uncertain positive linear systems without switching are provided. The theoretical findings are illustrated by two numerical examples for continuous- and discrete-time systems.","authors":["Naohisa Otsuka","Daiki Kakehi","Przemys{\\l}aw Ignaciuk"],"url":"https://arxiv.org/abs/2505.06372"}
{"created":"2025-05-15","title":"HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation","abstract":"Text-to-image synthesis has progressed to the point where models can generate visually compelling images from natural language prompts. Yet, existing methods often fail to reconcile high-level semantic fidelity with explicit spatial control, particularly in scenes involving multiple objects, nuanced relations, or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal Alignment (HCMA) framework for grounded text-to-image generation. HCMA integrates two alignment modules into each diffusion sampling step: a global module that continuously aligns latent representations with textual descriptions to ensure scene-level coherence, and a local module that employs bounding-box layouts to anchor objects at specified locations, enabling fine-grained spatial control. Extensive experiments on the MS-COCO 2014 validation set show that HCMA surpasses state-of-the-art baselines, achieving a 0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These results demonstrate HCMA's effectiveness in faithfully capturing intricate textual semantics while adhering to user-defined spatial constraints, offering a robust solution for semantically grounded image generation.Our code is available at https://github.com/hwang-cs-ime/HCMA","authors":["Hang Wang","Zhi-Qi Cheng","Chenhao Lin","Chao Shen","Lei Zhang"],"url":"https://arxiv.org/abs/2505.06512"}
{"created":"2025-05-15","title":"I Know What You Said: Unveiling Hardware Cache Side-Channels in Local Large Language Model Inference","abstract":"Large Language Models (LLMs) that can be deployed locally have recently gained popularity for privacy-sensitive tasks, with companies such as Meta, Google, and Intel playing significant roles in their development. However, the security of local LLMs through the lens of hardware cache side-channels remains unexplored. In this paper, we unveil novel side-channel vulnerabilities in local LLM inference: token value and token position leakage, which can expose both the victim's input and output text, thereby compromising user privacy. Specifically, we found that adversaries can infer the token values from the cache access patterns of the token embedding operation, and deduce the token positions from the timing of autoregressive decoding phases. To demonstrate the potential of these leaks, we design a novel eavesdropping attack framework targeting both open-source and proprietary LLM inference systems. The attack framework does not directly interact with the victim's LLM and can be executed without privilege.","authors":["Zibo Gao","Junjie Hu","Feng Guo","Yixin Zhang","Yinglong Han","Siyuan Liu","Haiyang Li","Zhiqiang Lv"],"url":"https://arxiv.org/abs/2505.06738"}
{"created":"2025-05-15","title":"Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery","abstract":"Commodity price volatility creates economic challenges, necessitating accurate multi-horizon forecasting. Predicting prices for commodities like copper and crude oil is complicated by diverse interacting factors (macroeconomic, supply/demand, geopolitical, etc.). Current models often lack transparency, limiting strategic use. This paper presents a Regularized Sparse Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon commodity price prediction and discovery of interpretable latent market drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series. Crucially, L1 regularization ($\\|\\mathbf{z}\\|_1$) on its latent vector $\\mathbf{z}$ enforces sparsity, promoting parsimonious explanations of market dynamics through learned factors representing underlying drivers (e.g., demand, supply shocks). Drawing from energy-based models and sparse coding, the RSAE optimizes predictive accuracy while learning sparse representations. Evaluated on historical Copper and Crude Oil data with numerous indicators, our findings indicate the RSAE offers competitive multi-horizon forecasting accuracy and data-driven insights into price dynamics via its interpretable latent space, a key advantage over traditional black-box approaches.","authors":["Abhijit Gupta"],"url":"https://arxiv.org/abs/2505.06795"}
{"created":"2025-05-15","title":"CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging","abstract":"Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.","authors":["Wenju Sun","Qingyong Li","Yangli-ao Geng","Boyang Li"],"url":"https://arxiv.org/abs/2505.06977"}
{"created":"2025-05-15","title":"Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design","abstract":"Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.","authors":["Tong Chen","Yinuo Zhang","Sophia Tang","Pranam Chatterjee"],"url":"https://arxiv.org/abs/2505.07086"}
{"created":"2025-05-15","title":"RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models","abstract":"Automated penetration testing (AutoPT) powered by large language models (LLMs) has gained attention for its ability to automate ethical hacking processes and identify vulnerabilities in target systems by leveraging the intrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks often underperform compared to human experts in challenging tasks for several reasons: the imbalanced knowledge used in LLM training, short-sighted planning in the planning process, and hallucinations during command generation. In addition, the penetration testing (PT) process, with its trial-and-error nature, is limited by existing frameworks that lack mechanisms to learn from previous failed operations, restricting adaptive improvement of PT strategies. To address these limitations, we propose a knowledge-informed self-reflective PT framework powered by LLMs, called RefPentester, which is an AutoPT framework designed to assist human operators in identifying the current stage of the PT process, selecting appropriate tactic and technique for the stage, choosing suggested action, providing step-by-step operational guidance, and learning from previous failed operations. We also modeled the PT process as a seven-state Stage Machine to integrate the proposed framework effectively. The evaluation shows that RefPentester can successfully reveal credentials on Hack The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7%. Across PT stages, RefPentester also demonstrates superior success rates on PT stage transitions.","authors":["Hanzheng Dai","Yuanliang Li","Zhibo Zhang","Jun Yan"],"url":"https://arxiv.org/abs/2505.07089"}
{"created":"2025-05-15","title":"Security through the Eyes of AI: How Visualization is Shaping Malware Detection","abstract":"Malware, a persistent cybersecurity threat, increasingly targets interconnected digital systems such as desktop, mobile, and IoT platforms through sophisticated attack vectors. By exploiting these vulnerabilities, attackers compromise the integrity and resilience of modern digital ecosystems. To address this risk, security experts actively employ Machine Learning or Deep Learning-based strategies, integrating static, dynamic, or hybrid approaches to categorize malware instances. Despite their advantages, these methods have inherent drawbacks and malware variants persistently evolve with increased sophistication, necessitating advancements in detection strategies. Visualization-based techniques are emerging as scalable and interpretable solutions for detecting and understanding malicious behaviors across diverse platforms including desktop, mobile, IoT, and distributed systems as well as through analysis of network packet capture files. In this comprehensive survey of more than 100 high-quality research articles, we evaluate existing visualization-based approaches applied to malware detection and classification. As a first contribution, we propose a new all-encompassing framework to study the landscape of visualization-based malware detection techniques. Within this framework, we systematically analyze state-of-the-art approaches across the critical stages of the malware detection pipeline. By analyzing not only the single techniques but also how they are combined to produce the final solution, we shed light on the main challenges in visualization-based approaches and provide insights into the advancements and potential future directions in this critical field.","authors":["Matteo Brosolo","Asmitha K. A.","Rafidha Rehiman K. A.","Muhammed Shafi K. P.","Serena Nicolazzo","Antonino Nocera","Vinod P"],"url":"https://arxiv.org/abs/2505.07574"}
{"created":"2025-05-15","title":"Neural Brain: A Neuroscience-inspired Framework for Embodied Agents","abstract":"The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.","authors":["Jian Liu","Xiongtao Shi","Thai Duy Nguyen","Haitian Zhang","Tianxiang Zhang","Wei Sun","Yanjie Li","Athanasios V. Vasilakos","Giovanni Iacca","Arshad Ali Khan","Arvind Kumar","Jae Won Cho","Ajmal Mian","Lihua Xie","Erik Cambria","Lin Wang"],"url":"https://arxiv.org/abs/2505.07634"}
{"created":"2025-05-15","title":"Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving","abstract":"Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{https://github.com/yyht/openrlhf_async_pipline}{https://github.com/yyht/openrlhf\\_async\\_pipline}.","authors":["Xinji Mai","Haotian Xu","Xing W","Weinong Wang","Yingying Zhang","Wenqiang Zhang"],"url":"https://arxiv.org/abs/2505.07773"}
{"created":"2025-05-15","title":"Improving Data Fidelity via Diffusion Model-based Correction and Super-Resolution","abstract":"We propose a unified diffusion model-based correction and super-resolution method to enhance the fidelity and resolution of diverse low-quality data through a two-step pipeline. First, the correction step employs a novel enhanced stochastic differential editing technique based on an imbalanced perturbation and denoising process, ensuring robust and effective bias correction at the low-resolution level. The robustness and effectiveness of this approach are validated theoretically and experimentally. Next, the super-resolution step leverages cascaded conditional diffusion models to iteratively refine the corrected data to high-resolution. Numerical experiments on three PDE problems and a climate dataset demonstrate that the proposed method effectively enhances low-fidelity, low-resolution data by correcting numerical errors and noise while simultaneously improving resolution to recover fine-scale structures.","authors":["Wuzhe Xu","Yulong Lu","Sifan Wang","Tong-Rui Liu"],"url":"https://arxiv.org/abs/2505.08526"}
{"created":"2025-05-15","title":"Data-driven multiscale modeling for correcting dynamical systems","abstract":"We propose a multiscale approach for predicting quantities in dynamical systems which is explicitly structured to extract information in both fine-to-coarse and coarse-to-fine directions. We envision this method being generally applicable to problems with significant self-similarity or in which the prediction task is challenging and where stability of a learned model's impact on the target dynamical system is important. We evaluate our approach on a climate subgrid parameterization task in which our multiscale networks correct chaotic underlying models to reflect the contributions of unresolved, fine-scale dynamics.","authors":["Karl Otness","Laure Zanna","Joan Bruna"],"url":"https://arxiv.org/abs/2303.17496"}
{"created":"2025-05-15","title":"Randomized adiabatic quantum linear solver algorithm with optimal complexity scaling and detailed running costs","abstract":"Solving linear systems of equations is a fundamental problem with a wide variety of applications across many fields of science, and there is increasing effort to develop quantum linear solver algorithms. [Suba\\c{s}i et al., Phys. Rev. Lett. (2019)] proposed a randomized algorithm inspired by adiabatic quantum computing, based on a sequence of random Hamiltonian simulation steps, with suboptimal scaling in the condition number $\\kappa$ of the linear system and the target error $\\epsilon$. Here we go beyond these results in several ways. Firstly, using filtering [Lin et al., Quantum (2019)] and Poissonization techniques [Cunningham et al., arXiv:2406.03972 (2024)], the algorithm complexity is improved to the optimal scaling $O(\\kappa \\log(1/\\epsilon))$ - an exponential improvement in $\\epsilon$, and a shaving of a $\\log \\kappa$ scaling factor in $\\kappa$. Secondly, the algorithm is further modified to achieve constant factor improvements, which are vital as we progress towards hardware implementations on fault-tolerant devices. We introduce a cheaper randomized walk operator method replacing Hamiltonian simulation - which also removes the need for potentially challenging classical precomputations; randomized routines are sampled over optimized random variables; circuit constructions are improved. We obtain a closed formula rigorously upper bounding the expected number of times one needs to apply a block-encoding of the linear system matrix to output a quantum state encoding the solution to the linear system. The upper bound is $867 \\kappa$ at $\\epsilon=10^{-10}$ for Hermitian matrices.","authors":["David Jennings","Matteo Lostaglio","Sam Pallister","Andrew T Sornborger","Yi\\u{g}it Suba\\c{s}{\\i}"],"url":"https://arxiv.org/abs/2305.11352"}
{"created":"2025-05-15","title":"Error correcting 2D-3D cascaded network for myocardial infarct scar segmentation on late gadolinium enhancement cardiac magnetic resonance images","abstract":"Late gadolinium enhancement (LGE) cardiac magnetic resonance (CMR) imaging is considered the in vivo reference standard for assessing infarct size (IS) and microvascular obstruction (MVO) in ST-elevation myocardial infarction (STEMI) patients. However, the exact quantification of those markers of myocardial infarct severity remains challenging and very time-consuming. As LGE distribution patterns can be quite complex and hard to delineate from the blood pool or epicardial fat, automatic segmentation of LGE CMR images is challenging. In this work, we propose a cascaded framework of two-dimensional and three-dimensional convolutional neural networks (CNNs) which enables to calculate the extent of myocardial infarction in a fully automated way. By artificially generating segmentation errors which are characteristic for 2D CNNs during training of the cascaded framework we are enforcing the detection and correction of 2D segmentation errors and hence improve the segmentation accuracy of the entire method. The proposed method was trained and evaluated on two publicly available datasets. We perform comparative experiments where we show that our framework outperforms state-of-the-art reference methods in segmentation of myocardial infarction. Furthermore, in extensive ablation studies we show the advantages that come with the proposed error correcting cascaded method. The code of this project is publicly available at https://github.com/matthi99/EcorC.git","authors":["Matthias Schwab","Mathias Pamminger","Christian Kremser","Daniel Obmann","Markus Haltmeier","Agnes Mayr"],"url":"https://arxiv.org/abs/2306.14725"}
{"created":"2025-05-15","title":"Iterative Occlusion-Aware Light Field Depth Estimation using 4D Geometrical Cues","abstract":"Light field cameras and multi-camera arrays have emerged as promising solutions for accurately estimating depth by passively capturing light information. This is possible because the 3D information of a scene is embedded in the 4D light field geometry. Commonly, depth estimation methods extract this information relying on gradient information, heuristic-based optimisation models, or learning-based approaches. This paper focuses mainly on explicitly understanding and exploiting 4D geometrical cues for light field depth estimation. Thus, a novel method is proposed, based on a non-learning-based optimisation approach for depth estimation that explicitly considers surface normal accuracy and occlusion regions by utilising a fully explainable 4D geometric model of the light field. The 4D model performs depth/disparity estimation by determining the orientations and analysing the intersections of key 2D planes in 4D space, which are the images of 3D-space points in the 4D light field. Experimental results show that the proposed method outperforms both learning-based and non-learning-based state-of-the-art methods in terms of surface normal angle accuracy, achieving a Median Angle Error on planar surfaces, on average, 26.3$\\%$ lower than the state-of-the-art, and still being competitive with state-of-the-art methods in terms of MSE ${\\times}$ 100 and Badpix 0.07.","authors":["Rui Louren\\c{c}o","Lucas Thomaz","Eduardo A. B. Silva","Sergio M. M. Faria"],"url":"https://arxiv.org/abs/2403.02043"}
{"created":"2025-05-15","title":"Optimal navigation of magnetic artificial microswimmers in blood capillaries with deep reinforcement learning","abstract":"Biomedical applications such as targeted drug delivery, microsurgery, and sensing rely on reaching precise areas within the body in a minimally invasive way. Artificial bacterial flagella (ABFs) have emerged as potential tools for this task by navigating through the circulatory system with the help of external magnetic fields. While their swimming characteristics are well understood in simple settings, their controlled navigation through realistic capillary networks remains a significant challenge due to the complexity of blood flow and the high computational cost of detailed simulations. We address this challenge by conducting numerical simulations of ABFs in retinal capillaries, propelled by an external magnetic field. The simulations are based on a validated blood model that predicts the dynamics of individual red blood cells and their hydrodynamic interactions with ABFs. The magnetic field follows a control policy that brings the ABF to a prescribed target. The control policy is learned with an actor-critic, off-policy reinforcement learning algorithm coupled with a reduced-order model of the system. We show that the same policy robustly guides the ABF to a prescribed target in both the reduced-order model and the fine-grained blood simulations. This approach is suitable for designing robust control policies for personalized medicine at moderate computational cost.","authors":["Lucas Amoudruz","Sergey Litvinov","Petros Koumoutsakos"],"url":"https://arxiv.org/abs/2404.02171"}
{"created":"2025-05-15","title":"Efficient Prior Calibration From Indirect Data","abstract":"Bayesian inversion is central to the quantification of uncertainty within problems arising from numerous applications in science and engineering. To formulate the approach, four ingredients are required: a forward model mapping the unknown parameter to an element of a solution space, often the solution space for a differential equation; an observation operator mapping an element of the solution space to the data space; a noise model describing how noise pollutes the observations; and a prior model describing knowledge about the unknown parameter before the data is acquired. This paper is concerned with learning the prior model from data; in particular, learning the prior from multiple realizations of indirect data obtained through the noisy observation process. The prior is represented, using a generative model, as the pushforward of a Gaussian in a latent space; the pushforward map is learned by minimizing an appropriate loss function. A metric that is well-defined under empirical approximation is used to define the loss function for the pushforward map to make an implementable methodology. Furthermore, an efficient residual-based neural operator approximation of the forward model is proposed and it is shown that this may be learned concurrently with the pushforward map, using a bilevel optimization formulation of the problem; this use of neural operator approximation has the potential to make prior learning from indirect data more computationally efficient, especially when the observation process is expensive, non-smooth or not known. The ideas are illustrated with the Darcy flow inverse problem of finding permeability from piezometric head measurements.","authors":["O. Deniz Akyildiz","Mark Girolami","Andrew M. Stuart","Arnaud Vadeboncoeur"],"url":"https://arxiv.org/abs/2405.17955"}
{"created":"2025-05-15","title":"Maximal number of subword occurrences in a word","abstract":"We consider the number of occurrences of subwords (non-consecutive sub-sequences) in a given word. We first define the notion of subword entropy of a given word that measures the maximal number of occurrences among all possible subwords. We then give upper and lower bounds of minimal subword entropy for words of fixed length in a fixed alphabet, and also showing that minimal subword entropy per letter has a limit value. A better upper bound of minimal subword entropy for a binary alphabet is then given by looking at certain families of periodic words. We also give some conjectures based on experimental observations.","authors":["Wenjie Fang"],"url":"https://arxiv.org/abs/2406.02971"}
{"created":"2025-05-15","title":"Resilient Random Time-hopping Reply against Distance Attacks in UWB Ranging","abstract":"In order to mitigate the distance reduction attack in Ultra-Wide Band (UWB) ranging, this paper proposes a secure ranging scheme based on a random time-hopping mechanism without redundant signaling overhead. Additionally, a secure ranging strategy is designed for backward compatibility with existing standards such as IEEE 802.15.4a/z, combined with an attack detection scheme. The effectiveness and feasibility of the proposed strategy are demonstrated through both simulation and experimental results in the case of the Ghost Peak attack, as demonstrated by Patrick Leu et al. The random time-hopping mechanism is verified to be capable of reducing the success rate of distance reduction attacks to less than 0.01%, thereby significantly enhancing the security of UWB ranging.","authors":["Wenlong Gou","Chuanhang Yu","Gang Wu"],"url":"https://arxiv.org/abs/2406.06252"}
{"created":"2025-05-15","title":"Hybrid Heuristic Algorithms for Adiabatic Quantum Machine Learning Models","abstract":"Numerous established machine learning models and various neural network architectures can be restructured as Quadratic Unconstrained Binary Optimization (QUBO) problems. A significant challenge in Adiabatic Quantum Machine Learning (AQML) is the computational demand of the training phase. To mitigate this, approximation techniques inspired by quantum annealing, like Simulated Annealing and Multiple Start Tabu Search (MSTS), have been employed to expedite QUBO-based AQML training. This paper introduces a novel hybrid algorithm that incorporates an \"r-flip\" strategy. This strategy is aimed at solving large-scale QUBO problems more effectively, offering better solution quality and lower computational costs compared to existing MSTS methods. The r-flip approach has practical applications in diverse fields, including cross-docking, supply chain management, machine scheduling, and fraud detection. The paper details extensive computational experiments comparing this r-flip enhanced hybrid heuristic against a standard MSTS approach. These tests utilize both standard benchmark problems and three particularly large QUBO instances. The results indicate that the r-flip enhanced method consistently produces high-quality solutions efficiently, operating within practical time constraints.","authors":["Bahram Alidaee","Haibo Wang","Lutfu Sua","Wade Liu"],"url":"https://arxiv.org/abs/2407.21062"}
{"created":"2025-05-15","title":"Fragment-Masked Diffusion for Molecular Optimization","abstract":"Molecular optimization is a crucial aspect of drug discovery, aimed at refining molecular structures to enhance drug efficacy and minimize side effects, ultimately accelerating the overall drug development process. Many molecular optimization methods have been proposed, significantly advancing drug discovery. These methods primarily on understanding the specific drug target structures or their hypothesized roles in combating diseases. However, challenges such as a limited number of available targets and a difficulty capturing clear structures hinder innovative drug development. In contrast, phenotypic drug discovery (PDD) does not depend on clear target structures and can identify hits with novel and unbiased polypharmacology signatures. As a result, PDD-based molecular optimization can reduce potential safety risks while optimizing phenotypic activity, thereby increasing the likelihood of clinical success. Therefore, we propose a fragment-masked molecular optimization method based on PDD (FMOP). FMOP employs a regression-free diffusion model to conditionally optimize the molecular masked regions, effectively generating new molecules with similar scaffolds. On the large-scale drug response dataset GDSCv2, we optimize the potential molecules across all 985 cell lines. The overall experiments demonstrate that the in-silico optimization success rate reaches 95.4\\%, with an average efficacy increase of 7.5\\%. Additionally, we conduct extensive ablation and visualization experiments, confirming that FMOP is an effective and robust molecular optimization method. The code is available at: https://anonymous.4open.science/r/FMOP-98C2.","authors":["Kun Li","Xiantao Cai","Jia Wu","Shirui Pan","Huiting Xu","Bo Du","Wenbin Hu"],"url":"https://arxiv.org/abs/2408.09106"}
{"created":"2025-05-15","title":"Introduction to Machine Learning","abstract":"This book introduces the mathematical foundations and techniques that lead to the development and analysis of many of the algorithms that are used in machine learning. It starts with an introductory chapter that describes notation used throughout the book and serve at a reminder of basic concepts in calculus, linear algebra and probability and also introduces some measure theoretic terminology, which can be used as a reading guide for the sections that use these tools. The introductory chapters also provide background material on matrix analysis and optimization. The latter chapter provides theoretical support to many algorithms that are used in the book, including stochastic gradient descent, proximal methods, etc. After discussing basic concepts for statistical prediction, the book includes an introduction to reproducing kernel theory and Hilbert space techniques, which are used in many places, before addressing the description of various algorithms for supervised statistical learning, including linear methods, support vector machines, decision trees, boosting, or neural networks. The subject then switches to generative methods, starting with a chapter that presents sampling methods and an introduction to the theory of Markov chains. The following chapter describe the theory of graphical models, an introduction to variational methods for models with latent variables, and to deep-learning based generative models. The next chapters focus on unsupervised learning methods, for clustering, factor analysis and manifold learning. The final chapter of the book is theory-oriented and discusses concentration inequalities and generalization bounds.","authors":["Laurent Younes"],"url":"https://arxiv.org/abs/2409.02668"}
{"created":"2025-05-15","title":"On Using Curved Mirrors to Decrease Shadowing in VLC","abstract":"Visible light communication (VLC) complements radio frequency in indoor environments with large wireless data traffic. However, VLC is hindered by dramatic path losses when an opaque object is interposed between the transmitter and the receiver. Prior works propose the use of plane mirrors as optical reconfigurable intelligent surfaces (ORISs) to enhance communications through non-line-of-sight links. Plane mirrors rely on their orientation to forward the light to the target user location, which is challenging to implement in practice. This paper studies the potential of curved mirrors as static reflective surfaces to provide a broadening specular reflection that increases the signal coverage in mirror-assisted VLC scenarios. We study the behavior of paraboloid and semi-spherical mirrors and derive the irradiance equations. We provide extensive numerical and analytical results and show that curved mirrors, when developed with proper dimensions, may reduce the shadowing probability to zero, while static plane mirrors of the same size have shadowing probabilities larger than 65%. Furthermore, the signal-to-noise ratio offered by curved mirrors may suffice to provide connectivity to users deployed in the room even when a line-of-sight link blockage occurs.","authors":["Borja Genoves Guzman","Ana Garcia Armada","Ma\\\"it\\'e Brandt-Pearce"],"url":"https://arxiv.org/abs/2409.03378"}
{"created":"2025-05-15","title":"A Deep Learning Approach for Pixel-level Material Classification via Hyperspectral Imaging","abstract":"Recent advancements in computer vision, particularly in detection, segmentation, and classification, have significantly impacted various domains. However, these advancements are tied to RGB-based systems, which are insufficient for applications in industries like waste sorting, pharmaceuticals, and defense, where advanced object characterization beyond shape or color is necessary. Hyperspectral (HS) imaging, capturing both spectral and spatial information, addresses these limitations and offers advantages over conventional technologies such as X-ray fluorescence and Raman spectroscopy, particularly in terms of speed, cost, and safety.","authors":["Savvas Sifnaios","George Arvanitakis","Fotios K. Konstantinidis","Georgios Tsimiklis","Angelos Amditis","Panayiotis Frangos"],"url":"https://arxiv.org/abs/2409.13498"}
{"created":"2025-05-15","title":"Bayesian computation with generative diffusion models by Multilevel Monte Carlo","abstract":"Generative diffusion models have recently emerged as a powerful strategy to perform stochastic sampling in Bayesian inverse problems, delivering remarkably accurate solutions for a wide range of challenging applications. However, diffusion models often require a large number of neural function evaluations per sample in order to deliver accurate posterior samples. As a result, using diffusion models as stochastic samplers for Monte Carlo integration in Bayesian computation can be highly computationally expensive, particularly in applications that require a substantial number of Monte Carlo samples for conducting uncertainty quantification analyses. This cost is especially high in large-scale inverse problems such as computational imaging, which rely on large neural networks that are expensive to evaluate. With quantitative imaging applications in mind, this paper presents a Multilevel Monte Carlo strategy that significantly reduces the cost of Bayesian computation with diffusion models. This is achieved by exploiting cost-accuracy trade-offs inherent to diffusion models to carefully couple models of different levels of accuracy in a manner that significantly reduces the overall cost of the calculation, without reducing the final accuracy. The proposed approach achieves a $4\\times$-to-$8\\times$ reduction in computational cost w.r.t. standard techniques across three benchmark imaging problems.","authors":["Abdul-Lateef Haji-Ali","Marcelo Pereyra","Luke Shaw","Konstantinos Zygalakis"],"url":"https://arxiv.org/abs/2409.15511"}
{"created":"2025-05-15","title":"Simulating Dynamic Tumor Contrast Enhancement in Breast MRI using Conditional Generative Adversarial Networks","abstract":"This paper presents a method for virtual contrast enhancement in breast MRI, offering a promising non-invasive alternative to traditional contrast agent-based DCE-MRI acquisition. Using a conditional generative adversarial network, we predict DCE-MRI images, including jointly-generated sequences of multiple corresponding DCE-MRI timepoints, from non-contrast-enhanced MRIs, enabling tumor localization and characterization without the associated health risks. Furthermore, we qualitatively and quantitatively evaluate the synthetic DCE-MRI images, proposing a multi-metric Scaled Aggregate Measure (SAMe), assessing their utility in a tumor segmentation downstream task, and conclude with an analysis of the temporal patterns in multi-sequence DCE-MRI generation. Our approach demonstrates promising results in generating realistic and useful DCE-MRI sequences, highlighting the potential of virtual contrast enhancement for improving breast cancer diagnosis and treatment, particularly for patients where contrast agent administration is contraindicated.","authors":["Richard Osuala","Smriti Joshi","Apostolia Tsirikoglou","Lidia Garrucho","Walter H. L. Pinaya","Daniel M. Lang","Julia A. Schnabel","Oliver Diaz","Karim Lekadir"],"url":"https://arxiv.org/abs/2409.18872"}
{"created":"2025-05-15","title":"Deep Signature: Characterization of Large-Scale Molecular Dynamics","abstract":"Understanding protein dynamics are essential for deciphering protein functional mechanisms and developing molecular therapies. However, the complex high-dimensional dynamics and interatomic interactions of biological processes pose significant challenge for existing computational techniques. In this paper, we approach this problem for the first time by introducing Deep Signature, a novel computationally tractable framework that characterizes complex dynamics and interatomic interactions based on their evolving trajectories. Specifically, our approach incorporates soft spectral clustering that locally aggregates cooperative dynamics to reduce the size of the system, as well as signature transform that collects iterated integrals to provide a global characterization of the non-smooth interactive dynamics. Theoretical analysis demonstrates that Deep Signature exhibits several desirable properties, including invariance to translation, near invariance to rotation, equivariance to permutation of atomic coordinates, and invariance under time reparameterization. Furthermore, experimental results on three benchmarks of biological processes verify that our approach can achieve superior performance compared to baseline methods.","authors":["Tiexin Qin","Mengxu Zhu","Chunyang Li","Terry Lyons","Hong Yan","Haoliang Li"],"url":"https://arxiv.org/abs/2410.02847"}
{"created":"2025-05-15","title":"Full-waveform earthquake source inversion using simulation-based inference","abstract":"This paper presents a novel framework for full-waveform seismic source inversion using simulation-based inference (SBI). Traditional probabilistic approaches often rely on simplifying assumptions about data errors, which we show can lead to inaccurate uncertainty quantification. SBI addresses this limitation by building an empirical probabilistic model of the data errors using machine learning models, known as neural density estimators, which can then be integrated into the Bayesian inference framework. We apply the SBI framework to point-source moment tensor inversions as well as joint moment tensor and time-location inversions. We construct a range of synthetic examples to explore the quality of the SBI solutions, as well as to compare the SBI results with standard Gaussian likelihood-based Bayesian inversions. We then demonstrate that under real seismic noise, common Gaussian likelihood assumptions for treating full-waveform data yield overconfident posterior distributions that underestimate the moment tensor component uncertainties by up to a factor of 3. We contrast this with SBI, which produces well-calibrated posteriors that generally agree with the true seismic source parameters, and offers an order-of-magnitude reduction in the number of simulations required to perform inference compared to standard Monte Carlo techniques. Finally, we apply our methodology to a pair of moderate magnitude earthquakes in the North Atlantic. We utilise seismic waveforms recorded by the recent UPFLOW ocean bottom seismometer array as well as by regional land stations in the Azores, comparing full moment tensor and source-time location posteriors between SBI and a Gaussian likelihood approach. We find that our adaptation of SBI can be directly applied to real earthquake sources to efficiently produce high quality posterior distributions that significantly improve upon Gaussian likelihood approaches.","authors":["A. A. Saoulis","D. Piras","A. Spurio Mancini","B. Joachimi","A. M. G. Ferreira"],"url":"https://arxiv.org/abs/2410.23238"}
{"created":"2025-05-15","title":"Naturality for higher-dimensional path types","abstract":"We define a naturality construction for the operations of weak omega-categories, as a meta-operation in a dependent type theory. Our construction has a geometrical motivation as a local tensor product with a directed interval, and behaves logically as a globular analogue of Reynolds parametricity. Our construction operates as a ``power tool'' to support construction of terms with geometrical structure, and we use it to define composition operations for cylinders and cones in omega-categories. The machinery can generate terms of high complexity, and we have implemented our construction in a proof assistant, which verifies that the generated terms have the correct type. All our results can be exported to homotopy type theory, allowing the explicit computation of complex path type inhabitants.","authors":["Thibaut Benjamin","Ioannis Markakis","Wilfred Offord","Chiara Sarti","Jamie Vicary"],"url":"https://arxiv.org/abs/2501.11620"}
{"created":"2025-05-15","title":"High-temperature superconductivity in Li$_2$AuH$_6$ mediated by strong electron-phonon coupling under ambient pressure","abstract":"We used our developed AI search engine~(InvDesFlow) to perform extensive investigations regarding ambient stable superconducting hydrides. A cubic structure Li$_2$AuH$_6$ with Au-H octahedral motifs is identified to be a candidate. After performing thermodynamical analysis, we provide a feasible route to experimentally synthesize this material via the known LiAu and LiH compounds under ambient pressure. The further first-principles calculations suggest that Li$_2$AuH$_6$ shows a high superconducting transition temperature ($T_c$) $\\sim$ 140 K under ambient pressure. The H-1$s$ electrons strongly couple with phonon modes of vibrations of Au-H octahedrons as well as vibrations of Li atoms, where the latter is not taken seriously in other previously similar cases. Hence, different from previous claims of searching metallic covalent bonds to find high-$T_c$ superconductors, we emphasize here the importance of those phonon modes with strong electron-phonon coupling (EPC). And we suggest that one can intercalate atoms into binary or ternary hydrides to introduce more potential phonon modes with strong EPC, which is an effective approach to find high-$T_c$ superconductors within multicomponent compounds.","authors":["Zhenfeng Ouyang","Bo-Wen Yao","Xiao-Qi Han","Peng-Jie Guo","Ze-Feng Gao","Zhong-Yi Lu"],"url":"https://arxiv.org/abs/2501.12222"}
{"created":"2025-05-15","title":"On Overlap Ratio in Defocused Electron Ptychography","abstract":"Four-dimensional Scanning Transmission Electron Microscopy (4D STEM) with data acquired using a defocused electron probe is a promising tool for characterising complex biological specimens and materials through a phase retrieval process known as Electron Ptychography (EP). The efficacy of 4D STEM acquisition and the resulting quality of EP reconstruction depends on the overlap ratio of adjacent illuminated areas. This paper demonstrates how the overlap ratio impacts the data redundancy and the quality of the EP reconstruction. We define two quantities as a function of the overlap ratio that are independent of both the object and the EP algorithm. Subsequently, we evaluate an EP algorithm for varying overlap ratios using simulated 4D STEM datasets. Notably, a 40% or greater overlap ratio yields stable, high-quality reconstructions.","authors":["Amirafshar Moshtaghpour","Angus I. Kirkland"],"url":"https://arxiv.org/abs/2502.00762"}
{"created":"2025-05-15","title":"An Efficient Iterative Algorithm for Qubit Mapping via Layer-Weight Assignment and Search Space Reduction","abstract":"Current quantum devices support interactions only between physically adjacent qubits, preventing quantum circuits from being directly executed on these devices. Therefore, SWAP gates are required to remap logical qubits to physical qubits, which in turn increases both quantum resource consumption and error rates. To minimize the insertion of additional SWAP gates, we propose HAIL, an efficient iterative qubit mapping algorithm. Leveraging the inherent parallelism in quantum circuits, a new layer-weight assignment method is integrated with subgraph isomorphism to derive an optimal initial qubit mapping. Moreover, we present a two-stage SWAP sequence search algorithm that effectively identifies the most efficient SWAP sequence by distilling feasible SWAP sequences at different stages. The whole qubit mapping algorithm is then refined through a few iterative bidirectional traversals, further reducing the number of SWAP gates required. Experimental results on the IBM Q20 architecture and various benchmarks show that HAIL-3 reduces the number of additional gates inserted in the $\\mathcal{B}_{23}$ by 20.62\\% compared to state-of-the-art algorithms. Moreover, we propose a partially extended SWAP sequence strategy combined with HAIL to reduce its time complexity, with experiments on the sparsely connected Google Sycamore architecture demonstrating reductions in both algorithm runtime and additional SWAP gates.","authors":["Kang Xu","Zeyang Li","Xinjian Liu","Dandan Li","Yukun Wang"],"url":"https://arxiv.org/abs/2502.07536"}
{"created":"2025-05-15","title":"Transfer Learning of CATE with Kernel Ridge Regression","abstract":"The proliferation of data has sparked significant interest in leveraging findings from one study to estimate treatment effects in a different target population without direct outcome observations. However, the transfer learning process is frequently hindered by substantial covariate shift and limited overlap between (i) the source and target populations, as well as (ii) the treatment and control groups within the source. We propose a novel method for overlap-adaptive transfer learning of conditional average treatment effect (CATE) using kernel ridge regression (KRR). Our approach involves partitioning the labeled source data into two subsets. The first one is used to train candidate CATE models based on regression adjustment and pseudo-outcomes. An optimal model is then selected using the second subset and unlabeled target data, employing another pseudo-outcome-based strategy. We provide a theoretical justification for our method through sharp non-asymptotic MSE bounds, highlighting its adaptivity to both weak overlaps and the complexity of CATE function. Extensive numerical studies confirm that our method achieves superior finite-sample efficiency and adaptability. We conclude by demonstrating the effectiveness of our approach using a 401(k) eligibility dataset.","authors":["Seok-Jin Kim","Hongjie Liu","Molei Liu","Kaizheng Wang"],"url":"https://arxiv.org/abs/2502.11331"}
{"created":"2025-05-15","title":"Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood","abstract":"We adopt Gaussian Processes (GPs) as latent functions for probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function and marginalizes it out when making predictions. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which has both a point mass at zero and heavy tails, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.","authors":["Stefano Damato","Dario Azzimonti","Giorgio Corani"],"url":"https://arxiv.org/abs/2502.19086"}
{"created":"2025-05-15","title":"The Problem of the Priors, or Posteriors?","abstract":"The problem of the priors is well known: it concerns the challenge of identifying norms that govern one's prior credences. I argue that a key to addressing this problem lies in considering what I call the problem of the posteriors -- the challenge of identifying norms that directly govern one's posterior credences, which backward induce some norms on the priors via the diachronic requirement of conditionalization. This forward-looking approach can be summarized as: Think ahead, work backward. Although this idea can be traced to Freedman (1963), Carnap (1963), and Shimony (1970), I believe that it has not received enough attention. In this paper, I initiate a systematic defense of forward-looking Bayesianism, addressing potential objections from more traditional views (both subjectivist and objectivist). I also develop a specific approach to forward-looking Bayesianism -- one that values the convergence of posterior credences to the truth, and treats it as a fundamental rather than derived norm. This approach, called {\\em convergentist Bayesianism}, is argued to be crucial for a Bayesian foundation of Ockham's razor in statistics and machine learning.","authors":["Hanti Lin"],"url":"https://arxiv.org/abs/2503.10984"}
{"created":"2025-05-15","title":"IAEmu: Learning Galaxy Intrinsic Alignment Correlations","abstract":"The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing analyses, arise from correlations in galaxy shapes driven by tidal interactions and galaxy formation processes. Accurate IA modeling is essential for robust cosmological inference, but current approaches rely on perturbative methods that break down on nonlinear scales or on expensive simulations. We introduce IAEmu, a neural network-based emulator that predicts the galaxy position-position ($\\xi$), position-orientation ($\\omega$), and orientation-orientation ($\\eta$) correlation functions and their uncertainties using mock catalogs based on the halo occupation distribution (HOD) framework. Compared to simulations, IAEmu achieves ~3% average error for $\\xi$ and ~5% for $\\omega$, while capturing the stochasticity of $\\eta$ without overfitting. The emulator provides both aleatoric and epistemic uncertainties, helping identify regions where predictions may be less reliable. We also demonstrate generalization to non-HOD alignment signals by fitting to IllustrisTNG hydrodynamical simulation data. As a fully differentiable neural network, IAEmu enables $\\sim$10,000$\\times$ speed-ups in mapping HOD parameters to correlation functions on GPUs, compared to CPU-based simulations. This acceleration facilitates inverse modeling via gradient-based sampling, making IAEmu a powerful surrogate model for galaxy bias and IA studies with direct applications to Stage IV weak lensing surveys.","authors":["Sneh Pandya","Yuanyuan Yang","Nicholas Van Alfen","Jonathan Blazek","Robin Walters"],"url":"https://arxiv.org/abs/2504.05235"}
{"created":"2025-05-15","title":"Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure","abstract":"Financial scenario simulation is essential for risk management and portfolio optimization, yet it remains challenging especially in high-dimensional and small data settings common in finance. We propose a diffusion factor model that integrates latent factor structure into generative diffusion processes, bridging econometrics with modern generative AI to address the challenges of the curse of dimensionality and data scarcity in financial simulation. By exploiting the low-dimensional factor structure inherent in asset returns, we decompose the score function--a key component in diffusion models--using time-varying orthogonal projections, and this decomposition is incorporated into the design of neural network architectures. We derive rigorous statistical guarantees, establishing nonasymptotic error bounds for both score estimation at O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4} n^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather than the number of assets d, surpassing the dimension-dependent limits in the classical nonparametric statistics literature and making the framework viable for markets with thousands of assets. Numerical studies confirm superior performance in latent subspace recovery under small data regimes. Empirical analysis demonstrates the economic significance of our framework in constructing mean-variance optimal portfolios and factor portfolios. This work presents the first theoretical integration of factor structure with diffusion models, offering a principled approach for high-dimensional financial simulation with limited data. Our code is available at https://github.com/xymmmm00/diffusion_factor_model.","authors":["Minshuo Chen","Renyuan Xu","Yumin Xu","Ruixun Zhang"],"url":"https://arxiv.org/abs/2504.06566"}
{"created":"2025-05-15","title":"Shifting Work Patterns with Generative AI","abstract":"We present evidence on how generative AI changes the work patterns of knowledge workers using data from a 6-month-long, cross-industry, randomized field experiment. Half of the 7,137 workers in the study received access to a generative AI tool integrated into the applications they already used for emails, document creation, and meetings. We find that access to the AI tool during the first year of its release primarily impacted behaviors that workers could change independently and not behaviors that require coordination to change: workers who used the tool in more than half of the sample weeks spent 3.6 fewer hours, or 31% less time on email each week (intent to treat estimate is 1.3 hours) and completed documents moderately faster, but did not significantly change time spent in meetings.","authors":["Eleanor Wiske Dillon","Sonia Jaffe","Nicole Immorlica","Christopher T. Stanton"],"url":"https://arxiv.org/abs/2504.11436"}
{"created":"2025-05-15","title":"Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets","abstract":"Deep learning models have great potential in medical imaging, including orthodontics and skeletal maturity assessment. However, applying a model to data different from its training set can lead to unreliable predictions that may impact patient care. To address this, we propose a comprehensive verification framework that evaluates model suitability through multiple complementary strategies. First, we introduce a Gradient Attention Map (GAM)-based approach that analyzes attention patterns using Grad-CAM and compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance. Second, we extend verification to early convolutional feature maps, capturing structural mis-alignments missed by attention alone. Finally, we incorporate an additional garbage class into the classification model to explicitly reject out-of-distribution inputs. Experimental results demonstrate that these combined methods effectively identify unsuitable models and inputs, promoting safer and more reliable deployment of deep learning in medical imaging.","authors":["Omid Halimi Milani","Amanda Nikho","Lauren Mills","Marouane Tliba","Ahmet Enis Cetin","Mohammed H. Elnagar"],"url":"https://arxiv.org/abs/2504.21227"}
{"created":"2025-05-15","title":"Nystr\\\"om Type Exponential Integrators for Strongly Magnetized Charged Particle Dynamics","abstract":"Calculating the dynamics of charged particles in electromagnetic fields (i.e. the particle pushing problem) is one of the most computationally intensive components of particle-in-cell (PIC) methods for plasma physics simulations. This task is especially challenging when the plasma is strongly magnetized, since in this case the particle motion consists of a wide range of temporal scales from highly oscillatory fast gyromotion to slow macroscopic behavior and the resulting numerical model is very stiff. Current state-of-the-art time integrators used to simulate particle motion have limitations given the severe numerical stiffness of the problem and more efficient methods are of interest. Recently, exponential integrators have been proposed as a promising new approach for these simulations and shown to offer computational advantages over commonly used schemes. Exponential methods can solve linear problems exactly and are $A$-stable. In this paper, the standard exponential algorithms framework is extended to derive Nystr\\\"om-type exponential methods that integrate the Newtonian equations of motion as a second-order differential equation. Specific Nystr\\\"om-type schemes of second and third orders are derived and applied to strongly magnetized particle pushing problems. Numerical experiments are presented to demonstrate that the Nystr\\\"om-type exponential integrators can provide significant improvement in computational efficiency over the standard exponential methods.","authors":["Tri P. Nguyen","Ilon Joseph","Mayya Tokman"],"url":"https://arxiv.org/abs/2505.00288"}
{"created":"2025-05-15","title":"Easz: An Agile Transformer-based Image Compression Framework for Resource-constrained IoTs","abstract":"Neural image compression, necessary in various machine-to-machine communication scenarios, suffers from its heavy encode-decode structures and inflexibility in switching between different compression levels. Consequently, it raises significant challenges in applying the neural image compression to edge devices that are developed for powerful servers with high computational and storage capacities. We take a step to solve the challenges by proposing a new transformer-based edge-compute-free image coding framework called Easz. Easz shifts the computational overhead to the server, and hence avoids the heavy encoding and model switching overhead on the edge. Easz utilizes a patch-erase algorithm to selectively remove image contents using a conditional uniform-based sampler. The erased pixels are reconstructed on the receiver side through a transformer-based framework. To further reduce the computational overhead on the receiver, we then introduce a lightweight transformer-based reconstruction structure to reduce the reconstruction load on the receiver side. Extensive evaluations conducted on a real-world testbed demonstrate multiple advantages of Easz over existing compression approaches, in terms of adaptability to different compression levels, computational efficiency, and image reconstruction quality.","authors":["Yu Mao","Jingzong Li","Jun Wang","Hong Xu","Tei-Wei Kuo","Nan Guan","Chun Jason Xue"],"url":"https://arxiv.org/abs/2505.01742"}
{"created":"2025-05-15","title":"Multiplication of polynomials over the binary field","abstract":"Additive Fourier Transform is sdudied. A fast multiplication algorithm for polynomials over the binary field is given. The bit complexity of the algorithm is $O(n(log n)(\\log\\log n)^2)$.","authors":["Chunlei Liu"],"url":"https://arxiv.org/abs/2505.03101"}
{"created":"2025-05-15","title":"Simulating many-engine spacecraft: Exceeding 100 trillion grid points via information geometric regularization and the MFC flow solver","abstract":"This work proposes a method and optimized implementation for exascale simulations of high-speed compressible fluid flows, enabling the simulation of multi-engine rocket craft at an unprecedented scale. We significantly improve upon the state-of-the-art in terms of computational cost and memory footprint through a carefully crafted implementation of the recently proposed information geometric regularization, which eliminates the need for numerical shock capturing. Unified addressing on tightly coupled CPU--GPU platforms increases the total problem size with negligible performance hit. Despite linear stencil algorithms being memory-bound, we achieve wall clock times that are four times faster than optimized baseline numerics. This enables the execution of CFD simulations at more than 100 trillion grid points, surpassing the largest state-of-the-art publicly available simulations by an order of magnitude. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps using the full system, entailing 37.8K AMD MI250X GPUs (Frontier) or 9.2K NVIDIA GH200 superchips (Alps).","authors":["Benjamin Wilfong","Anand Radhakrishnan","Henry Le Berre","Nikolaos Tselepidis","Benedikt Dorschner","Reuben Budiardja","Brian Cornille","Stephen Abbott","Florian Sch\\\"afer","Spencer H. Bryngelson"],"url":"https://arxiv.org/abs/2505.07392"}
{"created":"2025-05-15","title":"Efficient Lifting of Discrete Logarithms Modulo Prime Powers","abstract":"We present a deterministic algorithm that, given a prime $p$ and a solution $x \\in \\mathbb Z$ to the discrete logarithm problem $a^x \\equiv b \\pmod p$ with $p\\nmid a$, efficiently lifts it to a solution modulo $p^k$, i.e., $a^x \\equiv b \\pmod {p^k}$, for any fixed $k \\geq 1$.","authors":["Giovanni Viglietta","Yasuyuki Kachi"],"url":"https://arxiv.org/abs/2505.07434"}
