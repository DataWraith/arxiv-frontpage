{"created":"2025-05-01","title":"Research on CNN-BiLSTM Network Traffic Anomaly Detection Model Based on MindSpore","abstract":"With the widespread adoption of the Internet of Things (IoT) and Industrial IoT (IIoT) technologies, network architectures have become increasingly complex, and the volume of traffic has grown substantially. This evolution poses significant challenges to traditional security mechanisms, particularly in detecting high-frequency, diverse, and highly covert network attacks. To address these challenges, this study proposes a novel network traffic anomaly detection model that integrates a Convolutional Neural Network (CNN) with a Bidirectional Long Short-Term Memory (BiLSTM) network, implemented on the MindSpore framework. Comprehensive experiments were conducted using the NF-BoT-IoT dataset. The results demonstrate that the proposed model achieves 99% across accuracy, precision, recall, and F1-score, indicating its strong performance and robustness in network intrusion detection tasks.","authors":["Qiuyan Xiang","Shuang Wu","Dongze Wu","Yuxin Liu","Zhenkai Qin"],"url":"https://arxiv.org/abs/2504.21008"}
{"created":"2025-05-01","title":"Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models","abstract":"What underlies intuitive human thinking? One approach to this question is to compare the cognitive dynamics of humans and large language models (LLMs). However, such a comparison requires a method to quantitatively analyze AI cognitive behavior under controlled conditions. While anecdotal observations suggest that certain prompts can dramatically change LLM behavior, these observations have remained largely qualitative. Here, we propose a two-part framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP) that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying Prompt (TQP) that evaluates this change using a separate LLM. Through controlled experiments, we examined how LLMs react to prompts embedding two semantically distant concepts (e.g., mathematical aperiodicity and traditional crafts)--either fused together or presented separately--by changing their linguistic quality and affective tone. Whereas humans tend to experience heightened engagement when such concepts are meaningfully blended producing a novel concept--a form of conceptual fusion--current LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts. This suggests that LLMs may not yet replicate the conceptual integration processes seen in human intuition. Our method enables fine-grained, reproducible measurement of cognitive responsiveness, and may help illuminate key differences in how intuition and conceptual leaps emerge in artificial versus human minds.","authors":["Makoto Sato"],"url":"https://arxiv.org/abs/2504.21012"}
{"created":"2025-05-01","title":"Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge","abstract":"Artificial Intelligence (AI)-generated feedback in educational settings has garnered considerable attention due to its potential to enhance learning outcomes. However, a comprehensive understanding of the linguistic characteristics of AI-generated feedback, including readability, lexical richness, and adaptability across varying challenge levels, remains limited. This study delves into the linguistic and structural attributes of feedback generated by Google's Gemini 1.5-flash text model for computer science multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed, considering three difficulty levels (easy, medium, hard) and three feedback tones (supportive, neutral, challenging). Key linguistic metrics, such as length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness, and lexical density, were computed and examined. A fine-tuned RoBERTa-based multi-task learning (MTL) model was trained to predict these linguistic properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and 0.03 for vocabulary richness. The findings reveal significant interaction effects between feedback tone and question difficulty, demonstrating the dynamic adaptation of AI-generated feedback within diverse educational contexts. These insights contribute to the development of more personalized and effective AI-driven feedback mechanisms, highlighting the potential for improved learning outcomes while underscoring the importance of ethical considerations in their design and deployment.","authors":["Antoun Yaacoub","Zainab Assaghir","Lionel Prevost","J\\'er\\^ome Da-Rugna"],"url":"https://arxiv.org/abs/2504.21013"}
{"created":"2025-05-01","title":"Don't Retrieve, Generate: Prompting LLMs for Synthetic Training Data in Dense Retrieval","abstract":"Training effective dense retrieval models often relies on hard negative (HN) examples mined from the document corpus via methods like BM25 or cross-encoders (CE), processes that can be computationally demanding and require full corpus access. This paper introduces a different approach, an end-to-end pipeline where a Large Language Model (LLM) first generates a query from a passage, and then generates a hard negative example using \\emph{only} that query text. This corpus-free negative generation contrasts with standard mining techniques. We evaluated this \\textsc{LLM Query $\\rightarrow$ LLM HN} approach against traditional \\textsc{LLM Query $\\rightarrow$ BM25 HN} and \\textsc{LLM Query $\\rightarrow$ CE HN} pipelines using E5-Base and GTE-Base models on several BEIR benchmark datasets. Our results show the proposed all-LLM pipeline achieves performance identical to both the BM25 and the computationally intensive CE baselines across nDCG@10, Precision@10, and Recall@100 metrics. This demonstrates that our corpus-free negative generation method matches the effectiveness of complex, corpus-dependent mining techniques, offering a potentially simpler and more efficient pathway for training high-performance retrievers without sacrificing results. We make the dataset including the queries and the hard-negatives for all three methods publicly available https://huggingface.co/collections/chungimungi/arxiv-hard-negatives-68027bbc601ff6cc8eb1f449.","authors":["Aarush Sinha"],"url":"https://arxiv.org/abs/2504.21015"}
{"created":"2025-05-01","title":"Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments","abstract":"The COVID-19 pandemic caused great losses worldwide, efforts are taken place to prevent but many countries have failed. In Vietnam, the traceability, localization, and quarantine of people who contact with patients contribute to effective disease prevention. However, this is done by hand, and take a lot of work. In this research, we describe a named-entity recognition (NER) study that assists in the prevention of COVID-19 pandemic in Vietnam. We also present our manually annotated COVID-19 dataset with nested named entity recognition task for Vietnamese which be defined new entity types using for our system.","authors":["Ngoc C. L\\^e","Hai-Chung Nguyen-Phung","Thu-Huong Pham Thi","Hue Vu","Phuong-Thao Nguyen Thi","Thu-Thuy Tran","Hong-Nhung Le Thi","Thuy-Duong Nguyen-Thi","Thanh-Huy Nguyen"],"url":"https://arxiv.org/abs/2504.21016"}
{"created":"2025-05-01","title":"ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese","abstract":"After two years of appearance, COVID-19 has negatively affected people and normal life around the world. As in May 2022, there are more than 522 million cases and six million deaths worldwide (including nearly ten million cases and over forty-three thousand deaths in Vietnam). Economy and society are both severely affected. The variant of COVID-19, Omicron, has broken disease prevention measures of countries and rapidly increased number of infections. Resources overloading in treatment and epidemics prevention is happening all over the world. It can be seen that, application of artificial intelligence (AI) to support people at this time is extremely necessary. There have been many studies applying AI to prevent COVID-19 which are extremely useful, and studies on machine reading comprehension (MRC) are also in it. Realizing that, we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and can be used to build models and systems, contributing to disease prevention. Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, we hope that it can contribute to promoting MRC studies in Vietnamese and multilingual.","authors":["Hai-Chung Nguyen-Phung","Ngoc C. L\\^e","Van-Chien Nguyen","Hang Thi Nguyen","Thuy Phuong Thi Nguyen"],"url":"https://arxiv.org/abs/2504.21017"}
{"created":"2025-05-01","title":"HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization","abstract":"Many pre-trained language models (PLMs) exhibit suboptimal performance on mid- and low-resource languages, largely due to limited exposure to these languages during pre-training. A common strategy to address this is to introduce new tokens specific to the target languages, initialize their embeddings, and apply continual pre-training on target-language data. Among such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword embedding initialization heuristic that is both effective and efficient. However, OFA restricts target-language token embeddings to be convex combinations of a fixed number of source-language embeddings, which may limit expressiveness. To overcome this limitation, we propose HYPEROFA, a hypernetwork-based approach for more adaptive token embedding initialization. The hypernetwork is trained to map from an external multilingual word vector space to the PLMs token embedding space using source-language tokens. Once trained, it can generate flexible embeddings for target-language tokens, serving as a good starting point for continual pretraining. Experiments demonstrate that HYPEROFA consistently outperforms random initialization baseline and matches or exceeds the performance of OFA in both continual pre-training convergence and downstream task performance. We make the code publicly available.","authors":["Enes \\\"Ozeren","Yihong Liu","Hinrich Sch\\\"utze"],"url":"https://arxiv.org/abs/2504.21018"}
{"created":"2025-05-01","title":"Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations","abstract":"The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness. However, existing methods either focus on model generalization or concentrate on robustness. The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we argue that robustness can be view as a specific form of domain shift, and empirically reveal an intrinsic mechanism for model generalization of AIGT detection task. Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action. Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios. Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks. The code is publicly available at https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.","authors":["Yinghan Zhou","Juan Wen","Wanli Peng","Yiming Xue","Ziwei Zhang","Zhengxian Wu"],"url":"https://arxiv.org/abs/2504.21019"}
{"created":"2025-05-01","title":"Context-Enhanced Contrastive Search for Improved LLM Text Generation","abstract":"Recently, Large Language Models (LLMs) have demonstrated remarkable advancements in Natural Language Processing (NLP). However, generating high-quality text that balances coherence, diversity, and relevance remains challenging. Traditional decoding methods, such as bean search and top-k sampling, often struggle with either repetitive or incoherent outputs, particularly in tasks that require long-form text generation. To address these limitations, the paper proposes a novel enhancement of the well-known Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with contextual calibration. The proposed scheme introduces several novelties including dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control, to optimize the balance between fluency, creativity, and precision. The performance of CECS is evaluated using several standard metrics such as BLEU, ROUGE, and semantic similarity. Experimental results demonstrate significant improvements in both coherence and relevance of the generated texts by CECS outperforming the existing Contrastive Search techniques. The proposed algorithm has several potential applications in the real world including legal document drafting, customer service chatbots, and content marketing.","authors":["Jaydip Sen","Rohit Pandey","Hetvi Waghela"],"url":"https://arxiv.org/abs/2504.21020"}
{"created":"2025-05-01","title":"ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees","abstract":"Linear Temporal Logic (LTL) has become a prevalent specification language for robotic tasks. To mitigate the significant manual effort and expertise required to define LTL-encoded tasks, several methods have been proposed for translating Natural Language (NL) instructions into LTL formulas, which, however, lack correctness guarantees. To address this, we introduce a new NL-to-LTL translation method, called ConformalNL2LTL, that can achieve user-defined translation success rates over unseen NL commands. Our method constructs LTL formulas iteratively by addressing a sequence of open-vocabulary Question-Answering (QA) problems with LLMs. To enable uncertainty-aware translation, we leverage conformal prediction (CP), a distribution-free uncertainty quantification tool for black-box models. CP enables our method to assess the uncertainty in LLM-generated answers, allowing it to proceed with translation when sufficiently confident and request help otherwise. We provide both theoretical and empirical results demonstrating that ConformalNL2LTL achieves user-specified translation accuracy while minimizing help rates.","authors":["Jun Wang","David Smith Sundarsingh","Jyotirmoy V. Deshmukh","Yiannis Kantaros"],"url":"https://arxiv.org/abs/2504.21022"}
{"created":"2025-05-01","title":"Param$\\Delta$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost","abstract":"The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to repeated post-training and evaluation after each base model update. This paper introduces $Param\\Delta$, a novel method that streamlines post-training by transferring knowledge from an existing post-trained model to a newly updated base model with ZERO additional training. By computing the difference between post-trained model weights ($\\Theta_\\text{post}$) and base model weights ($\\Theta_\\text{base}$), and adding this to the updated base model ($\\Theta'_\\text{base}$), we define $Param\\Delta$ Model as: $\\Theta_{\\text{Param}\\Delta} = \\Theta_\\text{post} - \\Theta_\\text{base} + \\Theta'_\\text{base}$. This approach surprisingly equips the new base model with post-trained capabilities, achieving performance comparable to direct post-training. We did analysis on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models. Results indicate $Param\\Delta$ Model effectively replicates traditional post-training. For example, the $Param\\Delta$ Model obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains approximately 95\\% of Llama3.1-inst model's performance on average. $Param\\Delta$ brings a new perspective on how to fully leverage models in the open-weight community, where checkpoints for base and instruct models are readily available and frequently updated, by providing a cost-free framework to accelerate the iterative cycle of model development.","authors":["Sheng Cao","Mingrui Wu","Karthik Prasad","Yuandong Tian","Zechun Liu"],"url":"https://arxiv.org/abs/2504.21023"}
{"created":"2025-05-01","title":"WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model","abstract":"Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability.","authors":["Tianqing Fang","Hongming Zhang","Zhisong Zhang","Kaixin Ma","Wenhao Yu","Haitao Mi","Dong Yu"],"url":"https://arxiv.org/abs/2504.21024"}
{"created":"2025-05-01","title":"Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh","abstract":"Road accidents pose significant concerns globally. They lead to large financial losses, injuries, disabilities, and societal challenges. Accurate and timely accident data is essential for predicting and mitigating these events. This paper presents a novel framework named 'Durghotona GPT' that integrates web scraping and Large Language Models (LLMs) to automate the generation of comprehensive accident datasets from prominent national dailies in Bangladesh. The authors collected accident reports from three major newspapers: Prothom Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework efficiently extracts relevant information, categorizes reports, and compiles detailed datasets. Thus, this framework overcomes limitations of manual data collection methods such as delays, errors, and communication gaps. The authors' evaluation demonstrates that Llama-3, an open-source model, performs comparably to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it can be considered a cost-effective alternative for similar tasks. The results suggest that the framework developed by the authors can drastically enhance the quality and availability of accident data. As a result, it can support critical applications in traffic safety analysis, urban planning, and public health. The authors also developed an interface for 'Durghotona GPT' for ease of use as part of this paper. Future work will focus on expanding data collection methods and refining LLMs to further increase dataset accuracy and applicability.","authors":["MD Thamed Bin Zaman Chowdhury","Moazzem Hossain","Md. Ridwanul Islam"],"url":"https://arxiv.org/abs/2504.21025"}
{"created":"2025-05-01","title":"Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models","abstract":"With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.","authors":["Manish Pandey","Nageshwar Prasad Yadav","Mokshada Adduru","Sawan Rai"],"url":"https://arxiv.org/abs/2504.21026"}
{"created":"2025-05-01","title":"UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models","abstract":"The advent of Large Language Models (LLMs) holds promise for revolutionizing various fields traditionally dominated by human expertise. Urban planning, a professional discipline that fundamentally shapes our daily surroundings, is one such field heavily relying on multifaceted domain knowledge and experience of human experts. The extent to which LLMs can assist human practitioners in urban planning remains largely unexplored. In this paper, we introduce a comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of LLMs in urban planning, which encompasses fundamental principles, professional knowledge, and management and regulations, aligning closely with the qualifications expected of human planners. Through extensive evaluation, we reveal a significant imbalance in the acquisition of planning knowledge among LLMs, with even the most proficient models falling short of meeting professional standards. For instance, we observe that 70% of LLMs achieve subpar performance in understanding planning regulations compared to other aspects. Besides the benchmark, we present the largest-ever supervised fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction pairs sourced from urban planning exams and textbooks. Our findings demonstrate that fine-tuned models exhibit enhanced performance in memorization tests and comprehension of urban planning knowledge, while there exists significant room for improvement, particularly in tasks requiring domain-specific terminology and reasoning. By making our benchmark, dataset, and associated evaluation and fine-tuning toolsets publicly available at https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the integration of LLMs into practical urban planning, fostering a symbiotic collaboration between human expertise and machine intelligence.","authors":["Yu Zheng","Longyi Liu","Yuming Lin","Jie Feng","Guozhen Zhang","Depeng Jin","Yong Li"],"url":"https://arxiv.org/abs/2504.21027"}
{"created":"2025-05-01","title":"Semantic-Aware Contrastive Fine-Tuning: Boosting Multimodal Malware Classification with Discriminative Embeddings","abstract":"The rapid evolution of malware variants requires robust classification methods to enhance cybersecurity. While Large Language Models (LLMs) offer potential for generating malware descriptions to aid family classification, their utility is limited by semantic embedding overlaps and misalignment with binary behavioral features. We propose a contrastive fine-tuning (CFT) method that refines LLM embeddings via targeted selection of hard negative samples based on cosine similarity, enabling LLMs to distinguish between closely related malware families. Our approach combines high-similarity negatives to enhance discriminative power and mid-tier negatives to increase embedding diversity, optimizing both precision and generalization. Evaluated on the CIC-AndMal-2020 and BODMAS datasets, our refined embeddings are integrated into a multimodal classifier within a Model-Agnostic Meta-Learning (MAML) framework on a few-shot setting. Experiments demonstrate significant improvements: our method achieves 63.15% classification accuracy with as few as 20 samples on CIC-AndMal-2020, outperforming baselines by 11--21 percentage points and surpassing prior negative sampling strategies. Ablation studies confirm the superiority of similarity-based selection over random sampling, with gains of 10-23%. Additionally, fine-tuned LLMs generate attribute-aware descriptions that generalize to unseen variants, bridging textual and binary feature gaps. This work advances malware classification by enabling nuanced semantic distinctions and provides a scalable framework for adapting LLMs to cybersecurity challenges.","authors":["Ivan Montoya Sanchez","Shaswata Mitra","Aritran Piplai","Sudip Mittal"],"url":"https://arxiv.org/abs/2504.21028"}
{"created":"2025-05-01","title":"PICO: Secure Transformers via Robust Prompt Isolation and Cybersecurity Oversight","abstract":"We propose a robust transformer architecture designed to prevent prompt injection attacks and ensure secure, reliable response generation. Our PICO (Prompt Isolation and Cybersecurity Oversight) framework structurally separates trusted system instructions from untrusted user inputs through dual channels that are processed independently and merged only by a controlled, gated fusion mechanism. In addition, we integrate a specialized Security Expert Agent within a Mixture-of-Experts (MoE) framework and incorporate a Cybersecurity Knowledge Graph (CKG) to supply domain-specific reasoning. Our training design further ensures that the system prompt branch remains immutable while the rest of the network learns to handle adversarial inputs safely. This PICO framework is presented via a general mathematical formulation, then elaborated in terms of the specifics of transformer architecture, and fleshed out via hypothetical case studies including Policy Puppetry attacks. While the most effective implementation may involve training transformers in a PICO-based way from scratch, we also present a cost-effective fine-tuning approach.","authors":["Ben Goertzel","Paulos Yibelo"],"url":"https://arxiv.org/abs/2504.21029"}
{"created":"2025-05-01","title":"Advancing Multi-Agent Systems Through Model Context Protocol: Architecture, Implementation, and Applications","abstract":"Multi-agent systems represent a significant advancement in artificial intelligence, enabling complex problem-solving through coordinated specialized agents. However, these systems face fundamental challenges in context management, coordination efficiency, and scalable operation. This paper introduces a comprehensive framework for advancing multi-agent systems through Model Context Protocol (MCP), addressing these challenges through standardized context sharing and coordination mechanisms. We extend previous work on AI agent architectures by developing a unified theoretical foundation, advanced context management techniques, and scalable coordination patterns. Through detailed implementation case studies across enterprise knowledge management, collaborative research, and distributed problem-solving domains, we demonstrate significant performance improvements compared to traditional approaches. Our evaluation methodology provides a systematic assessment framework with benchmark tasks and datasets specifically designed for multi-agent systems. We identify current limitations, emerging research opportunities, and potential transformative applications across industries. This work contributes to the evolution of more capable, collaborative, and context-aware artificial intelligence systems that can effectively address complex real-world challenges.","authors":["Naveen Krishnan"],"url":"https://arxiv.org/abs/2504.21030"}
{"created":"2025-05-01","title":"Selecting the Right LLM for eGov Explanations","abstract":"The perceived quality of the explanations accompanying e-government services is key to gaining trust in these institutions, consequently amplifying further usage of these services. Recent advances in generative AI, and concretely in Large Language Models (LLMs) allow the automation of such content articulations, eliciting explanations' interpretability and fidelity, and more generally, adapting content to various audiences. However, selecting the right LLM type for this has become a non-trivial task for e-government service providers. In this work, we adapted a previously developed scale to assist with this selection, providing a systematic approach for the comparative analysis of the perceived quality of explanations generated by various LLMs. We further demonstrated its applicability through the tax-return process, using it as an exemplar use case that could benefit from employing an LLM to generate explanations about tax refund decisions. This was attained through a user study with 128 survey respondents who were asked to rate different versions of LLM-generated explanations about tax refund decisions, providing a methodological basis for selecting the most appropriate LLM. Recognizing the practical challenges of conducting such a survey, we also began exploring the automation of this process by attempting to replicate human feedback using a selection of cutting-edge predictive techniques.","authors":["Lior Limonad","Fabiana Fournier","Hadar Mulian","George Manias","Spiros Borotis","Danai Kyrkou"],"url":"https://arxiv.org/abs/2504.21032"}
{"created":"2025-05-01","title":"Transcending Dimensions using Generative AI: Real-Time 3D Model Generation in Augmented Reality","abstract":"Traditional 3D modeling requires technical expertise, specialized software, and time-intensive processes, making it inaccessible for many users. Our research aims to lower these barriers by combining generative AI and augmented reality (AR) into a cohesive system that allows users to easily generate, manipulate, and interact with 3D models in real time, directly within AR environments. Utilizing cutting-edge AI models like Shap-E, we address the complex challenges of transforming 2D images into 3D representations in AR environments. Key challenges such as object isolation, handling intricate backgrounds, and achieving seamless user interaction are tackled through advanced object detection methods, such as Mask R-CNN. Evaluation results from 35 participants reveal an overall System Usability Scale (SUS) score of 69.64, with participants who engaged with AR/VR technologies more frequently rating the system significantly higher, at 80.71. This research is particularly relevant for applications in gaming, education, and AR-based e-commerce, offering intuitive, model creation for users without specialized skills.","authors":["Majid Behravan","Maryam Haghani","Denis Gracanin"],"url":"https://arxiv.org/abs/2504.21033"}
{"created":"2025-05-01","title":"SAGA: A Security Architecture for Governing AI Agentic Systems","abstract":"Large Language Model (LLM)-based agents increasingly interact, collaborate, and delegate tasks to one another autonomously with minimal human interaction. Industry guidelines for agentic system governance emphasize the need for users to maintain comprehensive control over their agents, mitigating potential damage from malicious agents. Several proposed agentic system designs address agent identity, authorization, and delegation, but remain purely theoretical, without concrete implementation and evaluation. Most importantly, they do not provide user-controlled agent management. To address this gap, we propose SAGA, a Security Architecture for Governing Agentic systems, that offers user oversight over their agents' lifecycle. In our design, users register their agents with a central entity, the Provider, that maintains agents contact information, user-defined access control policies, and helps agents enforce these policies on inter-agent communication. We introduce a cryptographic mechanism for deriving access control tokens, that offers fine-grained control over an agent's interaction with other agents, balancing security and performance consideration. We evaluate SAGA on several agentic tasks, using agents in different geolocations, and multiple on-device and cloud LLMs, demonstrating minimal performance overhead with no impact on underlying task utility in a wide range of conditions. Our architecture enables secure and trustworthy deployment of autonomous agents, accelerating the responsible adoption of this technology in sensitive environments.","authors":["Georgios Syros","Anshuman Suri","Cristina Nita-Rotaru","Alina Oprea"],"url":"https://arxiv.org/abs/2504.21034"}
{"created":"2025-05-01","title":"A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage","abstract":"Sanitizing sensitive text data typically involves removing personally identifiable information (PII) or generating synthetic data under the assumption that these methods adequately protect privacy; however, their effectiveness is often only assessed by measuring the leakage of explicit identifiers but ignoring nuanced textual markers that can lead to re-identification. We challenge the above illusion of privacy by proposing a new framework that evaluates re-identification attacks to quantify individual privacy risks upon data release. Our approach shows that seemingly innocuous auxiliary information -- such as routine social activities -- can be used to infer sensitive attributes like age or substance use history from sanitized data. For instance, we demonstrate that Azure's commercial PII removal tool fails to protect 74\\% of information in the MedQA dataset. Although differential privacy mitigates these risks to some extent, it significantly reduces the utility of the sanitized text for downstream tasks. Our findings indicate that current sanitization techniques offer a \\textit{false sense of privacy}, highlighting the need for more robust methods that protect against semantic-level information leakage.","authors":["Rui Xin","Niloofar Mireshghallah","Shuyue Stella Li","Michael Duan","Hyunwoo Kim","Yejin Choi","Yulia Tsvetkov","Sewoong Oh","Pang Wei Koh"],"url":"https://arxiv.org/abs/2504.21035"}
{"created":"2025-05-01","title":"Can Differentially Private Fine-tuning LLMs Protect Against Privacy Attacks?","abstract":"Fine-tuning large language models (LLMs) has become an essential strategy for adapting them to specialized tasks; however, this process introduces significant privacy challenges, as sensitive training data may be inadvertently memorized and exposed. Although differential privacy (DP) offers strong theoretical guarantees against such leakage, its empirical privacy effectiveness on LLMs remains unclear, especially under different fine-tuning methods. In this paper, we systematically investigate the impact of DP across fine-tuning methods and privacy budgets, using both data extraction and membership inference attacks to assess empirical privacy risks. Our main findings are as follows: (1) Differential privacy reduces model utility, but its impact varies significantly across different fine-tuning methods. (2) Without DP, the privacy risks of models fine-tuned with different approaches differ considerably. (3) When DP is applied, even a relatively high privacy budget can substantially lower privacy risk. (4) The privacy-utility trade-off under DP training differs greatly among fine-tuning methods, with some methods being unsuitable for DP due to severe utility degradation. Our results provide practical guidance for privacy-conscious deployment of LLMs and pave the way for future research on optimizing the privacy-utility trade-off in fine-tuning methodologies.","authors":["Hao Du","Shang Liu","Yang Cao"],"url":"https://arxiv.org/abs/2504.21036"}
{"created":"2025-05-01","title":"Security Bug Report Prediction Within and Across Projects: A Comparative Study of BERT and Random Forest","abstract":"Early detection of security bug reports (SBRs) is crucial for preventing vulnerabilities and ensuring system reliability. While machine learning models have been developed for SBR prediction, their predictive performance still has room for improvement. In this study, we conduct a comprehensive comparison between BERT and Random Forest (RF), a competitive baseline for predicting SBRs. The results show that RF outperforms BERT with a 34% higher average G-measure for within-project predictions. Adding only SBRs from various projects improves both models' average performance. However, including both security and nonsecurity bug reports significantly reduces RF's average performance to 46%, while boosts BERT to its best average performance of 66%, surpassing RF. In cross-project SBR prediction, BERT achieves a remarkable 62% G-measure, which is substantially higher than RF.","authors":["Farnaz Soltaniani","Mohammad Ghafari","Mohammed Sayagh"],"url":"https://arxiv.org/abs/2504.21037"}
{"created":"2025-05-01","title":"Prefill-Based Jailbreak: A Novel Approach of Bypassing LLM Safety Boundary","abstract":"Large Language Models (LLMs) are designed to generate helpful and safe content. However, adversarial attacks, commonly referred to as jailbreak, can bypass their safety protocols, prompting LLMs to generate harmful content or reveal sensitive data. Consequently, investigating jailbreak methodologies is crucial for exposing systemic vulnerabilities within LLMs, ultimately guiding the continuous implementation of security enhancements by developers. In this paper, we introduce a novel jailbreak attack method that leverages the prefilling feature of LLMs, a feature designed to enhance model output constraints. Unlike traditional jailbreak methods, the proposed attack circumvents LLMs' safety mechanisms by directly manipulating the probability distribution of subsequent tokens, thereby exerting control over the model's output. We propose two attack variants: Static Prefilling (SP), which employs a universal prefill text, and Optimized Prefilling (OP), which iteratively optimizes the prefill text to maximize the attack success rate. Experiments on six state-of-the-art LLMs using the AdvBench benchmark validate the effectiveness of our method and demonstrate its capability to substantially enhance attack success rates when combined with existing jailbreak approaches. The OP method achieved attack success rates of up to 99.82% on certain models, significantly outperforming baseline methods. This work introduces a new jailbreak attack method in LLMs, emphasizing the need for robust content validation mechanisms to mitigate the adversarial exploitation of prefilling features. All code and data used in this paper are publicly available.","authors":["Yakai Li","Jiekang Hu","Weiduan Sang","Luping Ma","Jing Xie","Weijuan Zhang","Aimin Yu","Shijie Zhao","Qingjia Huang","Qihang Zhou"],"url":"https://arxiv.org/abs/2504.21038"}
{"created":"2025-05-01","title":"Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report","abstract":"As transformer-based large language models (LLMs) increasingly permeate society, they have revolutionized domains such as software engineering, creative writing, and digital arts. However, their adoption in cybersecurity remains limited due to challenges like scarcity of specialized training data and complexity of representing cybersecurity-specific knowledge. To address these gaps, we present Foundation-Sec-8B, a cybersecurity-focused LLM built on the Llama 3.1 architecture and enhanced through continued pretraining on a carefully curated cybersecurity corpus. We evaluate Foundation-Sec-8B across both established and new cybersecurity benchmarks, showing that it matches Llama 3.1-70B and GPT-4o-mini in certain cybersecurity-specific tasks. By releasing our model to the public, we aim to accelerate progress and adoption of AI-driven tools in both public and private cybersecurity contexts.","authors":["Paul Kassianik","Baturay Saglam","Alexander Chen","Blaine Nelson","Anu Vellore","Massimo Aufiero","Fraser Burch","Dhruv Kedia","Avi Zohary","Sajana Weerawardhena","Aman Priyanshu","Adam Swanda","Amy Chang","Hyrum Anderson","Kojin Oshiba","Omar Santos","Yaron Singer","Amin Karbasi"],"url":"https://arxiv.org/abs/2504.21039"}
{"created":"2025-05-01","title":"Can a Large Language Model Assess Urban Design Quality? Evaluating Walkability Metrics Across Expertise Levels","abstract":"Urban street environments are vital to supporting human activity in public spaces. The emergence of big data, such as street view images (SVIs) combined with multimodal large language models (MLLMs), is transforming how researchers and practitioners investigate, measure, and evaluate semantic and visual elements of urban environments. Considering the low threshold for creating automated evaluative workflows using MLLMs, it is crucial to explore both the risks and opportunities associated with these probabilistic models. In particular, the extent to which the integration of expert knowledge can influence the performance of MLLMs in evaluating the quality of urban design has not been fully explored. This study sets out an initial exploration of how integrating more formal and structured representations of expert urban design knowledge into the input prompts of an MLLM (ChatGPT-4) can enhance the model's capability and reliability in evaluating the walkability of built environments using SVIs. We collect walkability metrics from the existing literature and categorize them using relevant ontologies. We then select a subset of these metrics, focusing on the subthemes of pedestrian safety and attractiveness, and develop prompts for the MLLM accordingly. We analyze the MLLM's ability to evaluate SVI walkability subthemes through prompts with varying levels of clarity and specificity regarding evaluation criteria. Our experiments demonstrate that MLLMs are capable of providing assessments and interpretations based on general knowledge and can support the automation of multimodal image-text evaluations. However, they generally provide more optimistic scores and can make mistakes when interpreting the provided metrics, resulting in incorrect evaluations. By integrating expert knowledge, the MLLM's evaluative performance exhibits higher consistency and concentration.","authors":["Chenyi Cai","Kosuke Kuriyama","Youlong Gu","Filip Biljecki","Pieter Herthogs"],"url":"https://arxiv.org/abs/2504.21040"}
{"created":"2025-05-01","title":"Fast and Robust Speckle Pattern Authentication by Scale Invariant Feature Transform algorithm in Physical Unclonable Functions","abstract":"Nowadays, due to the growing phenomenon of forgery in many fields, the interest in developing new anti-counterfeiting device and cryptography keys, based on the Physical Unclonable Functions (PUFs) paradigm, is widely increased. PUFs are physical hardware with an intrinsic, irreproducible disorder that allows for on-demand cryptographic key extraction. Among them, optical PUF are characterized by a large number of degrees of freedom resulting in higher security and higher sensitivity to environmental conditions. While these promising features led to the growth of advanced fabrication strategies and materials for new PUF devices, their combination with robust recognition algorithm remains largely unexplored. In this work, we present a metric-independent authentication approach that leverages the Scale Invariant Feature Transform (SIFT) algorithm to extract unique and invariant features from the speckle patterns generated by optical Physical Unclonable Functions (PUFs). The application of SIFT to the challenge response pairs (CRPs) protocol allows us to correctly authenticate a client while denying any other fraudulent access. In this way, the authentication process is highly reliable even in presence of response rotation, zooming, and cropping that may occur in consecutive PUF interrogations and to which other postprocessing algorithm are highly sensitive. This characteristics together with the speed of the method (tens of microseconds for each operation) broaden the applicability and reliability of PUF to practical high-security authentication or merchandise anti-counterfeiting.","authors":["Giuseppe Emanuele Lio","Mauro Daniel Luigi Bruno","Francesco Riboli","Sara Nocentini","Antonio Ferraro"],"url":"https://arxiv.org/abs/2504.21041"}
{"created":"2025-05-01","title":"What's Pulling the Strings? Evaluating Integrity and Attribution in AI Training and Inference through Concept Shift","abstract":"The growing adoption of artificial intelligence (AI) has amplified concerns about trustworthiness, including integrity, privacy, robustness, and bias. To assess and attribute these threats, we propose ConceptLens, a generic framework that leverages pre-trained multimodal models to identify the root causes of integrity threats by analyzing Concept Shift in probing samples. ConceptLens demonstrates strong detection performance for vanilla data poisoning attacks and uncovers vulnerabilities to bias injection, such as the generation of covert advertisements through malicious concept shifts. It identifies privacy risks in unaltered but high-risk samples, filters them before training, and provides insights into model weaknesses arising from incomplete or imbalanced training data. Additionally, at the model level, it attributes concepts that the target model is overly dependent on, identifies misleading concepts, and explains how disrupting key concepts negatively impacts the model. Furthermore, it uncovers sociological biases in generative content, revealing disparities across sociological contexts. Strikingly, ConceptLens reveals how safe training and inference data can be unintentionally and easily exploited, potentially undermining safety alignment. Our study informs actionable insights to breed trust in AI systems, thereby speeding adoption and driving greater innovation.","authors":["Jiamin Chang","Haoyang Li","Hammond Pearce","Ruoxi Sun","Bo Li","Minhui Xue"],"url":"https://arxiv.org/abs/2504.21042"}
{"created":"2025-05-01","title":"CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain","abstract":"Large language models (LLMs) excel at generating code from natural language instructions, yet they often lack an understanding of security vulnerabilities. This limitation makes it difficult for LLMs to avoid security risks in generated code, particularly in high-security programming tasks such as smart contract development for blockchain. Researchers have attempted to enhance the vulnerability awareness of these models by training them to differentiate between vulnerable and fixed code snippets. However, this approach relies heavily on manually labeled vulnerability data, which is only available for popular languages like Python and C++. For low-resource languages like Solidity, used in smart contracts, large-scale annotated datasets are scarce and difficult to obtain. To address this challenge, we introduce CodeBC, a code generation model specifically designed for generating secure smart contracts in blockchain. CodeBC employs a three-stage fine-tuning approach based on CodeLlama, distinguishing itself from previous methods by not relying on pairwise vulnerability location annotations. Instead, it leverages vulnerability and security tags to teach the model the differences between vulnerable and secure code. During the inference phase, the model leverages security tags to generate secure and robust code. Experimental results demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU, and compilation pass rates, while significantly reducing vulnerability rates. These findings validate the effectiveness and cost-efficiency of our three-stage fine-tuning strategy, making CodeBC a promising solution for generating secure smart contract code.","authors":["Lingxiang wang","Hainan Zhang","Qinnan Zhang","Ziwei Wang","Hongwei Zheng","Jin Dong","Zhiming Zheng"],"url":"https://arxiv.org/abs/2504.21043"}
{"created":"2025-05-01","title":"AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection","abstract":"Recent advancement in large-scale Artificial Intelligence (AI) models offering multimodal services have become foundational in AI systems, making them prime targets for model theft. Existing methods select Out-of-Distribution (OoD) data as backdoor watermarks and retrain the original model for copyright protection. However, existing methods are susceptible to malicious detection and forgery by adversaries, resulting in watermark evasion. In this work, we propose Model-\\underline{ag}nostic Black-box Backdoor W\\underline{ate}rmarking Framework (AGATE) to address stealthiness and robustness challenges in multimodal model copyright protection. Specifically, we propose an adversarial trigger generation method to generate stealthy adversarial triggers from ordinary dataset, providing visual fidelity while inducing semantic shifts. To alleviate the issue of anomaly detection among model outputs, we propose a post-transform module to correct the model output by narrowing the distance between adversarial trigger image embedding and text embedding. Subsequently, a two-phase watermark verification is proposed to judge whether the current model infringes by comparing the two results with and without the transform module. Consequently, we consistently outperform state-of-the-art methods across five datasets in the downstream tasks of multimodal image-text retrieval and image classification. Additionally, we validated the robustness of AGATE under two adversarial attack scenarios.","authors":["Jianbo Gao","Keke Gai","Jing Yu","Liehuang Zhu","Qi Wu"],"url":"https://arxiv.org/abs/2504.21044"}
{"created":"2025-05-01","title":"Leveraging LLM to Strengthen ML-Based Cross-Site Scripting Detection","abstract":"According to the Open Web Application Security Project (OWASP), Cross-Site Scripting (XSS) is a critical security vulnerability. Despite decades of research, XSS remains among the top 10 security vulnerabilities. Researchers have proposed various techniques to protect systems from XSS attacks, with machine learning (ML) being one of the most widely used methods. An ML model is trained on a dataset to identify potential XSS threats, making its effectiveness highly dependent on the size and diversity of the training data. A variation of XSS is obfuscated XSS, where attackers apply obfuscation techniques to alter the code's structure, making it challenging for security systems to detect its malicious intent. Our study's random forest model was trained on traditional (non-obfuscated) XSS data achieved 99.8% accuracy. However, when tested against obfuscated XSS samples, accuracy dropped to 81.9%, underscoring the importance of training ML models with obfuscated data to improve their effectiveness in detecting XSS attacks. A significant challenge is to generate highly complex obfuscated code despite the availability of several public tools. These tools can only produce obfuscation up to certain levels of complexity.","authors":["Dennis Miczek","Divyesh Gabbireddy","Suman Saha"],"url":"https://arxiv.org/abs/2504.21045"}
{"created":"2025-05-01","title":"Model Connectomes: A Generational Approach to Data-Efficient Language Models","abstract":"Biological neural networks are shaped both by evolution across generations and by individual learning within an organism's lifetime, whereas standard artificial neural networks undergo a single, large training procedure without inherited constraints. In this preliminary work, we propose a framework that incorporates this crucial generational dimension - an \"outer loop\" of evolution that shapes the \"inner loop\" of learning - so that artificial networks better mirror the effects of evolution and individual learning in biological organisms. Focusing on language, we train a model that inherits a \"model connectome\" from the outer evolution loop before exposing it to a developmental-scale corpus of 100M tokens. Compared with two closely matched control models, we show that the connectome model performs better or on par on natural language processing tasks as well as alignment to human behavior and brain data. These findings suggest that a model connectome serves as an efficient prior for learning in low-data regimes - narrowing the gap between single-generation artificial models and biologically evolved neural networks.","authors":["Klemen Kotar","Greta Tuckute"],"url":"https://arxiv.org/abs/2504.21047"}
{"created":"2025-05-01","title":"Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey","abstract":"Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing pivotal roles to Industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, and a structured taxonomy. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL's potential to advance resource allocation solutions.","authors":["Mohamad A. Hady","Siyi Hu","Mahardhika Pratama","Jimmy Cao","Ryszard Kowalczyk"],"url":"https://arxiv.org/abs/2504.21048"}
{"created":"2025-05-01","title":"Phishing URL Detection using Bi-LSTM","abstract":"Phishing attacks threaten online users, often leading to data breaches, financial losses, and identity theft. Traditional phishing detection systems struggle with high false positive rates and are usually limited by the types of attacks they can identify. This paper proposes a deep learning-based approach using a Bidirectional Long Short-Term Memory (Bi-LSTM) network to classify URLs into four categories: benign, phishing, defacement, and malware. The model leverages sequential URL data and captures contextual information, improving the accuracy of phishing detection. Experimental results on a dataset comprising over 650,000 URLs demonstrate the model's effectiveness, achieving 97% accuracy and significant improvements over traditional techniques.","authors":["Sneha Baskota"],"url":"https://arxiv.org/abs/2504.21049"}
{"created":"2025-05-01","title":"Multimodal Large Language Models for Medicine: A Comprehensive Survey","abstract":"MLLMs have recently become a focal point in the field of artificial intelligence research. Building on the strong capabilities of LLMs, MLLMs are adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs have gained substantial attention from different domains. Researchers have begun to explore the potential of MLLMs in the medical and healthcare domain. In this paper, we first introduce the background and fundamental concepts related to LLMs and MLLMs, while emphasizing the working principles of MLLMs. Subsequently, we summarize three main directions of application within healthcare: medical reporting, medical diagnosis, and medical treatment. Our findings are based on a comprehensive review of 330 recent papers in this area. We illustrate the remarkable capabilities of MLLMs in these domains by providing specific examples. For data, we present six mainstream modes of data along with their corresponding evaluation benchmarks. At the end of the survey, we discuss the challenges faced by MLLMs in the medical and healthcare domain and propose feasible methods to mitigate or overcome these issues.","authors":["Jiarui Ye","Hao Tang"],"url":"https://arxiv.org/abs/2504.21051"}
{"created":"2025-05-01","title":"SFIBA: Spatial-based Full-target Invisible Backdoor Attacks","abstract":"Multi-target backdoor attacks pose significant security threats to deep neural networks, as they can preset multiple target classes through a single backdoor injection. This allows attackers to control the model to misclassify poisoned samples with triggers into any desired target class during inference, exhibiting superior attack performance compared with conventional backdoor attacks. However, existing multi-target backdoor attacks fail to guarantee trigger specificity and stealthiness in black-box settings, resulting in two main issues. First, they are unable to simultaneously target all classes when only training data can be manipulated, limiting their effectiveness in realistic attack scenarios. Second, the triggers often lack visual imperceptibility, making poisoned samples easy to detect. To address these problems, we propose a Spatial-based Full-target Invisible Backdoor Attack, called SFIBA. It restricts triggers for different classes to specific local spatial regions and morphologies in the pixel space to ensure specificity, while employing a frequency-domain-based trigger injection method to guarantee stealthiness. Specifically, for injection of each trigger, we first apply fast fourier transform to obtain the amplitude spectrum of clean samples in local spatial regions. Then, we employ discrete wavelet transform to extract the features from the amplitude spectrum and use singular value decomposition to integrate the trigger. Subsequently, we selectively filter parts of the trigger in pixel space to implement trigger morphology constraints and adjust injection coefficients based on visual effects. We conduct experiments on multiple datasets and models. The results demonstrate that SFIBA can achieve excellent attack performance and stealthiness, while preserving the model's performance on benign samples, and can also bypass existing backdoor defenses.","authors":["Yangxu Yin","Honglong Chen","Yudong Gao","Peng Sun","Zhishuai Li","Weifeng Liu"],"url":"https://arxiv.org/abs/2504.21052"}
{"created":"2025-05-01","title":"NeuRel-Attack: Neuron Relearning for Safety Disalignment in Large Language Models","abstract":"Safety alignment in large language models (LLMs) is achieved through fine-tuning mechanisms that regulate neuron activations to suppress harmful content. In this work, we propose a novel approach to induce disalignment by identifying and modifying the neurons responsible for safety constraints. Our method consists of three key steps: Neuron Activation Analysis, where we examine activation patterns in response to harmful and harmless prompts to detect neurons that are critical for distinguishing between harmful and harmless inputs; Similarity-Based Neuron Identification, which systematically locates the neurons responsible for safe alignment; and Neuron Relearning for Safety Removal, where we fine-tune these selected neurons to restore the model's ability to generate previously restricted responses. Experimental results demonstrate that our method effectively removes safety constraints with minimal fine-tuning, highlighting a critical vulnerability in current alignment techniques. Our findings underscore the need for robust defenses against adversarial fine-tuning attacks on LLMs.","authors":["Yi Zhou","Wenpeng Xing","Dezhang Kong","Changting Lin","Meng Han"],"url":"https://arxiv.org/abs/2504.21053"}
{"created":"2025-05-01","title":"FFCBA: Feature-based Full-target Clean-label Backdoor Attacks","abstract":"Backdoor attacks pose a significant threat to deep neural networks, as backdoored models would misclassify poisoned samples with specific triggers into target classes while maintaining normal performance on clean samples. Among these, multi-target backdoor attacks can simultaneously target multiple classes. However, existing multi-target backdoor attacks all follow the dirty-label paradigm, where poisoned samples are mislabeled, and most of them require an extremely high poisoning rate. This makes them easily detectable by manual inspection. In contrast, clean-label attacks are more stealthy, as they avoid modifying the labels of poisoned samples. However, they generally struggle to achieve stable and satisfactory attack performance and often fail to scale effectively to multi-target attacks. To address this issue, we propose the Feature-based Full-target Clean-label Backdoor Attacks (FFCBA) which consists of two paradigms: Feature-Spanning Backdoor Attacks (FSBA) and Feature-Migrating Backdoor Attacks (FMBA). FSBA leverages class-conditional autoencoders to generate noise triggers that align perturbed in-class samples with the original category's features, ensuring the effectiveness, intra-class consistency, inter-class specificity and natural-feature correlation of triggers. While FSBA supports swift and efficient attacks, its cross-model attack capability is relatively weak. FMBA employs a two-stage class-conditional autoencoder training process that alternates between using out-of-class samples and in-class samples. This allows FMBA to generate triggers with strong target-class features, making it highly effective for cross-model attacks. We conduct experiments on multiple datasets and models, the results show that FFCBA achieves outstanding attack performance and maintains desirable robustness against the state-of-the-art backdoor defenses.","authors":["Yangxu Yin","Honglong Chen","Yudong Gao","Peng Sun","Liantao Wu","Zhe Li","Weifeng Liu"],"url":"https://arxiv.org/abs/2504.21054"}
{"created":"2025-05-01","title":"Modeling and Performance Analysis for Semantic Communications Based on Empirical Results","abstract":"Due to the black-box characteristics of deep learning based semantic encoders and decoders, finding a tractable method for the performance analysis of semantic communications is a challenging problem. In this paper, we propose an Alpha-Beta-Gamma (ABG) formula to model the relationship between the end-to-end measurement and SNR, which can be applied for both image reconstruction tasks and inference tasks. Specifically, for image reconstruction tasks, the proposed ABG formula can well fit the commonly used DL networks, such as SCUNet, and Vision Transformer, for semantic encoding with the multi scale-structural similarity index measure (MS-SSIM) measurement. Furthermore, we find that the upper bound of the MS-SSIM depends on the number of quantized output bits of semantic encoders, and we also propose a closed-form expression to fit the relationship between the MS-SSIM and quantized output bits. To the best of our knowledge, this is the first theoretical expression between end-to-end performance metrics and SNR for semantic communications. Based on the proposed ABG formula, we investigate an adaptive power control scheme for semantic communications over random fading channels, which can effectively guarantee quality of service (QoS) for semantic communications, and then design the optimal power allocation scheme to maximize the energy efficiency of the semantic communication system. Furthermore, by exploiting the bisection algorithm, we develop the power allocation scheme to maximize the minimum QoS of multiple users for OFDMA downlink semantic communication Extensive simulations verify the effectiveness and superiority of the proposed ABG formula and power allocation schemes.","authors":["Shuai Ma","Bin Shen","Chuanhui Zhang","Youlong Wu","Hang Li","Shiyin Li","Guangming Shi","Naofal Al-Dhahir"],"url":"https://arxiv.org/abs/2504.21055"}
{"created":"2025-05-01","title":"Computing change of level and isogenies between abelian varieties","abstract":"Let $m,n,d > 1$ be integers such that $n=md$. In this paper, we present an efficient change of level algorithm that takes as input $(B, \\mathscr{M}, \\Theta_{\\mathscr{M}})$ a marked abelian variety of level $m$ over the base field $k$ of odd characteristic and returns $(B, \\mathscr{M}^d, \\Theta_{\\mathscr{M}^d})$ a marked abelian variety of level $n$ at the expense of $O(m^g d^{2g})$ operations in $k$. A similar algorithm allows to compute $d$-isogenies: from $(B, \\mathscr{M}, \\Theta_{\\mathscr{M}})$ a marked abelian variety of level $m$, $K\\subset B[d]$ isotropic for the Weil pairing isomorphic to $(\\mathbb{Z}/d\\mathbb{Z})^g$ defined over $k$, the isogeny algorithm returns $(A, \\mathscr{L}, \\Theta_{\\mathscr{L}})$ of level $m$ such that $A=B/K$ with $O(m^g d^g)$ operations in $k$. Our algorithms extend previous known results in the case that $d \\wedge m=1$ and $d$ odd. In this paper, we lift theses restrictions. We use the same general approach as in the literature in conjunction with the notion of symmetric compatible that we introduce, study and link to previous results of Mumford. For practical computation, most of the time $m$ is $2$ or $4$ so that our algorithms allows in particular to compute $2^e$-isogenies which are important for the theory of theta functions but also for computational applications such as isogeny based cryptography.","authors":["Antoine Dequay (IRMAR)","David Lubicz (DGA.MI","IRMAR)"],"url":"https://arxiv.org/abs/2504.21058"}
{"created":"2025-05-01","title":"Seeking Specifications: The Case for Neuro-Symbolic Specification Synthesis","abstract":"This work is concerned with the generation of formal specifications from code, using Large Language Models (LLMs) in combination with symbolic methods. Concretely, in our study, the programming language is C, the specification language is ACSL, and the LLM is Deepseek-R1. In this context, we address two research directions, namely the specification of intent vs. implementation on the one hand, and the combination of symbolic analyses with LLMs on the other hand. For the first, we investigate how the absence or presence of bugs in the code impacts the generated specifications, as well as whether and how a user can direct the LLM to specify intent or implementation, respectively. For the second, we investigate the impact of results from symbolic analyses on the specifications generated by the LLM. The LLM prompts are augmented with outputs from two formal methods tools in the Frama-C ecosystem, Pathcrawler and EVA. We demonstrate how the addition of symbolic analysis to the workflow impacts the quality of annotations.","authors":["George Granberry","Wolfgang Ahrendt","Moa Johansson"],"url":"https://arxiv.org/abs/2504.21061"}
{"created":"2025-05-01","title":"A Hamiltonian Higher-Order Elasticity Framework for Dynamic Diagnostics(2HOED)","abstract":"Machine learning detects patterns, block chain guarantees trust and immutability, and modern causal inference identifies directional linkages, yet none alone exposes the full energetic anatomy of complex systems; the Hamiltonian Higher Order Elasticity Dynamics(2HOED) framework bridges these gaps. Grounded in classical mechanics but extended to Economics order elasticity terms, 2HOED represents economic, social, and physical systems as energy-based Hamiltonians whose position, velocity, acceleration, and jerk of elasticity jointly determine systemic power, Inertia, policy sensitivity, and marginal responses. Because the formalism is scaling free and coordinate agnostic, it transfers seamlessly from financial markets to climate science, from supply chain logistics to epidemiology, thus any discipline in which adaptation and shocks coexist. By embedding standard econometric variables inside a Hamiltonian, 2HOED enriches conventional economic analysis with rigorous diagnostics of resilience, tipping points, and feedback loops, revealing failure modes invisible to linear models. Wavelet spectra, phase space attractors, and topological persistence diagrams derived from 2HOED expose multistage policy leverage that machine learning detects only empirically and block chain secures only after the fact. For economists, physicians and other scientists, the method opens a new causal energetic channel linking biological or mechanical elasticity to macro level outcomes. Portable, interpretable, and computationally light, 2HOED turns data streams into dynamical energy maps, empowering decision makers to anticipate crises, design adaptive policies, and engineer robust systems delivering the predictive punch of AI with the explanatory clarity of physics.","authors":["Ngueuleweu Tiwang Gildas"],"url":"https://arxiv.org/abs/2504.21062"}
{"created":"2025-05-01","title":"Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization","abstract":"Federated domain generalization (FedDG) aims to learn a globally generalizable model from decentralized clients with heterogeneous data while preserving privacy. Recent studies have introduced prompt learning to adapt vision-language models (VLMs) in FedDG by learning a single global prompt. However, such a one-prompt-fits-all learning paradigm typically leads to performance degradation on personalized samples. Although the mixture of experts (MoE) offers a promising solution for specialization, existing MoE-based methods suffer from coarse image-level expert assignment and high communication costs from parameterized routers. To address these limitations, we propose TRIP, a Token-level prompt mixture with parameter-free routing framework for FedDG, which treats multiple prompts as distinct experts. Unlike existing image-level routing designs, TRIP assigns different tokens within an image to specific experts. To ensure communication efficiency, TRIP incorporates a parameter-free routing mechanism based on token clustering and optimal transport. The instance-specific prompt is then synthesized by aggregating experts, weighted by the number of tokens assigned to each. Additionally, TRIP develops an unbiased learning strategy for prompt experts, leveraging the VLM's zero-shot generalization capability. Extensive experiments across four benchmarks demonstrate that TRIP achieves optimal generalization results, with communication of only 1K parameters per round. Our code is available at https://github.com/GongShuai8210/TRIP.","authors":["Shuai Gong","Chaoran Cui","Xiaolin Dong","Xiushan Nie","Lei Zhu","Xiaojun Chang"],"url":"https://arxiv.org/abs/2504.21063"}
{"created":"2025-05-01","title":"Frequency Feature Fusion Graph Network For Depression Diagnosis Via fNIRS","abstract":"Data-driven approaches for depression diagnosis have emerged as a significant research focus in neuromedicine, driven by the development of relevant datasets. Recently, graph neural network (GNN)-based models have gained widespread adoption due to their ability to capture brain channel functional connectivity from both spatial and temporal perspectives. However, their effectiveness is hindered by the absence of a robust temporal biomarker. In this paper, we introduce a novel and effective biomarker for depression diagnosis by leveraging the discrete Fourier transform (DFT) and propose a customized graph network architecture based on Temporal Graph Convolutional Network (TGCN). Our model was trained on a dataset comprising 1,086 subjects, which is over 10 times larger than previous datasets in the field of depression diagnosis. Furthermore, to align with medical requirements, we performed propensity score matching (PSM) to create a refined subset, referred to as the PSM dataset. Experimental results demonstrate that incorporating our newly designed biomarker enhances the representation of temporal characteristics in brain channels, leading to improved F1 scores in both the real-world dataset and the PSM dataset. This advancement has the potential to contribute to the development of more effective depression diagnostic tools. In addition, we used SHapley Additive exPlaination (SHAP) to validate the interpretability of our model, ensuring its practical applicability in medical settings.","authors":["Chengkai Yang","Xingping Dong","Xiaofen Zong"],"url":"https://arxiv.org/abs/2504.21064"}
{"created":"2025-05-01","title":"A 3D pocket-aware and affinity-guided diffusion model for lead optimization","abstract":"Molecular optimization, aimed at improving binding affinity or other molecular properties, is a crucial task in drug discovery that often relies on the expertise of medicinal chemists. Recently, deep learning-based 3D generative models showed promise in enhancing the efficiency of molecular optimization. However, these models often struggle to adequately consider binding affinities with protein targets during lead optimization. Herein, we propose a 3D pocket-aware and affinity-guided diffusion model, named Diffleop, to optimize molecules with enhanced binding affinity. The model explicitly incorporates the knowledge of protein-ligand binding affinity to guide the denoising sampling for molecule generation with high affinity. The comprehensive evaluations indicated that Diffleop outperforms baseline models across multiple metrics, especially in terms of binding affinity.","authors":["Anjie Qiao","Junjie Xie","Weifeng Huang","Hao Zhang","Jiahua Rao","Shuangjia Zheng","Yuedong Yang","Zhen Wang","Guo-Bo Li","Jinping Lei"],"url":"https://arxiv.org/abs/2504.21065"}
{"created":"2025-05-01","title":"A Brief Review for Compression and Transfer Learning Techniques in DeepFake Detection","abstract":"Training and deploying deepfake detection models on edge devices offers the advantage of maintaining data privacy and confidentiality by processing it close to its source. However, this approach is constrained by the limited computational and memory resources available at the edge. To address this challenge, we explore compression techniques to reduce computational demands and inference time, alongside transfer learning methods to minimize training overhead. Using the Synthbuster, RAISE, and ForenSynths datasets, we evaluate the effectiveness of pruning, knowledge distillation (KD), quantization, fine-tuning, and adapter-based techniques. Our experimental results demonstrate that both compression and transfer learning can be effectively achieved, even with a high compression level of 90%, remaining at the same performance level when the training and validation data originate from the same DeepFake model. However, when the testing dataset is generated by DeepFake models not present in the training set, a domain generalization issue becomes evident.","authors":["Andreas Karathanasis","John Violos","Ioannis Kompatsiaris","Symeon Papadopoulos"],"url":"https://arxiv.org/abs/2504.21066"}
{"created":"2025-05-01","title":"GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction","abstract":"This research tackles the challenge of real-time active view selection and uncertainty quantification on visual quality for active 3D reconstruction. Visual quality is a critical aspect of 3D reconstruction. Recent advancements such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have notably enhanced the image rendering quality of reconstruction models. Nonetheless, the efficient and effective acquisition of input images for reconstruction-specifically, the selection of the most informative viewpoint-remains an open challenge, which is crucial for active reconstruction. Existing studies have primarily focused on evaluating geometric completeness and exploring unobserved or unknown regions, without direct evaluation of the visual uncertainty within the reconstruction model. To address this gap, this paper introduces a probabilistic model that quantifies visual uncertainty for each Gaussian. Leveraging Shannon Mutual Information, we formulate a criterion, Gaussian Splatting Shannon Mutual Information (GauSS-MI), for real-time assessment of visual mutual information from novel viewpoints, facilitating the selection of next best view. GauSS-MI is implemented within an active reconstruction system integrated with a view and motion planner. Extensive experiments across various simulated and real-world scenes showcase the superior visual quality and reconstruction efficiency performance of the proposed system.","authors":["Yuhan Xie","Yixi Cai","Yinqiang Zhang","Lei Yang","Jia Pan"],"url":"https://arxiv.org/abs/2504.21067"}
{"created":"2025-05-01","title":"R^2VFL: A Robust Random Vector Functional Link Network with Huber-Weighted Framework","abstract":"The random vector functional link (RVFL) neural network has shown significant potential in overcoming the constraints of traditional artificial neural networks, such as excessive computation time and suboptimal solutions. However, RVFL faces challenges when dealing with noise and outliers, as it assumes all data samples contribute equally. To address this issue, we propose a novel robust framework, R2VFL, RVFL with Huber weighting function and class probability, which enhances the model's robustness and adaptability by effectively mitigating the impact of noise and outliers in the training data. The Huber weighting function reduces the influence of outliers, while the class probability mechanism assigns less weight to noisy data points, resulting in a more resilient model. We explore two distinct approaches for calculating class centers within the R2VFL framework: the simple average of all data points in each class and the median of each feature, the later providing a robust alternative by minimizing the effect of extreme values. These approaches give rise to two novel variants of the model-R2VFL-A and R2VFL-M. We extensively evaluate the proposed models on 47 UCI datasets, encompassing both binary and multiclass datasets, and conduct rigorous statistical testing, which confirms the superiority of the proposed models. Notably, the models also demonstrate exceptional performance in classifying EEG signals, highlighting their practical applicability in real-world biomedical domain.","authors":["Anuradha Kumari","Mushir Akhtar","P. N. Suganthan","M. Tanveer"],"url":"https://arxiv.org/abs/2504.21069"}
{"created":"2025-05-01","title":"Cost-Effective Edge Data Distribution with End-To-End Delay Guarantees in Edge Computing","abstract":"Cloud Computing is the delivery of computing resources which includes servers, storage, databases, networking, software, analytics, and intelligence over the internet to offer faster innovation, flexible resources, and economies of scale. Since these computing resources are hosted centrally, the data transactions from the cloud to its users can get very expensive. Edge Computing plays a crucial role in minimizing these costs by shifting the data from the cloud to the edge servers located closer to the user's geographical location, thereby providing low-latency app-functionalities to the users of that area. However, the data transaction from the cloud to each of these edge servers can still be expensive both in time and cost. Thus, we need an application data distribution strategy that minimizes these penalities. In this research, we attempt to formulate this Edge Data Distribution as a constrained optimization problem with end-to-end delay guarantees. We then provide an optimal approach to solve this problem using the Integer Programming (IP) technique. Since the IP approach has an exponential time complexity, we also then provide a modified implementation of the EDD-NSTE algorithm, for estimating solutions to large-scale EDD problems. These algorithms are then evaluated on standard real-world datasets named EUA and SLNDC and the result demonstrates that EDD-NSTE significantly outperformed, with a performance margin of 80.35\\% over the other representative approaches in comparison.","authors":["Ravi Shankar","Aryabartta Sahu"],"url":"https://arxiv.org/abs/2504.21070"}
{"created":"2025-05-01","title":"Automated Parking Trajectory Generation Using Deep Reinforcement Learning","abstract":"Autonomous parking is a key technology in modern autonomous driving systems, requiring high precision, strong adaptability, and efficiency in complex environments. This paper proposes a Deep Reinforcement Learning (DRL) framework based on the Soft Actor-Critic (SAC) algorithm to optimize autonomous parking tasks. SAC, an off-policy method with entropy regularization, is particularly well-suited for continuous action spaces, enabling fine-grained vehicle control. We model the parking task as a Markov Decision Process (MDP) and train an agent to maximize cumulative rewards while balancing exploration and exploitation through entropy maximization. The proposed system integrates multiple sensor inputs into a high-dimensional state space and leverages SAC's dual critic networks and policy network to achieve stable learning. Simulation results show that the SAC-based approach delivers high parking success rates, reduced maneuver times, and robust handling of dynamic obstacles, outperforming traditional rule-based methods and other DRL algorithms. This study demonstrates SAC's potential in autonomous parking and lays the foundation for real-world applications.","authors":["Zheyu Zhang","Yutong Luo","Yongzhou Chen","Haopeng Zhao","Zhichao Ma","Hao Liu"],"url":"https://arxiv.org/abs/2504.21071"}
{"created":"2025-05-01","title":"Erased but Not Forgotten: How Backdoors Compromise Concept Erasure","abstract":"The expansion of large-scale text-to-image diffusion models has raised growing concerns about their potential to generate undesirable or harmful content, ranging from fabricated depictions of public figures to sexually explicit images. To mitigate these risks, prior work has devised machine unlearning techniques that attempt to erase unwanted concepts through fine-tuning. However, in this paper, we introduce a new threat model, Toxic Erasure (ToxE), and demonstrate how recent unlearning algorithms, including those explicitly designed for robustness, can be circumvented through targeted backdoor attacks. The threat is realized by establishing a link between a trigger and the undesired content. Subsequent unlearning attempts fail to erase this link, allowing adversaries to produce harmful content. We instantiate ToxE via two established backdoor attacks: one targeting the text encoder and another manipulating the cross-attention layers. Further, we introduce Deep Intervention Score-based Attack (DISA), a novel, deeper backdoor attack that optimizes the entire U-Net using a score-based objective, improving the attack's persistence across different erasure methods. We evaluate five recent concept erasure methods against our threat model. For celebrity identity erasure, our deep attack circumvents erasure with up to 82% success, averaging 57% across all erasure methods. For explicit content erasure, ToxE attacks can elicit up to 9 times more exposed body parts, with DISA yielding an average increase by a factor of 2.9. These results highlight a critical security gap in current unlearning strategies.","authors":["Jonas Henry Grebe","Tobias Braun","Marcus Rohrbach","Anna Rohrbach"],"url":"https://arxiv.org/abs/2504.21072"}
{"created":"2025-05-01","title":"On the Potential of Large Language Models to Solve Semantics-Aware Process Mining Tasks","abstract":"Large language models (LLMs) have shown to be valuable tools for tackling process mining tasks. Existing studies report on their capability to support various data-driven process analyses and even, to some extent, that they are able to reason about how processes work. This reasoning ability suggests that there is potential for LLMs to tackle semantics-aware process mining tasks, which are tasks that rely on an understanding of the meaning of activities and their relationships. Examples of these include process discovery, where the meaning of activities can indicate their dependency, whereas in anomaly detection the meaning can be used to recognize process behavior that is abnormal. In this paper, we systematically explore the capabilities of LLMs for such tasks. Unlike prior work, which largely evaluates LLMs in their default state, we investigate their utility through both in-context learning and supervised fine-tuning. Concretely, we define five process mining tasks requiring semantic understanding and provide extensive benchmarking datasets for evaluation. Our experiments reveal that while LLMs struggle with challenging process mining tasks when used out of the box or with minimal in-context examples, they achieve strong performance when fine-tuned for these tasks across a broad range of process types and industries.","authors":["Adrian Rebmann","Fabian David Schmidt","Goran Glava\\v{s}","Han van der Aa"],"url":"https://arxiv.org/abs/2504.21074"}
{"created":"2025-05-01","title":"Nominal anti-unification","abstract":"We study nominal anti-unification, which is concerned with computing least general generalizations for given terms-in-context. In general, the problem does not have a least general solution, but if the set of atoms permitted in generalizations is finite, then there exists a least general generalization which is unique modulo variable renaming and $\\alpha$-equivalence. We present an algorithm that computes it. The algorithm relies on a subalgorithm that constructively decides equivariance between two terms-in-context. We prove soundness and completeness properties of both algorithms and analyze their complexity. Nominal anti-unification can be applied to problems were generalization of first-order terms is needed (inductive learning, clone detection, etc.), but bindings are involved.","authors":["Alexander Baumgartner","Temur Kutsia","Jordi Levy","Mateu Villaret"],"url":"https://arxiv.org/abs/2504.21097"}
{"created":"2025-05-01","title":"A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning","abstract":"Foundation models have revolutionized artificial intelligence by providing robust, versatile architectures pre-trained on large-scale datasets. However, adapting these massive models to specific downstream tasks requires fine-tuning, which can be prohibitively expensive in computational resources. Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by selectively updating only a small subset of parameters. Meanwhile, Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it ideal for privacy-sensitive applications. This survey provides a comprehensive review of the integration of PEFT techniques within federated learning environments. We systematically categorize existing approaches into three main groups: Additive PEFT (which introduces new trainable parameters), Selective PEFT (which fine-tunes only subsets of existing parameters), and Reparameterized PEFT (which transforms model architectures to enable efficient updates). For each category, we analyze how these methods address the unique challenges of federated settings, including data heterogeneity, communication efficiency, computational constraints, and privacy concerns. We further organize the literature based on application domains, covering both natural language processing and computer vision tasks. Finally, we discuss promising research directions, including scaling to larger foundation models, theoretical analysis of federated PEFT methods, and sustainable approaches for resource-constrained environments.","authors":["Jieming Bian","Yuanzhe Peng","Lei Wang","Yin Huang","Jie Xu"],"url":"https://arxiv.org/abs/2504.21099"}
{"created":"2025-05-01","title":"A smack of all neighbouring languages: How multilingual is scholarly communication?","abstract":"Language is a major source of systemic inequities in science, particularly among scholars whose first language is not English. Studies have examined scientists' linguistic practices in specific contexts; few, however, have provided a global analysis of multilingualism in science. Using two major bibliometric databases (OpenAlex and Dimensions), we provide a large-scale analysis of linguistic diversity in science, considering both the language of publications (N=87,577,942) and of cited references (N=1,480,570,087). For the 1990-2023 period, we find that only Indonesian, Portuguese and Spanish have expanded at a faster pace than English. Country-level analyses show that this trend is due to the growing strength of the Latin American and Indonesian academic circuits. Our results also confirm the own-language preference phenomenon (particularly for languages other than English), the strong connection between multilingualism and bibliodiversity, and that social sciences and humanities are the least English-dominated fields. Our findings suggest that policies recognizing the value of both national-language and English-language publications have had a concrete impact on the distribution of languages in the global field of scholarly communication.","authors":["Carolina Pradier","Luc\\'ia C\\'espedes","Vincent Larivi\\`ere"],"url":"https://arxiv.org/abs/2504.21100"}
{"created":"2025-05-01","title":"Relaxed Choices in Bottom-Up Asynchronous Multiparty Session Types","abstract":"Asynchronous multiparty session types provide a formal model for expressing the behaviour of communicating processes and verifying that they correctly implement desired protocols. In the ``bottom-up'' approach to session typing, local session types are specified directly, and the properties of their composition (e.g. deadlock freedom and liveness) are checked and transferred to well-typed processes. This method allows expressing and verifying a broad range of protocols, but still has a key limitation: it only supports protocols where every send/receive operation is directed towards strictly one recipient/sender at a time. This makes the technique too restrictive for modelling some classes of protocols, e.g. those used in the field of federated learning.","authors":["Ivan Proki\\'c","Simona Proki\\'c","Silvia Ghilezan","Alceste Scalas","Nobuko Yoshida"],"url":"https://arxiv.org/abs/2504.21108"}
{"created":"2025-05-01","title":"How to Coordinate UAVs and UGVs for Efficient Mission Planning? Optimizing Energy-Constrained Cooperative Routing with a DRL Framework","abstract":"Efficient mission planning for cooperative systems involving Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) requires addressing energy constraints, scalability, and coordination challenges between agents. UAVs excel in rapidly covering large areas but are constrained by limited battery life, while UGVs, with their extended operational range and capability to serve as mobile recharging stations, are hindered by slower speeds. This heterogeneity makes coordination between UAVs and UGVs critical for achieving optimal mission outcomes. In this work, we propose a scalable deep reinforcement learning (DRL) framework to address the energy-constrained cooperative routing problem for multi-agent UAV-UGV teams, aiming to visit a set of task points in minimal time with UAVs relying on UGVs for recharging during the mission. The framework incorporates sortie-wise agent switching to efficiently manage multiple agents, by allocating task points and coordinating actions. Using an encoder-decoder transformer architecture, it optimizes routes and recharging rendezvous for the UAV-UGV team in the task scenario. Extensive computational experiments demonstrate the framework's superior performance over heuristic methods and a DRL baseline, delivering significant improvements in solution quality and runtime efficiency across diverse scenarios. Generalization studies validate its robustness, while dynamic scenario highlights its adaptability to real-time changes with a case study. This work advances UAV-UGV cooperative routing by providing a scalable, efficient, and robust solution for multi-agent mission planning.","authors":["Md Safwan Mondal","Subramanian Ramasamy","Luca Russo","James D. Humann","James M. Dotterweich","Pranav Bhounsule"],"url":"https://arxiv.org/abs/2504.21111"}
{"created":"2025-05-01","title":"NavEX: A Multi-Agent Coverage in Non-Convex and Uneven Environments via Exemplar-Clustering","abstract":"This paper addresses multi-agent deployment in non-convex and uneven environments. To overcome the limitations of traditional approaches, we introduce Navigable Exemplar-Based Dispatch Coverage (NavEX), a novel dispatch coverage framework that combines exemplar-clustering with obstacle-aware and traversability-aware shortest distances, offering a deployment framework based on submodular optimization. NavEX provides a unified approach to solve two critical coverage tasks: (a) fair-access deployment, aiming to provide equitable service by minimizing agent-target distances, and (b) hotspot deployment, prioritizing high-density target regions. A key feature of NavEX is the use of exemplar-clustering for the coverage utility measure, which provides the flexibility to employ non-Euclidean distance metrics that do not necessarily conform to the triangle inequality. This allows NavEX to incorporate visibility graphs for shortest-path computation in environments with planar obstacles, and traversability-aware RRT* for complex, rugged terrains. By leveraging submodular optimization, the NavEX framework enables efficient, near-optimal solutions with provable performance guarantees for multi-agent deployment in realistic and complex settings, as demonstrated by our simulations.","authors":["Donipolo Ghimire","Carlos Nieto-Granda","Solmaz S. Kia"],"url":"https://arxiv.org/abs/2504.21113"}
{"created":"2025-05-01","title":"Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts","abstract":"Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.","authors":["Hanhua Hong","Chenghao Xiao","Yang Wang","Yiqi Liu","Wenge Rong","Chenghua Lin"],"url":"https://arxiv.org/abs/2504.21117"}
{"created":"2025-05-01","title":"Numerical Performance of the Implicitly Restarted Arnoldi Method in OFP8, Bfloat16, Posit, and Takum Arithmetics","abstract":"The computation of select eigenvalues and eigenvectors of large, sparse matrices is fundamental to a wide range of applications. Accordingly, evaluating the numerical performance of emerging alternatives to the IEEE 754 floating-point standard -- such as OFP8 (E4M3 and E5M2), bfloat16, and the tapered-precision posit and takum formats -- is of significant interest. Among the most widely used methods for this task is the implicitly restarted Arnoldi method, as implemented in ARPACK.","authors":["Laslo Hunhold","James Quinlan","Stefan Wesner"],"url":"https://arxiv.org/abs/2504.21130"}
{"created":"2025-05-01","title":"A Formalism for Optimal Search with Dynamic Heuristics","abstract":"While most heuristics studied in heuristic search depend only on the state, some accumulate information during search and thus also depend on the search history. Various existing approaches use such dynamic heuristics in $\\mathrm{A}^*$-like algorithms and appeal to classic results for $\\mathrm{A}^*$ to show optimality. However, doing so ignores the complexities of searching with a mutable heuristic. In this paper we formalize the idea of dynamic heuristics and use them in a generic algorithm framework. We study a particular instantiation that models $\\mathrm{A}^*$ with dynamic heuristics and show general optimality results. Finally we show how existing approaches from classical planning can be viewed as special cases of this instantiation, making it possible to directly apply our optimality results.","authors":["Remo Christen","Florian Pommerening","Clemens B\\\"uchner","Malte Helmert"],"url":"https://arxiv.org/abs/2504.21131"}
{"created":"2025-05-01","title":"LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge","abstract":"Large Language Models (LLMs), such as ChatGPT, have demonstrated the capability to generate human like, natural responses across a range of tasks, including task oriented dialogue and question answering. However, their application in real world, critical scenarios is often hindered by a tendency to produce inaccurate information and a limited ability to leverage external knowledge sources. This paper introduces the LLM ENHANCER system, designed to integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to enhance data accuracy. The LLMs employed within this system are open source. The data acquisition process for the LLM ENHANCER system operates in parallel, utilizing custom agent tools to manage the flow of information. Vector embeddings are used to identify the most pertinent information, which is subsequently supplied to the LLM for user interaction. The LLM ENHANCER system mitigates hallucinations in chat based LLMs while preserving response naturalness and accuracy.","authors":["Naheed Rayhan","Md. Ashrafuzzaman"],"url":"https://arxiv.org/abs/2504.21132"}
{"created":"2025-05-01","title":"Legilimens: Performant Video Analytics on the System-on-Chip Edge","abstract":"Continually retraining models has emerged as a primary technique to enable high-accuracy video analytics on edge devices. Yet, existing systems employ such adaptation by relying on the spare compute resources that traditional (memory-constrained) edge servers afford. In contrast, mobile edge devices such as drones and dashcams offer a fundamentally different resource profile: weak(er) compute with abundant unified memory pools. We present Legilimens, a continuous learning system for the mobile edge's System-on-Chip GPUs. Our driving insight is that visually distinct scenes that require retraining exhibit substantial overlap in model embeddings; if captured into a base model on device memory, specializing to each new scene can become lightweight, requiring very few samples. To practically realize this approach, Legilimens presents new, compute-efficient techniques to (1) select high-utility data samples for retraining specialized models, (2) update the base model without complete retraining, and (3) time-share compute resources between retraining and live inference for maximal accuracy. Across diverse workloads, Legilimens lowers retraining costs by 2.8-10x compared to existing systems, resulting in 18-45% higher accuracies.","authors":["Murali Ramanujam","Yinwei Dai","Kyle Jamieson","Ravi Netravali"],"url":"https://arxiv.org/abs/2504.21136"}
{"created":"2025-05-01","title":"STAMP-2.5D: Structural and Thermal Aware Methodology for Placement in 2.5D Integration","abstract":"Chiplet-based architectures and advanced packaging has emerged as transformative approaches in semiconductor design. While conventional physical design for 2.5D heterogeneous systems typically prioritizes wirelength reduction through tight chiplet packing, this strategy creates thermal bottlenecks and intensifies coefficient of thermal expansion (CTE) mismatches, compromising long-term reliability. Addressing these challenges requires holistic consideration of thermal performance, mechanical stress, and interconnect efficiency. We introduce STAMP-2.5D, the first automated floorplanning methodology that simultaneously optimizes these critical factors. Our approach employs finite element analysis to simulate temperature distributions and stress profiles across chiplet configurations while minimizing interconnect wirelength. Experimental results demonstrate that our thermal structural aware automated floorplanning approach reduces overall stress by 11% while maintaining excellent thermal performance with a negligible 0.5% temperature increase and simultaneously reducing total wirelength by 11% compared to temperature-only optimization. Additionally, we conduct an exploratory study on the effects of temperature gradients on structural integrity, providing crucial insights for reliability-conscious chiplet design. STAMP-2.5D establishes a robust platform for navigating critical trade-offs in advanced semiconductor packaging.","authors":["Varun Darshana Parekh","Zachary Wyatt Hazenstab","Srivatsa Rangachar Srinivasa","Krishnendu Chakrabarty","Kai Ni","Vijaykrishnan Narayanan"],"url":"https://arxiv.org/abs/2504.21140"}
{"created":"2025-05-01","title":"SMOGAN: Synthetic Minority Oversampling with GAN Refinement for Imbalanced Regression","abstract":"Imbalanced regression refers to prediction tasks where the target variable is skewed. This skewness hinders machine learning models, especially neural networks, which concentrate on dense regions and therefore perform poorly on underrepresented (minority) samples. Despite the importance of this problem, only a few methods have been proposed for imbalanced regression. Many of the available solutions for imbalanced regression adapt techniques from the class imbalance domain, such as linear interpolation and the addition of Gaussian noise, to create synthetic data in sparse regions. However, in many cases, the underlying distribution of the data is complex and non-linear. Consequently, these approaches generate synthetic samples that do not accurately represent the true feature-target relationship. To overcome these limitations, we propose SMOGAN, a two-step oversampling framework for imbalanced regression. In Stage 1, an existing oversampler generates initial synthetic samples in sparse target regions. In Stage 2, we introduce DistGAN, a distribution-aware GAN that serves as SMOGAN's filtering layer and refines these samples via adversarial loss augmented with a Maximum Mean Discrepancy objective, aligning them with the true joint feature-target distribution. Extensive experiments on 23 imbalanced datasets show that SMOGAN consistently outperforms the default oversampling method without the DistGAN filtering layer.","authors":["Shayan Alahyari","Mike Domaratzki"],"url":"https://arxiv.org/abs/2504.21152"}
{"created":"2025-05-01","title":"Climate Science and Control Engineering: Insights, Parallels, and Connections","abstract":"Climate science is the multidisciplinary field that studies the Earth's climate and its evolution. At the very core of climate science are indispensable climate models that predict future climate scenarios, inform policy decisions, and dictate how a country's economy should change in light of the changing climate. Climate models capture a wide range of interacting dynamic processes via extremely complex ordinary and partial differential equations. To model these large-scale complex processes, climate science leverages supercomputers, advanced simulations, and statistical methods to predict future climate. An area of engineering that is rarely studied in climate science is control engineering. Given that climate systems are inherently dynamic, it is intuitive to analyze them within the framework of dynamic system science. This perspective that has been underexplored in the literature. In this manuscript, we provide a tutorial that: (i) introduces the control engineering community to climate dynamics and modeling, including spatiotemporal scales and challenges in climate modeling; (ii) offers a fresh perspective on climate models from a control systems viewpoint; and (iii) explores the relevance and applicability of various advanced graph and network control-based approaches in building a physics-informed framework for learning, control and estimation in climate systems. We also present simple and then more complex climate models, depicting fundamental ideas and processes that are instrumental in building climate change projections. This tutorial also builds parallels and observes connections between various contemporary problems at the forefront of climate science and their control theoretic counterparts. We specifically observe that an abundance of climate science problems can be linguistically reworded and mathematically framed as control theoretic ones.","authors":["Salma M. Elsherif","Ahmad F. Taha"],"url":"https://arxiv.org/abs/2504.21153"}
{"created":"2025-05-01","title":"Emotion Recognition in Contemporary Dance Performances Using Laban Movement Analysis","abstract":"This paper presents a novel framework for emotion recognition in contemporary dance by improving existing Laban Movement Analysis (LMA) feature descriptors and introducing robust, novel descriptors that capture both quantitative and qualitative aspects of the movement. Our approach extracts expressive characteristics from 3D keypoints data of professional dancers performing contemporary dance under various emotional states, and trains multiple classifiers, including Random Forests and Support Vector Machines. Additionally, we provide in-depth explanation of features and their impact on model predictions using explainable machine learning methods. Overall, our study improves emotion recognition in contemporary dance and offers promising applications in performance analysis, dance training, and human--computer interaction, with a highest accuracy of 96.85\\%.","authors":["Muhammad Turab","Philippe Colantoni","Damien Muselet","Alain Tremeau"],"url":"https://arxiv.org/abs/2504.21154"}
{"created":"2025-05-01","title":"Composite Safety Potential Field for Highway Driving Risk Assessment","abstract":"In the era of rapid advancements in vehicle safety technologies, driving risk assessment has become a focal point of attention. Technologies such as collision warning systems, advanced driver assistance systems (ADAS), and autonomous driving require driving risks to be evaluated proactively and in real time. To be effective, driving risk assessment metrics must not only accurately identify potential collisions but also exhibit human-like reasoning to enable safe and seamless interactions between vehicles. Existing safety potential field models assess driving risks by considering both objective and subjective safety factors. However, their practical applicability in real-world risk assessment tasks is limited. These models are often challenging to calibrate due to the arbitrary nature of their structures, and calibration can be inefficient because of the scarcity of accident statistics. Additionally, they struggle to generalize across both longitudinal and lateral risks. To address these challenges, we propose a composite safety potential field framework, namely C-SPF, involving a subjective field to capture drivers' risk perception about spatial proximity and an objective field to quantify the imminent collision probability, to comprehensively evaluate driving risks. The C-SPF is calibrated using abundant two-dimensional spacing data from trajectory datasets, enabling it to effectively capture drivers' proximity risk perception and provide a more realistic explanation of driving behaviors. Analysis of a naturalistic driving dataset demonstrates that the C-SPF can capture both longitudinal and lateral risks that trigger drivers' safety maneuvers. Further case studies highlight the C-SPF's ability to explain lateral driver behaviors, such as abandoning lane changes or adjusting lateral position relative to adjacent vehicles, which are capabilities that existing models fail to achieve.","authors":["Dachuan Zuo","Zilin Bian","Fan Zuo","Kaan Ozbay"],"url":"https://arxiv.org/abs/2504.21158"}
{"created":"2025-05-01","title":"Task and Joint Space Dual-Arm Compliant Control","abstract":"Robots that interact with humans or perform delicate manipulation tasks must exhibit compliance. However, most commercial manipulators are rigid and suffer from significant friction, limiting end-effector tracking accuracy in torque-controlled modes. To address this, we present a real-time, open-source impedance controller that smoothly interpolates between joint-space and task-space compliance. This hybrid approach ensures safe interaction and precise task execution, such as sub-centimetre pin insertions. We deploy our controller on Frank, a dual-arm platform with two Kinova Gen3 arms, and compensate for modelled friction dynamics using a model-free observer. The system is real-time capable and integrates with standard ROS tools like MoveIt!. It also supports high-frequency trajectory streaming, enabling closed-loop execution of trajectories generated by learning-based methods, optimal control, or teleoperation. Our results demonstrate robust tracking and compliant behaviour even under high-friction conditions. The complete system is available open-source at https://github.com/applied-ai-lab/compliant_controllers.","authors":["Alexander L. Mitchell","Tobit Flatscher","Ingmar Posner"],"url":"https://arxiv.org/abs/2504.21159"}
{"created":"2025-05-01","title":"An $r$-adaptive finite element method using neural networks for parametric self-adjoint elliptic problem","abstract":"This work proposes an $r$-adaptive finite element method (FEM) using neural networks (NNs). The method employs the Ritz energy functional as the loss function, currently limiting its applicability to symmetric and coercive problems, such as those arising from self-adjoint elliptic problems. The objective of the NN optimization is to determine the mesh node locations. For simplicity in two-dimensional problems, these locations are assumed to form a tensor product structure. The method is designed to solve parametric partial differential equations (PDEs). For each PDE parameter instance, the optimal $r$-adapted mesh generated by the NN is then solved with a standard FEM. The construction of FEM matrices and load vectors is implemented such that their derivatives with respect to mesh node locations, required for NN training, can be efficiently computed using automatic differentiation. However, the linear equation solver does not need to be differentiable, enabling the use of efficient, readily available `out-of-the-box' solvers. Consequently, the proposed approach retains the robustness and reliability guarantees of the FEM for each parameter instance, while the NN optimization adaptively adjusts the mesh node locations. The method's performance is demonstrated on parametric Poisson problems using one- and two-dimensional tensor product meshes.","authors":["Danilo Aballay","Federico Fuentes","Vicente Iligaray","\\'Angel J. Omella","David Pardo","Manuel A. S\\'anchez","Ignacio Tapia","Carlos Uriarte"],"url":"https://arxiv.org/abs/2504.21160"}
{"created":"2025-05-01","title":"Automated Test Generation from Program Documentation Encoded in Code Comments","abstract":"Documenting the functionality of software units with code comments, e.g., Javadoc comments, is a common programmer best-practice in software engineering. This paper introduces a novel test generation technique that exploits the code-comment documentation constructively. We originally address those behaviors as test objectives, which we pursue in search-based fashion. We deliver test cases with names and oracles properly contextualized on the target behaviors. Our experiments against a benchmark of 118 Java classes indicate that the proposed approach successfully tests many software behaviors that may remain untested with coverage-driven test generation approaches, and distinctively detects unknown failures.","authors":["Giovanni Denaro","Luca Guglielmo"],"url":"https://arxiv.org/abs/2504.21161"}
{"created":"2025-05-01","title":"Learning Large-Scale Competitive Team Behaviors with Mean-Field Interactions","abstract":"State-of-the-art multi-agent reinforcement learning (MARL) algorithms such as MADDPG and MAAC fail to scale in situations where the number of agents becomes large. Mean-field theory has shown encouraging results in modeling macroscopic agent behavior for teams with a large number of agents through a continuum approximation of the agent population and its interaction with the environment. In this work, we extend proximal policy optimization (PPO) to the mean-field domain by introducing the Mean-Field Multi-Agent Proximal Policy Optimization (MF-MAPPO), a novel algorithm that utilizes the effectiveness of the finite-population mean-field approximation in the context of zero-sum competitive multi-agent games between two teams. The proposed algorithm can be easily scaled to hundreds and thousands of agents in each team as shown through numerical experiments. In particular, the algorithm is applied to realistic applications such as large-scale offense-defense battlefield scenarios.","authors":["Bhavini Jeloka","Yue Guan","Panagiotis Tsiotras"],"url":"https://arxiv.org/abs/2504.21164"}
{"created":"2025-05-01","title":"Detecting Manipulated Contents Using Knowledge-Grounded Inference","abstract":"The detection of manipulated content, a prevalent form of fake news, has been widely studied in recent years. While existing solutions have been proven effective in fact-checking and analyzing fake news based on historical events, the reliance on either intrinsic knowledge obtained during training or manually curated context hinders them from tackling zero-day manipulated content, which can only be recognized with real-time contextual information. In this work, we propose Manicod, a tool designed for detecting zero-day manipulated content. Manicod first sources contextual information about the input claim from mainstream search engines, and subsequently vectorizes the context for the large language model (LLM) through retrieval-augmented generation (RAG). The LLM-based inference can produce a \"truthful\" or \"manipulated\" decision and offer a textual explanation for the decision. To validate the effectiveness of Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake news derived from 2500 recent real-world news headlines. Manicod achieves an overall F1 score of 0.856 on this dataset and outperforms existing methods by up to 1.9x in F1 score on their benchmarks on fact-checking and claim verification.","authors":["Mark Huasong Meng","Ruizhe Wang","Meng Xu","Chuan Yan","Guangdong Bai"],"url":"https://arxiv.org/abs/2504.21165"}
{"created":"2025-05-01","title":"Dance Style Recognition Using Laban Movement Analysis","abstract":"The growing interest in automated movement analysis has presented new challenges in recognition of complex human activities including dance. This study focuses on dance style recognition using features extracted using Laban Movement Analysis. Previous studies for dance style recognition often focus on cross-frame movement analysis, which limits the ability to capture temporal context and dynamic transitions between movements. This gap highlights the need for a method that can add temporal context to LMA features. For this, we introduce a novel pipeline which combines 3D pose estimation, 3D human mesh reconstruction, and floor aware body modeling to effectively extract LMA features. To address the temporal limitation, we propose a sliding window approach that captures movement evolution across time in features. These features are then used to train various machine learning methods for classification, and their explainability explainable AI methods to evaluate the contribution of each feature to classification performance. Our proposed method achieves a highest classification accuracy of 99.18\\% which shows that the addition of temporal context significantly improves dance style recognition performance.","authors":["Muhammad Turab","Philippe Colantoni","Damien Muselet","Alain Tremeau"],"url":"https://arxiv.org/abs/2504.21166"}
{"created":"2025-05-01","title":"A Summation-Based Algorithm For Integer Factorization","abstract":"Numerous methods have been considered to create a fast integer factorization algorithm. Despite its apparent simplicity, the difficulty to find such an algorithm plays a crucial role in modern cryptography, notably, in the security of RSA encryption. Some approaches to factoring integers quickly include the Trial Division method, Pollard's Rho and p-1 methods, and various Sieve algorithms.","authors":["Justin Friedlander"],"url":"https://arxiv.org/abs/2504.21168"}
{"created":"2025-05-01","title":"Design, analysis, and experimental validation of a stepped plate parametric array loudspeaker","abstract":"This study investigates the design and analysis of a stepped plate parametric array loudspeaker (SPPAL) as an alternative to conventional array-based parametric loudspeakers. The SPPAL utilizes a single Langevin-type ultrasonic transducer coupled with a flexural stepped plate to generate narrow-beam audible sound via nonlinear acoustic interaction. To evaluate and optimize the performance of the SPPAL, an integrated modeling framework is developed, consisting of an approximate analytical 3D model for transducer dynamics, an equivalence ratio formulation to relate stepped plate and rigid piston behavior, and a spherical wave expansion method for nonlinear sound field simulation. The dual-resonance behavior of the transducer is optimized through multi-objective analysis to enhance low-frequency audio performance. Experimental validation includes frequency response and modal analysis of the transducer, as well as sound field measurements. The analytical methods are further verified through comparison with experimental data. Furthermore, combination resonance--an unintended structural excitation resulting from intermodulation--is identified as an inherent phenomenon in SPPAL operation. The findings offer practical guidance for the development of efficient, compact, and manufacturable parametric array loudspeakers employing plate-based flexural vibration.","authors":["Woongji Kim","Beomseok Oh","Chayeong Kim","Wonkyu Moon"],"url":"https://arxiv.org/abs/2504.21171"}
{"created":"2025-05-01","title":"Efficient LLMs with AMP: Attention Heads and MLP Pruning","abstract":"Deep learning drives a new wave in computing systems and triggers the automation of increasingly complex problems. In particular, Large Language Models (LLMs) have significantly advanced cognitive tasks, often matching or even surpassing human-level performance. However, their extensive parameters result in high computational costs and slow inference, posing challenges for deployment in resource-limited settings. Among the strategies to overcome the aforementioned challenges, pruning emerges as a successful mechanism since it reduces model size while maintaining predictive ability. In this paper, we introduce AMP: Attention Heads and MLP Pruning, a novel structured pruning method that efficiently compresses LLMs by removing less critical structures within Multi-Head Attention (MHA) and Multilayer Perceptron (MLP). By projecting the input data onto weights, AMP assesses structural importance and overcomes the limitations of existing techniques, which often fall short in flexibility or efficiency. In particular, AMP surpasses the current state-of-the-art on commonsense reasoning tasks by up to 1.49 percentage points, achieving a 30% pruning ratio with minimal impact on zero-shot task performance. Moreover, AMP also improves inference speeds, making it well-suited for deployment in resource-constrained environments. We confirm the flexibility of AMP on different families of LLMs, including LLaMA and Phi.","authors":["Leandro Giusti Mugnaini","Bruno Lopes Yamamoto","Lucas Lauton de Alcantara","Victor Zacarias","Edson Bollis","Lucas Pellicer","Anna Helena Reali Costa","Artur Jordao"],"url":"https://arxiv.org/abs/2504.21174"}
{"created":"2025-05-01","title":"Tight Bounds for Heavy-Hitters and Moment Estimation in the Sliding Window Model","abstract":"We consider the heavy-hitters and $F_p$ moment estimation problems in the sliding window model. For $F_p$ moment estimation with $1<p\\leq 2$, we show that it is possible to give a $(1\\pm \\epsilon)$ multiplicative approximation to the $F_p$ moment with $2/3$ probability on any given window of size $n$ using $\\tilde{O}(\\frac{1}{\\epsilon^p}\\log^2 n + \\frac{1}{\\epsilon^2}\\log n)$ bits of space. We complement this result with a lower bound showing that our algorithm gives tight bounds up to factors of $\\log\\log n$ and $\\log\\frac{1}{\\epsilon}.$ As a consequence of our $F_2$ moment estimation algorithm, we show that the heavy-hitters problem can be solved on an arbitrary window using $O(\\frac{1}{\\epsilon^2}\\log^2 n)$ space which is tight.","authors":["Shiyuan Feng","William Swartworth","David P. Woodruff"],"url":"https://arxiv.org/abs/2504.21175"}
{"created":"2025-05-01","title":"Differentially Private Secure Multiplication with Erasures and Adversaries","abstract":"We consider a private distributed multiplication problem involving N computation nodes and T colluding nodes. Shamir's secret sharing algorithm provides perfect information-theoretic privacy, while requiring an honest majority, i.e., N \\ge 2T + 1. Recent work has investigated approximate computation and characterized privacy-accuracy trade-offs for the honest minority setting N \\le 2T for real-valued data, quantifying privacy leakage via the differential privacy (DP) framework and accuracy via the mean squared error. However, it does not incorporate the error correction capabilities of Shamir's secret-sharing algorithm. This paper develops a new polynomial-based coding scheme for secure multiplication with an honest minority, and characterizes its achievable privacy-utility tradeoff, showing that the tradeoff can approach the converse bound as closely as desired. Unlike previous schemes, the proposed scheme inherits the capability of the Reed-Solomon (RS) code to tolerate erasures and adversaries. We utilize a modified Berlekamp-Welch algorithm over the real number field to detect adversarial nodes.","authors":["Haoyang Hu","Viveck R. Cadambe"],"url":"https://arxiv.org/abs/2504.21178"}
{"created":"2025-05-01","title":"Green Satellite Networks Using Segment Routing and Software-Defined Networking","abstract":"This paper presents a comprehensive evaluation of network performance in software defined networking (SDN)-based low Earth orbit (LEO) satellite networks, focusing on the Telesat Lightspeed constellation. We propose a green traffic engineering (TE) approach leveraging segment routing IPv6 (SRv6) to enhance energy efficiency. Through simulations, we analyze the impact of SRv6, multi-protocol label switching (MPLS), IPv4, and IPv6 with open shortest path first (OSPF) on key network performance metrics, including peak and average CPU usage, memory consumption, packet delivery rate (PDR), and packet overhead under varying traffic loads. Results show that the proposed green TE approach using SRv6 achieves notable energy efficiency, maintaining lower CPU usage and high PDR compared to traditional protocols. While SRv6 and MPLS introduce slightly higher memory usage and overhead due to their advanced configurations, these trade-offs remain manageable. Our findings highlight SRv6 with green TE as a promising solution for optimizing energy efficiency in LEO satellite networks, contributing to the development of more sustainable and efficient satellite communications.","authors":["Jintao Liang","Pablo G. Madoery","Chung-Horng Lung","Halim Yanikomeroglu","Gunes Karabulut Kurt"],"url":"https://arxiv.org/abs/2504.21181"}
{"created":"2025-05-01","title":"Federated One-Shot Learning with Data Privacy and Objective-Hiding","abstract":"Privacy in federated learning is crucial, encompassing two key aspects: safeguarding the privacy of clients' data and maintaining the privacy of the federator's objective from the clients. While the first aspect has been extensively studied, the second has received much less attention.","authors":["Maximilian Egger","R\\\"udiger Urbanke","Rawad Bitar"],"url":"https://arxiv.org/abs/2504.21182"}
{"created":"2025-05-01","title":"AffectEval: A Modular and Customizable Framework for Affective Computing","abstract":"The field of affective computing focuses on recognizing, interpreting, and responding to human emotions, and has broad applications across education, child development, and human health and wellness. However, developing affective computing pipelines remains labor-intensive due to the lack of software frameworks that support multimodal, multi-domain emotion recognition applications. This often results in redundant effort when building pipelines for different applications. While recent frameworks attempt to address these challenges, they remain limited in reducing manual effort and ensuring cross-domain generalizability. We introduce AffectEval, a modular and customizable framework to facilitate the development of affective computing pipelines while reducing the manual effort and duplicate work involved in developing such pipelines. We validate AffectEval by replicating prior affective computing experiments, and we demonstrate that our framework reduces programming effort by up to 90%, as measured by the reduction in raw lines of code.","authors":["Emily Zhou","Khushboo Khatri","Yixue Zhao","Bhaskar Krishnamachari"],"url":"https://arxiv.org/abs/2504.21184"}
{"created":"2025-05-01","title":"AI-in-the-Loop Planning for Transportation Electrification: Case Studies from Austin, Texas","abstract":"This study explores the integration of AI in transportation electrification planning in Austin, TX, focusing on the use of Geospatial AI (GeoAI), Generative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site selection, localized GenAI models support meta-level estimations, and LLMs enable scenario simulations. These AI applications require human oversight. GeoAI outputs must be evaluated with land use data, GenAI models are not always accurate, and LLMs are prone to hallucinations. To ensure accountable planning, human planners must work alongside AI agents. Establishing a community feedback loop is essential to audit automated decisions. Planners should place Community Experience (CX) at the center of Urban Planning AI.","authors":["Seung Jun Choi"],"url":"https://arxiv.org/abs/2504.21185"}
{"created":"2025-05-01","title":"GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model","abstract":"Out-of-distribution (OOD) detection is critical for ensuring the safety and reliability of machine learning systems, particularly in dynamic and open-world environments. In the vision and text domains, zero-shot OOD detection - which requires no training on in-distribution (ID) data - has made significant progress through the use of large-scale pretrained models such as vision-language models (VLMs) and large language models (LLMs). However, zero-shot OOD detection in graph-structured data remains largely unexplored, primarily due to the challenges posed by complex relational structures and the absence of powerful, large-scale pretrained models for graphs. In this work, we take the first step toward enabling zero-shot graph OOD detection by leveraging a graph foundation model (GFM). We show that, when provided only with class label names, the GFM can perform OOD detection without any node-level supervision - outperforming existing supervised methods across multiple datasets. To address the more practical setting where OOD label names are unavailable, we introduce GLIP-OOD, a novel framework that employs LLMs to generate semantically informative pseudo-OOD labels from unlabeled data. These labels enable the GFM to capture nuanced semantic boundaries between ID and OOD classes and perform fine-grained OOD detection - without requiring any labeled nodes. Our approach is the first to enable node-level graph OOD detection in a fully zero-shot setting, and achieves state-of-the-art performance on four benchmark text-attributed graph datasets.","authors":["Haoyan Xu","Zhengtao Yao","Xuzhi Zhang","Ziyi Wang","Langzhou He","Yushun Dong","Philip S. Yu","Mengyuan Li","Yue Zhao"],"url":"https://arxiv.org/abs/2504.21186"}
{"created":"2025-05-01","title":"LIFT: LLM-Based Pragma Insertion for HLS via GNN Supervised Fine-Tuning","abstract":"FPGAs are increasingly adopted in datacenter environments for their reconfigurability and energy efficiency. High-Level Synthesis (HLS) tools have eased FPGA programming by raising the abstraction level from RTL to untimed C/C++, yet attaining high performance still demands expert knowledge and iterative manual insertion of optimization pragmas to modify the microarchitecture. To address this challenge, we propose LIFT, a large language model (LLM)-based coding assistant for HLS that automatically generates performance-critical pragmas given a C/C++ design. We fine-tune the LLM by tightly integrating and supervising the training process with a graph neural network (GNN), combining the sequential modeling capabilities of LLMs with the structural and semantic understanding of GNNs necessary for reasoning over code and its control/data dependencies. On average, LIFT produces designs that improve performance by 3.52x and 2.16x than prior state-of the art AutoDSE and HARP respectively, and 66x than GPT-4o.","authors":["Neha Prakriya","Zijian Ding","Yizhou Sun","Jason Cong"],"url":"https://arxiv.org/abs/2504.21187"}
{"created":"2025-05-01","title":"Artificial Intelligence for Personalized Prediction of Alzheimer's Disease Progression: A Survey of Methods, Data Challenges, and Future Directions","abstract":"Alzheimer's Disease (AD) is marked by significant inter-individual variability in its progression, complicating accurate prognosis and personalized care planning. This heterogeneity underscores the critical need for predictive models capable of forecasting patient-specific disease trajectories. Artificial Intelligence (AI) offers powerful tools to address this challenge by analyzing complex, multi-modal, and longitudinal patient data. This paper provides a comprehensive survey of AI methodologies applied to personalized AD progression prediction. We review key approaches including state-space models for capturing temporal dynamics, deep learning techniques like Recurrent Neural Networks for sequence modeling, Graph Neural Networks (GNNs) for leveraging network structures, and the emerging concept of AI-driven digital twins for individualized simulation. Recognizing that data limitations often impede progress, we examine common challenges such as high dimensionality, missing data, and dataset imbalance. We further discuss AI-driven mitigation strategies, with a specific focus on synthetic data generation using Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) to augment and balance datasets. The survey synthesizes the strengths and limitations of current approaches, emphasizing the trend towards multimodal integration and the persistent need for model interpretability and generalizability. Finally, we identify critical open challenges, including robust external validation, clinical integration, and ethical considerations, and outline promising future research directions such as hybrid models, causal inference, and federated learning. This review aims to consolidate current knowledge and guide future efforts in developing clinically relevant AI tools for personalized AD prognostication.","authors":["Gulsah Hancerliogullari Koksalmis","Bulent Soykan","Laura J. Brattain","Hsin-Hsiung Huang"],"url":"https://arxiv.org/abs/2504.21189"}
{"created":"2025-05-01","title":"TT-LoRA MoE: Unifying Parameter-Efficient Fine-Tuning and Sparse Mixture-of-Experts","abstract":"We propose Tensor-Trained Low-Rank Adaptation Mixture of Experts (TT-LoRA MoE), a novel computational framework integrating Parameter-Efficient Fine-Tuning (PEFT) with sparse MoE routing to address scalability challenges in large model deployments. Unlike traditional MoE approaches, which face substantial computational overhead as expert counts grow, TT-LoRA MoE decomposes training into two distinct, optimized stages. First, we independently train lightweight, tensorized low-rank adapters (TT-LoRA experts), each specialized for specific tasks. Subsequently, these expert adapters remain frozen, eliminating inter-task interference and catastrophic forgetting in multi-task setting. A sparse MoE router, trained separately, dynamically leverages base model representations to select exactly one specialized adapter per input at inference time, automating expert selection without explicit task specification. Comprehensive experiments confirm our architecture retains the memory efficiency of low-rank adapters, seamlessly scales to large expert pools, and achieves robust task-level optimization. This structured decoupling significantly enhances computational efficiency and flexibility: uses only 2% of LoRA, 0.3% of Adapters and 0.03% of AdapterFusion parameters and outperforms AdapterFusion by 4 value in multi-tasking, enabling practical and scalable multi-task inference deployments.","authors":["Pradip Kunwar","Minh N. Vu","Maanak Gupta","Mahmoud Abdelsalam","Manish Bhattarai"],"url":"https://arxiv.org/abs/2504.21190"}
{"created":"2025-05-01","title":"Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare","abstract":"This study aims to guide language model selection by investigating: 1) the necessity of finetuning versus zero-shot usage, 2) the benefits of domain-adjacent versus generic pretrained models, 3) the value of further domain-specific pretraining, and 4) the continued relevance of Small Language Models (SLMs) compared to Large Language Models (LLMs) for specific tasks. Using electronic pathology reports from the British Columbia Cancer Registry (BCCR), three classification scenarios with varying difficulty and data size are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning significantly improved SLM performance across all scenarios compared to their zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally performed better than the generic SLM after finetuning, especially on harder tasks. Further domain-specific pretraining yielded modest gains on easier tasks but significant improvements on the complex, data-scarce task. The results highlight the critical role of finetuning for SLMs in specialized domains, enabling them to surpass zero-shot LLM performance on targeted classification tasks. Pretraining on domain-adjacent or domain-specific data provides further advantages, particularly for complex problems or limited finetuning data. While LLMs offer strong zero-shot capabilities, their performance on these specific tasks did not match that of appropriately finetuned SLMs. In the era of LLMs, SLMs remain relevant and effective, offering a potentially superior performance-resource trade-off compared to LLMs.","authors":["Lovedeep Gondara","Jonathan Simkin","Graham Sayle","Shebnum Devji","Gregory Arbour","Raymond Ng"],"url":"https://arxiv.org/abs/2504.21191"}
{"created":"2025-05-01","title":"Geolocating Earth Imagery from ISS: Integrating Machine Learning with Astronaut Photography for Enhanced Geographic Mapping","abstract":"This paper presents a novel approach to geolocating images captured from the International Space Station (ISS) using advanced machine learning algorithms. Despite having precise ISS coordinates, the specific Earth locations depicted in astronaut-taken photographs often remain unidentified. Our research addresses this gap by employing three distinct image processing pipelines: a Neural Network based approach, a SIFT based method, and GPT-4 model. Each pipeline is tailored to process high-resolution ISS imagery, identifying both natural and man-made geographical features. Through extensive evaluation on a diverse dataset of over 140 ISS images, our methods demonstrate significant promise in automated geolocation with varied levels of success. The NN approach showed a high success rate in accurately matching geographical features, while the SIFT pipeline excelled in processing zoomed-in images. GPT-4 model provided enriched geographical descriptions alongside location predictions. This research contributes to the fields of remote sensing and Earth observation by enhancing the accuracy and efficiency of geolocating space-based imagery, thereby aiding environmental monitoring and global mapping efforts.","authors":["Vedika Srivastava","Hemant Kumar Singh","Jaisal Singh"],"url":"https://arxiv.org/abs/2504.21194"}
{"created":"2025-05-01","title":"Spectral Methods via FFTs in Emerging Machine Number Formats: OFP8, Bfloat16, Posit, and Takum Arithmetics","abstract":"The Fast Fourier Transform (FFT) is one of the most widely used algorithms in high performance computing, with critical applications in spectral analysis for both signal processing and the numerical solution of partial differential equations (PDEs). These data-intensive workloads are primarily constrained by the memory wall, motivating the exploration of emerging number formats -- such as OFP8 (E4M3 and E5M2), bfloat16, and the tapered-precision posit and takum formats -- as potential alternatives to conventional IEEE 754 floating-point representations.","authors":["Laslo Hunhold","John Gustafson"],"url":"https://arxiv.org/abs/2504.21197"}
{"created":"2025-05-01","title":"Graph Synthetic Out-of-Distribution Exposure with Large Language Models","abstract":"Out-of-distribution (OOD) detection in graphs is critical for ensuring model robustness in open-world and safety-sensitive applications. Existing approaches to graph OOD detection typically involve training an in-distribution (ID) classifier using only ID data, followed by the application of post-hoc OOD scoring techniques. Although OOD exposure - introducing auxiliary OOD samples during training - has proven to be an effective strategy for enhancing detection performance, current methods in the graph domain generally assume access to a set of real OOD nodes. This assumption, however, is often impractical due to the difficulty and cost of acquiring representative OOD samples. In this paper, we introduce GOE-LLM, a novel framework that leverages Large Language Models (LLMs) for OOD exposure in graph OOD detection without requiring real OOD nodes. GOE-LLM introduces two pipelines: (1) identifying pseudo-OOD nodes from the initially unlabeled graph using zero-shot LLM annotations, and (2) generating semantically informative synthetic OOD nodes via LLM-prompted text generation. These pseudo-OOD nodes are then used to regularize the training of the ID classifier for improved OOD awareness. We evaluate our approach across multiple benchmark datasets, showing that GOE-LLM significantly outperforms state-of-the-art graph OOD detection methods that do not use OOD exposure and achieves comparable performance to those relying on real OOD data.","authors":["Haoyan Xu","Zhengtao Yao","Ziyi Wang","Zhan Cheng","Xiyang Hu","Mengyuan Li","Yue Zhao"],"url":"https://arxiv.org/abs/2504.21198"}
{"created":"2025-05-01","title":"Automatic Legal Writing Evaluation of LLMs","abstract":"Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available.","authors":["Ramon Pires","Roseval Malaquias Junior","Rodrigo Nogueira"],"url":"https://arxiv.org/abs/2504.21202"}
{"created":"2025-05-01","title":"SecRepoBench: Benchmarking LLMs for Secure Code Generation in Real-World Repositories","abstract":"This paper introduces SecRepoBench, a benchmark to evaluate LLMs on secure code generation in real-world repositories. SecRepoBench has 318 code generation tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 19 state-of-the-art LLMs using our benchmark and find that the models struggle with generating correct and secure code. In addition, the performance of LLMs to generate self-contained programs as measured by prior benchmarks do not translate to comparative performance at generating secure and correct code at the repository level in SecRepoBench. We show that the state-of-the-art prompt engineering techniques become less effective when applied to the repository level secure code generation problem. We conduct extensive experiments, including an agentic technique to generate secure code, to demonstrate that our benchmark is currently the most difficult secure coding benchmark, compared to previous state-of-the-art benchmarks. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of LLMs to generate correct and secure code in real-world repositories.","authors":["Connor Dilgren","Purva Chiniya","Luke Griffith","Yu Ding","Yizheng Chen"],"url":"https://arxiv.org/abs/2504.21205"}
{"created":"2025-05-01","title":"FedHERO: A Federated Learning Approach for Node Classification Task on Heterophilic Graphs","abstract":"Federated Graph Learning (FGL) empowers clients to collaboratively train Graph neural networks (GNNs) in a distributed manner while preserving data privacy. However, FGL methods usually require that the graph data owned by all clients is homophilic to ensure similar neighbor distribution patterns of nodes. Such an assumption ensures that the learned knowledge is consistent across the local models from all clients. Therefore, these local models can be properly aggregated as a global model without undermining the overall performance. Nevertheless, when the neighbor distribution patterns of nodes vary across different clients (e.g., when clients hold graphs with different levels of heterophily), their local models may gain different and even conflict knowledge from their node-level predictive tasks. Consequently, aggregating these local models usually leads to catastrophic performance deterioration on the global model. To address this challenge, we propose FedHERO, an FGL framework designed to harness and share insights from heterophilic graphs effectively. At the heart of FedHERO is a dual-channel GNN equipped with a structure learner, engineered to discern the structural knowledge encoded in the local graphs. With this specialized component, FedHERO enables the local model for each client to identify and learn patterns that are universally applicable across graphs with different patterns of node neighbor distributions. FedHERO not only enhances the performance of individual client models by leveraging both local and shared structural insights but also sets a new precedent in this field to effectively handle graph data with various node neighbor distribution patterns. We conduct extensive experiments to validate the superior performance of FedHERO against existing alternatives.","authors":["Zihan Chen","Xingbo Fu","Yushun Dong","Jundong Li","Cong Shen"],"url":"https://arxiv.org/abs/2504.21206"}
{"created":"2025-05-01","title":"A Cost-Effective LLM-based Approach to Identify Wildlife Trafficking in Online Marketplaces","abstract":"Wildlife trafficking remains a critical global issue, significantly impacting biodiversity, ecological stability, and public health. Despite efforts to combat this illicit trade, the rise of e-commerce platforms has made it easier to sell wildlife products, putting new pressure on wild populations of endangered and threatened species. The use of these platforms also opens a new opportunity: as criminals sell wildlife products online, they leave digital traces of their activity that can provide insights into trafficking activities as well as how they can be disrupted. The challenge lies in finding these traces. Online marketplaces publish ads for a plethora of products, and identifying ads for wildlife-related products is like finding a needle in a haystack. Learning classifiers can automate ad identification, but creating them requires costly, time-consuming data labeling that hinders support for diverse ads and research questions. This paper addresses a critical challenge in the data science pipeline for wildlife trafficking analytics: generating quality labeled data for classifiers that select relevant data. While large language models (LLMs) can directly label advertisements, doing so at scale is prohibitively expensive. We propose a cost-effective strategy that leverages LLMs to generate pseudo labels for a small sample of the data and uses these labels to create specialized classification models. Our novel method automatically gathers diverse and representative samples to be labeled while minimizing the labeling costs. Our experimental evaluation shows that our classifiers achieve up to 95% F1 score, outperforming LLMs at a lower cost. We present real use cases that demonstrate the effectiveness of our approach in enabling analyses of different aspects of wildlife trafficking.","authors":["Juliana Barbosa","Ulhas Gondhali","Gohar Petrossian","Kinshuk Sharma","Sunandan Chakraborty","Jennifer Jacquet","Juliana Freire"],"url":"https://arxiv.org/abs/2504.21211"}
{"created":"2025-05-01","title":"Pretraining Large Brain Language Model for Active BCI: Silent Speech","abstract":"This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\\% accuracy on semantic-level classification and 39.6\\% in word-level classification, outperforming baseline methods by 5.4\\% and 7.3\\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.","authors":["Jinzhao Zhou","Zehong Cao","Yiqun Duan","Connor Barkley","Daniel Leong","Xiaowei Jiang","Quoc-Toan Nguyen","Ziyi Zhao","Thomas Do","Yu-Cheng Chang","Sheng-Fu Liang","Chin-teng Lin"],"url":"https://arxiv.org/abs/2504.21214"}
{"created":"2025-05-01","title":"A Koopman Operator-based NMPC Framework for Mobile Robot Navigation under Uncertainty","abstract":"Mobile robot navigation can be challenged by system uncertainty. For example, ground friction may vary abruptly causing slipping, and noisy sensor data can lead to inaccurate feedback control. Traditional model-based methods may be limited when considering such variations, making them fragile to varying types of uncertainty. One way to address this is by leveraging learned prediction models by means of the Koopman operator into nonlinear model predictive control (NMPC). This paper describes the formulation of, and provides the solution to, an NMPC problem using a lifted bilinear model that can accurately predict affine input systems with stochastic perturbations. System constraints are defined in the Koopman space, while the optimization problem is solved in the state space to reduce computational complexity. Training data to estimate the Koopman operator for the system are given via randomized control inputs. The output of the developed method enables closed-loop navigation control over environments populated with obstacles. The effectiveness of the proposed method has been tested through numerical simulations using a wheeled robot with additive stochastic velocity perturbations, Gazebo simulations with a realistic digital twin robot, and physical hardware experiments without knowledge of the true dynamics.","authors":["Xiaobin Zhang","Mohamed Karim Bouafoura","Lu Shi","Konstantinos Karydis"],"url":"https://arxiv.org/abs/2504.21215"}
{"created":"2025-05-01","title":"PhysicsFC: Learning User-Controlled Skills for a Physics-Based Football Player Controller","abstract":"We propose PhysicsFC, a method for controlling physically simulated football player characters to perform a variety of football skills--such as dribbling, trapping, moving, and kicking--based on user input, while seamlessly transitioning between these skills. Our skill-specific policies, which generate latent variables for each football skill, are trained using an existing physics-based motion embedding model that serves as a foundation for reproducing football motions. Key features include a tailored reward design for the Dribble policy, a two-phase reward structure combined with projectile dynamics-based initialization for the Trap policy, and a Data-Embedded Goal-Conditioned Latent Guidance (DEGCL) method for the Move policy. Using the trained skill policies, the proposed football player finite state machine (PhysicsFC FSM) allows users to interactively control the character. To ensure smooth and agile transitions between skill policies, as defined in the FSM, we introduce the Skill Transition-Based Initialization (STI), which is applied during the training of each skill policy. We develop several interactive scenarios to showcase PhysicsFC's effectiveness, including competitive trapping and dribbling, give-and-go plays, and 11v11 football games, where multiple PhysicsFC agents produce natural and controllable physics-based football player behaviors. Quantitative evaluations further validate the performance of individual skill policies and the transitions between them, using the presented metrics and experimental designs.","authors":["Minsu Kim","Eunho Jung","Yoonsang Lee"],"url":"https://arxiv.org/abs/2504.21216"}
{"created":"2025-05-01","title":"Theoretical Foundations for Semantic Cognition in Artificial Intelligence","abstract":"This monograph presents a modular cognitive architecture for artificial intelligence grounded in the formal modeling of belief as structured semantic state. Belief states are defined as dynamic ensembles of linguistic expressions embedded within a navigable manifold, where operators enable assimilation, abstraction, nullification, memory, and introspection. Drawing from philosophy, cognitive science, and neuroscience, we develop a layered framework that enables self-regulating epistemic agents capable of reflective, goal-directed thought. At the core of this framework is the epistemic vacuum: a class of semantically inert cognitive states that serves as the conceptual origin of belief space. From this foundation, the Null Tower arises as a generative structure recursively built through internal representational capacities. The theoretical constructs are designed to be implementable in both symbolic and neural systems, including large language models, hybrid agents, and adaptive memory architectures. This work offers a foundational substrate for constructing agents that reason, remember, and regulate their beliefs in structured, interpretable ways.","authors":["Sebastian Dumbrava"],"url":"https://arxiv.org/abs/2504.21218"}
{"created":"2025-05-01","title":"MemeBLIP2: A novel lightweight multimodal system to detect harmful memes","abstract":"Memes often merge visuals with brief text to share humor or opinions, yet some memes contain harmful messages such as hate speech. In this paper, we introduces MemeBLIP2, a light weight multimodal system that detects harmful memes by combining image and text features effectively. We build on previous studies by adding modules that align image and text representations into a shared space and fuse them for better classification. Using BLIP-2 as the core vision-language model, our system is evaluated on the PrideMM datasets. The results show that MemeBLIP2 can capture subtle cues in both modalities, even in cases with ironic or culturally specific content, thereby improving the detection of harmful material.","authors":["Jiaqi Liu","Ran Tong","Aowei Shen","Shuzheng Li","Changlin Yang","Lisha Xu"],"url":"https://arxiv.org/abs/2504.21226"}
{"created":"2025-05-01","title":"CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks","abstract":"Large Language Models (LLMs) are identified as being susceptible to indirect prompt injection attack, where the model undesirably deviates from user-provided instructions by executing tasks injected in the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune that defends against this attack by identifying and pruning task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to treat the text spans of input prompt context as only pure data, instead of any indicator of instruction following. These neurons are identified via feature attribution with a loss function induced from an upperbound of the Direct Preference Optimization (DPO) objective. We show that such a loss function enables effective feature attribution with only a few samples. We further improve on the quality of feature attribution, by exploiting an observed triggering effect in instruction following. Our approach does not impose any formatting on the original prompt or introduce extra test-time LLM calls. Experiments show that CachePrune significantly reduces attack success rates without compromising the response quality. Note: This paper aims to defend against indirect prompt injection attacks, with the goal of developing more secure and robust AI systems.","authors":["Rui Wang","Junda Wu","Yu Xia","Tong Yu","Ruiyi Zhang","Ryan Rossi","Lina Yao","Julian McAuley"],"url":"https://arxiv.org/abs/2504.21228"}
{"created":"2025-05-01","title":"Kimina Lean Server: Technical Report","abstract":"We introduce the Kimina Lean Server, an open-source project that enables fast and scalable interaction with Lean 4 via a unified REST API, designed as a simple verifier for reinforcement learning pipelines. Built on top of the Lean FRO's LeanREPL, it combines server-side parallelization by managing multiple Lean REPL processes in parallel, with an LRU caching strategy that reuses Lean imports across multiple requests. These features help reduce initialization overhead and allow large-scale batch processing of Lean code. The client-side interface allows users to submit batches of proofs and receive Lean feedback, including extracted tactics and tactic states via infotree processing. These features enable a high-performance, scalable workflow for both interaction and extraction of proofs, tactics, and tactic states. We open source our implementation on GitHub.","authors":["Marco Dos Santos","Haiming Wang","Hugues de Saxc\\'e","Ran Wang","Mantas Baksys","Mert Unsal","Junqi Liu","Zhengying Liu","Jia Li"],"url":"https://arxiv.org/abs/2504.21230"}
{"created":"2025-05-01","title":"T2ID-CAS: Diffusion Model and Class Aware Sampling to Mitigate Class Imbalance in Neck Ultrasound Anatomical Landmark Detection","abstract":"Neck ultrasound (US) plays a vital role in airway management by providing non-invasive, real-time imaging that enables rapid and precise interventions. Deep learning-based anatomical landmark detection in neck US can further facilitate procedural efficiency. However, class imbalance within datasets, where key structures like tracheal rings and vocal folds are underrepresented, presents significant challenges for object detection models. To address this, we propose T2ID-CAS, a hybrid approach that combines a text-to-image latent diffusion model with class-aware sampling to generate high-quality synthetic samples for underrepresented classes. This approach, rarely explored in the ultrasound domain, improves the representation of minority classes. Experimental results using YOLOv9 for anatomical landmark detection in neck US demonstrated that T2ID-CAS achieved a mean Average Precision of 88.2, significantly surpassing the baseline of 66. This highlights its potential as a computationally efficient and scalable solution for mitigating class imbalance in AI-assisted ultrasound-guided interventions.","authors":["Manikanta Varaganti","Amulya Vankayalapati","Nour Awad","Gregory R. Dion","Laura J. Brattain"],"url":"https://arxiv.org/abs/2504.21231"}
{"created":"2025-05-01","title":"Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math","abstract":"Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.","authors":["Haoran Xu","Baolin Peng","Hany Awadalla","Dongdong Chen","Yen-Chun Chen","Mei Gao","Young Jin Kim","Yunsheng Li","Liliang Ren","Yelong Shen","Shuohang Wang","Weijian Xu","Jianfeng Gao","Weizhu Chen"],"url":"https://arxiv.org/abs/2504.21233"}
{"created":"2025-05-01","title":"Realizing Quantum Wireless Sensing Without Extra Reference Sources: Architecture, Algorithm, and Sensitivity Maximization","abstract":"Rydberg Atomic REceivers (RAREs) have shown compelling advantages in the precise measurement of radio-frequency signals, empowering quantum wireless sensing. Existing RARE-based sensing systems primarily rely on the heterodyne-sensing technique, which introduces an extra reference source to serve as the atomic mixer. However, this approach entails a bulky transceiver architecture and is limited in the supportable sensing bandwidth. To address these challenges, we propose self-heterodyne sensing, a novel concept where the self-interference caused by the transmitter acts as the reference signal. It is shown that a self-heterodyne RARE functions as an atomic autocorrelator, eliminating the need for extra reference sources while supporting sensing signals with much wider bandwidth than the conventional heterodyne-sensing method. Next, a two-stage algorithm is devised to estimate the target range for self-heterodyne RAREs. This algorithm is shown to closely approach the Cramer-Rao lower bound. Furthermore, we introduce the power-trajectory (P-trajectory) design for RAREs, which maximizes the sensing sensitivity through time-varying transmission power optimization. A heuristic P-trajectory is developed to capture the profile of the asymptotically optimal time-varying power. This design is then extended to practical P-trajectories by incorporating the transmitter power constraints. Numerical results validate the superiority of the proposed designs for quantum wireless sensing.","authors":["Mingyao Cui","Qunsong Zeng","Zhanwei Wang","Kaibin Huang"],"url":"https://arxiv.org/abs/2504.21234"}
{"created":"2025-05-01","title":"Memorization and Knowledge Injection in Gated LLMs","abstract":"Large Language Models (LLMs) currently struggle to sequentially add new memories and integrate new knowledge. These limitations contrast with the human ability to continuously learn from new experiences and acquire knowledge throughout life. Most existing approaches add memories either through large context windows or external memory buffers (e.g., Retrieval-Augmented Generation), and studies on knowledge injection rarely test scenarios resembling everyday life events. In this work, we introduce a continual learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event memories directly into the weights of LLMs. Each memory is stored in a dedicated set of gated low-rank weights. During inference, a gating mechanism activates relevant memory weights by matching query embeddings to stored memory embeddings. This enables the model to both recall entire memories and answer related questions. On two datasets - fictional characters and Wikipedia events - MEGa outperforms baseline approaches in mitigating catastrophic forgetting. Our model draws inspiration from the complementary memory system of the human brain.","authors":["Xu Pan","Ely Hahami","Zechen Zhang","Haim Sompolinsky"],"url":"https://arxiv.org/abs/2504.21239"}
{"created":"2025-05-01","title":"Countering underproduction of peer produced goods","abstract":"Peer produced goods such as online knowledge bases and free/libre open source software rely on contributors who often choose their tasks regardless of consumer needs. These goods are susceptible to underproduction: when popular goods are relatively low quality. Although underproduction is a common feature of peer production, very little is known about how to counteract it. We use a detailed longitudinal dataset from English Wikipedia to show that more experienced contributors -- including those who contribute without an account -- tend to contribute to underproduced goods. A within-person analysis shows that contributors' efforts shift toward underproduced goods over time. These findings illustrate the value of retaining contributors in peer production, including those contributing without accounts, as a means to counter underproduction.","authors":["Kaylea Champion","Benjamin Mako Hill"],"url":"https://arxiv.org/abs/2504.21240"}
{"created":"2025-05-01","title":"Passive Measurement of Autonomic Arousal in Real-World Settings","abstract":"The autonomic nervous system (ANS) is activated during stress, which can have negative effects on cardiovascular health, sleep, the immune system, and mental health. While there are ways to quantify ANS activity in laboratories, there is a paucity of methods that have been validated in real-world contexts. We present the Fitbit Body Response Algorithm, an approach to continuous remote measurement of ANS activation through widely available remote wrist-based sensors. The design was validated via two experiments, a Trier Social Stress Test (n = 45) and ecological momentary assessments (EMA) of perceived stress (n=87), providing both controlled and ecologically valid test data. Model performance predicting perceived stress when using all available sensor modalities was consistent with expectations (accuracy=0.85) and outperformed models with access to only a subset of the signals. We discuss and address challenges to sensing that arise in real world settings that do not present in conventional lab environments.","authors":["Samy Abdel-Ghaffar","Isaac Galatzer-Levy","Conor Heneghan","Xin Liu","Sarah Kernasovskiy","Brennan Garrett","Andrew Barakat","Daniel McDuff"],"url":"https://arxiv.org/abs/2504.21242"}
{"created":"2025-05-01","title":"Data-driven operator learning for energy-efficient building control","abstract":"Energy-efficient ventilation control plays a vital role in reducing building energy consumption while ensuring occupant health and comfort. While Computational Fluid Dynamics (CFD) simulations offer high-fidelity modeling of airflow for building HVAC design, their high computational cost makes them impractical for practical adoption in real-time building management system. In this work, we present a data-driven framework that combines the physical accuracy of CFD with the computational efficiency of machine learning to enable energy-efficient building ventilation control. Our method jointly optimizes airflow supply rates and vent angles to reduce energy use and adhere to air quality constraints. We train a neural operator transformer to learn the mapping from building control actions to airflow field distributions using high-resolution CFD data. This learned operator enables a gradient-based control framework capable of optimal decision-making. Experimental results demonstrate that our approach achieves substantial energy savings compared to maximum airflow rate control, rule-based control, and data-driven control based on regional average CO2 predictions, while consistently maintaining safe indoor air quality. These results highlight the practicality and scalability of our method for enabling safe and energy-efficient building management.","authors":["Yuexin Bian","Yuanyuan Shi"],"url":"https://arxiv.org/abs/2504.21243"}
{"created":"2025-05-01","title":"Subject Information Extraction for Novelty Detection with Domain Shifts","abstract":"Unsupervised novelty detection (UND), aimed at identifying novel samples, is essential in fields like medical diagnosis, cybersecurity, and industrial quality control. Most existing UND methods assume that the training data and testing normal data originate from the same domain and only consider the distribution variation between training data and testing data. However, in real scenarios, it is common for normal testing and training data to originate from different domains, a challenge known as domain shift. The discrepancies between training and testing data often lead to incorrect classification of normal data as novel by existing methods. A typical situation is that testing normal data and training data describe the same subject, yet they differ in the background conditions. To address this problem, we introduce a novel method that separates subject information from background variation encapsulating the domain information to enhance detection performance under domain shifts. The proposed method minimizes the mutual information between the representations of the subject and background while modelling the background variation using a deep Gaussian mixture model, where the novelty detection is conducted on the subject representations solely and hence is not affected by the variation of domains. Extensive experiments demonstrate that our model generalizes effectively to unseen domains and significantly outperforms baseline methods, especially under substantial domain shifts between training and testing data.","authors":["Yangyang Qu","Dazhi Fu","Jicong Fan"],"url":"https://arxiv.org/abs/2504.21247"}
{"created":"2025-05-01","title":"Multi-modal Transfer Learning for Dynamic Facial Emotion Recognition in the Wild","abstract":"Facial expression recognition (FER) is a subset of computer vision with important applications for human-computer-interaction, healthcare, and customer service. FER represents a challenging problem-space because accurate classification requires a model to differentiate between subtle changes in facial features. In this paper, we examine the use of multi-modal transfer learning to improve performance on a challenging video-based FER dataset, Dynamic Facial Expression in-the-Wild (DFEW). Using a combination of pretrained ResNets, OpenPose, and OmniVec networks, we explore the impact of cross-temporal, multi-modal features on classification accuracy. Ultimately, we find that these finely-tuned multi-modal feature generators modestly improve accuracy of our transformer-based classification model.","authors":["Ezra Engel","Lishan Li","Chris Hudy","Robert Schleusner"],"url":"https://arxiv.org/abs/2504.21248"}
{"created":"2025-05-01","title":"Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA","abstract":"Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.","authors":["Xuanzhao Dong","Wenhui Zhu","Hao Wang","Xiwen Chen","Peijie Qiu","Rui Yin","Yi Su","Yalin Wang"],"url":"https://arxiv.org/abs/2504.21252"}
{"created":"2025-05-01","title":"ABG-NAS: Adaptive Bayesian Genetic Neural Architecture Search for Graph Representation Learning","abstract":"Effective and efficient graph representation learning is essential for enabling critical downstream tasks, such as node classification, link prediction, and subgraph search. However, existing graph neural network (GNN) architectures often struggle to adapt to diverse and complex graph structures, limiting their ability to provide robust and generalizable representations. To address this challenge, we propose ABG-NAS, a novel framework for automated graph neural network architecture search tailored for efficient graph representation learning. ABG-NAS encompasses three key components: a Comprehensive Architecture Search Space (CASS), an Adaptive Genetic Optimization Strategy (AGOS), and a Bayesian-Guided Tuning Module (BGTM). CASS systematically explores diverse propagation (P) and transformation (T) operations, enabling the discovery of GNN architectures capable of capturing intricate graph characteristics. AGOS dynamically balances exploration and exploitation, ensuring search efficiency and preserving solution diversity. BGTM further optimizes hyperparameters periodically, enhancing the scalability and robustness of the resulting architectures. Empirical evaluations on benchmark datasets (Cora, PubMed, Citeseer, and CoraFull) demonstrate that ABG-NAS consistently outperforms both manually designed GNNs and state-of-the-art neural architecture search (NAS) methods. These results highlight the potential of ABG-NAS to advance graph representation learning by providing scalable and adaptive solutions for diverse graph structures. Our code is publicly available at https://github.com/sserranw/ABG-NAS.","authors":["Sixuan Wang","Jiao Yin","Jinli Cao","MingJian Tang","Hua Wang","Yanchun Zhang"],"url":"https://arxiv.org/abs/2504.21254"}
{"created":"2025-05-01","title":"LSTM+Geo with xgBoost Filtering: A Novel Approach for Race and Ethnicity Imputation with Reduced Bias","abstract":"Accurate imputation of race and ethnicity (R&amp;E) is crucial for analyzing disparities and informing policy. Methods like Bayesian Improved Surname Geocoding (BISG) are widely used but exhibit limitations, including systematic misclassification biases linked to socioeconomic status. This paper introduces LSTM+Geo, a novel approach enhancing Long Short-Term Memory (LSTM) networks with census tract geolocation information. Using a large voter dataset, we demonstrate that LSTM+Geo (88.7% accuracy) significantly outperforms standalone LSTM (86.4%) and Bayesian methods like BISG (82.9%) and BIFSG (86.8%) in accuracy and F1-score on a held-out validation set. LSTM+Geo reduces the rate at which non-White individuals are misclassified as White (White FPR 19.3%) compared to name-only LSTMs (White FPR 24.6%). While sophisticated ensemble methods incorporating XGBoost achieve the highest overall accuracy (up to 89.4%) and lowest White FPR (17.8%), LSTM+Geo offers strong standalone performance with improved bias characteristics compared to baseline models. Integrating LSTM+Geo into an XGBoost ensemble further boosts accuracy, highlighting its utility as both a standalone model and a component for advanced systems. We give a caution at the end regarding the appropriate use of these methods.","authors":["S. Chalavadi","A. Pastor","T. Leitch"],"url":"https://arxiv.org/abs/2504.21259"}
{"created":"2025-05-01","title":"Power Flow Approximations for Multiphase Distribution Networks using Gaussian Processes","abstract":"Learning-based approaches are increasingly leveraged to manage and coordinate the operation of grid-edge resources in active power distribution networks. Among these, model-based techniques stand out for their superior data efficiency and robustness compared to model-free methods. However, effective model learning requires a learning-based approximator for the underlying power flow model. This study extends existing work by introducing a data-driven power flow method based on Gaussian Processes (GPs) to approximate the multiphase power flow model, by mapping net load injections to nodal voltages. Simulation results using the IEEE 123-bus and 8500-node distribution test feeders demonstrate that the trained GP model can reliably predict the nonlinear power flow solutions with minimal training data. We also conduct a comparative analysis of the training efficiency and testing performance of the proposed GP-based power flow approximator against a deep neural network-based approximator, highlighting the advantages of our data-efficient approach. Results over realistic operating conditions show that despite an 85% reduction in the training sample size (corresponding to a 92.8% improvement in training time), GP models produce a 99.9% relative reduction in mean absolute error compared to the baselines of deep neural networks.","authors":["Daniel Glover","Parikshit Pareek","Deepjyoti Deka","Anamika Dubey"],"url":"https://arxiv.org/abs/2504.21260"}
{"created":"2025-05-01","title":"Multi-Domain Causal Discovery in Bijective Causal Models","abstract":"We consider the problem of causal discovery (a.k.a., causal structure learning) in a multi-domain setting. We assume that the causal functions are invariant across the domains, while the distribution of the exogenous noise may vary. Under causal sufficiency (i.e., no confounders exist), we show that the causal diagram can be discovered under less restrictive functional assumptions compared to previous work. What enables causal discovery in this setting is bijective generation mechanisms (BGM), which ensures that the functional relation between the exogenous noise $E$ and the endogenous variable $Y$ is bijective and differentiable in both directions at every level of the cause variable $X = x$. BGM generalizes a variety of models including additive noise model, LiNGAM, post-nonlinear model, and location-scale noise model. Further, we derive a statistical test to find the parents set of the target variable. Experiments on various synthetic and real-world datasets validate our theoretical findings.","authors":["Kasra Jalaldoust","Saber Salehkaleybar","Negar Kiyavash"],"url":"https://arxiv.org/abs/2504.21261"}
{"created":"2025-05-01","title":"Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning","abstract":"Visual In-Context Learning (VICL) enables adaptively solving vision tasks by leveraging pixel demonstrations, mimicking human-like task completion through analogy. Prompt selection is critical in VICL, but current methods assume the existence of a single \"ideal\" prompt in a pool of candidates, which in practice may not hold true. Multiple suitable prompts may exist, but individually they often fall short, leading to difficulties in selection and the exclusion of useful context. To address this, we propose a new perspective: prompt condensation. Rather than relying on a single prompt, candidate prompts collaborate to efficiently integrate informative contexts without sacrificing resolution. We devise Condenser, a lightweight external plugin that compresses relevant fine-grained context across multiple prompts. Optimized end-to-end with the backbone, Condenser ensures accurate integration of contextual cues. Experiments demonstrate Condenser outperforms state-of-the-arts across benchmark tasks, showing superior context compression, scalability with more prompts, and enhanced computational efficiency compared to ensemble methods, positioning it as a highly competitive solution for VICL. Code is open-sourced at https://github.com/gimpong/CVPR25-Condenser.","authors":["Jinpeng Wang","Tianci Luo","Yaohua Zha","Yan Feng","Ruisheng Luo","Bin Chen","Tao Dai","Long Chen","Yaowei Wang","Shu-Tao Xia"],"url":"https://arxiv.org/abs/2504.21263"}
{"created":"2025-05-01","title":"CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion","abstract":"In action recognition tasks, feature diversity is essential for enhancing model generalization and performance. Existing methods typically promote feature diversity by expanding the training data in the sample space, which often leads to inefficiencies and semantic inconsistencies. To overcome these problems, we propose a novel Coarse-fine text co-guidance Diffusion model (CoCoDiff). CoCoDiff generates diverse yet semantically consistent features in the latent space by leveraging diffusion and multi-granularity textual guidance. Specifically, our approach feeds spatio-temporal features extracted from skeleton sequences into a latent diffusion model to generate diverse action representations. Meanwhile, we introduce a coarse-fine text co-guided strategy that leverages textual information from large language models (LLMs) to ensure semantic consistency between the generated features and the original inputs. It is noted that CoCoDiff operates as a plug-and-play auxiliary module during training, incurring no additional inference cost. Extensive experiments demonstrate that CoCoDiff achieves SOTA performance on skeleton-based action recognition benchmarks, including NTU RGB+D, NTU RGB+D 120 and Kinetics-Skeleton.","authors":["Zhifu Zhao","Hanyang Hua","Jianan Li","Shaoxin Wu","Fu Li","Yangtao Zhou","Yang Li"],"url":"https://arxiv.org/abs/2504.21266"}
{"created":"2025-05-01","title":"A Framework for Elastic Adaptation of User Multiple Intents in Sequential Recommendation","abstract":"Recently, substantial research has been conducted on sequential recommendation, with the objective of forecasting the subsequent item by leveraging a user's historical sequence of interacted items. Prior studies employ both capsule networks and self-attention techniques to effectively capture diverse underlying intents within a user's interaction sequence, thereby achieving the most advanced performance in sequential recommendation. However, users could potentially form novel intents from fresh interactions as the lengths of user interaction sequences grow. Consequently, models need to be continually updated or even extended to adeptly encompass these emerging user intents, referred as incremental multi-intent sequential recommendation. % We refer to this problem as incremental multi-intent sequential recommendation, which has not yet been well investigated in the existing literature. In this paper, we propose an effective Incremental learning framework for user Multi-intent Adaptation in sequential recommendation called IMA, which augments the traditional fine-tuning strategy with the existing-intents retainer, new-intents detector, and projection-based intents trimmer to adaptively expand the model to accommodate user's new intents and prevent it from forgetting user's existing intents. Furthermore, we upgrade the IMA into an Elastic Multi-intent Adaptation (EMA) framework which can elastically remove inactive intents and compress user intent vectors under memory space limit. Extensive experiments on real-world datasets verify the effectiveness of the proposed IMA and EMA on incremental multi-intent sequential recommendation, compared with various baselines.","authors":["Zhikai Wang","Yanyan Shen"],"url":"https://arxiv.org/abs/2504.21270"}
{"created":"2025-05-01","title":"Assessing LLM code generation quality through path planning tasks","abstract":"As LLM-generated code grows in popularity, more evaluation is needed to assess the risks of using such tools, especially for safety-critical applications such as path planning. Existing coding benchmarks are insufficient as they do not reflect the context and complexity of safety-critical applications. To this end, we assessed six LLMs' abilities to generate the code for three different path-planning algorithms and tested them on three maps of various difficulties. Our results suggest that LLM-generated code presents serious hazards for path planning applications and should not be applied in safety-critical contexts without rigorous testing.","authors":["Wanyi Chen","Meng-Wen Su","Mary L. Cummings"],"url":"https://arxiv.org/abs/2504.21276"}
{"created":"2025-05-01","title":"Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models","abstract":"The integration of reinforcement learning (RL) into the reasoning capabilities of Multimodal Large Language Models (MLLMs) has rapidly emerged as a transformative research direction. While MLLMs significantly extend Large Language Models (LLMs) to handle diverse modalities such as vision, audio, and video, enabling robust reasoning across multimodal inputs remains a major challenge. This survey systematically reviews recent advances in RL-based reasoning for MLLMs, covering key algorithmic designs, reward mechanism innovations, and practical applications. We highlight two main RL paradigms--value-free and value-based methods--and analyze how RL enhances reasoning abilities by optimizing reasoning trajectories and aligning multimodal information. Furthermore, we provide an extensive overview of benchmark datasets, evaluation protocols, and existing limitations, and propose future research directions to address current bottlenecks such as sparse rewards, inefficient cross-modal reasoning, and real-world deployment constraints. Our goal is to offer a comprehensive and structured guide to researchers interested in advancing RL-based reasoning in the multimodal era.","authors":["Guanghao Zhou","Panjia Qiu","Cen Chen","Jie Wang","Zheming Yang","Jian Xu","Minghui Qiu"],"url":"https://arxiv.org/abs/2504.21277"}
{"created":"2025-05-01","title":"Robust Multi-agent Communication Based on Decentralization-Oriented Adversarial Training","abstract":"In typical multi-agent reinforcement learning (MARL) problems, communication is important for agents to share information and make the right decisions. However, due to the complexity of training multi-agent communication, existing methods often fall into the dilemma of local optimization, which leads to the concentration of communication in a limited number of channels and presents an unbalanced structure. Such unbalanced communication policy are vulnerable to abnormal conditions, where the damage of critical communication channels can trigger the crash of the entire system. Inspired by decentralization theory in sociology, we propose DMAC, which enhances the robustness of multi-agent communication policies by retraining them into decentralized patterns. Specifically, we train an adversary DMAC\\_Adv which can dynamically identify and mask the critical communication channels, and then apply the adversarial samples generated by DMAC\\_Adv to the adversarial learning of the communication policy to force the policy in exploring other potential communication schemes and transition to a decentralized structure. As a training method to improve robustness, DMAC can be fused with any learnable communication policy algorithm. The experimental results in two communication policies and four multi-agent tasks demonstrate that DMAC achieves higher improvement on robustness and performance of communication policy compared with two state-of-the-art and commonly-used baselines. Also, the results demonstrate that DMAC can achieve decentralized communication structure with acceptable communication cost.","authors":["Xuyan Ma","Yawen Wang","Junjie Wang","Xiaofei Xie","Boyu Wu","Shoubin Li","Fanjiang Xu","Qing Wang"],"url":"https://arxiv.org/abs/2504.21278"}
{"created":"2025-05-01","title":"Device-Algorithm Co-Design of Ferroelectric Compute-in-Memory In-Situ Annealer for Combinatorial Optimization Problems","abstract":"Combinatorial optimization problems (COPs) are crucial in many applications but are computationally demanding. Traditional Ising annealers address COPs by directly converting them into Ising models (known as direct-E transformation) and solving them through iterative annealing. However, these approaches require vector-matrix-vector (VMV) multiplications with a complexity of $O(n^2)$ for Ising energy computation and complex exponential annealing factor calculations during annealing process, thus significantly increasing hardware costs. In this work, we propose a ferroelectric compute-in-memory (CiM) in-situ annealer to overcome aforementioned challenges. The proposed device-algorithm co-design framework consists of (i) a novel transformation method (first to our known) that converts COPs into an innovative incremental-E form, which reduces the complexity of VMV multiplication from $O(n^2)$ to $O(n)$, and approximates exponential annealing factor with a much simplified fractional form; (ii) a double gate ferroelectric FET (DG FeFET)-based CiM crossbar that efficiently computes the in-situ incremental-E form by leveraging the unique structure of DG FeFETs; (iii) %When feasible solutions are detected, a CiM annealer that approaches the solutions of COPs via iterative incremental-E computations within a tunable back gate-based in-situ annealing flow. Evaluation results show that our proposed CiM annealer significantly reduces hardware overhead, reducing energy consumption by 1503/1716$\\times$ and time cost by 8.08/8.15$\\times$ in solving 3000-node Max-Cut problems compared to two state-of-the-art annealers. It also exhibits high solving efficiency, achieving a remarkable average success rate of 98\\%, whereas other annealers show only 50\\% given the same iteration counts.","authors":["Yu Qian","Xianmin Huang","Ranran Wang","Zeyu Yang","Min Zhou","Thomas K\\\"ampfe","Cheng Zhuo","Xunzhao Yin"],"url":"https://arxiv.org/abs/2504.21280"}
{"created":"2025-05-01","title":"Mamba Based Feature Extraction And Adaptive Multilevel Feature Fusion For 3D Tumor Segmentation From Multi-modal Medical Image","abstract":"Multi-modal 3D medical image segmentation aims to accurately identify tumor regions across different modalities, facing challenges from variations in image intensity and tumor morphology. Traditional convolutional neural network (CNN)-based methods struggle with capturing global features, while Transformers-based methods, despite effectively capturing global context, encounter high computational costs in 3D medical image segmentation. The Mamba model combines linear scalability with long-distance modeling, making it a promising approach for visual representation learning. However, Mamba-based 3D multi-modal segmentation still struggles to leverage modality-specific features and fuse complementary information effectively. In this paper, we propose a Mamba based feature extraction and adaptive multilevel feature fusion for 3D tumor segmentation using multi-modal medical image. We first develop the specific modality Mamba encoder to efficiently extract long-range relevant features that represent anatomical and pathological structures present in each modality. Moreover, we design an bi-level synergistic integration block that dynamically merges multi-modal and multi-level complementary features by the modality attention and channel attention learning. Lastly, the decoder combines deep semantic information with fine-grained details to generate the tumor segmentation map. Experimental results on medical image datasets (PET/CT and MRI multi-sequence) show that our approach achieve competitive performance compared to the state-of-the-art CNN, Transformer, and Mamba-based approaches.","authors":["Zexin Ji","Beiji Zou","Xiaoyan Kui","Hua Li","Pierre Vera","Su Ruan"],"url":"https://arxiv.org/abs/2504.21281"}
{"created":"2025-05-01","title":"Birdie: Natural Language-Driven Table Discovery Using Differentiable Search Index","abstract":"Natural language (NL)-driven table discovery identifies relevant tables from large table repositories based on NL queries. While current deep-learning-based methods using the traditional dense vector search pipeline, i.e., representation-index-search, achieve remarkable accuracy, they face several limitations that impede further performance improvements: (i) the errors accumulated during the table representation and indexing phases affect the subsequent search accuracy; and (ii) insufficient query-table interaction hinders effective semantic alignment, impeding accuracy improvements. In this paper, we propose a novel framework Birdie, using a differentiable search index. It unifies the indexing and search into a single encoder-decoder language model, thus getting rid of error accumulations. Birdie first assigns each table a prefix-aware identifier and leverages a large language model-based query generator to create synthetic queries for each table. It then encodes the mapping between synthetic queries/tables and their corresponding table identifiers into the parameters of an encoder-decoder language model, enabling deep query-table interactions. During search, the trained model directly generates table identifiers for a given query. To accommodate the continual indexing of dynamic tables, we introduce an index update strategy via parameter isolation, which mitigates the issue of catastrophic forgetting. Extensive experiments demonstrate that Birdie outperforms state-of-the-art dense methods by 16.8% in accuracy, and reduces forgetting by over 90% compared to other continual learning approaches.","authors":["Yuxiang Guo","Zhonghao Hu","Yuren Mao","Baihua Zheng","Yunjun Gao","Mingwei Zhou"],"url":"https://arxiv.org/abs/2504.21282"}
{"created":"2025-05-01","title":"Unified Network Modeling for Six Cross-Layer Scenarios in Space-Air-Ground Integrated Networks","abstract":"The space-air-ground integrated network (SAGIN) can enable global range and seamless coverage in the future network. SAGINs consist of three spatial layer network nodes: 1) satellites on the space layer, 2) aerial vehicles on the aerial layer, and 3) ground devices on the ground layer. Data transmissions in SAGINs include six unique cross-spatial-layer scenarios, i.e., three uplink and three downlink transmissions across three spatial layers. For simplicity, we call them \\textit{six cross-layer scenarios}. Considering the diverse cross-layer scenarios, it is crucial to conduct a unified network modeling regarding node coverage and distributions in all scenarios. To achieve this goal, we develop a unified modeling approach of coverage regions for all six cross-layer scenarios. Given a receiver in each scenario, its coverage region on a transmitter-distributed surface is modeled as a spherical dome. Utilizing spherical geometry, the analytical models of the spherical-dome coverage regions are derived and unified for six cross-layer scenarios. We conduct extensive numerical results to examine the coverage models under varying carrier frequencies, receiver elevation angles, and transceivers' altitudes. Based on the coverage model, we develop an algorithm to generate node distributions under spherical coverage regions, which can assist in testing SAGINs before practical implementations.","authors":["Yalin Liu","Yaru Fu","Qubeijian Wang","Hong-Ning Dai"],"url":"https://arxiv.org/abs/2504.21284"}
{"created":"2025-05-01","title":"Orthogonal Factor-Based Biclustering Algorithm (BCBOF) for High-Dimensional Data and Its Application in Stock Trend Prediction","abstract":"Biclustering is an effective technique in data mining and pattern recognition. Biclustering algorithms based on traditional clustering face two fundamental limitations when processing high-dimensional data: (1) The distance concentration phenomenon in high-dimensional spaces leads to data sparsity, rendering similarity measures ineffective; (2) Mainstream linear dimensionality reduction methods disrupt critical local structural patterns. To apply biclustering to high-dimensional datasets, we propose an orthogonal factor-based biclustering algorithm (BCBOF). First, we constructed orthogonal factors in the vector space of the high-dimensional dataset. Then, we performed clustering using the coordinates of the original data in the orthogonal subspace as clustering targets. Finally, we obtained biclustering results of the original dataset. Since dimensionality reduction was applied before clustering, the proposed algorithm effectively mitigated the data sparsity problem caused by high dimensionality. Additionally, we applied this biclustering algorithm to stock technical indicator combinations and stock price trend prediction. Biclustering results were transformed into fuzzy rules, and we incorporated profit-preserving and stop-loss rules into the rule set, ultimately forming a fuzzy inference system for stock price trend predictions and trading signals. To evaluate the performance of BCBOF, we compared it with existing biclustering methods using multiple evaluation metrics. The results showed that our algorithm outperformed other biclustering techniques. To validate the effectiveness of the fuzzy inference system, we conducted virtual trading experiments using historical data from 10 A-share stocks. The experimental results showed that the generated trading strategies yielded higher returns for investors.","authors":["Yan Huang","Da-Qing Zhang"],"url":"https://arxiv.org/abs/2504.21289"}
{"created":"2025-05-01","title":"Efficiency of Analysis of Transitive Relations using Query-Driven, Ground-and-Solve, and Fact-Driven Inference","abstract":"Logic rules allow analysis of complex relationships, especially including transitive relations, to be expressed easily and clearly. Rule systems allow queries using such rules to be done automatically. It is well known that rule systems using different inference methods can have very different efficiency on the same rules and queries. In fact, different variants of rules and queries expressing the same relationships can have more drastically different efficiency in the same rule system. Many other differences can also cause differences in efficiency. What exactly are the differences? Can we capture them exactly and predict efficiency precisely? What are the best systems to use?","authors":["Yanhong A. Liu","Scott D. Stoller","John Idogun","Yi Tong"],"url":"https://arxiv.org/abs/2504.21291"}
{"created":"2025-05-01","title":"Can We Achieve Efficient Diffusion without Self-Attention? Distilling Self-Attention into Convolutions","abstract":"Contemporary diffusion models built upon U-Net or Diffusion Transformer (DiT) architectures have revolutionized image generation through transformer-based attention mechanisms. The prevailing paradigm has commonly employed self-attention with quadratic computational complexity to handle global spatial relationships in complex images, thereby synthesizing high-fidelity images with coherent visual semantics.Contrary to conventional wisdom, our systematic layer-wise analysis reveals an interesting discrepancy: self-attention in pre-trained diffusion models predominantly exhibits localized attention patterns, closely resembling convolutional inductive biases. This suggests that global interactions in self-attention may be less critical than commonly assumed.Driven by this, we propose \\(\\Delta\\)ConvFusion to replace conventional self-attention modules with Pyramid Convolution Blocks (\\(\\Delta\\)ConvBlocks).By distilling attention patterns into localized convolutional operations while keeping other components frozen, \\(\\Delta\\)ConvFusion achieves performance comparable to transformer-based counterparts while reducing computational cost by 6929$\\times$ and surpassing LinFusion by 5.42$\\times$ in efficiency--all without compromising generative fidelity.","authors":["ZiYi Dong","Chengxing Zhou","Weijian Deng","Pengxu Wei","Xiangyang Ji","Liang Lin"],"url":"https://arxiv.org/abs/2504.21292"}
{"created":"2025-05-01","title":"Learning Multi-view Multi-class Anomaly Detection","abstract":"The latest trend in anomaly detection is to train a unified model instead of training a separate model for each category. However, existing multi-class anomaly detection (MCAD) models perform poorly in multi-view scenarios because they often fail to effectively model the relationships and complementary information among different views. In this paper, we introduce a Multi-View Multi-Class Anomaly Detection model (MVMCAD), which integrates information from multiple views to accurately identify anomalies. Specifically, we propose a semi-frozen encoder, where a pre-encoder prior enhancement mechanism is added before the frozen encoder, enabling stable cross-view feature modeling and efficient adaptation for improved anomaly detection. Furthermore, we propose an Anomaly Amplification Module (AAM) that models global token interactions and suppresses normal regions to enhance anomaly signals, leading to improved detection performance in multi-view settings. Finally, we propose a Cross-Feature Loss that aligns shallow encoder features with deep decoder features and vice versa, enhancing the model's sensitivity to anomalies at different semantic levels under multi-view scenarios. Extensive experiments on the Real-IAD dataset for multi-view multi-class anomaly detection validate the effectiveness of our approach, achieving state-of-the-art performance of 91.0/88.6/82.1 and 99.1/43.9/48.2/95.2 for image-level and the pixel-level, respectively.","authors":["Qianzi Yu","Yang Cao","Yu Kang"],"url":"https://arxiv.org/abs/2504.21294"}
{"created":"2025-05-01","title":"Fairness in Graph Learning Augmented with Machine Learning: A Survey","abstract":"Augmenting specialised machine learning techniques into traditional graph learning models has achieved notable success across various domains, including federated graph learning, dynamic graph learning, and graph transformers. However, the intricate mechanisms of these specialised techniques introduce significant challenges in maintaining model fairness, potentially resulting in discriminatory outcomes in high-stakes applications such as recommendation systems, disaster response, criminal justice, and loan approval. This paper systematically examines the unique fairness challenges posed by Graph Learning augmented with Machine Learning (GL-ML). It highlights the complex interplay between graph learning mechanisms and machine learning techniques, emphasising how the augmentation of machine learning both enhances and complicates fairness. Additionally, we explore four critical techniques frequently employed to improve fairness in GL-ML methods. By thoroughly investigating the root causes and broader implications of fairness challenges in this rapidly evolving field, this work establishes a robust foundation for future research and innovation in GL-ML fairness.","authors":["Renqiang Luo","Ziqi Xu","Xikun Zhang","Qing Qing","Huafei Huang","Enyan Dai","Zhe Wang","Bo Yang"],"url":"https://arxiv.org/abs/2504.21296"}
{"created":"2025-05-01","title":"Participatory AI, Public Sector AI, Differential Privacy, Conversational Interfaces, Explainable AI, Citizen Engagement in AI","abstract":"This paper introduces a conversational interface system that enables participatory design of differentially private AI systems in public sector applications. Addressing the challenge of balancing mathematical privacy guarantees with democratic accountability, we propose three key contributions: (1) an adaptive $\\epsilon$-selection protocol leveraging TOPSIS multi-criteria decision analysis to align citizen preferences with differential privacy (DP) parameters, (2) an explainable noise-injection framework featuring real-time Mean Absolute Error (MAE) visualizations and GPT-4-powered impact analysis, and (3) an integrated legal-compliance mechanism that dynamically modulates privacy budgets based on evolving regulatory constraints. Our results advance participatory AI practices by demonstrating how conversational interfaces can enhance public engagement in algorithmic privacy mechanisms, ensuring that privacy-preserving AI in public sector governance remains both mathematically robust and democratically accountable.","authors":["Wenjun Yang","Eyhab Al-Masri"],"url":"https://arxiv.org/abs/2504.21297"}
{"created":"2025-05-01","title":"BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models","abstract":"Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline.","authors":["Zhiting Fan","Ruizhe Chen","Zuozhu Liu"],"url":"https://arxiv.org/abs/2504.21299"}
{"created":"2025-05-01","title":"CMD: Constraining Multimodal Distribution for Domain Adaptation in Stereo Matching","abstract":"Recently, learning-based stereo matching methods have achieved great improvement in public benchmarks, where soft argmin and smooth L1 loss play a core contribution to their success. However, in unsupervised domain adaptation scenarios, we observe that these two operations often yield multimodal disparity probability distributions in target domains, resulting in degraded generalization. In this paper, we propose a novel approach, Constrain Multi-modal Distribution (CMD), to address this issue. Specifically, we introduce \\textit{uncertainty-regularized minimization} and \\textit{anisotropic soft argmin} to encourage the network to produce predominantly unimodal disparity distributions in the target domain, thereby improving prediction accuracy. Experimentally, we apply the proposed method to multiple representative stereo-matching networks and conduct domain adaptation from synthetic data to unlabeled real-world scenes. Results consistently demonstrate improved generalization in both top-performing and domain-adaptable stereo-matching models. The code for CMD will be available at: \\href{https://github.com/gallenszl/CMD}{https://github.com/gallenszl/CMD}.","authors":["Zhelun Shen","Zhuo Li","Chenming Wu","Zhibo Rao","Lina Liu","Yuchao Dai","Liangjun Zhang"],"url":"https://arxiv.org/abs/2504.21302"}
{"created":"2025-05-01","title":"Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges","abstract":"Large language models (LLMs) exhibit probabilistic output characteristics, yet conventional evaluation frameworks rely on deterministic scalar metrics. This study introduces a Bayesian approach for LLM capability assessment that integrates prior knowledge through probabilistic inference, addressing limitations under limited-sample regimes. By treating model capabilities as latent variables and leveraging a curated query set to induce discriminative responses, we formalize model ranking as a Bayesian hypothesis testing problem over mutually exclusive capability intervals. Experimental evaluations with GPT-series models demonstrate that the proposed method achieves superior discrimination compared to conventional evaluation methods. Results indicate that even with reduced sample sizes, the approach maintains statistical robustness while providing actionable insights, such as probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by bridging Bayesian inference with practical constraints in real-world deployment scenarios.","authors":["Xiao Xiao","Yu Su","Sijing Zhang","Zhang Chen","Yadong Chen","Tian Liu"],"url":"https://arxiv.org/abs/2504.21303"}
{"created":"2025-05-01","title":"Unsupervised Feature Transformation via In-context Generation, Generator-critic LLM Agents, and Duet-play Teaming","abstract":"Feature transformation involves generating a new set of features from the original dataset to enhance the data's utility. In certain domains like material performance screening, dimensionality is large and collecting labels is expensive and lengthy. It highly necessitates transforming feature spaces efficiently and without supervision to enhance data readiness and AI utility. However, existing methods fall short in efficient navigation of a vast space of feature combinations, and are mostly designed for supervised settings. To fill this gap, our unique perspective is to leverage a generator-critic duet-play teaming framework using LLM agents and in-context learning to derive pseudo-supervision from unsupervised data. The framework consists of three interconnected steps: (1) Critic agent diagnoses data to generate actionable advice, (2) Generator agent produces tokenized feature transformations guided by the critic's advice, and (3) Iterative refinement ensures continuous improvement through feedback between agents. The generator-critic framework can be generalized to human-agent collaborative generation, by replacing the critic agent with human experts. Extensive experiments demonstrate that the proposed framework outperforms even supervised baselines in feature transformation efficiency, robustness, and practical applicability across diverse datasets.","authors":["Nanxu Gong","Xinyuan Wang","Wangyang Ying","Haoyue Bai","Sixun Dong","Haifeng Chen","Yanjie Fu"],"url":"https://arxiv.org/abs/2504.21304"}
{"created":"2025-05-01","title":"Virtual Element Method Applied to Two Dimensional Axisymmetric Elastic Problems","abstract":"This work presents a Virtual Element Method (VEM) formulation tailored for two-dimensional axisymmetric problems in linear elasticity. By exploiting the rotational symmetry of the geometry and loading conditions, the problem is reduced to a meridional cross-section, where all fields depend only on the radial and axial coordinates. The method incorporates the radial weight $r$ in both the weak formulation and the interpolation estimates to remain consistent with the physical volume measure of cylindrical coordinates. A projection operator onto constant strain fields is constructed via boundary integrals, and a volumetric correction term is introduced to account for the divergence of the stress field arising from axisymmetry. The stabilization term is designed to act only on the kernel of the projection and is implemented using a boundary-based formulation that guarantees stability without affecting polynomial consistency. Furthermore, an a priori interpolation error estimate is established in a weighted Sobolev space, showing optimal convergence rates. The implementation is validated through patch tests that demonstrate the accuracy, consistency, and robustness of the proposed approach.","authors":["Paulo Akira F. Enabe","Rodrigo Provasi"],"url":"https://arxiv.org/abs/2504.21305"}
{"created":"2025-05-01","title":"The Dual Power of Interpretable Token Embeddings: Jailbreaking Attacks and Defenses for Diffusion Model Unlearning","abstract":"Despite the remarkable generalization capabilities of diffusion models, recent studies have shown that these models can memorize and generate harmful content when prompted with specific text instructions. Although fine-tuning approaches have been developed to mitigate this issue by unlearning harmful concepts, these methods can be easily circumvented through jailbreaking attacks. This indicates that the harmful concept has not been fully erased from the model. However, existing attack methods, while effective, lack interpretability regarding why unlearned models still retain the concept, thereby hindering the development of defense strategies. In this work, we address these limitations by proposing an attack method that learns an orthogonal set of interpretable attack token embeddings. The attack token embeddings can be decomposed into human-interpretable textual elements, revealing that unlearned models still retain the target concept through implicit textual components. Furthermore, these attack token embeddings are robust and transferable across text prompts, initial noises, and unlearned models. Finally, leveraging this diverse set of embeddings, we design a defense method applicable to both our proposed attack and existing attack methods. Experimental results demonstrate the effectiveness of both our attack and defense strategies.","authors":["Siyi Chen","Yimeng Zhang","Sijia Liu","Qing Qu"],"url":"https://arxiv.org/abs/2504.21307"}
{"created":"2025-05-01","title":"AGHI-QA: A Subjective-Aligned Dataset and Metric for AI-Generated Human Images","abstract":"The rapid development of text-to-image (T2I) generation approaches has attracted extensive interest in evaluating the quality of generated images, leading to the development of various quality assessment methods for general-purpose T2I outputs. However, existing image quality assessment (IQA) methods are limited to providing global quality scores, failing to deliver fine-grained perceptual evaluations for structurally complex subjects like humans, which is a critical challenge considering the frequent anatomical and textural distortions in AI-generated human images (AGHIs). To address this gap, we introduce AGHI-QA, the first large-scale benchmark specifically designed for quality assessment of AGHIs. The dataset comprises 4,000 images generated from 400 carefully crafted text prompts using 10 state of-the-art T2I models. We conduct a systematic subjective study to collect multidimensional annotations, including perceptual quality scores, text-image correspondence scores, visible and distorted body part labels. Based on AGHI-QA, we evaluate the strengths and weaknesses of current T2I methods in generating human images from multiple dimensions. Furthermore, we propose AGHI-Assessor, a novel quality metric that integrates the large multimodal model (LMM) with domain-specific human features for precise quality prediction and identification of visible and distorted body parts in AGHIs. Extensive experimental results demonstrate that AGHI-Assessor showcases state-of-the-art performance, significantly outperforming existing IQA methods in multidimensional quality assessment and surpassing leading LMMs in detecting structural distortions in AGHIs.","authors":["Yunhao Li","Sijing Wu","Wei Sun","Zhichao Zhang","Yucheng Zhu","Zicheng Zhang","Huiyu Duan","Xiongkuo Min","Guangtao Zhai"],"url":"https://arxiv.org/abs/2504.21308"}
{"created":"2025-05-01","title":"An Evaluation of a Visual Question Answering Strategy for Zero-shot Facial Expression Recognition in Still Images","abstract":"Facial expression recognition (FER) is a key research area in computer vision and human-computer interaction. Despite recent advances in deep learning, challenges persist, especially in generalizing to new scenarios. In fact, zero-shot FER significantly reduces the performance of state-of-the-art FER models. To address this problem, the community has recently started to explore the integration of knowledge from Large Language Models for visual tasks. In this work, we evaluate a broad collection of locally executed Visual Language Models (VLMs), avoiding the lack of task-specific knowledge by adopting a Visual Question Answering strategy. We compare the proposed pipeline with state-of-the-art FER models, both integrating and excluding VLMs, evaluating well-known FER benchmarks: AffectNet, FERPlus, and RAF-DB. The results show excellent performance for some VLMs in zero-shot FER scenarios, indicating the need for further exploration to improve FER generalization.","authors":["Modesto Castrill\\'on-Santana","Oliverio J Santana","David Freire-Obreg\\'on","Daniel Hern\\'andez-Sosa","Javier Lorenzo-Navarro"],"url":"https://arxiv.org/abs/2504.21309"}
{"created":"2025-05-01","title":"Covert Prompt Transmission for Secure Large Language Model Services","abstract":"This paper investigates covert prompt transmission for secure and efficient large language model (LLM) services over wireless networks. We formulate a latency minimization problem under fidelity and detectability constraints to ensure confidential and covert communication by jointly optimizing the transmit power and prompt compression ratio. To solve this problem, we first propose a prompt compression and encryption (PCAE) framework, performing surprisal-guided compression followed by lightweight permutation-based encryption. Specifically, PCAE employs a locally deployed small language model (SLM) to estimate token-level surprisal scores, selectively retaining semantically critical tokens while discarding redundant ones. This significantly reduces computational overhead and transmission duration. To further enhance covert wireless transmission, we then develop a group-based proximal policy optimization (GPPO) method that samples multiple candidate actions for each state, selecting the optimal one within each group and incorporating a Kullback-Leibler (KL) divergence penalty to improve policy stability and exploration. Simulation results show that PCAE achieves comparable LLM response fidelity to baseline methods while reducing preprocessing latency by over five orders of magnitude, enabling real-time edge deployment. We further validate PCAE effectiveness across diverse LLM backbones, including DeepSeek-32B, Qwen-32B, and their smaller variants. Moreover, GPPO reduces covert transmission latency by up to 38.6\\% compared to existing reinforcement learning strategies, with further analysis showing that increased transmit power provides additional latency benefits.","authors":["Ruichen Zhang","Yinqiu Liu","Shunpu Tang","Jiacheng Wang","Dusit Niyato","Geng Sun","Yonghui Li","Sumei Sun"],"url":"https://arxiv.org/abs/2504.21311"}
{"created":"2025-05-01","title":"Annotating and Auditing the Safety Properties of Unsafe Rust","abstract":"Unsafe code is a critical topic in ensuring the security of system software development in Rust. It is the sole source of potential undefined behaviors, assuming the compiler is sound. To avoid the misuse of unsafe code, Rust developers should provide clear safety property annotations for unsafe APIs. However, there is limited official guidance and few best practices for annotating unsafe code. Even the current best practices for safety property annotations in the Rust standard library are ad hoc and informal. In this paper, we design a domain-specific language to describe the safety properties of unsafe APIs, which may serve as a precursor for automated verification in the future. Furthermore, to ensure that the caller of an unsafe API properly delegates the safety property required by the callee, we propose a novel unsafety propagation graph to model the usage and propagation of unsafe code. Based on this graph, we further introduce a method to partition the graph into smaller graphs, such that each graph serves as a self-contained audit unit for examining the soundness of unsafe code encapsulation and safety property annotation. We applied our approach to the Rust standard library, and the experimental results demonstrate that our method is both practical and effective. Additionally, we have fixed safety property description issues in 23 APIs.","authors":["Zihao Rao","Hongliang Tian","Xin Wang","Hui Xu"],"url":"https://arxiv.org/abs/2504.21312"}
{"created":"2025-05-01","title":"Capturing Conditional Dependence via Auto-regressive Diffusion Models","abstract":"Diffusion models have demonstrated appealing performance in both image and video generation. However, many works discover that they struggle to capture important, high-level relationships that are present in the real world. For example, they fail to learn physical laws from data, and even fail to understand that the objects in the world exist in a stable fashion. This is due to the fact that important conditional dependence structures are not adequately captured in the vanilla diffusion models. In this work, we initiate an in-depth study on strengthening the diffusion model to capture the conditional dependence structures in the data. In particular, we examine the efficacy of the auto-regressive (AR) diffusion models for such purpose and develop the first theoretical results on the sampling error of AR diffusion models under (possibly) the mildest data assumption. Our theoretical findings indicate that, compared with typical diffusion models, the AR variant produces samples with a reduced gap in approximating the data conditional distribution. On the other hand, the overall inference time of the AR-diffusion models is only moderately larger than that for the vanilla diffusion models, making them still practical for large scale applications. We also provide empirical results showing that when there is clear conditional dependence structure in the data, the AR diffusion models captures such structure, whereas vanilla DDPM fails to do so. On the other hand, when there is no obvious conditional dependence across patches of the data, AR diffusion does not outperform DDPM.","authors":["Xunpeng Huang","Yujin Han","Difan Zou","Yian Ma","Tong Zhang"],"url":"https://arxiv.org/abs/2504.21314"}
{"created":"2025-05-01","title":"Reduced order asymptotic observer of friction in motion control","abstract":"An asymptotic observer of the motion state variables with nonlinear friction [1] benefits from a robust to the noise state-space representation of the dynamic friction force, including pre-sliding transitions, and implements the reduced order Luenberger observation law with only measurable output displacement. The uniform asymptotic stability and convergence analysis of the proposed observer are elaborated by using the Lyapunov function-based stability criteria by Ignatyev and imposing the parametric constraints on the time dependent eigenvalues to be always negative real. A design procedure for assigning a dominant (thus slowest) real pole of the observer system matrix is proposed. A thorough experimental evaluation is given for the proposed observer-based friction compensation, which is performed for positioning and tracking tasks and compared with an optimally tuned PID feedback control.","authors":["Michael Ruderman"],"url":"https://arxiv.org/abs/2504.21316"}
{"created":"2025-05-01","title":"Redundancy Analysis and Mitigation for Machine Learning-Based Process Monitoring of Additive Manufacturing","abstract":"The deployment of machine learning (ML)-based process monitoring systems has significantly advanced additive manufacturing (AM) by enabling real-time defect detection, quality assessment, and process optimization. However, redundancy is a critical yet often overlooked challenge in the deployment and operation of ML-based AM process monitoring systems. Excessive redundancy leads to increased equipment costs, compromised model performance, and high computational requirements, posing barriers to industrial adoption. However, existing research lacks a unified definition of redundancy and a systematic framework for its evaluation and mitigation. This paper defines redundancy in ML-based AM process monitoring and categorizes it into sample-level, feature-level, and model-level redundancy. A comprehensive multi-level redundancy mitigation (MLRM) framework is proposed, incorporating advanced methods such as data registration, downscaling, cross-modality knowledge transfer, and model pruning to systematically reduce redundancy while improving model performance. The framework is validated through an ML-based in-situ defect detection case study for directed energy deposition (DED), demonstrating a 91% reduction in latency, a 47% decrease in error rate, and a 99.4% reduction in storage requirements. Additionally, the proposed approach lowers sensor costs and energy consumption, enabling a lightweight, cost-effective, and scalable monitoring system. By defining redundancy and introducing a structured mitigation framework, this study establishes redundancy analysis and mitigation as a key enabler of efficient ML-based process monitoring in production environments.","authors":["Jiarui Xie","Yaoyao Fiona Zhao"],"url":"https://arxiv.org/abs/2504.21317"}
{"created":"2025-05-01","title":"Phi-4-reasoning Technical Report","abstract":"We introduce Phi-4-reasoning, a 14-billion parameter reasoning model that achieves strong performance on complex reasoning tasks. Trained via supervised fine-tuning of Phi-4 on carefully curated set of \"teachable\" prompts-selected for the right level of complexity and diversity-and reasoning demonstrations generated using o3-mini, Phi-4-reasoning generates detailed reasoning chains that effectively leverage inference-time compute. We further develop Phi-4-reasoning-plus, a variant enhanced through a short phase of outcome-based reinforcement learning that offers higher performance by generating longer reasoning traces. Across a wide range of reasoning tasks, both models outperform significantly larger open-weight models such as DeepSeek-R1-Distill-Llama-70B model and approach the performance levels of full DeepSeek-R1 model. Our comprehensive evaluations span benchmarks in math and scientific reasoning, coding, algorithmic problem solving, planning, and spatial understanding. Interestingly, we observe a non-trivial transfer of improvements to general-purpose benchmarks as well. In this report, we provide insights into our training data, our training methodologies, and our evaluations. We show that the benefit of careful data curation for supervised fine-tuning (SFT) extends to reasoning language models, and can be further amplified by reinforcement learning (RL). Finally, our evaluation points to opportunities for improving how we assess the performance and robustness of reasoning models.","authors":["Marah Abdin","Sahaj Agarwal","Ahmed Awadallah","Vidhisha Balachandran","Harkirat Behl","Lingjiao Chen","Gustavo de Rosa","Suriya Gunasekar","Mojan Javaheripi","Neel Joshi","Piero Kauffmann","Yash Lara","Caio C\\'esar Teodoro Mendes","Arindam Mitra","Besmira Nushi","Dimitris Papailiopoulos","Olli Saarikivi","Shital Shah","Vaishnavi Shrivastava","Vibhav Vineet","Yue Wu","Safoora Yousefi","Guoqing Zheng"],"url":"https://arxiv.org/abs/2504.21318"}
{"created":"2025-05-01","title":"Universal Encryption of Individual Sequences Under Maximal Leakage","abstract":"We consider the Shannon cipher system in the framework of individual sequences and finite-state encrypters under the metric of maximal leakage of information. A lower bound and an asymptotically matching upper bound on the leakage are derived, which lead to the conclusion that asymptotically minimum leakage can be attained by Lempel-Ziv compression followed by one-time pad encryption of the compressed bit-stream.","authors":["Neri Merhav"],"url":"https://arxiv.org/abs/2504.21321"}
{"created":"2025-05-01","title":"How to Backdoor the Knowledge Distillation","abstract":"Knowledge distillation has become a cornerstone in modern machine learning systems, celebrated for its ability to transfer knowledge from a large, complex teacher model to a more efficient student model. Traditionally, this process is regarded as secure, assuming the teacher model is clean. This belief stems from conventional backdoor attacks relying on poisoned training data with backdoor triggers and attacker-chosen labels, which are not involved in the distillation process. Instead, knowledge distillation uses the outputs of a clean teacher model to guide the student model, inherently preventing recognition or response to backdoor triggers as intended by an attacker. In this paper, we challenge this assumption by introducing a novel attack methodology that strategically poisons the distillation dataset with adversarial examples embedded with backdoor triggers. This technique allows for the stealthy compromise of the student model while maintaining the integrity of the teacher model. Our innovative approach represents the first successful exploitation of vulnerabilities within the knowledge distillation process using clean teacher models. Through extensive experiments conducted across various datasets and attack settings, we demonstrate the robustness, stealthiness, and effectiveness of our method. Our findings reveal previously unrecognized vulnerabilities and pave the way for future research aimed at securing knowledge distillation processes against backdoor attacks.","authors":["Chen Wu","Qian Ma","Prasenjit Mitra","Sencun Zhu"],"url":"https://arxiv.org/abs/2504.21323"}
{"created":"2025-05-01","title":"Text-Conditioned Diffusion Model for High-Fidelity Korean Font Generation","abstract":"Automatic font generation (AFG) is the process of creating a new font using only a few examples of the style images. Generating fonts for complex languages like Korean and Chinese, particularly in handwritten styles, presents significant challenges. Traditional AFGs, like Generative adversarial networks (GANs) and Variational Auto-Encoders (VAEs), are usually unstable during training and often face mode collapse problems. They also struggle to capture fine details within font images. To address these problems, we present a diffusion-based AFG method which generates high-quality, diverse Korean font images using only a single reference image, focusing on handwritten and printed styles. Our approach refines noisy images incrementally, ensuring stable training and visually appealing results. A key innovation is our text encoder, which processes phonetic representations to generate accurate and contextually correct characters, even for unseen characters. We used a pre-trained style encoder from DG FONT to effectively and accurately encode the style images. To further enhance the generation quality, we used perceptual loss that guides the model to focus on the global style of generated images. Experimental results on over 2000 Korean characters demonstrate that our model consistently generates accurate and detailed font images and outperforms benchmark methods, making it a reliable tool for generating authentic Korean fonts across different styles.","authors":["Abdul Sami","Avinash Kumar","Irfanullah Memon","Youngwon Jo","Muhammad Rizwan","Jaeyoung Choi"],"url":"https://arxiv.org/abs/2504.21325"}
{"created":"2025-05-01","title":"Q-function Decomposition with Intervention Semantics with Factored Action Spaces","abstract":"Many practical reinforcement learning environments have a discrete factored action space that induces a large combinatorial set of actions, thereby posing significant challenges. Existing approaches leverage the regular structure of the action space and resort to a linear decomposition of Q-functions, which avoids enumerating all combinations of factored actions. In this paper, we consider Q-functions defined over a lower dimensional projected subspace of the original action space, and study the condition for the unbiasedness of decomposed Q-functions using causal effect estimation from the no unobserved confounder setting in causal statistics. This leads to a general scheme which we call action decomposed reinforcement learning that uses the projected Q-functions to approximate the Q-function in standard model-free reinforcement learning algorithms. The proposed approach is shown to improve sample complexity in a model-based reinforcement learning setting. We demonstrate improvements in sample efficiency compared to state-of-the-art baselines in online continuous control environments and a real-world offline sepsis treatment environment.","authors":["Junkyu Lee","Tian Gao","Elliot Nelson","Miao Liu","Debarun Bhattacharjya","Songtao Lu"],"url":"https://arxiv.org/abs/2504.21326"}
{"created":"2025-05-01","title":"A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees","abstract":"Meta federated learning (FL) is a personalized variant of FL, where multiple agents collaborate on training an initial shared model without exchanging raw data samples. The initial model should be trained in a way that current or new agents can easily adapt it to their local datasets after one or a few fine-tuning steps, thus improving the model personalization. Conventional meta FL approaches minimize the average loss of agents on the local models obtained after one step of fine-tuning. In practice, agents may need to apply several fine-tuning steps to adapt the global model to their local data, especially under highly heterogeneous data distributions across agents. To this end, we present a generalized framework for the meta FL by minimizing the average loss of agents on their local model after any arbitrary number $\\nu$ of fine-tuning steps. For this generalized framework, we present a variant of the well-known federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical convergence analysis to characterize the convergence speed as well as behavior of the meta loss functions in both the exact and approximated cases. Our experiments on real-world datasets demonstrate superior accuracy and faster convergence for the proposed scheme compared to conventional approaches.","authors":["Mohammad Vahid Jamali","Hamid Saber","Jung Hyun Bae"],"url":"https://arxiv.org/abs/2504.21327"}
{"created":"2025-05-01","title":"Multi-level datasets training method in Physics-Informed Neural Networks","abstract":"Physics-Informed Neural Networks have emerged as a promising methodology for solving PDEs, gaining significant attention in computer science and various physics-related fields. Despite being demonstrated the ability to incorporate the physics of laws for versatile applications, PINNs still struggle with the challenging problems which are stiff to be solved and/or have high-frequency components in the solutions, resulting in accuracy and convergence issues. It may not only increase computational costs, but also lead to accuracy loss or solution divergence. In this study, an alternative approach is proposed to mitigate the above-mentioned problems. Inspired by the multi-grid method in CFD community, the underlying idea of the current approach is to efficiently remove different frequency errors via training with different levels of training samples, resulting in a simpler way to improve the training accuracy without spending time in fine-tuning of neural network structures, loss weights as well as hyperparameters. To demonstrate the efficacy of current approach, we first investigate canonical 1D ODE with high-frequency component and 2D convection-diffusion equation with V-cycle training strategy. Finally, the current method is employed for the classical benchmark problem of steady Lid-driven cavity flows at different Reynolds numbers, to investigate the applicability and efficacy for the problem involved multiple modes of high and low frequency. By virtue of various training sequence modes, improvement through predictions lead to 30% to 60% accuracy improvement. We also investigate the synergies between current method and transfer learning techniques for more challenging problems (i.e., higher Re). From the present results, it also revealed that the current framework can produce good predictions even for the case of Re=5000, demonstrating the ability to solve complex high-frequency PDEs.","authors":["Yao-Hsuan Tsai","Hsiao-Tung Juan","Pao-Hsiung Chiu","Chao-An Lin"],"url":"https://arxiv.org/abs/2504.21328"}
{"created":"2025-05-01","title":"Drawing Reeb Graphs","abstract":"Reeb graphs are simple topological descriptors which find applications in many areas like topological data analysis and computational geometry. Despite their prevalence, visualization of Reeb graphs has received less attention. In this paper, we bridge an essential gap in the literature by exploring the complexity of drawing Reeb graphs. Specifically, we demonstrate that Reeb graph crossing number minimization is NP-hard, both for straight-line and curve representations of edges. On the other hand, we identify specific classes of Reeb graphs, namely paths and caterpillars, for which crossing-free drawings exist. We also give an optimal algorithm for drawing cycle-shaped Reeb graphs with the least number of crossings and provide initial observations on the complexities of drawing multi-cycle Reeb graphs. We hope that this work establishes the foundation for an understanding of the graph drawing challenges inherent in Reeb graph visualization and paves the way for future work in this area.","authors":["Erin Chambers","Brittany Terese Fasy","Erfan Hosseini Sereshgi","Maarten L\\\"offler"],"url":"https://arxiv.org/abs/2504.21329"}
{"created":"2025-05-01","title":"Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?","abstract":"Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES) due to their ability to capture semantic meaning. Traditional fine-tuning approaches required technical expertise, limiting accessibility for educators with limited technical backgrounds. However, prompt-based tools like ChatGPT have made AES more accessible, enabling educators to obtain machine-generated scores using natural-language prompts (i.e., the prompt-based paradigm). Despite advancements, prior studies have shown bias in fine-tuned LLMs, particularly against disadvantaged groups. It remains unclear whether such biases persist or are amplified in the prompt-based paradigm with cutting-edge tools. Since such biases are believed to stem from the demographic information embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to predict demographic attributes), this study explores the relationship between the model's predictive power of students' demographic attributes based on their written works and its predictive bias in the scoring task in the prompt-based paradigm. Using a publicly available dataset of over 25,000 students' argumentative essays, we designed prompts to elicit demographic inferences (i.e., gender, first-language background) from GPT-4o and assessed fairness in automated scoring. Then we conducted multivariate regression analysis to explore the impact of the model's ability to predict demographics on its scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat infer students' demographics, particularly their first-language backgrounds, from their essays; (ii) scoring biases are more pronounced when the LLM correctly predicts students' first-language background than when it does not; and (iii) scoring error for non-native English speakers increases when the LLM correctly identifies them as non-native.","authors":["Kaixun Yang","Mladen Rakovi\\'c","Dragan Ga\\v{s}evi\\'c","Guanliang Chen"],"url":"https://arxiv.org/abs/2504.21330"}
{"created":"2025-05-01","title":"MagicCraft: Natural Language-Driven Generation of Dynamic and Interactive 3D Objects for Commercial Metaverse Platforms","abstract":"Metaverse platforms are rapidly evolving to provide immersive spaces for user interaction and content creation. However, the generation of dynamic and interactive 3D objects remains challenging due to the need for advanced 3D modeling and programming skills. To address this challenge, we present MagicCraft, a system that generates functional 3D objects from natural language prompts for metaverse platforms. MagicCraft uses generative AI models to manage the entire content creation pipeline: converting user text descriptions into images, transforming images into 3D models, predicting object behavior, and assigning necessary attributes and scripts. It also provides an interactive interface for users to refine generated objects by adjusting features such as orientation, scale, seating positions, and grip points.","authors":["Ryutaro Kurai","Takefumi Hiraki","Yuichi Hiroi","Yutaro Hirao","Monica Perusqu\\'ia-Hern\\'andez","Hideaki Uchiyama","Kiyoshi Kiyokawa"],"url":"https://arxiv.org/abs/2504.21332"}
{"created":"2025-05-01","title":"Simple Visual Artifact Detection in Sora-Generated Videos","abstract":"The December 2024 release of OpenAI's Sora, a powerful video generation model driven by natural language prompts, highlights a growing convergence between large language models (LLMs) and video synthesis. As these multimodal systems evolve into video-enabled LLMs (VidLLMs), capable of interpreting, generating, and interacting with visual content, understanding their limitations and ensuring their safe deployment becomes essential. This study investigates visual artifacts frequently found and reported in Sora-generated videos, which can compromise quality, mislead viewers, or propagate disinformation. We propose a multi-label classification framework targeting four common artifact label types: label 1: boundary / edge defects, label 2: texture / noise issues, label 3: movement / joint anomalies, and label 4: object mismatches / disappearances. Using a dataset of 300 manually annotated frames extracted from 15 Sora-generated videos, we trained multiple 2D CNN architectures (ResNet-50, EfficientNet-B3 / B4, ViT-Base). The best-performing model trained by ResNet-50 achieved an average multi-label classification accuracy of 94.14%. This work supports the broader development of VidLLMs by contributing to (1) the creation of datasets for video quality evaluation, (2) interpretable artifact-based analysis beyond language metrics, and (3) the identification of visual risks relevant to factuality and safety.","authors":["Misora Sugiyama","Hirokatsu Kataoka"],"url":"https://arxiv.org/abs/2504.21334"}
{"created":"2025-05-01","title":"UniBiomed: A Universal Foundation Model for Grounded Biomedical Image Interpretation","abstract":"Multi-modal interpretation of biomedical images opens up novel opportunities in biomedical image analysis. Conventional AI approaches typically rely on disjointed training, i.e., Large Language Models (LLMs) for clinical text generation and segmentation models for target extraction, which results in inflexible real-world deployment and a failure to leverage holistic biomedical information. To this end, we introduce UniBiomed, the first universal foundation model for grounded biomedical image interpretation. UniBiomed is based on a novel integration of Multi-modal Large Language Model (MLLM) and Segment Anything Model (SAM), which effectively unifies the generation of clinical texts and the segmentation of corresponding biomedical objects for grounded interpretation. In this way, UniBiomed is capable of tackling a wide range of biomedical tasks across ten diverse biomedical imaging modalities. To develop UniBiomed, we curate a large-scale dataset comprising over 27 million triplets of images, annotations, and text descriptions across ten imaging modalities. Extensive validation on 84 internal and external datasets demonstrated that UniBiomed achieves state-of-the-art performance in segmentation, disease recognition, region-aware diagnosis, visual question answering, and report generation. Moreover, unlike previous models that rely on clinical experts to pre-diagnose images and manually craft precise textual or visual prompts, UniBiomed can provide automated and end-to-end grounded interpretation for biomedical image analysis. This represents a novel paradigm shift in clinical workflows, which will significantly improve diagnostic efficiency. In summary, UniBiomed represents a novel breakthrough in biomedical AI, unlocking powerful grounded interpretation capabilities for more accurate and efficient biomedical image analysis.","authors":["Linshan Wu","Yuxiang Nie","Sunan He","Jiaxin Zhuang","Hao Chen"],"url":"https://arxiv.org/abs/2504.21336"}
{"created":"2025-05-01","title":"Cross-Reality Lifestyle: Integrating Physical and Virtual Lives through Multi-Platform Metaverse","abstract":"Technological advances are redefining the relationship between physical and virtual space. Traditionally, when users engage in virtual reality (VR), they are completely cut off from the physical space; similarly, they are unable to access virtual experiences while engaged in physical activities. However, modern multi-platform metaverse environments allow simultaneous participation through mobile devices, creating new opportunities for integrated experiences. This study introduces the concept of \"cross-reality lifestyles\" to examine how users actively combine their physical and virtual activities. We identify three patterns of integration: 1) amplification: one space enhances experiences in the other; 2) complementary: spaces offer different but equally valuable alternatives; and 3) emergence: simultaneous engagement creates entirely new experiences. By analyzing commercial platforms, we create a technical framework that addresses content design, platform infrastructure, and device interfaces. This framework guides the development of cross-reality applications while demonstrating how metaverse technologies blur the traditional boundaries between physical and virtual experiences.","authors":["Yuichi Hiroi","Yuji Hatada","Takefumi Hiraki"],"url":"https://arxiv.org/abs/2504.21337"}
{"created":"2025-05-01","title":"A Memetic Algorithm based on Variational Autoencoder for Black-Box Discrete Optimization with Epistasis among Parameters","abstract":"Black-box discrete optimization (BB-DO) problems arise in many real-world applications, such as neural architecture search and mathematical model estimation. A key challenge in BB-DO is epistasis among parameters where multiple variables must be modified simultaneously to effectively improve the objective function. Estimation of Distribution Algorithms (EDAs) provide a powerful framework for tackling BB-DO problems. In particular, an EDA leveraging a Variational Autoencoder (VAE) has demonstrated strong performance on relatively low-dimensional problems with epistasis while reducing computational cost. Meanwhile, evolutionary algorithms such as DSMGA-II and P3, which integrate bit-flip-based local search with linkage learning, have shown excellent performance on high-dimensional problems. In this study, we propose a new memetic algorithm that combines VAE-based sampling with local search. The proposed method inherits the strengths of both VAE-based EDAs and local search-based approaches: it effectively handles high-dimensional problems with epistasis among parameters without incurring excessive computational overhead. Experiments on NK landscapes -- a challenging benchmark for BB-DO involving epistasis among parameters -- demonstrate that our method outperforms state-of-the-art VAE-based EDA methods, as well as leading approaches such as P3 and DSMGA-II.","authors":["Aoi Kato","Kenta Kojima","Masahiro Nomura","Isao Ono"],"url":"https://arxiv.org/abs/2504.21338"}
{"created":"2025-05-01","title":"Towards Improved Cervical Cancer Screening: Vision Transformer-Based Classification and Interpretability","abstract":"We propose a novel approach to cervical cell image classification for cervical cancer screening using the EVA-02 transformer model. We developed a four-step pipeline: fine-tuning EVA-02, feature extraction, selecting important features through multiple machine learning models, and training a new artificial neural network with optional loss weighting for improved generalization. With this design, our best model achieved an F1-score of 0.85227, outperforming the baseline EVA-02 model (0.84878). We also utilized Kernel SHAP analysis and identified key features correlating with cell morphology and staining characteristics, providing interpretable insights into the decision-making process of the fine-tuned model. Our code is available at https://github.com/Khoa-NT/isbi2025_ps3c.","authors":["Khoa Tuan Nguyen","Ho-min Park","Gaeun Oh","Joris Vankerschaver","Wesley De Neve"],"url":"https://arxiv.org/abs/2504.21340"}
{"created":"2025-05-01","title":"Low latency FPGA implementation of twisted Edward curve cryptography hardware accelerator over prime field","abstract":"The performance of any elliptic curve cryptography hardware accelerator significantly relies on the efficiency of the underlying point multiplication (PM) architecture. This article presents a hardware implementation of field-programmable gate array (FPGA) based modular arithmetic, group operation, and point multiplication unit on the twisted Edwards curve (Edwards25519) over the 256-bit prime field. An original hardware architecture of a unified point operation module in projective coordinates that executes point addition and point doubling within a single module has been developed, taking only 646 clock cycles and ensuring a better security level than conventional approaches. The proposed point multiplication module consumes 1.4 ms time, operating at a maximal clock frequency of 117.8 MHz utilising 164,730 clock cycles having 183.38 kbps throughput on the Xilinx Virtex-5 FPGA platform for 256-bit length of key. The comparative assessment of latency and throughput across various related recent works indicates the effectiveness of our proposed PM architecture. Finally, this high throughput and low latency PM architecture will be a good candidate for rapid data encryption in high-speed wireless communication networks.","authors":["Md Rownak Hossain","Md Sazedur Rahman","Kh Shahriya Zaman","Walid El Fezzani","Mohammad Arif Sobhan Bhuiyan","Chia Chao Kang","Teh Jia Yew","Mahdi H. Miraz"],"url":"https://arxiv.org/abs/2504.21342"}
{"created":"2025-05-01","title":"Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection","abstract":"Objective: A number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. Methods: We obtained 938 low-dose CT scans from the National Lung Screening Trial with 1,246 nodules and semantic features. The Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We finetuned a pretrained Contrastive Language-Image Pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. Results: We evaluated the performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and compared it to three state-of-the-art models. Our model demonstrated an AUROC of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on external datasets. Using CLIP, we also obtained predictions on semantic features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. Conclusion: Our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings.","authors":["Luoting Zhuang","Seyed Mohammad Hossein Tabatabaei","Ramin Salehi-Rad","Linh M. Tran","Denise R. Aberle","Ashley E. Prosper","William Hsu"],"url":"https://arxiv.org/abs/2504.21344"}
{"created":"2025-05-01","title":"IRL Dittos: Embodied Multimodal AI Agent Interactions in Open Spaces","abstract":"We introduce the In Real Life (IRL) Ditto, an AI-driven embodied agent designed to represent remote colleagues in shared office spaces, creating opportunities for real-time exchanges even in their absence. IRL Ditto offers a unique hybrid experience by allowing in-person colleagues to encounter a digital version of their remote teammates, initiating greetings, updates, or small talk as they might in person. Our research question examines: How can the IRL Ditto influence interactions and relationships among colleagues in a shared office space? Through a four-day study, we assessed IRL Ditto's ability to strengthen social ties by simulating presence and enabling meaningful interactions across different levels of social familiarity. We find that enhancing social relationships depended deeply on the foundation of the relationship participants had with the source of the IRL Ditto. This study provides insights into the role of embodied agents in enriching workplace dynamics for distributed teams.","authors":["Seonghee Lee","Denae Ford","John Tang","Sasa Junuzovic","Asta Roseway","Ed Cutrell","Kori Inkpen"],"url":"https://arxiv.org/abs/2504.21347"}
{"created":"2025-05-01","title":"Generative QoE Modeling: A Lightweight Approach for Telecom Networks","abstract":"Quality of Experience (QoE) prediction plays a crucial role in optimizing resource management and enhancing user satisfaction across both telecommunication and OTT services. While recent advances predominantly rely on deep learning models, this study introduces a lightweight generative modeling framework that balances computational efficiency, interpretability, and predictive accuracy. By validating the use of Vector Quantization (VQ) as a preprocessing technique, continuous network features are effectively transformed into discrete categorical symbols, enabling integration with a Hidden Markov Model (HMM) for temporal sequence modeling. This VQ-HMM pipeline enhances the model's capacity to capture dynamic QoE patterns while supporting probabilistic inference on new and unseen data. Experimental results on publicly available time-series datasets incorporating both objective indicators and subjective QoE scores demonstrate the viability of this approach in real-time and resource-constrained environments, where inference latency is also critical. The framework offers a scalable alternative to complex deep learning methods, particularly in scenarios with limited computational resources or where latency constraints are critical.","authors":["Vinti Nayar","Kanica Sachdev","Brejesh Lall"],"url":"https://arxiv.org/abs/2504.21353"}
{"created":"2025-05-01","title":"Nexus-Gen: A Unified Model for Image Understanding, Generation, and Editing","abstract":"Unified multimodal large language models (MLLMs) aim to integrate multimodal understanding and generation abilities through a single framework. Despite their versatility, existing open-source unified models exhibit performance gaps against domain-specific architectures. To bridge this gap, we present Nexus-Gen, a unified model that synergizes the language reasoning capabilities of LLMs with the image synthesis power of diffusion models. To align the embedding space of the LLM and diffusion model, we conduct a dual-phase alignment training process. (1) The autoregressive LLM learns to predict image embeddings conditioned on multimodal inputs, while (2) the vision decoder is trained to reconstruct high-fidelity images from these embeddings. During training the LLM, we identified a critical discrepancy between the autoregressive paradigm's training and inference phases, where error accumulation in continuous embedding space severely degrades generation quality. To avoid this issue, we introduce a prefilled autoregression strategy that prefills input sequence with position-embedded special tokens instead of continuous embeddings. Through dual-phase training, Nexus-Gen has developed the integrated capability to comprehensively address the image understanding, generation and editing tasks. All models, datasets, and codes are published at https://github.com/modelscope/Nexus-Gen.git to facilitate further advancements across the field.","authors":["Hong Zhang","Zhongjie Duan","Xingjun Wang","Yingda Chen","Yuze Zhao","Yu Zhang"],"url":"https://arxiv.org/abs/2504.21356"}
{"created":"2025-05-01","title":"Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection","abstract":"With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort.","authors":["Yang Suwen","Shi Lei"],"url":"https://arxiv.org/abs/2504.21357"}
{"created":"2025-05-01","title":"A comparative study of deep learning and ensemble learning to extend the horizon of traffic forecasting","abstract":"Traffic forecasting is vital for Intelligent Transportation Systems, for which Machine Learning (ML) methods have been extensively explored to develop data-driven Artificial Intelligence (AI) solutions. Recent research focuses on modelling spatial-temporal correlations for short-term traffic prediction, leaving the favourable long-term forecasting a challenging and open issue. This paper presents a comparative study on large-scale real-world signalized arterials and freeway traffic flow datasets, aiming to evaluate promising ML methods in the context of large forecasting horizons up to 30 days. Focusing on modelling capacity for temporal dynamics, we develop one ensemble ML method, eXtreme Gradient Boosting (XGBoost), and a range of Deep Learning (DL) methods, including Recurrent Neural Network (RNN)-based methods and the state-of-the-art Transformer-based method. Time embedding is leveraged to enhance their understanding of seasonality and event factors. Experimental results highlight that while the attention mechanism/Transformer framework is effective for capturing long-range dependencies in sequential data, as the forecasting horizon extends, the key to effective traffic forecasting gradually shifts from temporal dependency capturing to periodicity modelling. Time embedding is particularly effective in this context, helping naive RNN outperform Informer by 31.1% for 30-day-ahead forecasting. Meanwhile, as an efficient and robust model, XGBoost, while learning solely from time features, performs competitively with DL methods. Moreover, we investigate the impacts of various factors like input sequence length, holiday traffic, data granularity, and training data size. The findings offer valuable insights and serve as a reference for future long-term traffic forecasting research and the improvement of AI's corresponding learning capabilities.","authors":["Xiao Zheng","Saeed Asadi Bagloee","Majid Sarvi"],"url":"https://arxiv.org/abs/2504.21358"}
{"created":"2025-05-01","title":"ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality","abstract":"While augmented reality (AR) enables new ways to play, tell stories, and explore ideas rooted in the physical world, authoring personalized AR content remains difficult for non-experts, often requiring professional tools and time. Prior systems have explored AI-driven XR design but typically rely on manually-defined environments and fixed asset libraries, limiting creative flexibility and real-world relevance. We introduce ImaginateAR, a mobile AI-assisted AR authoring system that aims to let anyone build anything, anywhere -- simply by speaking their imagination. ImaginateAR is powered by custom pipelines for offline scene understanding, fast 3D asset generation, and LLM-driven speech interaction. Users might say \"a dragon enjoying a campfire\" (P7) and iteratively refine the scene using both AI and manual tools. Our technical evaluation shows that ImaginateAR produces more accurate outdoor scene graphs and generates 3D meshes faster than prior methods. A three-part user study (N=20) revealed preferred roles for AI in authoring, what and how users create in free-form use, and design implications for future AR authoring tools.","authors":["Jaewook Lee","Filippo Aleotti","Diego Mazala","Guillermo Garcia-Hernando","Sara Vicente","Oliver James Johnston","Isabel Kraus-Liang","Jakub Powierza","Donghoon Shin","Jon E. Froehlich","Gabriel Brostow","Jessica Van Brummelen"],"url":"https://arxiv.org/abs/2504.21360"}
{"created":"2025-05-01","title":"Enhancing New-item Fairness in Dynamic Recommender Systems","abstract":"New-items play a crucial role in recommender systems (RSs) for delivering fresh and engaging user experiences. However, traditional methods struggle to effectively recommend new-items due to their short exposure time and limited interaction records, especially in dynamic recommender systems (DRSs) where new-items get continuously introduced and users' preferences evolve over time. This leads to significant unfairness towards new-items, which could accumulate over the successive model updates, ultimately compromising the stability of the entire system. Therefore, we propose FairAgent, a reinforcement learning (RL)-based new-item fairness enhancement framework specifically designed for DRSs. It leverages knowledge distillation to extract collaborative signals from traditional models, retaining strong recommendation capabilities for old-items. In addition, FairAgent introduces a novel reward mechanism for recommendation tailored to the characteristics of DRSs, which consists of three components: 1) a new-item exploration reward to promote the exposure of dynamically introduced new-items, 2) a fairness reward to adapt to users' personalized fairness requirements for new-items, and 3) an accuracy reward which leverages users' dynamic feedback to enhance recommendation accuracy. Extensive experiments on three public datasets and backbone models demonstrate the superior performance of FairAgent. The results present that FairAgent can effectively boost new-item exposure, achieve personalized new-item fairness, while maintaining high recommendation accuracy.","authors":["Huizhong Guo","Zhu Sun","Dongxia Wang","Tianjun Wei","Jinfeng Li","Jie Zhang"],"url":"https://arxiv.org/abs/2504.21362"}
{"created":"2025-05-01","title":"Uniform-in-time weak error estimates of explicit full-discretization schemes for SPDEs with non-globally Lipschitz coefficients","abstract":"This article is devoted to long-time weak approximations of stochastic partial differential equations (SPDEs) evolving in a bounded domain $\\mathcal{D} \\subset \\mathbb{R}^d$, $d \\leq 3$, with non-globally Lipschitz and possibly non-contractive coefficients. Both the space-time white noise ($d=1$) and the trace-class noise in multiple dimensions $d=2,3$ are examined for the considered SPDEs. Based on a spectral Galerkin spatial semi-discretization, we propose a class of novel full-discretization schemes of exponential type, which are explicit, easily implementable and preserve the ergodicity of the original dissipative SPDEs with possibly non-contractive coefficients. The uniform-in-time weak approximation errors are carefully analyzed in a low regularity and non-contractive setting, with uniform-in-time weak convergence rates obtained. A key ingredient is to establish the uniform-in-time moment bounds (in $L^{4q-2}$-norm, $q \\geq 1$) for the proposed fully discrete schemes in a super-linear setting. This is highly non-trivial for the explicit full-discretization schemes and new arguments are elaborated by fully exploiting a contractive property of the semi-group in $L^{4q-2}$, the dissipativity of the nonlinearity and the particular benefit of the taming strategy. Numerical experiments are finally reported to verify the theoretical findings.","authors":["Yingsong Jiang","Xiaojie Wang"],"url":"https://arxiv.org/abs/2504.21364"}
{"created":"2025-05-01","title":"DGFNet: End-to-End Audio-Visual Source Separation Based on Dynamic Gating Fusion","abstract":"Current Audio-Visual Source Separation methods primarily adopt two design strategies. The first strategy involves fusing audio and visual features at the bottleneck layer of the encoder, followed by processing the fused features through the decoder. However, when there is a significant disparity between the two modalities, this approach may lead to the loss of critical information. The second strategy avoids direct fusion and instead relies on the decoder to handle the interaction between audio and visual features. Nonetheless, if the encoder fails to integrate information across modalities adequately, the decoder may be unable to effectively capture the complex relationships between them. To address these issues, this paper proposes a dynamic fusion method based on a gating mechanism that dynamically adjusts the modality fusion degree. This approach mitigates the limitations of solely relying on the decoder and facilitates efficient collaboration between audio and visual features. Additionally, an audio attention module is introduced to enhance the expressive capacity of audio features, thereby further improving model performance. Experimental results demonstrate that our method achieves significant performance improvements on two benchmark datasets, validating its effectiveness and advantages in Audio-Visual Source Separation tasks.","authors":["Yinfeng Yu","Shiyu Sun"],"url":"https://arxiv.org/abs/2504.21366"}
{"created":"2025-05-01","title":"Security Analysis and Implementation of Cryptocurrency Systems on Blockchain 2.0","abstract":"Blockchain technology has set off a wave of decentralization in the world since its birth. The trust system constructed by blockchain technology based on cryptography algorithm and computing power provides a practical and powerful solution to solve the trust problem in human society. In order to make more convenient use of the characteristics of blockchain and build applications on it, smart contracts appear. By defining some trigger automatic execution contracts, the application space of blockchain is expanded and the foundation for the rapid development of blockchain is laid. This is blockchain 2.0. However, the programmability of smart contracts also introduces vulnerabilities. In order to cope with the insufficient security guarantee of high-value application networks running on blockchain 2.0 and smart contracts, this article will be represented by Ethereum to introduce the technical details of understanding blockchain 2.0 and the operation principle of contract virtual machines, and explain how cryptocurrencies based on blockchain 2.0 are constructed and operated. The common security problems and solutions are also discussed. Based on relevant research and on-chain practice, this paper provides a complete and comprehensive perspective to understanding cryptocurrency technology based on blockchain 2.0 and provides a reference for building more secure cryptocurrency contracts.","authors":["Pengfei Gao","Dechao Kong","Xiaoqi Li"],"url":"https://arxiv.org/abs/2504.21367"}
{"created":"2025-05-01","title":"Revisiting Diffusion Autoencoder Training for Image Reconstruction Quality","abstract":"Diffusion autoencoders (DAEs) are typically formulated as a noise prediction model and trained with a linear-$\\beta$ noise schedule that spends much of its sampling steps at high noise levels. Because high noise levels are associated with recovering large-scale image structures and low noise levels with recovering details, this configuration can result in low-quality and blurry images. However, it should be possible to improve details while spending fewer steps recovering structures because the latent code should already contain structural information. Based on this insight, we propose a new DAE training method that improves the quality of reconstructed images. We divide training into two phases. In the first phase, the DAE is trained as a vanilla autoencoder by always setting the noise level to the highest, forcing the encoder and decoder to populate the latent code with structural information. In the second phase, we incorporate a noise schedule that spends more time in the low-noise region, allowing the DAE to learn how to perfect the details. Our method results in images that have accurate high-level structures and low-level details while still preserving useful properties of the latent codes.","authors":["Pramook Khungurn","Sukit Seripanitkarn","Phonphrm Thawatdamrongkit","Supasorn Suwajanakorn"],"url":"https://arxiv.org/abs/2504.21368"}
{"created":"2025-05-01","title":"ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning","abstract":"Reasoning models such as OpenAI o3 and DeepSeek-R1 have demonstrated strong performance on reasoning-intensive tasks through extended Chain-of-Thought (CoT) prompting. While longer reasoning traces can facilitate a more thorough exploration of solution paths for complex problems, researchers have observed that these models often \"overthink\", leading to inefficient inference. In this paper, we introduce ShorterBetter, a simple yet effective reinforcement learning methed that enables reasoning language models to discover their own optimal CoT lengths without human intervention. By sampling multiple outputs per problem and defining the Sample Optimal Length (SOL) as the shortest correct response among all the outputs, our method dynamically guides the model toward optimal inference lengths. Applied to the DeepSeek-Distill-Qwen-1.5B model, ShorterBetter achieves up to an 80% reduction in output length on both in-domain and out-of-domain reasoning tasks while maintaining accuracy. Our analysis shows that overly long reasoning traces often reflect loss of reasoning direction, and thus suggests that the extended CoT produced by reasoning models is highly compressible.","authors":["Jingyang Yi","Jiazheng Wang"],"url":"https://arxiv.org/abs/2504.21370"}
{"created":"2025-05-01","title":"Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction","abstract":"Speech Event Extraction (SpeechEE) is a challenging task that lies at the intersection of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), requiring the identification of structured event information from spoken language. In this work, we present a modular, pipeline-based SpeechEE framework that integrates high-performance ASR with semantic search-enhanced prompting of Large Language Models (LLMs). Our system first classifies speech segments likely to contain events using a hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models. It then employs few-shot LLM prompting, dynamically enriched via semantic similarity retrieval, to identify event triggers and extract corresponding arguments. We evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini) highlighting significant performance gains with o1-mini, which achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming prior benchmarks. Our results demonstrate that pipeline approaches, when empowered by retrieval-augmented LLMs, can rival or exceed end-to-end systems while maintaining interpretability and modularity. This work provides practical insights into LLM-driven event extraction and opens pathways for future hybrid models combining textual and acoustic features.","authors":["M\\'at\\'e Gedeon"],"url":"https://arxiv.org/abs/2504.21372"}
{"created":"2025-05-01","title":"Synergy-CLIP: Extending CLIP with Multi-modal Integration for Robust Representation Learning","abstract":"Multi-modal representation learning has become a pivotal area in artificial intelligence, enabling the integration of diverse modalities such as vision, text, and audio to solve complex problems. However, existing approaches predominantly focus on bimodal interactions, such as image-text pairs, which limits their ability to fully exploit the richness of multi-modal data. Furthermore, the integration of modalities in equal-scale environments remains underexplored due to the challenges of constructing large-scale, balanced datasets. In this study, we propose Synergy-CLIP, a novel framework that extends the contrastive language-image pre-training (CLIP) architecture to enhance multi-modal representation learning by integrating visual, textual, and audio modalities. Unlike existing methods that focus on adapting individual modalities to vanilla-CLIP, Synergy-CLIP aligns and captures latent information across three modalities equally. To address the high cost of constructing large-scale multi-modal datasets, we introduce VGG-sound+, a triple-modal dataset designed to provide equal-scale representation of visual, textual, and audio data. Synergy-CLIP is validated on various downstream tasks, including zero-shot classification, where it outperforms existing baselines. Additionally, we introduce a missing modality reconstruction task, demonstrating Synergy-CLIP's ability to extract synergy among modalities in realistic application scenarios. These contributions provide a robust foundation for advancing multi-modal representation learning and exploring new research directions.","authors":["Sangyeon Cho","Jangyeong Jeon","Mingi Kim","Junyeong Kim"],"url":"https://arxiv.org/abs/2504.21375"}
{"created":"2025-05-01","title":"Sparse-to-Sparse Training of Diffusion Models","abstract":"Diffusion models (DMs) are a powerful type of generative models that have achieved state-of-the-art results in various image synthesis tasks and have shown potential in other domains, such as natural language processing and temporal data modeling. Despite their stable training dynamics and ability to produce diverse high-quality samples, DMs are notorious for requiring significant computational resources, both in the training and inference stages. Previous work has focused mostly on increasing the efficiency of model inference. This paper introduces, for the first time, the paradigm of sparse-to-sparse training to DMs, with the aim of improving both training and inference efficiency. We focus on unconditional generation and train sparse DMs from scratch (Latent Diffusion and ChiroDiff) on six datasets using three different methods (Static-DM, RigL-DM, and MagRan-DM) to study the effect of sparsity in model performance. Our experiments show that sparse DMs are able to match and often outperform their Dense counterparts, while substantially reducing the number of trainable parameters and FLOPs. We also identify safe and effective values to perform sparse-to-sparse training of DMs.","authors":["In\\^es Cardoso Oliveira","Decebal Constantin Mocanu","Luis A. Leiva"],"url":"https://arxiv.org/abs/2504.21380"}
{"created":"2025-05-01","title":"Robust and Scalable Renaming with Subquadratic Bits","abstract":"In the renaming problem, a set of $n$ nodes, each with a unique identity from a large namespace $[N]$, needs to obtain new unique identities in a smaller namespace $[M]$. A renaming algorithm is strong if $M=n$. Renaming is a classical problem in distributed computing with a range of applications, and there exist many time-efficient solutions for fault-tolerant renaming in synchronous message-passing systems. However, all previous algorithms send $\\Omega(n^2)$ messages, and many of them also send large messages each containing $\\Omega(n)$ bits. Moreover, most algorithms' performance do not scale with the actual number of failures. These limitations restrict their practical performance.","authors":["Sirui Bai","Xinyu Fu","Yuheng Wang","Yuyi Wang","Chaodong Zheng"],"url":"https://arxiv.org/abs/2504.21382"}
{"created":"2025-05-01","title":"FAST-Q: Fast-track Exploration with Adversarially Balanced State Representations for Counterfactual Action Estimation in Offline Reinforcement Learning","abstract":"Recent advancements in state-of-the-art (SOTA) offline reinforcement learning (RL) have primarily focused on addressing function approximation errors, which contribute to the overestimation of Q-values for out-of-distribution actions, a challenge that static datasets exacerbate. However, high stakes applications such as recommendation systems in online gaming, introduce further complexities due to player's psychology (intent) driven by gameplay experiences and the inherent volatility on the platform. These factors create highly sparse, partially overlapping state spaces across policies, further influenced by the experiment path selection logic which biases state spaces towards specific policies. Current SOTA methods constrain learning from such offline data by clipping known counterfactual actions as out-of-distribution due to poor generalization across unobserved states. Further aggravating conservative Q-learning and necessitating more online exploration. FAST-Q introduces a novel approach that (1) leverages Gradient Reversal Learning to construct balanced state representations, regularizing the policy-specific bias between the player's state and action thereby enabling counterfactual estimation; (2) supports offline counterfactual exploration in parallel with static data exploitation; and (3) proposes a Q-value decomposition strategy for multi-objective optimization, facilitating explainable recommendations over short and long-term objectives. These innovations demonstrate superiority of FAST-Q over prior SOTA approaches and demonstrates at least 0.15 percent increase in player returns, 2 percent improvement in lifetime value (LTV), 0.4 percent enhancement in the recommendation driven engagement, 2 percent improvement in the player's platform dwell time and an impressive 10 percent reduction in the costs associated with the recommendation, on our volatile gaming platform.","authors":["Pulkit Agrawal","Rukma Talwadker","Aditya Pareek","Tridib Mukherjee"],"url":"https://arxiv.org/abs/2504.21383"}
{"created":"2025-05-01","title":"Logical Modelling in CS Education: Bridging the Natural Language Gap","abstract":"An important learning objective for computer science students is to learn how to formalize descriptions of real world scenarios in order to subsequently solve real world challenges using methods and algorithms from formal foundations of computer science. Two key steps when formalizing with logical formalisms are to (a) choose a suitable vocabulary, that is, e.g., which propositional variables or first-order symbols to use, and with which intended meaning, and then to (b) construct actual formal descriptions, i.e. logical formulas over the chosen vocabulary. While (b) is addressed by several educational support systems for formal foundations of computer science, (a) is so far not addressed at all -- likely because it involves specifying the intended meaning of symbols in natural language.","authors":["Tristan Kneisel","Fabian Vehlken","Thomas Zeume"],"url":"https://arxiv.org/abs/2504.21384"}
{"created":"2025-05-01","title":"IDDM: Bridging Synthetic-to-Real Domain Gap from Physics-Guided Diffusion for Real-world Image Dehazing","abstract":"Due to the domain gap between real-world and synthetic hazy images, current data-driven dehazing algorithms trained on synthetic datasets perform well on synthetic data but struggle to generalize to real-world scenarios. To address this challenge, we propose \\textbf{I}mage \\textbf{D}ehazing \\textbf{D}iffusion \\textbf{M}odels (IDDM), a novel diffusion process that incorporates the atmospheric scattering model into noise diffusion. IDDM aims to use the gradual haze formation process to help the denoising Unet robustly learn the distribution of clear images from the conditional input hazy images. We design a specialized training strategy centered around IDDM. Diffusion models are leveraged to bridge the domain gap from synthetic to real-world, while the atmospheric scattering model provides physical guidance for haze formation. During the forward process, IDDM simultaneously introduces haze and noise into clear images, and then robustly separates them during the sampling process. By training with physics-guided information, IDDM shows the ability of domain generalization, and effectively restores the real-world hazy images despite being trained on synthetic datasets. Extensive experiments demonstrate the effectiveness of our method through both quantitative and qualitative comparisons with state-of-the-art approaches.","authors":["Shijun Zhou","Yajing Liu","Chunhui Hao","Zhiyuan Liu","Jiandong Tian"],"url":"https://arxiv.org/abs/2504.21385"}
{"created":"2025-05-01","title":"Comparison of Different Deep Neural Network Models in the Cultural Heritage Domain","abstract":"The integration of computer vision and deep learning is an essential part of documenting and preserving cultural heritage, as well as improving visitor experiences. In recent years, two deep learning paradigms have been established in the field of computer vision: convolutional neural networks and transformer architectures. The present study aims to make a comparative analysis of some representatives of these two techniques of their ability to transfer knowledge from generic dataset, such as ImageNet, to cultural heritage specific tasks. The results of testing examples of the architectures VGG, ResNet, DenseNet, Visual Transformer, Swin Transformer, and PoolFormer, showed that DenseNet is the best in terms of efficiency-computability ratio.","authors":["Teodor Boyadzhiev","Gabriele Lagani","Luca Ciampi","Giuseppe Amato","Krassimira Ivanova"],"url":"https://arxiv.org/abs/2504.21387"}
{"created":"2025-05-01","title":"Enhanced Semi-Supervised Stamping Process Monitoring with Physically-Informed Feature Extraction","abstract":"In tackling frequent anomalies in stamping processes, this study introduces a novel semi-supervised in-process anomaly monitoring framework, utilizing accelerometer signals and physics information, to capture the process anomaly effectively. The proposed framework facilitates the construction of a monitoring model with imbalanced sample distribution, which enables in-process condition monitoring in real-time to prevent batch anomalies, which helps to reduce batch defects risk and enhance production yield. Firstly, to effectively capture key features from raw data containing redundant information, a hybrid feature extraction algorithm is proposed to utilize data-driven methods and physical mechanisms simultaneously. Secondly, to address the challenge brought by imbalanced sample distribution, a semi-supervised anomaly detection model is established, which merely employs normal samples to build a golden baseline model, and a novel deviation score is proposed to quantify the anomaly level of each online stamping stroke. The effectiveness of the proposed feature extraction method is validated with various classification algorithms. A real-world in-process dataset from stamping manufacturing workshop is employed to illustrate the superiority of proposed semi-supervised framework with enhance performance for process anomaly monitoring.","authors":["Jianyu Zhang","Jianshe Feng","Yizhang Zhu","Fanyu Qi"],"url":"https://arxiv.org/abs/2504.21389"}
{"created":"2025-05-01","title":"Statistical process discovery","abstract":"Stochastic process discovery is concerned with deriving a model capable of reproducing the stochastic character of observed executions of a given process, stored in a log. This leads to an optimisation problem in which the model's parameter space is searched for, driven by the resemblance between the log's and the model's stochastic languages. The bottleneck of such optimisation problem lay in the determination of the model's stochastic language which existing approaches deal with through, hardly scalable, exact computation approaches. In this paper we introduce a novel framework in which we combine a simulation-based Bayesian parameter inference scheme, used to search for the ``optimal'' instance of a stochastic model, with an expressive statistical model checking engine, used (during inference) to approximate the language of the considered model's instance. Because of its simulation-based nature, the payoff is that, the runtime for discovering of the optimal instance of a model can be easily traded in for accuracy, hence allowing to treat large models which would result in a prohibitive runtime with non-simulation based alternatives. We validate our approach on several popular event logs concerning real-life systems.","authors":["Pierre Cry","Paolo Ballarini","Andr\\'as Horv\\'ath","Pascale Le Gall"],"url":"https://arxiv.org/abs/2504.21390"}
{"created":"2025-05-01","title":"Concurrency Testing in the Linux Kernel via eBPF","abstract":"Concurrency is vital for our critical software to meet modern performance requirements, yet concurrency bugs are notoriously difficult to detect and reproduce. Controlled Concurrency Testing (CCT) can make bugs easier to expose by enabling control over thread interleavings and systematically exploring the interleaving space through scheduling algorithms. However, existing CCT solutions for kernel code are heavyweight, leading to significant performance, maintainability and extensibility issues. In this work, we introduce LACE, a lightweight CCT framework for kernel code empowered by eBPF. Without hypervisor modification, LACE features a custom scheduler tailored for CCT algorithms to serialize non-determistic thread execution into a controlled ordering. LACE also provides a mechanism to safely inject scheduling points into the kernel for fine-grained control. Furthermore, LACE employs a two-phase mutation strategy to integrate the scheduler with a concurrency fuzzer, allowing for automated exploration of both the input and schedule space. In our evaluation, LACE achieves 38\\% more branches, 57\\% overhead reduction and 11.4$\\times$ speed-up in bug exposure compared to the state-of-the-art kernel concurrency fuzzers. Our qualitative analysis also demonstrates the extensibility and maintainability of LACE. Furthermore, LACE discovers eight previously unknown bugs in the Linux kernel, with six confirmed by developers.","authors":["Jiacheng Xu","Dylan Wolff","Xing Yi Han","Jialin Li","Abhik Roychoudhury"],"url":"https://arxiv.org/abs/2504.21394"}
{"created":"2025-05-01","title":"Coping with Uncertainty in UX Design Practice: Practitioner Strategies and Judgment","abstract":"The complexity of UX design practice extends beyond ill-structured design problems to include uncertainties shaped by shifting stakeholder priorities, team dynamics, limited resources, and implementation constraints. While prior research in related fields has addressed uncertainty in design more broadly, the specific character of uncertainty in UX practice remains underexplored. This study examines how UX practitioners experience and respond to uncertainty in real-world projects, drawing on a multi-week diary study and follow-up interviews with ten designers. We identify a range of practitioner strategies-including adaptive framing, negotiation, and judgment-that allow designers to move forward amid ambiguity. Our findings highlight the central role of design judgment in navigating uncertainty, including emergent forms such as temporal and sacrificial judgment, and extend prior understandings by showing how UX practitioners engage uncertainty as a persistent, situated feature of practice.","authors":["Prakash Shukla","Phuong Bui","Paul Parsons"],"url":"https://arxiv.org/abs/2504.21397"}
{"created":"2025-05-01","title":"In a Few Words: Comparing Weak Supervision and LLMs for Short Query Intent Classification","abstract":"User intent classification is an important task in information retrieval. Previously, user intents were classified manually and automatically; the latter helped to avoid hand labelling of large datasets. Recent studies explored whether LLMs can reliably determine user intent. However, researchers have recognized the limitations of using generative LLMs for classification tasks. In this study, we empirically compare user intent classification into informational, navigational, and transactional categories, using weak supervision and LLMs. Specifically, we evaluate LLaMA-3.1-8B-Instruct and LLaMA-3.1-70B-Instruct for in-context learning and LLaMA-3.1-8B-Instruct for fine-tuning, comparing their performance to an established baseline classifier trained using weak supervision (ORCAS-I). Our results indicate that while LLMs outperform weak supervision in recall, they continue to struggle with precision, which shows the need for improved methods to balance both metrics effectively.","authors":["Daria Alexander","Arjen P. de Vries"],"url":"https://arxiv.org/abs/2504.21398"}
{"created":"2025-05-01","title":"Static or Dynamic: Towards Query-Adaptive Token Selection for Video Question Answering","abstract":"Video question answering benefits from the rich information available in videos, enabling a wide range of applications. However, the large volume of tokens generated from longer videos presents significant challenges to memory efficiency and model performance. To alleviate this issue, existing works propose to compress video inputs, but usually overlooking the varying importance of static and dynamic information across different queries, leading to inefficient token usage within limited budgets. To tackle this, we propose a novel token selection strategy, EXPLORE-THEN-SELECT, that adaptively adjust static and dynamic information needed based on question requirements. Our framework first explores different token allocations between static frames, which preserve spatial details, and dynamic frames, which capture temporal changes. Next, it employs a query-aware attention-based metric to select the optimal token combination without model updates. Our proposed framework is plug-and-play that can be seamlessly integrated within diverse video-language models. Extensive experiments show that our method achieves significant performance improvements (up to 5.8%) among various video question answering benchmarks.","authors":["Yumeng Shi","Quanyu Long","Wenya Wang"],"url":"https://arxiv.org/abs/2504.21403"}
{"created":"2025-05-01","title":"Tolerating Disasters with Hierarchical Consensus","abstract":"Geo-replication provides disaster recovery after catastrophic accidental failures or attacks, such as fires, blackouts or denial-of-service attacks to a data center or region. Naturally distributed data structures, such as Blockchains, when well designed, are immune against such disruptions, but they also benefit from leveraging locality. In this work, we consolidate the performance of geo-replicated consensus by leveraging novel insights about hierarchical consensus and a construction methodology that allows creating novel protocols from existing building blocks. In particular we show that cluster confirmation, paired with subgroup rotation, allows protocols to safely operate through situations where all members of the global consensus group are Byzantine. We demonstrate our compositional construction by combining the recent HotStuff and Damysus protocols into a hierarchical geo-replicated blockchain with global durability guarantees. We present a compositionality proof and demonstrate the correctness of our protocol, including its ability to tolerate cluster crashes. Our protocol -ORION 1 -achieves a 20% higher throughput than GeoBFT, the latest hierarchical Byzantine Fault-Tolerant (BFT) protocol.","authors":["Wassim Yahyaoui (SnT)","Joachim Bruneau-Queyreix (LaBRI)","J\\'er\\'emie Decouchant (TU Delft)","Marcus V\\\"olp (SnT)"],"url":"https://arxiv.org/abs/2504.21410"}
{"created":"2025-05-01","title":"Galvatron: An Automatic Distributed System for Efficient Foundation Model Training","abstract":"Galvatron is a distributed system for efficiently training large-scale Foundation Models. It overcomes the complexities of selecting optimal parallelism strategies by automatically identifying the most efficient hybrid strategy, incorporating data, tensor, pipeline, sharded data, and sequence parallelism, along with recomputation. The system's architecture includes a profiler for hardware and model analysis, a search engine for strategy optimization using decision trees and dynamic programming, and a runtime for executing these strategies efficiently. Benchmarking on various clusters demonstrates Galvatron's superior throughput compared to existing frameworks. This open-source system offers user-friendly interfaces and comprehensive documentation, making complex distributed training accessible and efficient. The source code of Galvatron is available at https://github.com/PKU-DAIR/Hetu-Galvatron.","authors":["Xinyi Liu","Yujie Wang","Shenhan Zhu","Fangcheng Fu","Qingshuo Liu","Guangming Lin","Bin Cui"],"url":"https://arxiv.org/abs/2504.21411"}
{"created":"2025-05-01","title":"On the Encapsulation of Medical Imaging AI Algorithms","abstract":"In the context of collaborative AI research and development projects, it would be ideal to have self-contained encapsulated algorithms that can be easily shared between different parties, executed and validated on data at different sites, or trained in a federated manner. In practice, all of this is possible but greatly complicated, because human supervision and expert knowledge is needed to set up the execution of algorithms based on their documentation, possibly implicit assumptions, and knowledge about the execution environment and data involved.","authors":["Hans Meine","Yongli Mou","Guido Prause","Horst Hahn"],"url":"https://arxiv.org/abs/2504.21412"}
{"created":"2025-05-01","title":"An Inversion Theorem for Buffered Linear Toeplitz (BLT) Matrices and Applications to Streaming Differential Privacy","abstract":"Buffered Linear Toeplitz (BLT) matrices are a family of parameterized lower-triangular matrices that play an important role in streaming differential privacy with correlated noise. Our main result is a BLT inversion theorem: the inverse of a BLT matrix is itself a BLT matrix with different parameters. We also present an efficient and differentiable $O(d^3)$ algorithm to compute the parameters of the inverse BLT matrix, where $d$ is the degree of the original BLT (typically $d < 10$). Our characterization enables direct optimization of BLT parameters for privacy mechanisms through automatic differentiation.","authors":["H. Brendan McMahan","Krishna Pillutla"],"url":"https://arxiv.org/abs/2504.21413"}
{"created":"2025-05-01","title":"Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining","abstract":"Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.","authors":["Qi Fan","Kaiqi Liu","Nian Liu","Hisham Cholakkal","Rao Muhammad Anwer","Wenbin Li","Yang Gao"],"url":"https://arxiv.org/abs/2504.21414"}
{"created":"2025-05-01","title":"Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges","abstract":"User authentication is essential to ensure secure access to computer systems, yet traditional methods face limitations in usability, cost, and security. Mouse dynamics authentication, based on the analysis of users' natural interaction behaviors with mouse devices, offers a cost-effective, non-intrusive, and adaptable solution. However, challenges remain in determining the optimal data volume, balancing accuracy and practicality, and effectively capturing temporal behavioral patterns. In this study, we propose a statistical method using Gaussian kernel density estimate (KDE) and Kullback-Leibler (KL) divergence to estimate the sufficient data volume for training authentication models. We introduce the Mouse Authentication Unit (MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for efficient and accurate behavioral representation. Furthermore, we design the Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet for local feature extraction and GRU for modeling long-term temporal dependencies. Taking the Balabit and DFL datasets as examples, we significantly reduced the data scale, particularly by a factor of 10 for the DFL dataset, greatly alleviating the training burden. Additionally, we determined the optimal input recognition unit length for the user authentication system on different datasets based on the slope of Approximate Entropy. Training with imbalanced samples, our model achieved a successful defense AUC 98.52% for blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing the current sota performance.","authors":["Yi Wang","Chengyv Wu","Yang Liao","Maowei You"],"url":"https://arxiv.org/abs/2504.21415"}
{"created":"2025-05-01","title":"A Test Suite for Efficient Robustness Evaluation of Face Recognition Systems","abstract":"Face recognition is a widely used authentication technology in practice, where robustness is required. It is thus essential to have an efficient and easy-to-use method for evaluating the robustness of (possibly third-party) trained face recognition systems. Existing approaches to evaluating the robustness of face recognition systems are either based on empirical evaluation (e.g., measuring attacking success rate using state-of-the-art attacking methods) or formal analysis (e.g., measuring the Lipschitz constant). While the former demands significant user efforts and expertise, the latter is extremely time-consuming. In pursuit of a comprehensive, efficient, easy-to-use and scalable estimation of the robustness of face recognition systems, we take an old-school alternative approach and introduce RobFace, i.e., evaluation using an optimised test suite. It contains transferable adversarial face images that are designed to comprehensively evaluate a face recognition system's robustness along a variety of dimensions. RobFace is system-agnostic and still consistent with system-specific empirical evaluation or formal analysis. We support this claim through extensive experimental results with various perturbations on multiple face recognition systems. To our knowledge, RobFace is the first system-agnostic robustness estimation test suite.","authors":["Ruihan Zhang","Jun Sun"],"url":"https://arxiv.org/abs/2504.21420"}
{"created":"2025-05-01","title":"The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors","abstract":"To explore the relationship between dependency distance (DD) and hierarchical distance (HD) in Japanese, we compared the probability distributions of DD and HD with and without sentence length fixed, and analyzed the changes in mean dependency distance (MDD) and mean hierarchical distance (MHD) as sentence length increases, along with their correlation coefficient based on the Balanced Corpus of Contemporary Written Japanese. It was found that the valency of the predicates is the underlying factor behind the trade-off relation between MDD and MHD in Japanese. Native speakers of Japanese regulate the linear complexity and hierarchical complexity through the valency of the predicates, and the relative sizes of MDD and MHD depend on whether the threshold of valency has been reached. Apart from the cognitive load, the valency of the predicates also affects the probability distributions of DD and HD. The effect of the valency of the predicates on the distribution of HD is greater than on that of DD, which leads to differences in their probability distributions and causes the mean of MDD to be lower than that of MHD.","authors":["Linxuan Wang","Shuiyuan Yu"],"url":"https://arxiv.org/abs/2504.21421"}
{"created":"2025-05-01","title":"Diff-Prompt: Diffusion-Driven Prompt Generator with Mask Supervision","abstract":"Prompt learning has demonstrated promising results in fine-tuning pre-trained multimodal models. However, the performance improvement is limited when applied to more complex and fine-grained tasks. The reason is that most existing methods directly optimize the parameters involved in the prompt generation process through loss backpropagation, which constrains the richness and specificity of the prompt representations. In this paper, we propose Diffusion-Driven Prompt Generator (Diff-Prompt), aiming to use the diffusion model to generate rich and fine-grained prompt information for complex downstream tasks. Specifically, our approach consists of three stages. In the first stage, we train a Mask-VAE to compress the masks into latent space. In the second stage, we leverage an improved Diffusion Transformer (DiT) to train a prompt generator in the latent space, using the masks for supervision. In the third stage, we align the denoising process of the prompt generator with the pre-trained model in the semantic space, and use the generated prompts to fine-tune the model. We conduct experiments on a complex pixel-level downstream task, referring expression comprehension, and compare our method with various parameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum improvement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model and also outperforms other state-of-the-art methods across multiple metrics. The experimental results validate the effectiveness of our approach and highlight the potential of using generative models for prompt generation. Code is available at https://github.com/Kelvin-ywc/diff-prompt.","authors":["Weicai Yan","Wang Lin","Zirun Guo","Ye Wang","Fangming Feng","Xiaoda Yang","Zehan Wang","Tao Jin"],"url":"https://arxiv.org/abs/2504.21423"}
{"created":"2025-05-01","title":"MPEC: Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers","abstract":"Accurate classification of EEG signals is crucial for brain-computer interfaces (BCIs) and neuroprosthetic applications, yet many existing methods fail to account for the non-Euclidean, manifold structure of EEG data, resulting in suboptimal performance. Preserving this manifold information is essential to capture the true geometry of EEG signals, but traditional classification techniques largely overlook this need. To this end, we propose MPEC (Manifold-Preserved EEG Classification via an Ensemble of Clustering-Based Classifiers), that introduces two key innovations: (1) a feature engineering phase that combines covariance matrices and Radial Basis Function (RBF) kernels to capture both linear and non-linear relationships among EEG channels, and (2) a clustering phase that employs a modified K-means algorithm tailored for the Riemannian manifold space, ensuring local geometric sensitivity. Ensembling multiple clustering-based classifiers, MPEC achieves superior results, validated by significant improvements on the BCI Competition IV dataset 2a.","authors":["Shermin Shahbazi","Mohammad-Reza Nasiri","Majid Ramezani"],"url":"https://arxiv.org/abs/2504.21427"}
{"created":"2025-05-01","title":"UAV Marketplace Simulation Tool for BVLOS Operations","abstract":"We present a simulation tool for evaluating team formation in autonomous multi-UAV (Unmanned Aerial Vehicle) missions that operate Beyond Visual Line of Sight (BVLOS). The tool models UAV collaboration and mission execution in dynamic and adversarial conditions, where Byzantine UAVs attempt to disrupt operations. Our tool allows researchers to integrate and compare various team formation strategies in a controlled environment with configurable mission parameters and adversarial behaviors. The log of each simulation run is stored in a structured way along with performance metrics so that statistical analysis could be done straightforwardly. The tool is versatile for testing and improving UAV coordination strategies in real-world applications.","authors":["K{\\i}van\\c{c} \\c{S}erefo\\u{g}lu","\\\"Onder G\\\"urcan","Reyhan Aydo\\u{g}an"],"url":"https://arxiv.org/abs/2504.21428"}
{"created":"2025-05-01","title":"Active learning of upward-closed sets of words","abstract":"We give a new proof of a result from well quasi-order theory on the computability of bases for upwards-closed sets of words. This new proof is based on Angluin's $L^*$ algorithm, that learns an automaton from a minimally adequate teacher. This relates in particular two results from the 1980s: Angluin's $L^*$ algorithm, and a result from Valk and Jantzen on the computability of bases for upwards-closed sets of tuples of integers. Along the way, we describe an algorithm for learning quasi-ordered automata from a minimally adequate teacher, and extend a generalization of Valk and Jantzen's result, encompassing both words and integers, to finitely generated monoids.","authors":["Quentin Aristote (UPCit\\'e","IRIF","PICUBE)"],"url":"https://arxiv.org/abs/2504.21429"}
{"created":"2025-05-01","title":"UAV-VLN: End-to-End Vision Language guided Navigation for UAVs","abstract":"A core challenge in AI-guided autonomy is enabling agents to navigate realistically and effectively in previously unseen environments based on natural language commands. We propose UAV-VLN, a novel end-to-end Vision-Language Navigation (VLN) framework for Unmanned Aerial Vehicles (UAVs) that seamlessly integrates Large Language Models (LLMs) with visual perception to facilitate human-interactive navigation. Our system interprets free-form natural language instructions, grounds them into visual observations, and plans feasible aerial trajectories in diverse environments.","authors":["Pranav Saxena","Nishant Raghuvanshi","Neena Goveas"],"url":"https://arxiv.org/abs/2504.21432"}
{"created":"2025-05-01","title":"NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities to Achieve Artificial General Intelligence","abstract":"This paper argues that the next generation of AI agent (NGENT) should integrate across-domain abilities to advance toward Artificial General Intelligence (AGI). Although current AI agents are effective in specialized tasks such as robotics, role-playing, and tool-using, they remain confined to narrow domains. We propose that future AI agents should synthesize the strengths of these specialized systems into a unified framework capable of operating across text, vision, robotics, reinforcement learning, emotional intelligence, and beyond. This integration is not only feasible but also essential for achieving the versatility and adaptability that characterize human intelligence. The convergence of technologies across AI domains, coupled with increasing user demand for cross-domain capabilities, suggests that such integration is within reach. Ultimately, the development of these versatile agents is a critical step toward realizing AGI. This paper explores the rationale for this shift, potential pathways for achieving it.","authors":["Zhicong Li","Hangyu Mao","Jiangjin Yin","Mingzhe Xing","Zhiwei Xu","Yuanxing Zhang","Yang Xiao"],"url":"https://arxiv.org/abs/2504.21433"}
{"created":"2025-05-01","title":"SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding","abstract":"With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on \\textbf{standalone} videos and mainly assess ``visual elements'' like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a \\textbf{series}. To address this challenge, we propose \\textbf{SeriesBench}, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, \\textbf{PC-DCoT}. Extensive results on \\textbf{SeriesBench} indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while \\textbf{PC-DCoT} enables these MLLMs to achieve performance improvements. Overall, our \\textbf{SeriesBench} and \\textbf{PC-DCoT} highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.","authors":["Chenkai Zhang","Yiming Lei","Zeming Liu","Haitao Leng","ShaoGuo Liu","Tingting Gao","Qingjie Liu","Yunhong Wang"],"url":"https://arxiv.org/abs/2504.21435"}
{"created":"2025-05-01","title":"Whispers of Data: Unveiling Label Distributions in Federated Learning Through Virtual Client Simulation","abstract":"Federated Learning enables collaborative training of a global model across multiple geographically dispersed clients without the need for data sharing. However, it is susceptible to inference attacks, particularly label inference attacks.","authors":["Zhixuan Ma","Haichang Gao","Junxiang Huang","Ping Wang"],"url":"https://arxiv.org/abs/2504.21436"}
{"created":"2025-05-01","title":"Identifying Critical Dependencies in Large-Scale Continuous Software Engineering","abstract":"Continuous Software Engineering (CSE) is widely adopted in the industry, integrating practices such as Continuous Integration and Continuous Deployment (CI/CD). Beyond technical aspects, CSE also encompasses business activities like continuous planning, budgeting, and operational processes. Coordinating these activities in large-scale product development involves multiple stakeholders, increasing complexity. This study aims to address this complexity by identifying and analyzing critical dependencies in large-scale CSE. Based on 17 semi-structured interviews conducted at two Nordic fintech companies, our preliminary findings indicate that dependencies between software teams and support functions, as well as between software teams and external entities, are the primary sources of delays and bottlenecks. As a next step, we plan to further refine our understanding of critical dependencies in large-scale CSE and explore coordination mechanisms that can better support software development teams in managing these challenges.","authors":["Anastasiia Tkalich","Eriks Klotins","Nils Brede Moe"],"url":"https://arxiv.org/abs/2504.21437"}
{"created":"2025-05-01","title":"Stability of Open Multi-agent Systems over Dynamic Signed Graphs","abstract":"This paper addresses the bipartite consensus-control problem in open multi-agent systems containing both cooperative and antagonistic interactions. In these systems, new agents can join and new interactions can be formed over time. Moreover, the types of interactions, cooperative or antagonistic, may change. To model these structural changes, we represent the system as a switched system interconnected over a dynamic signed graph. Using the signed edge-based agreement protocol and constructing strict Lyapunov functions for signed edge-Laplacian matrices with multiple zero eigenvalues, we establish global asymptotic stability of the bipartite consensus control. Numerical simulations validate our theoretical results.","authors":["Pelin Sekercioglu","Angela Fontan","Dimos V. Dimarogonas"],"url":"https://arxiv.org/abs/2504.21443"}
{"created":"2025-05-01","title":"A Unified QoS-Aware Multiplexing Framework for Next Generation Immersive Communication with Legacy Wireless Applications","abstract":"Immersive communication, including emerging augmented reality, virtual reality, and holographic telepresence, has been identified as a key service for enabling next-generation wireless applications. To align with legacy wireless applications, such as enhanced mobile broadband or ultra-reliable low-latency communication, network slicing has been widely adopted. However, attempting to statistically isolate the above types of wireless applications through different network slices may lead to throughput degradation and increased queue backlog. To address these challenges, we establish a unified QoS-aware framework that supports immersive communication and legacy wireless applications simultaneously. Based on the Lyapunov drift theorem, we transform the original long-term throughput maximization problem into an equivalent short-term throughput maximization weighted by virtual queue length. Moreover, to cope with the challenges introduced by the interaction between large-timescale network slicing and short-timescale resource allocation, we propose an adaptive adversarial slicing (Ad2S) scheme for networks with invarying channel statistics. To track the network channel variations, we also propose a measurement extrapolation-Kalman filter (ME-KF)-based method and refine our scheme into Ad2S-non-stationary refinement (Ad2S-NR). Through extended numerical examples, we demonstrate that our proposed schemes achieve 3.86 Mbps throughput improvement and 63.96\\% latency reduction with 24.36\\% convergence time reduction. Within our framework, the trade-off between total throughput and user service experience can be achieved by tuning systematic parameters.","authors":["Jihong Li","Shunqing Zhang","Tao Yu","Guangjin Pan","Kaixuan Huang","Xiaojing Chen","Yanzan Sun","Junyu Liu","Jiandong Li","Derrick Wing Kwan Ng"],"url":"https://arxiv.org/abs/2504.21444"}
{"created":"2025-05-01","title":"Rethinking Visual Layer Selection in Multimodal LLMs","abstract":"Multimodal large language models (MLLMs) have achieved impressive performance across a wide range of tasks, typically using CLIP-ViT as their visual encoder due to its strong text-image alignment capabilities. While prior studies suggest that different CLIP-ViT layers capture different types of information, with shallower layers focusing on fine visual details and deeper layers aligning more closely with textual semantics, most MLLMs still select visual features based on empirical heuristics rather than systematic analysis. In this work, we propose a Layer-wise Representation Similarity approach to group CLIP-ViT layers with similar behaviors into {shallow, middle, and deep} categories and assess their impact on MLLM performance. Building on this foundation, we revisit the visual layer selection problem in MLLMs at scale, training LLaVA-style models ranging from 1.4B to 7B parameters. Through extensive experiments across 10 datasets and 4 tasks, we find that: (1) deep layers are essential for OCR tasks; (2) shallow and middle layers substantially outperform deep layers on reasoning tasks involving counting, positioning, and object localization; (3) a lightweight fusion of features across shallow, middle, and deep layers consistently outperforms specialized fusion baselines and single-layer selections, achieving gains on 9 out of 10 datasets. Our work offers the first principled study of visual layer selection in MLLMs, laying the groundwork for deeper investigations into visual representation learning for MLLMs.","authors":["Haoran Chen","Junyan Lin","Xinhao Chen","Yue Fan","Xin Jin","Hui Su","Jianfeng Dong","Jinlan Fu","Xiaoyu Shen"],"url":"https://arxiv.org/abs/2504.21447"}
{"created":"2025-05-01","title":"On phase in scaled graphs","abstract":"The scaled graph has been introduced recently as a nonlinear extension of the classical Nyquist plot for linear time-invariant systems. In this paper, we introduce a modified definition for the scaled graph, termed the signed scaled graph (SSG), in which the phase component is characterized by making use of the Hilbert transform. Whereas the original definition of the scaled graph uses unsigned phase angles, the new definition has signed phase angles which ensures the possibility to differentiate between phase-lead and phase-lag properties in a system. Making such distinction is important from both an analysis and a synthesis perspective, and helps in providing tighter stability estimates of feedback interconnections. We show how the proposed SSG leads to intuitive characterizations of positive real and negative imaginary nonlinear systems, and present various interconnection results. We showcase the effectiveness of our results through several motivating examples.","authors":["Sebastiaan van den Eijnden","Chao Chen","Koen Scheres","Thomas Chaffey","Alexander Lanzon"],"url":"https://arxiv.org/abs/2504.21448"}
{"created":"2025-05-01","title":"SimPRIVE: a Simulation framework for Physical Robot Interaction with Virtual Environments","abstract":"The use of machine learning in cyber-physical systems has attracted the interest of both industry and academia. However, no general solution has yet been found against the unpredictable behavior of neural networks and reinforcement learning agents. Nevertheless, the improvements of photo-realistic simulators have paved the way towards extensive testing of complex algorithms in different virtual scenarios, which would be expensive and dangerous to implement in the real world.","authors":["Federico Nesti","Gianluca D'Amico","Mauro Marinoni","Giorgio Buttazzo"],"url":"https://arxiv.org/abs/2504.21454"}
{"created":"2025-05-01","title":"xEEGNet: Towards Explainable AI in EEG Dementia Classification","abstract":"This work presents xEEGNet, a novel, compact, and explainable neural network for EEG data analysis. It is fully interpretable and reduces overfitting through major parameter reduction. As an applicative use case, we focused on classifying common dementia conditions, Alzheimer's and frontotemporal dementia, versus controls. xEEGNet is broadly applicable to other neurological conditions involving spectral alterations. We initially used ShallowNet, a simple and popular model from the EEGNet-family. Its structure was analyzed and gradually modified to move from a \"black box\" to a more transparent model, without compromising performance. The learned kernels and weights were examined from a clinical standpoint to assess medical relevance. Model variants, including ShallowNet and the final xEEGNet, were evaluated using robust Nested-Leave-N-Subjects-Out cross-validation for unbiased performance estimates. Variability across data splits was explained using embedded EEG representations, grouped by class and set, with pairwise separability to quantify group distinction. Overfitting was assessed through training-validation loss correlation and training speed. xEEGNet uses only 168 parameters, 200 times fewer than ShallowNet, yet retains interpretability, resists overfitting, achieves comparable median performance (-1.5%), and reduces variability across splits. This variability is explained by embedded EEG representations: higher accuracy correlates with greater separation between test set controls and Alzheimer's cases, without significant influence from training data. xEEGNet's ability to filter specific EEG bands, learn band-specific topographies, and use relevant spectral features demonstrates its interpretability. While large deep learning models are often prioritized for performance, this study shows smaller architectures like xEEGNet can be equally effective in EEG pathology classification.","authors":["Andrea Zanola","Louis Fabrice Tshimanga","Federico Del Pup","Marco Baiesi","Manfredo Atzori"],"url":"https://arxiv.org/abs/2504.21457"}
{"created":"2025-05-01","title":"An Intermediate Program Representation for Optimizing Stream-Based Languages","abstract":"Stream-based runtime monitors are safety assurance tools that check at runtime whether the system's behavior satisfies a formal specification. Specifications consist of stream equations, which relate input streams, containing sensor readings and other incoming information, to output streams, representing filtered and aggregated data. This paper presents a framework for the stream-based specification language RTLola. We introduce a new intermediate representation for stream-based languages, the StreamIR, which, like the specification language, operates on streams of unbounded length; while the stream equations are replaced by imperative programs. We developed a set of optimizations based on static analysis of the specification and have implemented an interpreter and a compiler for several target languages. In our evaluation, we measure the performance of several real-world case studies. The results show that using the StreamIR framework reduces the runtime significantly compared to the existing StreamIR interpreter. We evaluate the effect of the optimizations and show that significant performance gains are possible beyond the optimizations of the target language's compiler. While our current implementation is limited to RTLola, the StreamIR is designed to accommodate other stream-based languages, enabling their interpretation and compilation into all available target languages.","authors":["Jan Baumeister","Arthur Correnson","Bernd Finkbeiner","Frederik Scheerer"],"url":"https://arxiv.org/abs/2504.21458"}
{"created":"2025-05-01","title":"RWKV-X: A Linear Complexity Hybrid Language Model","abstract":"In this paper, we introduce \\textbf{RWKV-X}, a novel hybrid architecture that combines the efficiency of RWKV for short-range modeling with a sparse attention mechanism designed to capture long-range context. Unlike previous hybrid approaches that rely on full attention layers and retain quadratic complexity, RWKV-X achieves linear-time complexity in training and constant-time complexity in inference decoding. We demonstrate that RWKV-X, when continually pretrained on 64K-token sequences, achieves near-perfect accuracy on the 64K passkey retrieval benchmark. It consistently outperforms prior RWKV-7 models on long-context benchmarks, while maintaining strong performance on short-context tasks. These results highlight RWKV-X as a scalable and efficient backbone for general-purpose language modeling, capable of decoding sequences up to 1 million tokens with stable speed and memory usage. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at: https://github.com/howard-hou/RWKV-X.","authors":["Haowen Hou","Zhiyi Huang","Kaifeng Tan","Rongchang Lu","Fei Richard Yu"],"url":"https://arxiv.org/abs/2504.21463"}
{"created":"2025-05-01","title":"VR-FuseNet: A Fusion of Heterogeneous Fundus Data and Explainable Deep Network for Diabetic Retinopathy Classification","abstract":"Diabetic retinopathy is a severe eye condition caused by diabetes where the retinal blood vessels get damaged and can lead to vision loss and blindness if not treated. Early and accurate detection is key to intervention and stopping the disease progressing. For addressing this disease properly, this paper presents a comprehensive approach for automated diabetic retinopathy detection by proposing a new hybrid deep learning model called VR-FuseNet. Diabetic retinopathy is a major eye disease and leading cause of blindness especially among diabetic patients so accurate and efficient automated detection methods are required. To address the limitations of existing methods including dataset imbalance, diversity and generalization issues this paper presents a hybrid dataset created from five publicly available diabetic retinopathy datasets. Essential preprocessing techniques such as SMOTE for class balancing and CLAHE for image enhancement are applied systematically to the dataset to improve the robustness and generalizability of the dataset. The proposed VR-FuseNet model combines the strengths of two state-of-the-art convolutional neural networks, VGG19 which captures fine-grained spatial features and ResNet50V2 which is known for its deep hierarchical feature extraction. This fusion improves the diagnostic performance and achieves an accuracy of 91.824%. The model outperforms individual architectures on all performance metrics demonstrating the effectiveness of hybrid feature extraction in Diabetic Retinopathy classification tasks. To make the proposed model more clinically useful and interpretable this paper incorporates multiple XAI techniques. These techniques generate visual explanations that clearly indicate the retinal features affecting the model's prediction such as microaneurysms, hemorrhages and exudates so that clinicians can interpret and validate.","authors":["Shamim Rahim Refat","Ziyan Shirin Raha","Shuvashis Sarker","Faika Fairuj Preotee","MD. Musfikur Rahman","Tashreef Muhammad","Mohammad Shafiul Islam"],"url":"https://arxiv.org/abs/2504.21464"}
{"created":"2025-05-01","title":"Semantic-aided Parallel Image Transmission Compatible with Practical System","abstract":"In this paper, we propose a novel semantic-aided image communication framework for supporting the compatibility with practical separation-based coding architectures. Particularly, the deep learning (DL)-based joint source-channel coding (JSCC) is integrated into the classical separate source-channel coding (SSCC) to transmit the images via the combination of semantic stream and image stream from DL networks and SSCC respectively, which we name as parallel-stream transmission. The positive coding gain stems from the sophisticated design of the JSCC encoder, which leverages the residual information neglected by the SSCC to enhance the learnable image features. Furthermore, a conditional rate adaptation mechanism is introduced to adjust the transmission rate of semantic stream according to residual, rendering the framework more flexible and efficient to bandwidth allocation. We also design a dynamic stream aggregation strategy at the receiver, which provides the composite framework with more robustness to signal-to-noise ratio (SNR) fluctuations in wireless systems compared to a single conventional codec. Finally, the proposed framework is verified to surpass the performance of both traditional and DL-based competitors in a large range of scenarios and meanwhile, maintains lightweight in terms of the transmission and computational complexity of semantic stream, which exhibits the potential to be applied in real systems.","authors":["Mingkai Xu","Yongpeng Wu","Yuxuan Shi","Xiang-Gen Xia","Merouane Debbah","Wenjun Zhang","Ping Zhang"],"url":"https://arxiv.org/abs/2504.21466"}
{"created":"2025-05-01","title":"Multiview Point Cloud Registration via Optimization in an Autoencoder Latent Space","abstract":"Point cloud rigid registration is a fundamental problem in 3D computer vision. In the multiview case, we aim to find a set of 6D poses to align a set of objects. Methods based on pairwise registration rely on a subsequent synchronization algorithm, which makes them poorly scalable with the number of views. Generative approaches overcome this limitation, but are based on Gaussian Mixture Models and use an Expectation-Maximization algorithm. Hence, they are not well suited to handle large transformations. Moreover, most existing methods cannot handle high levels of degradations. In this paper, we introduce POLAR (POint cloud LAtent Registration), a multiview registration method able to efficiently deal with a large number of views, while being robust to a high level of degradations and large initial angles. To achieve this, we transpose the registration problem into the latent space of a pretrained autoencoder, design a loss taking degradations into account, and develop an efficient multistart optimization strategy. Our proposed method significantly outperforms state-of-the-art approaches on synthetic and real data. POLAR is available at github.com/pypolar/polar or as a standalone package which can be installed with pip install polaregistration.","authors":["Luc Vedrenne","Sylvain Faisan","Denis Fortun"],"url":"https://arxiv.org/abs/2504.21467"}
{"created":"2025-05-01","title":"Quaternion Nuclear Norms Over Frobenius Norms Minimization for Robust Matrix Completion","abstract":"Recovering hidden structures from incomplete or noisy data remains a pervasive challenge across many fields, particularly where multi-dimensional data representation is essential. Quaternion matrices, with their ability to naturally model multi-dimensional data, offer a promising framework for this problem. This paper introduces the quaternion nuclear norm over the Frobenius norm (QNOF) as a novel nonconvex approximation for the rank of quaternion matrices. QNOF is parameter-free and scale-invariant. Utilizing quaternion singular value decomposition, we prove that solving the QNOF can be simplified to solving the singular value $L_1/L_2$ problem. Additionally, we extend the QNOF to robust quaternion matrix completion, employing the alternating direction multiplier method to derive solutions that guarantee weak convergence under mild conditions. Extensive numerical experiments validate the proposed model's superiority, consistently outperforming state-of-the-art quaternion methods.","authors":["Yu Guo","Guoqing Chen","Tieyong Zeng","Qiyu Jin","Michael Kwok-Po Ng"],"url":"https://arxiv.org/abs/2504.21468"}
{"created":"2025-05-01","title":"Efficiently Finding All Minimal and Shortest Absent Subsequences in a String","abstract":"Given a string $w$, another string $v$ is said to be a subsequence of $w$ if $v$ can be obtained from $w$ by removing some of its letters; on the other hand, $v$ is called an absent subsequence of $w$ if $v$ is not a subsequence of $w$. The existing literature on absent subsequences focused on understanding, for a string $w$, the set of its shortest absent subsequences (i.e., the shortest strings which are absent subsequences of $w$) and that of its minimal absent subsequences (i.e., those strings which are absent subsequences of $w$ but whose every proper subsequence occurs in $w$). Our contributions to this area of research are the following. Firstly, we present optimal algorithms (with linear time preprocessing and output-linear delay) for the enumeration of the shortest and, respectively, minimal absent subsequences. Secondly, we present optimal algorithms for the incremental enumeration of these strings with linear time preprocessing and constant delay; in this setting, we only output short edit-scripts showing how the currently enumerated string differs from the previous one. Finally, we provide an efficient algorithm for identifying a longest minimal absent subsequence of a string. All our algorithms improve the state-of-the-art results for the aforementioned problems.","authors":["Florin Manea","Tina Ringleb","Stefan Siemer","Maximilian Winkler"],"url":"https://arxiv.org/abs/2504.21471"}
{"created":"2025-05-01","title":"Robust Orthogonal NMF with Label Propagation for Image Clustering","abstract":"Non-negative matrix factorization (NMF) is a popular unsupervised learning approach widely used in image clustering. However, in real-world clustering scenarios, most existing NMF methods are highly sensitive to noise corruption and are unable to effectively leverage limited supervised information. To overcome these drawbacks, we propose a unified non-convex framework with label propagation called robust orthogonal nonnegative matrix factorization (RONMF). This method not only considers the graph Laplacian and label propagation as regularization terms but also introduces a more effective non-convex structure to measure the reconstruction error and imposes orthogonal constraints on the basis matrix to reduce the noise corruption, thereby achieving higher robustness. To solve RONMF, we develop an alternating direction method of multipliers (ADMM)-based optimization algorithm. In particular, all subproblems have closed-form solutions, which ensures its efficiency. Experimental evaluations on eight public image datasets demonstrate that the proposed RONMF outperforms state-of-the-art NMF methods across various standard metrics and shows excellent robustness. The code will be available at https://github.com/slinda-liu.","authors":["Jingjing Liu","Nian Wu","Xianchao Xiu","Jianhua Zhang"],"url":"https://arxiv.org/abs/2504.21472"}
{"created":"2025-05-01","title":"Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging","abstract":"This paper presents our system, Homa, for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.","authors":["Hadi Bayrami Asl Tekanlou","Jafar Razmara","Mahsa Sanaei","Mostafa Rahgouy","Hamed Babaei Giglou"],"url":"https://arxiv.org/abs/2504.21474"}
{"created":"2025-05-01","title":"Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines","abstract":"This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.","authors":["Serry Sibaee","Samar Ahmed","Abdullah Al Harbi","Omer Nacar","Adel Ammar","Yasser Habashi","Wadii Boulila"],"url":"https://arxiv.org/abs/2504.21475"}
{"created":"2025-05-01","title":"GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers","abstract":"Garment sewing patterns are fundamental design elements that bridge the gap between design concepts and practical manufacturing. The generative modeling of sewing patterns is crucial for creating diversified garments. However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. In this work, we present \\textbf{\\textit{GarmentDiffusion}}, a new generative model capable of producing centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text, image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, achieving a sequence length that is $\\textbf{10}\\times$ shorter than that of the autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics. With all combination of designs of our model, the sewing pattern generation speed is accelerated by $\\textbf{100}\\times$ compared to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well as on the largest sewing pattern dataset, namely GarmentCodeData. The project website is available at https://shenfu-research.github.io/Garment-Diffusion/.","authors":["Xinyu Li","Qi Yao","Yuanda Wang"],"url":"https://arxiv.org/abs/2504.21476"}
{"created":"2025-05-01","title":"A Comprehensive Survey of Electrical Stimulation Haptic Feedback in Human-Computer Interaction","abstract":"Haptic perception and feedback play a pivotal role in interactive experiences, forming an essential component of human-computer interaction (HCI). In recent years, the field of haptic interaction has witnessed significant advancements, particularly in the area of electrical haptic feedback, driving innovation across various domains. To gain a comprehensive understanding of the current state of research and the latest developments in electrical haptic interaction, this study systematically reviews the literature in this area. Our investigation covers key aspects including haptic devices, haptic perception mechanisms, the comparison and integration of electrical haptic feedback with other feedback modalities, and their diverse applications. Specifically, we conduct a systematic analysis of 110 research papers to explore the forefront of electrical haptic feedback, providing insights into its latest trends, challenges, and future directions.","authors":["Simin Yang","Xian Wang","Yang Li","Lik-Hang Lee","Tristan Camille","Pan Hui"],"url":"https://arxiv.org/abs/2504.21477"}
{"created":"2025-05-01","title":"CAE-DFKD: Bridging the Transferability Gap in Data-Free Knowledge Distillation","abstract":"Data-Free Knowledge Distillation (DFKD) enables the knowledge transfer from the given pre-trained teacher network to the target student model without access to the real training data. Existing DFKD methods focus primarily on improving image recognition performance on associated datasets, often neglecting the crucial aspect of the transferability of learned representations. In this paper, we propose Category-Aware Embedding Data-Free Knowledge Distillation (CAE-DFKD), which addresses at the embedding level the limitations of previous rely on image-level methods to improve model generalization but fail when directly applied to DFKD. The superiority and flexibility of CAE-DFKD are extensively evaluated, including: \\textit{\\textbf{i.)}} Significant efficiency advantages resulting from altering the generator training paradigm; \\textit{\\textbf{ii.)}} Competitive performance with existing DFKD state-of-the-art methods on image recognition tasks; \\textit{\\textbf{iii.)}} Remarkable transferability of data-free learned representations demonstrated in downstream tasks.","authors":["Zherui Zhang","Changwei Wang","Rongtao Xu","Wenhao Xu","Shibiao Xu","Yu Zhang","Li Guo"],"url":"https://arxiv.org/abs/2504.21478"}
{"created":"2025-05-01","title":"A Comprehensive Study of Exploitable Patterns in Smart Contracts: From Vulnerability to Defense","abstract":"With the rapid advancement of blockchain technology, smart contracts have enabled the implementation of increasingly complex functionalities. However, ensuring the security of smart contracts remains a persistent challenge across the stages of development, compilation, and execution. Vulnerabilities within smart contracts not only undermine the security of individual applications but also pose significant risks to the broader blockchain ecosystem, as demonstrated by the growing frequency of attacks since 2016, resulting in substantial financial losses. This paper provides a comprehensive analysis of key security risks in Ethereum smart contracts, specifically those written in Solidity and executed on the Ethereum Virtual Machine (EVM). We focus on two prevalent and critical vulnerability types (reentrancy and integer overflow) by examining their underlying mechanisms, replicating attack scenarios, and assessing effective countermeasures.","authors":["Yuchen Ding","Hongli Peng","Xiaoqi Li"],"url":"https://arxiv.org/abs/2504.21480"}
{"created":"2025-05-01","title":"Provably-Safe, Online System Identification","abstract":"Precise manipulation tasks require accurate knowledge of payload inertial parameters. Unfortunately, identifying these parameters for unknown payloads while ensuring that the robotic system satisfies its input and state constraints while avoiding collisions with the environment remains a significant challenge. This paper presents an integrated framework that enables robotic manipulators to safely and automatically identify payload parameters while maintaining operational safety guarantees. The framework consists of two synergistic components: an online trajectory planning and control framework that generates provably-safe exciting trajectories for system identification that can be tracked while respecting robot constraints and avoiding obstacles and a robust system identification method that computes rigorous overapproximative bounds on end-effector inertial parameters assuming bounded sensor noise. Experimental validation on a robotic manipulator performing challenging tasks with various unknown payloads demonstrates the framework's effectiveness in establishing accurate parameter bounds while maintaining safety throughout the identification process. The code is available at our project webpage: https://roahmlab.github.io/OnlineSafeSysID/.","authors":["Bohao Zhang","Zichang Zhou","Ram Vasudevan"],"url":"https://arxiv.org/abs/2504.21486"}
{"created":"2025-05-01","title":"DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration","abstract":"Diffusion models have achieved remarkable progress in universal image restoration. While existing methods speed up inference by reducing sampling steps, substantial step intervals often introduce cumulative errors. Moreover, they struggle to balance the commonality of degradation representations and restoration quality. To address these challenges, we introduce \\textbf{DGSolver}, a diffusion generalist solver with universal posterior sampling. We first derive the exact ordinary differential equations for generalist diffusion models and tailor high-order solvers with a queue-based accelerated sampling strategy to improve both accuracy and efficiency. We then integrate universal posterior sampling to better approximate manifold-constrained gradients, yielding a more accurate noise estimation and correcting errors in inverse inference. Extensive experiments show that DGSolver outperforms state-of-the-art methods in restoration accuracy, stability, and scalability, both qualitatively and quantitatively. Code and models will be available at https://github.com/MiliLab/DGSolver.","authors":["Hebaixu Wang","Jing Zhang","Haonan Guo","Di Wang","Jiayi Ma","Bo Du"],"url":"https://arxiv.org/abs/2504.21487"}
{"created":"2025-05-01","title":"TRIED: Truly Innovative and Effective Detection Benchmark, developed by WITNESS","abstract":"The rise of generative AI and deceptive synthetic media threatens the global information ecosystem, especially across the Global Majority. This report from WITNESS highlights the limitations of current AI detection tools, which often underperform in real-world scenarios due to challenges related to explainability, fairness, accessibility, and contextual relevance. In response, WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED) Benchmark, a new framework for evaluating detection tools based on their real-world impact and capacity for innovation. Drawing on frontline experiences, deceptive AI cases, and global consultations, the report outlines how detection tools must evolve to become truly innovative and relevant by meeting diverse linguistic, cultural, and technological contexts. It offers practical guidance for developers, policymakers, and standards bodies to design accountable, transparent, and user-centered detection solutions, and incorporate sociotechnical considerations into future AI standards, procedures and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can drive innovation, safeguard public trust, strengthen AI literacy, and contribute to a more resilient global information credibility.","authors":["Shirin Anlen (WITNESS)","Zuzanna Wojciak (WITNESS)"],"url":"https://arxiv.org/abs/2504.21489"}
{"created":"2025-05-01","title":"ClassWise-CRF: Category-Specific Fusion for Enhanced Semantic Segmentation of Remote Sensing Imagery","abstract":"We propose a result-level category-specific fusion architecture called ClassWise-CRF. This architecture employs a two-stage process: first, it selects expert networks that perform well in specific categories from a pool of candidate networks using a greedy algorithm; second, it integrates the segmentation predictions of these selected networks by adaptively weighting their contributions based on their segmentation performance in each category. Inspired by Conditional Random Field (CRF), the ClassWise-CRF architecture treats the segmentation predictions from multiple networks as confidence vector fields. It leverages segmentation metrics (such as Intersection over Union) from the validation set as priors and employs an exponential weighting strategy to fuse the category-specific confidence scores predicted by each network. This fusion method dynamically adjusts the weights of each network for different categories, achieving category-specific optimization. Building on this, the architecture further optimizes the fused results using unary and pairwise potentials in CRF to ensure spatial consistency and boundary accuracy. To validate the effectiveness of ClassWise-CRF, we conducted experiments on two remote sensing datasets, LoveDA and Vaihingen, using eight classic and advanced semantic segmentation networks. The results show that the ClassWise-CRF architecture significantly improves segmentation performance: on the LoveDA dataset, the mean Intersection over Union (mIoU) metric increased by 1.00% on the validation set and by 0.68% on the test set; on the Vaihingen dataset, the mIoU improved by 0.87% on the validation set and by 0.91% on the test set. These results fully demonstrate the effectiveness and generality of the ClassWise-CRF architecture in semantic segmentation of remote sensing images. The full code is available at https://github.com/zhuqinfeng1999/ClassWise-CRF.","authors":["Qinfeng Zhu","Yunxi Jiang","Lei Fan"],"url":"https://arxiv.org/abs/2504.21491"}
{"created":"2025-05-01","title":"Consistency-aware Fake Videos Detection on Short Video Platforms","abstract":"This paper focuses to detect the fake news on the short video platforms. While significant research efforts have been devoted to this task with notable progress in recent years, current detection accuracy remains suboptimal due to the rapid evolution of content manipulation and generation technologies. Existing approaches typically employ a cross-modal fusion strategy that directly combines raw video data with metadata inputs before applying a classification layer. However, our empirical observations reveal a critical oversight: manipulated content frequently exhibits inter-modal inconsistencies that could serve as valuable discriminative features, yet remain underutilized in contemporary detection frameworks. Motivated by this insight, we propose a novel detection paradigm that explicitly identifies and leverages cross-modal contradictions as discriminative cues. Our approach consists of two core modules: Cross-modal Consistency Learning (CMCL) and Multi-modal Collaborative Diagnosis (MMCD). CMCL includes Pseudo-label Generation (PLG) and Cross-modal Consistency Diagnosis (CMCD). In PLG, a Multimodal Large Language Model is used to generate pseudo-labels for evaluating cross-modal semantic consistency. Then, CMCD extracts [CLS] tokens and computes cosine loss to quantify cross-modal inconsistencies. MMCD further integrates multimodal features through Multimodal Feature Fusion (MFF) and Probability Scores Fusion (PSF). MFF employs a co-attention mechanism to enhance semantic interactions across different modalities, while a Transformer is utilized for comprehensive feature fusion. Meanwhile, PSF further integrates the fake news probability scores obtained in the previous step. Extensive experiments on established benchmarks (FakeSV and FakeTT) demonstrate our model exhibits outstanding performance in Fake videos detection.","authors":["Junxi Wang","Jize liu","Na Zhang","Yaxiong Wang"],"url":"https://arxiv.org/abs/2504.21495"}
{"created":"2025-05-01","title":"MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance","abstract":"In this paper, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This enables precise extraction of detailed face geometry and motion features from driving videos. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. A multi-layer face movements fusion module with integrated self-attention mechanisms is used to combine identity and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.","authors":["Mengting Wei","Yante Li","Tuomas Varanka","Yan Jiang","Licai Sun","Guoying Zhao"],"url":"https://arxiv.org/abs/2504.21497"}
{"created":"2025-05-01","title":"Visual Analytics Challenges and Trends in the Age of AI: The BigVis Community Perspective","abstract":"This report provides insights into the challenges, emerging topics, and opportunities related to human-data interaction and visual analytics in the AI era. The BigVis 2024 organizing committee conducted a survey among experts in the field. They invite the Program Committee members and the authors of accepted papers to share their views. Thirty-two scientists from diverse research communities, including Databases, Information Visualization, and Human-Computer Interaction, participated in the study. These scientists, representing both industry and academia, provided valuable insights into the current and future landscape of the field.","authors":["Nikos Bikakis","Panos K. Chrysanthis","Guoliang Li","George Papastefanatos","Lingyun Yu"],"url":"https://arxiv.org/abs/2504.21500"}
{"created":"2025-05-01","title":"Deep Learning Optimization Using Self-Adaptive Weighted Auxiliary Variables","abstract":"In this paper, we develop a new optimization framework for the least squares learning problem via fully connected neural networks or physics-informed neural networks. The gradient descent sometimes behaves inefficiently in deep learning because of the high non-convexity of loss functions and the vanishing gradient issue. Our idea is to introduce auxiliary variables to separate the layers of the deep neural networks and reformulate the loss functions for ease of optimization. We design the self-adaptive weights to preserve the consistency between the reformulated loss and the original mean squared loss, which guarantees that optimizing the new loss helps optimize the original problem. Numerical experiments are presented to verify the consistency and show the effectiveness and robustness of our models over gradient descent.","authors":["Yaru Liu","Yiqi Gu","Michael K. Ng"],"url":"https://arxiv.org/abs/2504.21501"}
{"created":"2025-05-01","title":"Concurrency Constrained Scheduling with Tree-Like Constraints","abstract":"This paper investigates concurrency-constrained scheduling problems, where the objective is to construct a schedule for a set of jobs subject to concurrency restrictions. Formally, we are given a conflict graph $G$ defined over a set of $n$ jobs, where an edge between two jobs in $G$ indicates that these jobs cannot be executed concurrently. Each job may have distinct attributes, such as processing time, due date, weight, and release time. The goal is to determine a schedule that optimizes a specified scheduling criterion while adhering to all concurrency constraints. This framework offers a versatile model for analyzing resource allocation problems where processes compete for shared resources, such as access to shared memory. From a theoretical perspective, it encompasses several classical graph coloring problems, including Chromatic Number, Sum Coloring, and Interval Chromatic Number.","authors":["Hans L. Bodlaender","Danny Hermelin","Erik Jan van Leeuwen"],"url":"https://arxiv.org/abs/2504.21502"}
{"created":"2025-05-01","title":"CWASI: A WebAssembly Runtime Shim for Inter-function Communication in the Serverless Edge-Cloud Continuum","abstract":"Serverless Computing brings advantages to the Edge-Cloud continuum, like simplified programming and infrastructure management. In composed workflows, where serverless functions need to exchange data constantly, serverless platforms rely on remote services such as object storage and key-value stores as a common approach to exchange data. In WebAssembly, functions leverage WebAssembly System Interface to connect to the network and exchange data via remote services. As a consequence, co-located serverless functions need remote services to exchange data, increasing latency and adding network overhead. To mitigate this problem, in this paper, we introduce CWASI: a WebAssembly OCI-compliant runtime shim that determines the best inter-function data exchange approach based on the serverless function locality. CWASI introduces a three-mode communication model for the Serverless Edge-Cloud continuum. This communication model enables CWASI Shim to optimize inter-function communication for co-located functions by leveraging the function host mechanisms. Experimental results show that CWASI reduces the communication latency between the co-located serverless functions by up to 95% and increases the communication throughput by up to 30x.","authors":["Cynthia Marcelino","Stefan Nastic"],"url":"https://arxiv.org/abs/2504.21503"}
{"created":"2025-05-01","title":"Efficient Conversational Search via Topical Locality in Dense Retrieval","abstract":"Pre-trained language models have been widely exploited to learn dense representations of documents and queries for information retrieval. While previous efforts have primarily focused on improving effectiveness and user satisfaction, response time remains a critical bottleneck of conversational search systems. To address this, we exploit the topical locality inherent in conversational queries, i.e., the tendency of queries within a conversation to focus on related topics. By leveraging query embedding similarities, we dynamically restrict the search space to semantically relevant document clusters, reducing computational complexity without compromising retrieval quality. We evaluate our approach on the TREC CAsT 2019 and 2020 datasets using multiple embedding models and vector indexes, achieving improvements in processing speed of up to 10.4X with little loss in performance (4.4X without any loss). Our results show that the proposed system effectively handles complex, multiturn queries with high precision and efficiency, offering a practical solution for real-time conversational search.","authors":["Cristina Ioana Muntean","Franco Maria Nardini","Raffaele Perego","Guido Rocchietti","Cosimo Rulli"],"url":"https://arxiv.org/abs/2504.21507"}
{"created":"2025-05-01","title":"Arbitrary precision computation of hydrodynamic stability eigenvalues","abstract":"We show that by using higher order precision arithmetic, i.e., using floating point types with more significant bits than standard double precision numbers, one may accurately compute eigenvalues for non-normal matrices arising in hydrodynamic stability problems. The basic principle is illustrated by a classical example of two $7\\times 7$ matrices for which it is well known that eigenvalue computations fail when using standard double precision arithmetic. We then present an implementation of the Chebyshev tau-QZ method allowing the use of a large number of Chebyshev polynomials together with arbitrary precision arithmetic. This is used to compute the behavior of the spectra for Couette and Poiseuille flow at high Reynolds number. An experimental convergence analysis finally makes it evident that high order precision is required to obtain accurate results.","authors":["Patrick Dondl","Ludwig Striet","Brian Straughan"],"url":"https://arxiv.org/abs/2504.21511"}
{"created":"2025-05-01","title":"Confidential Serverless Computing","abstract":"Although serverless computing offers compelling cost and deployment simplicity advantages, a significant challenge remains in securely managing sensitive data as it flows through the network of ephemeral function executions in serverless computing environments within untrusted clouds. While Confidential Virtual Machines (CVMs) offer a promising secure execution environment, their integration with serverless architectures currently faces fundamental limitations in key areas: security, performance, and resource efficiency.","authors":["Patrick Sabanic (Technical University of Munich)","Masanori Misono (Technical University of Munich)","Teofil Bodea (Technical University of Munich)","Julian Pritzi (Technical University of Munich)","Michael Hackl (Technical University of Munich)","Dimitrios Stavrakakis (Technical University of Munich)","Pramod Bhatotia (Technical University of Munich)"],"url":"https://arxiv.org/abs/2504.21518"}
{"created":"2025-05-01","title":"Padding Matters -- Exploring Function Detection in PE Files","abstract":"Function detection is a well-known problem in binary analysis. While previous research has primarily focused on Linux/ELF, Windows/PE binaries have been overlooked or only partially considered. This paper introduces FuncPEval, a new dataset for Windows x86 and x64 PE files, featuring Chromium and the Conti ransomware, along with ground truth data for 1,092,820 function starts. Utilizing FuncPEval, we evaluate five heuristics-based (Ghidra, IDA, Nucleus, rev.ng, SMDA) and three machine-learning-based (DeepDi, RNN, XDA) function start detection tools. Among the tested tools, IDA achieves the highest F1-score (98.44%) for Chromium x64, while DeepDi closely follows (97%) but stands out as the fastest by a significant margin. Working towards explainability, we examine the impact of padding between functions on the detection results. Our analysis shows that all tested tools, except rev.ng, are susceptible to randomized padding. The randomized padding significantly diminishes the effectiveness for the RNN, XDA, and Nucleus. Among the learning-based tools, DeepDi exhibits the least sensitivity and demonstrates overall the fastest performance, while Nucleus is the most adversely affected among non-learning-based tools. In addition, we improve the recurrent neural network (RNN) proposed by Shin et al. and enhance the XDA tool, increasing the F1-score by approximately 10%.","authors":["Raphael Springer","Alexander Schmitz","Artur Leinweber","Tobias Urban","Christian Dietrich"],"url":"https://arxiv.org/abs/2504.21520"}
{"created":"2025-05-01","title":"Adaptive Neural Control with Desired Approximation: An Integral Lyapunov Function Approach","abstract":"The inherent approximation ability of neural networks plays an essential role in adaptive neural control, where the prerequisite for existence of the compact set is crucial in the control designs. Instead of using practical system state, in this paper, the desired approximation approach is characterized to tackle such a problem, where the desired state signal is required only as the input to the network. An integral Lyapunov function-based adaptive controller is designed, in the sense of the error tracking, where the treatment of the state-dependent input gain is adopted. Theoretical results for the performance analysis of the integral and incremental adaptation algorithms are presented in details. In particular, the boundedness of the variables in the closed-loop is characterized, while the transient performance of the output error is analytically quantified. It is shown that the proposed control schemes assure that the tracking error converges to an adjustable set without any requirement on the knowledge of the region that the practical variables evolve, and remove the requirement for the setting of initial conditions including system states and weight estimates.","authors":["Mingxuan Sun","Shengxiang Zou"],"url":"https://arxiv.org/abs/2504.21521"}
{"created":"2025-05-01","title":"Sibuya probability distributions and numerical evaluation of fractional-order operators","abstract":"In this work we explore the Sibuya discrete probability distribution, which serves as the basis and the main instrument for numerical simulations of Grunwald--Letnikov fractional derivatives by the Monte Carlo method. We provide three methods for simulating the Sibuya distribution. We also introduce the Sibuya-like sieved probability distributions, and apply them to numerical fractional-order differentiation. Additionally, we use the Monte Carlo method for evaluating fractional-order integrals, and suggest the notion of the continuous Sibuya probability distribution. The developed methods and tools are illustrated by examples of computation. We provide the MATLAB toolboxes for simulation of the Sibuya probability distribution, and for the numerical examples.","authors":["Nikolai Leonenko","Igor Podlubny"],"url":"https://arxiv.org/abs/2504.21523"}
{"created":"2025-05-01","title":"Low-rank computation of the posterior mean in Multi-Output Gaussian Processes","abstract":"Gaussian processes (GP) are a versatile tool in machine learning and computational science. We here consider the case of multi-output Gaussian processes (MOGP) and present low-rank approaches for efficiently computing the posterior mean of a MOGP. Starting from low-rank spatio-temporal data we consider a structured covariance function, assuming separability across space and time. This separability, in turn, gives a decomposition of the covariance matrix into a Kronecker product of individual covariance matrices. Incorporating the typical noise term to the model then requires the solution of a large-scale Stein equation for computing the posterior mean. For this, we propose efficient low-rank methods based on a combination of a LRPCG method with the Sylvester equation solver KPIK adjusted for solving Stein equations. We test the developed method on real world street network graphs by using graph filters as covariance matrices. Moreover, we propose a degree-weighted average covariance matrix, which can be employed under specific assumptions to achieve more efficient convergence.","authors":["Sebastian Esche","Martin Stoll"],"url":"https://arxiv.org/abs/2504.21527"}
{"created":"2025-05-01","title":"RoboGround: Robotic Manipulation with Grounded Vision-Language Priors","abstract":"Recent advancements in robotic manipulation have highlighted the potential of intermediate representations for improving policy generalization. In this work, we explore grounding masks as an effective intermediate representation, balancing two key advantages: (1) effective spatial guidance that specifies target objects and placement areas while also conveying information about object shape and size, and (2) broad generalization potential driven by large-scale vision-language models pretrained on diverse grounding datasets. We introduce RoboGround, a grounding-aware robotic manipulation system that leverages grounding masks as an intermediate representation to guide policy networks in object manipulation tasks. To further explore and enhance generalization, we propose an automated pipeline for generating large-scale, simulated data with a diverse set of objects and instructions. Extensive experiments show the value of our dataset and the effectiveness of grounding masks as intermediate guidance, significantly enhancing the generalization abilities of robot policies.","authors":["Haifeng Huang","Xinyi Chen","Yilun Chen","Hao Li","Xiaoshen Han","Zehan Wang","Tai Wang","Jiangmiao Pang","Zhou Zhao"],"url":"https://arxiv.org/abs/2504.21530"}
{"created":"2025-05-01","title":"Scientific Workflow Scheduling in Cloud Considering Cold Start and Variable Pricing Model","abstract":"Cloud computing has become a pivotal platform for executing scientific workflows due to its scalable and cost-effective infrastructure. Scientific Cloud Service Providers (SCSPs) act as intermediaries that rent virtual machines (VMs) from Infrastructure-as-a-Service (IaaS) providers to meet users' workflow execution demands. The SCSP earns profit from the execution of scientific workflows if it completes the execution of the workflow before the specified deadline of the workflow. This paper addresses two key challenges that impact the profitability of SCSPs: the cold start problem and the efficient management of diverse VM pricing models, namely reserved, on-demand, and spot instances.","authors":["Suvarthi Sarkar","Sparsh Mittal","Shivam Garg","Aryabartta Sahu"],"url":"https://arxiv.org/abs/2504.21536"}
{"created":"2025-05-01","title":"Coyote v2: Raising the Level of Abstraction for Data Center FPGAs","abstract":"In the trend towards hardware specialization, FPGAs play a dual role as accelerators for offloading, e.g., network virtualization, and as a vehicle for prototyping and exploring hardware designs. While FPGAs offer versatility and performance, integrating them in larger systems remains challenging. Thus, recent efforts have focused on raising the level of abstraction through better interfaces and high-level programming languages. Yet, there is still quite some room for improvement. In this paper, we present Coyote v2, an open source FPGA shell built with a novel, three-layer hierarchical design supporting dynamic partial reconfiguration of services and user logic, with a unified logic interface, and high-level software abstractions such as support for multithreading and multitenancy. Experimental results indicate Coyote v2 reduces synthesis times between 15% and 20% and run-time reconfiguration times by an order of magnitude, when compared to existing systems. We also demonstrate the advantages of Coyote v2 by deploying several realistic applications, including HyperLogLog cardinality estimation, AES encryption, and neural network inference. Finally, Coyote v2 places a great deal of emphasis on integration with real systems through reusable and reconfigurable services, including a fully RoCE v2-compliant networking stack, a shared virtual memory model with the host, and a DMA engine between FPGAs and GPUs. We demonstrate these features by, e.g., seamlessly deploying an FPGA-accelerated neural network from Python.","authors":["Benjamin Ramhorst","Dario Korolija","Maximilian Jakob Heer","Jonas Dann","Luhao Liu","Gustavo Alonso"],"url":"https://arxiv.org/abs/2504.21538"}
{"created":"2025-05-01","title":"Improving Informally Romanized Language Identification","abstract":"The Latin script is often used to informally write languages with non-Latin native scripts. In many cases (e.g., most languages in India), there is no conventional spelling of words in the Latin script, hence there will be high spelling variability in written text. Such romanization renders languages that are normally easily distinguished based on script highly confusable, such as Hindi and Urdu. In this work, we increase language identification (LID) accuracy for romanized text by improving the methods used to synthesize training sets. We find that training on synthetic samples which incorporate natural spelling variation yields higher LID system accuracy than including available naturally occurring examples in the training set, or even training higher capacity models. We demonstrate new state-of-the-art LID performance on romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set (Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a pretrained neural model) to 85.4% using a linear classifier trained solely on synthetic data and 88.2% when also training on available harvested text.","authors":["Adrian Benton","Alexander Gutkin","Christo Kirov","Brian Roark"],"url":"https://arxiv.org/abs/2504.21540"}
{"created":"2025-05-01","title":"CryptoUNets: Applying Convolutional Networks to Encrypted Data for Biomedical Image Segmentation","abstract":"In this manuscript, we demonstrate the feasibility of a privacy-preserving U-Net deep learning inference framework, namely, homomorphic encryption-based U-Net inference. That is, U-Net inference can be performed solely using homomorphic encryption techniques. To our knowledge, this is the first work to achieve support perform implement enable U-Net inference entirely based on homomorphic encryption ?.","authors":["John Chiang"],"url":"https://arxiv.org/abs/2504.21543"}
{"created":"2025-05-01","title":"SAM4EM: Efficient memory-based two stage prompt-free segment anything model adapter for complex 3D neuroscience electron microscopy stacks","abstract":"We present SAM4EM, a novel approach for 3D segmentation of complex neural structures in electron microscopy (EM) data by leveraging the Segment Anything Model (SAM) alongside advanced fine-tuning strategies. Our contributions include the development of a prompt-free adapter for SAM using two stage mask decoding to automatically generate prompt embeddings, a dual-stage fine-tuning method based on Low-Rank Adaptation (LoRA) for enhancing segmentation with limited annotated data, and a 3D memory attention mechanism to ensure segmentation consistency across 3D stacks. We further release a unique benchmark dataset for the segmentation of astrocytic processes and synapses. We evaluated our method on challenging neuroscience segmentation benchmarks, specifically targeting mitochondria, glia, and synapses, with significant accuracy improvements over state-of-the-art (SOTA) methods, including recent SAM-based adapters developed for the medical domain and other vision transformer-based approaches. Experimental results indicate that our approach outperforms existing solutions in the segmentation of complex processes like glia and post-synaptic densities. Our code and models are available at https://github.com/Uzshah/SAM4EM.","authors":["Uzair Shah","Marco Agus","Daniya Boges","Vanessa Chiappini","Mahmood Alzubaidi","Jens Schneider","Markus Hadwiger","Pierre J. Magistretti","Mowafa Househ","Corrado Cal{\\i}"],"url":"https://arxiv.org/abs/2504.21544"}
{"created":"2025-05-01","title":"Meta knowledge assisted Evolutionary Neural Architecture Search","abstract":"Evolutionary computation (EC)-based neural architecture search (NAS) has achieved remarkable performance in the automatic design of neural architectures. However, the high computational cost associated with evaluating searched architectures poses a challenge for these methods, and a fixed form of learning rate (LR) schedule means greater information loss on diverse searched architectures. This paper introduces an efficient EC-based NAS method to solve these problems via an innovative meta-learning framework. Specifically, a meta-learning-rate (Meta-LR) scheme is used through pretraining to obtain a suitable LR schedule, which guides the training process with lower information loss when evaluating each individual. An adaptive surrogate model is designed through an adaptive threshold to select the potential architectures in a few epochs and then evaluate the potential architectures with complete epochs. Additionally, a periodic mutation operator is proposed to increase the diversity of the population, which enhances the generalizability and robustness. Experiments on CIFAR-10, CIFAR-100, and ImageNet1K datasets demonstrate that the proposed method achieves high performance comparable to that of many state-of-the-art peer methods, with lower computational cost and greater robustness.","authors":["Yangyang Li","Guanlong Liu","Ronghua Shang","Licheng Jiao"],"url":"https://arxiv.org/abs/2504.21545"}
{"created":"2025-05-01","title":"TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval","abstract":"We present our submission to the Task 5 of SemEval-2025 that aims to aid librarians in assigning subject tags to the library records by producing a list of likely relevant tags for a given document. We frame the task as an information retrieval problem, where the document content is used to retrieve subject tags from a large subject taxonomy. We leverage two types of encoder models to build a two-stage information retrieval system -- a bi-encoder for coarse-grained candidate extraction at the first stage, and a cross-encoder for fine-grained re-ranking at the second stage. This approach proved effective, demonstrating significant improvements in recall compared to single-stage methods and showing competitive results according to qualitative evaluation.","authors":["Aleksei Dorkin","Kairit Sirts"],"url":"https://arxiv.org/abs/2504.21547"}
{"created":"2025-05-01","title":"Leveraging Systems and Control Theory for Social Robotics: A Model-Based Behavioral Control Approach to Human-Robot Interaction","abstract":"Social robots (SRs) should autonomously interact with humans, while exhibiting proper social behaviors associated to their role. By contributing to health-care, education, and companionship, SRs will enhance life quality. However, personalization and sustaining user engagement remain a challenge for SRs, due to their limited understanding of human mental states. Accordingly, we leverage a recently introduced mathematical dynamic model of human perception, cognition, and decision-making for SRs. Identifying the parameters of this model and deploying it in behavioral steering system of SRs allows to effectively personalize the responses of SRs to evolving mental states of their users, enhancing long-term engagement and personalization. Our approach uniquely enables autonomous adaptability of SRs by modeling the dynamics of invisible mental states, significantly contributing to the transparency and awareness of SRs. We validated our model-based control system in experiments with 10 participants who interacted with a Nao robot over three chess puzzle sessions, 45 - 90 minutes each. The identified model achieved a mean squared error (MSE) of 0.067 (i.e., 1.675% of the maximum possible MSE) in tracking beliefs, goals, and emotions of participants. Compared to a model-free controller that did not track mental states of participants, our approach increased engagement by 16% on average. Post-interaction feedback of participants (provided via dedicated questionnaires) further confirmed the perceived engagement and awareness of the model-driven robot. These results highlight the unique potential of model-based approaches and control theory in advancing human-SR interactions.","authors":["Maria Mor\\~ao Patr\\'icio","Anahita Jamshidnejad"],"url":"https://arxiv.org/abs/2504.21548"}
{"created":"2025-05-01","title":"Online Experimental Design for Network Tomography","abstract":"How to efficiently perform network tomography is a fundamental problem in network management and monitoring. A network tomography task usually consists of applying multiple probing experiments, e.g., across different paths or via different casts (including unicast and multicast). We study how to optimize the network tomography process through online sequential decision-making. From the methodology perspective, we introduce an online probe allocation algorithm that dynamically performs network tomography based on the principles of optimal experimental design and the maximum likelihood estimation. We rigorously analyze the regret of the algorithm under the conditions that i) the optimal allocation is Lipschitz continuous in the parameters being estimated and ii) the parameter estimators satisfy a concentration property. From the application perspective, we present two case studies: a) the classical lossy packet-switched network and b) the quantum bit-flip network. We show that both cases fulfill the two theoretical conditions and provide their corresponding regrets when deploying our proposed online probe allocation algorithm. Besides these two case studies with theoretical guarantees, we also conduct simulations to compare our proposed algorithm with existing methods and demonstrate our algorithm's effectiveness in a broader range of scenarios.","authors":["Xuchuang Wang","Yu-Zhen Janice Chen","Matheus Guedes de Andrade","Mohammad Hajiesmaili","John C. S. Lui","Ting He","Don Towsley"],"url":"https://arxiv.org/abs/2504.21549"}
{"created":"2025-05-01","title":"The First Theoretical Approximation Guarantees for the Non-Dominated Sorting Genetic Algorithm III (NSGA-III)","abstract":"This work conducts a first theoretical analysis studying how well the NSGA-III approximates the Pareto front when the population size $N$ is less than the Pareto front size. We show that when $N$ is at least the number $N_r$ of reference points, then the approximation quality, measured by the maximum empty interval (MEI) indicator, on the OneMinMax benchmark is such that there is no empty interval longer than $\\lceil\\frac{(5-2\\sqrt2)n}{N_r-1}\\rceil$. This bound is independent of $N$, which suggests that further increasing the population size does not increase the quality of approximation when $N_r$ is fixed. This is a notable difference to the NSGA-II with sequential survival selection, where increasing the population size improves the quality of the approximations. We also prove two results indicating approximation difficulties when $N<N_r$. These theoretical results suggest that the best setting to approximate the Pareto front is $N_r=N$. In our experiments, we observe that with this setting the NSGA-III computes optimal approximations, very different from the NSGA-II, for which optimal approximations have not been observed so far.","authors":["Renzhong Deng","Weijie Zheng","Benjamin Doerr"],"url":"https://arxiv.org/abs/2504.21552"}
{"created":"2025-05-01","title":"Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, their size presents significant challenges for deployment and inference. This paper investigates the quantization of LLMs, focusing on the LLaMA architecture and its derivatives. We challenge existing assumptions about activation outliers in LLMs and propose a novel mixed-precision quantization approach tailored for LLaMA-like models. Our method leverages the observation that activation spikes in LLaMA architectures are predominantly concentrated in specific projection layers. By applying higher precision (FP16 or FP8) to these layers while quantizing the rest of the model to lower bit-widths, we achieve superior performance compared to existing quantization techniques. Experimental results on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in perplexity and zero-shot accuracy, particularly for 8-bit per-tensor quantization. Our approach outperforms general-purpose methods designed to handle outliers across all architecture types, highlighting the benefits of architecture-specific quantization strategies. This research contributes to the ongoing efforts to make LLMs more efficient and deployable, potentially enabling their use in resource-constrained environments. Our findings emphasize the importance of considering model-specific characteristics in developing effective quantization pipelines for state-of-the-art language models by identifying and targeting a small number of projections that concentrate activation spikes.","authors":["Lucas Maisonnave","Cyril Moineau","Olivier Bichler","Fabrice Rastello"],"url":"https://arxiv.org/abs/2504.21553"}
{"created":"2025-05-01","title":"Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models","abstract":"Large Vision Language Models (LVLMs) often suffer from object hallucination, which undermines their reliability. Surprisingly, we find that simple object-based visual prompting -- overlaying visual cues (e.g., bounding box, circle) on images -- can significantly mitigate such hallucination; however, different visual prompts (VPs) vary in effectiveness. To address this, we propose Black-Box Visual Prompt Engineering (BBVPE), a framework to identify optimal VPs that enhance LVLM responses without needing access to model internals. Our approach employs a pool of candidate VPs and trains a router model to dynamically select the most effective VP for a given input image. This black-box approach is model-agnostic, making it applicable to both open-source and proprietary LVLMs. Evaluations on benchmarks such as POPE and CHAIR demonstrate that BBVPE effectively reduces object hallucination.","authors":["Sangmin Woo","Kang Zhou","Yun Zhou","Shuai Wang","Sheng Guan","Haibo Ding","Lin Lee Cheong"],"url":"https://arxiv.org/abs/2504.21559"}
{"created":"2025-05-01","title":"Iterative Trajectory Exploration for Multimodal Agents","abstract":"Multimodal agents, which integrate a controller (e.g., a large language model) with external tools, have demonstrated remarkable capabilities in tackling complex tasks. However, existing agents need to collect a large number of expert data for fine-tuning to adapt to new environments. In this paper, we propose an online self-exploration method for multimodal agents, namely SPORT, via step-wise preference optimization to refine the trajectories of agents, which automatically generates tasks and learns from solving the generated tasks, without any expert annotation. SPORT operates through four iterative components: task synthesis, step sampling, step verification, and preference tuning. First, we synthesize multi-modal tasks using language models. Then, we introduce a novel search scheme, where step sampling and step verification are executed alternately to solve each generated task. We employ a verifier to provide AI feedback to construct step-wise preference data. The data is subsequently used to update the controller's policy through preference tuning, producing a SPORT Agent. By interacting with real environments, the SPORT Agent evolves into a more refined and capable system. Evaluation in the GTA and GAIA benchmarks show that the SPORT Agent achieves 6.41\\% and 3.64\\% improvements, underscoring the generalization and effectiveness introduced by our method. The project page is https://SPORT-Agents.github.io.","authors":["Pengxiang Li","Zhi Gao","Bofei Zhang","Yapeng Mi","Xiaojian Ma","Chenrui Shi","Tao Yuan","Yuwei Wu","Yunde Jia","Song-Chun Zhu","Qing Li"],"url":"https://arxiv.org/abs/2504.21561"}
{"created":"2025-05-01","title":"eNCApsulate: NCA for Precision Diagnosis on Capsule Endoscopes","abstract":"Wireless Capsule Endoscopy is a non-invasive imaging method for the entire gastrointestinal tract, and is a pain-free alternative to traditional endoscopy. It generates extensive video data that requires significant review time, and localizing the capsule after ingestion is a challenge. Techniques like bleeding detection and depth estimation can help with localization of pathologies, but deep learning models are typically too large to run directly on the capsule. Neural Cellular Automata (NCA) for bleeding segmentation and depth estimation are trained on capsule endoscopic images. For monocular depth estimation, we distill a large foundation model into the lean NCA architecture, by treating the outputs of the foundation model as pseudo ground truth. We then port the trained NCA to the ESP32 microcontroller, enabling efficient image processing on hardware as small as a camera capsule. NCA are more accurate (Dice) than other portable segmentation models, while requiring more than 100x fewer parameters stored in memory than other small-scale models. The visual results of NCA depth estimation look convincing, and in some cases beat the realism and detail of the pseudo ground truth. Runtime optimizations on the ESP32-S3 accelerate the average inference speed significantly, by more than factor 3. With several algorithmic adjustments and distillation, it is possible to eNCApsulate NCA models into microcontrollers that fit into wireless capsule endoscopes. This is the first work that enables reliable bleeding segmentation and depth estimation on a miniaturized device, paving the way for precise diagnosis combined with visual odometry as a means of precise localization of the capsule -- on the capsule.","authors":["Henry John Krumb","Anirban Mukhopadhyay"],"url":"https://arxiv.org/abs/2504.21562"}
{"created":"2025-05-01","title":"A User-Centered Teleoperation GUI for Automated Vehicles: Identifying and Evaluating Information Requirements for Remote Driving and Assistance","abstract":"Teleoperation emerged as a promising fallback for situations beyond the capabilities of automated vehicles. Nevertheless, teleoperation still faces challenges, such as reduced situational awareness. Since situational awareness is primarily built through the remote operator's visual perception, the Graphical User Interface (GUI) design is critical. In addition to video feeds, supplemental informational elements are crucial - not only for the predominantly studied Remote Driving but also for the arising desk-based Remote Assistance concepts. This work develops a GUI for different teleoperation concepts by identifying key informational elements during the teleoperation process through expert interviews (N = 9). Following this, a static and dynamic GUI prototype is developed and evaluated in a click-dummy study (N = 36). Thereby, the dynamic GUI adapts the number of displayed elements according to the teleoperation phase. Results show that both GUIs achieve good System Usability Scale (SUS) ratings, with the dynamic GUI significantly outperforming the static version in both usability and task completion time. The User Experience Questionnaire (UEQ) score shows potential for improvement. To enhance the user experience, the GUI should be evaluated in a follow-up study that includes interaction with a real vehicle.","authors":["Maria-Magdalena Wolf","Henrik Schmidt","Michael Christl","Jana Fank","Frank Diermeyer"],"url":"https://arxiv.org/abs/2504.21563"}
{"created":"2025-05-01","title":"Towards proactive self-adaptive AI for non-stationary environments with dataset shifts","abstract":"Artificial Intelligence (AI) models deployed in production frequently face challenges in maintaining their performance in non-stationary environments. This issue is particularly noticeable in medical settings, where temporal dataset shifts often occur. These shifts arise when the distributions of training data differ from those of the data encountered during deployment over time. Further, new labeled data to continuously retrain AI is not typically available in a timely manner due to data access limitations. To address these challenges, we propose a proactive self-adaptive AI approach, or pro-adaptive, where we model the temporal trajectory of AI parameters, allowing us to short-term forecast parameter values. To this end, we use polynomial spline bases, within an extensible Functional Data Analysis framework. We validate our methodology with a logistic regression model addressing prior probability shift, covariate shift, and concept shift. This validation is conducted on both a controlled simulated dataset and a publicly available real-world COVID-19 dataset from Mexico, with various shifts occurring between 2020 and 2024. Our results indicate that this approach enhances the performance of AI against shifts compared to baseline stable models trained at different time distances from the present, without requiring updated training data. This work lays the foundation for pro-adaptive AI research against dynamic, non-stationary environments, being compatible with data protection, in resilient AI production environments for health.","authors":["David Fern\\'andez Narro","Pablo Ferri","Juan M. Garc\\'ia-G\\'omez","Carlos S\\'aez"],"url":"https://arxiv.org/abs/2504.21565"}
{"created":"2025-05-01","title":"A Study on Group Decision Making Problem Based on Fuzzy Reasoning and Bayesian Networks","abstract":"Aiming at the group decision - making problem with multi - objective attributes, this study proposes a group decision - making system that integrates fuzzy inference and Bayesian network. A fuzzy rule base is constructed by combining threshold values, membership functions, expert experience, and domain knowledge to address quantitative challenges such as scale differences and expert linguistic variables. A hierarchical Bayesian network is designed, featuring a directed acyclic graph with nodes selected by experts, and maximum likelihood estimation is used to dynamically optimize the conditional probability table, modeling the nonlinear correlations among multidimensional indices for posterior probability aggregation. In a comprehensive student evaluation case, this method is compared with the traditional weighted scoring approach. The results indicate that the proposed method demonstrates effectiveness in both rule criterion construction and ranking consistency, with a classification accuracy of 86.0% and an F1 value improvement of 53.4% over the traditional method. Additionally, computational experiments on real - world datasets across various group decision scenarios assess the method's performance and robustness, providing evidence of its reliability in diverse contexts.","authors":["Shui-jin Rong","Wei Guo","Da-qing Zhang"],"url":"https://arxiv.org/abs/2504.21568"}
{"created":"2025-05-01","title":"A Systematic Literature Review of Parameter-Efficient Fine-Tuning for Large Code Models","abstract":"The rise of Artificial Intelligence (AI)-and particularly Large Language Models (LLMs) for code-has reshaped Software Engineering (SE) by enabling the automation of tasks such as code generation, bug detection, and repair. However, these models require significant computational resources for training and fine-tuning, posing challenges for real-world adoption in resource-constrained environments. To address this, the research community has increasingly turned to Parameter-Efficient Fine-Tuning (PEFT)-a class of techniques that enables the adaptation of large models by updating only a small subset of parameters, rather than the entire model. In this Systematic Literature Review (SLR), we examine the growing application of PEFT techniques-across a wide range of software engineering tasks. We analyze how these methods are used to optimize various deep learning (DL) architectures, focusing on their impact on both performance and efficiency. Our study synthesizes findings from 27 peer-reviewed papers, identifying patterns in configuration strategies and adaptation trade-offs. The outcome of this review is a comprehensive taxonomy that categorizes PEFT usage by task type, distinguishing between generative (e.g., Code Summarization) and non-generative (e.g., Code Clone Detection) scenarios. Our findings aim to inform future research and guide the practical deployment of PEFT in sustainable, AI-powered software development. Our artifacts are publicly available at https://github.com/alvi75/SLR-PEFT","authors":["Md Zahidul Haque","Saima Afrin","Antonio Mastropaolo"],"url":"https://arxiv.org/abs/2504.21569"}
{"created":"2025-05-01","title":"FreeBeacon: Efficient Communication and Data Aggregation in Battery-Free IoT","abstract":"To improve sustainability, Internet-of-Things (IoT) is increasingly adopting battery-free devices powered by ambient energy scavenged from the environment. The unpredictable availability of ambient energy leads to device intermittency, bringing critical challenges to device communication and related fundamental operations like data aggregation.","authors":["Gaosheng Liu","Kas{\\i}m Sinan Y{\\i}ld{\\i}r{\\i}m","Lin Wang"],"url":"https://arxiv.org/abs/2504.21571"}
{"created":"2025-05-01","title":"Generative AI in Financial Institution: A Global Survey of Opportunities, Threats, and Regulation","abstract":"Generative Artificial Intelligence (GenAI) is rapidly reshaping the global financial landscape, offering unprecedented opportunities to enhance customer engagement, automate complex workflows, and extract actionable insights from vast financial data. This survey provides an overview of GenAI adoption across the financial ecosystem, examining how banks, insurers, asset managers, and fintech startups worldwide are integrating large language models and other generative tools into their operations. From AI-powered virtual assistants and personalized financial advisory to fraud detection and compliance automation, GenAI is driving innovation across functions. However, this transformation comes with significant cybersecurity and ethical risks. We discuss emerging threats such as AI-generated phishing, deepfake-enabled fraud, and adversarial attacks on AI systems, as well as concerns around bias, opacity, and data misuse. The evolving global regulatory landscape is explored in depth, including initiatives by major financial regulators and international efforts to develop risk-based AI governance. Finally, we propose best practices for secure and responsible adoption - including explainability techniques, adversarial testing, auditability, and human oversight. Drawing from academic literature, industry case studies, and policy frameworks, this chapter offers a perspective on how the financial sector can harness GenAI's transformative potential while navigating the complex risks it introduces.","authors":["Bikash Saha","Nanda Rani","Sandeep Kumar Shukla"],"url":"https://arxiv.org/abs/2504.21574"}
{"created":"2025-05-01","title":"Latent Feature-Guided Conditional Diffusion for High-Fidelity Generative Image Semantic Communication","abstract":"Semantic communication is proposed and expected to improve the efficiency and effectiveness of massive data transmission over sixth generation (6G) networks. However, existing deep learning-based joint source and channel coding (DeepJSCC) image semantic communication scheme predominantly focuses on optimizing pixel-level metrics, and neglects human perceptual requirements, which results in degraded perceptual quality. To address this issue, we propose a latent representation-oriented image semantic communication (LRISC) system, which transmits latent semantic features for image generation with semantic consistency, thereby ensuring the perceptual quality at the receiver. In particular, we first map the source image to latent features in a high-dimensional semantic space via a neural network (NN)- based non-linear transformation. Subsequently, these features are encoded using a joint source and channel coding (JSCC) scheme with adaptive coding length for efficient transmission over a wireless channel. At the receiver, a conditional diffusion model is developed by using the received latent features as conditional guidance to steer the reverse diffusion process, progressively reconstructing high-fidelity images while preserving semantic consistency. Moreover, we introduce a channel signal-to-noise ratio (SNR) adaptation mechanism, allowing one model to work across various channel states. Experiments show that the proposed method significantly outperforms existing methods, in terms of learned perceptual image patch similarity (LPIPS) and robustness against channel noise, with an average LPIPS reduction of 43.3% compared to DeepJSCC, while guaranteeing the semantic consistency.","authors":["Zehao Chen","Xinfeng Wei","Haonan Tong","Zhaohui Yang","Changchuan Yin"],"url":"https://arxiv.org/abs/2504.21577"}
{"created":"2025-05-01","title":"Uncertainty, bias and the institution bootstrapping problem","abstract":"Institutions play a critical role in enabling communities to manage common-pool resources and avert tragedies of the commons. However, a fundamental issue arises: Individuals typically perceive participation as advantageous only after an institution is established, creating a paradox: How can institutions form if no one will join before a critical mass exists? We term this conundrum the institution bootstrapping problem and propose that misperception, specifically, agents' erroneous belief that an institution already exists, could resolve this paradox. By integrating well-documented psychological phenomena, including cognitive biases, probability distortion, and perceptual noise, into a game-theoretic framework, we demonstrate how these factors collectively mitigate the bootstrapping problem. Notably, unbiased perceptual noise (e.g., noise arising from agents' heterogeneous physical or social contexts) drastically reduces the critical mass of cooperators required for institutional emergence. This effect intensifies with greater diversity of perceptions. We explain this counter-intuitive result through asymmetric boundary conditions: proportional underestimation of low-probability sanctions produces distinct outcomes compared to equivalent overestimation. Furthermore, the type of perceptual distortion, proportional versus absolute, yields qualitatively different evolutionary pathways. These findings challenge conventional assumptions about rationality in institutional design, highlighting how \"noisy\" cognition can paradoxically enhance cooperation. Finally, we contextualize these insights within broader discussions of multi-agent system design and collective action. Our analysis underscores the importance of incorporating human-like cognitive constraints, not just idealized rationality, into models of institutional emergence and resilience.","authors":["Stavros Anagnou","Christoph Salge","Peter R. Lewis"],"url":"https://arxiv.org/abs/2504.21579"}
{"created":"2025-05-01","title":"MF-LLM: Simulating Collective Decision Dynamics via a Mean-Field Large Language Model Framework","abstract":"Simulating collective decision-making involves more than aggregating individual behaviors; it arises from dynamic interactions among individuals. While large language models (LLMs) show promise for social simulation, existing approaches often exhibit deviations from real-world data. To address this gap, we propose the Mean-Field LLM (MF-LLM) framework, which explicitly models the feedback loop between micro-level decisions and macro-level population. MF-LLM alternates between two models: a policy model that generates individual actions based on personal states and group-level information, and a mean field model that updates the population distribution from the latest individual decisions. Together, they produce rollouts that simulate the evolving trajectories of collective decision-making. To better match real-world data, we introduce IB-Tune, a fine-tuning method for LLMs grounded in the information bottleneck principle, which maximizes the relevance of population distributions to future actions while minimizing redundancy with historical data. We evaluate MF-LLM on a real-world social dataset, where it reduces KL divergence to human population distributions by 47 percent over non-mean-field baselines, and enables accurate trend forecasting and intervention planning. It generalizes across seven domains and four LLM backbones, providing a scalable foundation for high-fidelity social simulation.","authors":["Qirui Mi","Mengyue Yang","Xiangning Yu","Zhiyu Zhao","Cheng Deng","Bo An","Haifeng Zhang","Xu Chen","Jun Wang"],"url":"https://arxiv.org/abs/2504.21582"}
{"created":"2025-05-01","title":"Toward Realization of Low-Altitude Economy Networks: Core Architecture, Integrated Technologies, and Future Directions","abstract":"The rise of the low-altitude economy (LAE) is propelling urban development and emerging industries by integrating advanced technologies to enhance efficiency, safety, and sustainability in low-altitude operations. The widespread adoption of unmanned aerial vehicles (UAVs) and electric vertical takeoff and landing (eVTOL) aircraft plays a crucial role in enabling key applications within LAE, such as urban logistics, emergency rescue, and aerial mobility. However, unlike traditional UAV networks, LAE networks encounter increased airspace management demands due to dense flying nodes and potential interference with ground communication systems. In addition, there are heightened and extended security risks in real-time operations, particularly the vulnerability of low-altitude aircraft to cyberattacks from ground-based threats. To address these, this paper first explores related standards and core architecture that support the development of LAE networks. Subsequently, we highlight the integration of technologies such as communication, sensing, computing, positioning, navigation, surveillance, flight control, and airspace management. This synergy of multi-technology drives the advancement of real-world LAE applications, particularly in improving operational efficiency, optimizing airspace usage, and ensuring safety. Finally, we outline future research directions for LAE networks, such as intelligent and adaptive optimization, security and privacy protection, sustainable energy and power management, quantum-driven coordination, generative governance, and three-dimensional (3D) airspace coverage, which collectively underscore the potential of collaborative technologies to advance LAE networks.","authors":["Yixian Wang","Geng Sun","Zemin Sun","Jiacheng Wang","Jiahui Li","Changyuan Zhao","Jing Wu","Shuang Liang","Minghao Yin","Pengfei Wang","Dusit Niyato","Sumei Sun","Dong In Kim"],"url":"https://arxiv.org/abs/2504.21583"}
{"created":"2025-05-01","title":"Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning","abstract":"This paper tackles the challenge of learning multi-goal dexterous hand manipulation tasks using model-based Reinforcement Learning. We propose Goal-Conditioned Probabilistic Model Predictive Control (GC-PMPC) by designing probabilistic neural network ensembles to describe the high-dimensional dexterous hand dynamics and introducing an asynchronous MPC policy to meet the control frequency requirements in real-world dexterous hand systems. Extensive evaluations on four simulated Shadow Hand manipulation scenarios with randomly generated goals demonstrate GC-PMPC's superior performance over state-of-the-art baselines. It successfully drives a cable-driven Dexterous hand, DexHand 021 with 12 Active DOFs and 5 tactile sensors, to learn manipulating a cubic die to three goal poses within approximately 80 minutes of interactions, demonstrating exceptional learning efficiency and control performance on a cost-effective dexterous hand platform.","authors":["Yingzhuo Jiang","Wenjun Huang","Rongdun Lin","Chenyang Miao","Tianfu Sun","Yunduan Cui"],"url":"https://arxiv.org/abs/2504.21585"}
{"created":"2025-05-01","title":"One Net to Rule Them All: Domain Randomization in Quadcopter Racing Across Different Platforms","abstract":"In high-speed quadcopter racing, finding a single controller that works well across different platforms remains challenging. This work presents the first neural network controller for drone racing that generalizes across physically distinct quadcopters. We demonstrate that a single network, trained with domain randomization, can robustly control various types of quadcopters. The network relies solely on the current state to directly compute motor commands. The effectiveness of this generalized controller is validated through real-world tests on two substantially different crafts (3-inch and 5-inch race quadcopters). We further compare the performance of this generalized controller with controllers specifically trained for the 3-inch and 5-inch drone, using their identified model parameters with varying levels of domain randomization (0%, 10%, 20%, 30%). While the generalized controller shows slightly slower speeds compared to the fine-tuned models, it excels in adaptability across different platforms. Our results show that no randomization fails sim-to-real transfer while increasing randomization improves robustness but reduces speed. Despite this trade-off, our findings highlight the potential of domain randomization for generalizing controllers, paving the way for universal AI controllers that can adapt to any platform.","authors":["Robin Ferede","Till Blaha","Erin Lucassen","Christophe De Wagter","Guido C. H. E. de Croon"],"url":"https://arxiv.org/abs/2504.21586"}
{"created":"2025-05-01","title":"DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing","abstract":"This paper presents our system developed for the SemEval-2025 Task 5: LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records. This few-shot prompting technique is combined with a series of post-processing steps that map the generated keywords to the target vocabulary, aggregate the resulting subject terms to an ensemble vote and, finally, rank them as to their relevance to the record. Our system is fourth in the quantitative ranking in the all-subjects track, but achieves the best result in the qualitative ranking conducted by subject indexing experts.","authors":["Lisa Kluge","Maximilian K\\\"ahler"],"url":"https://arxiv.org/abs/2504.21589"}
{"created":"2025-05-01","title":"A Protection-Interoperable Fault Ride-Through Control for Grid-Forming Inverters","abstract":"Differing from synchronous generators (SGs), grid-forming inverter-based resources (GFM-IBRs) exhibit rapid variations in their output impedances during transmission line faults due to the overcurrent limitation. As a result, the source dynamics during the fault period deviate significantly from those under pre-fault conditions. This fundamental difference alters the fault responses of incremental quantities, thereby jeopardizing the reliability of the supervising elements in protective relays that are based on these quantities. To address this challenge, a protection-interoperable fault ride-through (FRT) method for GFM-IBRs is proposed. This method dynamically adjusts power control of GFM-IBRs in response to the changes in output impedance, effectively mitigating variations in source dynamics and thereby preserving the reliability of incremental quantity-based supervising elements. This method also ensures effective overcurrent limitation and transient stability of GFM-IBRs. Controller hardware-in-the-loop (CHIL) and experimental tests validate the effectiveness of the proposed method.","authors":["Yifei Li","Heng Wu","Xiongfei Wang"],"url":"https://arxiv.org/abs/2504.21592"}
{"created":"2025-05-01","title":"Switching Transients in Constrained Transformer-Line/Cable Configurations","abstract":"This paper investigates the transient phenomena that occur in two special cases in the Netherlands: (A) during the energization of a power transformer via a cable feeder and (B) the energization of a power transformer together with an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV transformer are connected and energized at the same time. In Case B a 150/50 kV transformer and a short 50 kV OHL are connected and energized simultaneously. The reason behind this kind of situations is related to space restrictions and cost efficiency.","authors":["Y. Xiang","L. Wu","K. Velitsikakis","A. L. J. Janssen"],"url":"https://arxiv.org/abs/2504.21594"}
{"created":"2025-05-01","title":"Leveraging Pre-trained Large Language Models with Refined Prompting for Online Task and Motion Planning","abstract":"With the rapid advancement of artificial intelligence, there is an increasing demand for intelligent robots capable of assisting humans in daily tasks and performing complex operations. Such robots not only require task planning capabilities but must also execute tasks with stability and robustness. In this paper, we present a closed-loop task planning and acting system, LLM-PAS, which is assisted by a pre-trained Large Language Model (LLM). While LLM-PAS plans long-horizon tasks in a manner similar to traditional task and motion planners, it also emphasizes the execution phase of the task. By transferring part of the constraint-checking process from the planning phase to the execution phase, LLM-PAS enables exploration of the constraint space and delivers more accurate feedback on environmental anomalies during execution. The reasoning capabilities of the LLM allow it to handle anomalies that cannot be addressed by the robust executor. To further enhance the system's ability to assist the planner during replanning, we propose the First Look Prompting (FLP) method, which induces LLM to generate effective PDDL goals. Through comparative prompting experiments and systematic experiments, we demonstrate the effectiveness and robustness of LLM-PAS in handling anomalous conditions during task execution.","authors":["Huihui Guo","Huilong Pi","Yunchuan Qin","Zhuo Tang","Kenli Li"],"url":"https://arxiv.org/abs/2504.21596"}
{"created":"2025-05-01","title":"Cascade Detector Analysis and Application to Biomedical Microscopy","abstract":"As both computer vision models and biomedical datasets grow in size, there is an increasing need for efficient inference algorithms. We utilize cascade detectors to efficiently identify sparse objects in multiresolution images. Given an object's prevalence and a set of detectors at different resolutions with known accuracies, we derive the accuracy, and expected number of classifier calls by a cascade detector. These results generalize across number of dimensions and number of cascade levels. Finally, we compare one- and two-level detectors in fluorescent cell detection, organelle segmentation, and tissue segmentation across various microscopy modalities. We show that the multi-level detector achieves comparable performance in 30-75% less time. Our work is compatible with a variety of computer vision models and data domains.","authors":["Thomas L. Athey","Shashata Sawmya","Nir Shavit"],"url":"https://arxiv.org/abs/2504.21598"}
{"created":"2025-05-01","title":"Real Time Semantic Segmentation of High Resolution Automotive LiDAR Scans","abstract":"In recent studies, numerous previous works emphasize the importance of semantic segmentation of LiDAR data as a critical component to the development of driver-assistance systems and autonomous vehicles. However, many state-of-the-art methods are tested on outdated, lower-resolution LiDAR sensors and struggle with real-time constraints. This study introduces a novel semantic segmentation framework tailored for modern high-resolution LiDAR sensors that addresses both accuracy and real-time processing demands. We propose a novel LiDAR dataset collected by a cutting-edge automotive 128 layer LiDAR in urban traffic scenes. Furthermore, we propose a semantic segmentation method utilizing surface normals as strong input features. Our approach is bridging the gap between cutting-edge research and practical automotive applications. Additionaly, we provide a Robot Operating System (ROS2) implementation that we operate on our research vehicle. Our dataset and code are publicly available: https://github.com/kav-institute/SemanticLiDAR.","authors":["Hannes Reichert","Benjamin Serfling","Elijah Sch\\\"ussler","Kerim Turacan","Konrad Doll","Bernhard Sick"],"url":"https://arxiv.org/abs/2504.21602"}
{"created":"2025-05-01","title":"Flow Through Porous Media: A Hopf-Cole Transformation Approach for Modeling Pressure-Dependent Viscosity","abstract":"Most organic liquids exhibit a pressure-dependent viscosity, making it crucial to consider this behavior in applications where pressures significantly exceed ambient conditions (e.g., geological carbon sequestration). Mathematical models describing flow through porous media while accounting for viscosity-pressure dependence are nonlinear (e.g., the Barus model). This nonlinearity complicates mathematical analysis and makes numerical solutions more time-intensive and prone to convergence issues. In this paper, we demonstrate that the Hopf-Cole transformation, originally developed for Burgers' equation, can recast the governing equations -- describing flow through porous media with pressure-dependent viscosity -- into a linear form. The transformed equations, resembling Darcy's equations in the transformed variables, enable (a) systematic mathematical analysis to establish uniqueness and maximum principles, (b) the derivation of a mechanics-based principle, and (c) the development of efficient numerical solutions using solvers optimized for Darcy equations. Notably, many properties of the linear Darcy equations naturally extend to nonlinear models that depend on pressure. For example, solutions to these nonlinear models adhere to a reciprocal relation analogous to that observed in Darcy's equations.","authors":["V. S. Maduri","K. B. Nakshatrala"],"url":"https://arxiv.org/abs/2504.21603"}
{"created":"2025-05-01","title":"Robust Misinformation Detection by Visiting Potential Commonsense Conflict","abstract":"The development of Internet technology has led to an increased prevalence of misinformation, causing severe negative effects across diverse domains. To mitigate this challenge, Misinformation Detection (MD), aiming to detect online misinformation automatically, emerges as a rapidly growing research topic in the community. In this paper, we propose a novel plug-and-play augmentation method for the MD task, namely Misinformation Detection with Potential Commonsense Conflict (MD-PCC). We take inspiration from the prior studies indicating that fake articles are more likely to involve commonsense conflict. Accordingly, we construct commonsense expressions for articles, serving to express potential commonsense conflicts inferred by the difference between extracted commonsense triplet and golden ones inferred by the well-established commonsense reasoning tool COMET. These expressions are then specified for each article as augmentation. Any specific MD methods can be then trained on those commonsense-augmented articles. Besides, we also collect a novel commonsense-oriented dataset named CoMis, whose all fake articles are caused by commonsense conflict. We integrate MD-PCC with various existing MD backbones and compare them across both 4 public benchmark datasets and CoMis. Empirical results demonstrate that MD-PCC can consistently outperform the existing MD baselines.","authors":["Bing Wang","Ximing Li","Changchun Li","Bingrui Zhao","Bo Fu","Renchu Guan","Shengsheng Wang"],"url":"https://arxiv.org/abs/2504.21604"}
{"created":"2025-05-01","title":"RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations","abstract":"Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study.","authors":["Jonas Gwozdz","Andreas Both"],"url":"https://arxiv.org/abs/2504.21605"}
{"created":"2025-05-01","title":"Measurement-Based Line-Impedance Estimation in the Absence of Phasor Measurement Units","abstract":"This paper proposes and compares experimentally several methods to estimate the series resistance and reactance (i.e., the transversal components of the $\\pi$-model of a line) of low-voltage lines in distribution grids. It first shows that if phasor measurements are available and the grid nodal voltages and power injections are known, the problem can be formulated and solved as a conventional load flow with properly adjusted unknowns. To solve this problem, we propose an analytical derivation of the Jacobian matrix. If only RMS values are available, such as from smart meters, integrating information from multiple intervals becomes necessary, ultimately opening to least-squares estimations, widely adopted in the literature. In this context, applying the proposed Jacobian contributes to accelerating the problem resolution of existing algorithms. The methods are compared in terms of estimation performance and convergence by using measurements from an experimental distribution grid interfacing real-world components and with realistic size implemented at the Gridlab at HES-SO Valais.","authors":["Plouton Grammatikos","Ali Mohamed Ali","Fabrizio Sossan"],"url":"https://arxiv.org/abs/2504.21606"}
{"created":"2025-05-01","title":"Applying Machine Learning for characterizing social networks Agent-based models","abstract":"Nowadays, social media networks are increasingly significant to our lives, the imperative to study social media networks becomes more and more essential. With billions of users across platforms and constant updates, the complexity of modeling social networks is immense. Agent-based modeling (ABM) is widely employed to study social networks community, allowing us to define individual behaviors and simulate system-level evolution. It can be a powerful tool to test how the algorithms affect users behavior. To fully leverage agent-based models,superior data processing and storage capabilities are essential. High Performance Computing (HPC) presents an optimal solution, adept at managing complex computations and analysis, particularly for voluminous or iteration-intensive tasks. We utilize Machine Learning (ML) methods to analyze social media users due to their ability to efficiently process vast amounts of data and derive insights that aid in understanding user behaviors, preferences, and trends. Therefore, our proposal involves ML to characterize user attributes and to develop a general user model for ABM simulation of in social networks on HPC systems.","authors":["Haoyuan Li","Lidia Conde Matos","Eduardo C\\'esar Galobardes","Anna Sikora"],"url":"https://arxiv.org/abs/2504.21609"}
{"created":"2025-05-01","title":"Mcity Data Engine: Iterative Model Improvement Through Open-Vocabulary Data Selection","abstract":"With an ever-increasing availability of data, it has become more and more challenging to select and label appropriate samples for the training of machine learning models. It is especially difficult to detect long-tail classes of interest in large amounts of unlabeled data. This holds especially true for Intelligent Transportation Systems (ITS), where vehicle fleets and roadside perception systems generate an abundance of raw data. While industrial, proprietary data engines for such iterative data selection and model training processes exist, researchers and the open-source community suffer from a lack of an openly available system. We present the Mcity Data Engine, which provides modules for the complete data-based development cycle, beginning at the data acquisition phase and ending at the model deployment stage. The Mcity Data Engine focuses on rare and novel classes through an open-vocabulary data selection process. All code is publicly available on GitHub under an MIT license: https://github.com/mcity/mcity_data_engine","authors":["Daniel Bogdoll","Rajanikant Patnaik Ananta","Abeyankar Giridharan","Isabel Moore","Gregory Stevens","Henry X. Liu"],"url":"https://arxiv.org/abs/2504.21614"}
{"created":"2025-05-01","title":"Overlapping data in network protocols: bridging OS and NIDS reassembly gap","abstract":"IPv4, IPv6, and TCP have a common mechanism allowing one to split an original data packet into several chunks. Such chunked packets may have overlapping data portions and, OS network stack implementations may reassemble these overlaps differently. A Network Intrusion Detection System (NIDS) that tries to reassemble a given flow data has to use the same reassembly policy as the monitored host OS; otherwise, the NIDS or the host may be subject to attack. In this paper, we provide several contributions that enable us to analyze NIDS resistance to overlapping data chunks-based attacks. First, we extend state-of-the-art insertion and evasion attack characterizations to address their limitations in an overlap-based context. Second, we propose a new way to model overlap types using Allen's interval algebra, a spatio-temporal reasoning. This new modeling allows us to formalize overlap test cases, which ensures exhaustiveness in overlap coverage and eases the reasoning about and use of reassembly policies. Third, we analyze the reassembly behavior of several OSes and NIDSes when processing the modeled overlap test cases. We show that 1) OS reassembly policies evolve over time and 2) all the tested NIDSes are (still) vulnerable to overlap-based evasion and insertion attacks.","authors":["Lucas Aubard","Johan Mazel","Gilles Guette","Pierre Chifflier"],"url":"https://arxiv.org/abs/2504.21618"}
{"created":"2025-05-01","title":"LRBO2: Improved 3D Vision Based Hand-Eye Calibration for Collaborative Robot Arm","abstract":"Hand-eye calibration is a common problem in the field of collaborative robotics, involving the determination of the transformation matrix between the visual sensor and the robot flange to enable vision-based robotic tasks. However, this process typically requires multiple movements of the robot arm and an external calibration object, making it both time-consuming and inconvenient, especially in scenarios where frequent recalibration is necessary. In this work, we extend our previous method, Look at Robot Base Once (LRBO), which eliminates the need for external calibration objects such as a chessboard. We propose a generic dataset generation approach for point cloud registration, focusing on aligning the robot base point cloud with the scanned data. Furthermore, a more detailed simulation study is conducted involving several different collaborative robot arms, followed by real-world experiments in an industrial setting. Our improved method is simulated and evaluated using a total of 14 robotic arms from 9 different brands, including KUKA, Universal Robots, UFACTORY, and Franka Emika, all of which are widely used in the field of collaborative robotics. Physical experiments demonstrate that our extended approach achieves performance comparable to existing commercial hand-eye calibration solutions, while completing the entire calibration procedure in just a few seconds. In addition, we provide a user-friendly hand-eye calibration solution, with the code publicly available at github.com/leihui6/LRBO2.","authors":["Leihui Li","Lixuepiao Wan","Volker Krueger","Xuping Zhang"],"url":"https://arxiv.org/abs/2504.21619"}
{"created":"2025-05-01","title":"Deterministic Distributed DFS via Cycle Separators in Planar Graphs","abstract":"One of the most basic techniques in algorithm design consists of breaking a problem into subproblems and then proceeding recursively. In the case of graph algorithms, one way to implement this approach is through separator sets. Given a graph $G=(V,E)$, a subset of nodes $S \\subseteq V$ is called a separator set of $G$ if the size of each connected component of $G-S$ is at most $2/3 \\cdot |V|$. The most useful separator sets are those that satisfy certain restrictions of cardinality or structure. For over 40 years, various efficient algorithms have been developed for computing separators of different kinds, particularly in planar graphs. Separator sets, combined with a divide and conquer approach, have been fundamental in the design of efficient algorithms in various settings.","authors":["Benjamin Jauregui","Pedro Montealegre","Ivan Rapaport"],"url":"https://arxiv.org/abs/2504.21620"}
{"created":"2025-05-01","title":"Path Planning on Multi-level Point Cloud with a Weighted Traversability Graph","abstract":"This article proposes a new path planning method for addressing multi-level terrain situations. The proposed method includes innovations in three aspects: 1) the pre-processing of point cloud maps with a multi-level skip-list structure and data-slimming algorithm for well-organized and simplified map formalization and management, 2) the direct acquisition of local traversability indexes through vehicle and point cloud interaction analysis, which saves work in surface fitting, and 3) the assignment of traversability indexes on a multi-level connectivity graph to generate a weighted traversability graph for generally search-based path planning. The A* algorithm is modified to utilize the traversability graph to generate a short and safe path. The effectiveness and reliability of the proposed method are verified through indoor and outdoor experiments conducted in various environments, including multi-floor buildings, woodland, and rugged mountainous regions. The results demonstrate that the proposed method can properly address 3D path planning problems for ground vehicles in a wide range of situations.","authors":["Yujie Tang","Quan Li","Hao Geng","Yangmin Xie","Hang Shi","Yusheng Yang"],"url":"https://arxiv.org/abs/2504.21622"}
{"created":"2025-05-01","title":"Multicut Problems in Almost-Planar Graphs: The Dependency of Complexity on the Demand Pattern","abstract":"Given a graph $G$, a set $T$ of terminal vertices, and a demand graph $H$ on $T$, the \\textsc{Multicut} problem asks for a set of edges of minimum weight that separates the pairs of terminals specified by the edges of $H$.","authors":["Florian H\\\"orsch","D\\'aniel Marx"],"url":"https://arxiv.org/abs/2504.21624"}
{"created":"2025-05-01","title":"Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability","abstract":"The ability to follow instructions accurately is fundamental for Large Language Models (LLMs) to serve as reliable agents in real-world applications. While existing instruction-following benchmarks are either single-turn or introduce new requirements in each turn without allowing self-correction, Meeseeks simulates realistic human-LLM interactions through an iterative feedback process. This design enables models to self-correct based on specific requirement failures, better reflecting real-world user-end usage patterns. The benchmark implements a comprehensive evaluation system with 38 capability tags organized across three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks provides valuable insights into LLMs' instruction-following capabilities in practical applications.","authors":["Jiaming Wang"],"url":"https://arxiv.org/abs/2504.21625"}
{"created":"2025-05-01","title":"LSNIF: Locally-Subdivided Neural Intersection Function","abstract":"Neural representations have shown the potential to accelerate ray casting in a conventional ray-tracing-based rendering pipeline. We introduce a novel approach called Locally-Subdivided Neural Intersection Function (LSNIF) that replaces bottom-level BVHs used as traditional geometric representations with a neural network. Our method introduces a sparse hash grid encoding scheme incorporating geometry voxelization, a scene-agnostic training data collection, and a tailored loss function. It enables the network to output not only visibility but also hit-point information and material indices. LSNIF can be trained offline for a single object, allowing us to use LSNIF as a replacement for its corresponding BVH. With these designs, the network can handle hit-point queries from any arbitrary viewpoint, supporting all types of rays in the rendering pipeline. We demonstrate that LSNIF can render a variety of scenes, including real-world scenes designed for other path tracers, while achieving a memory footprint reduction of up to 106.2x compared to a compressed BVH.","authors":["Shin Fujieda","Chih-Chen Kao","Takahiro Harada"],"url":"https://arxiv.org/abs/2504.21627"}
{"created":"2025-05-01","title":"Fast Sign Retrieval via Sub-band Convolution: An Elementary Extension of Binary Classification","abstract":"To efficiently compress the sign information of images, we address a sign retrieval problem for the block-wise discrete cosine transformation~(DCT): reconstruction of the signs of DCT coefficients from their amplitudes. To this end, we propose a fast sign retrieval method on the basis of binary classification machine learning. We first introduce 3D representations of the amplitudes and signs, where we pack amplitudes/signs belonging to the same frequency band into a 2D slice, referred to as the sub-band block. We then retrieve the signs from the 3D amplitudes via binary classification, where each sign is regarded as a binary label. We implement a binary classification algorithm using convolutional neural networks, which are advantageous for efficiently extracting features in the 3D amplitudes. Experimental results demonstrate that our method achieves accurate sign retrieval with an overwhelmingly low computation cost.","authors":["Fuma Ito","Chihiro Tsutake","Keita Takahashi","Toshiaki Fujii"],"url":"https://arxiv.org/abs/2504.21632"}
{"created":"2025-05-01","title":"Quantitative Auditing of AI Fairness with Differentially Private Synthetic Data","abstract":"Fairness auditing of AI systems can identify and quantify biases. However, traditional auditing using real-world data raises security and privacy concerns. It exposes auditors to security risks as they become custodians of sensitive information and targets for cyberattacks. Privacy risks arise even without direct breaches, as data analyses can inadvertently expose confidential information. To address these, we propose a framework that leverages differentially private synthetic data to audit the fairness of AI systems. By applying privacy-preserving mechanisms, it generates synthetic data that mirrors the statistical properties of the original dataset while ensuring privacy. This method balances the goal of rigorous fairness auditing and the need for strong privacy protections. Through experiments on real datasets like Adult, COMPAS, and Diabetes, we compare fairness metrics of synthetic and real data. By analyzing the alignment and discrepancies between these metrics, we assess the capacity of synthetic data to preserve the fairness properties of real data. Our results demonstrate the framework's ability to enable meaningful fairness evaluations while safeguarding sensitive information, proving its applicability across critical and sensitive domains.","authors":["Chih-Cheng Rex Yuan","Bow-Yaw Wang"],"url":"https://arxiv.org/abs/2504.21634"}
{"created":"2025-05-01","title":"Sadeed: Advancing Arabic Diacritization Through Small Language Model","abstract":"Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.","authors":["Zeina Aldallal","Sara Chrouf","Khalil Hennara","Mohamed Motaism Hamed","Muhammad Hreden","Safwan AlModhayan"],"url":"https://arxiv.org/abs/2504.21635"}
{"created":"2025-05-01","title":"Sparsity for Infinite-Parametric Holomorphic Functions on Gaussian Spaces","abstract":"We investigate the sparsity of Wiener polynomial chaos expansions of holomorphic maps $\\mathcal{G}$ on Gaussian Hilbert spaces, as arise in the coefficient-to-solution maps of linear, second order, divergence-form elliptic PDEs with log-Gaussian diffusion coefficient. Representing the Gaussian random field input as an affine-parametric expansion, the nonlinear map becomes a countably-parametric, deterministic holomorphic map of the coordinate sequence $\\boldsymbol{y} = (y_j)_{j\\in\\mathbb{N}} \\in \\mathbb{R}^\\infty$. We establish weighted summability results for the Wiener-Hermite coefficient sequences of images of affine-parametric expansions of the log-Gaussian input under $\\mathcal{G}$. These results give rise to $N$-term approximation rate bounds for the full range of input summability exponents $p\\in (0,2)$. We show that these approximation rate bounds apply to parameter-to-solution maps for elliptic diffusion PDEs with lognormal coefficients.","authors":["Carlo Marcati","Christoph Schwab","Jakob Zech"],"url":"https://arxiv.org/abs/2504.21639"}
{"created":"2025-05-01","title":"Designing Control Barrier Function via Probabilistic Enumeration for Safe Reinforcement Learning Navigation","abstract":"Achieving safe autonomous navigation systems is critical for deploying robots in dynamic and uncertain real-world environments. In this paper, we propose a hierarchical control framework leveraging neural network verification techniques to design control barrier functions (CBFs) and policy correction mechanisms that ensure safe reinforcement learning navigation policies. Our approach relies on probabilistic enumeration to identify unsafe regions of operation, which are then used to construct a safe CBF-based control layer applicable to arbitrary policies. We validate our framework both in simulation and on a real robot, using a standard mobile robot benchmark and a highly dynamic aquatic environmental monitoring task. These experiments demonstrate the ability of the proposed solution to correct unsafe actions while preserving efficient navigation behavior. Our results show the promise of developing hierarchical verification-based systems to enable safe and robust navigation behaviors in complex scenarios.","authors":["Luca Marzari","Francesco Trotti","Enrico Marchesini","Alessandro Farinelli"],"url":"https://arxiv.org/abs/2504.21643"}
{"created":"2025-05-01","title":"Diffusion-based Adversarial Identity Manipulation for Facial Privacy Protection","abstract":"The success of face recognition (FR) systems has led to serious privacy concerns due to potential unauthorized surveillance and user tracking on social networks. Existing methods for enhancing privacy fail to generate natural face images that can protect facial privacy. In this paper, we propose diffusion-based adversarial identity manipulation (DiffAIM) to generate natural and highly transferable adversarial faces against malicious FR systems. To be specific, we manipulate facial identity within the low-dimensional latent space of a diffusion model. This involves iteratively injecting gradient-based adversarial identity guidance during the reverse diffusion process, progressively steering the generation toward the desired adversarial faces. The guidance is optimized for identity convergence towards a target while promoting semantic divergence from the source, facilitating effective impersonation while maintaining visual naturalness. We further incorporate structure-preserving regularization to preserve facial structure consistency during manipulation. Extensive experiments on both face verification and identification tasks demonstrate that compared with the state-of-the-art, DiffAIM achieves stronger black-box attack transferability while maintaining superior visual quality. We also demonstrate the effectiveness of the proposed approach for commercial FR APIs, including Face++ and Aliyun.","authors":["Liqin Wang","Qianyue Hu","Wei Lu","Xiangyang Luo"],"url":"https://arxiv.org/abs/2504.21646"}
{"created":"2025-05-01","title":"HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation","abstract":"The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.","authors":["Haiyang Zhou","Wangbo Yu","Jiawen Guan","Xinhua Cheng","Yonghong Tian","Li Yuan"],"url":"https://arxiv.org/abs/2504.21650"}
{"created":"2025-05-01","title":"DBSCAN-based Vehicle Clustering and UAV Placement for NOMA-based Resource Management in Cellular V2X Communications","abstract":"In the future wireless networks, terrestrial, aerial, space, and maritime wireless networks are integrated into a unified network to meet the needs of a fully connected global network. Nowadays, vehicular communication has become one of the challenging applications of wireless networks. In this article, we aim to address the radio resource management in Cellular V2X (C-V2X) networks using Unmanned Aerial Vehicles (UAV) and Non-orthogonal multiple access (NOMA). The goal of this problem is to maximize the spectral efficiency of vehicular users in Cellular Vehicle-to-Everything (C-V2X) networks under a fronthaul constraint. To solve this problem, a two-stage approach is utilized. In the first stage, vehicles in dense area are clustered based on their geographical locations, predicted location of vehicles, and speeds. Then UAVs are deployed to serve the clusters. In the second stage, NOMA groups are formed within each cluster and radio resources are allocated to vehicles based on NOMA groups. An optimization problem is formulated and a suboptimal method is used to solve it. The performance of the proposed method is evaluated through simulations where results demonstrate superiority of proposed method in spectral efficiency, min point, and distance.","authors":["Hossein Davoudi","Behrouz Shahgholi Ghahfarokhi","Neda Moghim","Sachin Shetty"],"url":"https://arxiv.org/abs/2504.21656"}
{"created":"2025-05-01","title":"A p-adaptive polytopal discontinuous Galerkin method for high-order approximation of brain electrophysiology","abstract":"Multiscale mathematical models have shown great promise in computational brain electrophysiology but are still hindered by high computational costs due to fast dynamics and complex brain geometries, requiring very fine spatio-temporal resolution. This paper introduces a novel p-adaptive discontinuous Galerkin method on polytopal grids (PolyDG) coupled with Crank-Nicolson time integration to approximate such models efficiently. The p-adaptive method enhances local accuracy via dynamic, element-wise polynomial refinement/de-refinement guided by a-posteriori error estimators. A novel clustering algorithm automatizes the selection of elements for adaptive updates, further improving efficiency. A wide set of numerical tests, including epileptic seizure simulations in a sagittal section of a human brain stem, demonstrate the method's ability to reduce computational load while maintaining the accuracy of the numerical solution in capturing the dynamics of multiple wavefronts.","authors":["Caterina Beatrice Leimer Saglio","Stefano Pagani","Paola F. Antonietti"],"url":"https://arxiv.org/abs/2504.21657"}
{"created":"2025-05-01","title":"Approximation and regularity results for the Heston model and related processes","abstract":"This Ph.D. thesis explores approximations and regularity for the Heston stochastic volatility model through three interconnected works.","authors":["Edoardo Lombardo"],"url":"https://arxiv.org/abs/2504.21658"}
{"created":"2025-05-01","title":"AdaR1: From Long-CoT to Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization","abstract":"Recently, long-thought reasoning models achieve strong performance on complex reasoning tasks, but often incur substantial inference overhead, making efficiency a critical concern. Our empirical analysis reveals that the benefit of using Long-CoT varies across problems: while some problems require elaborate reasoning, others show no improvement, or even degraded accuracy. This motivates adaptive reasoning strategies that tailor reasoning depth to the input. However, prior work primarily reduces redundancy within long reasoning paths, limiting exploration of more efficient strategies beyond the Long-CoT paradigm. To address this, we propose a novel two-stage framework for adaptive and efficient reasoning. First, we construct a hybrid reasoning model by merging long and short CoT models to enable diverse reasoning styles. Second, we apply bi-level preference training to guide the model to select suitable reasoning styles (group-level), and prefer concise and correct reasoning within each style group (instance-level). Experiments demonstrate that our method significantly reduces inference costs compared to other baseline approaches, while maintaining performance. Notably, on five mathematical datasets, the average length of reasoning is reduced by more than 50%, highlighting the potential of adaptive strategies to optimize reasoning efficiency in large language models. Our code is coming soon at https://github.com/StarDewXXX/AdaR1","authors":["Haotian Luo","Haiying He","Yibo Wang","Jinluan Yang","Rui Liu","Naiqiang Tan","Xiaochun Cao","Dacheng Tao","Li Shen"],"url":"https://arxiv.org/abs/2504.21659"}
{"created":"2025-05-01","title":"Probabilistic Time Series Forecasting of Residential Loads -- A Copula Approach","abstract":"Predicting the time series of future evolutions of renewable injections and demands is of utmost importance for the operation of power systems. However, the current state of the art is mostly focused on mean-value time series predictions and only very few methods provide probabilistic forecasts. In this paper, we rely on kernel density estimation and vine copulas to construct probabilistic models for individual load profiles of private households. Our approach allows the quantification of variability of individual energy consumption in general and of daily peak loads in particular. We draw upon an Australian distribution grid dataset to illustrate our findings. We generate synthetic loads that follow the distribution of the real data.","authors":["Marco Jeschke","Timm Faulwasser","Roland Fried"],"url":"https://arxiv.org/abs/2504.21661"}
{"created":"2025-05-01","title":"On Advancements of the Forward-Forward Algorithm","abstract":"The Forward-Forward algorithm has evolved in machine learning research, tackling more complex tasks that mimic real-life applications. In the last years, it has been improved by several techniques to perform better than its original version, handling a challenging dataset like CIFAR10 without losing its flexibility and low memory usage. We have shown in our results that improvements are achieved through a combination of convolutional channel grouping, learning rate schedules, and independent block structures during training that lead to a 20\\% decrease in test error percentage. Additionally, to approach further implementations on low-capacity hardware projects we have presented a series of lighter models that achieve low test error percentages within (21$\\pm$6)\\% and number of trainable parameters between 164,706 and 754,386. This serving also as a basis for our future study on complete verification and validation of these kinds of neural networks.","authors":["Mauricio Ortiz Torres","Markus Lange","Arne P. Raulf"],"url":"https://arxiv.org/abs/2504.21662"}
{"created":"2025-05-01","title":"From Precision to Perception: User-Centred Evaluation of Keyword Extraction Algorithms for Internet-Scale Contextual Advertising","abstract":"Keyword extraction is a foundational task in natural language processing, underpinning countless real-world applications. A salient example is contextual advertising, where keywords help predict the topical congruence between ads and their surrounding media contexts to enhance advertising effectiveness. Recent advances in artificial intelligence, particularly large language models, have improved keyword extraction capabilities but also introduced concerns about computational cost. Moreover, although the end-user experience is of vital importance, human evaluation of keyword extraction performances remains under-explored. This study provides a comparative evaluation of three prevalent keyword extraction algorithms that vary in complexity: TF-IDF, KeyBERT, and Llama 2. To evaluate their effectiveness, a mixed-methods approach is employed, combining quantitative benchmarking with qualitative assessments from 552 participants through three survey-based experiments. Findings indicate a slight user preference for KeyBERT, which offers a favourable balance between performance and computational efficiency compared to the other two algorithms. Despite a strong overall preference for gold-standard keywords, differences between the algorithmic outputs are not statistically significant, highlighting a long-overlooked gap between traditional precision-focused metrics and user-perceived algorithm efficiency. The study highlights the importance of user-centred evaluation methodologies and proposes analytical tools to support their implementation.","authors":["Jingwen Cai","Sara Leckner","Johanna Bj\\\"orklund"],"url":"https://arxiv.org/abs/2504.21667"}
{"created":"2025-05-01","title":"Traceback of Poisoning Attacks to Retrieval-Augmented Generation","abstract":"Large language models (LLMs) integrated with retrieval-augmented generation (RAG) systems improve accuracy by leveraging external knowledge sources. However, recent research has revealed RAG's susceptibility to poisoning attacks, where the attacker injects poisoned texts into the knowledge database, leading to attacker-desired responses. Existing defenses, which predominantly focus on inference-time mitigation, have proven insufficient against sophisticated attacks. In this paper, we introduce RAGForensics, the first traceback system for RAG, designed to identify poisoned texts within the knowledge database that are responsible for the attacks. RAGForensics operates iteratively, first retrieving a subset of texts from the database and then utilizing a specially crafted prompt to guide an LLM in detecting potential poisoning texts. Empirical evaluations across multiple datasets demonstrate the effectiveness of RAGForensics against state-of-the-art poisoning attacks. This work pioneers the traceback of poisoned texts in RAG systems, providing a practical and promising defense mechanism to enhance their security.","authors":["Baolei Zhang","Haoran Xin","Minghong Fang","Zhuqing Liu","Biao Yi","Tong Li","Zheli Liu"],"url":"https://arxiv.org/abs/2504.21668"}
{"created":"2025-05-01","title":"Complexities of Well-Quasi-Ordered Substructural Logics","abstract":"Substructural logics are formal logical systems that omit familiar structural rules of classical and intuitionistic logic such as contraction, weakening, exchange (commutativity), and associativity. This leads to a resource-sensitive logical framework that has proven influential beyond mathematical logic and its algebraic semantics, across theoretical computer science, linguistics, and philosophical logic. The set of theorems of a substructural logic is recursively enumerable and, in many cases, recursive. These logics also possess an intricate mathematical structure that has been the subject of research for over six decades.","authors":["Nikolaos Galatos","Vitor Greati","Revantha Ramanayake","Gavin St. John"],"url":"https://arxiv.org/abs/2504.21674"}
{"created":"2025-05-01","title":"Elimination Distance to Dominated Clusters","abstract":"In the Dominated Cluster Deletion problem we are given an undirected graph $G$ and integers $k$ and $d$ and the question is to decide whether there exists a set of at most $k$ vertices whose removal results in a graph in which each connected component has a dominating set of size at most $d$. In the Elimination Distance to Dominated Clusters problem we are again given an undirected graph $G$ and integers $k$ and $d$ and the question is to decide whether we can recursively delete vertices up to depth $k$ such that each remaining connected component has a dominating set of size at most $d$. Bentert et al.~[Bentert et al., MFCS 2024] recently provided an almost complete classification of the parameterized complexity of Dominated Cluster Deletion with respect to the parameters $k$, $d$, $c$, and $\\Delta$, where $c$ and $\\Delta$ are the degeneracy, and the maximum degree of the input graph, respectively. In particular, they provided a non-uniform algorithm with running time $f(k,d)\\cdot n^{O(d)}$. They left as an open problem whether the problem is fixed-parameter tractable with respect to the parameter $k+d+c$. We provide a uniform algorithm running in time $f(k,d)\\cdot n^{O(d)}$ for both Dominated Cluster Deletion and Elimination Distance to Dominated Clusters. We furthermore show that both problems are FPT when parameterized by $k+d+\\ell$, where $\\ell$ is the semi-ladder index of the input graph, a parameter that is upper bounded and may be much smaller than the degeneracy $c$, positively answering the open question of Bentert et al. We almost complete the picture by providing an almost full classification for the parameterized complexity and kernelization complexity of Elimination Distance to Dominated Clusters. The one difficult base case that remains open is whether treedepth (the case $d=0$) is NP-hard on graphs of bounded maximum degree.","authors":["Nicole Schirrmacher","Sebastian Siebertz","Alexandre Vigny"],"url":"https://arxiv.org/abs/2504.21675"}
{"created":"2025-05-01","title":"20min-XD: A Comparable Corpus of Swiss News Articles","abstract":"We present 20min-XD (20 Minuten cross-lingual document-level), a French-German, document-level comparable corpus of news articles, sourced from the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises around 15,000 article pairs spanning 2015 to 2024, automatically aligned based on semantic similarity. We detail the data collection process and alignment methodology. Furthermore, we provide a qualitative and quantitative analysis of the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual similarity, ranging from near-translations to loosely related articles, making it valuable for various NLP applications and broad linguistically motivated studies. We publicly release the dataset in document- and sentence-aligned versions and code for the described experiments.","authors":["Michelle Wastl","Jannis Vamvas","Selena Calleri","Rico Sennrich"],"url":"https://arxiv.org/abs/2504.21677"}
{"created":"2025-05-01","title":"Canonicalization for Unreproducible Builds in Java","abstract":"The increasing complexity of software supply chains and the rise of supply chain attacks have elevated concerns around software integrity. Users and stakeholders face significant challenges in validating that a given software artifact corresponds to its declared source. Reproducible Builds address this challenge by ensuring that independently performed builds from identical source code produce identical binaries. However, achieving reproducibility at scale remains difficult, especially in Java, due to a range of non-deterministic factors and caveats in the build process. In this work, we focus on reproducibility in Java-based software, archetypal of enterprise applications. We introduce a conceptual framework for reproducible builds, we analyze a large dataset from Reproducible Central, and we develop a novel taxonomy of six root causes of unreproducibility. We study actionable mitigations: artifact and bytecode canonicalization using OSS-Rebuild and jNorm respectively. Finally, we present Chains-Rebuild, a tool that raises reproducibility success from 9.48% to 26.89% on 12,283 unreproducible artifacts. To sum up, our contributions are the first large-scale taxonomy of build unreproducibility causes in Java, a publicly available dataset of unreproducible builds, and Chains-Rebuild, a canonicalization tool for mitigating unreproducible builds in Java.","authors":["Aman Sharma","Benoit Baudry","Martin Monperrus"],"url":"https://arxiv.org/abs/2504.21679"}
{"created":"2025-05-01","title":"Hoist with His Own Petard: Inducing Guardrails to Facilitate Denial-of-Service Attacks on Retrieval-Augmented Generation of LLMs","abstract":"Retrieval-Augmented Generation (RAG) integrates Large Language Models (LLMs) with external knowledge bases, improving output quality while introducing new security risks. Existing studies on RAG vulnerabilities typically focus on exploiting the retrieval mechanism to inject erroneous knowledge or malicious texts, inducing incorrect outputs. However, these approaches overlook critical weaknesses within LLMs, leaving important attack vectors unexplored and limiting the scope and efficiency of attacks. In this paper, we uncover a novel vulnerability: the safety guardrails of LLMs, while designed for protection, can also be exploited as an attack vector by adversaries. Building on this vulnerability, we propose MutedRAG, a novel denial-of-service attack that reversely leverages the guardrails of LLMs to undermine the availability of RAG systems. By injecting minimalistic jailbreak texts, such as \"\\textit{How to build a bomb}\", into the knowledge base, MutedRAG intentionally triggers the LLM's safety guardrails, causing the system to reject legitimate queries. Besides, due to the high sensitivity of guardrails, a single jailbreak sample can affect multiple queries, effectively amplifying the efficiency of attacks while reducing their costs. Experimental results on three datasets demonstrate that MutedRAG achieves an attack success rate exceeding 60% in many scenarios, requiring only less than one malicious text to each target query on average. In addition, we evaluate potential defense strategies against MutedRAG, finding that some of current mechanisms are insufficient to mitigate this threat, underscoring the urgent need for more robust solutions.","authors":["Pan Suo","Yu-Ming Shang","San-Chuan Guo","Xi Zhang"],"url":"https://arxiv.org/abs/2504.21680"}
{"created":"2025-05-01","title":"Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders","abstract":"Most pre-trained Vision-Language (VL) models and training data for the downstream tasks are only available in English. Therefore, multilingual VL tasks are solved using cross-lingual transfer: fine-tune a multilingual pre-trained model or transfer the text encoder using parallel data. We study the alternative approach: transferring an already trained encoder using parallel data. We investigate the effect of parallel data: domain and the number of languages, which were out of focus in previous work. Our results show that even machine-translated task data are the best on average, caption-like authentic parallel data outperformed it in some languages. Further, we show that most languages benefit from multilingual training.","authors":["Andrei-Alexandru Manea","Jind\\v{r}ich Libovick\\'y"],"url":"https://arxiv.org/abs/2504.21681"}
{"created":"2025-05-01","title":"Visual Text Processing: A Comprehensive Review and Unified Evaluation","abstract":"Visual text is a crucial component in both document and scene images, conveying rich semantic information and attracting significant attention in the computer vision community. Beyond traditional tasks such as text detection and recognition, visual text processing has witnessed rapid advancements driven by the emergence of foundation models, including text image reconstruction and text image manipulation. Despite significant progress, challenges remain due to the unique properties that differentiate text from general objects. Effectively capturing and leveraging these distinct textual characteristics is essential for developing robust visual text processing models. In this survey, we present a comprehensive, multi-perspective analysis of recent advancements in visual text processing, focusing on two key questions: (1) What textual features are most suitable for different visual text processing tasks? (2) How can these distinctive text features be effectively incorporated into processing frameworks? Furthermore, we introduce VTPBench, a new benchmark that encompasses a broad range of visual text processing datasets. Leveraging the advanced visual quality assessment capabilities of multimodal large language models (MLLMs), we propose VTPScore, a novel evaluation metric designed to ensure fair and reliable evaluation. Our empirical study with more than 20 specific models reveals substantial room for improvement in the current techniques. Our aim is to establish this work as a fundamental resource that fosters future exploration and innovation in the dynamic field of visual text processing. The relevant repository is available at https://github.com/shuyansy/Visual-Text-Processing-survey.","authors":["Yan Shu","Weichao Zeng","Fangmin Zhao","Zeyu Chen","Zhenhang Li","Xiaomeng Yang","Yu Zhou","Paolo Rota","Xiang Bai","Lianwen Jin","Xu-Cheng Yin","Nicu Sebe"],"url":"https://arxiv.org/abs/2504.21682"}
{"created":"2025-05-01","title":"Extension-ranking Semantics for Abstract Argumentation Preprint","abstract":"In this paper, we present a general framework for ranking sets of arguments in abstract argumentation based on their plausibility of acceptance. We present a generalisation of Dung's extension semantics as extension-ranking semantics, which induce a preorder over the power set of all arguments, allowing us to state that one set is \"closer\" to being acceptable than another. To evaluate the extension-ranking semantics, we introduce a number of principles that a well-behaved extension-ranking semantics should satisfy. We consider several simple base relations, each of which models a single central aspect of argumentative reasoning. The combination of these base relations provides us with a family of extension-ranking semantics. We also adapt a number of approaches from the literature for ranking extensions to be usable in the context of extension-ranking semantics, and evaluate their behaviour.","authors":["Kenneth Skiba","Tjitze Rienstra","Matthias Thimm","Jesse Heyninck","Gabriele Kern-Isberner"],"url":"https://arxiv.org/abs/2504.21683"}
{"created":"2025-05-01","title":"Using quantum annealing to generate test cases for cyber-physical systems","abstract":"Quantum computing has emerged as a powerful tool to efficiently solve computational challenges, particularly in simulation and optimisation. However, hardware limitations prevent quantum computers from achieving the full theoretical potential. Among the quantum algorithms, quantum annealing is a prime candidate to solve optimisation problems. This makes it a natural candidate for search-based software testing in the Cyber-Physical Systems (CPS) domain, which demands effective test cases due to their safety-critical nature. This work explores the use of quantum annealing to enhance test case generation for CPS through a mutation-based approach. We encode test case mutation as a binary optimisation problem, and use quantum annealing to identify and target critical regions of the test cases for improvement. Our approach mechanises this process into an algorithm that uses D-Wave's quantum annealer to find the solution. As a main contribution, we offer insights into how quantum annealing can advance software testing methodologies by empirically evaluating the correlation between problem size, hardware limitations, and the effectiveness of the results. Moreover, we compare the proposed method against state-of-the-art classical optimisation algorithms, targeting efficiency (time to generate test cases) and effectiveness (fault detection rates). Results indicate that quantum annealing enables faster test case generation while achieving comparable fault detection performance to state-of-the-art alternatives.","authors":["Hugo Araujo","Xinyi Wang","Mohammad Mousavi","Shaukat Ali"],"url":"https://arxiv.org/abs/2504.21684"}
{"created":"2025-05-01","title":"Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning","abstract":"Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency.","authors":["Reem Abdel-Salam","Mary Adewunmi"],"url":"https://arxiv.org/abs/2504.21685"}
{"created":"2025-05-01","title":"Enhancing Self-Supervised Fine-Grained Video Object Tracking with Dynamic Memory Prediction","abstract":"Successful video analysis relies on accurate recognition of pixels across frames, and frame reconstruction methods based on video correspondence learning are popular due to their efficiency. Existing frame reconstruction methods, while efficient, neglect the value of direct involvement of multiple reference frames for reconstruction and decision-making aspects, especially in complex situations such as occlusion or fast movement. In this paper, we introduce a Dynamic Memory Prediction (DMP) framework that innovatively utilizes multiple reference frames to concisely and directly enhance frame reconstruction. Its core component is a Reference Frame Memory Engine that dynamically selects frames based on object pixel features to improve tracking accuracy. In addition, a Bidirectional Target Prediction Network is built to utilize multiple reference frames to improve the robustness of the model. Through experiments, our algorithm outperforms the state-of-the-art self-supervised techniques on two fine-grained video object tracking tasks: object segmentation and keypoint tracking.","authors":["Zihan Zhou","Changrui Dai","Aibo Song","Xiaolin Fang"],"url":"https://arxiv.org/abs/2504.21692"}
{"created":"2025-05-01","title":"Automatic Mapping of AutomationML Files to Ontologies for Graph Queries and Validation","abstract":"AutomationML has seen widespread adoption as an open data exchange format in the automation domain. It is an open and vendor neutral standard based on the extensible markup language XML. However, AutomationML extends XML with additional semantics, that limit the applicability of common XML-tools for applications like querying or data validation. This article provides practitioners with 1) an up-to-date ontology of the concepts in the AutomationML-standard, as well as 2) a declarative mapping to automatically transform any AutomationML model into RDF triples. Together, these artifacts allow practitioners an easy integration of AutomationML information into industrial knowledge graphs. A study on examples from the automation domain concludes that transforming AutomationML to OWL opens up new powerful ways for querying and validation that are impossible without transformation.","authors":["Tom Westermann","Malte Ramonat","Johannes Hujer","Felix Gehlhoff","Alexander Fay"],"url":"https://arxiv.org/abs/2504.21694"}
{"created":"2025-05-01","title":"Self-Supervised Monocular Visual Drone Model Identification through Improved Occlusion Handling","abstract":"Ego-motion estimation is vital for drones when flying in GPS-denied environments. Vision-based methods struggle when flight speed increases and close-by objects lead to difficult visual conditions with considerable motion blur and large occlusions. To tackle this, vision is typically complemented by state estimation filters that combine a drone model with inertial measurements. However, these drone models are currently learned in a supervised manner with ground-truth data from external motion capture systems, limiting scalability to different environments and drones. In this work, we propose a self-supervised learning scheme to train a neural-network-based drone model using only onboard monocular video and flight controller data (IMU and motor feedback). We achieve this by first training a self-supervised relative pose estimation model, which then serves as a teacher for the drone model. To allow this to work at high speed close to obstacles, we propose an improved occlusion handling method for training self-supervised pose estimation models. Due to this method, the root mean squared error of resulting odometry estimates is reduced by an average of 15%. Moreover, the student neural drone model can be successfully obtained from the onboard data. It even becomes more accurate at higher speeds compared to its teacher, the self-supervised vision-based model. We demonstrate the value of the neural drone model by integrating it into a traditional filter-based VIO system (ROVIO), resulting in superior odometry accuracy on aggressive 3D racing trajectories near obstacles. Self-supervised learning of ego-motion estimation represents a significant step toward bridging the gap between flying in controlled, expensive lab environments and real-world drone applications. The fusion of vision and drone models will enable higher-speed flight and improve state estimation, on any drone in any environment.","authors":["Stavrow A. Bahnam","Christophe De Wagter","Guido C. H. E. de Croon"],"url":"https://arxiv.org/abs/2504.21695"}
{"created":"2025-05-01","title":"REHEARSE-3D: A Multi-modal Emulated Rain Dataset for 3D Point Cloud De-raining","abstract":"Sensor degradation poses a significant challenge in autonomous driving. During heavy rainfall, the interference from raindrops can adversely affect the quality of LiDAR point clouds, resulting in, for instance, inaccurate point measurements. This, in turn, can potentially lead to safety concerns if autonomous driving systems are not weather-aware, i.e., if they are unable to discern such changes. In this study, we release a new, large-scale, multi-modal emulated rain dataset, REHEARSE-3D, to promote research advancements in 3D point cloud de-raining. Distinct from the most relevant competitors, our dataset is unique in several respects. First, it is the largest point-wise annotated dataset, and second, it is the only one with high-resolution LiDAR data (LiDAR-256) enriched with 4D Radar point clouds logged in both daytime and nighttime conditions in a controlled weather environment. Furthermore, REHEARSE-3D involves rain-characteristic information, which is of significant value not only for sensor noise modeling but also for analyzing the impact of weather at a point level. Leveraging REHEARSE-3D, we benchmark raindrop detection and removal in fused LiDAR and 4D Radar point clouds. Our comprehensive study further evaluates the performance of various statistical and deep-learning models. Upon publication, the dataset and benchmark models will be made publicly available at: https://sporsho.github.io/REHEARSE3D.","authors":["Abu Mohammed Raisuddin","Jesper Holmblad","Hamed Haghighi","Yuri Poledna","Maikol Funk Drechsler","Valentina Donzella","Eren Erdal Aksoy"],"url":"https://arxiv.org/abs/2504.21699"}
{"created":"2025-05-01","title":"XBreaking: Explainable Artificial Intelligence for Jailbreaking LLMs","abstract":"Large Language Models are fundamental actors in the modern IT landscape dominated by AI solutions. However, security threats associated with them might prevent their reliable adoption in critical application scenarios such as government organizations and medical institutions. For this reason, commercial LLMs typically undergo a sophisticated censoring mechanism to eliminate any harmful output they could possibly produce. In response to this, LLM Jailbreaking is a significant threat to such protections, and many previous approaches have already demonstrated its effectiveness across diverse domains. Existing jailbreak proposals mostly adopt a generate-and-test strategy to craft malicious input. To improve the comprehension of censoring mechanisms and design a targeted jailbreak attack, we propose an Explainable-AI solution that comparatively analyzes the behavior of censored and uncensored models to derive unique exploitable alignment patterns. Then, we propose XBreaking, a novel jailbreak attack that exploits these unique patterns to break the security constraints of LLMs by targeted noise injection. Our thorough experimental campaign returns important insights about the censoring mechanisms and demonstrates the effectiveness and performance of our attack.","authors":["Marco Arazzi","Vignesh Kumar Kembu","Antonino Nocera","Vinod P"],"url":"https://arxiv.org/abs/2504.21700"}
{"created":"2025-05-01","title":"A Conversational Approach to Well-being Awareness Creation and Behavioural Intention","abstract":"The promotion of a healthy lifestyle is one of the main drivers of an individual's overall physical and psycho-emotional well-being. Digital technologies are more and more adopted as ''facilitators'' for this goal, to raise awareness and solicit healthy lifestyle habits.","authors":["Antonia Azzini","Ilaria Baroni","Irene Celino"],"url":"https://arxiv.org/abs/2504.21702"}
{"created":"2025-05-01","title":"Vision Transformers in Precision Agriculture: A Comprehensive Survey","abstract":"Detecting plant diseases is a crucial aspect of modern agriculture - it plays a key role in maintaining crop health and increasing overall yield. Traditional approaches, though still valuable, often rely on manual inspection or conventional machine learning techniques, both of which face limitations in scalability and accuracy. Recently, Vision Transformers (ViTs) have emerged as a promising alternative, offering benefits such as improved handling of long-range dependencies and better scalability for visual tasks. This survey explores the application of ViTs in precision agriculture, covering tasks from classification to detection and segmentation. We begin by introducing the foundational architecture of ViTs and discuss their transition from Natural Language Processing (NLP) to computer vision. The discussion includes the concept of inductive bias in traditional models like Convolutional Neural Networks (CNNs), and how ViTs mitigate these biases. We provide a comprehensive review of recent literature, focusing on key methodologies, datasets, and performance metrics. The survey also includes a comparative analysis of CNNs and ViTs, with a look at hybrid models and performance enhancements. Technical challenges - such as data requirements, computational demands, and model interpretability - are addressed alongside potential solutions. Finally, we outline potential research directions and technological advancements that could further support the integration of ViTs in real-world agricultural settings. Our goal with this study is to offer practitioners and researchers a deeper understanding of how ViTs are poised to transform smart and precision agriculture.","authors":["Saber Mehdipour","Seyed Abolghasem Mirroshandel","Seyed Amirhossein Tabatabaei"],"url":"https://arxiv.org/abs/2504.21706"}
{"created":"2025-05-01","title":"Recursive KL Divergence Optimization: A Dynamic Framework for Representation Learning","abstract":"We propose a generalization of modern representation learning objectives by reframing them as recursive divergence alignment processes over localized conditional distributions While recent frameworks like Information Contrastive Learning I-Con unify multiple learning paradigms through KL divergence between fixed neighborhood conditionals we argue this view underplays a crucial recursive structure inherent in the learning process. We introduce Recursive KL Divergence Optimization RKDO a dynamic formalism where representation learning is framed as the evolution of KL divergences across data neighborhoods. This formulation captures contrastive clustering and dimensionality reduction methods as static slices while offering a new path to model stability and local adaptation. Our experiments demonstrate that RKDO offers dual efficiency advantages approximately 30 percent lower loss values compared to static approaches across three different datasets and 60 to 80 percent reduction in computational resources needed to achieve comparable results. This suggests that RKDOs recursive updating mechanism provides a fundamentally more efficient optimization landscape for representation learning with significant implications for resource constrained applications.","authors":["Anthony D Martin"],"url":"https://arxiv.org/abs/2504.21707"}
{"created":"2025-05-01","title":"Computing Polynomial Representation in Subrings of Multivariate Polynomial Rings","abstract":"Let $\\mathcal{R} = \\mathbb{K}[x_1, \\dots, x_n]$ be a multivariate polynomial ring over a field $\\mathbb{K}$ of characteristic 0. Consider $n$ algebraically independent elements $g_1, \\dots, g_n$ in $\\mathcal{R}$. Let $\\mathcal{S}$ denote the subring of $\\mathcal{R}$ generated by $g_1, \\dots, g_n$, and let $h$ be an element of $\\mathcal{S}$. Then, there exists a unique element ${f} \\in \\mathbb{K}[u_1, \\dots, u_n]$ such that $h = f(g_1, \\dots, g_n)$.","authors":["Thi Xuan Vu"],"url":"https://arxiv.org/abs/2504.21708"}
{"created":"2025-05-01","title":"LLM-Empowered Embodied Agent for Memory-Augmented Task Planning in Household Robotics","abstract":"We present an embodied robotic system with an LLM-driven agent-orchestration architecture for autonomous household object management. The system integrates memory-augmented task planning, enabling robots to execute high-level user commands while tracking past actions. It employs three specialized agents: a routing agent, a task planning agent, and a knowledge base agent, each powered by task-specific LLMs. By leveraging in-context learning, our system avoids the need for explicit model training. RAG enables the system to retrieve context from past interactions, enhancing long-term object tracking. A combination of Grounded SAM and LLaMa3.2-Vision provides robust object detection, facilitating semantic scene understanding for task planning. Evaluation across three household scenarios demonstrates high task planning accuracy and an improvement in memory recall due to RAG. Specifically, Qwen2.5 yields best performance for specialized agents, while LLaMA3.1 excels in routing tasks. The source code is available at: https://github.com/marc1198/chat-hsr.","authors":["Marc Glocker","Peter H\\\"onig","Matthias Hirschmanner","Markus Vincze"],"url":"https://arxiv.org/abs/2504.21716"}
{"created":"2025-05-01","title":"VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction","abstract":"Generating responsive listener head dynamics with nuanced emotions and expressive reactions is crucial for practical dialogue modeling in various virtual avatar animations. Previous studies mainly focus on the direct short-term production of listener behavior. They overlook the fine-grained control over motion variations and emotional intensity, especially in long-sequence modeling. Moreover, the lack of long-term and large-scale paired speaker-listener corpora including head dynamics and fine-grained multi-modality annotations (e.g., text-based expression descriptions, emotional intensity) also limits the application of dialogue modeling.Therefore, we first newly collect a large-scale multi-turn dataset of 3D dyadic conversation containing more than 1.4M valid frames for multi-modal responsive interaction, dubbed ListenerX. Additionally, we propose VividListener, a novel framework enabling fine-grained, expressive and controllable listener dynamics modeling. This framework leverages multi-modal conditions as guiding principles for fostering coherent interactions between speakers and listeners.Specifically, we design the Responsive Interaction Module (RIM) to adaptively represent the multi-modal interactive embeddings. RIM ensures the listener dynamics achieve fine-grained semantic coordination with textual descriptions and adjustments, while preserving expressive reaction with speaker behavior. Meanwhile, we design the Emotional Intensity Tags (EIT) for emotion intensity editing with multi-modal information integration, applying to both text descriptions and listener motion amplitude.Extensive experiments conducted on our newly collected ListenerX dataset demonstrate that VividListener achieves state-of-the-art performance, realizing expressive and controllable listener dynamics.","authors":["Shiying Li","Xingqun Qi","Bingkun Yang","Chen Weile","Zezhao Tian","Muyi Sun","Qifeng Liu","Man Zhang","Zhenan Sun"],"url":"https://arxiv.org/abs/2504.21718"}
{"created":"2025-05-01","title":"Sionna RT: Technical Report","abstract":"Sionna is an open-source, GPU-accelerated library that, as of version 0.14, incorporates a ray tracer for simulating radio wave propagation. A unique feature of Sionna RT is differentiability, enabling the calculation of gradients for the channel impulse responses (CIRs), radio maps, and other related metrics with respect to system and environmental parameters, such as material properties, antenna patterns, and array geometries. The release of Sionna 1.0 provides a complete overhaul of the ray tracer, significantly improving its speed, memory efficiency, and extensibility. This document details the algorithms employed by Sionna RT to simulate radio wave propagation efficiently, while also addressing their current limitations. Given that the computation of CIRs and radio maps requires distinct algorithms, these are detailed in separate sections. For CIRs, Sionna RT integrates shooting and bouncing of rays (SBR) with the image method and uses a hashing-based mechanism to efficiently eliminate duplicate paths. Radio maps are computed using a purely SBR-based approach.","authors":["Fay\\c{c}al A\\\"it Aoudia","Jakob Hoydis","Merlin Nimier-David","Sebastian Cammerer","Alexander Keller"],"url":"https://arxiv.org/abs/2504.21719"}
{"created":"2025-05-01","title":"Generalizing Biased Backpressure Routing and Scheduling to Wireless Multi-hop Networks with Advanced Air-interfaces","abstract":"Backpressure (BP) routing and scheduling is a well-established resource allocation method for wireless multi-hop networks, known for its fully distributed operations and proven maximum queue stability. Recent advances in shortest path-biased BP routing (SP-BP) mitigate shortcomings such as slow startup and random walk, but exclusive link-level commodity selection still suffers from the last-packet problem and bandwidth underutilization. Moreover, classic BP routing implicitly assumes single-input-single-output (SISO) transceivers, which can lead to the same packets being scheduled on multiple outgoing links for multiple-input-multiple-output (MIMO) transceivers, causing detouring and looping in MIMO networks. In this paper, we revisit the foundational Lyapunov drift theory underlying BP routing and demonstrate that exclusive commodity selection is unnecessary, and instead propose a Max-Utility link-sharing method. Additionally, we generalize MaxWeight scheduling to MIMO networks by introducing attributed capacity hypergraphs (ACH), an extension of traditional conflict graphs for SISO networks, and by incorporating backlog reassignment into scheduling iterations to prevent redundant packet routing. Numerical evaluations show that our approach substantially mitigates the last-packet problem in state-of-the-art (SOTA) SP-BP under lightweight traffic, and slightly expands the network capacity region for heavier traffic.","authors":["Zhongyuan Zhao","Yujun Ming","Ananthram Swami","Kevin Chan","Fikadu Dagefu","Santiago Segarra"],"url":"https://arxiv.org/abs/2504.21721"}
{"created":"2025-05-01","title":"Asymptotic Analysis of Weighted Fair Division","abstract":"Several resource allocation settings involve agents with unequal entitlements represented by weights. We analyze weighted fair division from an asymptotic perspective: if $m$ items are divided among $n$ agents whose utilities are independently sampled from a probability distribution, when is it likely that a fair allocation exist? We show that if the ratio between the weights is bounded, a weighted envy-free allocation exists with high probability provided that $m = \\Omega(n\\log n/\\log\\log n)$, generalizing a prior unweighted result. For weighted proportionality, we establish a sharp threshold of $m = n/(1-\\mu)$ for the transition from non-existence to existence, where $\\mu\\in (0,1)$ denotes the mean of the distribution. In addition, we prove that for two agents, a weighted envy-free (and weighted proportional) allocation is likely to exist if $m = \\omega(\\sqrt{r})$, where $r$ denotes the ratio between the two weights.","authors":["Pasin Manurangsi","Warut Suksompong","Tomohiko Yokoyama"],"url":"https://arxiv.org/abs/2504.21728"}
{"created":"2025-05-01","title":"Cert-SSB: Toward Certified Sample-Specific Backdoor Defense","abstract":"Deep neural networks (DNNs) are vulnerable to backdoor attacks, where an attacker manipulates a small portion of the training data to implant hidden backdoors into the model. The compromised model behaves normally on clean samples but misclassifies backdoored samples into the attacker-specified target class, posing a significant threat to real-world DNN applications. Currently, several empirical defense methods have been proposed to mitigate backdoor attacks, but they are often bypassed by more advanced backdoor techniques. In contrast, certified defenses based on randomized smoothing have shown promise by adding random noise to training and testing samples to counteract backdoor attacks. In this paper, we reveal that existing randomized smoothing defenses implicitly assume that all samples are equidistant from the decision boundary. However, it may not hold in practice, leading to suboptimal certification performance. To address this issue, we propose a sample-specific certified backdoor defense method, termed Cert-SSB. Cert-SSB first employs stochastic gradient ascent to optimize the noise magnitude for each sample, ensuring a sample-specific noise level that is then applied to multiple poisoned training sets to retrain several smoothed models. After that, Cert-SSB aggregates the predictions of multiple smoothed models to generate the final robust prediction. In particular, in this case, existing certification methods become inapplicable since the optimized noise varies across different samples. To conquer this challenge, we introduce a storage-update-based certification method, which dynamically adjusts each sample's certification region to improve certification performance. We conduct extensive experiments on multiple benchmark datasets, demonstrating the effectiveness of our proposed method. Our code is available at https://github.com/NcepuQiaoTing/Cert-SSB.","authors":["Ting Qiao","Yingjia Wang","Xing Liu","Sixing Wu","Jianbing Li","Yiming Li"],"url":"https://arxiv.org/abs/2504.21730"}
{"created":"2025-05-01","title":"Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning","abstract":"Mixed Reality (MR) could assist users' tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users' poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.","authors":["Feiyu Lu","Mengyu Chen","Hsiang Hsu","Pranav Deshpande","Cheng Yao Wang","Blair MacIntyre"],"url":"https://arxiv.org/abs/2504.21731"}
{"created":"2025-05-01","title":"TheraQuest: A Gamified, LLM-Powered Simulation for Massage Therapy Training","abstract":"Massage therapy training emphasizes hands-on techniques and effective therapist--patient communication. However, many educational programs struggle to provide realistic practice scenarios. To address this problem, we propose TheraQuest, a gamified, web-based simulation platform that employs large language models (LLMs) to generate diverse virtual patients with varying symptoms and cultural backgrounds. Through interactive dialogue, anatomical decision-making, and immediate assessment, trainees develop both diagnostic reasoning and empathetic communication skills in a low-risk environment. Unlike exclusively VR-based solutions, TheraQuest remains accessible via standard web browsers, mitigating the cost and discomfort associated with extended headset use. Preliminary testing suggests that integrating LLM-driven virtual patients with real-time skill metrics can enhance trainee engagement and help bridge the gap between theoretical knowledge and clinical proficiency.","authors":["Shengqian Wang"],"url":"https://arxiv.org/abs/2504.21735"}
{"created":"2025-05-01","title":"LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning","abstract":"General-purpose humanoid robots are expected to interact intuitively with humans, enabling seamless integration into daily life. Natural language provides the most accessible medium for this purpose. However, translating language into humanoid whole-body motion remains a significant challenge, primarily due to the gap between linguistic understanding and physical actions. In this work, we present an end-to-end, language-directed policy for real-world humanoid whole-body control. Our approach combines reinforcement learning with policy distillation, allowing a single neural network to interpret language commands and execute corresponding physical actions directly. To enhance motion diversity and compositionality, we incorporate a Conditional Variational Autoencoder (CVAE) structure. The resulting policy achieves agile and versatile whole-body behaviors conditioned on language inputs, with smooth transitions between various motions, enabling adaptation to linguistic variations and the emergence of novel motions. We validate the efficacy and generalizability of our method through extensive simulations and real-world experiments, demonstrating robust whole-body control. Please see our website at LangWBC.github.io for more information.","authors":["Yiyang Shao","Xiaoyu Huang","Bike Zhang","Qiayuan Liao","Yuman Gao","Yufeng Chi","Zhongyu Li","Sophia Shao","Koushil Sreenath"],"url":"https://arxiv.org/abs/2504.21738"}
{"created":"2025-05-01","title":"Bilateral Differentially Private Vertical Federated Boosted Decision Trees","abstract":"Federated learning is a distributed machine learning paradigm that enables collaborative training across multiple parties while ensuring data privacy. Gradient Boosting Decision Trees (GBDT), such as XGBoost, have gained popularity due to their high performance and strong interpretability. Therefore, there has been a growing interest in adapting XGBoost for use in federated settings via cryptographic techniques. However, it should be noted that these approaches may not always provide rigorous theoretical privacy guarantees, and they often come with a high computational cost in terms of time and space requirements. In this paper, we propose a variant of vertical federated XGBoost with bilateral differential privacy guarantee: MaskedXGBoost. We build well-calibrated noise to perturb the intermediate information to protect privacy. The noise is structured with part of its ingredients in the null space of the arithmetical operation for splitting score evaluation in XGBoost, helping us achieve consistently better utility than other perturbation methods and relatively lower overhead than encryption-based techniques. We provide theoretical utility analysis and empirically verify privacy preservation. Compared with other algorithms, our algorithm's superiority in both utility and efficiency has been validated on multiple datasets.","authors":["Bokang Zhang","Zhikun Zhang","Haodong Jiang","Yang Liu","Lihao Zheng","Yuxiao Zhou","Shuaiting Huang","Junfeng Wu"],"url":"https://arxiv.org/abs/2504.21739"}
{"created":"2025-05-01","title":"Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models","abstract":"The Greek fictional narratives often termed love novels or romances, ranging from the first century CE to the middle of the 15th century, have long been considered as similar in many ways, not least in the use of particular literary motifs. By applying the use of fine-tuned large language models, this study aims to investigate which motifs exactly that the texts in this corpus have in common, and in which ways they differ from each other. The results show that while some motifs persist throughout the corpus, others fluctuate in frequency, indicating certain trends or external influences. Conclusively, the method proves to adequately extract literary motifs according to a set definition, providing data for both quantitative and qualitative analyses.","authors":["Emelie Hallenberg"],"url":"https://arxiv.org/abs/2504.21742"}
{"created":"2025-05-01","title":"Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data","abstract":"Conventional retrieval-augmented neural machine translation (RANMT) systems leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many settings, in-domain monolingual target-side corpora are often available. This work explores ways to take advantage of such resources by retrieving relevant segments directly in the target language, based on a source-side query. For this, we design improved cross-lingual retrieval systems, trained with both sentence level and word-level matching objectives. In our experiments with two RANMT architectures, we first demonstrate the benefits of such cross-lingual objectives in a controlled setting, obtaining translation performances that surpass standard TM-based models. We then showcase our method on a real-world set-up, where the target monolingual resources far exceed the amount of parallel data and observe large improvements of our new techniques, which outperform both the baseline setting, and general-purpose cross-lingual retrievers.","authors":["Maxime Bouthors","Josep Crego","Fran\\c{c}ois Yvon"],"url":"https://arxiv.org/abs/2504.21747"}
{"created":"2025-05-01","title":"Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space","abstract":"3D morphable models (3DMMs) are a powerful tool to represent the possible shapes and appearances of an object category. Given a single test image, 3DMMs can be used to solve various tasks, such as predicting the 3D shape, pose, semantic correspondence, and instance segmentation of an object. Unfortunately, 3DMMs are only available for very few object categories that are of particular interest, like faces or human bodies, as they require a demanding 3D data acquisition and category-specific training process. In contrast, we introduce a new method, Common3D, that learns 3DMMs of common objects in a fully self-supervised manner from a collection of object-centric videos. For this purpose, our model represents objects as a learned 3D template mesh and a deformation field that is parameterized as an image-conditioned neural network. Different from prior works, Common3D represents the object appearance with neural features instead of RGB colors, which enables the learning of more generalizable representations through an abstraction from pixel intensities. Importantly, we train the appearance features using a contrastive objective by exploiting the correspondences defined through the deformable template mesh. This leads to higher quality correspondence features compared to related works and a significantly improved model performance at estimating 3D object pose and semantic correspondence. Common3D is the first completely self-supervised method that can solve various vision tasks in a zero-shot manner.","authors":["Leonhard Sommer","Olaf D\\\"unkel","Christian Theobalt","Adam Kortylewski"],"url":"https://arxiv.org/abs/2504.21749"}
{"created":"2025-05-01","title":"Online Knapsack Problems with Estimates","abstract":"Imagine you are a computer scientist who enjoys attending conferences or workshops within the year. Sadly, your travel budget is limited, so you must select a subset of events you can travel to.","authors":["Jakub Balab\\'an","Matthias Gehnen","Henri Lotze","Finn Seesemann","Moritz Stocker"],"url":"https://arxiv.org/abs/2504.21750"}
{"created":"2025-05-01","title":"CodeFlowBench: A Multi-turn, Iterative Benchmark for Complex Code Generation","abstract":"Real world development demands code that is readable, extensible, and testable by organizing the implementation into modular components and iteratively reuse pre-implemented code. We term this iterative, multi-turn process codeflow and introduce CodeFlowBench, the first benchmark designed for comprehensively evaluating LLMs' ability to perform codeflow, namely to implement new functionality by reusing existing functions over multiple turns. CodeFlowBench comprises 5258 problems drawn from Codeforces and is continuously updated via an automated pipeline that decomposes each problem into a series of function-level subproblems based on its dependency tree and each subproblem is paired with unit tests. We further propose a novel evaluation framework with tasks and metrics tailored to multi-turn code reuse to assess model performance. In experiments across various LLMs under both multi-turn and single-turn patterns. We observe models' poor performance on CodeFlowBench, with a substantial performance drop in the iterative codeflow scenario. For instance, o1-mini achieves a pass@1 of 20.8% in multi-turn pattern versus 37.8% in single-turn pattern. Further analysis shows that different models excel at different dependency depths, yet all struggle to correctly solve structurally complex problems, highlighting challenges for current LLMs to serve as code generation tools when performing codeflow. Overall, CodeFlowBench offers a comprehensive benchmark and new insights into LLM capabilities for multi-turn, iterative code generation, guiding future advances in code generation tasks.","authors":["Sizhe Wang","Zhengren Wang","Dongsheng Ma","Yongan Yu","Rui Ling","Zhiyu Li","Feiyu Xiong","Wentao Zhang"],"url":"https://arxiv.org/abs/2504.21751"}
{"created":"2025-05-01","title":"VDDP: Verifiable Distributed Differential Privacy under the Client-Server-Verifier Setup","abstract":"Despite differential privacy (DP) often being considered the de facto standard for data privacy, its realization is vulnerable to unfaithful execution of its mechanisms by servers, especially in distributed settings. Specifically, servers may sample noise from incorrect distributions or generate correlated noise while appearing to follow established protocols. This work analyzes these malicious behaviors in a general differential privacy framework within a distributed client-server-verifier setup. To address these adversarial problems, we propose a novel definition called Verifiable Distributed Differential Privacy (VDDP) by incorporating additional verification mechanisms. We also explore the relationship between zero-knowledge proofs (ZKP) and DP, demonstrating that while ZKPs are sufficient for achieving DP under verifiability requirements, they are not necessary. Furthermore, we develop two novel and efficient mechanisms that satisfy VDDP: (1) the Verifiable Distributed Discrete Laplacian Mechanism (VDDLM), which offers up to a $4 \\times 10^5$x improvement in proof generation efficiency with only 0.1-0.2x error compared to the previous state-of-the-art verifiable differentially private mechanism; (2) an improved solution to Verifiable Randomized Response (VRR) under local DP, a special case of VDDP, achieving up a reduction of up to 5000x in communication costs and the verifier's overhead.","authors":["Haochen Sun","Xi He"],"url":"https://arxiv.org/abs/2504.21752"}
{"created":"2025-05-01","title":"Whleaper: A 10-DOF Flexible Bipedal Wheeled Robot","abstract":"Wheel-legged robots combine the advantages of both wheeled robots and legged robots, offering versatile locomotion capabilities with excellent stability on challenging terrains and high efficiency on flat surfaces. However, existing wheel-legged robots typically have limited hip joint mobility compared to humans, while hip joint plays a crucial role in locomotion. In this paper, we introduce Whleaper, a novel 10-degree-of-freedom (DOF) bipedal wheeled robot, with 3 DOFs at the hip of each leg. Its humanoid joint design enables adaptable motion in complex scenarios, ensuring stability and flexibility. This paper introduces the details of Whleaper, with a focus on innovative mechanical design, control algorithms and system implementation. Firstly, stability stems from the increased DOFs at the hip, which expand the range of possible postures and improve the robot's foot-ground contact. Secondly, the extra DOFs also augment its mobility. During walking or sliding, more complex movements can be adopted to execute obstacle avoidance tasks. Thirdly, we utilize two control algorithms to implement multimodal motion for walking and sliding. By controlling specific DOFs of the robot, we conducted a series of simulations and practical experiments, demonstrating that a high-DOF hip joint design can effectively enhance the stability and flexibility of wheel-legged robots. Whleaper shows its capability to perform actions such as squatting, obstacle avoidance sliding, and rapid turning in real-world scenarios.","authors":["Yinglei Zhu","Sixiao He","Zhenghao Qi","Zhuoyuan Yong","Yihua Qin","Jianyu Chen"],"url":"https://arxiv.org/abs/2504.21767"}
{"created":"2025-05-01","title":"LLM-based Interactive Imitation Learning for Robotic Manipulation","abstract":"Recent advancements in machine learning provide methods to train autonomous agents capable of handling the increasing complexity of sequential decision-making in robotics. Imitation Learning (IL) is a prominent approach, where agents learn to control robots based on human demonstrations. However, IL commonly suffers from violating the independent and identically distributed (i.i.d) assumption in robotic tasks. Interactive Imitation Learning (IIL) achieves improved performance by allowing agents to learn from interactive feedback from human teachers. Despite these improvements, both approaches come with significant costs due to the necessity of human involvement. Leveraging the emergent capabilities of Large Language Models (LLMs) in reasoning and generating human-like responses, we introduce LLM-iTeach -- a novel IIL framework that utilizes an LLM as an interactive teacher to enhance agent performance while alleviating the dependence on human resources. Firstly, LLM-iTeach uses a hierarchical prompting strategy that guides the LLM in generating a policy in Python code. Then, with a designed similarity-based feedback mechanism, LLM-iTeach provides corrective and evaluative feedback interactively during the agent's training. We evaluate LLM-iTeach against baseline methods such as Behavior Cloning (BC), an IL method, and CEILing, a state-of-the-art IIL method using a human teacher, on various robotic manipulation tasks. Our results demonstrate that LLM-iTeach surpasses BC in the success rate and achieves or even outscores that of CEILing, highlighting the potential of LLMs as cost-effective, human-like teachers in interactive learning environments. We further demonstrate the method's potential for generalization by evaluating it on additional tasks. The code and prompts are provided at: https://github.com/Tubicor/LLM-iTeach.","authors":["Jonas Werner","Kun Chu","Cornelius Weber","Stefan Wermter"],"url":"https://arxiv.org/abs/2504.21769"}
{"created":"2025-05-01","title":"LASHED: LLMs And Static Hardware Analysis for Early Detection of RTL Bugs","abstract":"While static analysis is useful in detecting early-stage hardware security bugs, its efficacy is limited because it requires information to form checks and is often unable to explain the security impact of a detected vulnerability. Large Language Models can be useful in filling these gaps by identifying relevant assets, removing false violations flagged by static analysis tools, and explaining the reported violations. LASHED combines the two approaches (LLMs and Static Analysis) to overcome each other's limitations for hardware security bug detection. We investigate our approach on four open-source SoCs for five Common Weakness Enumerations (CWEs) and present strategies for improvement with better prompt engineering. We find that 87.5% of instances flagged by our recommended scheme are plausible CWEs. In-context learning and asking the model to 'think again' improves LASHED's precision.","authors":["Baleegh Ahmad","Hammond Pearce","Ramesh Karri","Benjamin Tan"],"url":"https://arxiv.org/abs/2504.21770"}
{"created":"2025-05-01","title":"Anatomical Similarity as a New Metric to Evaluate Brain Generative Models","abstract":"Generative models enhance neuroimaging through data augmentation, quality improvement, and rare condition studies. Despite advances in realistic synthetic MRIs, evaluations focus on texture and perception, lacking sensitivity to crucial anatomical fidelity. This study proposes a new metric, called WASABI (Wasserstein-Based Anatomical Brain Index), to assess the anatomical realism of synthetic brain MRIs. WASABI leverages \\textit{SynthSeg}, a deep learning-based brain parcellation tool, to derive volumetric measures of brain regions in each MRI and uses the multivariate Wasserstein distance to compare distributions between real and synthetic anatomies. Based on controlled experiments on two real datasets and synthetic MRIs from five generative models, WASABI demonstrates higher sensitivity in quantifying anatomical discrepancies compared to traditional image-level metrics, even when synthetic images achieve near-perfect visual quality. Our findings advocate for shifting the evaluation paradigm beyond visual inspection and conventional metrics, emphasizing anatomical fidelity as a crucial benchmark for clinically meaningful brain MRI synthesis. Our code is available at https://github.com/BahramJafrasteh/wasabi-mri.","authors":["Bahram Jafrasteh","Wei Peng","Cheng Wan","Yimin Luo","Ehsan Adeli","Qingyu Zhao"],"url":"https://arxiv.org/abs/2504.21771"}
{"created":"2025-05-01","title":"Solving Copyright Infringement on Short Video Platforms: Novel Datasets and an Audio Restoration Deep Learning Pipeline","abstract":"Short video platforms like YouTube Shorts and TikTok face significant copyright compliance challenges, as infringers frequently embed arbitrary background music (BGM) to obscure original soundtracks (OST) and evade content originality detection. To tackle this issue, we propose a novel pipeline that integrates Music Source Separation (MSS) and cross-modal video-music retrieval (CMVMR). Our approach effectively separates arbitrary BGM from the original OST, enabling the restoration of authentic video audio tracks. To support this work, we introduce two domain-specific datasets: OASD-20K for audio separation and OSVAR-160 for pipeline evaluation. OASD-20K contains 20,000 audio clips featuring mixed BGM and OST pairs, while OSVAR160 is a unique benchmark dataset comprising 1,121 video and mixed-audio pairs, specifically designed for short video restoration tasks. Experimental results demonstrate that our pipeline not only removes arbitrary BGM with high accuracy but also restores OSTs, ensuring content integrity. This approach provides an ethical and scalable solution to copyright challenges in user-generated content on short video platforms.","authors":["Minwoo Oh","Minsu Park","Eunil Park"],"url":"https://arxiv.org/abs/2504.21772"}
{"created":"2025-05-01","title":"MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness","abstract":"With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.","authors":["Junsheng Huang","Zhitao He","Sandeep Polisetty","Qingyun Wang","May Fung"],"url":"https://arxiv.org/abs/2504.21773"}
{"created":"2025-05-01","title":"Is Intermediate Fusion All You Need for UAV-based Collaborative Perception?","abstract":"Collaborative perception enhances environmental awareness through inter-agent communication and is regarded as a promising solution to intelligent transportation systems. However, existing collaborative methods for Unmanned Aerial Vehicles (UAVs) overlook the unique characteristics of the UAV perspective, resulting in substantial communication overhead. To address this issue, we propose a novel communication-efficient collaborative perception framework based on late-intermediate fusion, dubbed LIF. The core concept is to exchange informative and compact detection results and shift the fusion stage to the feature representation level. In particular, we leverage vision-guided positional embedding (VPE) and box-based virtual augmented feature (BoBEV) to effectively integrate complementary information from various agents. Additionally, we innovatively introduce an uncertainty-driven communication mechanism that uses uncertainty evaluation to select high-quality and reliable shared areas. Experimental results demonstrate that our LIF achieves superior performance with minimal communication bandwidth, proving its effectiveness and practicality. Code and models are available at https://github.com/uestchjw/LIF.","authors":["Jiuwu Hao","Liguo Sun","Yuting Wan","Yueyang Wu","Ti Xiang","Haolin Song","Pin Lv"],"url":"https://arxiv.org/abs/2504.21774"}
{"created":"2025-05-01","title":"Learning Heterogeneous Performance-Fairness Trade-offs in Federated Learning","abstract":"Recent methods leverage a hypernet to handle the performance-fairness trade-offs in federated learning. This hypernet maps the clients' preferences between model performance and fairness to preference-specifc models on the trade-off curve, known as local Pareto front. However, existing methods typically adopt a uniform preference sampling distribution to train the hypernet across clients, neglecting the inherent heterogeneity of their local Pareto fronts. Meanwhile, from the perspective of generalization, they do not consider the gap between local and global Pareto fronts on the global dataset. To address these limitations, we propose HetPFL to effectively learn both local and global Pareto fronts. HetPFL comprises Preference Sampling Adaptation (PSA) and Preference-aware Hypernet Fusion (PHF). PSA adaptively determines the optimal preference sampling distribution for each client to accommodate heterogeneous local Pareto fronts. While PHF performs preference-aware fusion of clients' hypernets to ensure the performance of the global Pareto front. We prove that HetPFL converges linearly with respect to the number of rounds, under weaker assumptions than existing methods. Extensive experiments on four datasets show that HetPFL significantly outperforms seven baselines in terms of the quality of learned local and global Pareto fronts.","authors":["Rongguang Ye","Ming Tang"],"url":"https://arxiv.org/abs/2504.21775"}
{"created":"2025-05-01","title":"WebThinker: Empowering Large Reasoning Models with Deep Research Capability","abstract":"Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \\textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \\textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \\textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \\textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.","authors":["Xiaoxi Li","Jiajie Jin","Guanting Dong","Hongjin Qian","Yutao Zhu","Yongkang Wu","Ji-Rong Wen","Zhicheng Dou"],"url":"https://arxiv.org/abs/2504.21776"}
{"created":"2025-05-01","title":"Near-Optimal Distributed Ruling Sets for Trees and High-Girth Graphs","abstract":"Given a graph $G=(V,E)$, a $\\beta$-ruling set is a subset $S\\subseteq V$ that is i) independent, and ii) every node $v\\in V$ has a node of $S$ within distance $\\beta$. In this paper we present almost optimal distributed algorithms for finding ruling sets in trees and high girth graphs in the classic LOCAL model. As our first contribution we present an $O(\\log\\log n)$-round randomized algorithm for computing $2$-ruling sets on trees, almost matching the $\\Omega(\\log\\log n/\\log\\log\\log n)$ lower bound given by Balliu et al. [FOCS'20]. Second, we show that $2$-ruling sets can be solved in $\\widetilde{O}(\\log^{5/3}\\log n)$ rounds in high-girth graphs. Lastly, we show that $O(\\log\\log\\log n)$-ruling sets can be computed in $\\widetilde{O}(\\log\\log n)$ rounds in high-girth graphs matching the lower bound up to triple-log factors. All of these results either improve polynomially or exponentially on the previously best algorithms and use a smaller domination distance $\\beta$.","authors":["Malte Baumecker","Yannic Maus","Jara Uitto"],"url":"https://arxiv.org/abs/2504.21777"}
{"created":"2025-05-01","title":"Normality of 8-Bit Bent Function","abstract":"Bent functions are Boolean functions in an even number of variables that are indicators of Hadamard difference sets in elementary abelian 2-groups. A bent function in m variables is said to be normal if it is constant on an affine space of dimension m/2. In this paper, we demonstrate that all bent functions in m = 8 variables -- whose exact count, determined by Langevin and Leander (Des. Codes Cryptogr. 59(1--3): 193--205, 2011), is approximately $2^106$ share a common algebraic property: every 8-variable bent function is normal, up to the addition of a linear function. With this result, we complete the analysis of the normality of bent functions for the last unresolvedcase, m= 8. It is already known that all bent functions in m variables are normal for m <= 6, while for m > = 10, there exist bent functions that cannot be made normal by adding linear functions. Consequently, we provide a complete solution to an open problem by Charpin (J. Complex. 20(2-3): 245-265, 2004)","authors":["Val\\'erie Gillot","Philippe Langevin","Alexandr Polujan"],"url":"https://arxiv.org/abs/2504.21779"}
{"created":"2025-05-01","title":"MAGNET: an open-source library for mesh agglomeration by Graph Neural Networks","abstract":"We introduce MAGNET, an open-source Python library designed for mesh agglomeration in both two- and three-dimensions, based on employing Graph Neural Networks (GNN). MAGNET serves as a comprehensive solution for training a variety of GNN models, integrating deep learning and other advanced algorithms such as METIS and k-means to facilitate mesh agglomeration and quality metric computation. The library's introduction is outlined through its code structure and primary features. The GNN framework adopts a graph bisection methodology that capitalizes on connectivity and geometric mesh information via SAGE convolutional layers, in line with the methodology proposed by Antonietti et al. (2024). Additionally, the proposed MAGNET library incorporates reinforcement learning to enhance the accuracy and robustness of the model for predicting coarse partitions within a multilevel framework. A detailed tutorial is provided to guide the user through the process of mesh agglomeration and the training of a GNN bisection model. We present several examples of mesh agglomeration conducted by MAGNET, demonstrating the library's applicability across various scenarios. Furthermore, the performance of the newly introduced models is contrasted with that of METIS and k-means, illustrating that the proposed GNN models are competitive regarding partition quality and computational efficiency. Finally, we exhibit the versatility of MAGNET's interface through its integration with Lymph, an open-source library implementing discontinuous Galerkin methods on polytopal grids for the numerical discretization of multiphysics differential problems.","authors":["Paola F. Antonietti","Matteo Caldana","Ilario Mazzieri","Andrea Re Fraschini"],"url":"https://arxiv.org/abs/2504.21780"}
{"created":"2025-05-01","title":"Message Optimality and Message-Time Trade-offs for APSP and Beyond","abstract":"Round complexity is an extensively studied metric of distributed algorithms. In contrast, our knowledge of the \\emph{message complexity} of distributed computing problems and its relationship (if any) with round complexity is still quite limited. To illustrate, for many fundamental distributed graph optimization problems such as (exact) diameter computation, All-Pairs Shortest Paths (APSP), Maximum Matching etc., while (near) round-optimal algorithms are known, message-optimal algorithms are hitherto unknown. More importantly, the existing round-optimal algorithms are not message-optimal. This raises two important questions: (1) Can we design message-optimal algorithms for these problems? (2) Can we give message-time tradeoffs for these problems in case the message-optimal algorithms are not round-optimal?","authors":["Fabien Dufoulon","Shreyas Pai","Gopal Pandurangan","Sriram Pemmaraju","Peter Robinson"],"url":"https://arxiv.org/abs/2504.21781"}
{"created":"2025-05-01","title":"A Comparison of the Consistent and Independent Second Moment Methods Applied to Thermal Radiative Transfer","abstract":"The design of efficient numerical methods for modeling thermal radiative transfer (TRT) is challenging due to the stiff, nonlinear coupling between radiation and material energies, especially at the time scales of interest in high energy density physics and astrophysics. Here, we investigate the use of the Second Moment Method (SMM) to accelerate absorption-emission within the context of the multigroup, Discrete Ordinates transport equations with discontinuous Galerkin spatial discretization. SMM employs a reduced-dimensional, diffusion-based model of radiation transport that, when coupled with suitable discrete closures, serves as a proxy for the transport equation, isolating the transport equation from the stiff absorption-emission physics. We use a gray low-order system to reduce the cost of solving the low-order system and leverage SMM low-order discretizations specifically designed to be scalably solvable with existing linear solver technology. Our algorithm robustly resolves the nonlinear TRT system while only relying on transport sweeps, linearly solving symmetric and positive definite, gray diffusion systems, and nonlinearly solving the spatially pointwise energy balance equation. This algorithm is used as a vehicle to compare the efficacy of low-order discretizations developed for steady-state, linear transport on gray and multigroup TRT problems in one and two spatial dimensions.","authors":["Samuel Olivier","James S. Warsa","HyeongKae Park"],"url":"https://arxiv.org/abs/2504.21784"}
{"created":"2025-05-01","title":"Frozen Gaussian Grid-point Correction For Semi-classical Schr\\\"odinger Equation","abstract":"We propose an efficient reconstruction algorithm named the frozen Gaussian grid-point correction (FGGC) for computing the Schr\\\"odinger equation in the semi-classical regime using the frozen Gaussian approximation (FGA). The FGA has demonstrated its superior efficiency in dealing with semi-classical problems and high-frequency wave propagations. However, reconstructing the wave function from a large number of Gaussian wave-packets is typically computationally intensive. This difficulty arises because these wave-packets propagate along the FGA trajectories to non-grid positions, making the application of the fast Fourier transform infeasible. In this work, we introduce the concept of ``on-grid correction'' and derive the formulas for the least squares approximation of Gaussian wave-packets, and also provide a detailed process of the FGGC algorithm. Furthermore, we rigorously prove that the error introduced by the least squares approximation on each Gaussian wave-packet is independent of the semi-classical parameter $\\varepsilon$. Numerical experiments show that the FGGC algorithm can significantly improve reconstruction efficiency while introducing only negligible error, making it a powerful tool for solving the semi-classical Schr\\\"odinger equation, especially in applications requiring both accuracy and efficiency.","authors":["Lihui Chai","Zili Deng"],"url":"https://arxiv.org/abs/2504.21785"}
{"created":"2025-05-01","title":"Anomaly-Driven Approach for Enhanced Prostate Cancer Segmentation","abstract":"Magnetic Resonance Imaging (MRI) plays an important role in identifying clinically significant prostate cancer (csPCa), yet automated methods face challenges such as data imbalance, variable tumor sizes, and a lack of annotated data. This study introduces Anomaly-Driven U-Net (adU-Net), which incorporates anomaly maps derived from biparametric MRI sequences into a deep learning-based segmentation framework to improve csPCa identification. We conduct a comparative analysis of anomaly detection methods and evaluate the integration of anomaly maps into the segmentation pipeline. Anomaly maps, generated using Fixed-Point GAN reconstruction, highlight deviations from normal prostate tissue, guiding the segmentation model to potential cancerous regions. We compare the performance by using the average score, computed as the mean of the AUROC and Average Precision (AP). On the external test set, adU-Net achieves the best average score of 0.618, outperforming the baseline nnU-Net model (0.605). The results demonstrate that incorporating anomaly detection into segmentation improves generalization and performance, particularly with ADC-based anomaly maps, offering a promising direction for automated csPCa identification.","authors":["Alessia Hu","Regina Beets-Tan","Lishan Cai","Eduardo Pooch"],"url":"https://arxiv.org/abs/2504.21789"}
{"created":"2025-05-01","title":"SWE-smith: Scaling Data for Software Engineering Agents","abstract":"Despite recent progress in Language Models (LMs) for software engineering, collecting training data remains a significant pain point. Existing datasets are small, with at most 1,000s of training instances from 11 or fewer GitHub repositories. The procedures to curate such datasets are often complex, necessitating hundreds of hours of human labor; companion execution environments also take up several terabytes of storage, severely limiting their scalability and usability. To address this pain point, we introduce SWE-smith, a novel pipeline for generating software engineering training data at scale. Given any Python codebase, SWE-smith constructs a corresponding execution environment, then automatically synthesizes 100s to 1,000s of task instances that break existing test(s) in the codebase. Using SWE-smith, we create a dataset of 50k instances sourced from 128 GitHub repositories, an order of magnitude larger than all previous works. We train SWE-agent-LM-32B, achieving 40.2% Pass@1 resolve rate on the SWE-bench Verified benchmark, state of the art among open source models. We open source SWE-smith (collection procedure, task instances, trajectories, models) to lower the barrier of entry for research in LM systems for automated software engineering. All assets available at https://swesmith.com.","authors":["John Yang","Kilian Leret","Carlos E. Jimenez","Alexander Wettig","Kabir Khandpur","Yanzhe Zhang","Binyuan Hui","Ofir Press","Ludwig Schmidt","Diyi Yang"],"url":"https://arxiv.org/abs/2504.21798"}
{"created":"2025-05-01","title":"How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues","abstract":"The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. In our dataset, synthetic dialogues match structural features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99), however, synthetic interactions do not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.","authors":["Suhas BN","Dominik Mattioli","Saeed Abdullah","Rosa I. Arriaga","Chris W. Wiese","Andrew M. Sherrill"],"url":"https://arxiv.org/abs/2504.21800"}
{"created":"2025-05-01","title":"DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition","abstract":"We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing.","authors":["Z. Z. Ren","Zhihong Shao","Junxiao Song","Huajian Xin","Haocheng Wang","Wanjia Zhao","Liyue Zhang","Zhe Fu","Qihao Zhu","Dejian Yang","Z. F. Wu","Zhibin Gou","Shirong Ma","Hongxuan Tang","Yuxuan Liu","Wenjun Gao","Daya Guo","Chong Ruan"],"url":"https://arxiv.org/abs/2504.21801"}
{"created":"2025-05-01","title":"An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding","abstract":"Binary code analysis plays a pivotal role in the field of software security and is widely used in tasks such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, reverse engineers face significant challenges in understanding binary code due to the lack of intuitive semantic information. Although traditional reverse tools can convert binary code into C-like pseudo code, the lack of code comments and symbolic information such as function names still makes code understanding difficult. In recent years, two groups of techniques have shown promising prospects: (1) Deep learning-based techniques have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This has left participants wondering about the capabilities of LLMs in binary code understanding. To this end, this work proposes a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios, which covers two key binary code understanding tasks, i.e., function name recovery and binary code summarization. To more comprehensively evaluate, we include binaries with multiple target architectures as well as different optimization options. We gain valuable insights into the capabilities and limitations through extensive empirical studies of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding, and provide new directions for binary code analysis techniques.","authors":["Xiuwei Shang","Zhenkan Fu","Shaoyin Cheng","Guoqiang Chen","Gangyang Li","Li Hu","Weiming Zhang","Nenghai Yu"],"url":"https://arxiv.org/abs/2504.21803"}
{"created":"2025-05-01","title":"Stable Trajectory Clustering: An Efficient Split and Merge Algorithm","abstract":"Clustering algorithms group data points by characteristics to identify patterns. Over the past two decades, researchers have extended these methods to analyze trajectories of humans, animals, and vehicles, studying their behavior and movement across applications. This paper presents whole-trajectory clustering and sub-trajectory clustering algorithms based on DBSCAN line segment clustering, which encompasses two key events: split and merge of line segments. The events are employed by object movement history and the average Euclidean distance between line segments. In this framework, whole-trajectory clustering considers entire entities' trajectories, whereas sub-trajectory clustering employs a sliding window model to identify similar sub-trajectories. Many existing trajectory clustering algorithms respond to temporary anomalies in data by splitting trajectories, which often obscures otherwise consistent clustering patterns and leads to less reliable insights. We introduce the stable trajectory clustering algorithm, which leverages the mean absolute deviation concept to demonstrate that selective omission of transient deviations not only preserves the integrity of clusters but also improves their stability and interpretability. We run all proposed algorithms on real trajectory datasets to illustrate their effectiveness and sensitivity to parameter variations.","authors":["Atieh Rahmani","Mansoor Davoodi","Justin M. Calabrese"],"url":"https://arxiv.org/abs/2504.21808"}
{"created":"2025-05-01","title":"A simple and effective approach for body part recognition on CT scans based on projection estimation","abstract":"It is well known that machine learning models require a high amount of annotated data to obtain optimal performance. Labelling Computed Tomography (CT) data can be a particularly challenging task due to its volumetric nature and often missing and$/$or incomplete associated meta-data. Even inspecting one CT scan requires additional computer software, or in the case of programming languages $-$ additional programming libraries. This study proposes a simple, yet effective approach based on 2D X-ray-like estimation of 3D CT scans for body region identification. Although body region is commonly associated with the CT scan, it often describes only the focused major body region neglecting other anatomical regions present in the observed CT. In the proposed approach, estimated 2D images were utilized to identify 14 distinct body regions, providing valuable information for constructing a high-quality medical dataset. To evaluate the effectiveness of the proposed method, it was compared against 2.5D, 3D and foundation model (MI2) based approaches. Our approach outperformed the others, where it came on top with statistical significance and F1-Score for the best-performing model EffNet-B0 of 0.980 $\\pm$ 0.016 in comparison to the 0.840 $\\pm$ 0.114 (2.5D DenseNet-161), 0.854 $\\pm$ 0.096 (3D VoxCNN), and 0.852 $\\pm$ 0.104 (MI2 foundation model). The utilized dataset comprised three different clinical centers and counted 15,622 CT scans (44,135 labels).","authors":["Franko Hrzic","Mohammadreza Movahhedi","Ophelie Lavoie-Gagne","Ata Kiapour"],"url":"https://arxiv.org/abs/2504.21810"}
{"created":"2025-05-01","title":"Why Compress What You Can Generate? When GPT-4o Generation Ushers in Image Compression Fields","abstract":"The rapid development of AIGC foundation models has revolutionized the paradigm of image compression, which paves the way for the abandonment of most pixel-level transform and coding, compelling us to ask: why compress what you can generate if the AIGC foundation model is powerful enough to faithfully generate intricate structure and fine-grained details from nothing more than some compact descriptors, i.e., texts, or cues. Fortunately, recent GPT-4o image generation of OpenAI has achieved impressive cross-modality generation, editing, and design capabilities, which motivates us to answer the above question by exploring its potential in image compression fields. In this work, we investigate two typical compression paradigms: textual coding and multimodal coding (i.e., text + extremely low-resolution image), where all/most pixel-level information is generated instead of compressing via the advanced GPT-4o image generation function. The essential challenge lies in how to maintain semantic and structure consistency during the decoding process. To overcome this, we propose a structure raster-scan prompt engineering mechanism to transform the image into textual space, which is compressed as the condition of GPT-4o image generation. Extensive experiments have shown that the combination of our designed structural raster-scan prompts and GPT-4o's image generation function achieved the impressive performance compared with recent multimodal/generative image compression at ultra-low bitrate, further indicating the potential of AIGC generation in image compression fields.","authors":["Yixin Gao","Xiaohan Pan","Xin Li","Zhibo Chen"],"url":"https://arxiv.org/abs/2504.21814"}
{"created":"2025-05-01","title":"Enumeration of minimum weight codewords of affine Cartesian codes","abstract":"Affine Cartesian codes were first discussed by Geil and Thomsen in 2013 in a broader framework and were formally introduced by L\\'opez, Renter\\'ia-M\\'arquez and Villarreal in 2014. These are linear error-correcting codes obtained by evaluating polynomials at points of a Cartesian product of subsets of the given finite field. They can be viewed as a vast generalization of Reed-Muller codes. In 1970, Delsarte, Goethals and MacWilliams gave a %characterization of minimum weight codewords of Reed-Muller codes and also formula for the minimum weight codewords of Reed-Muller codes. Carvalho and Neumann in 2020 considered affine Cartesian codes in a special setting where the subsets in the Cartesian product are nested subfields of the given finite field, and gave a characterization of their minimum weight codewords. We use this to give an explicit formula for the number of minimum weight codewords of affine Cartesian codes in the case of nested subfields. This is seen to unify the known formulas for the number of minimum weight codewords of Reed-Solomon codes and Reed-Muller codes.","authors":["Sakshi Dang","Sudhir R. Ghorpade"],"url":"https://arxiv.org/abs/2504.21816"}
{"created":"2025-05-01","title":"An Underwater, Fault-Tolerant, Laser-Aided Robotic Multi-Modal Dense SLAM System for Continuous Underwater In-Situ Observation","abstract":"Existing underwater SLAM systems are difficult to work effectively in texture-sparse and geometrically degraded underwater environments, resulting in intermittent tracking and sparse mapping. Therefore, we present Water-DSLAM, a novel laser-aided multi-sensor fusion system that can achieve uninterrupted, fault-tolerant dense SLAM capable of continuous in-situ observation in diverse complex underwater scenarios through three key innovations: Firstly, we develop Water-Scanner, a multi-sensor fusion robotic platform featuring a self-designed Underwater Binocular Structured Light (UBSL) module that enables high-precision 3D perception. Secondly, we propose a fault-tolerant triple-subsystem architecture combining: 1) DP-INS (DVL- and Pressure-aided Inertial Navigation System): fusing inertial measurement unit, doppler velocity log, and pressure sensor based Error-State Kalman Filter (ESKF) to provide high-frequency absolute odometry 2) Water-UBSL: a novel Iterated ESKF (IESKF)-based tight coupling between UBSL and DP-INS to mitigate UBSL's degeneration issues 3) Water-Stereo: a fusion of DP-INS and stereo camera for accurate initialization and tracking. Thirdly, we introduce a multi-modal factor graph back-end that dynamically fuses heterogeneous sensor data. The proposed multi-sensor factor graph maintenance strategy efficiently addresses issues caused by asynchronous sensor frequencies and partial data loss. Experimental results demonstrate Water-DSLAM achieves superior robustness (0.039 m trajectory RMSE and 100\\% continuity ratio during partial sensor dropout) and dense mapping (6922.4 points/m^3 in 750 m^3 water volume, approximately 10 times denser than existing methods) in various challenging environments, including pools, dark underwater scenes, 16-meter-deep sinkholes, and field rivers. Our project is available at https://water-scanner.github.io/.","authors":["Yaming Ou","Junfeng Fan","Chao Zhou","Pengju Zhang","Zongyuan Shen","Yichen Fu","Xiaoyan Liu","Zengguang Hou"],"url":"https://arxiv.org/abs/2504.21826"}
{"created":"2025-05-01","title":"Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization","abstract":"We introduce DEEVISum (Distilled Early Exit Vision language model for Summarization), a lightweight, efficient, and scalable vision language model designed for segment wise video summarization. Leveraging multi modal prompts that combine textual and audio derived signals, DEEVISum incorporates Multi Stage Knowledge Distillation (MSKD) and Early Exit (EE) to strike a balance between performance and efficiency. MSKD offers a 1.33% absolute F1 improvement over baseline distillation (0.5%), while EE reduces inference time by approximately 21% with a 1.3 point drop in F1. Evaluated on the TVSum dataset, our best model PaLI Gemma2 3B + MSKD achieves an F1 score of 61.1, competing the performance of significantly larger models, all while maintaining a lower computational footprint. We publicly release our code and processed dataset to support further research.","authors":["Anas Anwarul Haq Khan","Utkarsh Verma","Prateek Chanda","Ganesh Ramakrishnan"],"url":"https://arxiv.org/abs/2504.21831"}
{"created":"2025-05-01","title":"Optimal Dynamic Control of Bounded Jacobian Discrete-Time Systems via Interval Observers","abstract":"This paper presents an optimal dynamic control framework for bounded Jacobian nonlinear discrete-time (DT) systems with nonlinear observations affected by both state and process noise. Rather than directly stabilizing the uncertain system, we focus on stabilizing an interval observer in a higher dimensional space, whose states bound the true system states. Our nonlinear dynamic control method introduces added flexibility over traditional static and linear approaches, effectively compensating for system nonlinearities and enabling potentially tighter closed-loop intervals. Additionally, we establish a separation principle that allows for the design of observer and control gains. We further derive tractable matrix inequalities to ensure system stability in the closed-loop configuration. The simulation results show that the proposed dynamic control approach significantly outperforms a static counterpart method.","authors":["Mohammad Khajenejad"],"url":"https://arxiv.org/abs/2504.21832"}
{"created":"2025-05-01","title":"Patch bubbles for advection-dominated problems","abstract":"A novel variant of the \\emph{residual-free bubble} method (RFB) for advection dominated problems is presented. Since the usual RFB still suffers from oscillations and strong under/overshoots, the bubble space is enriched by \\emph{patch bubbles}, giving more freedom to the bubble space.","authors":["Eberhard B\\\"ansch","Pedro Morin","Itat\\'i Zocola"],"url":"https://arxiv.org/abs/2504.21835"}
{"created":"2025-05-01","title":"3D Stylization via Large Reconstruction Model","abstract":"With the growing success of text or image guided 3D generators, users demand more control over the generation process, appearance stylization being one of them. Given a reference image, this requires adapting the appearance of a generated 3D asset to reflect the visual style of the reference while maintaining visual consistency from multiple viewpoints. To tackle this problem, we draw inspiration from the success of 2D stylization methods that leverage the attention mechanisms in large image generation models to capture and transfer visual style. In particular, we probe if large reconstruction models, commonly used in the context of 3D generation, has a similar capability. We discover that the certain attention blocks in these models capture the appearance specific features. By injecting features from a visual style image to such blocks, we develop a simple yet effective 3D appearance stylization method. Our method does not require training or test time optimization. Through both quantitative and qualitative evaluations, we demonstrate that our approach achieves superior results in terms of 3D appearance stylization, significantly improving efficiency while maintaining high-quality visual outcomes.","authors":["Ipek Oztas","Duygu Ceylan","Aysegul Dundar"],"url":"https://arxiv.org/abs/2504.21836"}
{"created":"2025-05-01","title":"Learning Universal User Representations Leveraging Cross-domain User Intent at Snapchat","abstract":"The development of powerful user representations is a key factor in the success of recommender systems (RecSys). Online platforms employ a range of RecSys techniques to personalize user experience across diverse in-app surfaces. User representations are often learned individually through user's historical interactions within each surface and user representations across different surfaces can be shared post-hoc as auxiliary features or additional retrieval sources. While effective, such schemes cannot directly encode collaborative filtering signals across different surfaces, hindering its capacity to discover complex relationships between user behaviors and preferences across the whole platform. To bridge this gap at Snapchat, we seek to conduct universal user modeling (UUM) across different in-app surfaces, learning general-purpose user representations which encode behaviors across surfaces. Instead of replacing domain-specific representations, UUM representations capture cross-domain trends, enriching existing representations with complementary information. This work discusses our efforts in developing initial UUM versions, practical challenges, technical choices and modeling and research directions with promising offline performance. Following successful A/B testing, UUM representations have been launched in production, powering multiple use cases and demonstrating their value. UUM embedding has been incorporated into (i) Long-form Video embedding-based retrieval, leading to 2.78% increase in Long-form Video Open Rate, (ii) Long-form Video L2 ranking, with 19.2% increase in Long-form Video View Time sum, (iii) Lens L2 ranking, leading to 1.76% increase in Lens play time, and (iv) Notification L2 ranking, with 0.87% increase in Notification Open Rate.","authors":["Clark Mingxuan Ju","Leonardo Neves","Bhuvesh Kumar","Liam Collins","Tong Zhao","Yuwei Qiu","Qing Dou","Yang Zhou","Sohail Nizam","Rengim Ozturk","Yvette Liu","Sen Yang","Manish Malik","Neil Shah"],"url":"https://arxiv.org/abs/2504.21838"}
{"created":"2025-05-01","title":"Neuro-Symbolic Generation of Explanations for Robot Policies with Weighted Signal Temporal Logic","abstract":"Neural network-based policies have demonstrated success in many robotic applications, but often lack human-explanability, which poses challenges in safety-critical deployments. To address this, we propose a neuro-symbolic explanation framework that generates a weighted signal temporal logic (wSTL) specification to describe a robot policy in a interpretable form. Existing methods typically produce explanations that are verbose and inconsistent, which hinders explainability, and loose, which do not give meaningful insights into the underlying policy. We address these issues by introducing a simplification process consisting of predicate filtering, regularization, and iterative pruning. We also introduce three novel explainability evaluation metrics -- conciseness, consistency, and strictness -- to assess explanation quality beyond conventional classification metrics. Our method is validated in three simulated robotic environments, where it outperforms baselines in generating concise, consistent, and strict wSTL explanations without sacrificing classification accuracy. This work bridges policy learning with formal methods, contributing to safer and more transparent decision-making in robotics.","authors":["Mikihisa Yuasa","Ramavarapu S. Sreenivas","Huy T. Tran"],"url":"https://arxiv.org/abs/2504.21841"}
{"created":"2025-05-01","title":"Active Light Modulation to Counter Manipulation of Speech Visual Content","abstract":"High-profile speech videos are prime targets for falsification, owing to their accessibility and influence. This work proposes Spotlight, a low-overhead and unobtrusive system for protecting live speech videos from visual falsification of speaker identity and lip and facial motion. Unlike predominant falsification detection methods operating in the digital domain, Spotlight creates dynamic physical signatures at the event site and embeds them into all video recordings via imperceptible modulated light. These physical signatures encode semantically-meaningful features unique to the speech event, including the speaker's identity and facial motion, and are cryptographically-secured to prevent spoofing. The signatures can be extracted from any video downstream and validated against the portrayed speech content to check its integrity. Key elements of Spotlight include (1) a framework for generating extremely compact (i.e., 150-bit), pose-invariant speech video features, based on locality-sensitive hashing; and (2) an optical modulation scheme that embeds >200 bps into video while remaining imperceptible both in video and live. Prototype experiments on extensive video datasets show Spotlight achieves AUCs $\\geq$ 0.99 and an overall true positive rate of 100% in detecting falsified videos. Further, Spotlight is highly robust across recording conditions, video post-processing techniques, and white-box adversarial attacks on its video feature extraction methodologies.","authors":["Hadleigh Schwartz","Xiaofeng Yan","Charles J. Carver","Xia Zhou"],"url":"https://arxiv.org/abs/2504.21846"}
{"created":"2025-05-01","title":"Differentiable Room Acoustic Rendering with Multi-View Vision Priors","abstract":"An immersive acoustic experience enabled by spatial audio is just as crucial as the visual aspect in creating realistic virtual environments. However, existing methods for room impulse response estimation rely either on data-demanding learning-based models or computationally expensive physics-based modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic Rendering (AV-DAR), a framework that leverages visual cues extracted from multi-view images and acoustic beam tracing for physics-based room acoustic rendering. Experiments across six real-world environments from two datasets demonstrate that our multimodal, physics-based approach is efficient, interpretable, and accurate, significantly outperforming a series of prior methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves comparable performance to models trained on 10 times more data while delivering relative gains ranging from 16.6% to 50.9% when trained at the same scale.","authors":["Derong Jin (University of Maryland","College Park)","Ruohan Gao (University of Maryland","College Park)"],"url":"https://arxiv.org/abs/2504.21847"}
{"created":"2025-05-01","title":"Characterizing AI Agents for Alignment and Governance","abstract":"The creation of effective governance mechanisms for AI agents requires a deeper understanding of their core properties and how these properties relate to questions surrounding the deployment and operation of agents in the world. This paper provides a characterization of AI agents that focuses on four dimensions: autonomy, efficacy, goal complexity, and generality. We propose different gradations for each dimension, and argue that each dimension raises unique questions about the design, operation, and governance of these systems. Moreover, we draw upon this framework to construct \"agentic profiles\" for different kinds of AI agents. These profiles help to illuminate cross-cutting technical and non-technical governance challenges posed by different classes of AI agents, ranging from narrow task-specific assistants to highly autonomous general-purpose systems. By mapping out key axes of variation and continuity, this framework provides developers, policymakers, and members of the public with the opportunity to develop governance approaches that better align with collective societal goals.","authors":["Atoosa Kasirzadeh","Iason Gabriel"],"url":"https://arxiv.org/abs/2504.21848"}
{"created":"2025-05-01","title":"Public Opinion and The Rise of Digital Minds: Perceived Risk, Trust, and Regulation Support","abstract":"Governance institutions must respond to societal risks, including those posed by generative AI. This study empirically examines how public trust in institutions and AI technologies, along with perceived risks, shape preferences for AI regulation. Using the nationally representative 2023 Artificial Intelligence, Morality, and Sentience (AIMS) survey, we assess trust in government, AI companies, and AI technologies, as well as public support for regulatory measures such as slowing AI development or outright bans on advanced AI. Our findings reveal broad public support for AI regulation, with risk perception playing a significant role in shaping policy preferences. Individuals with higher trust in government favor regulation, while those with greater trust in AI companies and AI technologies are less inclined to support restrictions. Trust in government and perceived risks significantly predict preferences for both soft (e.g., slowing development) and strong (e.g., banning AI systems) regulatory interventions. These results highlight the importance of public opinion in AI governance. As AI capabilities advance, effective regulation will require balancing public concerns about risks with trust in institutions. This study provides a foundational empirical baseline for policymakers navigating AI governance and underscores the need for further research into public trust, risk perception, and regulatory strategies in the evolving AI landscape.","authors":["Justin B. Bullock","Janet V. T. Pauketat","Hsini Huang","Yi-Fan Wang","Jacy Reese Anthis"],"url":"https://arxiv.org/abs/2504.21849"}
{"created":"2025-05-01","title":"COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning","abstract":"Multimodal Large Language Models (MLLMs) excel at simple vision-language tasks but struggle when faced with complex tasks that require multiple capabilities, such as simultaneously recognizing objects, counting them, and understanding their spatial relationships. This might be partially the result of the fact that Visual Instruction Tuning (VIT), a critical training step for MLLMs, has traditionally focused on scaling data volume, but not the compositional complexity of training examples. We propose COMPACT (COMPositional Atomic-to-complex visual Capability Tuning), which generates a training dataset explicitly controlling for the compositional complexity of the training examples. The data from COMPACT allows MLLMs to train on combinations of atomic capabilities to learn complex capabilities more efficiently. Across all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT while using less than 10% of its data budget, and even outperforms it on several, especially those involving complex multi-capability tasks. For example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0% improvement on MM-Vet compared to the full-scale VIT on particularly complex questions that require four or more atomic capabilities. COMPACT offers a scalable, data-efficient, visual compositional tuning recipe to improve on complex visual-language tasks.","authors":["Xindi Wu","Hee Seung Hwang","Polina Kirichenko","Olga Russakovsky"],"url":"https://arxiv.org/abs/2504.21850"}
{"created":"2025-05-01","title":"TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments","abstract":"Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.","authors":["Sichang Tu","Abigail Powers","Stephen Doogan","Jinho D. Choi"],"url":"https://arxiv.org/abs/2504.21851"}
{"created":"2025-05-01","title":"A Survey of Interactive Generative Video","abstract":"Interactive Generative Video (IGV) has emerged as a crucial technology in response to the growing demand for high-quality, interactive video content across various domains. In this paper, we define IGV as a technology that combines generative capabilities to produce diverse high-quality video content with interactive features that enable user engagement through control signals and responsive feedback. We survey the current landscape of IGV applications, focusing on three major domains: 1) gaming, where IGV enables infinite exploration in virtual worlds; 2) embodied AI, where IGV serves as a physics-aware environment synthesizer for training agents in multimodal interaction with dynamically evolving scenes; and 3) autonomous driving, where IGV provides closed-loop simulation capabilities for safety-critical testing and validation. To guide future development, we propose a comprehensive framework that decomposes an ideal IGV system into five essential modules: Generation, Control, Memory, Dynamics, and Intelligence. Furthermore, we systematically analyze the technical challenges and future directions in realizing each component for an ideal IGV system, such as achieving real-time generation, enabling open-domain control, maintaining long-term coherence, simulating accurate physics, and integrating causal reasoning. We believe that this systematic analysis will facilitate future research and development in the field of IGV, ultimately advancing the technology toward more sophisticated and practical applications.","authors":["Jiwen Yu","Yiran Qin","Haoxuan Che","Quande Liu","Xintao Wang","Pengfei Wan","Di Zhang","Kun Gai","Hao Chen","Xihui Liu"],"url":"https://arxiv.org/abs/2504.21853"}
{"created":"2025-05-01","title":"ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction","abstract":"In recent years, video generation has seen significant advancements. However, challenges still persist in generating complex motions and interactions. To address these challenges, we introduce ReVision, a plug-and-play framework that explicitly integrates parameterized 3D physical knowledge into a pretrained conditional video generation model, significantly enhancing its ability to generate high-quality videos with complex motion and interactions. Specifically, ReVision consists of three stages. First, a video diffusion model is used to generate a coarse video. Next, we extract a set of 2D and 3D features from the coarse video to construct a 3D object-centric representation, which is then refined by our proposed parameterized physical prior model to produce an accurate 3D motion sequence. Finally, this refined motion sequence is fed back into the same video diffusion model as additional conditioning, enabling the generation of motion-consistent videos, even in scenarios involving complex actions and interactions. We validate the effectiveness of our approach on Stable Video Diffusion, where ReVision significantly improves motion fidelity and coherence. Remarkably, with only 1.5B parameters, it even outperforms a state-of-the-art video generation model with over 13B parameters on complex video generation by a substantial margin. Our results suggest that, by incorporating 3D physical knowledge, even a relatively small video diffusion model can generate complex motions and interactions with greater realism and controllability, offering a promising solution for physically plausible video generation.","authors":["Qihao Liu","Ju He","Qihang Yu","Liang-Chieh Chen","Alan Yuille"],"url":"https://arxiv.org/abs/2504.21855"}
{"created":"2025-05-01","title":"PiXi: Password Inspiration by Exploring Information","abstract":"Passwords, a first line of defense against unauthorized access, must be secure and memorable. However, people often struggle to create secure passwords they can recall. To address this problem, we design Password inspiration by eXploring information (PiXi), a novel approach to nudge users towards creating secure passwords. PiXi is the first of its kind that employs a password creation nudge to support users in the task of generating a unique secure password themselves. PiXi prompts users to explore unusual information right before creating a password, to shake them out of their typical habits and thought processes, and to inspire them to create unique (and therefore stronger) passwords. PiXi's design aims to create an engaging, interactive, and effective nudge to improve secure password creation. We conducted a user study ($N=238$) to compare the efficacy of PiXi to typical password creation. Our findings indicate that PiXi's nudges do influence users' password choices such that passwords are significantly longer and more secure (less predictable and guessable).","authors":["Shengqian Wang","Amirali Salehi-Abari","Julie Thorpe"],"url":"https://arxiv.org/abs/2304.10728"}
{"created":"2025-05-01","title":"Automated Generation of Precedence Graphs in Digital Value Chains for Automotive Production","abstract":"This study examines the digital value chain in automotive manufacturing, focusing on the identification, software flashing, customization, and commissioning of electronic control units in vehicle networks. A novel precedence graph design is proposed to optimize this process chain using an automated scheduling algorithm that employs mixed integer linear programming techniques. The results show significant improvements in key metrics. The algorithm reduces the number of production stations equipped with expensive hardware and software to execute digital value chain processes, while increasing capacity utilization through efficient scheduling and reduced idle time. Task parallelization is optimized, resulting in streamlined workflows and increased throughput. Compared to the traditional method, the automated approach has reduced preparation time by 50% and reduced scheduling activities, as it now takes two minutes to create the precedence graph. The flexibility of the algorithm's constraints allows for vehicle-specific configurations while maintaining high responsiveness, eliminating backup stations and facilitating the integration of new topologies. Automated scheduling significantly outperforms manual methods in efficiency, functionality, and adaptability.","authors":["Cornelius Hake","Christian Friedrich"],"url":"https://arxiv.org/abs/2504.19835"}
{"created":"2025-05-01","title":"Construct to Commitment: The Effect of Narratives on Economic Growth","abstract":"We study how government-led narratives through mass media evolve from construct, a mechanism for framing expectations, into commitment, a sustainable pillar for growth. We propose the ``Narratives-Construct-Commitment (NCC)\" framework outlining the mechanism and institutionalization of narratives, and formalize it as a dynamic Bayesian game. Using the Innovation-Driven Development Strategy (2016) as a case study, we identify the narrative shock from high-frequency financial data and trace its impact using local projection method. By shaping expectations, credible narratives institutionalize investment incentives, channel resources into R\\&amp;D, and facilitate sustained improvements in total factor productivity (TFP). Our findings strive to provide insights into the New Quality Productive Forces initiative, highlighting the role of narratives in transforming vision into tangible economic growth.","authors":["Hanyuan Jiang","Yi Man"],"url":"https://arxiv.org/abs/2504.21060"}
{"created":"2025-05-01","title":"Induced Minors and Region Intersection Graphs","abstract":"We show that for any positive integers $g$ and $t$, there is a $K_{6}^{(1)}$-induced-minor-free graph of girth at least $g$ that is not a region intersection graph over the class of $K_t$-minor-free graphs. This answers in a strong form the recently raised question of whether for every graph $H$ there is a graph $H'$ such that $H$-induced-minor-free graphs are region intersection graphs over $H'$-minor-free graphs.","authors":["\\'Edouard Bonnet","Robert Hickingbotham"],"url":"https://arxiv.org/abs/2504.21115"}
{"created":"2025-05-01","title":"QAOA Parameter Transferability for Maximum Independent Set using Graph Attention Networks","abstract":"The quantum approximate optimization algorithm (QAOA) is one of the promising variational approaches of quantum computing to solve combinatorial optimization problems. In QAOA, variational parameters need to be optimized by solving a series of nonlinear, nonconvex optimization programs. In this work, we propose a QAOA parameter transfer scheme using Graph Attention Networks (GAT) to solve Maximum Independent Set (MIS) problems. We prepare optimized parameters for graphs of 12 and 14 vertices and use GATs to transfer their parameters to larger graphs. Additionally, we design a hybrid distributed resource-aware algorithm for MIS (HyDRA-MIS), which decomposes large problems into smaller ones that can fit onto noisy intermediate-scale quantum (NISQ) computers. We integrate our GAT-based parameter transfer approach to HyDRA-MIS and demonstrate competitive results compared to KaMIS, a state-of-the-art classical MIS solver, on graphs with several thousands vertices.","authors":["Hanjing Xu","Xiaoyuan Liu","Alex Pothen","Ilya Safro"],"url":"https://arxiv.org/abs/2504.21135"}
{"created":"2025-05-01","title":"On feedback stabilisation for the Cahn-Hilliard equation and its numerical approximation","abstract":"We consider the stabilisation of solutions to the Cahn-Hilliard equation towards a given trajectory by means of a finite-dimensional static output feedback mechanism. Exponential stabilisation of the controlled state around the target trajectory is proven using careful energy estimates and a spectral condition which characterizes the strength of the feedback. The analysis is general enough to allow for pointwise and distributed measurements and actuation. The main results are derived via arguments that carry over to appropriate discretisation schemes which allows us to establish corresponding exponential stabilisation results also on the discrete level. The validity of our results and the importance of some of our assumptions are illustrated by numerical tests.","authors":["Herbert Egger","Marvin Fritz","Karl Kunisch","S\\'ergio S. Rodrigues"],"url":"https://arxiv.org/abs/2504.21150"}
{"created":"2025-05-01","title":"Evaluation and Verification of Physics-Informed Neural Models of the Grad-Shafranov Equation","abstract":"Our contributions are motivated by fusion reactors that rely on maintaining magnetohydrodynamic (MHD) equilibrium, where the balance between plasma pressure and confining magnetic fields is required for stable operation. In axisymmetric tokamak reactors in particular, and under the assumption of toroidal symmetry, this equilibrium can be mathematically modelled using the Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing studies did not examine realistic scenarios in which a single network generalizes to a variety of boundary conditions. Addressing that limitation, we evaluate a PINN architecture that incorporates boundary points as network inputs. Additionally, we compare PINN model accuracy and inference speeds with a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most performant, and accurate in our setting, we use the network verification tool Marabou to perform a range of verification tasks. Although we find some discrepancies between evaluations of the networks natively in PyTorch, compared to via Marabou, we are able to demonstrate useful and practical verification workflows. Our study is the first investigation of verification of such networks.","authors":["Fauzan Nazranda Rizqa","Matthew Hole","Charles Gretton"],"url":"https://arxiv.org/abs/2504.21155"}
{"created":"2025-05-01","title":"Iceberg Beyond the Tip: Co-Compilation of a Quantum Error Detection Code and a Quantum Algorithm","abstract":"The rapid progress in quantum hardware is expected to make them viable tools for the study of quantum algorithms in the near term. The timeline to useful algorithmic experimentation can be accelerated by techniques that use many noisy shots to produce an accurate estimate of the observable of interest. One such technique is to encode the quantum circuit using an error detection code and discard the samples for which an error has been detected. An underexplored property of error-detecting codes is the flexibility in the circuit encoding and fault-tolerant gadgets, which enables their co-optimization with the algorthmic circuit. However, standard circuit optimization tools cannot be used to exploit this flexibility as optimization must preserve the fault-tolerance of the gadget. In this work, we focus on the $[[k+2, k, 2]]$ Iceberg quantum error detection code, which is tailored to trapped-ion quantum processors. We design new flexible fault-tolerant gadgets for the Iceberg code, which we then co-optimize with the algorithmic circuit for the quantum approximate optimization algorithm (QAOA) using tree search. By co-optimizing the QAOA circuit and the Iceberg gadgets, we achieve an improvement in QAOA success probability from $44\\%$ to $65\\%$ and an increase in post-selection rate from $4\\%$ to $33\\%$ at 22 algorithmic qubits, utilizing 330 algorithmic two-qubit gates and 744 physical two-qubit gates on the Quantinuum H2-1 quantum computer, compared to the previous state-of-the-art hardware demonstration. Furthermore, we demonstrate better-than-unencoded performance for up to 34 algorithmic qubits, employing 510 algorithmic two-qubit gates and 1140 physical two-qubit gates.","authors":["Yuwei Jin","Zichang He","Tianyi Hao","David Amaro","Swamit Tannu","Ruslan Shaydulin","Marco Pistoia"],"url":"https://arxiv.org/abs/2504.21172"}
{"created":"2025-05-01","title":"Light Weight CNN for classification of Brain Tumors from MRI Images","abstract":"This study presents a convolutional neural network (CNN)-based approach for the multi-class classification of brain tumors using magnetic resonance imaging (MRI) scans. We utilize a publicly available dataset containing MRI images categorized into four classes: glioma, meningioma, pituitary tumor, and no tumor. Our primary objective is to build a light weight deep learning model that can automatically classify brain tumor types with high accuracy. To achieve this goal, we incorporate image preprocessing steps, including normalization, data augmentation, and a cropping technique designed to reduce background noise and emphasize relevant regions. The CNN architecture is optimized through hyperparameter tuning using Keras Tuner, enabling systematic exploration of network parameters. To ensure reliable evaluation, we apply 5-fold cross-validation, where each hyperparameter configuration is evaluated across multiple data splits to mitigate overfitting. Experimental results demonstrate that the proposed model achieves a classification accuracy of 98.78%, indicating its potential as a diagnostic aid in clinical settings. The proposed method offers a low-complexity yet effective solution for assisting in early brain tumor diagnosis.","authors":["Natnael Alemayehu"],"url":"https://arxiv.org/abs/2504.21188"}
{"created":"2025-05-01","title":"Abstract computation over first-order structures. Part IIa: Moschovakis' operator and other non-determinisms","abstract":"BSS RAMs over first-order structures help to characterize algorithms for processing objects by means of useful operations and relations. They are the result of a generalization of several types of abstract machines. We want to discuss whether this concept that allows a machine-oriented characterization of algorithms is sufficiently general for describing also other models of computation. Yiannis N. Moschovakis introduced a concept of abstract computability of functions on the basis of recursive definability over first-order structures. Moschovakis' search operator is the counterpart to the operator introduced by Stephen C. Kleene and suitable for structures without computable minima. To compare our concept with Moschovakis' generalization of the theory of recursive functions, we extend the abilities of BSS RAMs by an operator that makes it possible to provide information about computable functions and their inverses in a non-deterministic way. In Part IIb, we compare several non-determinisms, summarize effects resulting from the restriction of guesses to constants, and take into account properties such as the semi-decidability of oracle sets, the semi-decidability of the identity relation, and the recognizability of constants.","authors":["Christine Ga{\\ss}ner"],"url":"https://arxiv.org/abs/2504.21192"}
{"created":"2025-05-01","title":"Turning Up the Heat: Assessing 2-m Temperature Forecast Errors in AI Weather Prediction Models During Heat Waves","abstract":"Extreme heat is the deadliest weather-related hazard in the United States. Furthermore, it is increasing in intensity, frequency, and duration, making skillful forecasts vital to protecting life and property. Traditional numerical weather prediction (NWP) models struggle with extreme heat for medium-range and subseasonal-to-seasonal (S2S) timescales. Meanwhile, artificial intelligence-based weather prediction (AIWP) models are progressing rapidly. However, it is largely unknown how well AIWP models forecast extremes, especially for medium-range and S2S timescales. This study investigates 2-m temperature forecasts for 60 heat waves across the four boreal seasons and over four CONUS regions at lead times up to 20 days, using two AIWP models (Google GraphCast and Pangu-Weather) and one traditional NWP model (NOAA United Forecast System Global Ensemble Forecast System (UFS GEFS)). First, case study analyses show that both AIWP models and the UFS GEFS exhibit consistent cold biases on regional scales in the 5-10 days of lead time before heat wave onset. GraphCast is the more skillful AIWP model, outperforming UFS GEFS and Pangu-Weather in most locations. Next, the two AIWP models are isolated and analyzed across all heat waves and seasons, with events split among the model's testing (2018-2023) and training (1979-2017) periods. There are cold biases before and during the heat waves in both models and all seasons, except Pangu-Weather in winter, which exhibits a mean warm bias before heat wave onset. Overall, results offer encouragement that AIWP models may be useful for medium-range and S2S predictability of extreme heat.","authors":["Kelsey E. Ennis","Elizabeth A. Barnes","Marybeth C. Arcodia","Martin A. Fernandez","Eric D. Maloney"],"url":"https://arxiv.org/abs/2504.21195"}
{"created":"2025-05-01","title":"Generate-then-Verify: Reconstructing Data from Limited Published Statistics","abstract":"We study the problem of reconstructing tabular data from aggregate statistics, in which the attacker aims to identify interesting claims about the sensitive data that can be verified with 100% certainty given the aggregates. Successful attempts in prior work have conducted studies in settings where the set of published statistics is rich enough that entire datasets can be reconstructed with certainty. In our work, we instead focus on the regime where many possible datasets match the published statistics, making it impossible to reconstruct the entire private dataset perfectly (i.e., when approaches in prior work fail). We propose the problem of partial data reconstruction, in which the goal of the adversary is to instead output a $\\textit{subset}$ of rows and/or columns that are $\\textit{guaranteed to be correct}$. We introduce a novel integer programming approach that first $\\textbf{generates}$ a set of claims and then $\\textbf{verifies}$ whether each claim holds for all possible datasets consistent with the published aggregates. We evaluate our approach on the housing-level microdata from the U.S. Decennial Census release, demonstrating that privacy violations can still persist even when information published about such data is relatively sparse.","authors":["Terrance Liu","Eileen Xiao","Pratiksha Thaker","Adam Smith","Zhiwei Steven Wu"],"url":"https://arxiv.org/abs/2504.21199"}
{"created":"2025-05-01","title":"Generalised Label-free Artefact Cleaning for Real-time Medical Pulsatile Time Series","abstract":"Artefacts compromise clinical decision-making in the use of medical time series. Pulsatile waveforms offer probabilities for accurate artefact detection, yet most approaches rely on supervised manners and overlook patient-level distribution shifts. To address these issues, we introduce a generalised label-free framework, GenClean, for real-time artefact cleaning and leverage an in-house dataset of 180,000 ten-second arterial blood pressure (ABP) samples for training. We first investigate patient-level generalisation, demonstrating robust performances under both intra- and inter-patient distribution shifts. We further validate its effectiveness through challenging cross-disease cohort experiments on the MIMIC-III database. Additionally, we extend our method to photoplethysmography (PPG), highlighting its applicability to diverse medical pulsatile signals. Finally, its integration into ICM+, a clinical research monitoring software, confirms the real-time feasibility of our framework, emphasising its practical utility in continuous physiological monitoring. This work provides a foundational step toward precision medicine in improving the reliability of high-resolution medical time series analysis","authors":["Xuhang Chen","Ihsane Olakorede","Stefan Yu B\\\"ogli","Wenhao Xu","Erta Beqiri","Xuemeng Li","Chenyu Tang","Zeyu Gao","Shuo Gao","Ari Ercole","Peter Smielewski"],"url":"https://arxiv.org/abs/2504.21209"}
{"created":"2025-05-01","title":"Gradient Attention Map Based Verification of Deep Convolutional Neural Networks with Application to X-ray Image Datasets","abstract":"Deep learning models have great potential in medical imaging, including orthodontics and skeletal maturity assessment. However, applying a model to data different from its training set can lead to unreliable predictions that may impact patient care. To address this, we propose a comprehensive verification framework that evaluates model suitability through multiple complementary strategies. First, we introduce a Gradient Attention Map (GAM)-based approach that analyzes attention patterns using Grad-CAM and compares them via similarity metrics such as IoU, Dice Similarity, SSIM, Cosine Similarity, Pearson Correlation, KL Divergence, and Wasserstein Distance. Second, we extend verification to early convolutional feature maps, capturing structural mis-alignments missed by attention alone. Finally, we incorporate an additional garbage class into the classification model to explicitly reject out-of-distribution inputs. Experimental results demonstrate that these combined methods effectively identify unsuitable models and inputs, promoting safer and more reliable deployment of deep learning in medical imaging.","authors":["Omid Halimi Milani","Amanda Nikho","Lauren Mills","Marouane Tliba","Ahmet Enis Cetin","Mohammed H. Elnagar"],"url":"https://arxiv.org/abs/2504.21227"}
{"created":"2025-05-01","title":"Efficient Quantum-Safe Homomorphic Encryption for Quantum Computer Programs","abstract":"We present a lattice-based scheme for homomorphic evaluation of quantum programs and proofs that remains secure against quantum adversaries. Classical homomorphic encryption is lifted to the quantum setting by replacing composite-order groups with Module Learning-With-Errors (MLWE) lattices and by generalizing polynomial functors to bounded natural super functors (BNSFs). A secret depolarizing BNSF mask hides amplitudes, while each quantum state is stored as an MLWE ciphertext pair. We formalize security with the qIND-CPA game that allows coherent access to the encryption oracle and give a four-hybrid reduction to decisional MLWE.","authors":["Ben Goertzel"],"url":"https://arxiv.org/abs/2504.21235"}
{"created":"2025-05-01","title":"The Metric Dimension of Sparse Random Graphs","abstract":"In 2013, Bollob\\'as, Mitsche, and Pralat at gave upper and lower bounds for the likely metric dimension of random Erd\\H{o}s-R\\'enyi graphs $G(n,p)$ for a large range of expected degrees $d=pn$. However, their results only apply when $d \\ge \\log^5 n$, leaving open sparser random graphs with $d < \\log^5 n$. Here we provide upper and lower bounds on the likely metric dimension of $G(n,p)$ from just above the connectivity transition, i.e., where $d=pn=c \\log n$ for some $c > 1$, up to $d=\\log^5 n$. Our lower bound technique is based on an entropic argument which is more general than the use of Suen's inequality by Bollob\\'as, Mitsche, and Pralat, whereas our upper bound is similar to theirs.","authors":["Josep D\\'iaz","Harrison Hartle","Cristopher Moore"],"url":"https://arxiv.org/abs/2504.21244"}
{"created":"2025-05-01","title":"Towards Space Group Determination from EBSD Patterns: The Role of Deep Learning and High-throughput Dynamical Simulations","abstract":"The design of novel materials hinges on the understanding of structure-property relationships. However, our capability to synthesize a large number of materials has outpaced the ability and speed needed to characterize them. While the overall chemical constituents can be readily known during synthesis, the structural evolution and characterization of newly synthesized samples remains a bottleneck for the ultimate goal of high throughput nanomaterials discovery. Thus, scalable methods for crystal symmetry determination that can analyze a large volume of material samples within a short time-frame are especially needed. Kikuchi diffraction in the SEM is a promising technique for this due to its sensitivity to dynamical scattering, which may provide information beyond just the seven crystal systems and fourteen Bravais lattices. After diffraction patterns are collected from material samples, deep learning methods may be able to classify the space group symmetries using the patterns as input, which paired with the elemental composition, would help enable the determination of the crystal structure. To investigate the feasibility of this solution, neural networks were trained to predict the space group type of background corrected EBSD patterns. Our networks were first trained and tested on an artificial dataset of EBSD patterns of 5,148 different cubic phases, created through physics-based dynamical simulations. Next, Maximum Classifier Discrepancy, an unsupervised deep learning-based domain adaptation method, was utilized to train neural networks to make predictions for experimental EBSD patterns. We introduce a relabeling scheme, which enables our models to achieve accuracy scores higher than 90% on simulated and experimental data, suggesting that neural networks are capable of making predictions of crystal symmetry from an EBSD pattern.","authors":["Alfred Yan","Muhammad Nur Talha Kilic","Gert Nolze","Ankit Agrawal","Alok Choudhary","Roberto dos Reis","Vinayak Dravid"],"url":"https://arxiv.org/abs/2504.21331"}
{"created":"2025-05-01","title":"Physics-informed Gaussian Processes for Model Predictive Control of Nonlinear Systems","abstract":"Recently, a novel linear model predictive control algorithm based on a physics-informed Gaussian Process has been introduced, whose realizations strictly follow a system of underlying linear ordinary differential equations with constant coefficients. The control task is formulated as an inference problem by conditioning the Gaussian process prior on the setpoints and incorporating pointwise soft-constraints as further virtual setpoints. We apply this method to systems of nonlinear differential equations, obtaining a local approximation through the linearization around an equilibrium point. In the case of an asymptotically stable equilibrium point convergence is given through the Bayesian inference schema of the Gaussian Process. Results for this are demonstrated in a numerical example.","authors":["Adrian Lepp","J\\\"orn Tebbe","Andreas Besginow"],"url":"https://arxiv.org/abs/2504.21377"}
{"created":"2025-05-01","title":"Who Gets the Callback? Generative AI and Gender Bias","abstract":"Generative artificial intelligence (AI), particularly large language models (LLMs), is being rapidly deployed in recruitment and for candidate shortlisting. We audit several mid-sized open-source LLMs for gender bias using a dataset of 332,044 real-world online job postings. For each posting, we prompt the model to recommend whether an equally qualified male or female candidate should receive an interview callback. We find that most models tend to favor men, especially for higher-wage roles. Mapping job descriptions to the Standard Occupational Classification system, we find lower callback rates for women in male-dominated occupations and higher rates in female-associated ones, indicating occupational segregation. A comprehensive analysis of linguistic features in job ads reveals strong alignment of model recommendations with traditional gender stereotypes. To examine the role of recruiter identity, we steer model behavior by infusing Big Five personality traits and simulating the perspectives of historical figures. We find that less agreeable personas reduce stereotyping, consistent with an agreeableness bias in LLMs. Our findings highlight how AI-driven hiring may perpetuate biases in the labor market and have implications for fairness and diversity within firms.","authors":["Sugat Chaturvedi","Rochana Chaturvedi"],"url":"https://arxiv.org/abs/2504.21400"}
{"created":"2025-05-01","title":"Kernel Density Machines","abstract":"We introduce kernel density machines (KDM), a novel density ratio estimator in a reproducing kernel Hilbert space setting. KDM applies to general probability measures on countably generated measurable spaces without restrictive assumptions on continuity, or the existence of a Lebesgue density. For computational efficiency, we incorporate a low-rank approximation with precisely controlled error that grants scalability to large-sample settings. We provide rigorous theoretical guarantees, including asymptotic consistency, a functional central limit theorem, and finite-sample error bounds, establishing a strong foundation for practical use. Empirical results based on simulated and real data demonstrate the efficacy and precision of KDM.","authors":["Damir Filipovic","Paul Schneider"],"url":"https://arxiv.org/abs/2504.21419"}
{"created":"2025-05-01","title":"Wasserstein-Aitchison GAN for angular measures of multivariate extremes","abstract":"Economically responsible mitigation of multivariate extreme risks -- extreme rainfall in a large area, huge variations of many stock prices, widespread breakdowns in transportation systems -- requires estimates of the probabilities that such risks will materialize in the future. This paper develops a new method, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN), which provides simulated values of future $d$-dimensional multivariate extreme events and which hence can be used to give estimates of such probabilities. The main hypothesis is that, after transforming the observations to the unit-Pareto scale, their distribution is regularly varying in the sense that the distributions of their radial and angular components (with respect to the $L_1$-norm) converge and become asymptotically independent as the radius gets large. The method is a combination of standard extreme value analysis modeling of the tails of the marginal distributions with nonparametric GAN modeling of the angular distribution. For the latter, the angular values are transformed to Aitchison coordinates in a full $(d-1)$-dimensional linear space, and a Wasserstein GAN is trained on these coordinates and used to generate new values. A reverse transformation is then applied to these values and gives simulated values on the original data scale. The method shows good performance compared to other existing methods in the literature, both in terms of capturing the dependence structure of the extremes in the data, as well as in generating accurate new extremes of the data distribution. The comparison is performed on simulated multivariate extremes from a logistic model in dimensions up to 50 and on a 30-dimensional financial data set.","authors":["St\\'ephane Lhaut","Holger Rootz\\'en","Johan Segers"],"url":"https://arxiv.org/abs/2504.21438"}
{"created":"2025-05-01","title":"Invariant Bridges Between Four Successive Points: A New Tool for Data Coding","abstract":"We introduce a simple yet powerful invariant relation connecting four successive terms of a class of exponentially decaying alternating functions. Specifically, for the sequence defined by f(n) = ((1/2)^n + (-1)^n) / n, we prove that the combination [(n-2)f(n-2) + (n-3)f(n-3)] / [n f(n) + (n-1)f(n-1)] is universally equal to 4 for all integers n >= 4. This invariant bridge across four points opens new possibilities for predictive coding, data compression, and error detection. We demonstrate how the relation can be used to reconstruct missing data, verify data integrity, and reduce redundancy in data streams with minimal computational overhead. The simplicity and universality of this invariant make it a promising tool for a wide range of applications in information theory and coding systems.","authors":["Stanislav Semenov"],"url":"https://arxiv.org/abs/2504.21473"}
{"created":"2025-05-01","title":"A comparison of generative deep learning methods for multivariate angular simulation","abstract":"With the recent development of new geometric and angular-radial frameworks for multivariate extremes, reliably simulating from angular variables in moderate-to-high dimensions is of increasing importance. Empirical approaches have the benefit of simplicity, and work reasonably well in low dimensions, but as the number of variables increases, they can lack the required flexibility and scalability. Classical parametric models for angular variables, such as the von Mises-Fisher (vMF) distribution, provide an alternative. Exploiting mixtures of vMF distributions increases their flexibility, but there are cases where even this is not sufficient to capture the intricate features that can arise in data. Owing to their flexibility, generative deep learning methods are able to capture complex data structures; they therefore have the potential to be useful in the simulation of angular variables. In this paper, we explore a range of deep learning approaches for this task, including generative adversarial networks, normalizing flows and flow matching. We assess their performance via a range of metrics and make comparisons to the more classical approach of using a mixture of vMF distributions. The methods are also applied to a metocean data set, demonstrating their applicability to real-world, complex data structures.","authors":["Jakob Benjamin Wessel","Callum J. R. Murphy-Barltrop","Emma S. Simpson"],"url":"https://arxiv.org/abs/2504.21505"}
{"created":"2025-05-01","title":"Simulating quantum collision models with Hamiltonian simulations using early fault-tolerant quantum computers","abstract":"We develop randomized quantum algorithms to simulate quantum collision models, also known as repeated interaction schemes, which provide a rich framework to model various open-system dynamics. The underlying technique involves composing time evolutions of the total (system, bath, and interaction) Hamiltonian and intermittent tracing out of the environment degrees of freedom. This results in a unified framework where any near-term Hamiltonian simulation algorithm can be incorporated to implement an arbitrary number of such collisions on early fault-tolerant quantum computers: we do not assume access to specialized oracles such as block encodings and minimize the number of ancilla qubits needed. In particular, using the correspondence between Lindbladian evolution and completely positive trace-preserving maps arising out of memoryless collisions, we provide an end-to-end quantum algorithm for simulating Lindbladian dynamics. For a system of $n$-qubits, we exhaustively compare the circuit depth needed to estimate the expectation value of an observable with respect to the reduced state of the system after time $t$ while employing different near-term Hamiltonian simulation techniques, requiring at most $n+2$ qubits in all. We compare the CNOT gate counts of the various approaches for estimating the Transverse Field Magnetization of a $10$-qubit XX-Heisenberg spin chain under amplitude damping. Finally, we also develop a framework to efficiently simulate an arbitrary number of memory-retaining collisions, i.e., where environments interact, leading to non-Markovian dynamics. Overall, our methods can leverage quantum collision models for both Markovian and non-Markovian dynamics on early fault-tolerant quantum computers, shedding light on the advantages and limitations of simulating open systems dynamics using this framework.","authors":["Kushagra Garg","Zeeshan Ahmed","Subhadip Mitra","Shantanav Chakraborty"],"url":"https://arxiv.org/abs/2504.21564"}
{"created":"2025-05-01","title":"Glucagon and insulin production in pancreatic cells modeled using Petri nets and Boolean networks","abstract":"Diabetes is a civilization chronic disease characterized by a constant elevated concentration of glucose in the blood. Many processes are involved in the glucose regulation, and their interactions are very complex. To better understand those processes we set ourselves a goal to create a Petri net model of the glucose regulation in the whole body. So far we have managed to create a model of glycolysis and synthesis of glucose in the liver, and the general overview models of the glucose regulation in a healthy and diabetic person. In this paper we introduce Petri nets models of insulin secretion in beta cell of the pancreas, and glucagon in the pancreas alpha cells. Those two hormones have mutually opposite effects: insulin preventing hyperglycemia, and glucagon preventing hypoglycemia. Understanding the mechanisms of insulin and glucagon secretion constitutes the basis for understanding diabetes. We also present a model in which both processes occur together, depending on the blood glucose level. The dynamics of each model is analysed. Additionally, we transform the overall insulin and glucagon secretion system to a Boolean network, following standard transformation rules.","authors":["Kamila Barylska","Frank Delaplace","Anna Gogoli\\'nska","Ewa Pa\\'nkowska"],"url":"https://arxiv.org/abs/2504.21578"}
{"created":"2025-05-01","title":"Efficient Decomposition of Forman-Ricci Curvature on Vietoris-Rips Complexes and Data Applications","abstract":"Discrete Forman-Ricci curvature (FRC) is an efficient tool that characterizes essential geometrical features and associated transitions of real-world networks, extending seamlessly to higher-dimensional computations in simplicial complexes. In this article, we provide two major advancements: First, we give a decomposition for FRC, enabling local computations of FRC. Second, we construct a set-theoretical proof enabling an efficient algorithm for the local computation of FRC in Vietoris-Rips (VR) complexes.Strikingly, this approach reveals critical information and geometric insights often overlooked by conventional classification techniques. Our findings open new avenues for geometric computations in VR complexes and highlight an essential yet under-explored aspect of data classification: the geometry underpinning statistical patterns.","authors":["Danillo Barros de Souza","Jonatas Teodomiro","Fernando A. N. Santos","Mengjun Ding","Weiqiang Sun","Mathieu Desroches","J\\\"urgen Nost","Serafim Rodrigues"],"url":"https://arxiv.org/abs/2504.21601"}
{"created":"2025-05-01","title":"A note on the quantum Wielandt inequality","abstract":"In this note, we show how to extend operator algebraic methods introduced by Rahaman to prove that the index of primitivity of any primitive Schwarz map is at most $2(D-1)^2$, where $D$ is the dimension of the underlying matrix algebra. This inequality was first proved by Rahaman for Schwarz maps which were both unital and trace preserving. We show here that the assumption of unitality is automatic (up to normalization) for primitive Schwarz maps, but, in general, not all primitive unital Schwarz maps are trace preserving. Therefore, the precise purpose of this note is to showcase how to apply the proof of Rahaman to arbitrary primitive Schwarz maps. As a corollary of this theorem, we show that the index of primitivity of any primitive 2-positive map is at most $2(D-1)^2$, so in particular this bound holds for arbitrary primitive completely positive maps. We briefly discuss of how this relates to a conjecture of Perez-Garcia, Verstraete, Wolf and Cirac concerning properties of parent Hamiltonians of matrix product states.","authors":["Owen Ekblad"],"url":"https://arxiv.org/abs/2504.21638"}
{"created":"2025-05-01","title":"LoC-LIC: Low Complexity Learned Image Coding Using Hierarchical Feature Transforms","abstract":"Current learned image compression models typically exhibit high complexity, which demands significant computational resources. To overcome these challenges, we propose an innovative approach that employs hierarchical feature extraction transforms to significantly reduce complexity while preserving bit rate reduction efficiency. Our novel architecture achieves this by using fewer channels for high spatial resolution inputs/feature maps. On the other hand, feature maps with a large number of channels have reduced spatial dimensions, thereby cutting down on computational load without sacrificing performance. This strategy effectively reduces the forward pass complexity from \\(1256 \\, \\text{kMAC/Pixel}\\) to just \\(270 \\, \\text{kMAC/Pixel}\\). As a result, the reduced complexity model can open the way for learned image compression models to operate efficiently across various devices and pave the way for the development of new architectures in image compression technology.","authors":["Ayman A. Ameen","Thomas Richter","Andr\\'e Kaup"],"url":"https://arxiv.org/abs/2504.21778"}
{"created":"2025-05-01","title":"Estimation of discrete distributions in relative entropy, and the deviations of the missing mass","abstract":"We study the problem of estimating a distribution over a finite alphabet from an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler divergence). While optimal expected risk bounds are known, high-probability guarantees remain less well-understood. First, we analyze the classical Laplace (add-$1$) estimator, obtaining matching upper and lower bounds on its performance and showing its optimality among confidence-independent estimators. We then characterize the minimax-optimal high-probability risk achievable by any estimator, which is attained via a simple confidence-dependent smoothing technique. Interestingly, the optimal non-asymptotic risk contains an additional logarithmic factor over the ideal asymptotic risk. Next, motivated by scenarios where the alphabet exceeds the sample size, we investigate methods that adapt to the sparsity of the distribution at hand. We introduce an estimator using data-dependent smoothing, for which we establish a high-probability risk bound depending on two effective sparsity parameters. As part of the analysis, we also derive a sharp high-probability upper bound on the missing mass.","authors":["Jaouad Mourtada"],"url":"https://arxiv.org/abs/2504.21787"}
{"created":"2025-05-01","title":"Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model","abstract":"The Hawkes process (HP) is commonly used to model event sequences with self-reinforcing dynamics, including electronic health records (EHRs). Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which is often critical in healthcare. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs yet more interpretable than other neural network approaches, and allows us to explicitly trade flexibility for interpretability by adding transformer encoder layers to further contextualize the event embeddings. Results show that our method accurately recovers impact functions in simulations, achieves competitive performance on MIMIC-IV procedure dataset, and gains clinically meaningful interpretation on XX-EHR with children diagnosis dataset even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics in EHRs and other data effectively, implying that interpretability can be maintained without loss of performance.","authors":["Yuankang Zhao","Matthew Engelhard"],"url":"https://arxiv.org/abs/2504.21795"}
{"created":"2025-05-01","title":"Cryptography without Long-Term Quantum Memory and Global Entanglement","abstract":"We show how oracles which only allow for classical query access can be used to construct a variety of quantum cryptographic primitives which do not require long-term quantum memory or global entanglement. Specifically, if a quantum party can execute a semi-quantum token scheme (Shmueli 2022) with probability of success $1/2 + \\delta$, we can build powerful cryptographic primitives with a multiplicative logarithmic overhead for the desired correctness error. Our scheme makes no assumptions about the quantum party's noise model except for a simple independence requirement: noise on two sets of non-entangled hardware must be independent.","authors":["Lev Stambler"],"url":"https://arxiv.org/abs/2504.21842"}
{"created":"2025-05-01","title":"Scalable Multi-Task Learning for Particle Collision Event Reconstruction with Heterogeneous Graph Neural Networks","abstract":"The growing luminosity frontier at the Large Hadron Collider is challenging the reconstruction and analysis of particle collision events. Increased particle multiplicities are straining latency and storage requirements at the data acquisition stage, while new complications are emerging, including higher background levels and more frequent particle vertex misassociations. This in turn necessitates the development of more holistic and scalable reconstruction methods that take advantage of recent advances in machine learning. We propose a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique representations for diverse particle collision relationships and integrated graph pruning layers for scalability. Trained with a multi-task paradigm in an environment mimicking the LHCb experiment, this HGNN significantly improves beauty hadron reconstruction performance. Notably, it concurrently performs particle vertex association and graph pruning within a single framework. We quantify reconstruction and pruning performance, demonstrate enhanced inference time scaling with event complexity, and mitigate potential performance loss using a weighted message passing scheme.","authors":["William Sutcliffe","Marta Calvi","Simone Capelli","Jonas Eschle","Juli\\'an Garc\\'ia Pardi\\~nas","Abhijit Mathad","Azusa Uzuki","Nicola Serra"],"url":"https://arxiv.org/abs/2504.21844"}
{"created":"2025-05-01","title":"On the Efficacy of the Peeling Decoder for the Quantum Expander Code","abstract":"The problem of recovering from qubit erasures has recently gained attention as erasures occur in many physical systems such as photonic systems, trapped ions, superconducting qubits and circuit quantum electrodynamics. While several linear-time decoders for error correction are known, their error-correcting capability is limited to half the minimum distance of the code, whereas erasure correction allows one to go beyond this limit. As in the classical case, stopping sets pose a major challenge in designing efficient erasure decoders for quantum LDPC codes. In this paper, we show through simulation, that an attractive alternative here, is the use of quantum expander codes in conjunction with the peeling decoder that has linear complexity. We also discuss additional techniques including small-set-flip decoding, that can be applied following the peeling operation, to improve decoding performance and their associated complexity.","authors":["Jefrin Sharmitha Prabhu","Abhinav Vaishya","Shobhit Bhatnagar","Aryaman Manish Kolhe","V. Lalitha","P. Vijay Kumar"],"url":"https://arxiv.org/abs/2504.21845"}
{"created":"2025-05-01","title":"A Large-scale Multimodal Study for Predicting Mortality Risk Using Minimal and Low Parameter Models and Separable Risk Assessment","abstract":"The majority of biomedical studies use limited datasets that may not generalize over large heterogeneous datasets that have been collected over several decades. The current paper develops and validates several multimodal models that can predict 1-year mortality based on a massive clinical dataset. Our focus on predicting 1-year mortality can provide a sense of urgency to the patients. Using the largest dataset of its kind, the paper considers the development and validation of multimodal models based on 25,137,015 videos associated with 699,822 echocardiography studies from 316,125 patients, and 2,922,990 8-lead electrocardiogram (ECG) traces from 631,353 patients. Our models allow us to assess the contribution of individual factors and modalities to the overall risk. Our approach allows us to develop extremely low-parameter models that use optimized feature selection based on feature importance. Based on available clinical information, we construct a family of models that are made available in the DISIML package. Overall, performance ranges from an AUC of 0.72 with just ten parameters to an AUC of 0.89 with under 105k for the full multimodal model. The proposed approach represents a modular neural network framework that can provide insights into global risk trends and guide therapies for reducing mortality risk.","authors":["Alvaro E. Ulloa Cerna","Marios Pattichis","David P. vanMaanen","Linyuan Jing","Aalpen A. Patel","Joshua V. Stough","Christopher M. Haggerty","Brandon K. Fornwalt"],"url":"https://arxiv.org/abs/1901.08125"}
{"created":"2025-05-01","title":"Exploiting Defenses against GAN-Based Feature Inference Attacks in Federated Learning","abstract":"Federated learning (FL) is a decentralized model training framework that aims to merge isolated data islands while maintaining data privacy. However, recent studies have revealed that Generative Adversarial Network (GAN) based attacks can be employed in FL to learn the distribution of private datasets and reconstruct recognizable images. In this paper, we exploit defenses against GAN-based attacks in FL and propose a framework, Anti-GAN, to prevent attackers from learning the real distribution of the victim's data. The core idea of Anti-GAN is to manipulate the visual features of private training images to make them indistinguishable to human eyes even restored by attackers. Specifically, Anti-GAN projects the private dataset onto a GAN's generator and combines the generated fake images with the actual images to create the training dataset, which is then used for federated model training. The experimental results demonstrate that Anti-GAN is effective in preventing attackers from learning the distribution of private images while causing minimal harm to the accuracy of the federated model.","authors":["Xinjian Luo","Xianglong Zhang"],"url":"https://arxiv.org/abs/2004.12571"}
{"created":"2025-05-01","title":"InvAASTCluster: On Applying Invariant-Based Program Clustering to Introductory Programming Assignments","abstract":"Due to the vast number of students enrolled in programming courses, there has been an increasing number of automated program repair techniques focused on introductory programming assignments (IPAs). Typically, such techniques use program clustering to take advantage of previous correct student implementations to repair a new incorrect submission. These repair techniques use clustering methods since analyzing all available correct submissions to repair a program is not feasible. However, conventional clustering methods rely on program representations based on features such as abstract syntax trees (ASTs), syntax, control flow, and data flow.","authors":["Pedro Orvalho","Mikol\\'a\\v{s} Janota","Vasco Manquinho"],"url":"https://arxiv.org/abs/2206.14175"}
{"created":"2025-05-01","title":"SDWPF: A Dataset for Spatial Dynamic Wind Power Forecasting Challenge at KDD Cup 2022","abstract":"The variability of wind power supply can present substantial challenges to incorporating wind power into a grid system. Thus, Wind Power Forecasting (WPF) has been widely recognized as one of the most critical issues in wind power integration and operation. There has been an explosion of studies on wind power forecasting problems in the past decades. Nevertheless, how to well handle the WPF problem is still challenging, since high prediction accuracy is always demanded to ensure grid stability and security of supply. We present a unique Spatial Dynamic Wind Power Forecasting dataset: SDWPF, which includes the spatial distribution of wind turbines, as well as the dynamic context factors. Whereas, most of the existing datasets have only a small number of wind turbines without knowing the locations and context information of wind turbines at a fine-grained time scale. By contrast, SDWPF provides the wind power data of 134 wind turbines from a wind farm over half a year with their relative positions and internal statuses. We use this dataset to launch the Baidu KDD Cup 2022 to examine the limit of current WPF solutions. The dataset is released at https://aistudio.baidu.com/aistudio/competition/detail/152/0/datasets.","authors":["Jingbo Zhou","Xinjiang Lu","Yixiong Xiao","Jiantao Su","Junfu Lyu","Yanjun Ma","Dejing Dou"],"url":"https://arxiv.org/abs/2208.04360"}
{"created":"2025-05-01","title":"Learning 6-DoF Fine-grained Grasp Detection Based on Part Affordance Grounding","abstract":"Robotic grasping is a fundamental ability for a robot to interact with the environment. Current methods focus on how to obtain a stable and reliable grasping pose in object level, while little work has been studied on part (shape)-wise grasping which is related to fine-grained grasping and robotic affordance. Parts can be seen as atomic elements to compose an object, which contains rich semantic knowledge and a strong correlation with affordance. However, lacking a large part-wise 3D robotic dataset limits the development of part representation learning and downstream applications. In this paper, we propose a new large Language-guided SHape grAsPing datasEt (named LangSHAPE) to promote 3D part-level affordance and grasping ability learning. From the perspective of robotic cognition, we design a two-stage fine-grained robotic grasping framework (named LangPartGPD), including a novel 3D part language grounding model and a part-aware grasp pose detection model, in which explicit language input from human or large language models (LLMs) could guide a robot to generate part-level 6-DoF grasping pose with textual explanation. Our method combines the advantages of human-robot collaboration and LLMs' planning ability using explicit language as a symbolic intermediate. To evaluate the effectiveness of our proposed method, we perform 3D part grounding and fine-grained grasp detection experiments on both simulation and physical robot settings, following language instructions across different degrees of textual complexity. Results show our method achieves competitive performance in 3D geometry fine-grained grounding, object affordance inference, and 3D part-aware grasping tasks. Our dataset and code are available on our project website https://sites.google.com/view/lang-shape","authors":["Yaoxian Song","Penglei Sun","Piaopiao Jin","Yi Ren","Yu Zheng","Zhixu Li","Xiaowen Chu","Yue Zhang","Tiefeng Li","Jason Gu"],"url":"https://arxiv.org/abs/2301.11564"}
{"created":"2025-05-01","title":"Flexible and Probabilistic Topology Tracking with Partial Optimal Transport","abstract":"In this paper, we present a flexible and probabilistic framework for tracking topological features in time-varying scalar fields using merge trees and partial optimal transport. Merge trees are topological descriptors that record the evolution of connected components in the sublevel sets of scalar fields. We present a new technique for modeling and comparing merge trees using tools from partial optimal transport. In particular, we model a merge tree as a measure network, that is, a network equipped with a probability distribution, and define a notion of distance on the space of merge trees inspired by partial optimal transport. Such a distance offers a new and flexible perspective for encoding intrinsic and extrinsic information in the comparative measures of merge trees. More importantly, it gives rise to a partial matching between topological features in time-varying data, thus enabling flexible topology tracking for scientific simulations. Furthermore, such partial matching may be interpreted as probabilistic coupling between features at adjacent time steps, which gives rise to probabilistic tracking graphs. We derive a stability result for our distance and provide numerous experiments indicating the efficacy of our framework in extracting meaningful feature tracks.","authors":["Mingzhe Li","Xinyuan Yan","Lin Yan","Tom Needham","Bei Wang"],"url":"https://arxiv.org/abs/2302.02895"}
{"created":"2025-05-01","title":"Fine-tuning Is a Surprisingly Effective Domain Adaptation Baseline in Handwriting Recognition","abstract":"In many machine learning tasks, a large general dataset and a small specialized dataset are available. In such situations, various domain adaptation methods can be used to adapt a general model to the target dataset. We show that in the case of neural networks trained for handwriting recognition using CTC, simple fine-tuning with data augmentation works surprisingly well in such scenarios and that it is resistant to overfitting even for very small target domain datasets. We evaluated the behavior of fine-tuning with respect to augmentation, training data size, and quality of the pre-trained network, both in writer-dependent and writer-independent settings. On a large real-world dataset, fine-tuning on new writers provided an average relative CER improvement of 25 % for 16 text lines and 50 % for 256 text lines.","authors":["Jan Koh\\'ut","Michal Hradi\\v{s}"],"url":"https://arxiv.org/abs/2302.06308"}
{"created":"2025-05-01","title":"Towards Writing Style Adaptation in Handwriting Recognition","abstract":"One of the challenges of handwriting recognition is to transcribe a large number of vastly different writing styles. State-of-the-art approaches do not explicitly use information about the writer's style, which may be limiting overall accuracy due to various ambiguities. We explore models with writer-dependent parameters which take the writer's identity as an additional input. The proposed models can be trained on datasets with partitions likely written by a single author (e.g. single letter, diary, or chronicle). We propose a Writer Style Block (WSB), an adaptive instance normalization layer conditioned on learned embeddings of the partitions. We experimented with various placements and settings of WSB and contrastively pre-trained embeddings. We show that our approach outperforms a baseline with no WSB in a writer-dependent scenario and that it is possible to estimate embeddings for new writers. However, domain adaptation using simple fine-tuning in a writer-independent setting provides superior accuracy at a similar computational cost. The proposed approach should be further investigated in terms of training stability and embedding regularization to overcome such a baseline.","authors":["Jan Koh\\'ut","Michal Hradi\\v{s}","Martin Ki\\v{s}\\v{s}"],"url":"https://arxiv.org/abs/2302.06318"}
{"created":"2025-05-01","title":"VecFontSDF: Learning to Reconstruct and Synthesize High-quality Vector Fonts via Signed Distance Functions","abstract":"Font design is of vital importance in the digital content design and modern printing industry. Developing algorithms capable of automatically synthesizing vector fonts can significantly facilitate the font design process. However, existing methods mainly concentrate on raster image generation, and only a few approaches can directly synthesize vector fonts. This paper proposes an end-to-end trainable method, VecFontSDF, to reconstruct and synthesize high-quality vector fonts using signed distance functions (SDFs). Specifically, based on the proposed SDF-based implicit shape representation, VecFontSDF learns to model each glyph as shape primitives enclosed by several parabolic curves, which can be precisely converted to quadratic B\\'ezier curves that are widely used in vector font products. In this manner, most image generation methods can be easily extended to synthesize vector fonts. Qualitative and quantitative experiments conducted on a publicly-available dataset demonstrate that our method obtains high-quality results on several tasks, including vector font reconstruction, interpolation, and few-shot vector font synthesis, markedly outperforming the state of the art.","authors":["Zeqing Xia","Bojun Xiong","Zhouhui Lian"],"url":"https://arxiv.org/abs/2303.12675"}
{"created":"2025-05-01","title":"Joint Message Detection and Channel Estimation for Unsourced Random Access in Cell-Free User-Centric Wireless Networks","abstract":"We consider unsourced random access (uRA) in a cell-free (CF) user-centric wireless network, where a large number of potential users compete for a random access slot, while only a finite subset is active. The random access users transmit codewords of length $L$ symbols from a shared codebook, which are received by $B$ geographically distributed radio units (RUs) equipped with $M$ antennas each. Our goal is to devise and analyze a \\emph{centralized} decoder to detect the transmitted messages (without prior knowledge of the active users) and estimate the corresponding channel state information. A specific challenge lies in the fact that, due to the geographically distributed nature of the CF network, there is no fixed correspondence between codewords and large-scale fading coefficients (LSFCs). This makes current activity detection approaches which make use of this fixed LSFC-codeword association not directly applicable. To overcome this problem, we propose a scheme where the access codebook is partitioned in location-based subcodes, such that users in a particular location make use of the corresponding subcode. The joint message detection and channel estimation is obtained via a novel {\\em Approximated Message Passing} (AMP) algorithm for a linear superposition of matrix-valued sources corrupted by noise. The statistical asymmetry in the fading profile and message activity leads to \\emph{different statistics} for the matrix sources, which distinguishes the AMP formulation from previous cases. In the regime where the codebook size scales linearly with $L$, while $B$ and $M$ are fixed, we present a rigorous high-dimensional (but finite-sample) analysis of the proposed AMP algorithm. Exploiting this, we then present a precise (and rigorous) large-system analysis of the message missed-detection and false-alarm rates, as well as the channel estimation mean-square error.","authors":["Burak \\c{C}akmak","Eleni Gkiouzepi","Manfred Opper","Giuseppe Caire"],"url":"https://arxiv.org/abs/2304.12290"}
{"created":"2025-05-01","title":"Generalizable Synthetic Image Detection via Language-guided Contrastive Learning","abstract":"The heightened realism of AI-generated images can be attributed to the rapid development of synthetic models, including generative adversarial networks (GANs) and diffusion models (DMs). The malevolent use of synthetic images, such as the dissemination of fake news or the creation of fake profiles, however, raises significant concerns regarding the authenticity of images. Though many forensic algorithms have been developed for detecting synthetic images, their performance, especially the generalization capability, is still far from being adequate to cope with the increasing number of synthetic models. In this work, we propose a simple yet very effective synthetic image detection method via a language-guided contrastive learning. Specifically, we augment the training images with carefully-designed textual labels, enabling us to use a joint visual-language contrastive supervision for learning a forensic feature space with better generalization. It is shown that our proposed LanguAge-guided SynThEsis Detection (LASTED) model achieves much improved generalizability to unseen image generation models and delivers promising performance that far exceeds state-of-the-art competitors over four datasets. The code is available at https://github.com/HighwayWu/LASTED.","authors":["Haiwei Wu","Jiantao Zhou","Shile Zhang"],"url":"https://arxiv.org/abs/2305.13800"}
{"created":"2025-05-01","title":"Defense Against Shortest Path Attacks","abstract":"Identifying shortest paths between nodes in a network is an important task in many applications. Recent work has shown that a malicious actor can manipulate a graph to make traffic between two nodes of interest follow their target path. In this paper, we develop a defense against such attacks by modifying the edge weights that users observe. The defender must balance inhibiting the attacker against any negative effects on benign users. Specifically, the defender's goals are: (a) recommend the shortest paths to users, (b) make the lengths of the shortest paths in the published graph close to those of the same paths in the true graph, and (c) minimize the probability of an attack. We formulate the defense as a Stackelberg game in which the defender is the leader and the attacker is the follower. We also consider a zero-sum version of the game in which the defender's goal is to minimize cost while achieving the minimum possible attack probability. We show that the defense problem is NP-hard and propose heuristic solutions for both the zero-sum and non-zero-sum settings. By relaxing some constraints of the original problem, we formulate a linear program for local optimization around a feasible point. We present defense results with both synthetic and real networks and show that our methods often reach the lower bound of the defender's cost.","authors":["Benjamin A. Miller","Zohair Shafi","Wheeler Ruml","Yevgeniy Vorobeychik","Tina Eliassi-Rad","Scott Alfeld"],"url":"https://arxiv.org/abs/2305.19083"}
{"created":"2025-05-01","title":"Semantic Positive Pairs for Enhancing Visual Representation Learning of Instance Discrimination Methods","abstract":"Self-supervised learning algorithms (SSL) based on instance discrimination have shown promising results, performing competitively or even outperforming supervised learning counterparts in some downstream tasks. Such approaches employ data augmentation to create two views of the same instance (i.e., positive pairs) and encourage the model to learn good representations by attracting these views closer in the embedding space without collapsing to the trivial solution. However, data augmentation is limited in representing positive pairs, and the repulsion process between the instances during contrastive learning may discard important features for instances that have similar categories. To address this issue, we propose an approach to identify those images with similar semantic content and treat them as positive instances, thereby reducing the chance of discarding important features during representation learning and increasing the richness of the latent representation. Our approach is generic and could work with any self-supervised instance discrimination frameworks such as MoCo and SimSiam. To evaluate our method, we run experiments on three benchmark datasets: ImageNet, STL-10 and CIFAR-10 with different instance discrimination SSL approaches. The experimental results show that our approach consistently outperforms the baseline methods across all three datasets; for instance, we improve upon the vanilla MoCo-v2 by 4.1% on ImageNet under a linear evaluation protocol over 800 epochs. We also report results on semi-supervised learning, transfer learning on downstream tasks, and object detection.","authors":["Mohammad Alkhalefi","Georgios Leontidis","Mingjun Zhong"],"url":"https://arxiv.org/abs/2306.16122"}
{"created":"2025-05-01","title":"Actual Knowledge Gain as Privacy Loss in Local Privacy Accounting","abstract":"This paper establishes the equivalence between Local Differential Privacy (LDP) and a global limit on learning any knowledge specific to a queried object. However, an output from an LDP query is not necessarily required to provide exact amount of knowledge equal to the upper bound of the learning limit. The LDP guarantee can overestimate the amount of knowledge gained by an analyst from some outputs. To address this issue, the least upper bound on the actual knowledge gain is derived and referred to as realized privacy loss. This measure is also shown to serve as an upper bound for the actual g-leakage in quantitative information flow. The gap between the LDP guarantee and realized privacy loss motivates the exploration of a more efficient privacy accounting for fully adaptive composition, where an adversary adaptively selects queries based on prior results. The Bayesian Privacy Filter is introduced to continuously accept queries until the realized privacy loss of the composed queries equals the LDP guarantee of the composition, enabling the full utilization of the privacy budget of an object. The realized privacy loss also functions as a privacy odometer for the composed queries, allowing the remaining privacy budget to accurately represent the capacity to accept new queries. Additionally, a branch-and-bound method is devised to compute the realized privacy loss when querying against continuous values. Experimental results indicate that Bayesian Privacy Filter outperforms the basic composition by a factor of one to four when composing linear and logistic regressions.","authors":["Mingen Pan"],"url":"https://arxiv.org/abs/2307.08159"}
{"created":"2025-05-01","title":"BOPIM: Bayesian Optimization for influence maximization on temporal networks","abstract":"The goal of influence maximization (IM) is to select a small set of seed nodes which maximizes the spread of influence on a network. In this work, we propose BOPIM, a Bayesian Optimization (BO) algorithm for IM on temporal networks. The IM task is well-suited for a BO solution due to its expensive and complicated objective function. There are at least two key challenges, however, that must be overcome, primarily due to the inputs coming from a cardinality-constrained, non-Euclidean, combinatorial space. The first is constructing the kernel function for the Gaussian Process regression. We propose two kernels, one based on the Hamming distance between seed sets and the other leveraging the Jaccard coefficient between node's neighbors. The second challenge is the acquisition function. For this, we use the Expected Improvement function, suitably adjusting for noise in the observations, and optimize it using a greedy algorithm to account for the cardinality constraint. In numerical experiments on real-world networks, we prove that BOPIM outperforms competing methods and yields comparable influence spreads to a gold-standard greedy algorithm while being as much as ten times faster. In addition, we find that the Hamming kernel performs favorably compared to the Jaccard kernel in nearly all settings, a somewhat surprising result as the former does not explicitly account for the graph structure. Finally, we demonstrate two ways that the proposed method can quantify uncertainty in optimal seed sets. To our knowledge, this is the first attempt to look at uncertainty in the seed sets for IM.","authors":["Eric Yanchenko"],"url":"https://arxiv.org/abs/2308.04700"}
{"created":"2025-05-01","title":"Parameterized Complexity of Simultaneous Planarity","abstract":"Given $k$ input graphs $G_1, \\dots ,G_k$, where each pair $G_i$, $G_j$ with $i \\neq j$ shares the same graph $G$, the problem Simultaneous Embedding With Fixed Edges (SEFE) asks whether there exists a planar drawing for each input graph such that all drawings coincide on $G$. While SEFE is still open for the case of two input graphs, the problem is NP-complete for $k \\geq 3$ [Schaefer, JGAA 13]. In this work, we explore the parameterized complexity of SEFE. We show that SEFE is FPT with respect to $k$ plus the vertex cover number or the feedback edge set number of the the union graph $G^\\cup = G_1 \\cup \\dots \\cup G_k$. Regarding the shared graph $G$, we show that SEFE is NP-complete, even if $G$ is a tree with maximum degree 4. Together with a known NP-hardness reduction [Angelini et al., TCS 15], this allows us to conclude that several parameters of $G$, including the maximum degree, the maximum number of degree-1 neighbors, the vertex cover number, and the number of cutvertices are intractable. We also settle the tractability of all pairs of these parameters. We give FPT algorithms for the vertex cover number plus either of the first two parameters and for the number of cutvertices plus the maximum degree, whereas we prove all remaining combinations to be intractable.","authors":["Simon D. Fink","Matthias Pfretzschner","Ignaz Rutter"],"url":"https://arxiv.org/abs/2308.11401"}
{"created":"2025-05-01","title":"Joint Modeling of Feature, Correspondence, and a Compressed Memory for Video Object Segmentation","abstract":"Current prevailing Video Object Segmentation methods follow the pipeline of extraction-then-matching, which first extracts features on current and reference frames independently, and then performs dense matching between them. This decoupled pipeline limits information propagation between frames to high-level features, hindering fine-grained details for matching. Furthermore, the pixel-wise matching lacks holistic target understanding, making it prone to disturbance by similar distractors. To address these issues, we propose a unified VOS framework, coined as JointFormer, for jointly modeling feature extraction, correspondence matching, and a compressed memory. The core Joint Modeling Block leverages attention to simultaneously extract and propagate the target information from the reference frame to the current frame and a compressed memory token. This joint scheme enables extensive multi-layer propagation beyond high-level feature space and facilitates robust instance-distinctive feature learning. To incorporate the long-term and holistic target information, we introduce a compressed memory token with a customized online updating mechanism, which aggregates target features and facilitates temporal information propagation in a frame-wise manner, enhancing global modeling consistency. Our JointFormer achieves a new state-of-the-art performance on the DAVIS 2017 val/test-dev (89.7\\% and 87.6\\%) benchmarks and the YouTube-VOS 2018/2019 val (87.0\\% and 87.0\\%) benchmarks, outperforming the existing works. To demonstrate the generalizability of our model, it is further evaluated on four new benchmarks with various difficulties, including MOSE for complex scenes, VISOR for egocentric videos, VOST for complex transformations, and LVOS for long-term videos.","authors":["Jiaming Zhang","Yutao Cui","Gangshan Wu","Limin Wang"],"url":"https://arxiv.org/abs/2308.13505"}
{"created":"2025-05-01","title":"SignDiff: Diffusion Model for American Sign Language Production","abstract":"In this paper, we propose a dual-condition diffusion pre-training model named SignDiff that can generate human sign language speakers from a skeleton pose. SignDiff has a novel Frame Reinforcement Network called FR-Net, similar to dense human pose estimation work, which enhances the correspondence between text lexical symbols and sign language dense pose frames, reduces the occurrence of multiple fingers in the diffusion model. In addition, we propose a new method for American Sign Language Production (ASLP), which can generate ASL skeletal pose videos from text input, integrating two new improved modules and a new loss function to improve the accuracy and quality of sign language skeletal posture and enhance the ability of the model to train on large-scale data. We propose the first baseline for ASL production and report the scores of 17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We evaluated our model on the previous mainstream dataset PHOENIX14T, and the experiments achieved the SOTA results. In addition, our image quality far exceeds all previous results by 10 percentage points in terms of SSIM.","authors":["Sen Fang","Chunyu Sui","Yanghao Zhou","Xuedong Zhang","Hongbin Zhong","Yapeng Tian","Chen Chen"],"url":"https://arxiv.org/abs/2308.16082"}
{"created":"2025-05-01","title":"Quantitative Energy Prediction based on Carbon Emission Analysis by DPR Framework","abstract":"This study proposes a novel analytical framework that integrates DBSCAN clustering with the Elastic Net regression model to address multifactorial problems characterized by structural complexity and multicollinearity, exemplified by carbon emissions analysis. DBSCAN is employed for unsupervised learning to objectively cluster features, while the Elastic Net is utilized for high-dimensional feature selection and complexity control. The Elastic Net is specifically chosen for its ability to balance feature selection and regularization by combining L1 (lasso) and L2 (ridge) penalties, making it particularly suited for datasets with correlated predictors. Applying this framework to energy consumption data from 46 industries in China (2000-2019) resulted in the identification of 16 categories. Emission characteristics and drivers were quantitatively assessed for each category, demonstrating the framework's capacity to identify primary emission sources and provide actionable insights. This research underscores the global applicability of the framework for analyzing complex regional challenges, such as carbon emissions, and highlights its potential to identify opportunities for emission reduction.","authors":["Xuanming Zhang"],"url":"https://arxiv.org/abs/2309.01115"}
{"created":"2025-05-01","title":"LEyes: A Lightweight Framework for Deep Learning-Based Eye Tracking using Synthetic Eye Images","abstract":"Deep learning has bolstered gaze estimation techniques, but real-world deployment has been impeded by inadequate training datasets. This problem is exacerbated by both hardware-induced variations in eye images and inherent biological differences across the recorded participants, leading to both feature and pixel-level variance that hinders the generalizability of models trained on specific datasets. While synthetic datasets can be a solution, their creation is both time and resource-intensive. To address this problem, we present a framework called Light Eyes or \"LEyes\" which, unlike conventional photorealistic methods, only models key image features required for video-based eye tracking using simple light distributions. LEyes facilitates easy configuration for training neural networks across diverse gaze-estimation tasks. We demonstrate that models trained using LEyes are consistently on-par or outperform other state-of-the-art algorithms in terms of pupil and CR localization across well-known datasets. In addition, a LEyes trained model outperforms the industry standard eye tracker using significantly more cost-effective hardware. Going forward, we are confident that LEyes will revolutionize synthetic data generation for gaze estimation models, and lead to significant improvements of the next generation video-based eye trackers.","authors":["Sean Anthony Byrne","Virmarie Maquiling","Marcus Nystr\\\"om","Enkelejda Kasneci","Diederick C. Niehorster"],"url":"https://arxiv.org/abs/2309.06129"}
{"created":"2025-05-01","title":"On Generating Explanations for Reinforcement Learning Policies: An Empirical Study","abstract":"Understanding a \\textit{reinforcement learning} policy, which guides state-to-action mappings to maximize rewards, necessitates an accompanying explanation for human comprehension. In this paper, we introduce a set of \\textit{linear temporal logic} formulae designed to provide explanations for policies, and an algorithm for searching through those formulae for the one that best explains a given policy. Our focus is on explanations that elucidate both the ultimate objectives accomplished by the policy and the prerequisite conditions it upholds throughout its execution. The effectiveness of our proposed approach is illustrated through a simulated game of capture-the-flag and a car-parking environment,","authors":["Mikihisa Yuasa","Huy T. Tran","Ramavarapu S. Sreenivas"],"url":"https://arxiv.org/abs/2309.16960"}
{"created":"2025-05-01","title":"The First Principle of Big Memory Systems","abstract":"Persistence is the first principle of big memory systems. We comprehensively analyze the vertical and horizontal extensions of existing memory hierarchy. Networks are flattening traditional storage hierarchies. We present the state-of-the-art studies upon the big memory systems, together with design methodology and implementations. We discuss the full-stack and moving persistence. In order to achieve cost efficiency and deliver high performance, we present the speculative and deterministic persistence.","authors":["Yu Hua"],"url":"https://arxiv.org/abs/2310.00428"}
{"created":"2025-05-01","title":"Performativity and Prospective Fairness","abstract":"Deploying an algorithmically informed policy is a significant intervention in the structure of society. As is increasingly acknowledged, predictive algorithms have performative effects: using them can shift the distribution of social outcomes away from the one on which the algorithms were trained. Algorithmic fairness research is usually motivated by the worry that these performative effects will exacerbate the structural inequalities that gave rise to the training data. However, standard retrospective fairness methodologies are ill-suited to predict these effects. They impose static fairness constraints that hold after the predictive algorithm is trained, but before it is deployed and, therefore, before performative effects have had a chance to kick in. However, satisfying static fairness criteria after training is not sufficient to avoid exacerbating inequality after deployment. Addressing the fundamental worry that motivates algorithmic fairness requires explicitly comparing the change in relevant structural inequalities before and after deployment. We propose a prospective methodology for estimating this post-deployment change from pre-deployment data and knowledge about the algorithmic policy. That requires a strategy for distinguishing between, and accounting for, different kinds of performative effects. In this paper, we focus on the algorithmic effect on the causally downstream outcome variable. Throughout, we are guided by an application from public administration: the use of algorithms to (1) predict who among the recently unemployed will stay unemployed for the long term and (2) targeting them with labor market programs. We illustrate our proposal by showing how to predict whether such policies will exacerbate gender inequalities in the labor market.","authors":["Sebastian Zezulka","Konstantin Genin"],"url":"https://arxiv.org/abs/2310.08349"}
{"created":"2025-05-01","title":"LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection","abstract":"In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. Ordinary least squares analyses suggest that the advantage of training with fine-grained hate speech labels is washed away with the increase in dataset size. While our research demonstrates the potential of large language models (LLMs) for hate speech detection, several limitations remain, particularly regarding the validity and the reproducibility of the results. We conclude with an exhaustive discussion of the challenges we faced in our experimentation and offer recommended best practices for future scholars designing benchmarking experiments of this kind.","authors":["Ahmad Nasir","Aadish Sharma","Kokil Jaidka","Saifuddin Ahmed"],"url":"https://arxiv.org/abs/2310.18964"}
{"created":"2025-05-01","title":"From Discrete to Continuous Binary Best-Response Dynamics: Discrete Fluctuations Almost Surely Vanish with Population Size","abstract":"In binary decision-making, individuals often choose either the rare or the common action. In the framework of evolutionary game theory, the best-response update rule can be used to model this dichotomy. Those who prefer the common action are called \\emph{coordinators}, and those who prefer the rare one are called \\emph{anticoordinators}. A finite mixed population of the two types may undergo perpetual fluctuations, the characterization of which appears to be challenging. It is particularly unknown whether the fluctuations persist as population size grows. To fill this gap, we approximate the discrete finite population dynamics of coordinators and anticoordinators with the associated mean dynamics in the form of differential inclusions. We show that the family of the state sequences of the discrete dynamics for increasing population sizes forms a generalized stochastic approximation process for the differential inclusion. On the other hand, we show that the differential inclusions always converge to an equilibrium. This implies that the reported perpetual fluctuations in the finite discrete dynamics of coordinators and anticoordinators almost surely vanish with population size. The results encourage to first analyze the often simpler {continuous-time} mean dynamics of the discrete population dynamics as the continuous-time dynamics partly reveal the asymptotic behavior of the discrete dynamics.","authors":["Azadeh Aghaeeyan","Pouria Ramazi"],"url":"https://arxiv.org/abs/2311.01995"}
{"created":"2025-05-01","title":"SoK: The Gap Between Data Rights Ideals and Reality","abstract":"As information economies burgeon, they unlock innovation and economic wealth while posing novel threats to civil liberties and altering power dynamics between individuals, companies, and governments. Legislatures have reacted with privacy laws designed to empower individuals over their data. These laws typically create rights for \"data subjects\" (individuals) to make requests of data collectors (companies and governments). The European Union General Data Protection Regulation (GDPR) exemplifies this, granting extensive data rights to data subjects, a model embraced globally. However, the question remains: do these rights-based privacy laws effectively empower individuals over their data? This paper scrutinizes these approaches by reviewing 201 interdisciplinary empirical studies, news articles, and blog posts. We pinpoint 15 key questions concerning the efficacy of rights allocations. The literature often presents conflicting results regarding the effectiveness of rights-based frameworks, but it generally emphasizes their limitations. We offer recommendations to policymakers and Computer Science (CS) groups committed to these frameworks, and suggest alternative privacy regulation approaches.","authors":["Yujin Potter","Ella Corren","Gonzalo Munilla Garrido","Chris Hoofnagle","Dawn Song"],"url":"https://arxiv.org/abs/2312.01511"}
{"created":"2025-05-01","title":"Visual Encoders for Data-Efficient Imitation Learning in Modern Video Games","abstract":"Video games have served as useful benchmarks for the decision-making community, but going beyond Atari games towards modern games has been prohibitively expensive for the vast majority of the research community. Prior work in modern video games typically relied on game-specific integration to obtain game features and enable online training, or on existing large datasets. An alternative approach is to train agents using imitation learning to play video games purely from images. However, this setting poses a fundamental question: which visual encoders obtain representations that retain information critical for decision making? To answer this question, we conduct a systematic study of imitation learning with publicly available pre-trained visual encoders compared to the typical task-specific end-to-end training approach in Minecraft, Counter-Strike: Global Offensive, and Minecraft Dungeons. Our results show that end-to-end training can be effective with comparably low-resolution images and only minutes of demonstrations, but significant improvements can be gained by utilising pre-trained encoders such as DINOv2 depending on the game. In addition to enabling effective decision making, we show that pre-trained encoders can make decision-making research in video games more accessible by significantly reducing the cost of training.","authors":["Lukas Sch\\\"afer","Logan Jones","Anssi Kanervisto","Yuhan Cao","Tabish Rashid","Raluca Georgescu","Dave Bignell","Siddhartha Sen","Andrea Trevi\\~no Gavito","Sam Devlin"],"url":"https://arxiv.org/abs/2312.02312"}
{"created":"2025-05-01","title":"Venn: Resource Management for Collaborative Learning Jobs","abstract":"In recent years, collaborative learning (CL) has emerged as a promising approach for machine learning (ML) and data science across distributed edge devices. As the deployment of CL jobs increases, they inevitably contend for limited resources. However, efficient resource scheduling in this context is challenging because of the ephemeral nature and resource heterogeneity of devices, coupled with the overlapping resource requirements of diverse CL jobs. Existing resource managers often assign devices to CL jobs randomly for simplicity and scalability, but this approach compromises job efficiency.","authors":["Jiachen Liu","Fan Lai","Ding Ding","Yiwen Zhang","Mosharaf Chowdhury"],"url":"https://arxiv.org/abs/2312.08298"}
{"created":"2025-05-01","title":"EyePreserve: Identity-Preserving Iris Synthesis","abstract":"Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to the intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying synthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities, as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model both preserves the identity when changing the pupil size, and offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear and non-linear (bio-mechanical-based) iris deformation models. Two immediate applications of the proposed approach are: (a) synthesis of, or enhancement of the existing biometric datasets for iris recognition, mimicking those acquired with iris sensors, and (b) helping forensic human experts examine iris image pairs with significant differences in pupil dilation. Images considered in this work conform to selected ISO/IEC 29794-6 quality metrics to make them applicable in biometric systems. The source codes and model weights are offered with this paper.","authors":["Siamul Karim Khan","Patrick Tinsley","Mahsa Mitcheff","Patrick Flynn","Kevin W. Bowyer","Adam Czajka"],"url":"https://arxiv.org/abs/2312.12028"}
{"created":"2025-05-01","title":"Always-Sparse Training by Growing Connections with Guided Stochastic Exploration","abstract":"The excessive computational requirements of modern artificial neural networks (ANNs) are posing limitations on the machines that can run them. Sparsification of ANNs is often motivated by time, memory and energy savings only during model inference, yielding no benefits during training. A growing body of work is now focusing on providing the benefits of model sparsification also during training. While these methods greatly improve the training efficiency, the training algorithms yielding the most accurate models still materialize the dense weights, or compute dense gradients during training. We propose an efficient, always-sparse training algorithm with excellent scaling to larger and sparser models, supported by its linear time complexity with respect to the model width during training and inference. Moreover, our guided stochastic exploration algorithm improves over the accuracy of previous sparse training methods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG, and ViT models, and compare it against a range of sparsification methods.","authors":["Mike Heddes","Narayan Srinivasa","Tony Givargis","Alexandru Nicolau"],"url":"https://arxiv.org/abs/2401.06898"}
{"created":"2025-05-01","title":"Centrality of shortest paths: Algorithms and complexity results","abstract":"The degree centrality of a node, defined as the number of nodes adjacent to it, is often used as a measure of importance of a node to the structure of a network. This metric can be extended to paths in a network, where the degree centrality of a path is defined as the number of nodes adjacent to it. In this paper, we reconsider the problem of finding the most degree-central shortest path in an unweighted network. We propose a polynomial algorithm with the worst-case running time of $O(|E||V|^2\\Delta(G))$, where $|V|$ is the number of vertices in the network, $|E|$ is the number of edges in the network, and $\\Delta(G)$ is the maximum degree of the graph. We conduct a numerical study of our algorithm on synthetic and real-world networks and compare our results to the existing literature. In addition, we show that the same problem is NP-hard when a weighted graph is considered. Furthermore, we consider other centrality measures, such as the betweenness and closeness centrality, showing that the problem of finding the most betweenness-central shortest path is solvable in polynomial time and finding the most closeness-central shortest path is NP-hard, regardless of whether the graph is weighted or not.","authors":["Johnson Phosavanh","Dmytro Matsypura"],"url":"https://arxiv.org/abs/2401.08019"}
{"created":"2025-05-01","title":"DDM: A Metric for Comparing 3D Shapes Using Directional Distance Fields","abstract":"Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. In this paper, we propose DDM, an efficient, effective, robust, and differentiable distance metric for 3D geometry data. Specifically, we construct DDM based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry. We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence. To demonstrate the advantage of our DDM, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. Extensive experiments show that our DDM achieves significantly higher accuracy under all tasks. As a generic distance metric, DDM has the potential to advance the field of 3D geometric modeling. The source code is available at https://github.com/rsy6318/DDM.","authors":["Siyu Ren","Junhui Hou","Xiaodong Chen","Hongkai Xiong","Wenping Wang"],"url":"https://arxiv.org/abs/2401.09736"}
{"created":"2025-05-01","title":"Tight Bounds on the Message Complexity of Distributed Tree Verification","abstract":"We consider the message complexity of verifying whether a given subgraph of the communication network forms a tree with specific properties both in the KT-$\\rho$ (nodes know their $\\rho$-hop neighborhood, including node IDs) and the KT-$0$ (nodes do not have this knowledge) models. We develop a rather general framework that helps in establishing tight lower bounds for various tree verification problems. We also consider two different verification requirements: namely that every node detects in the case the input is incorrect, as well as the requirement that at least one node detects. The results are stronger than previous ones in the sense that we assume that each node knows the number $n$ of nodes in the graph (in some cases) or an $\\alpha$ approximation of $n$ (in other cases). For spanning tree verification, we show that the message complexity inherently depends on the quality of the given approximation of $n$: We show a tight lower bound of $\\Omega(n^2)$ for the case $\\alpha \\ge \\sqrt{2}$ and a much better upper bound (i.e., $O(n \\log n)$) when nodes are given a tighter approximation. On the other hand, our framework also yields an $\\Omega(n^2)$ lower bound on the message complexity of verifying a minimum spanning tree (MST), which reveals a polynomial separation between ST verification and MST verification. This result holds for randomized algorithms with perfect knowledge of the network size, and even when just one node detects illegal inputs, thus improving over the work of Kor, Korman, and Peleg (2013). For verifying a $d$-approximate BFS tree, we show that the same lower bound holds even if nodes know $n$ exactly, however, the lower bound is sensitive to $d$, which is the stretch parameter.","authors":["Shay Kutten","Peter Robinson","Ming Ming Tan"],"url":"https://arxiv.org/abs/2401.11991"}
{"created":"2025-05-01","title":"Fast Partition-Based Cross-Validation With Centering and Scaling for $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$","abstract":"We present algorithms that substantially accelerate partition-based cross-validation for machine learning models that require matrix products $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Our algorithms have applications in model selection for, for example, principal component analysis (PCA), principal component regression (PCR), ridge regression (RR), ordinary least squares (OLS), and partial least squares (PLS). Our algorithms support all combinations of column-wise centering and scaling of $\\mathbf{X}$ and $\\mathbf{Y}$, and we demonstrate in our accompanying implementation that this adds only a manageable, practical constant over efficient variants without preprocessing. We prove the correctness of our algorithms under a fold-based partitioning scheme and show that the running time is independent of the number of folds; that is, they have the same time complexity as that of computing $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ and space complexity equivalent to storing $\\mathbf{X}$, $\\mathbf{Y}$, $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$, and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Importantly, unlike alternatives found in the literature, we avoid data leakage due to preprocessing. We achieve these results by eliminating redundant computations in the overlap between training partitions. Concretely, we show how to manipulate $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ using only samples from the validation partition to obtain the preprocessed training partition-wise $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. To our knowledge, we are the first to derive correct and efficient cross-validation algorithms for any of the $16$ combinations of column-wise centering and scaling, for which we also prove only $12$ give distinct matrix products.","authors":["Ole-Christian Galbo Engstr{\\o}m","Martin Holm Jensen"],"url":"https://arxiv.org/abs/2401.13185"}
{"created":"2025-05-01","title":"Round Trip Translation Defence against Large Language Model Jailbreaking Attacks","abstract":"Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence","authors":["Canaan Yung","Hadi Mohaghegh Dolatabadi","Sarah Erfani","Christopher Leckie"],"url":"https://arxiv.org/abs/2402.13517"}
{"created":"2025-05-01","title":"Mochi: Collision Detection for Spherical Particles using GPU Ray Tracing","abstract":"Efficient Discrete Collision Detection (DCD) uses indexing structures for acceleration, and developing these structures demands meticulous programmer efforts to achieve performance. The Ray-Tracing (RT) architecture of GPUs builds and traverses an indexing structure called Bounding Volume Hierarchy (BVH) and performs geometric intersection tests, which are all the essential components of a DCD kernel. However, BVHs built by the RT architecture are neither accessible nor programmable; the only way to use this architecture is to launch rays and map DCD queries to ray traversal.","authors":["Durga Keerthi Mandarapu","Isaac Fuksman","Artem Pelenitsyn","Gilbert Bernstein","Milind Kulkarni"],"url":"https://arxiv.org/abs/2402.14801"}
{"created":"2025-05-01","title":"'SSL?! What on earth is that?': Towards Designing Age-Inclusive Secure Smartphone Browsing","abstract":"Owing to the increase in 'certified' phishing websites, there is a steady increase in the number of phishing cases and general susceptibility to phishing. Trust mechanisms (e.g., HTTPS Lock Indicators, SSL Certificates) that help differentiate genuine and phishing websites should therefore be evaluated for their effectiveness in preventing vulnerable users from accessing phishing websites. In this article, we present a study involving 18 adults (male-6; female-12) and 12 older adults (male-4; female-8) to understand the usability of current trust mechanisms and preferred modalities in a conceptualized mechanism. In the first part of the study, using Chrome browser on Android, we asked the participants to browse a banking website and a government website for digital particulars. We asked them to identify which one of the two was a phishing website, rate the usability of both websites and provide qualitative feedback on the trust mechanisms. In the second part, we conceptualized an alternative trust mechanism, which allows seeking social, community and AI-based support to make website trust-related decisions. Herein, we asked the participants as to which modality (social, community or AI) they prefer to seek support from and why it is preferred. Using the current trust mechanisms, none of the participants were able to identify the phishing website. As the participants rated the current mechanisms poorly in terms of usability, they expressed various difficulties that largely did not differ between adults and older adults. In the conceptualized mechanism, we observed a notable difference in the preferred modalities, in that, older adults primarily preferred social support. In addition to these overall findings, specific observations suggest that future trust mechanisms should not only consider age-specific needs but also incorporate substantial improvement in terms of usability.","authors":["Pavithren V. S. Pakianathan","L. Siddharth","Sujithra Raviselvam","Kristin L. Wood","Hyowon Lee","Pin Sym Foong","Jianying Zhou","Simon Tangi Perrault"],"url":"https://arxiv.org/abs/2403.02145"}
{"created":"2025-05-01","title":"Neural Redshift: Random Networks are not Random Functions","abstract":"Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.","authors":["Damien Teney","Armand Nicolicioiu","Valentin Hartmann","Ehsan Abbasnejad"],"url":"https://arxiv.org/abs/2403.02241"}
{"created":"2025-05-01","title":"HeadEvolver: Text to Head Avatars via Expressive and Attribute-Preserving Mesh Deformation","abstract":"Current text-to-avatar methods often rely on implicit representations (e.g., NeRF, SDF, and DMTet), leading to 3D content that artists cannot easily edit and animate in graphics software. This paper introduces a novel framework for generating stylized head avatars from text guidance, which leverages locally learnable mesh deformation and 2D diffusion priors to achieve high-quality digital assets for attribute-preserving manipulation. Given a template mesh, our method represents mesh deformation with per-face Jacobians and adaptively modulates local deformation using a learnable vector field. This vector field enables anisotropic scaling while preserving the rotation of vertices, which can better express identity and geometric details. We employ landmark- and contour-based regularization terms to balance the expressiveness and plausibility of generated avatars from multiple views without relying on any specific shape prior. Our framework can generate realistic shapes and textures that can be further edited via text, while supporting seamless editing using the preserved attributes from the template mesh, such as 3DMM parameters, blendshapes, and UV coordinates. Extensive experiments demonstrate that our framework can generate diverse and expressive head avatars with high-quality meshes that artists can easily manipulate in graphics software, facilitating downstream applications such as efficient asset creation and animation with preserved attributes.","authors":["Duotun Wang","Hengyu Meng","Zeyu Cai","Zhijing Shao","Qianxi Liu","Lin Wang","Mingming Fan","Xiaohang Zhan","Zeyu Wang"],"url":"https://arxiv.org/abs/2403.09326"}
{"created":"2025-05-01","title":"MeDSLIP: Medical Dual-Stream Language-Image Pre-training with Pathology-Anatomy Semantic Alignment","abstract":"Pathology and anatomy are two essential groups of semantics in medical data. Pathology describes what the diseases are, while anatomy explains where the diseases occur. They describe diseases from different perspectives, providing complementary insights into diseases. Thus, properly understanding these semantics and their relationships can enhance medical vision-language models (VLMs). However, pathology and anatomy semantics are usually entangled in medical data, hindering VLMs from explicitly modeling these semantics and their relationships. To address this challenge, we propose MeDSLIP, a novel Medical Dual-Stream Language-Image Pre-training pipeline, to disentangle pathology and anatomy semantics and model the relationships between them. We introduce a dual-stream mechanism in MeDSLIP to explicitly disentangle medical semantics into pathology-relevant and anatomy-relevant streams and align visual and textual information within each stream. Furthermore, we propose an interaction modeling module with prototypical contrastive learning loss and intra-image contrastive learning loss to regularize the relationships between pathology and anatomy semantics. We apply MeDSLIP to chest X-ray analysis and conduct comprehensive evaluations with four benchmark datasets: NIH CXR14, RSNA Pneumonia, SIIM-ACR Pneumothorax, and COVIDx CXR-4. The results demonstrate MeDSLIP's superior generalizability and transferability across different scenarios. The code is available at https://github.com/Shef-AIRE/MeDSLIP, and the pre-trained model is released at https://huggingface.co/pykale/MeDSLIP.","authors":["Wenrui Fan","Mohammod N. I. Suvon","Shuo Zhou","Xianyuan Liu","Samer Alabed","Venet Osmani","Andrew J. Swift","Chen Chen","Haiping Lu"],"url":"https://arxiv.org/abs/2403.10635"}
{"created":"2025-05-01","title":"RankingSHAP -- Listwise Feature Attribution Explanations for Ranking Models","abstract":"While SHAP (SHapley Additive exPlanations) and other feature attribution methods are commonly employed to explain model predictions, their application within information retrieval (IR), particularly for complex outputs such as ranked lists, remains limited. Existing attribution methods typically provide pointwise explanations, focusing on why a single document received a high-ranking score, rather than considering the relationships between documents in a ranked list. We present three key contributions to address this gap. First, we rigorously define listwise feature attribution for ranking models. Secondly, we introduce RankingSHAP, extending the popular SHAP framework to accommodate listwise ranking attribution, addressing a significant methodological gap in the field. Third, we propose two novel evaluation paradigms for assessing the faithfulness of attributions in learning-to-rank models, measuring the correctness and completeness of the explanation with respect to different aspects. Through experiments on standard learning-to-rank datasets, we demonstrate RankingSHAP's practical application while identifying the constraints of selection-based explanations. We further employ a simulated study with an interpretable model to showcase how listwise ranking attributions can be used to examine model decisions and conduct a qualitative evaluation of explanations. Due to the contrastive nature of the ranking task, our understanding of ranking model decisions can substantially benefit from feature attribution explanations like RankingSHAP.","authors":["Maria Heuss","Maarten de Rijke","Avishek Anand"],"url":"https://arxiv.org/abs/2403.16085"}
{"created":"2025-05-01","title":"Garment3DGen: 3D Garment Stylization and Texture Generation","abstract":"We introduce Garment3DGen a new method to synthesize 3D garment assets from a base mesh given a single input image as guidance. Our proposed approach allows users to generate 3D textured clothes based on both real and synthetic images, such as those generated by text prompts. The generated assets can be directly draped and simulated on human bodies. We leverage the recent progress of image-to-3D diffusion methods to generate 3D garment geometries. However, since these geometries cannot be utilized directly for downstream tasks, we propose to use them as pseudo ground-truth and set up a mesh deformation optimization procedure that deforms a base template mesh to match the generated 3D target. Carefully designed losses allow the base mesh to freely deform towards the desired target, yet preserve mesh quality and topology such that they can be simulated. Finally, we generate high-fidelity texture maps that are globally and locally consistent and faithfully capture the input guidance, allowing us to render the generated 3D assets. With Garment3DGen users can generate the simulation-ready 3D garment of their choice without the need of artist intervention. We present a plethora of quantitative and qualitative comparisons on various assets and demonstrate that Garment3DGen unlocks key applications ranging from sketch-to-simulated garments or interacting with the garments in VR. Code is publicly available.","authors":["Nikolaos Sarafianos","Tuur Stuyck","Xiaoyu Xiang","Yilei Li","Jovan Popovic","Rakesh Ranjan"],"url":"https://arxiv.org/abs/2403.18816"}
{"created":"2025-05-01","title":"Alternating Optimization Approach for Computing $\\alpha$-Mutual Information and $\\alpha$-Capacity","abstract":"This study presents alternating optimization (AO) algorithms for computing $\\alpha$-mutual information ($\\alpha$-MI) and $\\alpha$-capacity based on variational characterizations of $\\alpha$-MI using a reverse channel. Specifically, we derive several variational characterizations of Sibson, Arimoto, Augustin--Csisz{\\' a}r, and Lapidoth--Pfister MI and introduce novel AO algorithms for computing $\\alpha$-MI and $\\alpha$-capacity; their performances for computing $\\alpha$-capacity are also compared. The comparison results show that the AO algorithm based on the Sibson MI's characterization has the fastest convergence speed.","authors":["Akira Kamatsuka","Koki Kazama","Takahiro Yoshida"],"url":"https://arxiv.org/abs/2404.10950"}
{"created":"2025-05-01","title":"Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs","abstract":"Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian Pidgin spoken by approximately 120M speakers and it is a mixed language (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a spoken language until recently, there are some online platforms (e.g., Wikipedia), publishing in written Naija as well. West African Pidgin English (WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the internet to a wider audience not only in Nigeria but also in other West African countries (e.g., Cameroon and Ghana). Through statistical analyses and Machine Translation experiments, our paper shows that these two pidgin varieties do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on WAPE. In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples. In addition to the statistical analyses, we also provide historical information on both pidgins as well as insights from the interviews conducted with volunteer Wikipedia contributors in Naija.","authors":["David Ifeoluwa Adelani","A. Seza Do\\u{g}ru\\\"oz","Iyanuoluwa Shode","Anuoluwapo Aremu"],"url":"https://arxiv.org/abs/2404.19442"}
{"created":"2025-05-01","title":"Decentralization of Ethereum's Builder Market","abstract":"Blockchains protect an ecosystem worth more than $500bn with strong security properties derived from the principle of decentralization. Is today's blockchain decentralized? In this paper, we empirically studied one of the least decentralized parts of Ethereum, its builder market.","authors":["Sen Yang","Kartik Nayak","Fan Zhang"],"url":"https://arxiv.org/abs/2405.01329"}
{"created":"2025-05-01","title":"Color: A Framework for Applying Graph Coloring to Subgraph Cardinality Estimation","abstract":"Graph workloads pose a particularly challenging problem for query optimizers. They typically feature large queries made up of entirely many-to-many joins with complex correlations. This puts significant stress on traditional cardinality estimation methods which generally see catastrophic errors when estimating the size of queries with only a handful of joins. To overcome this, we propose COLOR, a framework for subgraph cardinality estimation which applies insights from graph compression theory to produce a compact summary that captures the global topology of the data graph. Further, we identify several key optimizations that enable tractable estimation over this summary even for large query graphs. We then evaluate several designs within this framework and find that they improve accuracy by up to 10$^3$x over all competing methods while maintaining fast inference, a small memory footprint, efficient construction, and graceful degradation under updates.","authors":["Kyle Deeds","Diandre Sabale","Moe Kayali","Dan Suciu"],"url":"https://arxiv.org/abs/2405.06767"}
{"created":"2025-05-01","title":"Historically Relevant Event Structuring for Temporal Knowledge Graph Reasoning","abstract":"Temporal Knowledge Graph (TKG) reasoning focuses on predicting events through historical information within snapshots distributed on a timeline. Existing studies mainly concentrate on two perspectives of leveraging the history of TKGs, including capturing evolution of each recent snapshot or correlations among global historical facts. Despite the achieved significant accomplishments, these models still fall short of I) investigating the impact of multi-granular interactions across recent snapshots, and II) harnessing the expressive semantics of significant links accorded with queries throughout the entire history, particularly events exerting a profound impact on the future. These inadequacies restrict representation ability to reflect historical dependencies and future trends thoroughly. To overcome these drawbacks, we propose an innovative TKG reasoning approach towards \\textbf{His}torically \\textbf{R}elevant \\textbf{E}vents \\textbf{S}tructuring (HisRES). Concretely, HisRES comprises two distinctive modules excelling in structuring historically relevant events within TKGs, including a multi-granularity evolutionary encoder that captures structural and temporal dependencies of the most recent snapshots, and a global relevance encoder that concentrates on crucial correlations among events relevant to queries from the entire history. Furthermore, HisRES incorporates a self-gating mechanism for adaptively merging multi-granularity recent and historically relevant structuring representations. Extensive experiments on four event-based benchmarks demonstrate the state-of-the-art performance of HisRES and indicate the superiority and effectiveness of structuring historical relevance for TKG reasoning.","authors":["Jinchuan Zhang","Ming Sun","Chong Mu","Jinhao Zhang","Quanjiang Guo","Ling Tian"],"url":"https://arxiv.org/abs/2405.10621"}
{"created":"2025-05-01","title":"SignLLM: Sign Language Production Large Language Models","abstract":"In this paper, we propose SignLLM, a multilingual Sign Language Production (SLP) large language model, which includes two novel multilingual SLP modes MLSF and Prompt2LangGloss that allow sign language gestures generation from query texts input and question-style prompts input respectively. Both modes can use a new RL loss based on reinforcement learning and a new RL module named Priority Learning Channel. These RL components can accelerate the training by enhancing the model's capability to sample high-quality data. To train SignLLM, we introduce Prompt2Sign, a comprehensive multilingual sign language dataset, which builds from public data, including American Sign Language (ASL) and seven others. This dataset standardizes information by extracting pose information from sign language videos into a unified compressed format. We extensively evaluate SignLLM, demonstrating that our model achieves state-of-the-art performance on SLP tasks across eight sign languages.","authors":["Sen Fang","Chen Chen","Lei Wang","Ce Zheng","Chunyu Sui","Yapeng Tian"],"url":"https://arxiv.org/abs/2405.10718"}
{"created":"2025-05-01","title":"Centralized vs Decentralized Monitors for Hyperproperties","abstract":"This paper focuses on the runtime verification of hyperproperties expressed in Hyper-recHML, an expressive yet simple logic for describing properties of sets of traces. To this end, we consider a simple language of monitors that observe sets of system executions and report verdicts w.r.t. a given Hyper-recHML formula. We first employ a unique omniscient monitor that centrally observes all system traces. Since centralised monitors are not ideal for distributed settings, we also provide a language for decentralized monitors, where each trace has a dedicated monitor; these monitors yield a unique verdict by communicating their observations to one another. For both the centralized and the decentralized settings, we provide a synthesis procedure that, given a formula, yields a monitor that is correct (i.e., sound and violation complete). A key step in proving the correctness of the synthesis for decentralized monitors is a result showing that, for each formula, the synthesized centralized monitor and its corresponding decentralized one are weakly bisimilar for a suitable notion of weak bisimulation.","authors":["Luca Aceto","Antonis Achilleos","Elli Anastasiadi","Adrian Francalanza","Daniele Gorla","Jana Wagemaker"],"url":"https://arxiv.org/abs/2405.12882"}
{"created":"2025-05-01","title":"Emotive Speech-to-Text Interfaces in XR: A Narrative Review of Psychophysiological and Accessibility Advances","abstract":"This narrative review on emotional expression in Speech-to-Text (STT) interfaces with Extended Reality (XR) aims to identify advancements, limitations, and research gaps in incorporating emotional expression into transcribed text generated by STT systems. Using a rigorous search strategy, relevant articles published between 2020 and 2024 are extracted and categorized into themes such as communication enhancement technologies, innovations in captioning, visual and affective augmentation, emotion recognition in AR and VR, and empathic machines. The findings reveal the evolution of tools and techniques to meet the needs of individuals with hearing impairments, showcasing innovations in live transcription, closed captioning, AR, VR, and emotion recognition technologies. Despite improvements in accessibility, the absence of emotional nuance in transcribed text remains a significant communication challenge. The study underscores the urgency for innovations in STT technology to capture emotional expressions. The research discusses integrating emotional expression into text through strategies like animated text captions, emojilization tools, and models associating emotions with animation properties. Extending these efforts into AR and VR environments opens new possibilities for immersive and emotionally resonant experiences, especially in educational contexts. The study also explores empathic applications in healthcare, education, and human-robot interactions, highlighting the potential for personalized and effective interactions. The multidisciplinary nature of the literature underscores the potential for collaborative and interdisciplinary research.","authors":["Sunday David Ubur","Denis Gracanin"],"url":"https://arxiv.org/abs/2405.13924"}
{"created":"2025-05-01","title":"Emergence of a High-Dimensional Abstraction Phase in Language Transformers","abstract":"A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.","authors":["Emily Cheng","Diego Doimo","Corentin Kervadec","Iuri Macocco","Jade Yu","Alessandro Laio","Marco Baroni"],"url":"https://arxiv.org/abs/2405.15471"}
{"created":"2025-05-01","title":"Variational Offline Multi-agent Skill Discovery","abstract":"Skills are effective temporal abstractions established for sequential decision making, which enable efficient hierarchical learning for long-horizon tasks and facilitate multi-task learning through their transferability. Despite extensive research, research gaps remain in multi-agent scenarios, particularly for automatically extracting subgroup coordination patterns in a multi-agent task. In this case, we propose two novel auto-encoder schemes: VO-MASD-3D and VO-MASD-Hier, to simultaneously capture subgroup- and temporal-level abstractions and form multi-agent skills, which firstly solves the aforementioned challenge. An essential algorithm component of these schemes is a dynamic grouping function that can automatically detect latent subgroups based on agent interactions in a task. Further, our method can be applied to offline multi-task data, and the discovered subgroup skills can be transferred across relevant tasks without retraining. Empirical evaluations on StarCraft tasks indicate that our approach significantly outperforms existing hierarchical multi-agent reinforcement learning (MARL) methods. Moreover, skills discovered using our method can effectively reduce the learning difficulty in MARL scenarios with delayed and sparse reward signals. The codebase is available at https://github.com/LucasCJYSDL/VOMASD.","authors":["Jiayu Chen","Tian Lan","Vaneet Aggarwal"],"url":"https://arxiv.org/abs/2405.16386"}
{"created":"2025-05-01","title":"SoK: Leveraging Transformers for Malware Analysis","abstract":"The introduction of transformers has been an important breakthrough for AI research and application as transformers are the foundation behind Generative AI. A promising application domain for transformers is cybersecurity, in particular the malware domain analysis. The reason is the flexibility of the transformer models in handling long sequential features and understanding contextual relationships. However, as the use of transformers for malware analysis is still in the infancy stage, it is critical to evaluate, systematize, and contextualize existing literature to foster future research. This Systematization of Knowledge (SoK) paper aims to provide a comprehensive analysis of transformer-based approaches designed for malware analysis. Based on our systematic analysis of existing knowledge, we structure and propose taxonomies based on: (a) how different transformers are adapted, organized, and modified across various use cases; and (b) how diverse feature types and their representation capabilities are reflected. We also provide an inventory of datasets used to explore multiple research avenues in the use of transformers for malware analysis and discuss open challenges with future research directions. We believe that this SoK paper will assist the research community in gaining detailed insights from existing work and will serve as a foundational resource for implementing novel research using transformers for malware analysis.","authors":["Pradip Kunwar","Kshitiz Aryal","Maanak Gupta","Mahmoud Abdelsalam","Elisa Bertino"],"url":"https://arxiv.org/abs/2405.17190"}
{"created":"2025-05-01","title":"Formalizing the notions of non-interactive and interactive algorithms","abstract":"An earlier paper gives an account of a quest for a satisfactory formalization of the classical informal notion of an algorithm. That notion only covers algorithms that are deterministic and non-interactive. In this paper, an attempt is made to generalize the results of that quest first to a notion of an algorithm that covers both deterministic and non-deterministic algorithms that are non-interactive and then further to a notion of an algorithm that covers both deterministic and non-deterministic algorithms that are interactive. The notions of an non-interactive proto-algorithm and an interactive proto-algorithm are introduced. Non-interactive algorithms and interactive algorithms are expected to be equivalence classes of non-interactive proto-algorithms and interactive proto-algorithms, respectively, under an appropriate equivalence relation. On both non-interactive proto-algorithms and interactive proto-algorithms, three equivalence relations are defined. Two of them are deemed to be bounds for an appropriate equivalence relation and the third is likely an appropriate one.","authors":["C. A. Middelburg"],"url":"https://arxiv.org/abs/2405.19037"}
{"created":"2025-05-01","title":"Uncovering Bias in Large Vision-Language Models at Scale with Counterfactuals","abstract":"With the advent of Large Language Models (LLMs) possessing increasingly impressive capabilities, a number of Large Vision-Language Models (LVLMs) have been proposed to augment LLMs with visual inputs. Such models condition generated text on both an input image and a text prompt, enabling a variety of use cases such as visual question answering and multimodal chat. While prior studies have examined the social biases contained in text generated by LLMs, this topic has been relatively unexplored in LVLMs. Examining social biases in LVLMs is particularly challenging due to the confounding contributions of bias induced by information contained across the text and visual modalities. To address this challenging problem, we conduct a large-scale study of text generated by different LVLMs under counterfactual changes to input images, producing over 57 million responses from popular models. Our multi-dimensional bias evaluation framework reveals that social attributes such as perceived race, gender, and physical characteristics depicted in images can significantly influence the generation of toxic content, competency-associated words, harmful stereotypes, and numerical ratings of individuals.","authors":["Phillip Howard","Kathleen C. Fraser","Anahita Bhiwandiwalla","Svetlana Kiritchenko"],"url":"https://arxiv.org/abs/2405.20152"}
{"created":"2025-05-01","title":"Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-based Decision-Making Systems","abstract":"Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.","authors":["Ruochen Jiao","Shaoyuan Xie","Justin Yue","Takami Sato","Lixu Wang","Yixuan Wang","Qi Alfred Chen","Qi Zhu"],"url":"https://arxiv.org/abs/2405.20774"}
{"created":"2025-05-01","title":"Demystifying AI Platform Design for Distributed Inference of Next-Generation LLM models","abstract":"Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these gigantic models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With constant innovation in LLM serving optimizations and model architecture evolving at breakneck speed, the hardware requirements to meet Service Level Objectives (SLOs) remain an open research question.","authors":["Abhimanyu Bambhaniya","Ritik Raj","Geonhwa Jeong","Souvik Kundu","Sudarshan Srinivasan","Suvinay Subramanian","Midhilesh Elavazhagan","Madhu Kumar","Tushar Krishna"],"url":"https://arxiv.org/abs/2406.01698"}
{"created":"2025-05-01","title":"Capacity Credit Evaluation of Generalized Energy Storage Considering Strategic Capacity Withholding and Decision-Dependent Uncertainty","abstract":"This paper proposes a novel capacity credit evaluation framework to accurately quantify the contribution of generalized energy storage (GES) to resource adequacy, considering both strategic capacity withholding and decision-dependent uncertainty (DDU). To this end, we establish a market-oriented risk-averse coordinated dispatch method to capture the cross-market reliable operation of GES. The proposed method is sequentially implemented along with the Monte Carlo simulation process, coordinating the pre-dispatched price arbitrage and capacity withholding in the energy market with adequacy-oriented re-dispatch during capacity market calls. In addition to decision-independent uncertainties in operational states and baseline behavior, we explicitly address the inherent DDU of GES (i.e., the uncertainty of available discharge capacity affected by the incentives and accumulated discomfort) during the re-dispatch stage using the proposed data-driven distributional robust chance-constrained approach. Furthermore, a capacity credit metric called equivalent storage capacity substitution is introduced to quantify the equivalent deterministic storage capacity of uncertain GES. Simulations on the modified IEEE RTS-79 benchmark system with 20 years real-world data from Elia demonstrate that the proposed method yields accurate capacity credit and improved economic performance. We show that the capacity credit of GES increases with more strategic capacity withholding but decreases with more DDU levels. Key factors, such as capacity withholding and DDU structure impacting GES's capacity credit are analyzed with insights into capacity market decision-making.","authors":["Ning Qi","Pierre Pinson","Mads R. Almassalkhi","Yingrui Zhuang","Yifan Su","Feng Liu"],"url":"https://arxiv.org/abs/2406.07338"}
{"created":"2025-05-01","title":"FADE: Towards Fairness-aware Generation for Domain Generalization via Classifier-Guided Score-based Diffusion Models","abstract":"Fairness-aware domain generalization (FairDG) has emerged as a critical challenge for deploying trustworthy AI systems, particularly in scenarios involving distribution shifts. Traditional methods for addressing fairness have failed in domain generalization due to their lack of consideration for distribution shifts. Although disentanglement has been used to tackle FairDG, it is limited by its strong assumptions. To overcome these limitations, we propose Fairness-aware Classifier-Guided Score-based Diffusion Models (FADE) as a novel approach to effectively address the FairDG issue. Specifically, we first pre-train a score-based diffusion model (SDM) and two classifiers to equip the model with strong generalization capabilities across different domains. Then, we guide the SDM using these pre-trained classifiers to effectively eliminate sensitive information from the generated data. Finally, the generated fair data is used to train downstream classifiers, ensuring robust performance under new data distributions. Extensive experiments on three real-world datasets demonstrate that FADE not only enhances fairness but also improves accuracy in the presence of distribution shifts. Additionally, FADE outperforms existing methods in achieving the best accuracy-fairness trade-offs.","authors":["Yujie Lin","Dong Li","Minglai Shao","Guihong Wan","Chen Zhao"],"url":"https://arxiv.org/abs/2406.09495"}
{"created":"2025-05-01","title":"Semi-Variance Reduction for Fair Federated Learning","abstract":"Ensuring fairness in a Federated Learning (FL) system, i.e., a satisfactory performance for all of the participating diverse clients, is an important and challenging problem. There are multiple fair FL algorithms in the literature, which have been relatively successful in providing fairness. However, these algorithms mostly emphasize on the loss functions of worst-off clients to improve their performance, which often results in the suppression of well-performing ones. As a consequence, they usually sacrifice the system's overall average performance for achieving fairness. Motivated by this and inspired by two well-known risk modeling methods in Finance, Mean-Variance and Mean-Semi-Variance, we propose and study two new fair FL algorithms, Variance Reduction (VRed) and Semi-Variance Reduction (SemiVRed). VRed encourages equality between clients' loss functions by penalizing their variance. In contrast, SemiVRed penalizes the discrepancy of only the worst-off clients' loss functions from the average loss. Through extensive experiments on multiple vision and language datasets, we show that, SemiVRed achieves SoTA performance in scenarios with heterogeneous data distributions and improves both fairness and system overall average performance.","authors":["Saber Malekmohammadi","Yaoliang Yu"],"url":"https://arxiv.org/abs/2406.16193"}
{"created":"2025-05-01","title":"Uncertainty for SVBRDF Acquisition using Frequency Analysis","abstract":"This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view captures. Under uncontrolled illumination and unstructured viewpoints, there is no guarantee that the observations contain enough information to reconstruct the appearance properties of a captured object. We study this ambiguity, or uncertainty, using entropy and accelerate the analysis by using the frequency domain, rather than the domain of incoming and outgoing viewing angles. The result is a method that computes a map of uncertainty over an entire object within a millisecond. We find that the frequency model allows us to recover SVBRDF parameters with competitive performance, that the accelerated entropy computation matches results with a physically-based path tracer, and that there is a positive correlation between error and uncertainty. We then show that the uncertainty map can be applied to improve SVBRDF acquisition using capture guidance, sharing information on the surface, and using a diffusion model to inpaint uncertain regions. Our code is available at https://github.com/rubenwiersma/svbrdf_uncertainty.","authors":["Ruben Wiersma","Julien Philip","Milo\\v{s} Ha\\v{s}an","Krishna Mullia","Fujun Luan","Elmar Eisemann","Valentin Deschaintre"],"url":"https://arxiv.org/abs/2406.17774"}
{"created":"2025-05-01","title":"MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications","abstract":"The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality data set, mainly due to privacy-related issues. Moreover, the recent rising of Large Multimodal Models (LMM) leads to a need for multimodal medical data sets, where clinical reports and findings are attached to the corresponding CT or MR scans. This paper illustrates the entire workflow for building the data set MedPix 2.0. Starting from the well-known multimodal data set MedPix, mainly used by physicians, nurses and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure where noisy samples were removed, thus creating a MongoDB database. Along with the data set, we developed a GUI aimed at navigating efficiently the MongoDB instance, and obtaining the raw data that can be easily used for training and/or fine-tuning LMMs. To enforce this point, we also propose a CLIP-based model trained on MedPix 2.0 for scanning modality and location classification tasks. MedPix 2.0 is available on GitHub","authors":["Irene Siragusa","Salvatore Contino","Massimo La Ciura","Rosario Alicata","Roberto Pirrone"],"url":"https://arxiv.org/abs/2407.02994"}
{"created":"2025-05-01","title":"Simultaneous System Identification and Model Predictive Control with No Dynamic Regret","abstract":"We provide an algorithm for the simultaneous system identification and model predictive control of nonlinear systems. The algorithm has finite-time near-optimality guarantees and asymptotically converges to the optimal (non-causal) controller. Particularly, the algorithm enjoys sublinear dynamic regret, defined herein as the suboptimality against an optimal clairvoyant controller that knows how the unknown disturbances and system dynamics will adapt to its actions. The algorithm is self-supervised and applies to control-affine systems with unknown dynamics and disturbances that can be expressed in reproducing kernel Hilbert spaces. Such spaces can model external disturbances and modeling errors that can even be adaptive to the system's state and control input. For example, they can model wind and wave disturbances to aerial and marine vehicles, or inaccurate model parameters such as inertia of mechanical systems. The algorithm first generates random Fourier features that are used to approximate the unknown dynamics or disturbances. Then, it employs model predictive control based on the current learned model of the unknown dynamics (or disturbances). The model of the unknown dynamics is updated online using least squares based on the data collected while controlling the system. We validate our algorithm in both hardware experiments and physics-based simulations. The simulations include (i) a cart-pole aiming to maintain the pole upright despite inaccurate model parameters, and (ii) a quadrotor aiming to track reference trajectories despite unmodeled aerodynamic drag effects. The hardware experiments include a quadrotor aiming to track a circular trajectory despite unmodeled aerodynamic drag effects, ground effects, and wind disturbances.","authors":["Hongyu Zhou","Vasileios Tzoumas"],"url":"https://arxiv.org/abs/2407.04143"}
{"created":"2025-05-01","title":"BEVWorld: A Multimodal World Simulator for Autonomous Driving via Scene-Level BEV Latents","abstract":"World models have attracted increasing attention in autonomous driving for their ability to forecast potential future scenarios. In this paper, we propose BEVWorld, a novel framework that transforms multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for holistic environment modeling. The proposed world model consists of two main components: a multi-modal tokenizer and a latent BEV sequence diffusion model. The multi-modal tokenizer first encodes heterogeneous sensory data, and its decoder reconstructs the latent BEV tokens into LiDAR and surround-view image observations via ray-casting rendering in a self-supervised manner. This enables joint modeling and bidirectional encoding-decoding of panoramic imagery and point cloud data within a shared spatial representation. On top of this, the latent BEV sequence diffusion model performs temporally consistent forecasting of future scenes, conditioned on high-level action tokens, enabling scene-level reasoning over time. Extensive experiments demonstrate the effectiveness of BEVWorld on autonomous driving benchmarks, showcasing its capability in realistic future scene generation and its benefits for downstream tasks such as perception and motion prediction.","authors":["Yumeng Zhang","Shi Gong","Kaixin Xiong","Xiaoqing Ye","Xiaofan Li","Xiao Tan","Fan Wang","Jizhou Huang","Hua Wu","Haifeng Wang"],"url":"https://arxiv.org/abs/2407.05679"}
{"created":"2025-05-01","title":"Lossless data compression by large models","abstract":"Modern data compression methods are slowly reaching their limits after 80 years of research, millions of papers, and wide range of applications. Yet, the extravagant 6G communication speed requirement raises a major open question for revolutionary new ideas of data compression. We have previously shown all understanding or learning are compression, under reasonable assumptions. Large language models (LLMs) understand data better than ever before. Can they help us to compress data? The LLMs may be seen to approximate the uncomputable Solomonoff induction. Therefore, under this new uncomputable paradigm, we present LMCompress. LMCompress shatters all previous lossless compression algorithms, doubling the lossless compression ratios of JPEG-XL for images, FLAC for audios, and H.264 for videos, and quadrupling the compression ratio of bz2 for texts. The better a large model understands the data, the better LMCompress compresses.","authors":["Ziguang Li","Chao Huang","Xuliang Wang","Haibo Hu","Cole Wyeth","Dongbo Bu","Quan Yu","Wen Gao","Xingwu Liu","Ming Li"],"url":"https://arxiv.org/abs/2407.07723"}
{"created":"2025-05-01","title":"CoopScenes: Multi-Scene Infrastructure and Vehicle Data for Advancing Collective Perception in Autonomous Driving","abstract":"The increasing complexity of urban environments has underscored the potential of effective collective perception systems. To address these challenges, we present the CoopScenes dataset, a large-scale, multi-scene dataset that provides synchronized sensor data from both the ego-vehicle and the supporting infrastructure.The dataset provides 104 minutes of spatially and temporally synchronized data at 10 Hz, resulting in 62,000 frames. It achieves competitive synchronization with a mean deviation of only 2.3 ms. Additionally the dataset includes a novel procedure for precise registration of point cloud data from the ego-vehicle and infrastructure sensors, automated annotation pipelines, and an open-source anonymization pipeline for faces and license plates. Covering nine diverse scenes with 100 maneuvers, the dataset features scenarios such as public transport hubs, city construction sites, and high-speed rural roads across three cities in the Stuttgart region, Germany. The full dataset amounts to 527 GB of data and is provided in the .4mse format, making it easily accessible through our comprehensive development kit. By providing precise, large-scale data, CoopScenes facilitates research in collective perception, real-time sensor registration, and cooperative intelligent systems for urban mobility, including machine learning-based approaches.","authors":["Marcel Vosshans","Alexander Baumann","Matthias Drueppel","Omar Ait-Aider","Youcef Mezouar","Thao Dang","Markus Enzweiler"],"url":"https://arxiv.org/abs/2407.08261"}
{"created":"2025-05-01","title":"Let Network Decide What to Learn: Symbolic Music Understanding Model Based on Large-scale Adversarial Pre-training","abstract":"As a crucial aspect of Music Information Retrieval (MIR), Symbolic Music Understanding (SMU) has garnered significant attention for its potential to assist both musicians and enthusiasts in learning and creating music. Recently, pre-trained language models have been widely adopted in SMU due to the substantial similarities between symbolic music and natural language, as well as the ability of these models to leverage limited music data effectively. However, some studies have shown the common pre-trained methods like Mask Language Model (MLM) may introduce bias issues like racism discrimination in Natural Language Process (NLP) and affects the performance of downstream tasks, which also happens in SMU. This bias often arises when masked tokens cannot be inferred from their context, forcing the model to overfit the training set instead of generalizing. To address this challenge, we propose Adversarial-MidiBERT for SMU, which adaptively determines what to mask during MLM via a masker network, rather than employing random masking. By avoiding the masking of tokens that are difficult to infer from context, our model is better equipped to capture contextual structures and relationships, rather than merely conforming to the training data distribution. We evaluate our method across four SMU tasks, and our approach demonstrates excellent performance in all cases. The code for our model is publicly available at https://github.com/RS2002/Adversarial-MidiBERT .","authors":["Zijian Zhao"],"url":"https://arxiv.org/abs/2407.08306"}
{"created":"2025-05-01","title":"Patched RTC: evaluating LLMs for diverse software development tasks","abstract":"This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on \"outer loop\" activities such as bug fixing, code review, and documentation updates. Patched RTC extends the original Round-Trip Correctness method to work with any LLM and downstream task, offering a self-evaluating framework that measures consistency and robustness of model responses without human intervention. The study demonstrates a correlation between Patched RTC scores and task-specific accuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation. We implement Patched RTC in an open-source framework called patchwork, allowing for transparent evaluation during inference across various patchflows. Experiments comparing GPT-3.5 and GPT-4 models across different software development tasks reveal that Patched RTC effectively distinguishes model performance and task difficulty. The paper also explores the impact of consistency prompts on improving model accuracy, suggesting that Patched RTC can guide prompt refinement and model selection for complex software development workflows.","authors":["Asankhaya Sharma"],"url":"https://arxiv.org/abs/2407.16557"}
{"created":"2025-05-01","title":"Patched MOA: optimizing inference for diverse software development tasks","abstract":"This paper introduces Patched MOA (Mixture of Agents), an inference optimization technique that significantly enhances the performance of large language models (LLMs) across diverse software development tasks. We evaluate three inference optimization algorithms - Best of N, Mixture of Agents, and Monte Carlo Tree Search and demonstrate that Patched MOA can boost the performance of smaller models to surpass that of larger, more expensive models. Notably, our approach improves the gpt-4o-mini model's performance on the Arena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of the cost. We also apply Patched MOA to various software development workflows, showing consistent improvements in task completion rates. Our method is model-agnostic, transparent to end-users, and can be easily integrated into existing LLM pipelines. This work contributes to the growing field of LLM optimization, offering a cost-effective solution for enhancing model performance without the need for fine-tuning or larger models. Our implementation is open-source and available at https://github.com/codelion/optillm.","authors":["Asankhaya Sharma"],"url":"https://arxiv.org/abs/2407.18521"}
{"created":"2025-05-01","title":"Remote Staking with Optimal Economic Safety","abstract":"The idea of security sharing traces back to Nakamoto's introduction of merge mining, a technique that enables Bitcoin miners to reuse their hash power to bootstrap and secure other Proof-of-Work (PoW) blockchains. However, with the rise of Proof-of-Stake (PoS) chains (where merge mining is inapplicable) there is a need for new methods of Bitcoin security sharing. In this paper, we introduce remote staking as a technique that allows Bitcoin holders to use their idle assets to secure PoS chains. Our remote staking protocol achieves optimal economic safety: in the event of a safety violation on the PoS chain, at least one-third of the Bitcoin stake securing the chain is slashed. We make two key technical contributions to enable this: 1) A cryptographic protocol that enables slashing of Bitcoin stake despite the absence of smart contracts on Bitcoin; 2) A secure unbonding mechanism that guarantees slashing can occur before the stake is withdrawn from Bitcoin if a safety violation occurs on the PoS chain. Our design is entirely modular and can be integrated with any PoS chain as the security consumer and any chain (including Bitcoin) as the security provider. A version of this protocol was deployed to mainnet in August 2024 and has since accumulated over 4.1 billion USD worth of staked bitcoins.","authors":["Xinshu Dong","Orfeas Stefanos Thyfronitis Litos","Ertem Nusret Tas","David Tse","Robin Linus Woll","Lei Yang","Mingchao Yu"],"url":"https://arxiv.org/abs/2408.01896"}
{"created":"2025-05-01","title":"HateSieve: A Contrastive Learning Framework for Detecting and Segmenting Hateful Content in Multimodal Memes","abstract":"Amidst the rise of Large Multimodal Models (LMMs) and their widespread application in generating and interpreting complex content, the risk of propagating biased and harmful memes remains significant. Current safety measures often fail to detect subtly integrated hateful content within ``Confounder Memes''. To address this, we introduce \\textsc{HateSieve}, a new framework designed to enhance the detection and segmentation of hateful elements in memes. \\textsc{HateSieve} features a novel Contrastive Meme Generator that creates semantically paired memes, a customized triplet dataset for contrastive learning, and an Image-Text Alignment module that produces context-aware embeddings for accurate meme segmentation. Empirical experiments on the Hateful Meme Dataset show that \\textsc{HateSieve} not only surpasses existing LMMs in performance with fewer trainable parameters but also offers a robust mechanism for precisely identifying and isolating hateful content. \\textcolor{red}{Caution: Contains academic discussions of hate speech; viewer discretion advised.}","authors":["Xuanyu Su","Yansong Li","Diana Inkpen","Nathalie Japkowicz"],"url":"https://arxiv.org/abs/2408.05794"}
{"created":"2025-05-01","title":"Prompt Recovery for Image Generation Models: A Comparative Study of Discrete Optimizers","abstract":"Recovering natural language prompts for image generation models, solely based on the generated images is a difficult discrete optimization problem. In this work, we present the first head-to-head comparison of recent discrete optimization techniques for the problem of prompt inversion. We evaluate Greedy Coordinate Gradients (GCG), PEZ , Random Search, AutoDAN and BLIP2's image captioner across various evaluation metrics related to the quality of inverted prompts and the quality of the images generated by the inverted prompts. We find that focusing on the CLIP similarity between the inverted prompts and the ground truth image acts as a poor proxy for the similarity between ground truth image and the image generated by the inverted prompts. While the discrete optimizers effectively minimize their objectives, simply using responses from a well-trained captioner often leads to generated images that more closely resemble those produced by the original prompts.","authors":["Joshua Nathaniel Williams","Avi Schwarzschild","Yutong He","J. Zico Kolter"],"url":"https://arxiv.org/abs/2408.06502"}
{"created":"2025-05-01","title":"How to Solve Contextual Goal-Oriented Problems with Offline Datasets?","abstract":"We present a novel method, Contextual goal-Oriented Data Augmentation (CODA), which uses commonly available unlabeled trajectories and context-goal pairs to solve Contextual Goal-Oriented (CGO) problems. By carefully constructing an action-augmented MDP that is equivalent to the original MDP, CODA creates a fully labeled transition dataset under training contexts without additional approximation error. We conduct a novel theoretical analysis to demonstrate CODA's capability to solve CGO problems in the offline data setup. Empirical results also showcase the effectiveness of CODA, which outperforms other baseline methods across various context-goal relationships of CGO problem. This approach offers a promising direction to solving CGO problems using offline datasets.","authors":["Ying Fan","Jingling Li","Adith Swaminathan","Aditya Modi","Ching-An Cheng"],"url":"https://arxiv.org/abs/2408.07753"}
{"created":"2025-05-01","title":"SustainDC: Benchmarking for Sustainable Data Center Control","abstract":"Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.","authors":["Avisek Naug","Antonio Guillen","Ricardo Luna","Vineet Gundecha","Desik Rengarajan","Sahand Ghorbanpour","Sajad Mousavi","Ashwin Ramesh Babu","Dejan Markovikj","Lekhapriya D Kashyap","Soumyendu Sarkar"],"url":"https://arxiv.org/abs/2408.07841"}
{"created":"2025-05-01","title":"Beyond Winning Strategies: Admissible and Admissible Winning Strategies for Quantitative Reachability Games","abstract":"Classical reactive synthesis approaches aim to synthesize a reactive system that always satisfies a given specifications. These approaches often reduce to playing a two-player zero-sum game where the goal is to synthesize a winning strategy. However, in many pragmatic domains, such as robotics, a winning strategy does not always exist, yet it is desirable for the system to make an effort to satisfy its requirements instead of \"giving up\". To this end, this paper investigates the notion of admissible strategies, which formalize \"doing-your-best\", in quantitative reachability games. We show that, unlike the qualitative case, quantitative admissible strategies are history-dependent even for finite payoff functions, making synthesis a challenging task. In addition, we prove that admissible strategies always exist but may produce undesirable optimistic behaviors. To mitigate this, we propose admissible winning strategies, which enforce the best possible outcome while being admissible. We show that both strategies always exist but are not memoryless. We provide necessary and sufficient conditions for the existence of both strategies and propose synthesis algorithms. Finally, we illustrate the strategies on gridworld and robot manipulator domains.","authors":["Karan Muvvala","Qi Heng Ho","Morteza Lahijanian"],"url":"https://arxiv.org/abs/2408.13369"}
{"created":"2025-05-01","title":"Best of two worlds: Cartesian sampling and volume computation for distance-constrained configuration spaces using Cayley coordinates","abstract":"Volume calculation of configurational spaces acts as a vital part in configurational entropy calculation, which contributes towards calculating free energy landscape for molecular systems. In this article, we present our sampling-based volume computation method using distance-based Cayley coordinate, mitigating drawbacks: our method guarantees that the sampling procedure stays in lower-dimensional coordinate space (instead of higher-dimensional Cartesian space) throughout the whole process; and our mapping function, utilizing Cayley parameterization, can be applied in both directions with low computational cost. Our method uniformly samples and computes a discrete volume measure of a Cartesian configuration space of point sets satisfying systems of distance inequality constraints. The systems belong to a large natural class whose feasible configuration spaces are effectively lower dimensional subsets of high dimensional ambient space. Their topological complexity makes discrete volume computation challenging, yet necessary in several application scenarios including free energy calculation in soft matter assembly modeling. The algorithm runs in linear time and empirically sub-linear space in the number of grid hypercubes (used to define the discrete volume measure) \\textit{that intersect} the configuration space. In other words, the number of wasted grid cube visits is insignificant compared to prevailing methods typically based on gradient descent. Specifically, the traversal stays within the feasible configuration space by viewing it as a branched covering, using a recent theory of Cayley or distance coordinates to convexify the base space, and by employing a space-efficient, frontier hypercube traversal data structure. A software implementation and comparison with existing methods is provided.","authors":["Yichi Zhang","Meera Sitharam"],"url":"https://arxiv.org/abs/2408.16946"}
{"created":"2025-05-01","title":"Oh the Prices You'll See: Designing a Fair Exchange System to Mitigate Personalized Pricing","abstract":"Many online marketplaces personalize prices based on consumer attributes. Since these prices are private, consumers may be unaware that they have spent more on a good than the lowest possible price, and cannot easily take action to pay less. In this paper, we introduce a fairness-centered exchange system that takes advantage of personalized pricing, while still allowing consumers to individually benefit. Our system produces a matching of consumers to promote trading; the lower-paying consumer buys the good for the higher-paying consumer for some fee. We explore various modeling choices and fairness targets to determine which schema will leave consumers best off, while also earning revenue for the system itself. We show that when consumers individually negotiate the transaction price, and our fairness objective is to minimize mean net cost, we are able to achieve the most fair outcomes. Conversely, when transaction prices are centrally set, consumers are often unwilling to transact. When price dispersion (or range) is high, the system can reduce the mean net cost to each individual by $66\\%$, or the mean net cost to a group by $69\\%$. We find that a high dispersion of original prices is necessary for our system to be viable. Higher dispersion can actually lead to decreased net price paid by consumers, and act as a check against extreme personalization, increasing seller accountability. Our results provide theoretical evidence that such a system could improve fairness for consumers while sustaining itself financially.","authors":["Aditya Karan","Naina Balepur","Hari Sundaram"],"url":"https://arxiv.org/abs/2409.02777"}
{"created":"2025-05-01","title":"Comparison of Kinematics and Kinetics Between OpenCap and a Marker-Based Motion Capture System in Cycling","abstract":"This study evaluates the agreement of marker-based and markerless (OpenCap) motion capture systems in assessing joint kinematics and kinetics during cycling. Markerless systems, such as OpenCap, offer the advantage of capturing natural movements without physical markers, making them more practical for real-world applications. However, the agreement of OpenCap with a marker-based system, particularly in cycling, remains underexplored. Ten participants cycled at varying speeds and resistances while motion data were recorded using both systems. Key metrics, including joint angles, moments, and joint reaction loads, were computed using OpenSim and compared using root mean squared error (RMSE) per trial across participants, Pearson correlation coefficients (r) per trial across participants and repeated measures Bland-Altman to control trials dependency within subject. Results revealed very strong agreement (r GT 0.9) for hip (flexion/extension), knee (flexion/extension), and ankle (dorsiflexion/plantarflexion) joint angles.","authors":["Reza Kakavand","Reza Ahmadi","Atousa Parsaei","W. Brent Edwards","Amin Komeili"],"url":"https://arxiv.org/abs/2409.03766"}
{"created":"2025-05-01","title":"Kraus is King: High-order Completely Positive and Trace Preserving (CPTP) Low Rank Method for the Lindblad Master Equation","abstract":"We design high order accurate methods that exploit low rank structure in the density matrix while respecting the essential structure of the Lindblad equation. Our methods preserves complete positivity and are trace preserving.","authors":["Daniel Appelo","Yingda Cheng"],"url":"https://arxiv.org/abs/2409.08898"}
{"created":"2025-05-01","title":"Underwater Image Enhancement via Dehazing and Color Restoration","abstract":"Underwater visual imaging is crucial for marine engineering, but it suffers from low contrast, blurriness, and color degradation, which hinders downstream analysis. Existing underwater image enhancement methods often treat the haze and color cast as a unified degradation process, neglecting their inherent independence while overlooking their synergistic relationship. To overcome this limitation, we propose a Vision Transformer (ViT)-based network (referred to as WaterFormer) to improve underwater image quality. WaterFormer contains three major components: a dehazing block (DehazeFormer Block) to capture the self-correlated haze features and extract deep-level features, a Color Restoration Block (CRB) to capture self-correlated color cast features, and a Channel Fusion Block (CFB) that dynamically integrates these decoupled features to achieve comprehensive enhancement. To ensure authenticity, a soft reconstruction layer based on the underwater imaging physics model is included. Further, a Chromatic Consistency Loss and Sobel Color Loss are designed to respectively preserve color fidelity and enhance structural details during network training. Comprehensive experimental results demonstrate that WaterFormer outperforms other state-of-the-art methods in enhancing underwater images.","authors":["Chengqin Wu","Shuai Yu","Tuyan Luo","Qiuhua Rao","Qingson Hu","Jingxiang Xu","Lijun Zhang"],"url":"https://arxiv.org/abs/2409.09779"}
{"created":"2025-05-01","title":"Estimating Wage Disparities Using Foundation Models","abstract":"The rise of foundation models marks a paradigm shift in machine learning: instead of training specialized models from scratch, foundation models are first trained on massive datasets before being adapted or fine-tuned to make predictions on smaller datasets. Initially developed for text, foundation models have also excelled at making predictions about social science data. However, while many estimation problems in the social sciences use prediction as an intermediate step, they ultimately require different criteria for success. In this paper, we develop methods for fine-tuning foundation models to perform these estimation problems. We first characterize an omitted variable bias that can arise when a foundation model is only fine-tuned to maximize predictive accuracy. We then provide a novel set of conditions for fine-tuning under which estimates derived from a foundation model are root-n-consistent. Based on this theory, we develop new fine-tuning algorithms that empirically mitigate this omitted variable bias. To demonstrate our ideas, we study gender wage decomposition. This is a statistical estimation problem from econometrics where the goal is to decompose the gender wage gap into components that can and cannot be explained by career histories of workers. Classical methods for decomposing the wage gap employ simple predictive models of wages which condition on coarse summaries of career history that may omit factors that are important for explaining the gap. Instead, we use a custom-built foundation model to decompose the gender wage gap, which captures a richer representation of career history. Using data from the Panel Study of Income Dynamics, we find that career history explains more of the gender wage gap than standard econometric models can measure, and we identify elements of career history that are omitted by standard models but are important for explaining the wage gap.","authors":["Keyon Vafa","Susan Athey","David M. Blei"],"url":"https://arxiv.org/abs/2409.09894"}
{"created":"2025-05-01","title":"A New Upper Bound for Distributed Hypothesis Testing Using the Auxiliary Receiver Approach","abstract":"This paper employs the add-and-subtract technique of the auxiliary receiver approach to establish a new upper bound for the distributed hypothesis testing problem. This new bound has fewer assumptions than the upper bound proposed by Rahman and Wagner, is at least as tight as the bound by Rahman and Wagner, and can outperform it in certain Gaussian settings. Conceptually speaking, unlike Rahman and Wagner, who view their additional receiver as side information, we view it as an auxiliary receiver and use a different manipulation for single-letterization.","authors":["Zhenduo Wen","Amin Gohari"],"url":"https://arxiv.org/abs/2409.14148"}
{"created":"2025-05-01","title":"Looped Transformers for Length Generalization","abstract":"Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an adaptive number of steps significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a RASP-L operation - a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.","authors":["Ying Fan","Yilun Du","Kannan Ramchandran","Kangwook Lee"],"url":"https://arxiv.org/abs/2409.15647"}
{"created":"2025-05-01","title":"Downscaling Extreme Precipitation with Wasserstein Regularized Diffusion","abstract":"Understanding the risks posed by extreme rainfall events necessitates both high-resolution products (to assess localized hazards) and extensive historical records (to capture rare occurrences). Radar and mesonet networks provide kilometer-scale precipitation fields, but with limited historical records and geographical coverage. Conversely, global gauge and blended products span decades, yet their coarse 30-50 km grids obscure local extremes. This work introduces Wasserstein Regularized Diffusion (WassDiff), a generative downscaling framework that integrates diffusion modeling with a distribution-matching (Wasserstein) regularizer, suppressing bias throughout the entire generative denoising process. Conditioned on 55 km CPC gauge-based precipitation and the 31 km ERA5 reanalysis, WassDiff generates 1 km precipitation estimates that remain well-calibrated to targets across the full intensity range, including the extremes. Comprehensive evaluations demonstrate that WassDiff outperforms existing state-of-the-art downscaling methods, delivering lower reconstruction error and reduced bias. Case studies further demonstrate its ability to reproduce realistic fine-scale structures and accurate peak intensities from extreme weather phenomena, such as tropical storms and cold fronts. By unlocking decades of high-resolution rainfall information from globally available coarse records, WassDiff offers a practical pathway toward more accurate flood-risk assessments and climate-adaptation planning.","authors":["Yuhao Liu","James Doss-Gollin","Qiushi Dai","Guha Balakrishnan","Ashok Veeraraghavan"],"url":"https://arxiv.org/abs/2410.00381"}
{"created":"2025-05-01","title":"A Formal Framework for Understanding Length Generalization in Transformers","abstract":"A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers.","authors":["Xinting Huang","Andy Yang","Satwik Bhattamishra","Yash Sarrof","Andreas Krebs","Hattie Zhou","Preetum Nakkiran","Michael Hahn"],"url":"https://arxiv.org/abs/2410.02140"}
{"created":"2025-05-01","title":"R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust Task-Oriented Communications","abstract":"Collaborative perception enhances sensing in multirobot and vehicular networks by fusing information from multiple agents, improving perception accuracy and sensing range. However, mobility and non-rigid sensor mounts introduce extrinsic calibration errors, necessitating online calibration, further complicated by limited overlap in sensing regions. Moreover, maintaining fresh information is crucial for timely and accurate sensing. To address calibration errors and ensure timely and accurate perception, we propose a robust task-oriented communication strategy to optimize online self-calibration and efficient feature sharing for Real-time Adaptive Collaborative Perception (R-ACP). Specifically, we first formulate an Age of Perceived Targets (AoPT) minimization problem to capture data timeliness of multi-view streaming. Then, in the calibration phase, we introduce a channel-aware self-calibration technique based on reidentification (Re-ID), which adaptively compresses key features according to channel capacities, effectively addressing calibration issues via spatial and temporal cross-camera correlations. In the streaming phase, we tackle the trade-off between bandwidth and inference accuracy by leveraging an Information Bottleneck (IB) based encoding method to adjust video compression rates based on task relevance, thereby reducing communication overhead and latency. Finally, we design a priority-aware network to filter corrupted features to mitigate performance degradation from packet corruption. Extensive studies demonstrate that our framework outperforms five baselines, improving multiple object detection accuracy (MODA) by 25.49% and reducing communication costs by 51.36% under severely poor channel conditions. Code will be made publicly available: github.com/fangzr/R-ACP.","authors":["Zhengru Fang","Jingjing Wang","Yanan Ma","Yihang Tao","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"url":"https://arxiv.org/abs/2410.04168"}
{"created":"2025-05-01","title":"Extended convexity and smoothness and their applications in deep learning","abstract":"Classical assumptions like strong convexity and Lipschitz smoothness often fail to capture the nature of deep learning optimization problems, which are typically non-convex and non-smooth, making traditional analyses less applicable. This study aims to elucidate the mechanisms of non-convex optimization in deep learning by extending the conventional notions of strong convexity and Lipschitz smoothness. By leveraging these concepts, we prove that, under the established constraints, the empirical risk minimization problem is equivalent to optimizing the local gradient norm and structural error, which together constitute the upper and lower bounds of the empirical risk. Furthermore, our analysis demonstrates that the stochastic gradient descent (SGD) algorithm can effectively minimize the local gradient norm. Additionally, techniques like skip connections, over-parameterization, and random parameter initialization are shown to help control the structural error. Ultimately, we validate the core conclusions of this paper through extensive experiments. Theoretical analysis and experimental results indicate that our findings provide new insights into the mechanisms of non-convex optimization in deep learning.","authors":["Binchuan Qi","Wei Gong","Li Li"],"url":"https://arxiv.org/abs/2410.05807"}
{"created":"2025-05-01","title":"Adaptive Random Fourier Features Training Stabilized By Resampling With Applications in Image Regression","abstract":"This paper presents an enhanced adaptive random Fourier features (ARFF) training algorithm for shallow neural networks, building upon the work introduced in \"Adaptive Random Fourier Features with Metropolis Sampling\", Kammonen et al., \\emph{Foundations of Data Science}, 2(3):309--332, 2020. This improved method uses a particle filter-type resampling technique to stabilize the training process and reduce the sensitivity to parameter choices. The Metropolis test can also be omitted when resampling is used, reducing the number of hyperparameters by one and reducing the computational cost per iteration compared to the ARFF method. We present comprehensive numerical experiments demonstrating the efficacy of the proposed algorithm in function regression tasks as a stand-alone method and as a pretraining step before gradient-based optimization, using the Adam optimizer. Furthermore, we apply the proposed algorithm to a simple image regression problem, illustrating its utility in sampling frequencies for the random Fourier features (RFF) layer of coordinate-based multilayer perceptrons. In this context, we use the proposed algorithm to sample the parameters of the RFF layer in an automated manner.","authors":["Aku Kammonen","Anamika Pandey","Erik von Schwerin","Ra\\'ul Tempone"],"url":"https://arxiv.org/abs/2410.06399"}
{"created":"2025-05-01","title":"Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models","abstract":"Multi-lingual ability transfer has become increasingly important for the broad application of large language models (LLMs). Existing work highly relies on training with the multi-lingual ability-related data, which may be not available for low-resource languages. To solve it, we propose a Multi-lingual Ability Extraction and Transfer approach, named as MAET. Our key idea is to decompose and extract language-agnostic ability-related weights from LLMs, and transfer them across different languages by simple addition and subtraction operations without training. Specially, our MAET consists of the extraction and transfer stages. In the extraction stage, we firstly locate key neurons that are highly related to specific abilities, and then employ them to extract the transferable ability-specific weights. In the transfer stage, we further select the ability-related parameter tensors, and design the merging strategy based on the linguistic and ability specific weights, to build the multi-lingual ability-enhanced LLM. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on mathematical and scientific tasks in both high-resource lingual and low-resource lingual scenarios. Experiment results have shown that MAET can effectively and efficiently extract and transfer the advanced abilities, and outperform training-based baseline methods. Our code and data are available at https://github.com/RUCAIBox/MAET.","authors":["Zhipeng Chen","Kun Zhou","Liang Song","Wayne Xin Zhao","Bingning Wang","Weipeng Chen","Ji-Rong Wen"],"url":"https://arxiv.org/abs/2410.07825"}
{"created":"2025-05-01","title":"Masked Generative Priors Improve World Models Sequence Modelling Capabilities","abstract":"Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments. Model-based approaches, which are RL methods with world models that predict environment dynamics, are among the most promising directions for improving data efficiency, forming a critical step toward bridging the gap between research and real-world deployment. In particular, world models enhance sample efficiency by learning in imagination, which involves training a generative sequence model of the environment in a self-supervised manner. Recently, Masked Generative Modelling has emerged as a more efficient and superior inductive bias for modelling and generating token sequences. Building on the Efficient Stochastic Transformer-based World Models (STORM) architecture, we replace the traditional MLP prior with a Masked Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM. We evaluate our model on two downstream tasks: reinforcement learning and video prediction. GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari 100k benchmark. Moreover, we apply Transformer-based World Models to continuous action environments for the first time, addressing a significant gap in prior research. To achieve this, we employ a state mixer function that integrates latent state representations with actions, enabling our model to handle continuous control tasks. We validate this approach through qualitative and quantitative analyses on the DeepMind Control Suite, showcasing the effectiveness of Transformer-based World Models in this new domain. Our results highlight the versatility and efficacy of the MaskGIT dynamics prior, paving the way for more accurate world models and effective RL policies.","authors":["Cristian Meo","Mircea Lica","Zarif Ikram","Akihiro Nakano","Vedant Shah","Aniket Rajiv Didolkar","Dianbo Liu","Anirudh Goyal","Justin Dauwels"],"url":"https://arxiv.org/abs/2410.07836"}
{"created":"2025-05-01","title":"Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback","abstract":"In interactive imitation learning (IL), uncertainty quantification offers a way for the learner (i.e. robot) to contend with distribution shifts encountered during deployment by actively seeking additional feedback from an expert (i.e. human) online. Prior works use mechanisms like ensemble disagreement or Monte Carlo dropout to quantify when black-box IL policies are uncertain; however, these approaches can lead to overconfident estimates when faced with deployment-time distribution shifts. Instead, we contend that we need uncertainty quantification algorithms that can leverage the expert human feedback received during deployment time to adapt the robot's uncertainty online. To tackle this, we draw upon online conformal prediction, a distribution-free method for constructing prediction intervals online given a stream of ground-truth labels. Human labels, however, are intermittent in the interactive IL setting. Thus, from the conformal prediction side, we introduce a novel uncertainty quantification algorithm called intermittent quantile tracking (IQT) that leverages a probabilistic model of intermittent labels, maintains asymptotic coverage guarantees, and empirically achieves desired coverage levels. From the interactive IL side, we develop ConformalDAgger, a new approach wherein the robot uses prediction intervals calibrated by IQT as a reliable measure of deployment-time uncertainty to actively query for more expert feedback. We compare ConformalDAgger to prior uncertainty-aware DAgger methods in scenarios where the distribution shift is (and isn't) present because of changes in the expert's policy. We find that in simulated and hardware deployments on a 7DOF robotic manipulator, ConformalDAgger detects high uncertainty when the expert shifts and increases the number of interventions compared to baselines, allowing the robot to more quickly learn the new behavior.","authors":["Michelle Zhao","Reid Simmons","Henny Admoni","Aaditya Ramdas","Andrea Bajcsy"],"url":"https://arxiv.org/abs/2410.08852"}
{"created":"2025-05-01","title":"Convergence Analysis of a Schrodinger Problem with Moving Boundary","abstract":"In this article, we present the mathematical analysis of the convergence of the linearized Crank-Nicolson Galerkin method for a nonlinear Schrodinger problem related to a domain with a moving boundary. The convergence analysis of the numerical method is carried out for both semi-discrete and fully discrete problems. An optimal error estimate in the $L^2$-norm with order ${O}(\\tau^2+ h^s),~ 2\\leq s\\leq r$, where $h$ is the finite element mesh size parameter, $\\tau$ is the time step, and $r-1$ represents the degree of the finite element polynomial basis. Numerical simulations are provided to confirm the consistency between theoretical and numerical results, validating the method and the order of convergence for different degrees $p\\geq 1$ of the Lagrange polynomials and also for Hermite polynomials (degree $p=3$), which form the basis of the approximate solution.","authors":["Daniel G. Alfaro Vigo (Institute of Computing","Federal University of Rio de Janeiro","Rio de Janeiro","Brazil","Graduate Program in Informatics","Federal University of Rio de Janeiro","Rio de Janeiro","Brazil)","Daniele C. R. Gomes (Graduate Program in Informatics","Federal University of Rio de Janeiro","Rio de Janeiro","Brazil)","Bruno A. do Carmo (Graduate Program in Informatics","Federal University of Rio de Janeiro","Rio de Janeiro","Brazil)","Mauro A. Rincon (Institute of Computing","Federal University of Rio de Janeiro","Rio de Janeiro","Brazil","Graduate Program in Informatics","Federal University of Rio de Janeiro","Rio de Janeiro","Brazil)"],"url":"https://arxiv.org/abs/2410.08910"}
{"created":"2025-05-01","title":"SmoothSegNet: A Global-Local Framework for Liver Tumor Segmentation with Clinical KnowledgeInformed Label Smoothing","abstract":"Liver cancer is a leading cause of mortality worldwide, and accurate Computed Tomography (CT)-based tumor segmentation is essential for diagnosis and treatment. Manual delineation is time-intensive, prone to variability, and highlights the need for reliable automation. While deep learning has shown promise for automated liver segmentation, precise liver tumor segmentation remains challenging due to the heterogeneous nature of tumors, imprecise tumor margins, and limited labeled data. We present SmoothSegNet, a novel deep learning framework that addresses these challenges with the three key designs: (1) A novel knowledge-informed label smoothing technique that distills knowledge from clinical data to generate smooth labels, which are used to regularize model training, reducing the overfitting risk and enhancing model performance; (2) A global and local segmentation framework that breaks down the main task into two simpler sub-tasks, allowing optimized preprocessing and training for each; and (3) Pre- and post-processing pipelines customized to the challenges of each subtask aimed to enhance tumor visibility and refines tumor boundaries. We apply the proposed model on a challenging HCC-TACE-Seg dataset and show that SmoothSegNet outperformed various benchmarks in segmentation performance, particularly at smaller tumors (<10cm). Our ablation studies show that the three design components complementarily contribute to the model improved performance. Code for the proposed method are available at https://github.com/lingchm/medassist-liver-cancer.","authors":["Hairong Wang","Lingchao Mao","Zihan Zhang","Jing Li"],"url":"https://arxiv.org/abs/2410.10005"}
{"created":"2025-05-01","title":"Achievable Second-Order Asymptotics for MAC and RAC with Additive Non-Gaussian Noise","abstract":"We first study the two-user additive noise multiple access channel (MAC) where the noise distribution is arbitrary. For such a MAC, we use spherical codebooks and either joint nearest neighbor (JNN) or successive interference cancellation (SIC) decoding. Under both decoding methods, we derive second-order achievable rate regions and compare the finite blocklength performance between JNN and SIC decoding. Our results indicate that although the first-order rate regions of JNN and SIC decoding are identical, JNN decoding has better second-order asymptotic performance. When specialized to the Gaussian noise, we provide an alternative achievability proof to the result by MolavianJazi and Laneman (T-IT, 2015). Furthermore, we generalize our results to the random access channel (RAC) where neither the transmitters nor the receiver knows the user activity pattern. We use spherical-type codebooks and a rateless transmission scheme combining JNN/SIC decoding, and derive second-order achievability bounds. Comparing second-order achievability results of JNN and SIC decoding in a RAC, we show that JNN decoding achieves strictly larger first-order asymptotic rate. When specialized to Gaussian noise, our second-order asymptotic results recover the corresponding results of Yavas, Kostina, and Effros (T-IT, 2021) up to second-order.","authors":["Yiming Wang","Lin Bai","Zhuangfei Wu","Lin Zhou"],"url":"https://arxiv.org/abs/2410.10312"}
{"created":"2025-05-01","title":"A Unit Proofing Framework for Code-level Verification: A Research Agenda","abstract":"Formal verification provides mathematical guarantees that a software is correct. Design-level verification tools ensure software specifications are correct, but they do not expose defects in actual implementations. For this purpose, engineers use code-level tools. However, such tools struggle to scale to large software. The process of \"Unit Proofing\" mitigates this by decomposing the software and verifying each unit independently. We examined AWS's use of unit proofing and observed that current approaches are manual and prone to faults that mask severe defects. We propose a research agenda for a unit proofing framework, both methods and tools, to support software engineers in applying unit proofing effectively and efficiently. This will enable engineers to discover code-level defects early.","authors":["Paschal C. Amusuo","Parth V. Patil","Owen Cochell","Taylor Le Lievre","James C. Davis"],"url":"https://arxiv.org/abs/2410.14818"}
{"created":"2025-05-01","title":"A 3D Framework for Improving Low-Latency Multi-Channel Live Streaming","abstract":"The advent of 5G has driven the demand for high-quality, low-latency live streaming. However, challenges such as managing the increased data volume, ensuring synchronization across multiple streams, and maintaining consistent quality under varying network conditions persist, particularly in real-time video streaming. To address these issues, we propose a novel framework that leverages 3D virtual environments within game engines (e.g., Unity 3D) to optimize multi-channel live streaming. Our approach consolidates multi-camera video data into a single stream using multiple virtual 3D canvases, significantly increasing channel amounts while reducing latency and enhancing user flexibility. For demonstration of our approach, we utilize the Unity 3D engine to integrate multiple video inputs into a single-channel stream, supporting one-to-many broadcasting, one-to-one video calling, and real-time control of video channels. By mapping video data onto a world-space canvas and capturing it via an in-world camera, we minimize redundant data transmission, achieving efficient, low-latency streaming. Our results demonstrate that this method outperforms some existing multi-channel live streaming solutions in both latency reduction and user interaction responsiveness improvement. Our live video streaming system affiliated with this paper is also open-source at https://github.com/Aizierjiang/LiveStreaming.","authors":["Aizierjiang Aiersilan","Zhiqiang Wang"],"url":"https://arxiv.org/abs/2410.16284"}
{"created":"2025-05-01","title":"Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent","abstract":"Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agent's performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions.","authors":["Janghoon Ock","Tirtha Vinchurkar","Yayati Jadhav","Amir Barati Farimani"],"url":"https://arxiv.org/abs/2410.16658"}
{"created":"2025-05-01","title":"WARP-LCA: Efficient Convolutional Sparse Coding with Locally Competitive Algorithm","abstract":"The locally competitive algorithm (LCA) can solve sparse coding problems across a wide range of use cases. Recently, convolution-based LCA approaches have been shown to be highly effective for enhancing robustness for image recognition tasks in vision pipelines. To additionally maximize representational sparsity, LCA with hard-thresholding can be applied. While this combination often yields very good solutions satisfying an $\\ell_0$ sparsity criterion, it comes with significant drawbacks for practical application: (i) LCA is very inefficient, typically requiring hundreds of optimization cycles for convergence; (ii) the use of hard-thresholding results in a non-convex loss function, which might lead to suboptimal minima. To address these issues, we propose the Locally Competitive Algorithm with State Warm-up via Predictive Priming (WARP-LCA), which leverages a predictor network to provide a suitable initial guess of the LCA state based on the current input. Our approach significantly improves both convergence speed and the quality of solutions, while maintaining and even enhancing the overall strengths of LCA. We demonstrate that WARP-LCA converges faster by orders of magnitude and reaches better minima compared to conventional LCA. Moreover, the learned representations are more sparse and exhibit superior properties in terms of reconstruction and denoising quality as well as robustness when applied in deep recognition pipelines. Furthermore, we apply WARP-LCA to image denoising tasks, showcasing its robustness and practical effectiveness. Our findings confirm that the naive use of LCA with hard-thresholding results in suboptimal minima, whereas initializing LCA with a predictive guess results in better outcomes. This research advances the field of biologically inspired deep learning by providing a novel approach to convolutional sparse coding.","authors":["Geoffrey Kasenbacher","Felix Ehret","Gerrit Ecke","Sebastian Otte"],"url":"https://arxiv.org/abs/2410.18794"}
{"created":"2025-05-01","title":"Towards a fully well-balanced and entropy-stable scheme for the Euler equations with gravity: General equations of state","abstract":"The present work concerns the derivation of a fully well-balanced Godunov-type finite volume scheme for the Euler equations with a gravitational potential based on an approximate Riemann solver in a one-dimensional framework. It is an extension to general equations of states of the entropy-stable and fully well-balanced scheme for ideal gases recently forwarded in [Berthon et al., 2025]. The scheme is provably entropy-stable and positivity-preserving for all thermodynamic variables. Numerical test cases illustrate the performance and entropy stability of the new scheme, using six different equations of state as examples, four analytic and two tabulated ones.","authors":["Victor Michel-Dansac","Andrea Thomann"],"url":"https://arxiv.org/abs/2410.19710"}
{"created":"2025-05-01","title":"Optimized Homomorphic Permutation From New Permutation Decomposition Techniques","abstract":"Homomorphic permutation is fundamental to privacy-preserving computations based on batch-encoding homomorphic encryption. It underpins nearly all homomorphic matrix operations and predominantly influences their complexity. Permutation decomposition as a potential approach to optimize this critical component remains underexplored. In this paper, we propose novel decomposition techniques to optimize homomorphic permutations, advancing homomorphic encryption-based privacy-preserving computations.","authors":["Xirong Ma","Junling Fang","Chunpeng Ge","Dung Hoang Duong","Yali Jiang","Yanbin Li"],"url":"https://arxiv.org/abs/2410.21840"}
{"created":"2025-05-01","title":"Two-Phase Switched Reluctance Motors: Optimal Magnet Placement and Drive System for Torque Density","abstract":"This paper focuses on designing new motors with high torque density, which is crucial for applications ranging from electric vehicles to robotics. We propose a double-teeth C-core switched reluctance motor with hybrid excitation, integrating permanent magnets and a novel drive technique to enhance motor torque density. We explore three magnet placement configurations to maximize torque. A common challenge with most self-starting methods used in two-phase SRMs is the generation of negative torque, which reduces the motor's torque density. Our adopted self-starting method minimizes negative torque, and we introduce a new drive strategy to control the switching on and off, effectively eliminating negative torque. Additionally, magnetic equivalent circuits are developed for the analytical design and theoretical analysis of all configurations. The SRMs under study are prototyped and tested, and their performances are evaluated in terms of torque-angle characteristics, current, and voltage. Both experimental and simulation results validate the effectiveness of the PM-assisted SRMs in enhancing torque density and efficiency.","authors":["Gholamreza Davarpanah","Sajjad Mohammadi","James L. Kirtley"],"url":"https://arxiv.org/abs/2410.24121"}
{"created":"2025-05-01","title":"A New Switched Reluctance Motor with Embedded Permanent Magnets","abstract":"A new three-phase hybrid-excited multi-tooth switched reluctance motor with embedded permanent magnets is proposed, capable of achieving higher torque density for transportation electrification applications. Operating principles and design considerations are discussed. A magnetic equivalent circuit is developed. Finite element method is employed in the field analysis. The advantages of the proposed topology over existing designs for switched reluctance motors and flux switching motors are presented. Finally, the optimized design is prototyped to experimentally confirm the design and simulation results.","authors":["Gholamreza Davarpanah","Sajjad Mohammadi"],"url":"https://arxiv.org/abs/2411.00224"}
{"created":"2025-05-01","title":"Toward Automated Algorithm Design: A Survey and Practical Guide to Meta-Black-Box-Optimization","abstract":"In this survey, we introduce Meta-Black-Box-Optimization~(MetaBBO) as an emerging avenue within the Evolutionary Computation~(EC) community, which incorporates Meta-learning approaches to assist automated algorithm design. Despite the success of MetaBBO, the current literature provides insufficient summaries of its key aspects and lacks practical guidance for implementation. To bridge this gap, we offer a comprehensive review of recent advances in MetaBBO, providing an in-depth examination of its key developments. We begin with a unified definition of the MetaBBO paradigm, followed by a systematic taxonomy of various algorithm design tasks, including algorithm selection, algorithm configuration, solution manipulation, and algorithm generation. Further, we conceptually summarize different learning methodologies behind current MetaBBO works, including reinforcement learning, supervised learning, neuroevolution, and in-context learning with Large Language Models. A comprehensive evaluation of the latest representative MetaBBO methods is then carried out, alongside an experimental analysis of their optimization performance, computational efficiency, and generalization ability. Based on the evaluation results, we meticulously identify a set of core designs that enhance the generalization and learning effectiveness of MetaBBO. Finally, we outline the vision for the field by providing insight into the latest trends and potential future directions. Relevant literature will be continuously collected and updated at https://github.com/MetaEvo/Awesome-MetaBBO.","authors":["Zeyuan Ma","Hongshu Guo","Yue-Jiao Gong","Jun Zhang","Kay Chen Tan"],"url":"https://arxiv.org/abs/2411.00625"}
{"created":"2025-05-01","title":"PreCM: The Padding-based Rotation Equivariant Convolution Mode for Semantic Segmentation","abstract":"Semantic segmentation is an important branch of image processing and computer vision. With the popularity of deep learning, various convolutional neural networks have been proposed for pixel-level classification and segmentation tasks. In practical scenarios, however, imaging angles are often arbitrary, encompassing instances such as water body images from remote sensing and capillary and polyp images in the medical domain, where prior orientation information is typically unavailable to guide these networks to extract more effective features. In this case, learning features from objects with diverse orientation information poses a significant challenge, as the majority of CNN-based semantic segmentation networks lack rotation equivariance to resist the disturbance from orientation information. To address this challenge, this paper first constructs a universal convolution-group framework aimed at more fully utilizing orientation information and equipping the network with rotation equivariance. Subsequently, we mathematically design a padding-based rotation equivariant convolution mode (PreCM), which is not only applicable to multi-scale images and convolutional kernels but can also serve as a replacement component for various types of convolutions, such as dilated convolutions, transposed convolutions, and asymmetric convolution. To quantitatively assess the impact of image rotation in semantic segmentation tasks, we also propose a new evaluation metric, Rotation Difference (RD). The replacement experiments related to six existing semantic segmentation networks on three datasets show that, the average Intersection Over Union (IOU) of their PreCM-based versions respectively improve 6.91%, 10.63%, 4.53%, 5.93%, 7.48%, 8.33% compared to their original versions in terms of random angle rotation. And the average RD values are decreased by 3.58%, 4.56%, 3.47%, 3.66%, 3.47%, 3.43% respectively.","authors":["Xinyu Xu","Huazhen Liu","Tao Zhang","Huilin Xiong","Wenxian Yu"],"url":"https://arxiv.org/abs/2411.01624"}
{"created":"2025-05-01","title":"When to Localize? A Risk-Constrained Reinforcement Learning Approach","abstract":"In a standard navigation pipeline, a robot localizes at every time step to lower navigational errors. However, in some scenarios, a robot needs to selectively localize when it is expensive to obtain observations. For example, an underwater robot surfacing to localize too often hinders it from searching for critical items underwater, such as black boxes from crashed aircraft. On the other hand, if the robot never localizes, poor state estimates cause failure to find the items due to inadvertently leaving the search area or entering hazardous, restricted areas. Motivated by these scenarios, we investigate approaches to help a robot determine \"when to localize?\" We formulate this as a bi-criteria optimization problem: minimize the number of localization actions while ensuring the probability of failure (due to collision or not reaching a desired goal) remains bounded. In recent work, we showed how to formulate this active localization problem as a constrained Partially Observable Markov Decision Process (POMDP), which was solved using an online POMDP solver. However, this approach is too slow and requires full knowledge of the robot transition and observation models. In this paper, we present RiskRL, a constrained Reinforcement Learning (RL) framework that overcomes these limitations. RiskRL uses particle filtering and recurrent Soft Actor-Critic network to learn a policy that minimizes the number of localizations while ensuring the probability of failure constraint is met. Our numerical experiments show that RiskRL learns a robust policy that leads to at least a 26% increase in success rates when traversing unseen test environments.","authors":["Chak Lam Shek","Kasra Torshizi","Troi Williams","Pratap Tokekar"],"url":"https://arxiv.org/abs/2411.02788"}
{"created":"2025-05-01","title":"DEIO: Deep Event Inertial Odometry","abstract":"Event cameras show great potential for visual odometry (VO) in handling challenging situations, such as fast motion and high dynamic range. Despite this promise, the sparse and motion-dependent characteristics of event data continue to limit the performance of feature-based or direct-based data association methods in practical applications. To address these limitations, we propose Deep Event Inertial Odometry (DEIO), the first monocular learning-based event-inertial framework, which combines a learning-based method with traditional nonlinear graph-based optimization. Specifically, an event-based recurrent network is adopted to provide accurate and sparse associations of event patches over time. DEIO further integrates it with the IMU to recover up-to-scale pose and provide robust state estimation. The Hessian information derived from the learned differentiable bundle adjustment (DBA) is utilized to optimize the co-visibility factor graph, which tightly incorporates event patch correspondences and IMU pre-integration within a keyframe-based sliding window. Comprehensive validations demonstrate that DEIO achieves superior performance on \\textit{10} challenging public benchmarks compared with more than 20 state-of-the-art methods.","authors":["Weipeng Guan","Fuling Lin","Peiyu Chen","Peng Lu"],"url":"https://arxiv.org/abs/2411.03928"}
{"created":"2025-05-01","title":"MicroScopiQ: Accelerating Foundational Models through Outlier-Aware Microscaling Quantization","abstract":"Quantization of foundational models (FMs) is significantly more challenging than traditional DNNs due to the emergence of large magnitude values called outliers. Existing outlier-aware algorithm-architecture co-design techniques either use mixed-precision, retaining outliers at high precision but compromise hardware efficiency, or quantize inliers and outliers at the same precision, improving hardware efficiency at the cost of accuracy. To address this mutual exclusivity, we propose MicroScopiQ, a novel co-design technique that leverages pruning to complement outlier-aware quantization. MicroScopiQ retains outliers at higher precision while pruning a certain fraction of least important weights to distribute the additional outlier bits; ensuring high accuracy, aligned memory and hardware efficiency. We design a high-throughput, low overhead accelerator architecture composed of multi-precision INT processing elements and a network-on-chip called ReCoN that efficiently abstracts the complexity of supporting high-precision outliers. Additionally, unlike prior techniques, MicroScopiQ does not assume any locality of outlier weights, enabling applicability to a broad range of FMs. Extensive experiments across diverse quantization settings demonstrate that MicroScopiQ achieves state-of-the-art quantization accuracy, while delivering up to 3x faster inference and 2x lower energy consumption compared to existing alternatives. Code is available at: https://github.com/georgia-tech-synergy-lab/MicroScopiQ-LLM-Quantization","authors":["Akshat Ramachandran","Souvik Kundu","Tushar Krishna"],"url":"https://arxiv.org/abs/2411.05282"}
{"created":"2025-05-01","title":"A New 8/14 Two-Phase Switched Reluctance Motor with Improved Performance","abstract":"Despite their simple and robust structure, low cost, and simple cooling system, switched reluctance motors (SRMs) face the challenge of low mean torque. A possible solution is to change the structure of SRMs. This article introduces an innovative combination of the number of rotor teeth and stator teeth of a two-phase switch reluctance motor (TPSRM) with eight teeth for the stator and fourteen teeth for the rotor. As a result of its unique design, which has a short path for passing the main flux, it requires less magnetomotive force. This leads to less core and copper loss, resulting in increased efficiency. Each tooth of the stator in a phase develops a positive torque during the rotation of the rotor, which increases the torque and consequently increases the mean torque of the proposed TPSRM. A current hysteresis control (CHC) is simulated by 2D FEM for the proposed 8/14 TPSRM and the conventional 8/12 TPSRM under the same mechanical load on the shaft to get a current hysteresis reference of 15A at the nominal speed of 600 rpm. To verify the novelty and advantages of the suggested TPSRM, it is compared with the conventional 8/12 TPSRM in terms of mean and peak torque, torque density, and core and copper losses were compared. Lastly, the proposed 8/14 TPSRM is shown to have better performance than the conventional 8/12 TPSRM.","authors":["Gholamreza Davarpanah","Hossein Shirzad","Jawad Faiz"],"url":"https://arxiv.org/abs/2411.06161"}
{"created":"2025-05-01","title":"High-Frequency Enhanced Hybrid Neural Representation for Video Compression","abstract":"Neural Representations for Videos (NeRV) have simplified the video codec process and achieved swift decoding speeds by encoding video content into a neural network, presenting a promising solution for video compression. However, existing work overlooks the crucial issue that videos reconstructed by these methods lack high-frequency details. To address this problem, this paper introduces a High-Frequency Enhanced Hybrid Neural Representation Network. Our method focuses on leveraging high-frequency information to improve the synthesis of fine details by the network. Specifically, we design a wavelet high-frequency encoder that incorporates Wavelet Frequency Decomposer (WFD) blocks to generate high-frequency feature embeddings. Next, we design the High-Frequency Feature Modulation (HFM) block, which leverages the extracted high-frequency embeddings to enhance the fitting process of the decoder. Finally, with the refined Harmonic decoder block and a Dynamic Weighted Frequency Loss, we further reduce the potential loss of high-frequency information. Experiments on the Bunny and UVG datasets demonstrate that our method outperforms other methods, showing notable improvements in detail preservation and compression performance.","authors":["Li Yu","Zhihui Li","Jimin Xiao","Moncef Gabbouj"],"url":"https://arxiv.org/abs/2411.06685"}
{"created":"2025-05-01","title":"Retrieval, Reasoning, Re-ranking: A Context-Enriched Framework for Knowledge Graph Completion","abstract":"The Knowledge Graph Completion~(KGC) task aims to infer the missing entity from an incomplete triple. Existing embedding-based methods rely solely on triples in the KG, which is vulnerable to specious relation patterns and long-tail entities. On the other hand, text-based methods struggle with the semantic gap between KG triples and natural language. Apart from triples, entity contexts (e.g., labels, descriptions, aliases) also play a significant role in augmenting KGs. To address these limitations, we propose KGR3, a context-enriched framework for KGC. KGR3 is composed of three modules. Firstly, the Retrieval module gathers supporting triples from the KG, collects plausible candidate answers from a base embedding model, and retrieves context for each related entity. Then, the Reasoning module employs a large language model to generate potential answers for each query triple. Finally, the Re-ranking module combines candidate answers from the two modules mentioned above, and fine-tunes an LLM to provide the best answer. Extensive experiments on widely used datasets demonstrate that KGR3 consistently improves various KGC methods. Specifically, the best variant of KGR3 achieves absolute Hits@1 improvements of 12.3% and 5.6% on the FB15k237 and WN18RR datasets.","authors":["Muzhi Li","Cehao Yang","Chengjin Xu","Xuhui Jiang","Yiyan Qi","Jian Guo","Ho-fung Leung","Irwin King"],"url":"https://arxiv.org/abs/2411.08165"}
{"created":"2025-05-01","title":"ColorEdit: Training-free Image-Guided Color editing with diffusion model","abstract":"Text-to-image (T2I) diffusion models, with their impressive generative capabilities, have been adopted for image editing tasks, demonstrating remarkable efficacy. However, due to attention leakage and collision between the cross-attention map of the object and the new color attribute from the text prompt, text-guided image editing methods may fail to change the color of an object, resulting in a misalignment between the resulting image and the text prompt. In this paper, we conduct an in-depth analysis on the process of text-guided image synthesizing and what semantic information different cross-attention blocks have learned. We observe that the visual representation of an object is determined in the up-block of the diffusion model in the early stage of the denoising process, and color adjustment can be achieved through value matrices alignment in the cross-attention layer. Based on our findings, we propose a straightforward, yet stable, and effective image-guided method to modify the color of an object without requiring any additional fine-tuning or training. Lastly, we present a benchmark dataset called COLORBENCH, the first benchmark to evaluate the performance of color change methods. Extensive experiments validate the effectiveness of our method in object-level color editing and surpass the performance of popular text-guided image editing approaches in both synthesized and real images.","authors":["Xingxi Yin","Zhi Li","Jingfeng Zhang","Chenglin Li","Yin Zhang"],"url":"https://arxiv.org/abs/2411.10232"}
{"created":"2025-05-01","title":"Restructuring Tractable Probabilistic Circuits","abstract":"Probabilistic circuits (PCs) are a unifying representation for probabilistic models that support tractable inference. Numerous applications of PCs like controllable text generation depend on the ability to efficiently multiply two circuits. Existing multiplication algorithms require that the circuits respect the same structure, i.e. variable scopes decomposes according to the same vtree. In this work, we propose and study the task of restructuring structured(-decomposable) PCs, that is, transforming a structured PC such that it conforms to a target vtree. We propose a generic approach for this problem and show that it leads to novel polynomial-time algorithms for multiplying circuits respecting different vtrees, as well as a practical depth-reduction algorithm that preserves structured decomposibility. Our work opens up new avenues for tractable PC inference, suggesting the possibility of training with less restrictive PC structures while enabling efficient inference by changing their structures at inference time.","authors":["Honghua Zhang","Benjie Wang","Marcelo Arenas","Guy Van den Broeck"],"url":"https://arxiv.org/abs/2411.12256"}
{"created":"2025-05-01","title":"A Contrast-Agnostic Method for Ultra-High Resolution Claustrum Segmentation","abstract":"The claustrum is a band-like gray matter structure located between putamen and insula whose exact functions are still actively researched. Its sheet-like structure makes it barely visible in in vivo Magnetic Resonance Imaging (MRI) scans at typical resolutions and neuroimaging tools for its study, including methods for automatic segmentation, are currently very limited. In this paper, we propose a contrast- and resolution-agnostic method for claustrum segmentation at ultra-high resolution (0.35 mm isotropic); the method is based on the SynthSeg segmentation framework (Billot et al., 2023), which leverages the use of synthetic training intensity images to achieve excellent generalization. In particular, SynthSeg requires only label maps to be trained, since corresponding intensity images are synthesized on the fly with random contrast and resolution. We trained a deep learning network for automatic claustrum segmentation, using claustrum manual labels obtained from 18 ultra-high resolution MRI scans (mostly ex vivo). We demonstrated the method to work on these 18 high resolution cases (Dice score = 0.632, mean surface distance = 0.458 mm, and volumetric similarity = 0.867 using 6-fold Cross Validation (CV)), and also on in vivo T1-weighted MRI scans at typical resolutions (~1 mm isotropic). We also demonstrated that the method is robust in a test-retest setting and when applied to multimodal imaging (T2-weighted, Proton Density and quantitative T1 scans). To the best of our knowledge this is the first accurate method for automatic ultra-high resolution claustrum segmentation, which is robust against changes in contrast and resolution. The method is released at https://github.com/chiara-mauri/claustrum_segmentation and as part of the neuroimaging package Freesurfer (Fischl, 2012).","authors":["Chiara Mauri","Ryan Fritz","Jocelyn Mora","Benjamin Billot","Juan Eugenio Iglesias","Koen Van Leemput","Jean Augustinack","Douglas N Greve"],"url":"https://arxiv.org/abs/2411.15388"}
{"created":"2025-05-01","title":"Partial Knowledge Distillation for Alleviating the Inherent Inter-Class Discrepancy in Federated Learning","abstract":"Substantial efforts have been devoted to alleviating the impact of the long-tailed class distribution in federated learning. In this work, we observe an interesting phenomenon that certain weak classes consistently exist even for class-balanced learning. These weak classes, different from the minority classes in the previous works, are inherent to data and remain fairly consistent for various network structures, learning paradigms, and data partitioning methods. The inherent inter-class accuracy discrepancy can reach over 36.9% for federated learning on the FashionMNIST and CIFAR-10 datasets, even when the class distribution is balanced both globally and locally. In this study, we empirically analyze the potential reason for this phenomenon. Furthermore, a partial knowledge distillation (PKD) method is proposed to improve the model's classification accuracy for weak classes. In this approach, knowledge transfer is initiated upon the occurrence of specific misclassifications within certain weak classes. Experimental results show that the accuracy of weak classes can be improved by 10.7%, reducing the inherent inter-class discrepancy effectively.","authors":["Xiaoyu Gan","Jingbo Jiang","Jingyang Zhu","Xiaomeng Wang","Xizi Chen","Chi-Ying Tsui"],"url":"https://arxiv.org/abs/2411.15403"}
{"created":"2025-05-01","title":"All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages","abstract":"Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in LMM research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including true/false, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark is publicly available.","authors":["Ashmal Vayani","Dinura Dissanayake","Hasindri Watawana","Noor Ahsan","Nevasini Sasikumar","Omkar Thawakar","Henok Biadglign Ademtew","Yahya Hmaiti","Amandeep Kumar","Kartik Kuckreja","Mykola Maslych","Wafa Al Ghallabi","Mihail Mihaylov","Chao Qin","Abdelrahman M Shaker","Mike Zhang","Mahardika Krisna Ihsani","Amiel Esplana","Monil Gokani","Shachar Mirkin","Harsh Singh","Ashay Srivastava","Endre Hamerlik","Fathinah Asma Izzati","Fadillah Adamsyah Maani","Sebastian Cavada","Jenny Chim","Rohit Gupta","Sanjay Manjunath","Kamila Zhumakhanova","Feno Heriniaina Rabevohitra","Azril Amirudin","Muhammad Ridzuan","Daniya Kareem","Ketan More","Kunyang Li","Pramesh Shakya","Muhammad Saad","Amirpouya Ghasemaghaei","Amirbek Djanibekov","Dilshod Azizov","Branislava Jankovic","Naman Bhatia","Alvaro Cabrera","Johan Obando-Ceron","Olympiah Otieno","Fabian Farestam","Muztoba Rabbani","Sanoojan Baliah","Santosh Sanjeev","Abduragim Shtanchaev","Maheen Fatima","Thao Nguyen","Amrin Kareem","Toluwani Aremu","Nathan Xavier","Amit Bhatkal","Hawau Toyin","Aman Chadha","Hisham Cholakkal","Rao Muhammad Anwer","Michael Felsberg","Jorma Laaksonen","Thamar Solorio","Monojit Choudhury","Ivan Laptev","Mubarak Shah","Salman Khan","Fahad Khan"],"url":"https://arxiv.org/abs/2411.16508"}
{"created":"2025-05-01","title":"APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving","abstract":"Efficiently serving Large Language Models (LLMs) requires selecting an optimal parallel execution plan, balancing computation, memory, and communication overhead. However, determining the best strategy is challenging due to varying parallelism techniques (data, pipeline, tensor) and workload characteristics (e.g., compute-intensive tasks with long prompts vs. memory-intensive tasks with long generation). We propose APEX, an LLM serving system simulator that efficiently identifies optimal parallel execution plans by considering key factors of LLM serving systems, such as memory usage, batching behavior, etc. APEX performs dynamism-aware simulation to model iteration-level batching, and leverages LLMs' repetitive structure to reduce design space, scaling efficiently to trillion-scale models. APEX abstracts the key components of LLM serving systems, including the model, batching module, quantization formats, and device clusters, enabling the simulator to be general and extensible. Simulating on a CPU, APEX evaluates execution plans for various device clusters, covering diverse LLMs and workloads. APEX finds plans up to 3.37x faster than heuristics, and also plans that reduce energy consumption by up to 45% compared to latency-optimal plans. APEX performs comprehensive evaluations, reporting key system metrics like time per output token and time to first token, which can help service providers meet SLOs. APEX identifies an optimal plan within 15 minutes on a CPU, making it 71x faster and 1234x more cost-effective than cloud-based GPU deployment. APEX can be accessed at https://github.com/microsoft/apex_plus","authors":["Yi-Chien Lin","Woosuk Kwon","Ronald Pineda","Fanny Nina Paravecino"],"url":"https://arxiv.org/abs/2411.17651"}
{"created":"2025-05-01","title":"Type-R: Automatically Retouching Typos for Text-to-Image Generation","abstract":"While recent text-to-image models can generate photorealistic images from text prompts that reflect detailed instructions, they still face significant challenges in accurately rendering words in the image. In this paper, we propose to retouch erroneous text renderings in the post-processing pipeline. Our approach, called Type-R, identifies typographical errors in the generated image, erases the erroneous text, regenerates text boxes for missing words, and finally corrects typos in the rendered words. Through extensive experiments, we show that Type-R, in combination with the latest text-to-image models such as Stable Diffusion or Flux, achieves the highest text rendering accuracy while maintaining image quality and also outperforms text-focused generation baselines in terms of balancing text accuracy and image quality.","authors":["Wataru Shimoda","Naoto Inoue","Daichi Haraguchi","Hayato Mitani","Seiichi Uchida","Kota Yamaguchi"],"url":"https://arxiv.org/abs/2411.18159"}
{"created":"2025-05-01","title":"HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos","abstract":"We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts.","authors":["Prithviraj Banerjee","Sindi Shkodrani","Pierre Moulon","Shreyas Hampali","Shangchen Han","Fan Zhang","Linguang Zhang","Jade Fountain","Edward Miller","Selen Basol","Richard Newcombe","Robert Wang","Jakob Julian Engel","Tomas Hodan"],"url":"https://arxiv.org/abs/2411.19167"}
{"created":"2025-05-01","title":"Ditto: Motion-Space Diffusion for Controllable Realtime Talking Head Synthesis","abstract":"Recent advances in diffusion models have endowed talking head synthesis with subtle expressions and vivid head movements, but have also led to slow inference speed and insufficient control over generated results. To address these issues, we propose Ditto, a diffusion-based talking head framework that enables fine-grained controls and real-time inference. Specifically, we utilize an off-the-shelf motion extractor and devise a diffusion transformer to generate representations in a specific motion space. We optimize the model architecture and training strategy to address the issues in generating motion representations, including insufficient disentanglement between motion and identity, and large internal discrepancies within the representation. Besides, we employ diverse conditional signals while establishing a mapping between motion representation and facial semantics, enabling control over the generation process and correction of the results. Moreover, we jointly optimize the holistic framework to enable streaming processing, real-time inference, and low first-frame delay, offering functionalities crucial for interactive applications such as AI assistants. Extensive experimental results demonstrate that Ditto generates compelling talking head videos and exhibits superiority in both controllability and real-time performance.","authors":["Tianqi Li","Ruobing Zheng","Minghui Yang","Jingdong Chen","Ming Yang"],"url":"https://arxiv.org/abs/2411.19509"}
{"created":"2025-05-01","title":"ObjectFinder: An Open-Vocabulary Assistive System for Interactive Object Search by Blind People","abstract":"Searching for objects in unfamiliar scenarios is a challenging task for blind people. It involves specifying the target object, detecting it, and then gathering detailed information according to the user's intent. However, existing description- and detection-based assistive technologies do not sufficiently support the multifaceted nature of interactive object search tasks. We present ObjectFinder, an open-vocabulary wearable assistive system for interactive object search by blind people. ObjectFinder allows users to query target objects using flexible wording. Once the target object is detected, it provides egocentric localization information in real-time, including distance and direction. Users can then initiate different branches to gather detailed information based on their intent towards the target object, such as navigating to it or perceiving its surroundings. ObjectFinder is powered by a seamless combination of open-vocabulary models, namely an open-vocabulary object detector and a multimodal large language model. The ObjectFinder design concept and its development were carried out in collaboration with a blind co-designer. To evaluate ObjectFinder, we conducted an exploratory user study with eight blind participants. We compared ObjectFinder to BeMyAI and Google Lookout, popular description- and detection-based assistive applications. Our findings indicate that most participants felt more independent with ObjectFinder and preferred it for object search, as it enhanced scene context gathering and navigation, and allowed for active target identification. Finally, we discuss the implications for future assistive systems to support interactive object search.","authors":["Ruiping Liu","Jiaming Zhang","Angela Sch\\\"on","Karin M\\\"uller","Junwei Zheng","Kailun Yang","Anhong Guo","Kathrin Gerling","Rainer Stiefelhagen"],"url":"https://arxiv.org/abs/2412.03118"}
{"created":"2025-05-01","title":"Testing CPS with Design Assumptions-Based Metamorphic Relations and Genetic Programming","abstract":"Cyber-Physical Systems (CPSs) software is used to enforce desired behaviours on physical systems. To test the interaction between the CPS software and the system's physics, engineers provide traces of desired physical states and observe traces of the actual physical states. CPS requirements describe how closely the actual physical traces should track the desired traces. These requirements are typically defined for specific, simple input traces such as step or ramp sequences, and thus are not applicable to arbitrary inputs. This limits the availability of oracles for CPSs. Our recent work proposes an approach to testing CPS using control-theoretical design assumptions instead of requirements. This approach circumvents the oracle problem by leveraging the control-theoretical guarantees that are provided when the design assumptions are satisfied. To address the test case generation and oracle problems, researchers have proposed metamorphic testing, which is based on the study of relations across tests, i.e., metamorphic relations (MRs). In this work, we define MRs based on the design assumptions and explore combinations of these MRs using genetic programming to generate CPS test cases. This enables the generation of CPS input traces with potentially arbitrary shapes, together with associated expected output traces. We use the deviation from the expected output traces to guide the generation of input traces that falsify the MRs. Our experiment results show that the MR-falsification provides engineers with new information, helping them identify passed and failed test cases. Furthermore, we show that the generation of traces that falsify the MRs is a non-trivial problem, which is successfully addressed by our genetic search.","authors":["Claudio Mandrioli","Seung Yeob Shin","Domenico Bianculli","Lionel Briand"],"url":"https://arxiv.org/abs/2412.03330"}
{"created":"2025-05-01","title":"PANGAEA: A Global and Inclusive Benchmark for Geospatial Foundation Models","abstract":"Geospatial Foundation Models (GFMs) have emerged as powerful tools for extracting representations from Earth observation data, but their evaluation remains inconsistent and narrow. Existing works often evaluate on suboptimal downstream datasets and tasks, that are often too easy or too narrow, limiting the usefulness of the evaluations to assess the real-world applicability of GFMs. Additionally, there is a distinct lack of diversity in current evaluation protocols, which fail to account for the multiplicity of image resolutions, sensor types, and temporalities, which further complicates the assessment of GFM performance. In particular, most existing benchmarks are geographically biased towards North America and Europe, questioning the global applicability of GFMs. To overcome these challenges, we introduce PANGAEA, a standardized evaluation protocol that covers a diverse set of datasets, tasks, resolutions, sensor modalities, and temporalities. It establishes a robust and widely applicable benchmark for GFMs. We evaluate the most popular GFMs openly available on this benchmark and analyze their performance across several domains. In particular, we compare these models to supervised baselines (e.g. UNet and vanilla ViT), and assess their effectiveness when faced with limited labeled data. Our findings highlight the limitations of GFMs, under different scenarios, showing that they do not consistently outperform supervised models. PANGAEA is designed to be highly extensible, allowing for the seamless inclusion of new datasets, models, and tasks in future research. By releasing the evaluation code and benchmark, we aim to enable other researchers to replicate our experiments and build upon our work, fostering a more principled evaluation protocol for large pre-trained geospatial models. The code is available at https://github.com/VMarsocci/pangaea-bench.","authors":["Valerio Marsocci","Yuru Jia","Georges Le Bellier","David Kerekes","Liang Zeng","Sebastian Hafner","Sebastian Gerard","Eric Brune","Ritu Yadav","Ali Shibli","Heng Fang","Yifang Ban","Maarten Vergauwen","Nicolas Audebert","Andrea Nascetti"],"url":"https://arxiv.org/abs/2412.04204"}
{"created":"2025-05-01","title":"Not Just Text: Uncovering Vision Modality Typographic Threats in Image Generation Models","abstract":"Current image generation models can effortlessly produce high-quality, highly realistic images, but this also increases the risk of misuse. In various Text-to-Image or Image-to-Image tasks, attackers can generate a series of images containing inappropriate content by simply editing the language modality input. To mitigate this security concern, numerous guarding or defensive strategies have been proposed, with a particular emphasis on safeguarding language modality. However, in practical applications, threats in the vision modality, particularly in tasks involving the editing of real-world images, present heightened security risks as they can easily infringe upon the rights of the image owner. Therefore, this paper employs a method named typographic attack to reveal that various image generation models are also susceptible to threats within the vision modality. Furthermore, we also evaluate the defense performance of various existing methods when facing threats in the vision modality and uncover their ineffectiveness. Finally, we propose the Vision Modal Threats in Image Generation Models (VMT-IGMs) dataset, which would serve as a baseline for evaluating the vision modality vulnerability of various image generation models.","authors":["Hao Cheng","Erjia Xiao","Jiayan Yang","Jiahang Cao","Qiang Zhang","Jize Zhang","Kaidi Xu","Jindong Gu","Renjing Xu"],"url":"https://arxiv.org/abs/2412.05538"}
{"created":"2025-05-01","title":"FILA: Fine-Grained Vision Language Models","abstract":"Recently, there has been growing interest in the capability of multimodal large language models (MLLMs) to process high-resolution images. A common approach currently involves dynamically cropping the original high-resolution image into smaller sub-images, which are then fed into a vision encoder that was pre-trained on lower-resolution images. However, this cropping approach often truncates objects and connected areas in the original image, causing semantic breaks. To address this limitation, we introduce HyViLM, designed to process images of any resolution while retaining the overall context during encoding. Specifically, we: (i) Design a new visual encoder called Hybrid Encoder that not only encodes individual sub-images but also interacts with detailed global visual features, significantly improving the model's ability to encode high-resolution images. (ii) Propose an optimal feature fusion strategy for the dynamic cropping approach, effectively leveraging information from different layers of the vision encoder. Compared with the state-of-the-art MLLMs under the same setting, our HyViLM outperforms existing MLLMs in nine out of ten tasks. Specifically, HyViLM achieves a 9.6% improvement in performance on the TextVQA task and a 6.9% enhancement on the DocVQA task.","authors":["Shiding Zhu","Wenhui Dong","Jun Song","Yingbo Wang","Yanan Guo","Bo Zheng"],"url":"https://arxiv.org/abs/2412.08378"}
{"created":"2025-05-01","title":"Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos","abstract":"Learning to understand dynamic 3D scenes from imagery is crucial for applications ranging from robotics to scene reconstruction. Yet, unlike other problems where large-scale supervised training has enabled rapid progress, directly supervising methods for recovering 3D motion remains challenging due to the fundamental difficulty of obtaining ground truth annotations. We present a system for mining high-quality 4D reconstructions from internet stereoscopic, wide-angle videos. Our system fuses and filters the outputs of camera pose estimation, stereo depth estimation, and temporal tracking methods into high-quality dynamic 3D reconstructions. We use this method to generate large-scale data in the form of world-consistent, pseudo-metric 3D point clouds with long-term motion trajectories. We demonstrate the utility of this data by training a variant of DUSt3R to predict structure and 3D motion from real-world image pairs, showing that training on our reconstructed data enables generalization to diverse real-world scenes. Project page and data at: https://stereo4d.github.io","authors":["Linyi Jin","Richard Tucker","Zhengqi Li","David Fouhey","Noah Snavely","Aleksander Holynski"],"url":"https://arxiv.org/abs/2412.09621"}
{"created":"2025-05-01","title":"You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary Projects","abstract":"The ability to execute the test suite of a project is essential in many scenarios, e.g., to assess code quality and code coverage, to validate code changes made by developers or automated tools, and to ensure compatibility with dependencies. Despite its importance, executing the test suite of a project can be challenging in practice because different projects use different programming languages, software ecosystems, build systems, testing frameworks, and other tools. These challenges make it difficult to create a reliable, universal test execution method that works across different projects. This paper presents ExecutionAgent, an automated technique that prepares scripts for building an arbitrary project from source code and running its test cases. Inspired by the way a human developer would address this task, our approach is a large language model (LLM)-based agent that autonomously executes commands and interacts with the host system. The agent uses meta-prompting to gather guidelines on the latest technologies related to the given project, and it iteratively refines its process based on feedback from the previous steps. Our evaluation applies ExecutionAgent to 50 open-source projects that use 14 different programming languages and many different build and testing tools. The approach successfully executes the test suites of 33/50 projects, while matching the test results of ground truth test suite executions with a deviation of only 7.5%. These results improve over the best previously available technique by 6.6x. The costs imposed by the approach are reasonable, with an execution time of 74 minutes and LLM costs of USD 0.16, on average per project. We envision ExecutionAgent to serve as a valuable tool for developers, automated programming tools, and researchers that need to execute tests across a wide variety of projects.","authors":["Islem Bouzenia","Michael Pradel"],"url":"https://arxiv.org/abs/2412.10133"}
{"created":"2025-05-01","title":"A Library for Learning Neural Operators","abstract":"We present NeuralOperator, an open-source Python library for operator learning. Neural operators generalize neural networks to maps between function spaces instead of finite-dimensional Euclidean spaces. They can be trained and inferenced on input and output functions given at various discretizations, satisfying a discretization convergence properties. Built on top of PyTorch, NeuralOperator provides all the tools for training and deploying neural operator models, as well as developing new ones, in a high-quality, tested, open-source package. It combines cutting-edge models and customizability with a gentle learning curve and simple user interface for newcomers.","authors":["Jean Kossaifi","Nikola Kovachki","Zongyi Li","David Pitt","Miguel Liu-Schiaffini","Robert Joseph George","Boris Bonev","Kamyar Azizzadenesheli","Julius Berner","Valentin Duruisseaux","Anima Anandkumar"],"url":"https://arxiv.org/abs/2412.10354"}
{"created":"2025-05-01","title":"Mastering Board Games by External and Internal Planning with Language Models","abstract":"Advancing planning and reasoning capabilities of Large Language Models (LLMs) is one of the key prerequisites towards unlocking their potential for performing reliably in complex and impactful domains. In this paper, we aim to demonstrate this across board games (Chess, Fischer Random / Chess960, Connect Four, and Hex), and we show that search-based planning can yield significant improvements in LLM game-playing strength. We introduce, compare and contrast two major approaches: In external search, the model guides Monte Carlo Tree Search (MCTS) rollouts and evaluations without calls to an external game engine, and in internal search, the model is trained to generate in-context a linearized tree of search and a resulting final choice. Both build on a language model pre-trained on relevant domain knowledge, reliably capturing the transition and value functions in the respective environments, with minimal hallucinations. We evaluate our LLM search implementations against game-specific state-of-the-art engines, showcasing substantial improvements in strength over the base model, and reaching Grandmaster-level performance in chess while operating closer to the human search budget. Our proposed approach, combining search with domain knowledge, is not specific to board games, hinting at more general future applications.","authors":["John Schultz","Jakub Adamek","Matej Jusup","Marc Lanctot","Michael Kaisers","Sarah Perrin","Daniel Hennes","Jeremy Shar","Cannada Lewis","Anian Ruoss","Tom Zahavy","Petar Veli\\v{c}kovi\\'c","Laurel Prince","Satinder Singh","Eric Malmi","Nenad Toma\\v{s}ev"],"url":"https://arxiv.org/abs/2412.12119"}
{"created":"2025-05-01","title":"Optical aberrations in autonomous driving: Physics-informed parameterized temperature scaling for neural network uncertainty calibration","abstract":"'A trustworthy representation of uncertainty is desirable and should be considered as a key feature of any machine learning method' (Huellermeier and Waegeman, 2021). This conclusion of Huellermeier et al. underpins the importance of calibrated uncertainties. Since AI-based algorithms are heavily impacted by dataset shifts, the automotive industry needs to safeguard its system against all possible contingencies. One important but often neglected dataset shift is caused by optical aberrations induced by the windshield. For the verification of the perception system performance, requirements on the AI performance need to be translated into optical metrics by a bijective mapping. Given this bijective mapping it is evident that the optical system characteristics add additional information about the magnitude of the dataset shift. As a consequence, we propose to incorporate a physical inductive bias into the neural network calibration architecture to enhance the robustness and the trustworthiness of the AI target application, which we demonstrate by using a semantic segmentation task as an example. By utilizing the Zernike coefficient vector of the optical system as a physical prior we can significantly reduce the mean expected calibration error in case of optical aberrations. As a result, we pave the way for a trustworthy uncertainty representation and for a holistic verification strategy of the perception chain.","authors":["Dominik Werner Wolf","Alexander Braun","Markus Ulrich"],"url":"https://arxiv.org/abs/2412.13695"}
{"created":"2025-05-01","title":"MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data","abstract":"In healthcare, the integration of multimodal data is pivotal for developing comprehensive diagnostic and predictive models. However, managing missing data remains a significant challenge in real-world applications. We introduce MARIA (Multimodal Attention Resilient to Incomplete datA), a novel transformer-based deep learning model designed to address these challenges through an intermediate fusion strategy. Unlike conventional approaches that depend on imputation, MARIA utilizes a masked self-attention mechanism, which processes only the available data without generating synthetic values. This approach enables it to effectively handle incomplete datasets, enhancing robustness and minimizing biases introduced by imputation methods. We evaluated MARIA against 10 state-of-the-art machine learning and deep learning models across 8 diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms existing methods in terms of performance and resilience to varying levels of data incompleteness, underscoring its potential for critical healthcare applications.","authors":["Camillo Maria Caruso","Paolo Soda","Valerio Guarrasi"],"url":"https://arxiv.org/abs/2412.14810"}
{"created":"2025-05-01","title":"MoEtion: Efficient and Reliable Sparse Checkpointing for Mixture-of-Experts Models at Scale","abstract":"As large language models continue to scale, training them requires thousands of GPUs over prolonged durations--making frequent failures an inevitable reality. While checkpointing remains the primary fault-tolerance mechanism, existing methods struggle to efficiently support Mixture-of-Experts (MoE) models. Due to the substantially larger training state of MoE models, traditional checkpointing techniques incur prohibitive overheads, resulting in frequent stalls or prolonged recovery periods that severely degrade training efficiency.","authors":["Swapnil Gandhi","Christos Kozyrakis"],"url":"https://arxiv.org/abs/2412.15411"}
{"created":"2025-05-01","title":"Learning Disease Progression Models That Capture Health Disparities","abstract":"Disease progression models are widely used to inform the diagnosis and treatment of many progressive diseases. However, a significant limitation of existing models is that they do not account for health disparities that can bias the observed data. To address this, we develop an interpretable Bayesian disease progression model that captures three key health disparities: certain patient populations may (1) start receiving care only when their disease is more severe, (2) experience faster disease progression even while receiving care, or (3) receive follow-up care less frequently conditional on disease severity. We show theoretically and empirically that failing to account for any of these disparities can result in biased estimates of severity (e.g., underestimating severity for disadvantaged groups). On a dataset of heart failure patients, we show that our model can identify groups that face each type of health disparity, and that accounting for these disparities while inferring disease severity meaningfully shifts which patients are considered high-risk.","authors":["Erica Chiang","Divya Shanmugam","Ashley N. Beecy","Gabriel Sayer","Deborah Estrin","Nikhil Garg","Emma Pierson"],"url":"https://arxiv.org/abs/2412.16406"}
{"created":"2025-05-01","title":"Tensor Density Estimator by Convolution-Deconvolution","abstract":"We propose a linear algebraic framework for performing density estimation. It consists of three simple steps: convolving the empirical distribution with certain smoothing kernels to remove the exponentially large variance; compressing the empirical distribution after convolution as a tensor train, with efficient tensor decomposition algorithms; and finally, applying a deconvolution step to recover the estimated density from such tensor-train representation. Numerical results demonstrate the high accuracy and efficiency of the proposed methods.","authors":["Yifan Peng","Siyao Yang","Yuehaw Khoo","Daren Wang"],"url":"https://arxiv.org/abs/2412.18964"}
{"created":"2025-05-01","title":"OS-Genesis: Automating GUI Agent Trajectory Construction via Reverse Task Synthesis","abstract":"Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have demonstrated human-like computer control capability. Despite their utility in advancing digital automation, a critical bottleneck persists: collecting high-quality trajectory data for training. Common practices for collecting such data rely on human supervision or synthetic data generation through executing pre-defined tasks, which are either resource-intensive or unable to guarantee data quality. Moreover, these methods suffer from limited data diversity and significant gaps between synthetic data and real-world environments. To address these challenges, we propose OS-Genesis, a novel GUI data synthesis pipeline that reverses the conventional trajectory collection process. Instead of relying on pre-defined tasks, OS-Genesis enables agents first to perceive environments and perform step-wise interactions, then retrospectively derive high-quality tasks to enable trajectory-level exploration. A trajectory reward model is then employed to ensure the quality of the generated trajectories. We demonstrate that training GUI agents with OS-Genesis significantly improves their performance on highly challenging online benchmarks. In-depth analysis further validates OS-Genesis's efficiency and its superior data quality and diversity compared to existing synthesis methods. Our codes, data, and checkpoints are available at https://qiushisun.github.io/OS-Genesis-Home/.","authors":["Qiushi Sun","Kanzhi Cheng","Zichen Ding","Chuanyang Jin","Yian Wang","Fangzhi Xu","Zhenyu Wu","Chengyou Jia","Liheng Chen","Zhoumianze Liu","Ben Kao","Guohao Li","Junxian He","Yu Qiao","Zhiyong Wu"],"url":"https://arxiv.org/abs/2412.19723"}
{"created":"2025-05-01","title":"KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities","abstract":"Document-level relation extraction (Doc-RE) aims to extract relations between entities across multiple sentences. Therefore, Doc-RE requires more comprehensive reasoning abilities like humans, involving complex cross-sentence interactions between entities, contexts, and external general knowledge, compared to the sentence-level RE. However, most existing Doc-RE methods focus on optimizing single reasoning ability, but lack the ability to utilize external knowledge for comprehensive reasoning on long documents. To solve these problems, a knowledge retrieval augmented method, named KnowRA, was proposed with comprehensive reasoning to autonomously determine whether to accept external knowledge to assist DocRE. Firstly, we constructed a document graph for semantic encoding and integrated the co-reference resolution model to augment the co-reference reasoning ability. Then, we expanded the document graph into a document knowledge graph by retrieving the external knowledge base for common-sense reasoning and a novel knowledge filtration method was presented to filter out irrelevant knowledge. Finally, we proposed the axis attention mechanism to build direct and indirect associations with intermediary entities for achieving cross-sentence logical reasoning. Extensive experiments conducted on two datasets verified the effectiveness of our method compared to the state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/KnowRA.","authors":["Chengcheng Mai","Yuxiang Wang","Ziyu Gong","Hanxiang Wang","Yihua Huang"],"url":"https://arxiv.org/abs/2501.00571"}
{"created":"2025-05-01","title":"A Hybrid Deep Learning and Model-Checking Framework for Accurate Brain Tumor Detection and Validation","abstract":"Model checking, a formal verification technique, ensures systems meet predefined requirements, playing a crucial role in minimizing errors and enhancing quality during development. This paper introduces a novel hybrid framework integrating model checking with deep learning for brain tumor detection and validation in medical imaging. By combining model-checking principles with CNN-based feature extraction and K-FCM clustering for segmentation, the proposed approach enhances the reliability of tumor detection and segmentation. Experimental results highlight the framework's effectiveness, achieving 98\\% accuracy, 96.15\\% precision, and 100\\% recall, demonstrating its potential as a robust tool for advanced medical image analysis.","authors":["Elhoucine Elfatimi","Lahcen El Fatimi","Hanifa Bouchaneb"],"url":"https://arxiv.org/abs/2501.01991"}
{"created":"2025-05-01","title":"Effective Two-Stage Double Auction for Dynamic Resource Provision over Edge Networks via Discovering The Power of Overbooking","abstract":"To facilitate responsive and cost-effective computing service delivery over edge networks, this paper investigates a novel two-stage double auction methodology via discovering an interesting idea of resource overbooking to overcome dynamic and uncertain nature of supply of edge servers (sellers) and demand generated from mobile devices (as buyers). The proposed auction integrates multiple essential goals such as maximizing social welfare as well as accelerating the decision-making process from both short-term and long-term views, (e.g., the time for determining winning seller-buyer pairs), by introducing a stagewise strategy: an overbooking-driven pre-double auction (OPDAuction) for determining long-term cooperations between sellers and buyers before practical resource transactions as Stage I, and a real-time backup double auction (RBDAuction) for quickly coping with residual resource demands during actual transactions. In particular, by embedding a proper overbooking rate, OPDAuction helps with facilitating trading contracts between appropriate sellers and buyers as guidance for future transactions, by allowing the booked resources to exceed theoretical supply. Then, since pre-auctions may cause risks, our RBDAuction adjusts to real-time market changes, further enhancing the overall social welfare. More importantly, we offer an interesting view to show that our proposed two-stage auction can support significant design properties such as truthfulness, individual rationality, and budget balance. Through extensive experiments, we demonstrate good performance in social welfare, time efficiency, and computational scalability, outstripping conventional methods in dynamic edge computing settings.","authors":["Sicheng Wu","Minghui Liwang","Deqing Wang","Xianbin Wang","Chao Wu","Junyi Tang","Li Li","Xiaoyu Xia"],"url":"https://arxiv.org/abs/2501.04507"}
{"created":"2025-05-01","title":"Monotonicity and convergence of two-relaxation-times lattice Boltzmann schemes for a non-linear conservation law","abstract":"We address the convergence analysis of lattice Boltzmann methods for scalar non-linear conservation laws, focusing on two-relaxation-times (TRT) schemes. Unlike Finite Difference/Finite Volume methods, lattice Boltzmann schemes offer exceptional computational efficiency and parallelization capabilities. However, their monotonicity and $L^{\\infty}$-stability remain underexplored. Extending existing results on simpler BGK schemes, we derive conditions ensuring that TRT schemes are monotone and stable by leveraging their unique relaxation structure. Our analysis culminates in proving convergence of the numerical solution to the weak entropy solution of the conservation law. Compared to BGK schemes, TRT schemes achieve reduced numerical diffusion while retaining provable convergence. Numerical experiments validate and illustrate the theoretical findings.","authors":["Denise Aregba-Driollet (IMB","Bordeaux INP)","Thomas Bellotti (EM2C)"],"url":"https://arxiv.org/abs/2501.07934"}
{"created":"2025-05-01","title":"BRIGHT-VO: Brightness-Guided Hybrid Transformer for Visual Odometry with Multi-modality Refinement Module","abstract":"Visual odometry (VO) plays a crucial role in autonomous driving, robotic navigation, and other related tasks by estimating the position and orientation of a camera based on visual input. Significant progress has been made in data-driven VO methods, particularly those leveraging deep learning techniques to extract image features and estimate camera poses. However, these methods often struggle in low-light conditions because of the reduced visibility of features and the increased difficulty of matching keypoints. To address this limitation, we introduce BrightVO, a novel VO model based on Transformer architecture, which not only performs front-end visual feature extraction, but also incorporates a multi-modality refinement module in the back-end that integrates Inertial Measurement Unit (IMU) data. Using pose graph optimization, this module iteratively refines pose estimates to reduce errors and improve both accuracy and robustness. Furthermore, we create a synthetic low-light dataset, KiC4R, which includes a variety of lighting conditions to facilitate the training and evaluation of VO frameworks in challenging environments. Experimental results demonstrate that BrightVO achieves state-of-the-art performance on both the KiC4R dataset and the KITTI benchmarks. Specifically, it provides an average improvement of 20% in pose estimation accuracy in normal outdoor environments and 259% in low-light conditions, outperforming existing methods. For widespread use and further development, the research work is fully open-source at https://github.com/Anastasiawd/BrightVO.","authors":["Dongzhihan Wang","Yang Yang","Liang Xu"],"url":"https://arxiv.org/abs/2501.08659"}
{"created":"2025-05-01","title":"Mining Diamonds in labeled Transition Systems","abstract":"Labeled transition systems can be a great way to visualize the complex behavior of parallel and communicating systems. However, if, during a particular timeframe, no synchronization or communication between processes occurs, then multiple parallel sequences of actions are able to interleave arbitrarily, and the resulting graph quickly becomes too complex for the human eye to understand easily. With that in mind, we propose an exact formalization of these arbitrary interleavings, and an algorithm to find all said interleavings in deterministic LTSs, to reduce the visual complexity of labeled transition systems.","authors":["P. H. M. van Spaendonck","K. H. J. Jilissen"],"url":"https://arxiv.org/abs/2501.08689"}
{"created":"2025-05-01","title":"Octopus: Scalable Low-Cost CXL Memory Pooling","abstract":"Compute Express Link (CXL) enables compute \"pods\" with memory pooling across hosts to reduce cost and improve efficiency. Existing pods are small, use exotic many-ported pooling devices, or require indirection through expensive switches. These conventional designs implicitly assume that pods must fully connect all hosts to all CXL pooling devices.","authors":["Daniel S. Berger","Yuhong Zhong","Fiodar Kazhamiaka","Pantea Zardoshti","Shuwei Teng","Mark D. Hill","Rodrigo Fonseca"],"url":"https://arxiv.org/abs/2501.09020"}
{"created":"2025-05-01","title":"Several Representations of $\\alpha$-Mutual Information and Interpretations as Privacy Leakage Measures","abstract":"In this paper, we present several novel representations of $\\alpha$-mutual information ($\\alpha$-MI) in terms of R{\\' e}nyi divergence and conditional R{\\' e}nyi entropy. The representations are based on the variational characterizations of $\\alpha$-MI using a reverse channel. Based on these representations, we provide several interpretations of the $\\alpha$-MI as privacy leakage measures using generalized mean and gain functions. Further, as byproducts of the representations, we propose novel conditional R{\\' e}nyi entropies that satisfy the property that conditioning reduces entropy and data-processing inequality.","authors":["Akira Kamatsuka","Takashiro Yoshida"],"url":"https://arxiv.org/abs/2501.10099"}
{"created":"2025-05-01","title":"Lossless data compression at pragmatic rates","abstract":"The problem of variable-rate lossless data compression is considered, for codes with and without prefix constraints. Sharp bounds are derived for the best achievable compression rate of memoryless sources, when the excess-rate probability is required to be exponentially small in the blocklength. Accurate nonasymptotic expansions with explicit constants are obtained for the optimal rate, using tools from large deviations and Gaussian approximation. Examples are shown indicating that, in the small excess-rate-probability regime, the approximation to the fundamental limit of the compression rate suggested by these bounds is significantly more accurate than the approximations provided by either normal approximation or error exponents. The new bounds reinforce the crucial operational conclusion that, in applications where the blocklength is relatively short and where stringent guarantees are required on the rate, the best achievable rate is no longer close to the entropy. Rather, it is an appropriate, more pragmatic rate, determined via the inverse error exponent function and the blocklength.","authors":["Andreas Theocharous","Ioannis Kontoyiannis"],"url":"https://arxiv.org/abs/2501.10103"}
{"created":"2025-05-01","title":"Rethinking Pseudo-Label Guided Learning for Weakly Supervised Temporal Action Localization from the Perspective of Noise Correction","abstract":"Pseudo-label learning methods have been widely applied in weakly-supervised temporal action localization. Existing works directly utilize weakly-supervised base model to generate instance-level pseudo-labels for training the fully-supervised detection head. We argue that the noise in pseudo-labels would interfere with the learning of fully-supervised detection head, leading to significant performance leakage. Issues with noisy labels include:(1) inaccurate boundary localization; (2) undetected short action clips; (3) multiple adjacent segments incorrectly detected as one segment. To target these issues, we introduce a two-stage noisy label learning strategy to harness every potential useful signal in noisy labels. First, we propose a frame-level pseudo-label generation model with a context-aware denoising algorithm to refine the boundaries. Second, we introduce an online-revised teacher-student framework with a missing instance compensation module and an ambiguous instance correction module to solve the short-action-missing and many-to-one problems. Besides, we apply a high-quality pseudo-label mining loss in our online-revised teacher-student framework to add different weights to the noisy labels to train more effectively. Our model outperforms the previous state-of-the-art method in detection accuracy and inference speed greatly upon the THUMOS14 and ActivityNet v1.2 benchmarks.","authors":["Quan Zhang","Yuxin Qi","Xi Tang","Rui Yuan","Xi Lin","Ke Zhang","Chun Yuan"],"url":"https://arxiv.org/abs/2501.11124"}
{"created":"2025-05-01","title":"An Improved Lower Bound on Oblivious Transfer Capacity Using Polarization and Interaction","abstract":"We consider the oblivious transfer (OT) capacities of noisy channels against the passive adversary; this problem has not been solved even for the binary symmetric channel (BSC). In the literature, the general construction of OT has been known only for generalized erasure channels (GECs); for the BSC, we convert the channel to the binary symmetric erasure channel (BSEC), which is a special instance of the GEC, via alphabet extension and erasure emulation. In a previous paper by the authors, we derived an improved lower bound on the OT capacity of BSC by proposing a method to recursively emulate BSEC via interactive communication. In this paper, we introduce two new ideas of OT construction: (i) via ``polarization\" and interactive communication, we recursively emulate GECs that are not necessarily a BSEC; (ii) in addition to the GEC emulation part, we also utilize interactive communication in the key agreement part of OT protocol. By these methods, we derive lower bounds on the OT capacity of BSC that are superior to the previous one for a certain range of crossover probabilities of the BSC. Via our new lower bound, we show that, at the crossover probability being zero, the slope of tangent of the OT capacity is unbounded.","authors":["So Suda","Shun Watanabe"],"url":"https://arxiv.org/abs/2501.11883"}
{"created":"2025-05-01","title":"Sample complexity of data-driven tuning of model hyperparameters in neural networks with structured parameter-dependent dual function","abstract":"Modern machine learning algorithms, especially deep learning based techniques, typically involve careful hyperparameter tuning to achieve the best performance. Despite the surge of intense interest in practical techniques like Bayesian optimization and random search based approaches to automating this laborious and compute intensive task, the fundamental learning theoretic complexity of tuning hyperparameters for deep neural networks is poorly understood. Inspired by this glaring gap, we initiate the formal study of hyperparameter tuning complexity in deep learning through a recently introduced data driven setting. We assume that we have a series of deep learning tasks, and we have to tune hyperparameters to do well on average over the distribution of tasks. A major difficulty is that the utility function as a function of the hyperparameter is very volatile and furthermore, it is given implicitly by an optimization problem over the model parameters. To tackle this challenge, we introduce a new technique to characterize the discontinuities and oscillations of the utility function on any fixed problem instance as we vary the hyperparameter; our analysis relies on subtle concepts including tools from differential/algebraic geometry and constrained optimization. This can be used to show that the learning theoretic complexity of the corresponding family of utility functions is bounded. We instantiate our results and provide sample complexity bounds for concrete applications tuning a hyperparameter that interpolates neural activation functions and setting the kernel parameter in graph neural networks.","authors":["Maria-Florina Balcan","Anh Tuan Nguyen","Dravyansh Sharma"],"url":"https://arxiv.org/abs/2501.13734"}
{"created":"2025-05-01","title":"Switched Feedback for the Multiple-Access Channel","abstract":"A mechanism called switched feedback is introduced; under switched feedback, each channel output goes forward to the receiver(s) or back to the transmitter(s) but never both. By studying the capacity of the Multiple-Access Channel (MAC) with switched feedback, this work investigates the benefits of feedback, seeking to maximize that benefit under reliable and unreliable feedback scenarios. The study is used to explore the tradeoffs between cooperation and transmission in the context of communication systems. Results include upper and lower bounds on the capacity region of the MAC with switched feedback.","authors":["Oliver Kosut","Michael Langberg","Michelle Effros"],"url":"https://arxiv.org/abs/2501.14064"}
{"created":"2025-05-01","title":"Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?","abstract":"Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, \"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.","authors":["Yutong Yin","Zhaoran Wang"],"url":"https://arxiv.org/abs/2501.15857"}
{"created":"2025-05-01","title":"Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles","abstract":"Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Additionally, our MSCN enhances feature representation robustness by training with unseen domain point clouds derived from source domain point clouds. This method acquires domain-invariant features and exhibits robust, consistent performance across various point cloud datasets, ensuring compatibility with diverse sensor configurations without the need for parameter adjustments. This highlights MSCN's potential to significantly improve the reliability and domain invariant features in different environments. Our code is available at https://github.com/MLMLab/MSCN.","authors":["Younggun Kim","Beomsik Cho","Seonghoon Ryoo","Soomok Lee"],"url":"https://arxiv.org/abs/2501.16289"}
{"created":"2025-05-01","title":"Experimental Evaluation of an SDN Controller for Open Optical-circuit-switched Networks","abstract":"Open optical networks have been considered to be important for cost-effectively building and operating the networks. Recently, the optical-circuit-switches (OCSes) have attracted industry and academia because of their cost efficiency and higher capacity than traditional electrical packet switches (EPSes) and reconfigurable optical add drop multiplexers (ROADMs). Though the open interfaces and control planes for traditional ROADMs and transponders have been defined by several standard-defining organizations (SDOs), those of OCSes have not. Considering that several OCSes have already been installed in production datacenter networks (DCNs) and several OCS products are on the market, bringing the openness and interoperability into the OCS-based networks has become important. Motivated by this fact, this paper investigates a software-defined networking (SDN) controller for open optical-circuit-switched networks. To this end, we identified the use cases of OCSes and derived the controller requirements for supporting them. We then proposed a multi-vendor (MV) OCS controller framework that satisfies the derived requirements; it was designed to quickly and consistently operate fiber paths upon receiving the operation requests. We validated our controller by implementing it and evaluating its performance on actual MV-OCS networks. It satisfied all the requirements, and fiber paths could be configured within 1.0 second by using our controller.","authors":["Kazuya Anazawa","Takeru Inoue","Toru Mano","Hiroshi Ou","Hirotaka Ujikawa","Dmitrii Briantcev","Sumaiya Binte Ali","Devika Dass","Hideki Nishizawa","Yoshiaki Sone","Eoin Kenny","Marco Ruffini","Daniel Kilper","Eiji Oki","Koichi Takasugi"],"url":"https://arxiv.org/abs/2501.16907"}
{"created":"2025-05-01","title":"Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment","abstract":"We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.","authors":["Zixue Zeng","Xiaoyan Zhao","Matthew Cartier","Tong Yu","Jing Wang","Xin Meng","Zhiyu Sheng","Maryam Satarpour","John M Cormack","Allison Bean","Ryan Nussbaum","Maya Maurer","Emily Landis-Walkenhorst","Dinesh Kumbhare","Kang Kim","Ajay Wasan","Jiantao Pu"],"url":"https://arxiv.org/abs/2501.17690"}
{"created":"2025-05-01","title":"WARP: An Efficient Engine for Multi-Vector Retrieval","abstract":"Multi-vector retrieval methods such as ColBERT and its recent variant, the ConteXtualized Token Retriever (XTR), offer high accuracy but face efficiency challenges at scale. To address this, we present WARP, a retrieval engine that substantially improves the efficiency of retrievers trained with the XTR objective through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation; (2) implicit decompression, avoiding costly vector reconstruction during retrieval; and (3) a two-stage reduction process for efficient score aggregation. Combined with highly-optimized C++ kernels, our system reduces end-to-end latency compared to XTR's reference implementation by 41x, and achieves a 3x speedup over the ColBERTv2/PLAID engine, while preserving retrieval quality.","authors":["Jan Luca Scheerer","Matei Zaharia","Christopher Potts","Gustavo Alonso","Omar Khattab"],"url":"https://arxiv.org/abs/2501.17788"}
{"created":"2025-05-01","title":"Using Read Promotion and Mixed Isolation Levels for Performant Yet Serializable Execution of Transaction Programs","abstract":"We propose a theory that can determine the lowest isolation level that can be allocated to each transaction program in an application in a mixed-isolation-level setting, to guarantee that all executions will be serializable and thus preserve all integrity constraints, even those that are not explicitly declared. This extends prior work applied to completely known transactions, to deal with the realistic situation where transactions are generated by running programs with parameters that are not known in advance. Using our theory, we propose an optimization method that allows for high throughput while ensuring that all executions are serializable. Our method is based on searching for application code modifications that are semantics-preserving while improving the isolation level allocation. We illustrate our approach to the SmallBank benchmark.","authors":["Brecht Vandevoort","Alan Fekete","Bas Ketsman","Frank Neven","Stijn Vansummeren"],"url":"https://arxiv.org/abs/2501.18377"}
{"created":"2025-05-01","title":"AuthOr: Lower Cost Authenticity-Oriented Garbling for Arbitrary Boolean Circuits","abstract":"Authenticity-oriented (previously named as privacy-free) garbling schemes of Frederiksen et al. Eurocrypt '15 are designed to satisfy only the authenticity criterion of Bellare et al. ACM CCS '12, and to be more efficient compared to full-fledged garbling schemes. In this work, we improve the state-of-the-art authenticity-oriented version of half gates (HG) garbling of Zahur et al. Crypto '15 by allowing it to be bandwidth-free if any of the input wires of an AND gate is freely settable by the garbler. Our full solution AuthOr then successfully combines the ideas from information-theoretical garbling of Kondi and Patra Crypto '17 and the HG garbling-based scheme that we obtained. AuthOr has a lower communication cost (i.e., garbled circuit or GC size) than HG garbling without any further security assumption. Theoretically, AuthOr's GC size reduction over HG garbling lies in the range between 0 to 100%, and the exact improvement depends on the circuit structure. We have implemented our scheme and conducted tests on various circuits that were constructed by independent researchers. Our experimental results show that in practice, the GC size gain may be up to around 98%.","authors":["Osman Bi\\c{c}er","Ali Ajorian"],"url":"https://arxiv.org/abs/2501.18387"}
{"created":"2025-05-01","title":"Path Planning and Optimization for Cuspidal 6R Manipulators","abstract":"A cuspidal robot can move from one inverse kinematics (IK) solution to another without crossing a singularity. Multiple industrial robots are cuspidal. They tend to have a beautiful mechanical design, but they pose path planning challenges. A task-space path may have a valid IK solution for each point along the path, but a continuous joint-space path may depend on the choice of the IK solution or even be infeasible. This paper presents new analysis, path planning, and optimization methods to enhance the utility of cuspidal robots. We first demonstrate an efficient method to identify cuspidal robots and show, for the first time, that the ABB GoFa and certain robots with three parallel joint axes are cuspidal. We then propose a new path planning method for cuspidal robots by finding all IK solutions for each point along a task-space path and constructing a graph to connect each vertex corresponding to an IK solution. Graph edges have a weight based on the optimization metric, such as minimizing joint velocity. The optimal feasible path is the shortest path in the graph. This method can find non-singular paths as well as smooth paths which pass through singularities. Finally, we incorporate this path planning method into a path optimization algorithm. Given a fixed workspace toolpath, we optimize the offset of the toolpath in the robot base frame while ensuring continuous joint motion. Code examples are available in a publicly accessible repository.","authors":["Alexander J. Elias","John T. Wen"],"url":"https://arxiv.org/abs/2501.18505"}
{"created":"2025-05-01","title":"Towards Understanding Depth Perception in Foveated Rendering","abstract":"The true vision for real-time virtual and augmented reality is reproducing our visual reality in its entirety on immersive displays. To this end, foveated rendering leverages the limitations of spatial acuity in human peripheral vision to allocate computational resources to the fovea while reducing quality in the periphery. Such methods are often derived from studies on the spatial resolution of the human visual system and its ability to perceive blur in the periphery, enabling the potential for high spatial quality in real-time. However, the effects of blur on other visual cues that depend on luminance contrast, such as depth, remain largely unexplored. It is critical to understand this interplay, as accurate depth representation is a fundamental aspect of visual realism. In this paper, we present the first evaluation exploring the effects of foveated rendering on stereoscopic depth perception. We design a psychovisual experiment to quantitatively study the effects of peripheral blur on depth perception. Our analysis demonstrates that stereoscopic acuity remains unaffected (or even improves) by high levels of peripheral blur. Based on our studies, we derive a simple perceptual model that determines the amount of foveation that does not affect stereoacuity. Furthermore, we analyze the model in the context of common foveation practices reported in literature. The findings indicate that foveated rendering does not impact stereoscopic depth perception, and stereoacuity remains unaffected with up to 2x stronger foveation than commonly used. Finally, we conduct a validation experiment and show that our findings hold for complex natural stimuli.","authors":["Sophie Kerga{\\ss}ner","Taimoor Tariq","Piotr Didyk"],"url":"https://arxiv.org/abs/2501.18635"}
{"created":"2025-05-01","title":"Deep Learning Model Inversion Attacks and Defenses: A Comprehensive Survey","abstract":"The rapid adoption of deep learning in sensitive domains has brought tremendous benefits. However, this widespread adoption has also given rise to serious vulnerabilities, particularly model inversion (MI) attacks, posing a significant threat to the privacy and integrity of personal data. The increasing prevalence of these attacks in applications such as biometrics, healthcare, and finance has created an urgent need to understand their mechanisms, impacts, and defense methods. This survey aims to fill the gap in the literature by providing a structured and in-depth review of MI attacks and defense strategies. Our contributions include a systematic taxonomy of MI attacks, extensive research on attack techniques and defense mechanisms, and a discussion about the challenges and future research directions in this evolving field. By exploring the technical and ethical implications of MI attacks, this survey aims to offer insights into the impact of AI-powered systems on privacy, security, and trust. In conjunction with this survey, we have developed a comprehensive repository to support research on MI attacks and defenses. The repository includes state-of-the-art research papers, datasets, evaluation metrics, and other resources to meet the needs of both novice and experienced researchers interested in MI attacks and defenses, as well as the broader field of AI security and privacy. The repository will be continuously maintained to ensure its relevance and utility. It is accessible at https://github.com/overgter/Deep-Learning-Model-Inversion-Attacks-and-Defenses.","authors":["Wencheng Yang","Song Wang","Di Wu","Taotao Cai","Yanming Zhu","Shicheng Wei","Yiying Zhang","Xu Yang","Zhaohui Tang","Yan Li"],"url":"https://arxiv.org/abs/2501.18934"}
{"created":"2025-05-01","title":"BCAT: A Block Causal Transformer for PDE Foundation Models for Fluid Dynamics","abstract":"We introduce BCAT, a PDE foundation model designed for autoregressive prediction of solutions to two dimensional fluid dynamics problems. Our approach uses a block causal transformer architecture to model next frame predictions, leveraging previous frames as contextual priors rather than relying solely on sub-frames or pixel-based inputs commonly used in image generation methods. This block causal framework more effectively captures the spatial dependencies inherent in nonlinear spatiotemporal dynamics and physical phenomena. In an ablation study, next frame prediction demonstrated a 3.5x accuracy improvement over next token prediction. BCAT is trained on a diverse range of fluid dynamics datasets, including incompressible and compressible Navier-Stokes equations across various geometries and parameter regimes, as well as the shallow-water equations. The model's performance was evaluated on 6 distinct downstream prediction tasks and tested on about 8K trajectories to measure robustness on a variety of fluid dynamics simulations. BCAT achieved an average relative error of 1.18% across all evaluation tasks, outperforming prior approaches on standard benchmarks. With fine-tuning on a turbulence dataset, we show that the method adapts to new settings with more than 40% better accuracy over prior methods.","authors":["Yuxuan Liu","Jingmin Sun","Hayden Schaeffer"],"url":"https://arxiv.org/abs/2501.18972"}
{"created":"2025-05-01","title":"Explorations of the Softmax Space: Knowing When the Neural Network Doesn't Know","abstract":"Ensuring the reliability of automated decision-making based on neural networks will be crucial as Artificial Intelligence systems are deployed more widely in critical situations. This paper proposes a new approach for measuring confidence in the predictions of any neural network that relies on the predictions of a softmax layer. We identify that a high-accuracy trained network may have certain outputs for which there should be low confidence. In such cases, decisions should be deferred and it is more appropriate for the network to provide a \\textit{not known} answer to a corresponding classification task. Our approach clusters the vectors in the softmax layer to measure distances between cluster centroids and network outputs. We show that a cluster with centroid calculated simply as the mean softmax output for all correct predictions can serve as a suitable proxy in the evaluation of confidence. Defining a distance threshold for a class as the smallest distance from an incorrect prediction to the given class centroid offers a simple approach to adding \\textit{not known} answers to any network classification falling outside of the threshold. We evaluate the approach on the MNIST and CIFAR-10 datasets using a Convolutional Neural Network and a Vision Transformer, respectively. The results show that our approach is consistent across datasets and network models, and indicate that the proposed distance metric can offer an efficient way of determining when automated predictions are acceptable and when they should be deferred to human operators.","authors":["Daniel Sikar","Artur d'Avila Garcez","Tillman Weyde"],"url":"https://arxiv.org/abs/2502.00456"}
{"created":"2025-05-01","title":"Multipath TCP with Single Radio Access Technologies: a Paradox or an Opportunity?","abstract":"This paper addresses the use of Multipath Transmission Control Protocol (MPTCP) in a single Radio Access Technology (RAT) network. Different from other studies where multiple RATs are explored by the MPTCP, a situation that cannot be always guaranteed, due to lack of coverage for example, in this work we assess and evaluate the capability of MPTCP to operate over a single RAT environment. With a vehicular network as use case, we show how the IEEE 802.11p interface is shared among the multiple logical links created between the On-Board Unit (OBU) and the several Road Side Units (RSUs) in its range, supporting the different MPTCP subflows. The results, obtained through experimentation with real vehicular networking hardware, show that MPTCP allows for seamless handovers, ensuring continuous, stable and efficient communication in highly mobile environments.","authors":["Jo\\~ao Torrinhas","Miguel Lu\\'is","Duarte Dias","Pedro Rito","Susana Sargento"],"url":"https://arxiv.org/abs/2502.01290"}
{"created":"2025-05-01","title":"IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning","abstract":"Using extensive training data from SA-1B, the Segment Anything Model (SAM) has demonstrated exceptional generalization and zero-shot capabilities, attracting widespread attention in areas such as medical image segmentation and remote sensing image segmentation. However, its performance in the field of image manipulation detection remains largely unexplored and unconfirmed. There are two main challenges in applying SAM to image manipulation detection: a) reliance on manual prompts, and b) the difficulty of single-view information in supporting cross-dataset generalization. To address these challenges, we develops a cross-view prompt learning paradigm called IMDPrompter based on SAM. Benefiting from the design of automated prompts, IMDPrompter no longer relies on manual guidance, enabling automated detection and localization. Additionally, we propose components such as Cross-view Feature Perception, Optimal Prompt Selection, and Cross-View Prompt Consistency, which facilitate cross-view perceptual learning and guide SAM to generate accurate masks. Extensive experimental results from five datasets (CASIA, Columbia, Coverage, IMD2020, and NIST16) validate the effectiveness of our proposed method.","authors":["Quan Zhang","Yuxin Qi","Xi Tang","Jinwei Fang","Xi Lin","Ke Zhang","Chun Yuan"],"url":"https://arxiv.org/abs/2502.02454"}
{"created":"2025-05-01","title":"Elucidating the Preconditioning in Consistency Distillation","abstract":"Consistency distillation is a prevalent way for accelerating diffusion models adopted in consistency (trajectory) models, in which a student model is trained to traverse backward on the probability flow (PF) ordinary differential equation (ODE) trajectory determined by the teacher model. Preconditioning is a vital technique for stabilizing consistency distillation, by linear combining the input data and the network output with pre-defined coefficients as the consistency function. It imposes the boundary condition of consistency functions without restricting the form and expressiveness of the neural network. However, previous preconditionings are hand-crafted and may be suboptimal choices. In this work, we offer the first theoretical insights into the preconditioning in consistency distillation, by elucidating its design criteria and the connection to the teacher ODE trajectory. Based on these analyses, we further propose a principled way dubbed \\textit{Analytic-Precond} to analytically optimize the preconditioning according to the consistency gap (defined as the gap between the teacher denoiser and the optimal student denoiser) on a generalized teacher ODE. We demonstrate that Analytic-Precond can facilitate the learning of trajectory jumpers, enhance the alignment of the student trajectory with the teacher's, and achieve $2\\times$ to $3\\times$ training acceleration of consistency trajectory models in multi-step generation across various datasets.","authors":["Kaiwen Zheng","Guande He","Jianfei Chen","Fan Bao","Jun Zhu"],"url":"https://arxiv.org/abs/2502.02922"}
{"created":"2025-05-01","title":"Agentic AI Systems Applied to tasks in Financial Services: Modeling and model risk management crews","abstract":"The advent of large language models has ushered in a new era of agentic systems, where artificial intelligence programs exhibit remarkable autonomous decision-making capabilities across diverse domains. This paper explores agentic system workflows in the financial services industry. In particular, we build agentic crews with human-in-the-loop module that can effectively collaborate to perform complex modeling and model risk management (MRM) tasks. The modeling crew consists of a judge agent and multiple agents who perform specific tasks such as exploratory data analysis, feature engineering, model selection/hyperparameter tuning, model training, model evaluation, and writing documentation. The MRM crew consists of a judge agent along with specialized agents who perform tasks such as checking compliance of modeling documentation, model replication, conceptual soundness, analysis of outcomes, and writing documentation. We demonstrate the effectiveness and robustness of modeling and MRM crews by presenting a series of numerical examples applied to credit card fraud detection, credit card approval, and portfolio credit risk modeling datasets.","authors":["Izunna Okpala","Ashkan Golgoon","Arjun Ravi Kannan"],"url":"https://arxiv.org/abs/2502.05439"}
{"created":"2025-05-01","title":"Efficient Diffusion Models: A Survey","abstract":"Diffusion models have emerged as powerful generative models capable of producing high-quality contents such as images, videos, and audio, demonstrating their potential to revolutionize digital content creation. However, these capabilities come at the cost of their significant computational resources and lengthy generation time, underscoring the critical need to develop efficient techniques for practical deployment. In this survey, we provide a systematic and comprehensive review of research on efficient diffusion models. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient diffusion model topics from algorithm-level, system-level, and framework perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-Diffusion-Model-Survey. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient diffusion model research and inspire them to contribute to this important and exciting field.","authors":["Hui Shen","Jingxuan Zhang","Boning Xiong","Rui Hu","Shoufa Chen","Zhongwei Wan","Xin Wang","Yu Zhang","Zixuan Gong","Guangyin Bao","Chaofan Tao","Yongfeng Huang","Ye Yuan","Mi Zhang"],"url":"https://arxiv.org/abs/2502.06805"}
{"created":"2025-05-01","title":"Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification","abstract":"Although large language models (LLMs) have demonstrated remarkable capabilities in recent years, the potential of information theory (IT) to enhance LLM development remains underexplored. This paper introduces the information theoretic principle of Conditional Mutual Information (CMI) to LLM fine-tuning for classification tasks, exploring its promise in two main ways: minimizing CMI to improve a model's standalone performance and maximizing CMI to enhance knowledge distillation (KD) for more capable student models. To apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained deep learning framework, which was initially developed for image classification, with some modification. By minimizing CMI during LLM fine-tuning, we achieve superior performance gains on 6 of 8 GLUE classification tasks compared to BERT. Additionally, maximizing CMI during the KD process results in significant performance improvements in 6 of 8 GLUE classification tasks compared to DistilBERT. These findings demonstrate CMI's adaptability for optimizing both standalone LLMs and student models, showcasing its potential as a robust framework for advancing LLM fine-tuning. Our work bridges the gap between information theory and LLM development, offering new insights for building high-performing language models.","authors":["Thanushon Sivakaran","En-Hui Yang"],"url":"https://arxiv.org/abs/2502.11258"}
{"created":"2025-05-01","title":"Changing the Rules of the Game: Reasoning about Dynamic Phenomena in Multi-Agent Systems","abstract":"The design and application of multi-agent systems (MAS) require reasoning about the effects of modifications on their underlying structure. In particular, such changes may impact the satisfaction of system specifications and the strategic abilities of their autonomous components. In this paper, we are concerned with the problem of verifying and synthesising modifications (or updates) of MAS. We propose an extension of the Alternating-Time Temporal Logic ($\\mathsf{ATL}$) that enables reasoning about the dynamics of model change, called the Logic for $\\mathsf{ATL}$ Model Building ($\\mathsf{LAMB}$). We show how $\\mathsf{LAMB}$ can express various intuitions and ideas about the dynamics of MAS, from normative updates to mechanism design. As the main technical result, we prove that, while being strictly more expressive than $\\mathsf{ATL}$, $\\mathsf{LAMB}$ enjoys a P-complete model-checking procedure.","authors":["Rustam Galimullin","Maksim Gladyshev","Munyque Mittelmann","Nima Motamed"],"url":"https://arxiv.org/abs/2502.11785"}
{"created":"2025-05-01","title":"A Shape Lemma for Ideals of Differential Operators","abstract":"We propose a version of the classical shape lemma for zero-dimensional ideals of a commutative multivariate polynomial ring to the noncommutative setting of zero-dimensional ideals in an algebra of differential operators.","authors":["Manuel Kauers","Christoph Koutschan","Thibaut Verron"],"url":"https://arxiv.org/abs/2502.11787"}
{"created":"2025-05-01","title":"Improving Grip Stability Using Passive Compliant Microspine Arrays for Soft Robots in Unstructured Terrain","abstract":"Microspine grippers are small spines commonly found on insect legs that reinforce surface interaction by engaging with asperities to increase shear force and traction. An array of such microspines, when integrated into the limbs or undercarriage of a robot, can provide the ability to maneuver uneven terrains, traverse inclines, and even climb walls. Conformability and adaptability of soft robots makes them ideal candidates for these applications involving traversal of complex, unstructured terrains. However, there remains a real-life realization gap for soft locomotors pertaining to their transition from controlled lab environment to the field by improving grip stability through effective integration of microspines. We propose a passive, compliant microspine stacked array design to enhance the locomotion capabilities of mobile soft robots, in our case, ones that are motor tendon actuated. We offer a standardized microspine array integration method with effective soft-compliant stiffness integration, and reduced complexity resulting from a single actuator passively controlling them. The presented design utilizes a two-row, stacked microspine array configuration that offers additional gripping capabilities on extremely steep/irregular surfaces from the top row while not hindering the effectiveness of the more frequently active bottom row. We explore different configurations of the microspine array to account for changing surface topologies and enable independent, adaptable gripping of asperities per microspine. Field test experiments are conducted on various rough surfaces including concrete, brick, compact sand, and tree roots with three robots consisting of a baseline without microspines compared against two robots with different combinations of microspine arrays. Tracking results indicate that the inclusion of microspine arrays increases planar displacement on average by 15 and 8 times.","authors":["Lauren Ervin","Harish Bezawada","Vishesh Vikas"],"url":"https://arxiv.org/abs/2502.12347"}
{"created":"2025-05-01","title":"Iron Sharpens Iron: Defending Against Attacks in Machine-Generated Text Detection with Adversarial Training","abstract":"Machine-generated Text (MGT) detection is crucial for regulating and attributing online texts. While the existing MGT detectors achieve strong performance, they remain vulnerable to simple perturbations and adversarial attacks. To build an effective defense against malicious perturbations, we view MGT detection from a threat modeling perspective, that is, analyzing the model's vulnerability from an adversary's point of view and exploring effective mitigations. To this end, we introduce an adversarial framework for training a robust MGT detector, named GREedy Adversary PromoTed DefendER (GREATER). The GREATER consists of two key components: an adversary GREATER-A and a detector GREATER-D. The GREATER-D learns to defend against the adversarial attack from GREATER-A and generalizes the defense to other attacks. GREATER-A identifies and perturbs the critical tokens in embedding space, along with greedy search and pruning to generate stealthy and disruptive adversarial examples. Besides, we update the GREATER-A and GREATER-D synchronously, encouraging the GREATER-D to generalize its defense to different attacks and varying attack intensities. Our experimental results across 10 text perturbation strategies and 6 adversarial attacks show that our GREATER-D reduces the Attack Success Rate (ASR) by 0.67% compared with SOTA defense methods while our GREATER-A is demonstrated to be more effective and efficient than SOTA attack approaches. Codes and dataset are available in https://github.com/Liyuuuu111/GREATER.","authors":["Yuanfan Li","Zhaohan Zhang","Chengzhengxu Li","Chao Shen","Xiaoming Liu"],"url":"https://arxiv.org/abs/2502.12734"}
{"created":"2025-05-01","title":"LabTOP: A Unified Model for Lab Test Outcome Prediction on Electronic Health Records","abstract":"Lab tests are fundamental for diagnosing diseases and monitoring patient conditions. However, frequent testing can be burdensome for patients, and test results may not always be immediately available. To address these challenges, we propose LabTOP, a unified model that predicts lab test outcomes by leveraging a language modeling approach on EHR data. Unlike conventional methods that estimate only a subset of lab tests or classify discrete value ranges, LabTOP performs continuous numerical predictions for a diverse range of lab items. We evaluate LabTOP on three publicly available EHR datasets and demonstrate that it outperforms existing methods, including traditional machine learning models and state-of-the-art large language models. We also conduct extensive ablation studies to confirm the effectiveness of our design choices. We believe that LabTOP will serve as an accurate and generalizable framework for lab test outcome prediction, with potential applications in clinical decision support and early detection of critical conditions.","authors":["Sujeong Im","Jungwoo Oh","Edward Choi"],"url":"https://arxiv.org/abs/2502.14259"}
{"created":"2025-05-01","title":"Adversarially-Robust Gossip Algorithms for Approximate Quantile and Mean Computations","abstract":"This paper presents gossip algorithms for aggregation tasks that demonstrate both robustness to adversarial corruptions of any order of magnitude and optimality across a substantial range of these corruption levels. Gossip algorithms distribute information in a scalable and efficient way by having random pairs of nodes exchange small messages. Value aggregation problems are of particular interest in this setting as they occur frequently in practice and many elegant algorithms have been proposed for computing aggregates and statistics such as averages and quantiles. An important and well-studied advantage of gossip algorithms is their robustness to message delays, network churn, and unreliable message transmissions. These crucial robustness guarantees however only hold if all nodes follow the protocol and no messages are corrupted. In this paper, we remedy this by providing a framework to model both adversarial participants and message corruptions in gossip-style communications by allowing an adversary to control a small fraction of the nodes or corrupt messages arbitrarily. Despite this very powerful and general corruption model, we show that one can design robust gossip algorithms for many important aggregation problems. Our algorithms guarantee that almost all nodes converge to an approximately correct answer with optimal efficiency and essentially as fast as without corruptions. The design of adversarially-robust gossip algorithms poses completely new challenges. Despite this, our algorithms remain very simple variations of known non-robust algorithms with often only subtle changes to avoid non-compliant nodes gaining too much influence over outcomes. While our algorithms remain simple, their analysis is much more complex and often requires a completely different approach than the non-adversarial setting.","authors":["Bernhard Haeupler","Marc Kaufmann","Raghu Raman Ravi","Ulysse Schaller"],"url":"https://arxiv.org/abs/2502.15320"}
{"created":"2025-05-01","title":"SparseTransX: Efficient Training of Translation-Based Knowledge Graph Embeddings Using Sparse Matrix Operations","abstract":"Knowledge graph (KG) learning offers a powerful framework for generating new knowledge and making inferences. Training KG embedding can take a significantly long time, especially for larger datasets. Our analysis shows that the gradient computation of embedding is one of the dominant functions in the translation-based KG embedding training loop. We address this issue by replacing the core embedding computation with SpMM (Sparse-Dense Matrix Multiplication) kernels. This allows us to unify multiple scatter (and gather) operations as a single operation, reducing training time and memory usage. We create a general framework for training KG models using sparse kernels and implement four models, namely TransE, TransR, TransH, and TorusE. Our sparse implementations exhibit up to 5.3x speedup on the CPU and up to 4.2x speedup on the GPU with a significantly low GPU memory footprint. The speedups are consistent across large and small datasets for a given model. Our proposed sparse approach can be extended to accelerate other translation-based (such as TransC, TransM, etc.) and non-translational (such as DistMult, ComplEx, RotatE, etc.) models as well. An implementation of the SpTransX framework is publicly available as a Python package in https://github.com/HipGraph/SpTransX.","authors":["Md Saidul Hoque Anik","Ariful Azad"],"url":"https://arxiv.org/abs/2502.16949"}
{"created":"2025-05-01","title":"Learning Code-Edit Embedding to Model Student Debugging Behavior","abstract":"Providing effective feedback for programming assignments in computer science education can be challenging: students solve problems by iteratively submitting code, executing it, and using limited feedback from the compiler or the auto-grader to debug. Analyzing student debugging behavior in this process may reveal important insights into their knowledge and inform better personalized support tools. In this work, we propose an encoder-decoder-based model that learns meaningful code-edit embeddings between consecutive student code submissions, to capture their debugging behavior. Our model leverages information on whether a student code submission passes each test case to fine-tune large language models (LLMs) to learn code editing representations. It enables personalized next-step code suggestions that maintain the student's coding style while improving test case correctness. Our model also enables us to analyze student code-editing patterns to uncover common student errors and debugging behaviors, using clustering techniques. Experimental results on a real-world student code submission dataset demonstrate that our model excels at code reconstruction and personalized code suggestion while revealing interesting patterns in student debugging behavior.","authors":["Hasnain Heickal","Andrew Lan"],"url":"https://arxiv.org/abs/2502.19407"}
{"created":"2025-05-01","title":"LLM-driven Effective Knowledge Tracing by Integrating Dual-channel Difficulty","abstract":"Knowledge Tracing (KT) is a fundamental technology in intelligent tutoring systems used to simulate changes in students' knowledge state during learning, track personalized knowledge mastery, and predict performance. However, current KT models face three major challenges: (1) When encountering new questions, models face cold-start problems due to sparse interaction records, making precise modeling difficult; (2) Traditional models only use historical interaction records for student personalization modeling, unable to accurately track individual mastery levels, resulting in unclear personalized modeling; (3) The decision-making process is opaque to educators, making it challenging for them to understand model judgments. To address these challenges, we propose a novel Dual-channel Difficulty-aware Knowledge Tracing (DDKT) framework that utilizes Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) for subjective difficulty assessment, while integrating difficulty bias-aware algorithms and student mastery algorithms for precise difficulty measurement. Our framework introduces three key innovations: (1) Difficulty Balance Perception Sequence (DBPS) - students' subjective perceptions combined with objective difficulty, measuring gaps between LLM-assessed difficulty, mathematical-statistical difficulty, and students' subjective perceived difficulty through attention mechanisms; (2) Difficulty Mastery Ratio (DMR) - precise modeling of student mastery levels through different difficulty zones; (3) Knowledge State Update Mechanism - implementing personalized knowledge acquisition through gated networks and updating student knowledge state. Experimental results on two real datasets show our method consistently outperforms nine baseline models, improving AUC metrics by 2% to 10% while effectively addressing cold-start problems and enhancing model interpretability.","authors":["Jiahui Cen","Jianghao Lin","Weixuan Zhong","Dong Zhou","Jin Chen","Aimin Yang","Yongmei Zhou"],"url":"https://arxiv.org/abs/2502.19915"}
{"created":"2025-05-01","title":"Visual Adaptive Prompting for Compositional Zero-Shot Learning","abstract":"Vision-Language Models (VLMs) have demonstrated impressive capabilities in learning joint representations of visual and textual data, making them powerful tools for tasks such as Compositional Zero-Shot Learning (CZSL). CZSL requires models to generalize to novel combinations of visual primitives-such as attributes and objects-that were not explicitly encountered during training. Recent works in prompting for CZSL have focused on modifying inputs for the text encoder, often using static prompts that do not change across varying visual contexts. However, these approaches struggle to fully capture varying visual contexts, as they focus on text adaptation rather than leveraging visual features for compositional reasoning. To address this, we propose Visual Adaptive Prompting System (VAPS) that leverages a learnable visual prompt repository and similarity-based retrieval mechanism within the framework of VLMs to bridge the gap between semantic and visual features. Our method introduces a dynamic visual prompt repository mechanism that selects the most relevant attribute and object prompts based on the visual features of the image. Our proposed system includes a visual prompt adapter that encourages the model to learn a more generalizable embedding space. Experiments on three CZSL benchmarks, across both closed and open-world scenarios, demonstrate state-of-the-art results.","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia","Eman El-Sheikh"],"url":"https://arxiv.org/abs/2502.20292"}
{"created":"2025-05-01","title":"Predicting clinical outcomes from patient care pathways represented with temporal knowledge graphs","abstract":"Background: With the increasing availability of healthcare data, predictive modeling finds many applications in the biomedical domain, such as the evaluation of the level of risk for various conditions, which in turn can guide clinical decision making. However, it is unclear how knowledge graph data representations and their embedding, which are competitive in some settings, could be of interest in biomedical predictive modeling. Method: We simulated synthetic but realistic data of patients with intracranial aneurysm and experimented on the task of predicting their clinical outcome. We compared the performance of various classification approaches on tabular data versus a graph-based representation of the same data. Next, we investigated how the adopted schema for representing first individual data and second temporal data impacts predictive performances. Results: Our study illustrates that in our case, a graph representation and Graph Convolutional Network (GCN) embeddings reach the best performance for a predictive task from observational data. We emphasize the importance of the adopted schema and of the consideration of literal values in the representation of individual data. Our study also moderates the relative impact of various time encoding on GCN performance.","authors":["Jong Ho Jhee","Alberto Megina","Pac\\^ome Constant Dit Beaufils","Matilde Karakachoff","Richard Redon","Alban Gaignard","Adrien Coulet"],"url":"https://arxiv.org/abs/2502.21138"}
{"created":"2025-05-01","title":"Gaussian process surrogate model to approximate power grid simulators -- An application to the certification of a congestion management controller","abstract":"With the digitalization of power grids, physical equations become insufficient to describe the network's behavior, and realistic but time-consuming simulators must be used. Numerical experiments, such as safety validation, that involve simulating a large number of scenarios become computationally intractable. A popular solution to reduce the computational burden is to learn a surrogate model of the simulator with Machine Learning (ML) and then conduct the experiment directly on the fast-to-evaluate surrogate model. Among the various ML possibilities for building surrogate models, Gaussian processes (GPs) emerged as a popular solution due to their flexibility, data efficiency, and interpretability. Their probabilistic nature enables them to provide both predictions and uncertainty quantification (UQ). This paper starts with a discussion on the interest of using GPs to approximate power grid simulators and fasten numerical experiments. Such simulators, however, often violate the GP's underlying Gaussian assumption, leading to poor approximations. To address this limitation, an approach that consists in adding an adaptive residual uncertainty term to the UQ is proposed. It enables the GP to remain accurate and reliable despite the simulator's non-Gaussian behaviors. This approach is successfully applied to the certification of the proper functioning of a congestion management controller, with over 98% of simulations avoided.","authors":["Pierre Houdouin","Lucas Saludjian"],"url":"https://arxiv.org/abs/2503.00094"}
{"created":"2025-05-01","title":"AC-Lite : A Lightweight Image Captioning Model for Low-Resource Assamese Language","abstract":"Most existing works in image caption synthesis use computation heavy deep neural networks and generates image descriptions in English language. This often restricts this important assistive tool for widespread use across language and accessibility barriers. This work presents AC-Lite, a computationally efficient model for image captioning in low-resource Assamese language. AC-Lite reduces computational requirements by replacing computation-heavy deep network components with lightweight alternatives. The AC-Lite model is designed through extensive ablation experiments with different image feature extractor networks and language decoders. A combination of ShuffleNetv2x1.5 with GRU based language decoder along with bilinear attention is found to provide the best performance with minimum compute. AC-Lite was observed to achieve an 82.3 CIDEr score on the COCO-AC dataset with 2.45 GFLOPs and 22.87M parameters.","authors":["Pankaj Choudhury","Yogesh Aggarwal","Prabhanjan Jadhav","Prithwijit Guha","Sukumar Nandi"],"url":"https://arxiv.org/abs/2503.01453"}
{"created":"2025-05-01","title":"SAGE: A Framework of Precise Retrieval for RAG","abstract":"Retrieval-augmented generation (RAG) has demonstrated significant proficiency in conducting question-answering (QA) tasks within a specified corpus. Nonetheless, numerous failure instances of RAG in QA still exist. These failures are not solely attributable to the limitations of Large Language Models (LLMs); instead, they predominantly arise from the retrieval of inaccurate information for LLMs due to two limitations: (1) Current RAG methods segment the corpus without considering semantics, making it difficult to find relevant context due to impaired correlation between questions and the segments. (2) There is a trade-off between missing essential context with fewer context retrieved and getting irrelevant context with more context retrieved.","authors":["Jintao Zhang","Guoliang Li","Jinyang Su"],"url":"https://arxiv.org/abs/2503.01713"}
{"created":"2025-05-01","title":"Vision Transformers on the Edge: A Comprehensive Survey of Model Compression and Acceleration Strategies","abstract":"In recent years, vision transformers (ViTs) have emerged as powerful and promising techniques for computer vision tasks such as image classification, object detection, and segmentation. Unlike convolutional neural networks (CNNs), which rely on hierarchical feature extraction, ViTs treat images as sequences of patches and leverage self-attention mechanisms. However, their high computational complexity and memory demands pose significant challenges for deployment on resource-constrained edge devices. To address these limitations, extensive research has focused on model compression techniques and hardware-aware acceleration strategies. Nonetheless, a comprehensive review that systematically categorizes these techniques and their trade-offs in accuracy, efficiency, and hardware adaptability for edge deployment remains lacking. This survey bridges this gap by providing a structured analysis of model compression techniques, software tools for inference on edge, and hardware acceleration strategies for ViTs. We discuss their impact on accuracy, efficiency, and hardware adaptability, highlighting key challenges and emerging research directions to advance ViT deployment on edge platforms, including graphics processing units (GPUs), application-specific integrated circuit (ASICs), and field-programmable gate arrays (FPGAs). The goal is to inspire further research with a contemporary guide on optimizing ViTs for efficient deployment on edge devices.","authors":["Shaibal Saha","Lanyu Xu"],"url":"https://arxiv.org/abs/2503.02891"}
{"created":"2025-05-01","title":"TrafficKAN-GCN: Graph Convolutional-based Kolmogorov-Arnold Network for Traffic Flow Optimization","abstract":"Urban traffic optimization is critical for improving transportation efficiency and alleviating congestion, particularly in large-scale dynamic networks. Traditional methods, such as Dijkstra's and Floyd's algorithms, provide effective solutions in static settings, but they struggle with the spatial-temporal complexity of real-world traffic flows. In this work, we propose TrafficKAN-GCN, a hybrid deep learning framework combining Kolmogorov-Arnold Networks (KAN) with Graph Convolutional Networks (GCN), designed to enhance urban traffic flow optimization. By integrating KAN's adaptive nonlinear function approximation with GCN's spatial graph learning capabilities, TrafficKAN-GCN captures both complex traffic patterns and topological dependencies. We evaluate the proposed framework using real-world traffic data from the Baltimore Metropolitan area. Compared with baseline models such as MLP-GCN, standard GCN, and Transformer-based approaches, TrafficKAN-GCN achieves competitive prediction accuracy while demonstrating improved robustness in handling noisy and irregular traffic data. Our experiments further highlight the framework's ability to redistribute traffic flow, mitigate congestion, and adapt to disruptive events, such as the Francis Scott Key Bridge collapse. This study contributes to the growing body of work on hybrid graph learning for intelligent transportation systems, highlighting the potential of combining KAN and GCN for real-time traffic optimization. Future work will focus on reducing computational overhead and integrating Transformer-based temporal modeling for enhanced long-term traffic prediction. The proposed TrafficKAN-GCN framework offers a promising direction for data-driven urban mobility management, balancing predictive accuracy, robustness, and computational efficiency.","authors":["Jiayi Zhang","Yiming Zhang","Yuan Zheng","Yuchen Wang","Jinjiang You","Yuchen Xu","Wenxing Jiang","Soumyabrata Dev"],"url":"https://arxiv.org/abs/2503.03276"}
{"created":"2025-05-01","title":"Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice","abstract":"The rapid proliferation of Large Language Models (LLMs) has raised pressing concerns regarding their trustworthiness, spanning issues of reliability, transparency, fairness, and ethical alignment. Despite the increasing adoption of LLMs across various domains, there remains a lack of consensus on how to operationalize trustworthiness in practice. This study bridges the gap between theoretical discussions and implementation by conducting a bibliometric mapping analysis of 2,006 publications from 2019 to 2025. Through co-authorship networks, keyword co-occurrence analysis, and thematic evolution tracking, we identify key research trends, influential authors, and prevailing definitions of LLM trustworthiness. Additionally, a systematic review of 68 core papers is conducted to examine conceptualizations of trust and their practical implications. Our findings reveal that trustworthiness in LLMs is often framed through existing organizational trust frameworks, emphasizing dimensions such as ability, benevolence, and integrity. However, a significant gap exists in translating these principles into concrete development strategies. To address this, we propose a structured mapping of 20 trust-enhancing techniques across the LLM lifecycle, including retrieval-augmented generation (RAG), explainability techniques, and post-training audits. By synthesizing bibliometric insights with practical strategies, this study contributes towards fostering more transparent, accountable, and ethically aligned LLMs, ensuring their responsible deployment in real-world applications.","authors":["Jos\\'e Siqueira de Cerqueira","Kai-Kristian Kemell","Muhammad Waseem","Rebekah Rousi","Nannan Xi","Juho Hamari","Pekka Abrahamsson"],"url":"https://arxiv.org/abs/2503.04785"}
{"created":"2025-05-01","title":"AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems","abstract":"We explore how scalable robot data can address real-world challenges for generalized robotic manipulation. Introducing AgiBot World, a large-scale platform comprising over 1 million trajectories across 217 tasks in five deployment scenarios, we achieve an order-of-magnitude increase in data scale compared to existing datasets. Accelerated by a standardized collection pipeline with human-in-the-loop verification, AgiBot World guarantees high-quality and diverse data distribution. It is extensible from grippers to dexterous hands and visuo-tactile sensors for fine-grained skill acquisition. Building on top of data, we introduce Genie Operator-1 (GO-1), a novel generalist policy that leverages latent action representations to maximize data utilization, demonstrating predictable performance scaling with increased data volume. Policies pre-trained on our dataset achieve an average performance improvement of 30% over those trained on Open X-Embodiment, both in in-domain and out-of-distribution scenarios. GO-1 exhibits exceptional capability in real-world dexterous and long-horizon tasks, achieving over 60% success rate on complex tasks and outperforming prior RDT approach by 32%. By open-sourcing the dataset, tools, and models, we aim to democratize access to large-scale, high-quality robot data, advancing the pursuit of scalable and general-purpose intelligence.","authors":["AgiBot-World-Contributors","Qingwen Bu","Jisong Cai","Li Chen","Xiuqi Cui","Yan Ding","Siyuan Feng","Shenyuan Gao","Xindong He","Xuan Hu","Xu Huang","Shu Jiang","Yuxin Jiang","Cheng Jing","Hongyang Li","Jialu Li","Chiming Liu","Yi Liu","Yuxiang Lu","Jianlan Luo","Ping Luo","Yao Mu","Yuehan Niu","Yixuan Pan","Jiangmiao Pang","Yu Qiao","Guanghui Ren","Cheng Ruan","Jiaqi Shan","Yongjian Shen","Chengshi Shi","Mingkang Shi","Modi Shi","Chonghao Sima","Jianheng Song","Huijie Wang","Wenhao Wang","Dafeng Wei","Chengen Xie","Guo Xu","Junchi Yan","Cunbiao Yang","Lei Yang","Shukai Yang","Maoqing Yao","Jia Zeng","Chi Zhang","Qinglin Zhang","Bin Zhao","Chengyue Zhao","Jiaqi Zhao","Jianchao Zhu"],"url":"https://arxiv.org/abs/2503.06669"}
{"created":"2025-05-01","title":"ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation","abstract":"Realistic Hand manipulation is a key component of immersive virtual reality (VR), yet existing methods often rely on kinematic approach or motion-capture datasets that omit crucial physical attributes such as contact forces and finger torques. Consequently, these approaches prioritize tight, one-size-fits-all grips rather than reflecting users' intended force levels. We present ForceGrip, a deep learning agent that synthesizes realistic hand manipulation motions, faithfully reflecting the user's grip force intention. Instead of mimicking predefined motion datasets, ForceGrip uses generated training scenarios-randomizing object shapes, wrist movements, and trigger input flows-to challenge the agent with a broad spectrum of physical interactions. To effectively learn from these complex tasks, we employ a three-phase curriculum learning framework comprising Finger Positioning, Intention Adaptation, and Dynamic Stabilization. This progressive strategy ensures stable hand-object contact, adaptive force control based on user inputs, and robust handling under dynamic conditions. Additionally, a proximity reward function enhances natural finger motions and accelerates training convergence. Quantitative and qualitative evaluations reveal ForceGrip's superior force controllability and plausibility compared to state-of-the-art methods. Demo videos are available as supplementary material and the code is provided at https://han-dongheun.github.io/ForceGrip.","authors":["DongHeun Han","Byungmin Kim","RoUn Lee","KyeongMin Kim","Hyoseok Hwang","HyeongYeop Kang"],"url":"https://arxiv.org/abs/2503.08061"}
{"created":"2025-05-01","title":"Shaved Ice: Optimal Compute Resource Commitments for Dynamic Multi-Cloud Workloads","abstract":"Cloud providers have introduced pricing models to incentivize long-term commitments of compute capacity. These long-term commitments allow the cloud providers to get guaranteed revenue for their investments in data centers and computing infrastructure. However, these commitments expose cloud customers to demand risk if expected future demand does not materialize. While there are existing studies of theoretical techniques for optimizing performance, latency, and cost, relatively little has been reported so far on the trade-offs between cost savings and demand risk for compute commitments for large-scale cloud services.","authors":["Murray Stokely","Neel Nadgir","Jack Peele","Orestis Kostakis"],"url":"https://arxiv.org/abs/2503.10235"}
{"created":"2025-05-01","title":"Leveraging Motion Information for Better Self-Supervised Video Correspondence Learning","abstract":"Self-supervised video correspondence learning depends on the ability to accurately associate pixels between video frames that correspond to the same visual object. However, achieving reliable pixel matching without supervision remains a major challenge. To address this issue, recent research has focused on feature learning techniques that aim to encode unique pixel representations for matching. Despite these advances, existing methods still struggle to achieve exact pixel correspondences and often suffer from false matches, limiting their effectiveness in self-supervised settings.","authors":["Zihan Zhou","Changrui Dai","Aibo Song","Xiaolin Fang"],"url":"https://arxiv.org/abs/2503.12026"}
{"created":"2025-05-01","title":"WideRange4D: Enabling High-Quality 4D Reconstruction with Wide-Range Movements and Scenes","abstract":"With the rapid development of 3D reconstruction technology, research in 4D reconstruction is also advancing, existing 4D reconstruction methods can generate high-quality 4D scenes. However, due to the challenges in acquiring multi-view video data, the current 4D reconstruction benchmarks mainly display actions performed in place, such as dancing, within limited scenarios. In practical scenarios, many scenes involve wide-range spatial movements, highlighting the limitations of existing 4D reconstruction datasets. Additionally, existing 4D reconstruction methods rely on deformation fields to estimate the dynamics of 3D objects, but deformation fields struggle with wide-range spatial movements, which limits the ability to achieve high-quality 4D scene reconstruction with wide-range spatial movements. In this paper, we focus on 4D scene reconstruction with significant object spatial movements and propose a novel 4D reconstruction benchmark, WideRange4D. This benchmark includes rich 4D scene data with large spatial variations, allowing for a more comprehensive evaluation of the generation capabilities of 4D generation methods. Furthermore, we introduce a new 4D reconstruction method, Progress4D, which generates stable and high-quality 4D results across various complex 4D scene reconstruction tasks. We conduct both quantitative and qualitative comparison experiments on WideRange4D, showing that our Progress4D outperforms existing state-of-the-art 4D reconstruction methods. Project: https://github.com/Gen-Verse/WideRange4D","authors":["Ling Yang","Kaixin Zhu","Juanxi Tian","Bohan Zeng","Mingbao Lin","Hongjuan Pei","Wentao Zhang","Shuicheng Yan"],"url":"https://arxiv.org/abs/2503.13435"}
{"created":"2025-05-01","title":"JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System","abstract":"This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: https://github.com/oneal2000/JuDGE.","authors":["Weihang Su","Baoqing Yue","Qingyao Ai","Yiran Hu","Jiaqi Li","Changyue Wang","Kaiyuan Zhang","Yueyue Wu","Yiqun Liu"],"url":"https://arxiv.org/abs/2503.14258"}
{"created":"2025-05-01","title":"A 3.3904-Competitive Online Algorithm for List Update with Uniform Costs","abstract":"We consider the List Update problem where the cost of each swap is assumed to be 1. This is in contrast to the ``standard'' model, in which an algorithm is allowed to swap the requested item with previous items for free. We construct an online algorithm Full-Or-Partial-Move (FPM), whose competitive ratio is at most $3.3904$, improving over the previous best known bound of $4$.","authors":["Mateusz Basiak","Marcin Bienkowski","Martin B\\\"ohm","Marek Chrobak","{\\L}ukasz Je\\.z","Ji\\v{r}\\'i Sgall","Agnieszka Tatarczuk"],"url":"https://arxiv.org/abs/2503.17264"}
{"created":"2025-05-01","title":"Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding","abstract":"Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation ($R^2$ scores up to $0.91$) and morphology classification tasks (up to $+0.17$ F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs.","authors":["Tianyu Chen","Xingcheng Fu","Yisen Gao","Haodong Qian","Yuecen Wei","Kun Yan","Haoyi Zhou","Jianxin Li"],"url":"https://arxiv.org/abs/2503.18578"}
{"created":"2025-05-01","title":"BiPrompt-SAM: Enhancing Image Segmentation via Explicit Selection between Point and Text Prompts","abstract":"Segmentation is a fundamental task in computer vision, with prompt-driven methods gaining prominence due to their flexibility. The Segment Anything Model (SAM) excels at point-prompted segmentation, while text-based models, often leveraging powerful multimodal encoders like BEIT-3, provide rich semantic understanding. However, effectively combining these complementary modalities remains a challenge. This paper introduces BiPrompt-SAM, a novel dual-modal prompt segmentation framework employing an explicit selection mechanism. We leverage SAM's ability to generate multiple mask candidates from a single point prompt and use a text-guided mask (generated via EVF-SAM with BEIT-3) to select the point-generated mask that best aligns spatially, measured by Intersection over Union (IoU). This approach, interpretable as a simplified Mixture of Experts (MoE), effectively fuses spatial precision and semantic context without complex model modifications. Notably, our method achieves strong zero-shot performance on the Endovis17 medical dataset (89.55% mDice, 81.46% mIoU) using only a single point prompt per instance. This significantly reduces annotation burden compared to bounding boxes and aligns better with practical clinical workflows, demonstrating the method's effectiveness without domain-specific training. On the RefCOCO series, BiPrompt-SAM attained 87.1%, 86.5%, and 85.8% IoU, significantly outperforming existing approaches. Experiments show BiPrompt-SAM excels in scenarios requiring both spatial accuracy and semantic disambiguation, offering a simple, effective, and interpretable perspective on multi-modal prompt fusion.","authors":["Suzhe Xu","Jialin Peng","Chengyuan Zhang"],"url":"https://arxiv.org/abs/2503.19769"}
{"created":"2025-05-01","title":"Leveraging VAE-Derived Latent Spaces for Enhanced Malware Detection with Machine Learning Classifiers","abstract":"This paper assesses the performance of five machine learning classifiers: Decision Tree, Naive Bayes, LightGBM, Logistic Regression, and Random Forest using latent representations learned by a Variational Autoencoder from malware datasets. Results from the experiments conducted on different training-test splits with different random seeds reveal that all the models perform well in detecting malware with ensemble methods (LightGBM and Random Forest) performing slightly better than the rest. In addition, the use of latent features reduces the computational cost of the model and the need for extensive hyperparameter tuning for improved efficiency of the model for deployment. Statistical tests show that these improvements are significant, and thus, the practical relevance of integrating latent space representation with traditional classifiers for effective malware detection in cybersecurity is established.","authors":["Bamidele Ajayi","Basel Barakat","Ken McGarry"],"url":"https://arxiv.org/abs/2503.20803"}
{"created":"2025-05-01","title":"Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad","abstract":"Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.","authors":["Ivo Petrov","Jasper Dekoninck","Lyuben Baltadzhiev","Maria Drencheva","Kristian Minchev","Mislav Balunovi\\'c","Nikola Jovanovi\\'c","Martin Vechev"],"url":"https://arxiv.org/abs/2503.21934"}
{"created":"2025-05-01","title":"GISE-TTT:A Framework for Global InformationSegmentation and Enhancement","abstract":"This paper addresses the challenge of capturing global temporaldependencies in long video sequences for Video Object Segmentation (VOS). Existing architectures often fail to effectively model these dependencies acrossextended temporal horizons. To overcome this limitation, we introduce GISE-TTT, anovel architecture that integrates Temporal Transformer (TTT) layers intotransformer-based frameworks through a co-designed hierarchical approach.The TTTlayer systematically condenses historical temporal information into hidden states thatencode globally coherent contextual representations. By leveraging multi-stagecontextual aggregation through hierarchical concatenation, our frameworkprogressively refines spatiotemporal dependencies across network layers. This designrepresents the first systematic empirical evidence that distributing global informationacross multiple network layers is critical for optimal dependency utilization in videosegmentation tasks.Ablation studies demonstrate that incorporating TTT modules athigh-level feature stages significantly enhances global modeling capabilities, therebyimproving the network's ability to capture long-range temporal relationships. Extensive experiments on DAVIS 2017 show that GISE-TTT achieves a 3.2%improvement in segmentation accuracy over the baseline model, providingcomprehensive evidence that global information should be strategically leveragedthroughout the network architecture.The code will be made available at:https://github.com/uuool/GISE-TTT.","authors":["Fenglei Hao","Yuliang Yang","Ruiyuan Su","Zhengran Zhao","Yukun Qiao","Mengyu Zhu"],"url":"https://arxiv.org/abs/2504.00879"}
{"created":"2025-05-01","title":"Urban Computing in the Era of Large Language Models","abstract":"Urban computing has emerged as a multidisciplinary field that harnesses data-driven technologies to address challenges and improve urban living. Traditional approaches, while beneficial, often face challenges with generalization, scalability, and contextual understanding. The advent of Large Language Models (LLMs) offers transformative potential in this domain. This survey explores the intersection of LLMs and urban computing, emphasizing the impact of LLMs in processing and analyzing urban data, enhancing decision-making, and fostering citizen engagement. We provide a concise overview of the evolution and core technologies of LLMs. Additionally, we survey their applications across key urban domains, such as transportation, public safety, and environmental monitoring, summarizing essential tasks and prior works in various urban contexts, while highlighting LLMs' functional roles and implementation patterns. Building on this, we propose potential LLM-based solutions to address unresolved challenges. To facilitate in-depth research, we compile a list of available datasets and tools applicable to diverse urban scenarios. Finally, we discuss the limitations of current approaches and outline future directions for advancing LLMs in urban computing.","authors":["Zhonghang Li","Lianghao Xia","Xubin Ren","Jiabin Tang","Tianyi Chen","Yong Xu","Chao Huang"],"url":"https://arxiv.org/abs/2504.02009"}
{"created":"2025-05-01","title":"Intrinsic Verification of Parsers and Formal Grammar Theory in Dependent Lambek Calculus (Extended Version)","abstract":"We present Dependent Lambek Calculus, a domain-specific dependent type theory for verified parsing and formal grammar theory. In $\\textrm{Lambek}^D$, linear types are used as a syntax for formal grammars,and parsers can be written as linear terms. The linear typing restriction provides a form of intrinsic verification that a parser yields only valid parse trees for the input string. We demonstrate the expressivity of this system by showing that the combination of inductive linear types and dependency on non-linear data can be used to encode commonly used grammar formalisms such as regular and context-free grammars as well as traces of various types of automata. Using these encodings, we define parsers for regular expressions using deterministic automata, as well as examples of verified parsers of context-free grammars.","authors":["Steven Schaefer","Nathan Varner","Pedro H. Azevedo de Amorim","Max S. New"],"url":"https://arxiv.org/abs/2504.03995"}
{"created":"2025-05-01","title":"MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender","abstract":"Large language models (LLMs), known for their comprehension capabilities and extensive knowledge, have been increasingly applied to recommendation systems (RS). Given the fundamental gap between the mechanism of LLMs and the requirement of RS, researchers have focused on fine-tuning LLMs with recommendation-specific data to enhance their performance. Language Modeling Loss (LML), originally designed for language generation tasks, is commonly adopted. However, we identify two critical limitations of LML: 1) it exhibits significant divergence from the recommendation objective; 2) it erroneously treats all fictitious item descriptions as negative samples, introducing misleading training signals.","authors":["Bohao Wang","Feng Liu","Jiawei Chen","Xingyu Lou","Changwang Zhang","Jun Wang","Yuegang Sun","Yan Feng","Chun Chen","Can Wang"],"url":"https://arxiv.org/abs/2504.04178"}
{"created":"2025-05-01","title":"Data Augmentation as Free Lunch: Exploring the Test-Time Augmentation for Sequential Recommendation","abstract":"Data augmentation has become a promising method of mitigating data sparsity in sequential recommendation. Existing methods generate new yet effective data during model training to improve performance. However, deploying them requires retraining, architecture modification, or introducing additional learnable parameters. The above steps are time-consuming and costly for well-trained models, especially when the model scale becomes large. In this work, we explore the test-time augmentation (TTA) for sequential recommendation, which augments the inputs during the model inference and then aggregates the model's predictions for augmented data to improve final accuracy. It avoids significant time and cost overhead from loss calculation and backward propagation. We first experimentally disclose the potential of existing augmentation operators for TTA and find that the Mask and Substitute consistently achieve better performance. Further analysis reveals that these two operators are effective because they retain the original sequential pattern while adding appropriate perturbations. Meanwhile, we argue that these two operators still face time-consuming item selection or interference information from mask tokens. Based on the analysis and limitations, we present TNoise and TMask. The former injects uniform noise into the original representation, avoiding the computational overhead of item selection. The latter blocks mask token from participating in model calculations or directly removes interactions that should have been replaced with mask tokens. Comprehensive experiments demonstrate the effectiveness, efficiency, and generalizability of our method. We provide an anonymous implementation at https://github.com/KingGugu/TTA4SR.","authors":["Yizhou Dang","Yuting Liu","Enneng Yang","Minhan Huang","Guibing Guo","Jianzhe Zhao","Xingwei Wang"],"url":"https://arxiv.org/abs/2504.04843"}
{"created":"2025-05-01","title":"Adversarial KA","abstract":"Regarding the representation theorem of Kolmogorov and Arnold (KA) as an algorithm for representing or {\\guillemotleft}expressing{\\guillemotright} functions, we test its robustness by analyzing its ability to withstand adversarial attacks. We find KA to be robust to countable collections of continuous adversaries, but unearth a question about the equi-continuity of the outer functions that, so far, obstructs taking limits and defeating continuous groups of adversaries. This question on the regularity of the outer functions is relevant to the debate over the applicability of KA to the general theory of NNs.","authors":["Sviatoslav Dzhenzher","Michael H. Freedman"],"url":"https://arxiv.org/abs/2504.05255"}
{"created":"2025-05-01","title":"SPARK-Remote: A Cost-Effective System for Remote Bimanual Robot Teleoperation","abstract":"Robot teleoperation enables human control over robotic systems in environments where full autonomy is challenging. Recent advancements in low-cost teleoperation devices and VR/AR technologies have expanded accessibility, particularly for bimanual robot manipulators. However, transitioning from in-person to remote teleoperation presents challenges in task performance. We introduce SPARK, a kinematically scaled, low-cost teleoperation system for operating bimanual robots. Its effectiveness is compared to existing technologies like the 3D SpaceMouse and VR/AR controllers. We further extend SPARK to SPARK-Remote, integrating sensor-based force feedback using haptic gloves and a force controller for remote teleoperation. We evaluate SPARK and SPARK-Remote variants on 5 bimanual manipulation tasks which feature operational properties - positional precision, rotational precision, large movements in the workspace, and bimanual collaboration - to test the effective teleoperation modes. Our findings offer insights into improving low-cost teleoperation interfaces for real-world applications. For supplementary materials, additional experiments, and qualitative results, visit the project webpage: https://bit.ly/41EfcJa","authors":["Adam Imdieke","Karthik Desingh"],"url":"https://arxiv.org/abs/2504.05488"}
{"created":"2025-05-01","title":"Trust-Region Twisted Policy Improvement","abstract":"Monte-Carlo tree search (MCTS) has driven many recent breakthroughs in deep reinforcement learning (RL). However, scaling MCTS to parallel compute has proven challenging in practice which has motivated alternative planners like sequential Monte-Carlo (SMC). Many of these SMC methods adopt particle filters for smoothing through a reformulation of RL as a policy inference problem. Yet, persisting design choices of these particle filters often conflict with the aim of online planning in RL, which is to obtain a policy improvement at the start of planning. Drawing inspiration from MCTS, we tailor SMC planners specifically for RL by improving data generation within the planner through constrained action sampling and explicit terminal state handling, as well as improving policy and value target estimation. This leads to our Trust-Region Twisted SMC (TRT-SMC), which shows improved runtime and sample-efficiency over baseline MCTS and SMC methods in both discrete and continuous domains.","authors":["Joery A. de Vries","Jinke He","Yaniv Oren","Matthijs T. J. Spaan"],"url":"https://arxiv.org/abs/2504.06048"}
{"created":"2025-05-01","title":"Knowledge Graph Completion with Relation-Aware Anchor Enhancement","abstract":"Text-based knowledge graph completion methods take advantage of pre-trained language models (PLM) to enhance intrinsic semantic connections of raw triplets with detailed text descriptions. Typical methods in this branch map an input query (textual descriptions associated with an entity and a relation) and its candidate entities into feature vectors, respectively, and then maximize the probability of valid triples. These methods are gaining promising performance and increasing attention for the rapid development of large language models. According to the property of the language models, the more related and specific context information the input query provides, the more discriminative the resultant embedding will be. In this paper, through observation and validation, we find a neglected fact that the relation-aware neighbors of the head entities in queries could act as effective contexts for more precise link prediction. Driven by this finding, we propose a relation-aware anchor enhanced knowledge graph completion method (RAA-KGC). Specifically, in our method, to provide a reference of what might the target entity be like, we first generate anchor entities within the relation-aware neighborhood of the head entity. Then, by pulling the query embedding towards the neighborhoods of the anchors, it is tuned to be more discriminative for target entity matching. The results of our extensive experiments not only validate the efficacy of RAA-KGC but also reveal that by integrating our relation-aware anchor enhancement strategy, the performance of current leading methods can be notably enhanced without substantial modifications.","authors":["Duanyang Yuan","Sihang Zhou","Xiaoshu Chen","Dong Wang","Ke Liang","Xinwang Liu","Jian Huang"],"url":"https://arxiv.org/abs/2504.06129"}
{"created":"2025-05-01","title":"A Review of HPC-Accelerated CFD in National Security and Defense","abstract":"Using High-Performance Computing (HPC), Computational Fluid Dynamics (CFD) now serves as an essential component in defense-related national security applications including missile interception and hypersonic propulsion as well as naval stealth optimization and urban hazard dispersion. This review combines two decades of open-source and public-domain research on HPC-accelerated CFD in defense, addressing three key questions: Which security-sensitive simulations have utilized open-source CFD frameworks such as OpenFOAM, SU2 and ADflow? Which HPC techniques, such as MPI domain decomposition and GPU acceleration together with hybrid parallelism best enhance open-source frameworks to manage large defense CFD simulations? Which technological advancements and research voids currently drive the directional development of the field? Examining several research studies sourced from NASA, DoD HPC centers, and academic institutions, scientific contributions have been classified into air, maritime, and space domains. Modular frameworks like NavyFOAM and SU2 and ADflow's adjoint-based solvers show how custom open-source solutions support workflows with rapid completion of multi-million cell simulations. The conclusion highlights new trends that combine exascale readiness with machine learning surrogate models for real-time CFD applications and interdisciplinary HPC-driven multi-physics integration to deliver practical insights for improving CFD use in defense research and development.","authors":["James Afful"],"url":"https://arxiv.org/abs/2504.07837"}
{"created":"2025-05-01","title":"Nondeterminism makes unary 1-limited automata concise","abstract":"We investigate the descriptional complexity of different variants of 1-limited automata (1-las), an extension of two-way finite automata (2nfas) characterizing regular languages. In particular, we consider 2nfas with common-guess (2nfas+cg), which are 2nfas equipped with a new kind of nondeterminism that allows the device to initially annotate each input symbol, before performing a read-only computation over the resulting annotated word. Their deterministic counterparts, namely two-way deterministic finite automata with common-guess (2dfas+cg), still have a nondeterministic annotation phase and can be considered as a restriction of 1-las. We prove exponential lower bounds for the simulations of 2dfas+cg (and thus of 1-las) by deterministic 1-las and by 2nfas. These results are derived from a doubly exponential lower bound for the simulation of 2dfas+cg by one-way deterministic finite automata (1dfas). Our lower bounds are witnessed by unary languages, namely languages defined over a singleton alphabet. As a consequence, we close a question left open in [Pighizzini and Prigioniero. Limited automata and unary languages. Inf. Comput., 266:60-74], about the existence of a double exponential gap between 1-las and 1dfas in the unary case. Lastly, we prove an exponential lower bound for complementing unary 2dfas+cg (and thus unary 1-las).","authors":["Bruno Guillon (UCA","INP Clermont Auvergne","LIMOS)","Luca Prigioniero (UCA","INP Clermont Auvergne","LIMOS)","Javad Taheri (UCA","INP Clermont Auvergne","LIMOS)"],"url":"https://arxiv.org/abs/2504.08464"}
{"created":"2025-05-01","title":"EmoAgent: Assessing and Safeguarding Human-AI Interaction for Mental Health Safety","abstract":"The rise of LLM-driven AI characters raises safety concerns, particularly for vulnerable human users with psychological disorders. To address these risks, we propose EmoAgent, a multi-agent AI framework designed to evaluate and mitigate mental health hazards in human-AI interactions. EmoAgent comprises two components: EmoEval simulates virtual users, including those portraying mentally vulnerable individuals, to assess mental health changes before and after interactions with AI characters. It uses clinically proven psychological and psychiatric assessment tools (PHQ-9, PDI, PANSS) to evaluate mental risks induced by LLM. EmoGuard serves as an intermediary, monitoring users' mental status, predicting potential harm, and providing corrective feedback to mitigate risks. Experiments conducted in popular character-based chatbots show that emotionally engaging dialogues can lead to psychological deterioration in vulnerable users, with mental state deterioration in more than 34.4% of the simulations. EmoGuard significantly reduces these deterioration rates, underscoring its role in ensuring safer AI-human interactions. Our code is available at: https://github.com/1akaman/EmoAgent","authors":["Jiahao Qiu","Yinghui He","Xinzhe Juan","Yimin Wang","Yuhan Liu","Zixin Yao","Yue Wu","Xun Jiang","Ling Yang","Mengdi Wang"],"url":"https://arxiv.org/abs/2504.09689"}
{"created":"2025-05-01","title":"Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes","abstract":"Dish images play a crucial role in the digital era, with the demand for culturally distinctive dish images continuously increasing due to the digitization of the food industry and e-commerce. In general cases, existing text-to-image generation models excel in producing high-quality images; however, they struggle to capture diverse characteristics and faithful details of specific domains, particularly Chinese dishes. To address this limitation, we propose Omni-Dish, the first text-to-image generation model specifically tailored for Chinese dishes. We develop a comprehensive dish curation pipeline, building the largest dish dataset to date. Additionally, we introduce a recaption strategy and employ a coarse-to-fine training scheme to help the model better learn fine-grained culinary nuances. During inference, we enhance the user's textual input using a pre-constructed high-quality caption library and a large language model, enabling more photorealistic and faithful image generation. Furthermore, to extend our model's capability for dish editing tasks, we propose Concept-Enhanced P2P. Based on this approach, we build a dish editing dataset and train a specialized editing model. Extensive experiments demonstrate the superiority of our methods.","authors":["Huijie Liu","Bingcan Wang","Jie Hu","Xiaoming Wei","Guoliang Kang"],"url":"https://arxiv.org/abs/2504.09948"}
{"created":"2025-05-01","title":"VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge","abstract":"Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with \"thinking\" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.","authors":["Yueqi Song","Tianyue Ou","Yibo Kong","Zecheng Li","Graham Neubig","Xiang Yue"],"url":"https://arxiv.org/abs/2504.10342"}
{"created":"2025-05-01","title":"Weight Ensembling Improves Reasoning in Language Models","abstract":"We investigate a failure mode that arises during the training of reasoning models, where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, the Pass@1 rate reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we find that WiSE-FT provides complementary performance gains that cannot be achieved only through diversity-inducing decoding strategies, like temperature scaling. We formalize a bias-variance tradeoff of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling inherently trades off between bias and variance.","authors":["Xingyu Dang","Christina Baek","Kaiyue Wen","Zico Kolter","Aditi Raghunathan"],"url":"https://arxiv.org/abs/2504.10478"}
{"created":"2025-05-01","title":"PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems","abstract":"Generative retrieval methods utilize generative sequential modeling techniques, such as transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models like two-tower architectures. However, current generative retrieval methods lack the scalability required for industrial recommender systems, and they are insufficiently flexible to satisfy the multiple metric requirements of modern systems. This paper introduces PinRec, a novel generative retrieval model developed for applications at Pinterest. PinRec utilizes outcome-conditioned generation, enabling modelers to specify how to balance various outcome metrics, such as the number of saves and clicks, to effectively align with business goals and user exploration. Additionally, PinRec incorporates multi-token generation to enhance output diversity while optimizing generation. Our experiments demonstrate that PinRec can successfully balance performance, diversity, and efficiency, delivering a significant positive impact to users using generative models. This paper marks a significant milestone in generative retrieval, as it presents, to our knowledge, the first rigorous study on implementing generative retrieval at the scale of Pinterest.","authors":["Anirudhan Badrinath","Prabhat Agarwal","Laksh Bhasin","Jaewon Yang","Jiajing Xu","Charles Rosenberg"],"url":"https://arxiv.org/abs/2504.10507"}
{"created":"2025-05-01","title":"Demo: ViolentUTF as An Accessible Platform for Generative AI Red Teaming","abstract":"The rapid integration of Generative AI (GenAI) into various applications necessitates robust risk management strategies which includes Red Teaming (RT) - an evaluation method for simulating adversarial attacks. Unfortunately, RT for GenAI is often hindered by technical complexity, lack of user-friendly interfaces, and inadequate reporting features. This paper introduces Violent UTF - an accessible, modular, and scalable platform for GenAI red teaming. Through intuitive interfaces (Web GUI, CLI, API, MCP) powered by LLMs and for LLMs, Violent UTF aims to empower non-technical domain experts and students alongside technical experts, facilitate comprehensive security evaluation by unifying capabilities from RT frameworks like Microsoft PyRIT, Nvidia Garak and its own specialized evaluators. ViolentUTF is being used for evaluating the robustness of a flagship LLM-based product in a large US Government department. It also demonstrates effectiveness in evaluating LLMs' cross-domain reasoning capability between cybersecurity and behavioral psychology.","authors":["Tam n. Nguyen"],"url":"https://arxiv.org/abs/2504.10603"}
{"created":"2025-05-01","title":"Token Sliding Reconfiguration on DAGs","abstract":"Given a graph $G$ and two independent sets of same size, the Independent Set Reconfiguration Problem under token sliding ask whether one can, in a step by step manner, transform the first independent set into the second one. In each step we must preserve the condition of independence. Further, referring to solution vertices as tokens, we are only permitted to slide a token along an edge. Until the recent work of Ito et al. [Ito et al. MFCS 2022] this problem was only considered on undirected graphs. In this work, we study reconfiguration under token sliding focusing on DAGs.","authors":["Jona Dirks","Alexandre Vigny"],"url":"https://arxiv.org/abs/2504.10671"}
{"created":"2025-05-01","title":"Offset-free Nonlinear MPC with Koopman-based Surrogate Models","abstract":"In this paper, we design offset-free nonlinear Model Predictive Control (MPC) for surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The model used for prediction in MPC is augmented with a disturbance term, that is estimated by an observer. If the full information about the equilibrium of the real system is not available, a reference calculator is introduced in the algorithm to compute the MPC state and input references. The control algorithm guarantees offset-free tracking of the controlled output under the assumption that the modeling errors are asymptotically constant. The effectiveness of the proposed approach is showcased with numerical simulations for two popular benchmark systems: the van-der-Pol oscillator and the four-tanks process.","authors":["Irene Schimperna","Lea Bold","Karl Worthmann"],"url":"https://arxiv.org/abs/2504.10954"}
{"created":"2025-05-01","title":"GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*","abstract":"The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/","authors":["Eunsoo Im","Changhyun Jee","Jung Kwon Lee"],"url":"https://arxiv.org/abs/2504.11014"}
{"created":"2025-05-01","title":"Safety with Agency: Human-Centered Safety Filter with Application to AI-Assisted Motorsports","abstract":"We propose a human-centered safety filter (HCSF) for shared autonomy that significantly enhances system safety without compromising human agency. Our HCSF is built on a neural safety value function, which we first learn scalably through black-box interactions and then use at deployment to enforce a novel state-action control barrier function (Q-CBF) safety constraint. Since this Q-CBF safety filter does not require any knowledge of the system dynamics for both synthesis and runtime safety monitoring and intervention, our method applies readily to complex, black-box shared autonomy systems. Notably, our HCSF's CBF-based interventions modify the human's actions minimally and smoothly, avoiding the abrupt, last-moment corrections delivered by many conventional safety filters. We validate our approach in a comprehensive in-person user study using Assetto Corsa-a high-fidelity car racing simulator with black-box dynamics-to assess robustness in \"driving on the edge\" scenarios. We compare both trajectory data and drivers' perceptions of our HCSF assistance against unassisted driving and a conventional safety filter. Experimental results show that 1) compared to having no assistance, our HCSF improves both safety and user satisfaction without compromising human agency or comfort, and 2) relative to a conventional safety filter, our proposed HCSF boosts human agency, comfort, and satisfaction while maintaining robustness.","authors":["Donggeon David Oh","Justin Lidard","Haimin Hu","Himani Sinhmar","Elle Lazarski","Deepak Gopinath","Emily S. Sumner","Jonathan A. DeCastro","Guy Rosman","Naomi Ehrich Leonard","Jaime Fern\\'andez Fisac"],"url":"https://arxiv.org/abs/2504.11717"}
{"created":"2025-05-01","title":"Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer","abstract":"Prompt tuning has emerged as a lightweight adaptation strategy for adapting foundation models to downstream tasks, particularly in resource-constrained systems. As pre-trained prompts have become valuable intellectual assets, combining multiple source prompts offers a promising approach to enhance generalization to new tasks by leveraging complementary knowledge from diverse sources. However, naive aggregation of these prompts often leads to representation collapse due to mutual interference, undermining their collective potential. To address these challenges, we propose HGPrompt, an adaptive framework for multi-source prompt transfer that learns optimal ensemble weights by jointly optimizing dual objectives: transferability and stability. Specifically, we first introduce an information-theoretic metric to evaluate the transferability of prompt-induced features on the target task, capturing the intrinsic alignment between the feature representations. Additionally, we propose a novel Gradient Alignment Regularization to mitigate gradient conflicts among prompts, enabling stable and coherent knowledge transfer from multiple sources while suppressing interference. Extensive experiments on the large-scale VTAB benchmark demonstrate that HGPrompt achieves state-of-the-art performance, validating its effectiveness in multi-source prompt transfer.","authors":["Enming Zhang","Liwen Cao","Yanru Wu","Zijie Zhao","Guan Wang","Yang Li"],"url":"https://arxiv.org/abs/2504.12311"}
{"created":"2025-05-01","title":"New Results on a General Class of Minimum Norm Optimization Problems","abstract":"We study the general norm optimization for combinatorial problems, initiated by Chakrabarty and Swamy (STOC 2019). We propose a general formulation that captures a large class of combinatorial structures: we are given a set $U$ of $n$ weighted elements and a family of feasible subsets $F$. Each subset $S\\in F$ is called a feasible solution/set of the problem. We denote the value vector by $v=\\{v_i\\}_{i\\in [n]}$, where $v_i\\geq 0$ is the value of element $i$. For any subset $S\\subseteq U$, we use $v[S]$ to denote the $n$-dimensional vector $\\{v_e\\cdot \\mathbf{1}[e\\in S]\\}_{e\\in U}$. Let $f: \\mathbb{R}^n\\rightarrow\\mathbb{R}_+$ be a symmetric monotone norm function. Our goal is to minimize the norm objective $f(v[S])$ over feasible subset $S\\in F$.","authors":["Kuowen Chen","Jian Li","Yuval Rabani","Yiran Zhang"],"url":"https://arxiv.org/abs/2504.13489"}
{"created":"2025-05-01","title":"Thousand Voices of Trauma: A Large-Scale Synthetic Dataset for Modeling Prolonged Exposure Therapy Conversations","abstract":"The advancement of AI systems for mental health support is hindered by limited access to therapeutic conversation data, particularly for trauma treatment. We present Thousand Voices of Trauma, a synthetic benchmark dataset of 3,000 therapy conversations based on Prolonged Exposure therapy protocols for Post-traumatic Stress Disorder (PTSD). The dataset comprises 500 unique cases, each explored through six conversational perspectives that mirror the progression of therapy from initial anxiety to peak distress to emotional processing. We incorporated diverse demographic profiles (ages 18-80, M=49.3, 49.4% male, 44.4% female, 6.2% non-binary), 20 trauma types, and 10 trauma-related behaviors using deterministic and probabilistic generation methods. Analysis reveals realistic distributions of trauma types (witnessing violence 10.6%, bullying 10.2%) and symptoms (nightmares 23.4%, substance abuse 20.8%). Clinical experts validated the dataset's therapeutic fidelity, highlighting its emotional depth while suggesting refinements for greater authenticity. We also developed an emotional trajectory benchmark with standardized metrics for evaluating model responses. This privacy-preserving dataset addresses critical gaps in trauma-focused mental health data, offering a valuable resource for advancing both patient-facing applications and clinician training tools.","authors":["Suhas BN","Dominik Mattioli","Saeed Abdullah","Rosa I. Arriaga","Chris W. Wiese","Andrew M. Sherrill"],"url":"https://arxiv.org/abs/2504.13955"}
{"created":"2025-05-01","title":"A CMOS Probabilistic Computing Chip With In-situ hardware Aware Learning","abstract":"This paper demonstrates a probabilistic bit physics inspired solver with 440 spins configured in a Chimera graph, occupying an area of 0.44 mm^2. Area efficiency is maximized through a current-mode implementation of the neuron update circuit, standard cell design for analog blocks pitch-matched to digital blocks, and a shared power supply for both digital and analog components. Process variation related mismatches introduced by this approach are effectively mitigated using a hardware aware contrastive divergence algorithm during training. We validate the chip's ability to perform probabilistic computing tasks such as modeling logic gates and full adders, as well as optimization tasks such as MaxCut, demonstrating its potential for AI and machine learning applications.","authors":["Jinesh Jhonsa","William Whitehead","David McCarthy","Shuvro Chowdhury","Kerem Camsari","Luke Theogarajan"],"url":"https://arxiv.org/abs/2504.14070"}
{"created":"2025-05-01","title":"FinSage: A Multi-aspect RAG System for Financial Filings Question Answering","abstract":"Leveraging large language models in real-world settings often entails a need to utilize domain-specific data and tools in order to follow the complex regulations that need to be followed for acceptable use. Within financial sectors, modern enterprises increasingly rely on Retrieval-Augmented Generation (RAG) systems to address complex compliance requirements in financial document workflows. However, existing solutions struggle to account for the inherent heterogeneity of data (e.g., text, tables, diagrams) and evolving nature of regulatory standards used in financial filings, leading to compromised accuracy in critical information extraction. We propose the FinSage framework as a solution, utilizing a multi-aspect RAG framework tailored for regulatory compliance analysis in multi-modal financial documents. FinSage introduces three innovative components: (1) a multi-modal pre-processing pipeline that unifies diverse data formats and generates chunk-level metadata summaries, (2) a multi-path sparse-dense retrieval system augmented with query expansion (HyDE) and metadata-aware semantic search, and (3) a domain-specialized re-ranking module fine-tuned via Direct Preference Optimization (DPO) to prioritize compliance-critical content. Extensive experiments demonstrate that FinSage achieves an impressive recall of 92.51% on 75 expert-curated questions derived from surpasses the best baseline method on the FinanceBench question answering datasets by 24.06% in accuracy. Moreover, FinSage has been successfully deployed as financial question-answering agent in online meetings, where it has already served more than 1,200 people.","authors":["Xinyu Wang","Jijun Chi","Zhenghan Tai","Tung Sum Thomas Kwok","Muzhi Li","Zhuhong Li","Hailin He","Yuchen Hua","Peng Lu","Suyuchen Wang","Yihong Wu","Jerry Huang","Jingrui Tian","Ling Zhou"],"url":"https://arxiv.org/abs/2504.14493"}
{"created":"2025-05-01","title":"Quantitative Clustering in Mean-Field Transformer Models","abstract":"The evolution of tokens through a deep transformer models can be modeled as an interacting particle system that has been shown to exhibit an asymptotic clustering behavior akin to the synchronization phenomenon in Kuramoto models. In this work, we investigate the long-time clustering of mean-field transformer models. More precisely, we establish exponential rates of contraction to a Dirac point mass for any suitably regular initialization under some assumptions on the parameters of transformer models, any suitably regular mean-field initialization synchronizes exponentially fast with some quantitative rates.","authors":["Shi Chen","Zhengjiang Lin","Yury Polyanskiy","Philippe Rigollet"],"url":"https://arxiv.org/abs/2504.14697"}
{"created":"2025-05-01","title":"Cultivating Multidisciplinary Research and Education on GPU Infrastructure for Mid-South Institutions at the University of Memphis: Practice and Challenge","abstract":"To support rapid scientific advancement and promote access to large-scale computing resources for under-resourced institutions at the Mid-South region, the University of Memphis (UofM) established the first regional mid-scale GPU cluster, iTiger, a valuable high-performance computing (HPC) infrastructure. In this study, we present our continuous efforts to manage the critical cyberinfrastructure and provide essential computing supports for educators, students, and researchers in AI, data sciences, and related scientific fields in the Mid-South region, such as precision agriculture, smart transportation, and health informatics. We outline our initiatives to broaden CI adoptions across regional computing-related scientific and engineering fields, such as seed grant, workshop trainings, course integration, and other outreach activities. While we've observed promising outcomes of regional CI adoptions, we will discuss insights and challenges of Mid-South CI users, which can inspire other institutions to implement similar programs.","authors":["Mayira Sharif","Guangzeng Han","Weisi Liu","Xiaolei Huang"],"url":"https://arxiv.org/abs/2504.14786"}
{"created":"2025-05-01","title":"vApps: Verifiable Applications at Internet Scale","abstract":"Blockchain technology promises a decentralized, trustless, and interoperable infrastructure. However, widespread adoption remains hindered by issues such as limited scalability, high transaction costs, and the complexity of maintaining coherent verification logic across different blockchain layers. This paper introduces Verifiable Applications (vApps), a novel development framework designed to streamline the creation and deployment of verifiable blockchain computing applications. vApps offer a unified Rust-based Domain-Specific Language (DSL) within a comprehensive SDK, featuring modular abstractions for verification, proof generation, and inter-chain connectivity. This eases the developer's burden in securing diverse software components, allowing them to focus on application logic. The DSL also ensures that applications can automatically take advantage of specialized precompiles and hardware acceleration to achieve consistently high performance with minimal developer effort, as demonstrated by benchmark results for zero-knowledge virtual machines (zkVMs). Experiments show that native Rust execution eliminates interpretation overhead, delivering up to an 197x cycle count improvement compared to EVM-based approaches. Precompiled circuits can accelerate the proof by more than 95%, while GPU acceleration increases throughput by up to 30x and recursion compresses the proof size by up to 230x, enabling succinct and efficient verification. The framework also supports seamless integration with the Web2 and Web3 systems, enabling developers to focus solely on their application logic. Through modular architecture, robust security guarantees, and composability, vApps pave the way toward a trust-minimized and verifiable Internet-scale application environment.","authors":["Isaac Zhang","Kshitij Kulkarni","Tan Li","Daniel Wong","Thomas Kim","John Guibas","Uma Roy","Bryan Pellegrino","Ryan Zarick"],"url":"https://arxiv.org/abs/2504.14809"}
{"created":"2025-05-01","title":"Towards Fuzzing Zero-Knowledge Proof Circuits (Short Paper)","abstract":"Zero-knowledge proofs (ZKPs) have evolved from a theoretical cryptographic concept into a powerful tool for implementing privacy-preserving and verifiable applications without requiring trust assumptions. Despite significant progress in the field, implementing and using ZKPs via \\emph{ZKP circuits} remains challenging, leading to numerous bugs that affect ZKP circuits in practice, and \\emph{fuzzing} remains largely unexplored as a method to detect bugs in ZKP circuits. We discuss the unique challenges of applying fuzzing to ZKP circuits, examine the oracle problem and its potential solutions, and propose techniques for input generation and test harness construction. We demonstrate that fuzzing can be effective in this domain by implementing a fuzzer for \\texttt{zk-regex}, a cornerstone library in modern ZKP applications. In our case study, we discovered \\textit{$10$} new bugs that have been confirmed by the developers.","authors":["Stefanos Chaliasos","Imam Al-Fath","Alastair Donaldson"],"url":"https://arxiv.org/abs/2504.14881"}
{"created":"2025-05-01","title":"DyST-XL: Dynamic Layout Planning and Content Control for Compositional Text-to-Video Generation","abstract":"Compositional text-to-video generation, which requires synthesizing dynamic scenes with multiple interacting entities and precise spatial-temporal relationships, remains a critical challenge for diffusion-based models. Existing methods struggle with layout discontinuity, entity identity drift, and implausible interaction dynamics due to unconstrained cross-attention mechanisms and inadequate physics-aware reasoning. To address these limitations, we propose DyST-XL, a \\textbf{training-free} framework that enhances off-the-shelf text-to-video models (e.g., CogVideoX-5B) through frame-aware control. DyST-XL integrates three key innovations: (1) A Dynamic Layout Planner that leverages large language models (LLMs) to parse input prompts into entity-attribute graphs and generates physics-aware keyframe layouts, with intermediate frames interpolated via trajectory optimization; (2) A Dual-Prompt Controlled Attention Mechanism that enforces localized text-video alignment through frame-aware attention masking, achieving precise control over individual entities; and (3) An Entity-Consistency Constraint strategy that propagates first-frame feature embeddings to subsequent frames during denoising, preserving object identity without manual annotation. Experiments demonstrate that DyST-XL excels in compositional text-to-video generation, significantly improving performance on complex prompts and bridging a crucial gap in training-free video synthesis. The code is released in https://github.com/XiaoBuL/DyST-XL.","authors":["Weijie He","Mushui Liu","Yunlong Yu","Zhao Wang","Chao Wu"],"url":"https://arxiv.org/abs/2504.15032"}
{"created":"2025-05-01","title":"MoBGS: Motion Deblurring Dynamic 3D Gaussian Splatting for Blurry Monocular Video","abstract":"We present MoBGS, a novel deblurring dynamic 3D Gaussian Splatting (3DGS) framework capable of reconstructing sharp and high-quality novel spatio-temporal views from blurry monocular videos in an end-to-end manner. Existing dynamic novel view synthesis (NVS) methods are highly sensitive to motion blur in casually captured videos, resulting in significant degradation of rendering quality. While recent approaches address motion-blurred inputs for NVS, they primarily focus on static scene reconstruction and lack dedicated motion modeling for dynamic objects. To overcome these limitations, our MoBGS introduces a novel Blur-adaptive Latent Camera Estimation (BLCE) method for effective latent camera trajectory estimation, improving global camera motion deblurring. In addition, we propose a physically-inspired Latent Camera-induced Exposure Estimation (LCEE) method to ensure consistent deblurring of both global camera and local object motion. Our MoBGS framework ensures the temporal consistency of unseen latent timestamps and robust motion decomposition of static and dynamic regions. Extensive experiments on the Stereo Blur dataset and real-world blurry videos show that our MoBGS significantly outperforms the very recent advanced methods (DyBluRF and Deblur4DGS), achieving state-of-the-art performance for dynamic NVS under motion blur.","authors":["Minh-Quan Viet Bui","Jongmin Park","Juan Luis Gonzalez Bello","Jaeho Moon","Jihyong Oh","Munchurl Kim"],"url":"https://arxiv.org/abs/2504.15122"}
{"created":"2025-05-01","title":"Improving Human-AI Coordination through Adversarial Training and Generative Models","abstract":"Being able to cooperate with new people is an important component of many economically valuable AI tasks, from household robotics to autonomous driving. However, generalizing to novel humans requires training on data that captures the diversity of human behaviors. Adversarial training is one avenue for searching for such data and ensuring that agents are robust. However, it is difficult to apply in the cooperative setting because adversarial policies intentionally learn to sabotage the task instead of simulating valid cooperation partners. To address this challenge, we propose a novel strategy for overcoming self-sabotage that combines a pre-trained generative model to simulate valid cooperative agent policies with adversarial training to maximize regret. We call our method GOAT: Generative Online Adversarial Training. In this framework, the GOAT dynamically searches for and generates coordination strategies where the learning policy -- the Cooperator agent -- underperforms. GOAT enables better generalization by exposing the Cooperator to various challenging interaction scenarios. We maintain realistic coordination strategies by updating only the generative model's embedding while keeping its parameters frozen, thus avoiding adversarial exploitation. We evaluate GOAT with real human partners, and the results demonstrate state-of-the-art performance on the Overcooked benchmark, highlighting its effectiveness in generalizing to diverse human behaviors.","authors":["Paresh Chaudhary","Yancheng Liang","Daphne Chen","Simon S. Du","Natasha Jaques"],"url":"https://arxiv.org/abs/2504.15457"}
{"created":"2025-05-01","title":"No-Regret Model Predictive Control with Online Learning of Koopman Operators","abstract":"We study a problem of simultaneous system identification and model predictive control of nonlinear systems. Particularly, we provide an algorithm for systems with unknown residual dynamics that can be expressed by Koopman operators. Such residual dynamics can model external disturbances and modeling errors, such as wind and wave disturbances to aerial and marine vehicles, or inaccurate model parameters. The algorithm has finite-time near-optimality guarantees and asymptotically converges to the optimal non-causal controller. Specifically, the algorithm enjoys sublinear \\textit{dynamic regret}, defined herein as the suboptimality against an optimal clairvoyant controller that knows how the unknown dynamics will adapt to its states and actions. To this end, we assume the algorithm is given Koopman observable functions such that the unknown dynamics can be approximated by a linear dynamical system. Then, it employs model predictive control based on the current learned model of the unknown residual dynamics. This model is updated online using least squares in a self-supervised manner based on the data collected while controlling the system. We validate our algorithm in physics-based simulations of a cart-pole system aiming to maintain the pole upright despite inaccurate model parameters.","authors":["Hongyu Zhou","Vasileios Tzoumas"],"url":"https://arxiv.org/abs/2504.15805"}
{"created":"2025-05-01","title":"Hexcute: A Tile-based Programming Language with Automatic Layout and Task-Mapping Synthesis","abstract":"Deep learning (DL) workloads mainly run on accelerators like GPUs. Recent DL quantization techniques demand a new matrix multiplication operator with mixed input data types, further complicating GPU optimization. Prior high-level compilers like Triton lack the expressiveness to implement key optimizations like fine-grained data pipelines and hardware-friendly memory layouts for these operators, while low-level programming models, such as Hidet, Graphene, and CUTLASS, require significant programming efforts. To balance expressiveness with engineering effort, we propose Hexcute, a tile-based programming language that exposes shared memory and register abstractions to enable fine-grained optimization for these operators. Additionally, Hexcute leverages task mapping to schedule the GPU program, and to reduce programming efforts, it automates layout and task mapping synthesis with a novel type-inference-based algorithm. Our evaluation shows that Hexcute generalizes to a wide range of DL operators, achieves 1.7-11.28$\\times$ speedup over existing DL compilers for mixed-type operators, and brings up to 2.91$\\times$ speedup in the end-to-end evaluation.","authors":["Xiao Zhang","Yaoyao Ding","Yang Hu","Gennady Pekhimenko"],"url":"https://arxiv.org/abs/2504.16214"}
{"created":"2025-05-01","title":"Naturally Computed Scale Invariance in the Residual Stream of ResNet18","abstract":"An important capacity in visual object recognition is invariance to image-altering variables which leave the identity of objects unchanged, such as lighting, rotation, and scale. How do neural networks achieve this? Prior mechanistic interpretability research has illuminated some invariance-building circuitry in InceptionV1, but the results are limited and networks with different architectures have remained largely unexplored. This work investigates ResNet18 with a particular focus on its residual stream, an architectural component which InceptionV1 lacks. We observe that many convolutional channels in intermediate blocks exhibit scale invariant properties, computed by the element-wise residual summation of scale equivariant representations: the block input's smaller-scale copy with the block pre-sum output's larger-scale copy. Through subsequent ablation experiments, we attempt to causally link these neural properties with scale-robust object recognition behavior. Our tentative findings suggest how the residual stream computes scale invariance and its possible role in behavior. Code is available at: https://github.com/cest-andre/residual-stream-interp","authors":["Andr\\'e Longon"],"url":"https://arxiv.org/abs/2504.16290"}
{"created":"2025-05-01","title":"Fast Online Adaptive Neural MPC via Meta-Learning","abstract":"Data-driven model predictive control (MPC) has demonstrated significant potential for improving robot control performance in the presence of model uncertainties. However, existing approaches often require extensive offline data collection and computationally intensive training, limiting their ability to adapt online. To address these challenges, this paper presents a fast online adaptive MPC framework that leverages neural networks integrated with Model-Agnostic Meta-Learning (MAML). Our approach focuses on few-shot adaptation of residual dynamics - capturing the discrepancy between nominal and true system behavior - using minimal online data and gradient steps. By embedding these meta-learned residual models into a computationally efficient L4CasADi-based MPC pipeline, the proposed method enables rapid model correction, enhances predictive accuracy, and improves real-time control performance. We validate the framework through simulation studies on a Van der Pol oscillator, a Cart-Pole system, and a 2D quadrotor. Results show significant gains in adaptation speed and prediction accuracy over both nominal MPC and nominal MPC augmented with a freshly initialized neural network, underscoring the effectiveness of our approach for real-time adaptive robot control.","authors":["Yu Mei","Xinyu Zhou","Shuyang Yu","Vaibhav Srivastava","Xiaobo Tan"],"url":"https://arxiv.org/abs/2504.16369"}
{"created":"2025-05-01","title":"EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment","abstract":"The furnishing of multi-modal large language models (MLLMs) has led to the emergence of numerous benchmark studies, particularly those evaluating their perception and understanding capabilities.","authors":["Lancheng Gao","Ziheng Jia","Yunhao Zeng","Wei Sun","Yiming Zhang","Wei Zhou","Guangtao Zhai","Xiongkuo Min"],"url":"https://arxiv.org/abs/2504.16405"}
{"created":"2025-05-01","title":"CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using Unsupervised Backbones","abstract":"Class-agnostic counting (CAC) aims to estimate the number of objects in images without being restricted to predefined categories. However, while current exemplar-based CAC methods offer flexibility at inference time, they still rely heavily on labeled data for training, which limits scalability and generalization to many downstream use cases. In this paper, we introduce CountingDINO, the first training-free exemplar-based CAC framework that exploits a fully unsupervised feature extractor. Specifically, our approach employs self-supervised vision-only backbones to extract object-aware features, and it eliminates the need for annotated data throughout the entire proposed pipeline. At inference time, we extract latent object prototypes via ROI-Align from DINO features and use them as convolutional kernels to generate similarity maps. These are then transformed into density maps through a simple yet effective normalization scheme. We evaluate our approach on the FSC-147 benchmark, where we consistently outperform a baseline based on an SOTA unsupervised object detector under the same label- and training-free setting. Additionally, we achieve competitive results -- and in some cases surpass -- training-free methods that rely on supervised backbones, non-training-free unsupervised methods, as well as several fully supervised SOTA approaches. This demonstrates that label- and training-free CAC can be both scalable and effective. Code: https://lorebianchi98.github.io/CountingDINO/.","authors":["Giacomo Pacini","Lorenzo Bianchi","Luca Ciampi","Nicola Messina","Giuseppe Amato","Fabrizio Falchi"],"url":"https://arxiv.org/abs/2504.16570"}
{"created":"2025-05-01","title":"Conditional Diffusion-Based Retrieval of Atmospheric CO2 from Earth Observing Spectroscopy","abstract":"Satellite-based estimates of greenhouse gas (GHG) properties from observations of reflected solar spectra are integral for understanding and monitoring complex terrestrial systems and their impact on the carbon cycle due to their near global coverage. Known as retrieval, making GHG concentration estimations from these observations is a non-linear Bayesian inverse problem, which is operationally solved using a computationally expensive algorithm called Optimal Estimation (OE), providing a Gaussian approximation to a non-Gaussian posterior. This leads to issues in solver algorithm convergence, and to unrealistically confident uncertainty estimates for the retrieved quantities. Upcoming satellite missions will provide orders of magnitude more data than the current constellation of GHG observers. Development of fast and accurate retrieval algorithms with robust uncertainty quantification is critical. Doing so stands to provide substantial climate impact of moving towards the goal of near continuous real-time global monitoring of carbon sources and sinks which is essential for policy making. To achieve this goal, we propose a diffusion-based approach to flexibly retrieve a Gaussian or non-Gaussian posterior, for NASA's Orbiting Carbon Observatory-2 spectrometer, while providing a substantial computational speed-up over the current operational state-of-the-art.","authors":["William R. Keely","Otto Lamminp\\\"a\\\"a","Steffen Mauceri","Sean M. R. Crowell","Christopher W. O'Dell","Gregory R. McGarragh"],"url":"https://arxiv.org/abs/2504.17074"}
{"created":"2025-05-01","title":"A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices","abstract":"In this article, we introduce a novel deep learning hybrid model that integrates attention Transformer and Gated Recurrent Unit (GRU) architectures to improve the accuracy of cryptocurrency price predictions. By combining the Transformer's strength in capturing long-range patterns with the GRU's ability to model short-term and sequential trends, the hybrid model provides a well-rounded approach to time series forecasting. We apply the model to predict the daily closing prices of Bitcoin and Ethereum based on historical data that include past prices, trading volumes, and the Fear and Greed index. We evaluate the performance of our proposed model by comparing it with four other machine learning models: two are non-sequential feedforward models: Radial Basis Function Network (RBFN) and General Regression Neural Network (GRNN), and two are bidirectional sequential memory-based models: Bidirectional Long-Short-Term Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance of the model is assessed using several metrics, including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE), along with statistical validation through the nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test. The results demonstrate that our hybrid model consistently achieves superior accuracy, highlighting its effectiveness for financial prediction tasks. These findings provide valuable insights for improving real-time decision making in cryptocurrency markets and support the growing use of hybrid deep learning models in financial analytics.","authors":["Esam Mahdi","C. Martin-Barreiro","X. Cabezas"],"url":"https://arxiv.org/abs/2504.17079"}
{"created":"2025-05-01","title":"Revisiting Reset Mechanisms in Spiking Neural Networks for Sequential Modeling: Specialized Discretization for Binary Activated RNN","abstract":"In the field of image recognition, spiking neural networks (SNNs) have achieved performance comparable to conventional artificial neural networks (ANNs). In such applications, SNNs essentially function as traditional neural networks with quantized activation values. This article focuses on an another alternative perspective,viewing SNNs as binary-activated recurrent neural networks (RNNs) for sequential modeling tasks. From this viewpoint, current SNN architectures face several fundamental challenges in sequence modeling: (1) Traditional models lack effective memory mechanisms for long-range sequence modeling; (2) The biological-inspired components in SNNs (such as reset mechanisms and refractory period applications) remain theoretically under-explored for sequence tasks; (3) The RNN-like computational paradigm in SNNs prevents parallel training across different timesteps. To address these challenges, this study conducts a systematic analysis of the fundamental mechanisms underlying reset operations and refractory periods in binary-activated RNN-based SNN sequence models. We re-examine whether such biological mechanisms are strictly necessary for generating sparse spiking patterns, provide new theoretical explanations and insights, and ultimately propose the fixed-refractory-period SNN architecture for sequence modeling.","authors":["Enqi Zhang"],"url":"https://arxiv.org/abs/2504.17751"}
{"created":"2025-05-01","title":"FIM: Frequency-Aware Multi-View Interest Modeling for Local-Life Service Recommendation","abstract":"People's daily lives involve numerous periodic behaviors, such as eating and traveling. Local-life platforms cater to these recurring needs by providing essential services tied to daily routines. Therefore, users' periodic intentions are reflected in their interactions with the platforms. There are two main challenges in modeling users' periodic behaviors in the local-life service recommendation systems: 1) the diverse demands of users exhibit varying periodicities, which are difficult to distinguish as they are mixed in the behavior sequences; 2) the periodic behaviors of users are subject to dynamic changes due to factors such as holidays and promotional events. Existing methods struggle to distinguish the periodicities of diverse demands and overlook the importance of dynamically capturing changes in users' periodic behaviors. To this end, we employ a Frequency-Aware Multi-View Interest Modeling framework (FIM). Specifically, we propose a multi-view search strategy that decomposes users' demands from different perspectives to separate their various periodic intentions. This allows the model to comprehensively extract their periodic features than category-searched-only methods. Moreover, we propose a frequency-domain perception and evolution module. This module uses the Fourier Transform to convert users' temporal behaviors into the frequency domain, enabling the model to dynamically perceive their periodic features. Extensive offline experiments demonstrate that FIM achieves significant improvements on public and industrial datasets, showing its capability to effectively model users' periodic intentions. Furthermore, the model has been deployed on the Kuaishou local-life service platform. Through online A/B experiments, the transaction volume has been significantly improved.","authors":["Guoquan Wang","Qiang Luo","Weisong Hu","Pengfei Yao","Wencong Zeng","Guorui Zhou","Kun Gai"],"url":"https://arxiv.org/abs/2504.17814"}
{"created":"2025-05-01","title":"Evolution Meets Diffusion: Efficient Neural Architecture Generation","abstract":"Neural Architecture Search (NAS) has gained widespread attention for its transformative potential in deep learning model design. However, the vast and complex search space of NAS leads to significant computational and time costs. Neural Architecture Generation (NAG) addresses this by reframing NAS as a generation problem, enabling the precise generation of optimal architectures for specific tasks. Despite its promise, mainstream methods like diffusion models face limitations in global search capabilities and are still hindered by high computational and time demands. To overcome these challenges, we propose Evolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel approach that achieves efficient and training-free architecture generation. EDNAG leverages evolutionary algorithms to simulate the denoising process in diffusion models, using fitness to guide the transition from random Gaussian distributions to optimal architecture distributions. This approach combines the strengths of evolutionary strategies and diffusion models, enabling rapid and effective architecture generation. Extensive experiments demonstrate that EDNAG achieves state-of-the-art (SOTA) performance in architecture optimization, with an improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need for time-consuming training and boosts inference speed by an average of 50 times, showcasing its exceptional efficiency and effectiveness.","authors":["Bingye Zhou","Caiyang Yu"],"url":"https://arxiv.org/abs/2504.17827"}
{"created":"2025-05-01","title":"PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts","abstract":"In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Qwen-3-235B-A22B-Thinking and Gemini-2.5-pro, achieve only 54.6 and 52.2 benchmark scores, with about 40% accuracy under the highest level From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs.","authors":["Yiming Wang","Pei Zhang","Jialong Tang","Haoran Wei","Baosong Yang","Rui Wang","Chenshu Sun","Feitong Sun","Jiran Zhang","Junxuan Wu","Qiqian Cang","Yichang Zhang","Fei Huang","Junyang Lin","Fei Huang","Jingren Zhou"],"url":"https://arxiv.org/abs/2504.18428"}
{"created":"2025-05-01","title":"SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning","abstract":"Large Language Models (LLMs) are powerful but often require extensive fine-tuning and large datasets for specialized domains like law. General-purpose pre-training may not capture legal nuances, and acquiring sufficient legal data is challenging. We introduce SynLexLM, a novel approach to efficiently pre-train a legal LLM. Our method employs curriculum learning, progressing from simple to complex legal texts and queries, combined with synthetic data augmentation using models like Gemini Pro to address data scarcity. We aim to achieve improved performance on legal benchmarks (BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned versions. Preliminary work involves generating synthetic QA pairs reflecting legal reasoning. This work aims to enhance legal document analysis and research tools, potentially democratizing access to advanced legal AI.","authors":["Ojasw Upadhyay","Abishek Saravanakumar","Ayman Ismail"],"url":"https://arxiv.org/abs/2504.18762"}
{"created":"2025-05-01","title":"4DGS-CC: A Contextual Coding Framework for 4D Gaussian Splatting Data Compression","abstract":"Storage is a significant challenge in reconstructing dynamic scenes with 4D Gaussian Splatting (4DGS) data. In this work, we introduce 4DGS-CC, a contextual coding framework that compresses 4DGS data to meet specific storage constraints. Building upon the established deformable 3D Gaussian Splatting (3DGS) method, our approach decomposes 4DGS data into 4D neural voxels and a canonical 3DGS component, which are then compressed using Neural Voxel Contextual Coding (NVCC) and Vector Quantization Contextual Coding (VQCC), respectively. Specifically, we first decompose the 4D neural voxels into distinct quantized features by separating the temporal and spatial dimensions. To losslessly compress each quantized feature, we leverage the previously compressed features from the temporal and spatial dimensions as priors and apply NVCC to generate the spatiotemporal context for contextual coding. Next, we employ a codebook to store spherical harmonics information from canonical 3DGS as quantized vectors, which are then losslessly compressed by using VQCC with the auxiliary learned hyperpriors for contextual coding, thereby reducing redundancy within the codebook. By integrating NVCC and VQCC, our contextual coding framework, 4DGS-CC, enables multi-rate 4DGS data compression tailored to specific storage requirements. Extensive experiments on three 4DGS data compression benchmarks demonstrate that our method achieves an average storage reduction of approximately 12 times while maintaining rendering fidelity compared to our baseline 4DGS approach.","authors":["Zicong Chen","Zhenghao Chen","Wei Jiang","Wei Wang","Lei Liu","Dong Xu"],"url":"https://arxiv.org/abs/2504.18925"}
{"created":"2025-05-01","title":"The Satisfiability and Validity Problems for Probabilistic Computational Tree Logic are Highly Undecidable","abstract":"The Probabilistic Computational Tree Logic (PCTL) is the main specification formalism for discrete probabilistic systems modeled by Markov chains. Despite serious research attempts, the decidability of PCTL satisfiability and validity problems remained unresolved for 30 years. We show that both problems are highly undecidable, i.e., beyond the arithmetical hierarchy. Consequently, there is no sound and complete deductive system for PCTL.","authors":["Miroslav Chodil","Anton\\'in Ku\\v{c}era"],"url":"https://arxiv.org/abs/2504.19207"}
{"created":"2025-05-01","title":"Robotic Trail Maker Platform for Rehabilitation in Neurological Conditions: Clinical Use Cases","abstract":"Patients with neurological conditions require rehabilitation to restore their motor, visual, and cognitive abilities. To meet the shortage of therapists and reduce their workload, a robotic rehabilitation platform involving the clinical trail making test is proposed. Therapists can create custom trails for each patient and the patient can trace the trails using a robotic device. The platform can track the performance of the patient and use these data to provide dynamic assistance through the robot to the patient interface. Therefore, the proposed platform not only functions as an evaluation platform, but also trains the patient in recovery. The developed platform has been validated at a rehabilitation center, with therapists and patients operating the device. It was found that patients performed poorly while using the platform compared to healthy subjects and that the assistance provided also improved performance amongst patients. Statistical analysis demonstrated that the speed of the patients was significantly enhanced with the robotic assistance. Further, neural networks are trained to classify between patients and healthy subjects and to forecast their movements using the data collected.","authors":["Srikar Annamraju","Harris Nisar","Dayu Xia","Shankar A. Deka","Anne Horowitz","Nadica Miljkovi\\'c","Du\\v{s}an M. Stipanovi\\'c"],"url":"https://arxiv.org/abs/2504.19230"}
{"created":"2025-05-01","title":"Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers","abstract":"Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we propose a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.","authors":["Dylan Bouchard","Mohit Singh Chauhan"],"url":"https://arxiv.org/abs/2504.19254"}
{"created":"2025-05-01","title":"OPAL: Visibility-aware LiDAR-to-OpenStreetMap Place Recognition via Adaptive Radial Fusion","abstract":"LiDAR place recognition is a critical capability for autonomous navigation and cross-modal localization in large-scale outdoor environments. Existing approaches predominantly depend on pre-built 3D dense maps or aerial imagery, which impose significant storage overhead and lack real-time adaptability. In this paper, we propose OPAL, a novel network for LiDAR place recognition that leverages OpenStreetMap (OSM) as a lightweight and up-to-date prior. Our key innovation lies in bridging the domain disparity between sparse LiDAR scans and structured OSM data through two carefully designed components. First, a cross-modal visibility mask that identifies maximal observable regions from both modalities to guide feature learning. Second, an adaptive radial fusion module that dynamically consolidates radial features into discriminative global descriptors. Extensive experiments on the KITTI and KITTI-360 datasets demonstrate OPAL's superiority, achieving 15.98% higher recall at @1m threshold for top-1 retrieved matches, along with 12x faster inference speed compared to the state-of-the-art approach. Code and datasets will be publicly available.","authors":["Shuhao Kang","Martin Y. Liao","Yan Xia","Olaf Wysocki","Boris Jutzi","Daniel Cremers"],"url":"https://arxiv.org/abs/2504.19258"}
{"created":"2025-05-01","title":"LLMs for Engineering: Teaching Models to Design High Powered Rockets","abstract":"Large Language Models (LLMs) have transformed software engineering, but their application to physical engineering domains remains underexplored. This paper evaluates LLMs' capabilities in high-powered rocketry design through RocketBench, a benchmark connecting LLMs to high-fidelity rocket simulations. We test models on two increasingly complex design tasks: target altitude optimization and precision landing challenges. Our findings reveal that while state-of-the-art LLMs demonstrate strong baseline engineering knowledge, they struggle to iterate on their designs when given simulation results and ultimately plateau below human performance levels. However, when enhanced with reinforcement learning (RL), we show that a 7B parameter model outperforms both SoTA foundation models and human experts. This research demonstrates that RL-trained LLMs can serve as effective tools for complex engineering optimization, potentially transforming engineering domains beyond software development.","authors":["Toby Simonds"],"url":"https://arxiv.org/abs/2504.19394"}
{"created":"2025-05-01","title":"Dynamic r-index: An Updatable Self-Index for Highly Repetitive Strings","abstract":"A self-index is a compressed data structure that supports locate queries-reporting all positions where a given pattern occurs in a string. While many self-indexes have been proposed, developing dynamically updatable ones supporting string insertions and deletions remains a challenge. The r-index (Gagie et al., SODA'18) is a representative static self-index based on the run-length Burrows-Wheeler transform (RLBWT), designed for highly repetitive strings - those with many repeated substrings. We present the dynamic r-index, an extension of the r-index that supports locate queries in $\\mathcal{O}((m + \\mathsf{occ}) \\log n)$ time using $\\mathcal{O}(r)$ words, where $n$ is the length of the string $T$, $m$ is the pattern length, $\\mathsf{occ}$ is the number of occurrences, and $r$ is the number of runs in the RLBWT of $T$. It supports string insertions and deletions in $\\mathcal{O}((m + L_{\\mathsf{max}}) \\log n)$ time, where $L_{\\max}$ is the maximum value in the LCP array of $T$. The average running time is $\\mathcal{O}((m + L_{\\mathsf{avg}}) \\log n)$, where $L_{\\mathsf{avg}}$ is the average LCP value. We experimentally evaluated the dynamic r-index on various highly repetitive strings and demonstrated its practicality.","authors":["Takaaki Nishimoto","Yasuo Tabei"],"url":"https://arxiv.org/abs/2504.19482"}
{"created":"2025-05-01","title":"How Cohesive Are Community Search Results on Online Social Networks?: An Experimental Evaluation","abstract":"Recently, numerous community search methods for large graphs have been proposed, at the core of which is defining and measuring cohesion. This paper experimentally evaluates the effectiveness of these community search algorithms w.r.t. cohesiveness in the context of online social networks. Social communities are formed and developed under the influence of group cohesion theory, which has been extensively studied in social psychology. However, current generic methods typically measure cohesiveness using structural or attribute-based approaches and overlook domain-specific concepts such as group cohesion. We introduce five novel psychology-informed cohesiveness measures, based on the concept of group cohesion from social psychology, and propose a novel framework called CHASE for evaluating eight representative CS algorithms w.r.t. these measures on online social networks. Our analysis reveals that there is no clear correlation between structural and psychological cohesiveness, and no algorithm effectively identifies psychologically cohesive communities in online social networks. This study provides new insights that could guide the development of future community search methods.","authors":["Yining Zhao","Sourav S Bhowmick","Nastassja L. Fischer","SH Annabel Chen"],"url":"https://arxiv.org/abs/2504.19489"}
{"created":"2025-05-01","title":"Motion Generation for Food Topping Challenge 2024: Serving Salmon Roe Bowl and Picking Fried Chicken","abstract":"Although robots have been introduced in many industries, food production robots are yet to be widely employed because the food industry requires not only delicate movements to handle food but also complex movements that adapt to the environment. Force control is important for handling delicate objects such as food. In addition, achieving complex movements is possible by making robot motions based on human teachings. Four-channel bilateral control is proposed, which enables the simultaneous teaching of position and force information. Moreover, methods have been developed to reproduce motions obtained through human teachings and generate adaptive motions using learning. We demonstrated the effectiveness of these methods for food handling tasks in the Food Topping Challenge at the 2024 IEEE International Conference on Robotics and Automation (ICRA 2024). For the task of serving salmon roe on rice, we achieved the best performance because of the high reproducibility and quick motion of the proposed method. Further, for the task of picking fried chicken, we successfully picked the most pieces of fried chicken among all participating teams. This paper describes the implementation and performance of these methods.","authors":["Koki Inami","Masashi Konosu","Koki Yamane","Nozomu Masuya","Yunhan Li","Yu-Han Shu","Hiroshi Sato","Shinnosuke Homma","Sho Sakaino"],"url":"https://arxiv.org/abs/2504.19498"}
{"created":"2025-05-01","title":"The frequency $K_i$s for symmetrical traveling salesman problem","abstract":"The frequency $K_i$s ($i\\in[4,n]$) are studied for symmetrical traveling salesman problem ($TSP$) to identify the edges in optimal Hamiltonian cycle ($OHC$). A frequency $K_i$ is computed with a sort of ${{i}\\choose{2}}$ optimal $i$-vertex paths with given endpoints (optimal $i$-vertex path) in a corresponding $K_i$ in $K_n$. In frequency $K_i$, the frequency of an edge is the number of the optimal $i$-vertex paths containing the edge in the corresponding $K_i$. Given an $OHC$ edge related to $K_i$, it has a frequency bigger than $\\frac{1}{2}{{i}\\choose{2}}$ in the corresponding frequency $K_i$, and that of an ordinary edge not in $OHC$ is smaller than $\\frac{i+2}{2}$. On average, an $OHC$ edge in $K_i$ has a frequency bigger than $\\frac{i^2-4i+7}{2}$ whereas an ordinary edge has a frequency smaller than 2. Moreover, given a frequency $K_i$ containing an $OHC$ edge related to $K_n$, the frequency of the $OHC$ edge is bigger than $\\frac{1}{2}{{i}\\choose{2}}$ in the worst average case. It implies that the average frequency of an $OHC$ edge computed with frequency $K_i$s is bigger than $\\frac{1}{2}{{i}\\choose{2}}$. It also found that the probability that an $OHC$ edge is contained in optimal $i$-vertex paths keeps stable or increases according to $i\\in [4, n]$. As the frequency $K_i$s are used to compute the frequency of an edge, each $OHC$ edge has its own peak frequency at $i=P_0$ where $P_0=\\frac{n}{2} + 2$ for even $n$ or $\\frac{n+1}{2} + 1$ for odd $n$. For ordinary edges out of $OHC$, the probability that they are contained in optimal $i$-vertex paths decreases according to $i$. Moreover, the average frequency of an ordinary edge will be smaller than $\\frac{1}{2}{{i}\\choose{2}}$ if $i \\geq [0.3660n + 1.5849]$. Based on these findings, an algorithm is presented to find $OHC$ in $O(n^62^{0.3660n})$ time using dynamic programming.","authors":["Yong Wang"],"url":"https://arxiv.org/abs/2504.19608"}
{"created":"2025-05-01","title":"Measuring Train Driver Performance as Key to Approval of Driverless Trains","abstract":"Points 2.1.4(b), 2.4.2(b) and 2.4.3(b) in Annex I of Implementing Regulation (EU) No. 402/2013 allow a simplified approach for the safety approval of computer vision systems for driverless trains, if they have 'similar' functions and interfaces as the replaced human driver. The human driver is not replaced one-to-one by a technical system - only a limited set of cognitive functions are replaced. However, performance in the most challenging function, obstacle detection, is difficult to quantify due to the deficiency of published measurement results. This article summarizes the data published so far. This article also goes a long way to remedy this situation by providing a new public and anonymized dataset of 711 train driver performance measurements from controlled experiments. The measurements are made for different speeds, obstacle sizes, train protection systems and obstacle color contrasts respectively. The measured values are reaction time and distance to the obstacle. The goal of this paper is an unbiased and exhaustive description of the presented dataset for research, standardization and regulation. The dataset with supplementing information and literature is published on https://data.fid-move.de/de/dataset/atosensedata","authors":["Rustam Tagiew (German Centre for Rail Traffic Research at the Federal Railway Authority)","Prasannavenkatesh Balaji (German Centre for Rail Traffic Research at the Federal Railway Authority)"],"url":"https://arxiv.org/abs/2504.19735"}
{"created":"2025-05-01","title":"Near-Optimal Minimum Cuts in Hypergraphs at Scale","abstract":"The hypergraph minimum cut problem aims to partition its vertices into two blocks while minimizing the total weight of the cut hyperedges. This fundamental problem arises in network reliability, VLSI design, and community detection. We present HeiCut, a scalable algorithm for computing near-optimal minimum cuts in both unweighted and weighted hypergraphs. HeiCut aggressively reduces the hypergraph size through a sequence of provably exact reductions that preserve the minimum cut, along with an optional heuristic contraction based on label propagation. It then solves a relaxed Binary Integer Linear Program (BIP) on the reduced hypergraph to compute a near-optimal minimum cut. Our extensive evaluation on over 500 real-world hypergraphs shows that HeiCut computes the exact minimum cut in over 85% of instances using our exact reductions alone, and offers the best solution quality across all instances. It solves over twice as many instances as the state-of-the-art within set computational limits, and is up to five orders of magnitude faster.","authors":["Adil Chhabra","Christian Schulz","Bora U\\c{c}ar","Loris Wilwert"],"url":"https://arxiv.org/abs/2504.19842"}
{"created":"2025-05-01","title":"A Formal Framework for Naturally Specifying and Verifying Sequential Algorithms","abstract":"Current approaches for formal verification of algorithms face important limitations. For specification, they cannot express algorithms naturally and concisely, especially for algorithms with states and flexible control flow. For verification, formal proof based on Hoare logic cannot reflect the logical structure of natural proof. To address these challenges, we introduce a formal framework for naturally specifying and verifying sequential algorithms in Coq. We use the state relation monad to integrate Coq's expressive type system with the flexible control flow of imperative languages. It supports nondeterministic operations and customizable program states, enabling specifying algorithms at an appropriate level of abstraction. For verification, we build a Hoare logic for the monad and propose a novel two-stage proof approach that separates natural logical reasoning from mechanical composition. It reflects the logical structure of natural proof, enhancing modularity and readability. We evaluate the framework by formalizing the Depth-First Search (DFS) algorithm and verifying the Knuth-Morris-Pratt (KMP) algorithm.","authors":["Chengxi Yang","Shushu Wu","Qinxiang Cao"],"url":"https://arxiv.org/abs/2504.19852"}
{"created":"2025-05-01","title":"Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language","abstract":"Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., language masking. Although popular, it requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that this approach performs better than traditional DAPT by 3.5 points of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times less computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.","authors":["Anastasia Zhukova","Christian E. Matt","Terry Ruas","Bela Gipp"],"url":"https://arxiv.org/abs/2504.19856"}
{"created":"2025-05-01","title":"Explanation format does not matter; but explanations do -- An Eggsbert study on explaining Bayesian Optimisation tasks","abstract":"Bayesian Optimisation (BO) is a family of methods for finding optimal parameters when the underlying function to be optimised is unknown. BO is used, for example, for hyperparameter tuning in machine learning and as an expert support tool for tuning cyberphysical systems. For settings where humans are involved in the tuning task, methods have been developed to explain BO (Explainable Bayesian Optimization, XBO). However, there is little guidance on how to present XBO results to humans so that they can tune the system effectively and efficiently. In this paper, we investigate how the XBO explanation format affects users' task performance, task load, understanding and trust in XBO. We chose a task that is accessible to a wide range of users. Specifically, we set up an egg cooking scenario with 6 parameters that participants had to adjust to achieve a perfect soft-boiled egg. We compared three different explanation formats: a bar chart, a list of rules and a textual explanation in a between-subjects online study with 213 participants. Our results show that adding any type of explanation increases task success, reduces the number of trials needed to achieve success, and improves comprehension and confidence. While explanations add more information for participants to process, we found no increase in user task load. We also found that the aforementioned results were independent of the explanation format; all formats had a similar effect. This is an interesting finding for practical applications, as it suggests that explanations can be added to BO tuning tasks without the burden of designing or selecting specific explanation formats. In the future, it would be interesting to investigate scenarios of prolonged use of the explanation formats and whether they have different effects on users' mental models of the underlying system.","authors":["Tanmay Chakraborty","Marion Koelle","J\\\"org Schl\\\"otterer","Nadine Schlicker","Christian Wirth","Christin Seifert"],"url":"https://arxiv.org/abs/2504.20567"}
{"created":"2025-05-01","title":"Protocol Dialects as Formal Patterns: A Composable Theory of Lingos -- Technical report","abstract":"Protocol dialects are methods for modifying protocols that provide light-weight security, especially against easy attacks that can lead to more serious ones. A lingo is a dialect's key security component by making attackers unable to \"speak\" the lingo. A lingo's \"talk\" changes all the time, becoming a moving target for attackers. We present several kinds of lingo transformations and compositions to generate stronger lingos from simpler ones, thus making dialects more secure.","authors":["V\\'ictor Garc\\'ia","Santiago Escobar","Catherine Meadows","Jose Meseguer"],"url":"https://arxiv.org/abs/2504.20637"}
{"created":"2025-05-01","title":"On the Complexity of Finding Small Subgradients in Nonsmooth Optimization","abstract":"We study the oracle complexity of producing $(\\delta,\\epsilon)$-stationary points of Lipschitz functions, in the sense proposed by Zhang et al. [2020]. While there exist dimension-free randomized algorithms for producing such points within $\\widetilde{O}(1/\\delta\\epsilon^3)$ first-order oracle calls, we show that no dimension-free rate can be achieved by a deterministic algorithm. On the other hand, we point out that this rate can be derandomized for smooth functions with merely a logarithmic dependence on the smoothness parameter. Moreover, we establish several lower bounds for this task which hold for any randomized algorithm, with or without convexity. Finally, we show how the convergence rate of finding $(\\delta,\\epsilon)$-stationary points can be improved in case the function is convex, a setting which we motivate by proving that in general no finite time algorithm can produce points with small subgradients even for convex functions.","authors":["Guy Kornowski","Ohad Shamir"],"url":"https://arxiv.org/abs/2209.10346"}
{"created":"2025-05-01","title":"Inferring the Langevin Equation with Uncertainty via Bayesian Neural Networks","abstract":"Pervasive across diverse domains, stochastic systems exhibit fluctuations in processes ranging from molecular dynamics to climate phenomena. The Langevin equation has served as a common mathematical model for studying such systems, enabling predictions of their temporal evolution and analyses of thermodynamic quantities, including absorbed heat, work done on the system, and entropy production. However, inferring the Langevin equation from observed trajectories is a challenging problem, and assessing the uncertainty associated with the inferred equation has yet to be accomplished. In this study, we present a comprehensive framework that employs Bayesian neural networks for inferring Langevin equations in both overdamped and underdamped regimes. Our framework first provides the drift force and diffusion matrix separately and then combines them to construct the Langevin equation. By providing a distribution of predictions instead of a single value, our approach allows us to assess prediction uncertainties, which can help prevent potential misunderstandings and erroneous decisions about the system. We demonstrate the effectiveness of our framework in inferring Langevin equations for various scenarios including a neuron model and microscopic engine, highlighting its versatility and potential impact.","authors":["Youngkyoung Bae","Seungwoong Ha","Hawoong Jeong"],"url":"https://arxiv.org/abs/2402.01338"}
{"created":"2025-05-01","title":"An Introduction to Computational Fluctuating Hydrodynamics","abstract":"These notes are an introduction to fluctuating hydrodynamics (FHD) and the formulation of numerical schemes for the resulting stochastic partial differential equations (PDEs). Fluctuating hydrodynamics was originally introduced by Landau and Lifshitz as a way to put thermal fluctuations into a continuum framework by including a stochastic forcing to each dissipative transport process (e.g., heat flux). While FHD has been useful in modeling transport and fluid dynamics at the mesoscopic scale, theoretical calculations have been feasible only with simplifying assumptions. As such there is great interest in numerical schemes for Computational Fluctuating Hydrodynamics (CFHD). There are a variety of algorithms (e.g., spectral, finite element, lattice Boltzmann) but in this introduction we focus on finite volume schemes. Accompanying these notes is a demonstration program in Python available on GitHub (https://github.com/AlejGarcia/IntroFHD).","authors":["Alejandro L. Garcia","John B. Bell","Andrew Nonaka","Ishan Srivastava","Daniel Ladiges","Changho Kim"],"url":"https://arxiv.org/abs/2406.12157"}
{"created":"2025-05-01","title":"Topological Graph Simplification Solutions to the Street Intersection Miscount Problem","abstract":"Street intersection counts and densities are ubiquitous measures in transport geography and planning. However, typical street network data and typical street network analysis tools can substantially overcount them. This article explains the three main reasons why this happens and presents solutions to each. It contributes algorithms to automatically simplify spatial graphs of urban street networks -- via edge simplification and node consolidation -- resulting in faster parsimonious models and more accurate network measures like intersection counts and densities, street segment lengths, and node degrees. These algorithms' information compression improves downstream graph analytics' memory and runtime efficiency, boosting analytical tractability without loss of model fidelity. Finally, this article validates these algorithms and empirically assesses intersection count biases worldwide to demonstrate the problem's widespread prevalence. Without consolidation, traditional methods would overestimate the median urban area intersection count by 14\\%. However, this bias varies drastically across regions, underscoring these algorithms' importance for consistent comparative empirical analyses.","authors":["Geoff Boeing"],"url":"https://arxiv.org/abs/2407.00258"}
{"created":"2025-05-01","title":"Enhanced Feature Learning via Regularisation: Integrating Neural Networks and Kernel Methods","abstract":"We propose a new method for feature learning and function estimation in supervised learning via regularised empirical risk minimisation. Our approach considers functions as expectations of Sobolev functions over all possible one-dimensional projections of the data. This framework is similar to kernel ridge regression, where the kernel is $\\mathbb{E}_w ( k^{(B)}(w^\\top x,w^\\top x^\\prime))$, with $k^{(B)}(a,b) := \\min(|a|, |b|)\\mathds{1}_{ab>0}$ the Brownian kernel, and the distribution of the projections $w$ is learnt. This can also be viewed as an infinite-width one-hidden layer neural network, optimising the first layer's weights through gradient descent and explicitly adjusting the non-linearity and weights of the second layer. We introduce a gradient-based computational method for the estimator, called Brownian Kernel Neural Network (BKerNN), using particles to approximate the expectation, where the positive homogeneity of the Brownian kernel \\red{leads to improved robustness to local minima}. Using Rademacher complexity, we show that BKerNN's expected risk converges to the minimal risk with explicit high-probability rates of $O( \\min((d/n)^{1/2}, n^{-1/6}))$ (up to logarithmic factors). Numerical experiments confirm our optimisation intuitions, and BKerNN outperforms kernel ridge regression, and favourably compares to a one-hidden layer neural network with ReLU activations in various settings and real data sets.","authors":["Bertille Follain","Francis Bach"],"url":"https://arxiv.org/abs/2407.17280"}
{"created":"2025-05-01","title":"Rigorous Hausdorff dimension estimates for conformal fractals","abstract":"We develop a versatile framework which allows us to rigorously estimate the Hausdorff dimension of maximal conformal graph directed Markov systems in $\\mathbb{R}^n$ for $n \\geq 2$. Our method is based on piecewise linear approximations of the eigenfunctions of the Perron-Frobenius operator via a finite element framework for discretization and iterative mesh schemes. One key element in our approach is obtaining bounds for the derivatives of these eigenfunctions, which, besides being essential for the implementation of our method, are of independent interest.","authors":["Vasileios Chousionis","Dmitriy Leykekhman","Mariusz Urba\\'nski","Erik Wendt"],"url":"https://arxiv.org/abs/2408.06330"}
{"created":"2025-05-01","title":"Addressing Emotion Bias in Music Emotion Recognition and Generation with Frechet Audio Distance","abstract":"The complex nature of musical emotion introduces inherent bias in both recognition and generation, particularly when relying on a single audio encoder, emotion classifier, or evaluation metric. In this work, we conduct a study on Music Emotion Recognition (MER) and Emotional Music Generation (EMG), employing diverse audio encoders alongside Frechet Audio Distance (FAD), a reference-free evaluation metric. Our study begins with a benchmark evaluation of MER, highlighting the limitations of using a single audio encoder and the disparities observed across different measurements. We then propose assessing MER performance using FAD derived from multiple encoders to provide a more objective measure of musical emotion. Furthermore, we introduce an enhanced EMG approach designed to improve both the variability and prominence of generated musical emotion, thereby enhancing its realism. Additionally, we investigate the differences in realism between the emotions conveyed in real and synthetic music, comparing our EMG model against two baseline models. Experimental results underscore the issue of emotion bias in both MER and EMG and demonstrate the potential of using FAD and diverse audio encoders to evaluate musical emotion more objectively and effectively.","authors":["Yuanchao Li","Azalea Gui","Dimitra Emmanouilidou","Hannes Gamper"],"url":"https://arxiv.org/abs/2409.15545"}
{"created":"2025-05-01","title":"Revise, Reason, and Recognize: LLM-Based Emotion Recognition via Emotion-Specific Prompts and ASR Error Correction","abstract":"Annotating and recognizing speech emotion using prompt engineering has recently emerged with the advancement of Large Language Models (LLMs), yet its efficacy and reliability remain questionable. In this paper, we conduct a systematic study on this topic, beginning with the proposal of novel prompts that incorporate emotion-specific knowledge from acoustics, linguistics, and psychology. Subsequently, we examine the effectiveness of LLM-based prompting on Automatic Speech Recognition (ASR) transcription, contrasting it with ground-truth transcription. Furthermore, we propose a Revise-Reason-Recognize prompting pipeline for robust LLM-based emotion recognition from spoken language with ASR errors. Additionally, experiments on context-aware learning, in-context learning, and instruction tuning are performed to examine the usefulness of LLM training schemes in this direction. Finally, we investigate the sensitivity of LLMs to minor prompt variations. Experimental results demonstrate the efficacy of the emotion-specific prompts, ASR error correction, and LLM training schemes for LLM-based emotion recognition. Our study aims to refine the use of LLMs in emotion recognition and related domains.","authors":["Yuanchao Li","Yuan Gong","Chao-Han Huck Yang","Peter Bell","Catherine Lai"],"url":"https://arxiv.org/abs/2409.15551"}
{"created":"2025-05-01","title":"Cross-Lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models","abstract":"Utilizing Self-Supervised Learning (SSL) models for Speech Emotion Recognition (SER) has proven effective, yet limited research has explored cross-lingual scenarios. This study presents a comparative analysis between human performance and SSL models, beginning with a layer-wise analysis and an exploration of parameter-efficient fine-tuning strategies in monolingual, cross-lingual, and transfer learning contexts. We further compare the SER ability of models and humans at both utterance- and segment-levels. Additionally, we investigate the impact of dialect on cross-lingual SER through human evaluation. Our findings reveal that models, with appropriate knowledge transfer, can adapt to the target language and achieve performance comparable to native speakers. We also demonstrate the significant effect of dialect on SER for individuals without prior linguistic and paralinguistic background. Moreover, both humans and models exhibit distinct behaviors across different emotions. These results offer new insights into the cross-lingual SER capabilities of SSL models, underscoring both their similarities to and differences from human emotion perception.","authors":["Zhichen Han","Tianqi Geng","Hui Feng","Jiahong Yuan","Korin Richmond","Yuanchao Li"],"url":"https://arxiv.org/abs/2409.16920"}
{"created":"2025-05-01","title":"Semi-Supervised Cognitive State Classification from Speech with Multi-View Pseudo-Labeling","abstract":"The lack of labeled data is a common challenge in speech classification tasks, particularly those requiring extensive subjective assessment, such as cognitive state classification. In this work, we propose a Semi-Supervised Learning (SSL) framework, introducing a novel multi-view pseudo-labeling method that leverages both acoustic and linguistic characteristics to select the most confident data for training the classification model. Acoustically, unlabeled data are compared to labeled data using the Frechet audio distance, calculated from embeddings generated by multiple audio encoders. Linguistically, large language models are prompted to revise automatic speech recognition transcriptions and predict labels based on our proposed task-specific knowledge. High-confidence data are identified when pseudo-labels from both sources align, while mismatches are treated as low-confidence data. A bimodal classifier is then trained to iteratively label the low-confidence data until a predefined criterion is met. We evaluate our SSL framework on emotion recognition and dementia detection tasks. Experimental results demonstrate that our method achieves competitive performance compared to fully supervised learning using only 30% of the labeled data and significantly outperforms two selected baselines.","authors":["Yuanchao Li","Zixing Zhang","Jing Han","Peter Bell","Catherine Lai"],"url":"https://arxiv.org/abs/2409.16937"}
{"created":"2025-05-01","title":"Exploring Acoustic Similarity in Emotional Speech and Music via Self-Supervised Representations","abstract":"Emotion recognition from speech and music shares similarities due to their acoustic overlap, which has led to interest in transferring knowledge between these domains. However, the shared acoustic cues between speech and music, particularly those encoded by Self-Supervised Learning (SSL) models, remain largely unexplored, given the fact that SSL models for speech and music have rarely been applied in cross-domain research. In this work, we revisit the acoustic similarity between emotion speech and music, starting with an analysis of the layerwise behavior of SSL models for Speech Emotion Recognition (SER) and Music Emotion Recognition (MER). Furthermore, we perform cross-domain adaptation by comparing several approaches in a two-stage fine-tuning process, examining effective ways to utilize music for SER and speech for MER. Lastly, we explore the acoustic similarities between emotional speech and music using Frechet audio distance for individual emotions, uncovering the issue of emotion bias in both speech and music SSL models. Our findings reveal that while speech and music SSL models do capture shared acoustic features, their behaviors can vary depending on different emotions due to their training strategies and domain-specificities. Additionally, parameter-efficient fine-tuning can enhance SER and MER performance by leveraging knowledge from each other. This study provides new insights into the acoustic similarity between emotional speech and music, and highlights the potential for cross-domain generalization to improve SER and MER systems.","authors":["Yujia Sun","Zeyu Zhao","Korin Richmond","Yuanchao Li"],"url":"https://arxiv.org/abs/2409.17899"}
{"created":"2025-05-01","title":"Asymmetry of the Relative Entropy in the Regularization of Empirical Risk Minimization","abstract":"The effect of relative entropy asymmetry is analyzed in the context of empirical risk minimization (ERM) with relative entropy regularization (ERM-RER). Two regularizations are considered: $(a)$ the relative entropy of the measure to be optimized with respect to a reference measure (Type-I ERM-RER); and $(b)$ the relative entropy of the reference measure with respect to the measure to be optimized (Type-II ERM-RER). The main result is the characterization of the solution to the Type-II ERM-RER problem and its key properties. By comparing the well-understood Type-I ERM-RER with Type-II ERM-RER, the effects of entropy asymmetry are highlighted. The analysis shows that in both cases, regularization by relative entropy forces the solution's support to collapse into the support of the reference measure, introducing a strong inductive bias that negates the evidence provided by the training data. Finally, it is shown that Type-II regularization is equivalent to Type-I regularization with an appropriate transformation of the empirical risk function.","authors":["Francisco Daunas","I\\~naki Esnaola","Samir M. Perlaza","H. Vincent Poor"],"url":"https://arxiv.org/abs/2410.02833"}
{"created":"2025-05-01","title":"Leveraging Internet Principles to Build a Quantum Network","abstract":"Designing an operational architecture for the Quantum Internet is challenging in light of both fundamental limits imposed by physics laws and technological constraints. Here, we propose a method to abstract away most of the quantum-specific elements and formulate a best-effort quantum network architecture based on packet switching, akin to that of the classical Internet. This reframing provides an opportunity to exploit the many available and well-understood protocols within the Internet context. As an illustration, we tailor and adapt classical congestion control and active queue management protocols to quantum networks, employing an architecture wherein quantum end and intermediate nodes effectively regulate demand and resource utilization, respectively. Results show that these classical networking tools can be effective in managing quantum memory decoherence and maintaining end-to-end fidelity around a target value.","authors":["Leonardo Bacciottini","Matheus Guedes De Andrade","Shahrooz Pouryousef","Emily A. Van Milligen","Aparimit Chandra","Nitish K. Panigrahy","Nageswara S. V. Rao","Gayane Vardoyan","Don Towsley"],"url":"https://arxiv.org/abs/2410.08980"}
{"created":"2025-05-01","title":"How to Construct Random Unitaries","abstract":"The existence of pseudorandom unitaries (PRUs) -- efficient quantum circuits that are computationally indistinguishable from Haar-random unitaries -- has been a central open question, with significant implications for cryptography, complexity theory, and fundamental physics. In this work, we close this question by proving that PRUs exist, assuming that any quantum-secure one-way function exists. We establish this result for both (1) the standard notion of PRUs, which are secure against any efficient adversary that makes queries to the unitary $U$, and (2) a stronger notion of PRUs, which are secure even against adversaries that can query both the unitary $U$ and its inverse $U^\\dagger$. In the process, we prove that any algorithm that makes queries to a Haar-random unitary can be efficiently simulated on a quantum computer, up to inverse-exponential trace distance.","authors":["Fermi Ma","Hsin-Yuan Huang"],"url":"https://arxiv.org/abs/2410.10116"}
{"created":"2025-05-01","title":"The internal languages of univalent categories","abstract":"Internal language theorems are fundamental in categorical logic, since they express an equivalence between syntax and semantics. One of such theorems was proven by Clairambault and Dybjer, who corrected the result originally by Seely. More specifically, they constructed a biequivalence between the bicategory of locally Cartesian closed categories and the bicategory of democratic categories with families with extensional identity types, $\\sum$-types, and $\\prod$-types. This theorem expresses that the internal language of locally Cartesian closed categories is extensional Martin-L\\\"of type theory with dependent sums and products. In this paper, we study the theorem by Clairambault and Dybjer for univalent categories, and we extend it to various classes of toposes, among which are $\\prod$-pretoposes and elementary toposes. The results in this paper have been formalized using the proof assistant Rocq and the UniMath library.","authors":["Niels van der Weide"],"url":"https://arxiv.org/abs/2411.06636"}
{"created":"2025-05-01","title":"CAD-Unet: A Capsule Network-Enhanced Unet Architecture for Accurate Segmentation of COVID-19 Lung Infections from CT Images","abstract":"Since the outbreak of the COVID-19 pandemic in 2019, medical imaging has emerged as a primary modality for diagnosing COVID-19 pneumonia. In clinical settings, the segmentation of lung infections from computed tomography images enables rapid and accurate quantification and diagnosis of COVID-19. Segmentation of COVID-19 infections in the lungs poses a formidable challenge, primarily due to the indistinct boundaries and limited contrast presented by ground glass opacity manifestations. Moreover, the confounding similarity between infiltrates, lung tissues, and lung walls further complicates this segmentation task. To address these challenges, this paper introduces a novel deep network architecture, called CAD-Unet, for segmenting COVID-19 lung infections. In this architecture, capsule networks are incorporated into the existing Unet framework. Capsule networks represent a novel network architecture that differs from traditional convolutional neural networks. They utilize vectors for information transfer among capsules, facilitating the extraction of intricate lesion spatial information. Additionally, we design a capsule encoder path and establish a coupling path between the unet encoder and the capsule encoder. This design maximizes the complementary advantages of both network structures while achieving efficient information fusion. \\noindent Finally, extensive experiments are conducted on four publicly available datasets, encompassing binary segmentation tasks and multi-class segmentation tasks. The experimental results demonstrate the superior segmentation performance of the proposed model. The code has been released at: https://github.com/AmanoTooko-jie/CAD-Unet.","authors":["Yijie Dang","Weijun Ma","Xiaohu Luo","Huaizhu Wang"],"url":"https://arxiv.org/abs/2412.06314"}
{"created":"2025-05-01","title":"Data-driven Discovery of Biophysical T Cell Receptor Co-specificity Rules","abstract":"The biophysical interactions between the T cell receptor (TCR) and its ligands determine the specificity of the cellular immune response. However, the immense diversity of receptors and ligands has made it challenging to discover generalizable rules across the distinct binding affinity landscapes created by different ligands. Here, we present an optimization framework for discovering biophysical rules that predict whether TCRs share specificity to a ligand. Applying this framework to TCRs associated with a collection of SARS-CoV-2 peptides we systematically characterize how co-specificity depends on the type and position of amino-acid differences between receptors. We also demonstrate that the inferred rules generalize to ligands highly dissimilar to any seen during training. Our analysis reveals that matching of steric properties between substituted amino acids is more important for receptor co-specificity red than the hydrophobic properties that prominently determine evolutionary substitutability. Our analysis also quantifies the substantial importance of positions not in direct contact with the peptide for specificity. These findings highlight the potential for data-driven approaches to uncover the molecular mechanisms underpinning the specificity of adaptive immune responses.","authors":["Andrew G. T. Pyo","Yuta Nagano","Martina Milighetti","James Henderson","Curtis G. Callan Jr.","Benny Chain","Ned S. Wingreen","Andreas Tiffeau-Mayer"],"url":"https://arxiv.org/abs/2412.13722"}
{"created":"2025-05-01","title":"FleSpeech: Flexibly Controllable Speech Generation with Various Prompts","abstract":"Controllable speech generation methods typically rely on single or fixed prompts, hindering creativity and flexibility. These limitations make it difficult to meet specific user needs in certain scenarios, such as adjusting the style while preserving a selected speaker's timbre, or choosing a style and generating a voice that matches a character's visual appearance. To overcome these challenges, we propose \\textit{FleSpeech}, a novel multi-stage speech generation framework that allows for more flexible manipulation of speech attributes by integrating various forms of control. FleSpeech employs a multimodal prompt encoder that processes and unifies different text, audio, and visual prompts into a cohesive representation. This approach enhances the adaptability of speech synthesis and supports creative and precise control over the generated speech. Additionally, we develop a data collection pipeline for multimodal datasets to facilitate further research and applications in this field. Comprehensive subjective and objective experiments demonstrate the effectiveness of FleSpeech. Audio samples are available at https://kkksuper.github.io/FleSpeech/","authors":["Hanzhao Li","Yuke Li","Xinsheng Wang","Jingbin Hu","Qicong Xie","Shan Yang","Lei Xie"],"url":"https://arxiv.org/abs/2501.04644"}
{"created":"2025-05-01","title":"Estimation-Aware Trajectory Optimization with Set-Valued Measurement Uncertainties","abstract":"In this paper, an optimization-based framework for generating estimation-aware trajectories is presented. In this setup, measurement (output) uncertainties are state-dependent and set-valued. Enveloping ellipsoids are employed to characterize state-dependent uncertainties with unknown distributions. The concept of regularity for set-valued output maps is then introduced, facilitating the formulation of the estimation-aware trajectory generation problem. Specifically, it is demonstrated that for output-regular maps, one can utilize a set-valued observability measure that is concave with respect to the finite horizon state trajectories. By maximizing this measure, estimation-aware trajectories can then be synthesized for a broad class of systems. Trajectory planning routines are also examined in this work, by which the observability measure is optimized for systems with locally linearized dynamics. To illustrate the effectiveness of the proposed approach, representative examples in the context of trajectory planning with vision-based estimation are presented. Moreover, the paper presents estimation-aware planning for an uncooperative Target-Rendezvous problem, where an Ego-satellite employs an onboard machine learning (ML)-based estimation module to realize the rendezvous trajectory.","authors":["Aditya Deole","Mehran Mesbahi"],"url":"https://arxiv.org/abs/2501.09192"}
{"created":"2025-05-01","title":"Zero-determinant strategies in repeated continuously-relaxed games","abstract":"Mixed extension has played an important role in game theory, especially in the proof of the existence of Nash equilibria in strategic form games. Mixed extension can be regarded as continuous relaxation of a strategic form game. Recently, in repeated games, a class of behavior strategies, called zero-determinant strategies, was introduced. Zero-determinant strategies control payoffs of players by unilaterally enforcing linear relations between payoffs. There are many attempts to extend zero-determinant strategies so as to apply them to broader situations. Here, we extend zero-determinant strategies to repeated games where action sets of players in stage game are continuously relaxed. We see that continuous relaxation broadens the range of possible zero-determinant strategies, compared to the original repeated games. Furthermore, we introduce a special type of zero-determinant strategies, called one-point zero-determinant strategies, which repeat only one continuously-relaxed action in all rounds. By investigating several examples, we show that some property of mixed-strategy Nash equilibria can be reinterpreted as a payoff-control property of one-point zero-determinant strategies.","authors":["Masahiko Ueda","Ayaka Fujita"],"url":"https://arxiv.org/abs/2501.11219"}
{"created":"2025-05-01","title":"Comparative Analysis of FPGA and GPU Performance for Machine Learning-Based Track Reconstruction at LHCb","abstract":"In high-energy physics, the increasing luminosity and detector granularity at the Large Hadron Collider are driving the need for more efficient data processing solutions. Machine Learning has emerged as a promising tool for reconstructing charged particle tracks, due to its potentially linear computational scaling with detector hits. The recent implementation of a graph neural network-based track reconstruction pipeline in the first level trigger of the LHCb experiment on GPUs serves as a platform for comparative studies between computational architectures in the context of high-energy physics. This paper presents a novel comparison of the throughput of ML model inference between FPGAs and GPUs, focusing on the first step of the track reconstruction pipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using HLS4ML for FPGA deployment, we benchmark its performance against the GPU implementation and demonstrate the potential of FPGAs for high-throughput, low-latency inference without the need for an expertise in FPGA development and while consuming significantly less power.","authors":["Fotis I. Giasemis","Vladimir Lon\\v{c}ar","Bertrand Granado","Vladimir Vava Gligorov"],"url":"https://arxiv.org/abs/2502.02304"}
{"created":"2025-05-01","title":"WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry","abstract":"Crystalline materials often exhibit a high level of symmetry. However, most generative models do not account for symmetry, but rather model each atom without any constraints on its position or element. We propose a generative model, Wyckoff Diffusion (WyckoffDiff), which generates symmetry-based descriptions of crystals. This is enabled by considering a crystal structure representation that encodes all symmetry, and we design a novel neural network architecture which enables using this representation inside a discrete generative model framework. In addition to respecting symmetry by construction, the discrete nature of our model enables fast generation. We additionally present a new metric, Fr\\'echet Wrenformer Distance, which captures the symmetry aspects of the materials generated, and we benchmark WyckoffDiff against recently proposed generative models for crystal generation. Code is available online at https://github.com/httk/wyckoffdiff","authors":["Filip Ekstr\\\"om Kelvinius","Oskar B. Andersson","Abhijith S. Parackal","Dong Qian","Rickard Armiento","Fredrik Lindsten"],"url":"https://arxiv.org/abs/2502.06485"}
{"created":"2025-05-01","title":"Practical classical error correction for parity-encoded spin systems","abstract":"Quantum annealing (QA) has emerged as a promising candidate for fast solvers for combinatorial optimization problems (COPs) and has attracted the interest of many researchers. Since COP is logically encoded in the Ising interaction among spins, its realization necessitates a spin system with all-to-all connectivity, presenting technical challenges in the physical implementation of large-scale QA devices. W. Lechner, P. Hauke, and P. Zoller proposed a parity-encoding (PE) architecture consisting of an expanded spin system with only local connectivity among them to circumvent this difficulty in developing near-future QA devices. They suggested that this architecture not only alleviates implementation challenges and enhances scalability but also possesses intrinsic fault tolerance. This paper proposes a practical decoding method tailored to correlated spin-flip errors in spin readout of PE architecture. Our work is based on the close connection between PE architecture and classical low-density parity-check (LDPC) codes. We show that the bit-flip (BF) decoding algorithm can correct independent and identically distributed errors in the readout of the SLHZ system with comparable performance to the belief propagation (BP) decoding algorithm. Then, we show evidence that the proposed BF decoding algorithm can efficiently correct correlated spinflip errors by simulation. The result suggests that introducing post-readout BF decoding reduces the computational cost of QA using the PE architecture and improves the performance of global optimal solution search. Our results emphasize the importance of the proper selection of decoding algorithms to exploit the inherent fault tolerance potential of the PE architecture.","authors":["Yoshihiro Nambu"],"url":"https://arxiv.org/abs/2502.07170"}
{"created":"2025-05-01","title":"Advancing Precision Oncology Through Modeling of Longitudinal and Multimodal Data","abstract":"Cancer evolves continuously over time through a complex interplay of genetic, epigenetic, microenvironmental, and phenotypic changes. This dynamic behavior drives uncontrolled cell growth, metastasis, immune evasion, and therapy resistance, posing challenges for effective monitoring and treatment. However, today's data-driven research in oncology has primarily focused on cross-sectional analysis using data from a single modality, limiting the ability to fully characterize and interpret the disease's dynamic heterogeneity. Advances in multiscale data collection and computational methods now enable the discovery of longitudinal multimodal biomarkers for precision oncology. Longitudinal data reveal patterns of disease progression and treatment response that are not evident from single-timepoint data, enabling timely abnormality detection and dynamic treatment adaptation. Multimodal data integration offers complementary information from diverse sources for more precise risk assessment and targeting of cancer therapy. In this review, we survey methods of longitudinal and multimodal modeling, highlighting their synergy in providing multifaceted insights for personalized care tailored to the unique characteristics of a patient's cancer. We summarize the current challenges and future directions of longitudinal multimodal analysis in advancing precision oncology.","authors":["Luoting Zhuang","Stephen H. Park","Steven J. Skates","Ashley E. Prosper","Denise R. Aberle","William Hsu"],"url":"https://arxiv.org/abs/2502.07836"}
{"created":"2025-05-01","title":"BCDDM: Branch-Corrected Denoising Diffusion Model for Black Hole Image Generation","abstract":"The properties of black holes and accretion flows can be inferred by fitting Event Horizon Telescope (EHT) data to simulated images generated through general relativistic ray tracing (GRRT). However, due to the computationally intensive nature of GRRT, the efficiency of generating specific radiation flux images needs to be improved. This paper introduces the Branch Correction Denoising Diffusion Model (BCDDM), which uses a branch correction mechanism and a weighted mixed loss function to improve the accuracy of generated black hole images based on seven physical parameters of the radiatively inefficient accretion flow (RIAF) model. Our experiments show a strong correlation between the generated images and their physical parameters. By enhancing the GRRT dataset with BCDDM-generated images and using ResNet50 for parameter regression, we achieve significant improvements in parameter prediction performance. This approach reduces computational costs and provides a faster, more efficient method for dataset expansion, parameter estimation, and model fitting.","authors":["Ao liu","Zelin Zhang","Songbai Chen","Cuihong Wen","Jieci Wang"],"url":"https://arxiv.org/abs/2502.08528"}
{"created":"2025-05-01","title":"Benefits of Mutual Coupling in Dynamic Metasurface Antennas for Optimizing Wireless Communications -- Theory and Experimental Validation","abstract":"Dynamic metasurface antennas (DMAs) are a promising embodiment of next-generation reconfigurable antenna technology to realize base stations and access points with reduced cost and power consumption. A DMA is a thin structure patterned on its front with reconfigurable radiating metamaterial elements (meta-atoms) that are excited by waveguides or cavities. Mutual coupling between the meta-atoms can result in a strongly non-linear dependence of the DMA's radiation pattern on the configuration of its meta-atoms. However, besides the obvious algorithmic challenges of working with physics-compliant DMA models, it remains unclear how mutual coupling in DMAs influences the ability to achieve a desired wireless functionality. In this paper, we provide theoretical, numerical and experimental evidence that strong mutual coupling in DMAs increases the radiation pattern sensitivity to the DMA configuration and thereby boosts the available control over the radiation pattern, improving the ability to tailor the radiation pattern to the requirements of a desired wireless functionality. Counterintuitively, we hence encourage next-generation DMA implementations to enhance (rather than suppress) mutual coupling, in combination with suitable physics-compliant modeling and optimization. We expect the unveiled mechanism by which mutual coupling boosts the radiation pattern control to also apply to other reconfigurable antenna systems based on tunable lumped elements.","authors":["Hugo Prod'homme","Jean Tapie","Luc Le Magoarou","Philipp del Hougne"],"url":"https://arxiv.org/abs/2502.15565"}
{"created":"2025-05-01","title":"Hamiltonian Learning at Heisenberg Limit for Hybrid Quantum Systems","abstract":"Hybrid quantum systems with different particle species are fundamental in quantum materials and quantum information science. In this work, we establish a rigorous theoretical framework proving that, given access to an unknown spin-boson type Hamiltonian, our algorithm achieves Heisenberg-limited estimation for all coupling parameters up to error $\\epsilon$ with a total evolution time ${O}(\\epsilon^{-1})$ using only ${O}({\\rm polylog}(\\epsilon^{-1}))$ measurements. It is also robust against small state preparation and measurement errors. In addition, we provide an alternative algorithm based on distributed quantum sensing, which significantly reduces the evolution time per measurement. To validate our method, we demonstrate its efficiency in hybrid Hamiltonian learning and spectrum learning, with broad applications in AMO, condensed matter and high energy physics. Our results provide a scalable and robust framework for precision Hamiltonian characterization in hybrid quantum platforms.","authors":["Lixing Zhang","Ze-Xun Lin","Prineha Narang","Di Luo"],"url":"https://arxiv.org/abs/2502.20373"}
{"created":"2025-05-01","title":"ScaleFusionNet: Transformer-Guided Multi-Scale Feature Fusion for Skin Lesion Segmentation","abstract":"Melanoma is a malignant tumor originating from skin cell lesions. Accurate and efficient segmentation of skin lesions is essential for quantitative medical analysis but remains challenging. To address this, we propose ScaleFusionNet, a segmentation model that integrates Cross-Attention Transformer Module (CATM) and AdaptiveFusionBlock to enhance feature extraction and fusion. The model employs a hybrid architecture encoder that effectively captures both local and global features. We introduce CATM, which utilizes Swin Transformer Blocks and Cross Attention Fusion (CAF) to adaptively refine encoder-decoder feature fusion, reducing semantic gaps and improving segmentation accuracy. Additionally, the AdaptiveFusionBlock is improved by integrating adaptive multi-scale fusion, where Swin Transformer-based attention complements deformable convolution-based multi-scale feature extraction. This enhancement refines lesion boundaries and preserves fine-grained details. ScaleFusionNet achieves Dice scores of 92.94% and 91.65% on ISIC-2016 and ISIC-2018 datasets, respectively, demonstrating its effectiveness in skin lesion analysis. Our code implementation is publicly available at GitHub.","authors":["Saqib Qamar","Syed Furqan Qadri","Roobaea Alroobaea","Goram Mufarah M Alshmrani","Richard Jiang"],"url":"https://arxiv.org/abs/2503.03327"}
{"created":"2025-05-01","title":"Analytic adjoint solution for incompressible potential flows","abstract":"We obtain the analytic adjoint solution for two-dimensional (2D) incompressible potential flow for a cost function measuring aerodynamic force using the connection of the adjoint approach to Green's functions and also by establishing and exploiting its relation to the adjoint incompressible Euler equations. By comparison with the analytic solution, it is shown that the naive approach based on solving Laplace's equation for the adjoint variables can be ill-defined. The analysis of the boundary behavior of the analytic solution is used to discuss the proper formulation of the adjoint problem as well as the mechanism for incorporating the Kutta condition in the adjoint formulation","authors":["Carlos Lozano","Jorge Ponsin"],"url":"https://arxiv.org/abs/2503.15121"}
{"created":"2025-05-01","title":"On Supports for graphs of bounded genus","abstract":"Let $(X,\\mathcal{E})$ be a hypergraph. A support is a graph $Q$ on $X$ such that for each $E\\in\\mathcal{E}$, the subgraph of $Q$ induced on the elements in $E$ is connected. We consider the problem of constructing a support for hypergraphs defined by connected subgraphs of a host graph. For a graph $G=(V,E)$, let $\\mathcal{H}$ be a set of connected subgraphs of $G$. Let the vertices of $G$ be partitioned into two sets the \\emph{terminals} $\\mathbf{b}(V)$ and the \\emph{non-terminals} $\\mathbf{r}(V)$. We define a hypergraph on $\\mathbf{b}(V)$, where each $H\\in\\mathcal{H}$ defines a hyperedge consisting of the vertices of $\\mathbf{b}(V)$ in $H$.","authors":["Rajiv Raman","Karamjeet Singh"],"url":"https://arxiv.org/abs/2503.21287"}
{"created":"2025-05-01","title":"Markovian Continuity of the MMSE","abstract":"Minimum mean square error (MMSE) estimation is widely used in signal processing and related fields. While it is known to be non-continuous with respect to all standard notions of stochastic convergence, it remains robust in practical applications. In this work, we review the known counterexamples to the continuity of the MMSE. We observe that, in these counterexamples, the discontinuity arises from an element in the converging measurement sequence providing more information about the estimand than the limit of the measurement sequence. We argue that this behavior is uncharacteristic of real-world applications and introduce a new stochastic convergence notion, termed Markovian convergence, to address this issue. We prove that the MMSE is, in fact, continuous under this new notion. We supplement this result with semi-continuity and continuity guarantees of the MMSE in other settings and prove the continuity of the MMSE under linear estimation.","authors":["Elad Domanovitz","Anatoly Khina"],"url":"https://arxiv.org/abs/2504.14659"}
{"created":"2025-05-01","title":"Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation","abstract":"Audio-visual speech recognition (AVSR) incorporates auditory and visual modalities to improve recognition accuracy, particularly in noisy environments where audio-only speech systems are insufficient. While previous research has largely addressed audio disruptions, few studies have dealt with visual corruptions, e.g., lip occlusions or blurred videos, which are also detrimental. To address this real-world challenge, we propose CAV2vec, a novel self-supervised speech representation learning framework particularly designed to handle audio-visual joint corruption. CAV2vec employs a self-distillation approach with a corrupted prediction task, where the student model learns to predict clean targets, generated by the teacher model, with corrupted input frames. Specifically, we suggest a unimodal multi-task learning, which distills cross-modal knowledge and aligns the corrupted modalities, by predicting clean audio targets with corrupted videos, and clean video targets with corrupted audios. This strategy mitigates the dispersion in the representation space caused by corrupted modalities, leading to more reliable and robust audio-visual fusion. Our experiments on robust AVSR benchmarks demonstrate that the corrupted representation learning method significantly enhances recognition accuracy across generalized environments involving various types of corruption. Our code is available at https://github.com/sungnyun/cav2vec.","authors":["Sungnyun Kim","Sungwoo Cho","Sangmin Bae","Kangwook Jang","Se-Young Yun"],"url":"https://arxiv.org/abs/2504.18539"}
{"created":"2025-05-01","title":"A Preliminary Investigation on the Usage of Quantum Approximate Optimization Algorithms for Test Case Selection","abstract":"Regression testing is key in verifying that software works correctly after changes. However, running the entire regression test suite can be impractical and expensive, especially for large-scale systems. Test suite optimization methods are highly effective but often become infeasible due to their high computational demands. In previous work, Trovato et al. proposed SelectQA, an approach based on quantum annealing that outperforms the traditional state-of-the-art methods, i.e., Additional Greedy and DIV-GA, in efficiency. This work envisions the usage of Quantum Approximate Optimization Algorithms (QAOAs) for test case selection by proposing QAOA-TCS. QAOAs merge the potential of gate-based quantum machines with the optimization capabilities of the adiabatic evolution. To prove the effectiveness of QAOAs for test case selection, we preliminarily investigate QAOA-TCS leveraging an ideal environment simulation before evaluating it on real quantum machines. Our results show that QAOAs perform better than the baseline algorithms in effectiveness while being comparable to SelectQA in terms of efficiency. These results encourage us to continue our experimentation with noisy environment simulations and real quantum machines.","authors":["Antonio Trovato","Martin Beseda","Dario Di Nucci"],"url":"https://arxiv.org/abs/2504.18955"}
{"created":"2025-05-01","title":"Contextual Online Uncertainty-Aware Preference Learning for Human Feedback","abstract":"Reinforcement Learning from Human Feedback (RLHF) has become a pivotal paradigm in artificial intelligence to align large models with human preferences. In this paper, we propose a novel statistical framework to simultaneously conduct the online decision-making and statistical inference on the optimal model using human preference data based on dynamic contextual information. Our approach introduces an efficient decision strategy that achieves both the optimal regret bound and the asymptotic distribution of the estimators. A key challenge in RLHF is handling the dependent online human preference outcomes with dynamic contexts. To address this, in the methodological aspect, we propose a two-stage algorithm starting with $\\epsilon$-greedy followed by exploitations; in the theoretical aspect, we tailor anti-concentration inequalities and matrix martingale concentration techniques to derive the uniform estimation rate and asymptotic normality of the estimators using dependent samples from both stages. Extensive simulation results demonstrate that our method outperforms state-of-the-art strategies. We apply the proposed framework to analyze the human preference data for ranking large language models on the Massive Multitask Language Understanding dataset, yielding insightful results on the performance of different large language models for medical anatomy knowledge.","authors":["Nan Lu","Ethan X. Fang","Junwei Lu"],"url":"https://arxiv.org/abs/2504.19342"}
