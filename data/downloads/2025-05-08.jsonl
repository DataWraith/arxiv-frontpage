{"created":"2025-05-08","title":"Pseudo Random Number Generator using Internet-of-Things Techniques on Portable Field-Programmable-Gate-Array Platform","abstract":"This paper conducts a comparative study of three IoT-based PRNG models, including Logistic Map, Double Pendulum, and Multi-LFSR, implemented on an FPGA platform. Comparisons are made across key performance metrics like randomness, latency, power consumption, hardware resource usage, energy efficiency, scalability, and application suitability. Compared to Multi-LFSR, Logistic Map, and Double Pendulum Models provide perfect quality randomness, which is quite apt for high-security grade applications; however, the requirements of these models concerning power and hardware resources are also considerably high. By contrast, the Multi-LFSR comes into its own due to its lower latency, power consumption, and resource-efficient design. It is, therefore, suited for embedded or real-time applications. Furthermore, environmental sensors will also be introduced as entropy sources for the PRNGs to enhance the randomness of the systems, particularly in IoT-enabled battery-powered FPGA platforms. The experimental results confirm that the Multi-LFSR model has the highest energy efficiency, while the Logistic Map and Double Pendulum outperform in generating numbers with very high security. The study thus provides a deeper insight into decision- making for selecting PRNG models.","authors":["Tee Hui Teo"],"url":"https://arxiv.org/abs/2505.03741"}
{"created":"2025-05-08","title":"Hardware-Enabled Mechanisms for Verifying Responsible AI Development","abstract":"Advancements in AI capabilities, driven in large part by scaling up computing resources used for AI training, have created opportunities to address major global challenges but also pose risks of misuse. Hardware-enabled mechanisms (HEMs) can support responsible AI development by enabling verifiable reporting of key properties of AI training activities such as quantity of compute used, training cluster configuration or location, as well as policy enforcement. Such tools can promote transparency and improve security, while addressing privacy and intellectual property concerns. Based on insights from an interdisciplinary workshop, we identify open questions regarding potential implementation approaches, emphasizing the need for further research to ensure robust, scalable solutions.","authors":["Aidan O'Gara","Gabriel Kulp","Will Hodgkins","James Petrie","Vincent Immler","Aydin Aysu","Kanad Basu","Shivam Bhasin","Stjepan Picek","Ankur Srivastava"],"url":"https://arxiv.org/abs/2505.03742"}
{"created":"2025-05-08","title":"Implementation of Shor Algorithm: Factoring a 4096-Bit Integer Under Specific Constraints","abstract":"In recent years, advancements in quantum chip technology, such as Willow, have contributed to reducing quantum computation error rates, potentially accelerating the practical adoption of quantum computing. As a result, the design of quantum algorithms suitable for real-world applications has become a crucial research direction. This study focuses on the implementation of Shor algorithm, aiming to improve modular computation efficiency and demonstrate the factorization of a 4096-bit integer under specific constraints. Experimental results, when compared with state-of-the-art (SOTA) methods, indicate a significant improvement in efficiency while enabling the factorization of longer integers.","authors":["Abel C. H. Chen"],"url":"https://arxiv.org/abs/2505.03743"}
{"created":"2025-05-08","title":"AccLLM: Accelerating Long-Context LLM Inference Via Algorithm-Hardware Co-Design","abstract":"Recently, large language models (LLMs) have achieved huge success in the natural language processing (NLP) field, driving a growing demand to extend their deployment from the cloud to edge devices. However, deploying LLMs on resource-constrained edge devices poses significant challenges, including (1) intensive computations and huge model sizes, (2) great memory and bandwidth demands introduced by the autoregressive generation process, and (3) limited scalability for handling long sequences. To address these challenges, we propose AccLLM, a comprehensive acceleration framework that enables efficient and fast long-context LLM inference through algorithm and hardware co-design. At the algorithmic level, we integrate (1) pruning, (2) {\\Lambda}-shaped attention, and (3) an innovative W2A8KV4 (2-bit weights, 8-bit activations, and 4-bit KV cache) quantization scheme, thus effectively reducing memory and bandwidth requirements while facilitating LLMs' long-sequence generation. At the hardware level, we design a dedicated FPGA-based accelerator with a reconfigurable computing engine to effectively and flexibly accommodate diverse operations arising from our compression algorithm, thereby fully translating the algorithmic innovations into tangible hardware efficiency. We validate AccLLM on the Xilinx Alveo U280 FPGA, demonstrating a 4.07x energy efficiency and a 2.98x throughput compared to the state-of-the-art work FlightLLM.","authors":["Yanbiao Liang","Huihong Shi","Haikuo Shao","Zhongfeng Wang"],"url":"https://arxiv.org/abs/2505.03745"}
{"created":"2025-05-08","title":"Promoting Security and Trust on Social Networks: Explainable Cyberbullying Detection Using Large Language Models in a Stream-Based Machine Learning Framework","abstract":"Social media platforms enable instant and ubiquitous connectivity and are essential to social interaction and communication in our technological society. Apart from its advantages, these platforms have given rise to negative behaviors in the online community, the so-called cyberbullying. Despite the many works involving generative Artificial Intelligence (AI) in the literature lately, there remain opportunities to study its performance apart from zero/few-shot learning strategies. Accordingly, we propose an innovative and real-time solution for cyberbullying detection that leverages stream-based Machine Learning (ML) models able to process the incoming samples incrementally and Large Language Models (LLMS) for feature engineering to address the evolving nature of abusive and hate speech online. An explainability dashboard is provided to promote the system's trustworthiness, reliability, and accountability. Results on experimental data report promising performance close to 90 % in all evaluation metrics and surpassing those obtained by competing works in the literature. Ultimately, our proposal contributes to the safety of online communities by timely detecting abusive behavior to prevent long-lasting harassment and reduce the negative consequences in society.","authors":["Silvia Garc\\'ia-M\\'endez","Francisco De Arriba-P\\'erez"],"url":"https://arxiv.org/abs/2505.03746"}
{"created":"2025-05-08","title":"APSQ: Additive Partial Sum Quantization with Algorithm-Hardware Co-Design","abstract":"DNN accelerators, significantly advanced by model compression and specialized dataflow techniques, have marked considerable progress. However, the frequent access of high-precision partial sums (PSUMs) leads to excessive memory demands in architectures utilizing input/weight stationary dataflows. Traditional compression strategies have typically overlooked PSUM quantization, which may account for 69% of power consumption. This study introduces a novel Additive Partial Sum Quantization (APSQ) method, seamlessly integrating PSUM accumulation into the quantization framework. A grouping strategy that combines APSQ with PSUM quantization enhanced by a reconfigurable architecture is further proposed. The APSQ performs nearly lossless on NLP and CV tasks across BERT, Segformer, and EfficientViT models while compressing PSUMs to INT8. This leads to a notable reduction in energy costs by 28-87%. Extended experiments on LLaMA2-7B demonstrate the potential of APSQ for large language models. Code is available at https://github.com/Yonghao-Tan/APSQ.","authors":["Yonghao Tan","Pingcheng Dong","Yongkun Wu","Yu Liu","Xuejiao Liu","Peng Luo","Shih-Yang Liu","Xijie Huang","Dong Zhang","Luhong Liang","Kwang-Ting Cheng"],"url":"https://arxiv.org/abs/2505.03748"}
{"created":"2025-05-08","title":"AI-Powered Agile Analog Circuit Design and Optimization","abstract":"Artificial intelligence (AI) techniques are transforming analog circuit design by automating device-level tuning and enabling system-level co-optimization. This paper integrates two approaches: (1) AI-assisted transistor sizing using Multi-Objective Bayesian Optimization (MOBO) for direct circuit parameter optimization, demonstrated on a linearly tunable transconductor; and (2) AI-integrated circuit transfer function modeling for system-level optimization in a keyword spotting (KWS) application, demonstrated by optimizing an analog bandpass filter within a machine learning training loop. The combined insights highlight how AI can improve analog performance, reduce design iteration effort, and jointly optimize analog components and application-level metrics.","authors":["Jinhai Hu","Wang Ling Goh","Yuan Gao"],"url":"https://arxiv.org/abs/2505.03750"}
{"created":"2025-05-08","title":"Improving the Serving Performance of Multi-LoRA Large Language Models via Efficient LoRA and KV Cache Management","abstract":"Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for task-specific Large Language Model (LLM) applications. For multi-LoRA serving, caching hot KV caches and LoRA adapters in high bandwidth memory of accelerations can improve inference performance. However, existing Multi-LoRA inference systems fail to optimize serving performance like Time-To-First-Toke (TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving performance. FASTLIBRA comprises a dependency-aware cache manager and a performance-driven cache swapper. The cache manager maintains the usage dependencies between LoRAs and KV caches during the inference with a unified caching pool. The cache swapper determines the swap-in or out of LoRAs and KV caches based on a unified cost model, when the HBM is idle or busy, respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on average, compared to state-of-the-art works.","authors":["Hang Zhang","Jiuchen Shi","Yixiao Wang","Quan Chen","Yizhou Shan","Minyi Guo"],"url":"https://arxiv.org/abs/2505.03756"}
{"created":"2025-05-08","title":"Soft yet Effective Robots via Holistic Co-Design","abstract":"Soft robots promise inherent safety via their material compliance for seamless interactions with humans or delicate environments. Yet, their development is challenging because it requires integrating materials, geometry, actuation, and autonomy into complex mechatronic systems. Despite progress, the field struggles to balance task-specific performance with broader factors like durability and manufacturability - a difficulty that we find is compounded by traditional sequential design processes with their lack of feedback loops. In this perspective, we review emerging co-design approaches that simultaneously optimize the body and brain, enabling the discovery of unconventional designs highly tailored to the given tasks. We then identify three key shortcomings that limit the broader adoption of such co-design methods within the soft robotics domain. First, many rely on simulation-based evaluations focusing on a single metric, while real-world designs must satisfy diverse criteria. Second, current methods emphasize computational modeling without ensuring feasible realization, risking sim-to-real performance gaps. Third, high computational demands limit the exploration of the complete design space. Finally, we propose a holistic co-design framework that addresses these challenges by incorporating a broader range of design values, integrating real-world prototyping to refine evaluations, and boosting efficiency through surrogate metrics and model-based control strategies. This holistic framework, by simultaneously optimizing functionality, durability, and manufacturability, has the potential to enhance reliability and foster broader acceptance of soft robotics, transforming human-robot interactions.","authors":["Maximilian St\\\"olzle","Niccol\\`o Pagliarani","Francesco Stella","Josie Hughes","Cecilia Laschi","Daniela Rus","Matteo Cianchetti","Cosimo Della Santina","Gioele Zardini"],"url":"https://arxiv.org/abs/2505.03761"}
{"created":"2025-05-08","title":"CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory Architecture","abstract":"Open-source RISC-V cores are increasingly adopted in high-end embedded domains such as automotive, where maximizing instructions per cycle (IPC) is becoming critical. Building on the industry-supported open-source CVA6 core and its superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version incorporating improved branch prediction, register renaming and enhanced operand forwarding. These optimizations enable CVA6S+ to achieve a 43.5% performance improvement over the scalar configuration and 10.9% over CVA6S, with an area overhead of just 9.30% over the scalar core (CVA6). Furthermore, we integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache (HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache subsystem.","authors":["Riccardo Tedeschi","Gianmarco Ottavi","C\\^ome Allart","Nils Wistoff","Zexin Fu","Filippo Grillotti","Fabio De Ambroggi","Elio Guidetti","Jean-Baptiste Rigaud","Olivier Potin","Jean Roch Coulon","C\\'esar Fuguet","Luca Benini","Davide Rossi"],"url":"https://arxiv.org/abs/2505.03762"}
{"created":"2025-05-08","title":"Splitwiser: Efficient LM inference with constrained resources","abstract":"Efficient inference of LLMs remains a crucial challenge, with two main phases: a compute-intensive prompt computation and a memory-intensive token generation. Despite existing batching and scheduling techniques, token generation phases fail to fully utilize compute resources, especially when compared to prompt computation phases. To address these challenges, we propose Splitwiser, a methodology that splits the two phases of an LLM inference request onto the same GPU, thereby reducing overhead and improving memory access and cache utilization. By eliminating the need to transfer data across devices, Splitwiser aims to minimize network-related overheads. In this report, we describe the basic structure of our proposed pipeline while sharing preliminary results and analysis. We implement our proposed multiprocessing design on two widely-used and independent LLM architectures: Huggingface and vLLM. We open-source our code for the respective implementations: 1) Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM (https://github.com/adney11/vllm-sysml).","authors":["Asad Aali","Adney Cardoza","Melissa Capo"],"url":"https://arxiv.org/abs/2505.03763"}
{"created":"2025-05-08","title":"Ultra-Low-Power Spiking Neurons in 7 nm FinFET Technology: A Comparative Analysis of Leaky Integrate-and-Fire, Morris-Lecar, and Axon-Hillock Architectures","abstract":"Neuromorphic computing aims to replicate the brain's remarkable energy efficiency and parallel processing capabilities for large-scale artificial intelligence applications. In this work, we present a comprehensive comparative study of three spiking neuron circuit architectures-Leaky-Integrate-and-Fire (LIF), Morris-Lecar (ML), and Axon-Hillock (AH)-implemented in a 7 nm FinFET technology. Through extensive SPICE simulations, we explore the optimization of spiking frequency, energy per spike, and static power consumption. Our results show that the AH design achieves the highest throughput, demonstrating multi-gigahertz firing rates (up to 3 GHz) with attojoule energy costs. By contrast, the ML architecture excels in subthreshold to near-threshold regimes, offering robust low-power operation (as low as 0.385 aJ/spike) and biological bursting behavior. Although LIF benefits from a decoupled current mirror for high-frequency operation, it exhibits slightly higher static leakage compared to ML and AH at elevated supply voltages. Comparisons with previous node implementations (22 nm planar, 28 nm) reveal that 7 nm FinFETs can drastically boost energy efficiency and speed albeit at the cost of increased subthreshold leakage in deep subthreshold regions. By quantifying design trade-offs for each neuron architecture, our work provides a roadmap for optimizing spiking neuron circuits in advanced nanoscale technologies to deliver neuromorphic hardware capable of both ultra-low-power operation and high computational throughput.","authors":["Logan Larsh","Raiyan Siddique","Sarah Sharif Yaser Mike Banad"],"url":"https://arxiv.org/abs/2505.03764"}
{"created":"2025-05-08","title":"From Concept to Measurement: A Survey of How the Blockchain Trilemma Can Be Analyzed","abstract":"To meet non-functional requirements, practitioners must identify Pareto-optimal configurations of the degree of decentralization, scalability, and security of blockchain systems. Maximizing all of these subconcepts is, however, impossible due to the trade-offs highlighted by the blockchain trilemma. We reviewed analysis approaches to identify constructs and their operationalization through metrics for analyzing the blockchain trilemma subconcepts and to assess the applicability of the operationalized constructs to various blockchain systems. By clarifying these constructs and metrics, this work offers a theoretical foundation for more sophisticated investigations into how the blockchain trilemma manifests in blockchain systems, helping practitioners identify Pareto-optimal configurations.","authors":["Mansur Aliyu","Niclas Kannengie{\\ss}er","Sunyaev Ali"],"url":"https://arxiv.org/abs/2505.03768"}
{"created":"2025-05-08","title":"The Influence of Text Variation on User Engagement in Cross-Platform Content Sharing","abstract":"In today's cross-platform social media landscape, understanding factors that drive engagement for multimodal content, especially text paired with visuals, remains complex. This study investigates how rewriting Reddit post titles adapted from YouTube video titles affects user engagement. First, we build and analyze a large dataset of Reddit posts sharing YouTube videos, revealing that 21% of post titles are minimally modified. Statistical analysis demonstrates that title rewrites measurably improve engagement. Second, we design a controlled, multi-phase experiment to rigorously isolate the effects of textual variations by neutralizing confounding factors like video popularity, timing, and community norms. Comprehensive statistical tests reveal that effective title rewrites tend to feature emotional resonance, lexical richness, and alignment with community-specific norms. Lastly, pairwise ranking prediction experiments using a fine-tuned BERT classifier achieves 74% accuracy, significantly outperforming near-random baselines, including GPT-4o. These results validate that our controlled dataset effectively minimizes confounding effects, allowing advanced models to both learn and demonstrate the impact of textual features on engagement. By bridging quantitative rigor with qualitative insights, this study uncovers engagement dynamics and offers a robust framework for future cross-platform, multimodal content strategies.","authors":["Yibo Hu","Yiqiao Jin","Meng Ye","Ajay Divakaran","Srijan Kumar"],"url":"https://arxiv.org/abs/2505.03769"}
{"created":"2025-05-08","title":"Proceedings of 1st Workshop on Advancing Artificial Intelligence through Theory of Mind","abstract":"This volume includes a selection of papers presented at the Workshop on Advancing Artificial Intelligence through Theory of Mind held at AAAI 2025 in Philadelphia US on 3rd March 2025. The purpose of this volume is to provide an open access and curated anthology for the ToM and AI research community.","authors":["Mouad Abrini","Omri Abend","Dina Acklin","Henny Admoni","Gregor Aichinger","Nitay Alon","Zahra Ashktorab","Ashish Atreja","Moises Auron","Alexander Aufreiter","Raghav Awasthi","Soumya Banerjee","Joe M. Barnby","Rhea Basappa","Severin Bergsmann","Djallel Bouneffouf","Patrick Callaghan","Marc Cavazza","Thierry Chaminade","Sonia Chernova","Mohamed Chetouan","Moumita Choudhury","Axel Cleeremans","Jacek B. Cywinski","Fabio Cuzzolin","Hokin Deng","N'yoma Diamond","Camilla Di Pasquasio","Guillaume Dumas","Max van Duijn","Mahapatra Dwarikanath","Qingying Gao","Ashok Goel","Rebecca Goldstein","Matthew Gombolay","Gabriel Enrique Gonzalez","Amar Halilovic","Tobias Halmdienst","Mahimul Islam","Julian Jara-Ettinger","Natalie Kastel","Renana Keydar","Ashish K. Khanna","Mahdi Khoramshahi","JiHyun Kim","MiHyeon Kim","YoungBin Kim","Senka Krivic","Nikita Krasnytskyi","Arun Kumar","JuneHyoung Kwon","Eunju Lee","Shane Lee","Peter R. Lewis","Xue Li","Yijiang Li","Michal Lewandowski","Nathan Lloyd","Matthew B. Luebbers","Dezhi Luo","Haiyun Lyu","Dwarikanath Mahapatra","Kamal Maheshwari","Mallika Mainali","Piyush Mathur","Patrick Mederitsch","Shuwa Miura","Manuel Preston de Miranda","Reuth Mirsky","Shreya Mishra","Nina Moorman","Katelyn Morrison","John Muchovej","Bernhard Nessler","Felix Nessler","Hieu Minh Jord Nguyen","Abby Ortego","Francis A. Papay","Antoine Pasquali","Hamed Rahimi","Charumathi Raghu","Amanda Royka","Stefan Sarkadi","Jaelle Scheuerman","Simon Schmid","Paul Schrater","Anik Sen","Zahra Sheikhbahaee","Ke Shi","Reid Simmons","Nishant Singh","Mason O. Smith","Ramira van der Meulen","Anthia Solaki","Haoran Sun","Viktor Szolga","Matthew E. Taylor","Travis Taylor","Sanne Van Waveren","Juan David Vargas","Rineke Verbrugge","Eitan Wagner","Justin D. Weisz","Ximing Wen","William Yeoh","Wenlong Zhang","Michelle Zhao","Shlomo Zilberstein"],"url":"https://arxiv.org/abs/2505.03770"}
{"created":"2025-05-08","title":"OneDSE: A Unified Microprocessor Metric Prediction and Design Space Exploration Framework","abstract":"With the diminishing returns of Moore Law scaling and as power constraints become more impactful, processor designs rely on architectural innovation to achieve differentiating performance. Innovation complexity has increased the design space of modern high-performance processors. This work offers an efficient and novel design space exploration (DSE) solution to these challenges of modern CPU design. We identify three key challenges in past DSE approaches: (a) Metric prediction is slow and inaccurate for unseen workloads, microarchitectures, (b) Search is slow and inaccurate in CPU parameter space, and (c) A Single model is unable to learn the huge design space. We present OneDSE, a unified metric predictor and CPU parameter explorer to mitigate these challenges with three key techniques: (a) Transformer-based workload-Aware CPU DSE (TrACE) predictor that outperforms state-of-the-art ANN-based prediction methods by 2.75x and 6.12x with and without fine-tuning, respectively, on several benchmarks; (b) a novel metric space search approach that outperforms optimized metaheuristics by 1.19x while reducing search time by an order of magnitude; (c) MARL-based multi-agent framework that achieves a 10.6% reduction in prediction error compared to its non-MARL counterpart, enabling more accurate and efficient exploration of the CPU design space.","authors":["Ritik Raj","Akshat Ramachandran","Jeff Nye","Shashank Nemawarkar","Tushar Krishna"],"url":"https://arxiv.org/abs/2505.03771"}
{"created":"2025-05-08","title":"Does Content Moderation Lead Users Away from Fringe Movements? Evidence from a Recovery Community","abstract":"Online platforms have sanctioned individuals and communities associated with fringe movements linked to hate speech, violence, and terrorism, but can these sanctions contribute to the abandonment of these movements? Here, we investigate this question through the lens of exredpill, a recovery community on Reddit meant to help individuals leave movements within the Manosphere, a conglomerate of fringe Web based movements focused on men's issues. We conduct an observational study on the impact of sanctioning some of Reddit's largest Manosphere communities on the activity levels and user influx of exredpill, the largest associated recovery subreddit. We find that banning a related radical community positively affects participation in exredpill in the period following the ban. Yet, quarantining the community, a softer moderation intervention, yields no such effects. We show that the effect induced by banning a radical community is stronger than for some of the widely discussed real-world events related to the Manosphere and that moderation actions against the Manosphere do not cause a spike in toxicity or malicious activity in exredpill. Overall, our findings suggest that content moderation acts as a deradicalization catalyst.","authors":["Giuseppe Russo","Maciej Styczen","Manoel Horta Ribeiro","Robert West"],"url":"https://arxiv.org/abs/2505.03772"}
{"created":"2025-05-08","title":"Social Media and Academia: How Gender Influences Online Scholarly Discourse","abstract":"This study investigates gender-based differences in online communication patterns of academics, focusing on how male and female academics represent themselves and how users interact with them on the social media platform X (formerly Twitter). We collect historical Twitter data of academics in computer science at the top 20 USA universities and analyze their tweets, retweets, and replies to uncover systematic patterns such as discussed topics, engagement disparities, and the prevalence of negative language or harassment. The findings indicate that while both genders discuss similar topics, men tend to post more tweets about AI innovation, current USA society, machine learning, and personal perspectives, whereas women post slightly more on engaging AI events and workshops. Women express stronger positive and negative sentiments about various events compared to men. However, the average emotional expression remains consistent across genders, with certain emotions being more strongly associated with specific topics. Writing-style analysis reveals that female academics show more empathy and are more likely to discuss personal problems and experiences, with no notable differences in other factors, such as self-praise, politeness, and stereotypical comments. Analyzing audience responses indicates that female academics are more frequently subjected to severe toxic and threatening replies. Our findings highlight the impact of gender in shaping the online communication of academics and emphasize the need for a more inclusive environment for scholarly engagement.","authors":["Rrubaa Panchendrarajan","Harsh Saxena","Akrati Saxena"],"url":"https://arxiv.org/abs/2505.03773"}
{"created":"2025-05-08","title":"Out-of-Distribution Detection in Heterogeneous Graphs via Energy Propagation","abstract":"Graph neural networks (GNNs) are proven effective in extracting complex node and structural information from graph data. While current GNNs perform well in node classification tasks within in-distribution (ID) settings, real-world scenarios often present distribution shifts, leading to the presence of out-of-distribution (OOD) nodes. OOD detection in graphs is a crucial and challenging task. Most existing research focuses on homogeneous graphs, but real-world graphs are often heterogeneous, consisting of diverse node and edge types. This heterogeneity adds complexity and enriches the informational content. To the best of our knowledge, OOD detection in heterogeneous graphs remains an underexplored area. In this context, we propose a novel methodology for OOD detection in heterogeneous graphs (OODHG) that aims to achieve two main objectives: 1) detecting OOD nodes and 2) classifying all ID nodes based on the first task's results. Specifically, we learn representations for each node in the heterogeneous graph, calculate energy values to determine whether nodes are OOD, and then classify ID nodes. To leverage the structural information of heterogeneous graphs, we introduce a meta-path-based energy propagation mechanism and an energy constraint to enhance the distinction between ID and OOD nodes. Extensive experimental findings substantiate the simplicity and effectiveness of OODHG, demonstrating its superiority over baseline models in OOD detection tasks and its accuracy in ID node classification.","authors":["Tao Yin","Chen Zhao","Xiaoyan Liu","Minglai Shao"],"url":"https://arxiv.org/abs/2505.03774"}
{"created":"2025-05-08","title":"Hierarchical Multi-Label Generation with Probabilistic Level-Constraint","abstract":"Hierarchical Extreme Multi-Label Classification poses greater difficulties compared to traditional multi-label classification because of the intricate hierarchical connections of labels within a domain-specific taxonomy and the substantial number of labels. Some of the prior research endeavors centered on classifying text through several ancillary stages such as the cluster algorithm and multiphase classification. Others made attempts to leverage the assistance of generative methods yet were unable to properly control the output of the generative model. We redefine the task from hierarchical multi-Label classification to Hierarchical Multi-Label Generation (HMG) and employ a generative framework with Probabilistic Level Constraints (PLC) to generate hierarchical labels within a specific taxonomy that have complex hierarchical relationships. The approach we proposed in this paper enables the framework to generate all relevant labels across levels for each document without relying on preliminary operations like clustering. Meanwhile, it can control the model output precisely in terms of count, length, and level aspects. Experiments demonstrate that our approach not only achieves a new SOTA performance in the HMG task, but also has a much better performance in constrained the output of model than previous research work.","authors":["Linqing Chen","Weilei Wang","Wentao Wu","Hanmeng Zhong"],"url":"https://arxiv.org/abs/2505.03775"}
{"created":"2025-05-08","title":"PAPN: Proximity Attention Encoder and Pointer Network Decoder for Parcel Pickup Route Prediction","abstract":"Optimization of the last-mile delivery and first-mile pickup of parcels is an integral part of the broader logistics optimization pipeline as it entails both cost and resource efficiency as well as a heightened service quality. Such optimization requires accurate route and time prediction systems to adapt to different scenarios in advance. This work tackles the first building block, namely route prediction. This is done by introducing a novel Proximity Attention mechanism in an encoder-decoder architecture utilizing a Pointer Network in the decoding process (Proximity Attention Encoder and Pointer Network decoder: PAPN) to leverage the underlying connections between the different visitable pickup positions at each timestep. To this local attention process is coupled global context computing via a multi-head attention transformer encoder. The obtained global context is then mixed to an aggregated version of the local embedding thus achieving a mix of global and local attention for complete modeling of the problems. Proximity attention is also used in the decoding process to skew predictions towards the locations with the highest attention scores and thus using inter-connectivity of locations as a base for next-location prediction. This method is trained, validated and tested on a large industry-level dataset of real-world, large-scale last-mile delivery and first-mile pickup named LaDE[1]. This approach shows noticeable promise, outperforming all state-of-the-art supervised systems in terms of most metrics used for benchmarking methods on this dataset while still being competitive with the best-performing reinforcement learning method named DRL4Route[2].","authors":["Hansi Denis","Siegfried Mercelis","Ngoc-Quang Luong"],"url":"https://arxiv.org/abs/2505.03776"}
{"created":"2025-05-08","title":"MolMole: Molecule Mining from Scientific Literature","abstract":"The extraction of molecular structures and reaction data from scientific documents is challenging due to their varied, unstructured chemical formats and complex document layouts. To address this, we introduce MolMole, a vision-based deep learning framework that unifies molecule detection, reaction diagram parsing, and optical chemical structure recognition (OCSR) into a single pipeline for automating the extraction of chemical data directly from page-level documents. Recognizing the lack of a standard page-level benchmark and evaluation metric, we also present a testset of 550 pages annotated with molecule bounding boxes, reaction labels, and MOLfiles, along with a novel evaluation metric. Experimental results demonstrate that MolMole outperforms existing toolkits on both our benchmark and public datasets. The benchmark testset will be publicly available, and the MolMole toolkit will be accessible soon through an interactive demo on the LG AI Research website. For commercial inquiries, please contact us at \\href{mailto:contact_ddu@lgresearch.ai}{contact\\_ddu@lgresearch.ai}.","authors":["LG AI Research","Sehyun Chun","Jiye Kim","Ahra Jo","Yeonsik Jo","Seungyul Oh","Seungjun Lee","Kwangrok Ryoo","Jongmin Lee","Seunghwan Kim","Byung Jun Kang","Soonyoung Lee","Jun Ha Park","Chanwoo Moon","Jiwon Ham","Haein Lee","Heejae Han","Jaeseung Byun","Soojong Do","Minju Ha","Dongyun Kim","Kyunghoon Bae","Woohyung Lim","Edward Hwayoung Lee","Yongmin Park","Jeongsang Yu","Gerrard Jeongwon Jo","Yeonjung Hong","Kyungjae Yoo","Sehui Han","Jaewan Lee","Changyoung Park","Kijeong Jeon","Sihyuk Yi"],"url":"https://arxiv.org/abs/2505.03777"}
{"created":"2025-05-08","title":"Dragonfly: a modular deep reinforcement learning library","abstract":"Dragonfly is a deep reinforcement learning library focused on modularity, in order to ease experimentation and developments. It relies on a json serialization that allows to swap building blocks and perform parameter sweep, while minimizing code maintenance. Some of its features are specifically designed for CPU-intensive environments, such as numerical simulations. Its performance on standard agents using common benchmarks compares favorably with the literature.","authors":["Jonathan Viquerat","Paul Garnier","Amirhossein Bateni","Elie Hachem"],"url":"https://arxiv.org/abs/2505.03778"}
{"created":"2025-05-08","title":"Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites","abstract":"We propose a neural network-based computational framework for the simultaneous optimization of structural topology, curved layers, and path orientations to achieve strong anisotropic strength in fiber-reinforced thermoplastic composites while ensuring manufacturability. Our framework employs three implicit neural fields to represent geometric shape, layer sequence, and fiber orientation. This enables the direct formulation of both design and manufacturability objectives - such as anisotropic strength, structural volume, machine motion control, layer curvature, and layer thickness - into an integrated and differentiable optimization process. By incorporating these objectives as loss functions, the framework ensures that the resultant composites exhibit optimized mechanical strength while remaining its manufacturability for filament-based multi-axis 3D printing across diverse hardware platforms. Physical experiments demonstrate that the composites generated by our co-optimization method can achieve an improvement of up to 33.1% in failure loads compared to composites with sequentially optimized structures and manufacturing sequences.","authors":["Tao Liu","Tianyu Zhang","Yongxue Chen","Weiming Wang","Yu Jiang","Yuming Huang","Charlie C. L. Wang"],"url":"https://arxiv.org/abs/2505.03779"}
{"created":"2025-05-08","title":"GPU Performance Portability needs Autotuning","abstract":"As LLMs grow in complexity, achieving state-of-the-art performance requires tight co-design across algorithms, software, and hardware. Today's reliance on a single dominant platform limits portability, creates vendor lock-in, and raises barriers for new AI hardware. In this work, we make the case for combining just-in-time (JIT) compilation with kernel parameter autotuning to enable portable, state-of-the-art performance LLM execution without code changes. Focusing on flash attention -- a widespread performance-critical LLM kernel -- we demonstrate that this approach explores up to 15x more kernel parameter configurations, produces significantly more diverse code across multiple dimensions, and even outperforms vendor-optimized implementations by up to 230%, all while reducing kernel code size by 70x and eliminating manual code optimizations. Our results highlight autotuning as a promising path to unlocking model portability across GPU vendors.","authors":["Burkhard Ringlein","Thomas Parnell","Radu Stoica"],"url":"https://arxiv.org/abs/2505.03780"}
{"created":"2025-05-08","title":"ALFRED: Ask a Large-language model For Reliable ECG Diagnosis","abstract":"Leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) for analyzing medical data, particularly Electrocardiogram (ECG), offers high accuracy and convenience. However, generating reliable, evidence-based results in specialized fields like healthcare remains a challenge, as RAG alone may not suffice. We propose a Zero-shot ECG diagnosis framework based on RAG for ECG analysis that incorporates expert-curated knowledge to enhance diagnostic accuracy and explainability. Evaluation on the PTB-XL dataset demonstrates the framework's effectiveness, highlighting the value of structured domain expertise in automated ECG interpretation. Our framework is designed to support comprehensive ECG analysis, addressing diverse diagnostic needs with potential applications beyond the tested dataset.","authors":["Jin Yu","JaeHo Park","TaeJun Park","Gyurin Kim","JiHyun Lee","Min Sung Lee","Joon-myoung Kwon","Jeong Min Son","Yong-Yeon Jo"],"url":"https://arxiv.org/abs/2505.03781"}
{"created":"2025-05-08","title":"Exploration of Cryptocurrency Mining-Specific GPUs in AI Applications: A Case Study of CMP 170HX","abstract":"This study systematically tests a computational power reuse scheme proposed by the open source community disabling specific instruction sets (Fused Multiply Add instructions) through CUDA source code modifications on the NVIDIA CMP 170HX platform. Experimental results validate the effectiveness of this approach, partially restoring the GPU's computational capabilities in artificial intelligence (AI) tasks. Performance evaluations using open-source GPU benchmarks (OpenCL benchmark, mixbench) and AI benchmarks (LLAMA-benchmark) reveal that its FP32 floating-point performance exceeds 15 times the original capability, while inference performance for certain precision levels in large language models surpasses threefold improvements. Furthermore, based on hardware architecture analysis, this paper proposes theoretical conjectures for further improving computational utilization through alternative adaptation pathways.Combining energy efficiency ratios and cost models, the recycling value of such obsolete GPUs in edge computing and lightweight AI inference scenarios is evaluated. The findings demonstrate that rationally reusing residual computational power from mining GPUs can significantly mitigate the environmental burden of electronic waste while offering cost-effective hardware solutions for low-budget computing scenarios.","authors":["Xing Kangwei"],"url":"https://arxiv.org/abs/2505.03782"}
{"created":"2025-05-08","title":"A general physics-constrained method for the modelling of equation's closure terms with sparse data","abstract":"Accurate modeling of closure terms is a critical challenge in engineering and scientific research, particularly when data is sparse (scarse or incomplete), making widely applicable models difficult to develop. This study proposes a novel approach for constructing closure models in such challenging scenarios. We introduce a Series-Parallel Multi-Network Architecture that integrates Physics-Informed Neural Networks (PINNs) to incorporate physical constraints and heterogeneous data from multiple initial and boundary conditions, while employing dedicated subnetworks to independently model unknown closure terms, enhancing generalizability across diverse problems. These closure models are integrated into an accurate Partial Differential Equation (PDE) solver, enabling robust solutions to complex predictive simulations in engineering applications.","authors":["Tian Chen","Shengping Liu","Li Liu","Heng Yong"],"url":"https://arxiv.org/abs/2505.03783"}
{"created":"2025-05-08","title":"Insulin Resistance Prediction From Wearables and Routine Blood Biomarkers","abstract":"Insulin resistance, a precursor to type 2 diabetes, is characterized by impaired insulin action in tissues. Current methods for measuring insulin resistance, while effective, are expensive, inaccessible, not widely available and hinder opportunities for early intervention. In this study, we remotely recruited the largest dataset to date across the US to study insulin resistance (N=1,165 participants, with median BMI=28 kg/m2, age=45 years, HbA1c=5.4%), incorporating wearable device time series data and blood biomarkers, including the ground-truth measure of insulin resistance, homeostatic model assessment for insulin resistance (HOMA-IR). We developed deep neural network models to predict insulin resistance based on readily available digital and blood biomarkers. Our results show that our models can predict insulin resistance by combining both wearable data and readily available blood biomarkers better than either of the two data sources separately (R2=0.5, auROC=0.80, Sensitivity=76%, and specificity 84%). The model showed 93% sensitivity and 95% adjusted specificity in obese and sedentary participants, a subpopulation most vulnerable to developing type 2 diabetes and who could benefit most from early intervention. Rigorous evaluation of model performance, including interpretability, and robustness, facilitates generalizability across larger cohorts, which is demonstrated by reproducing the prediction performance on an independent validation cohort (N=72 participants). Additionally, we demonstrated how the predicted insulin resistance can be integrated into a large language model agent to help understand and contextualize HOMA-IR values, facilitating interpretation and safe personalized recommendations. This work offers the potential for early detection of people at risk of type 2 diabetes and thereby facilitate earlier implementation of preventative strategies.","authors":["Ahmed A. Metwally","A. Ali Heydari","Daniel McDuff","Alexandru Solot","Zeinab Esmaeilpour","Anthony Z Faranesh","Menglian Zhou","David B. Savage","Conor Heneghan","Shwetak Patel","Cathy Speed","Javier L. Prieto"],"url":"https://arxiv.org/abs/2505.03784"}
{"created":"2025-05-08","title":"mAIstro: an open-source multi-agentic system for automated end-to-end development of radiomics and deep learning models for medical imaging","abstract":"Agentic systems built on large language models (LLMs) offer promising capabilities for automating complex workflows in healthcare AI. We introduce mAIstro, an open-source, autonomous multi-agentic framework for end-to-end development and deployment of medical AI models. The system orchestrates exploratory data analysis, radiomic feature extraction, image segmentation, classification, and regression through a natural language interface, requiring no coding from the user. Built on a modular architecture, mAIstro supports both open- and closed-source LLMs, and was evaluated using a large and diverse set of prompts across 16 open-source datasets, covering a wide range of imaging modalities, anatomical regions, and data types. The agents successfully executed all tasks, producing interpretable outputs and validated models. This work presents the first agentic framework capable of unifying data analysis, AI model development, and inference across varied healthcare applications, offering a reproducible and extensible foundation for clinical and research AI integration. The code is available at: https://github.com/eltzanis/mAIstro","authors":["Eleftherios Tzanis","Michail E. Klontzas"],"url":"https://arxiv.org/abs/2505.03785"}
{"created":"2025-05-08","title":"When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator","abstract":"Large Language Models (LLM) with reasoning capabilities offer a promising path for improving candidate evaluation in planning frameworks, but their relative performance against traditional non-reasoning models remains largely underexplored. In this study, we benchmark a distilled 1.5B parameter reasoning model (DeepSeek-R1) against several state-of-the-art non-reasoning LLMs within a generator-discriminator LLM planning framework for the text-to-SQL task. For this, we introduce a novel method for extracting soft scores from the chain-of-thought (CoT) outputs from reasoning that enables fine-grained ranking of candidates. Our central hypothesis is that reasoning models are more effective discriminators than non-reasoning LLMs. Our results show that distilled DeepSeek-R1-1.5B achieves up to $87\\%$ higher F1 and $3.7\\%$ better discrimination accuracy than CodeLlama-7B, as well as $3.7\\%$ higher execution accuracy than CodeLlama-13B, despite having significantly fewer parameters. Furthermore, we find that there is a limit to the logical capabilities of reasoning models, and only providing more context or allowing more compute budget for reasoning is not enough to improve their discrimination performance. Finally, we demonstrate that, unlike non-reasoning LLMs, reasoning models find generation more challenging than discrimination and may underperform as generators compared to smaller non-reasoning LLMs. Our work highlights the potential of reasoning models as discriminators in agentic frameworks, far outweighing their capabilities as generators, offering insights into their optimal role within LLM planning infrastructures.","authors":["Md Fahim Anjum"],"url":"https://arxiv.org/abs/2505.03786"}
{"created":"2025-05-08","title":"ArrhythmiaVision: Resource-Conscious Deep Learning Models with Visual Explanations for ECG Arrhythmia Classification","abstract":"Cardiac arrhythmias are a leading cause of life-threatening cardiac events, highlighting the urgent need for accurate and timely detection. Electrocardiography (ECG) remains the clinical gold standard for arrhythmia diagnosis; however, manual interpretation is time-consuming, dependent on clinical expertise, and prone to human error. Although deep learning has advanced automated ECG analysis, many existing models abstract away the signal's intrinsic temporal and morphological features, lack interpretability, and are computationally intensive-hindering their deployment on resource-constrained platforms. In this work, we propose two novel lightweight 1D convolutional neural networks, ArrhythmiNet V1 and V2, optimized for efficient, real-time arrhythmia classification on edge devices. Inspired by MobileNet's depthwise separable convolutional design, these models maintain memory footprints of just 302.18 KB and 157.76 KB, respectively, while achieving classification accuracies of 0.99 (V1) and 0.98 (V2) on the MIT-BIH Arrhythmia Dataset across five classes: Normal Sinus Rhythm, Left Bundle Branch Block, Right Bundle Branch Block, Atrial Premature Contraction, and Premature Ventricular Contraction. In order to ensure clinical transparency and relevance, we integrate Shapley Additive Explanations and Gradient-weighted Class Activation Mapping, enabling both local and global interpretability. These techniques highlight physiologically meaningful patterns such as the QRS complex and T-wave that contribute to the model's predictions. We also discuss performance-efficiency trade-offs and address current limitations related to dataset diversity and generalizability. Overall, our findings demonstrate the feasibility of combining interpretability, predictive accuracy, and computational efficiency in practical, wearable, and embedded ECG monitoring systems.","authors":["Zuraiz Baig","Sidra Nasir","Rizwan Ahmed Khan","Muhammad Zeeshan Ul Haque"],"url":"https://arxiv.org/abs/2505.03787"}
{"created":"2025-05-08","title":"Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding","abstract":"We introduce a novel approach for calibrating uncertainty quantification (UQ) tailored for multi-modal large language models (LLMs). Existing state-of-the-art UQ methods rely on consistency among multiple responses generated by the LLM on an input query under diverse settings. However, these approaches often report higher confidence in scenarios where the LLM is consistently incorrect. This leads to a poorly calibrated confidence with respect to accuracy. To address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. Specifically, we ground the textual responses to the visual inputs. The confidence from the grounding model is used to calibrate the overall confidence. Given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. We evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (Slake) and visual question answering (VQAv2), considering multi-modal models such as LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.","authors":["Trilok Padhi","Ramneet Kaur","Adam D. Cobb","Manoj Acharya","Anirban Roy","Colin Samplawski","Brian Matejek","Alexander M. Berenbeim","Nathaniel D. Bastian","Susmit Jha"],"url":"https://arxiv.org/abs/2505.03788"}
{"created":"2025-05-08","title":"A new architecture of high-order deep neural networks that learn martingales","abstract":"A new deep-learning neural network architecture based on high-order weak approximation algorithms for stochastic differential equations (SDEs) is proposed. The architecture enables the efficient learning of martingales by deep learning models. The behaviour of deep neural networks based on this architecture, when applied to the problem of pricing financial derivatives, is also examined. The core of this new architecture lies in the high-order weak approximation algorithms of the explicit Runge--Kutta type, wherein the approximation is realised solely through iterative compositions and linear combinations of vector fields of the target SDEs.","authors":["Syoiti Ninomiya","Yuming Ma"],"url":"https://arxiv.org/abs/2505.03789"}
{"created":"2025-05-08","title":"A Time-Series Data Augmentation Model through Diffusion and Transformer Integration","abstract":"With the development of Artificial Intelligence, numerous real-world tasks have been accomplished using technology integrated with deep learning. To achieve optimal performance, deep neural networks typically require large volumes of data for training. Although advances in data augmentation have facilitated the acquisition of vast datasets, most of this data is concentrated in domains like images and speech. However, there has been relatively less focus on augmenting time-series data. To address this gap and generate a substantial amount of time-series data, we propose a simple and effective method that combines the Diffusion and Transformer models. By utilizing an adjusted diffusion denoising model to generate a large volume of initial time-step action data, followed by employing a Transformer model to predict subsequent actions, and incorporating a weighted loss function to achieve convergence, the method demonstrates its effectiveness. Using the performance improvement of the model after applying augmented data as a benchmark, and comparing the results with those obtained without data augmentation or using traditional data augmentation methods, this approach shows its capability to produce high-quality augmented data.","authors":["Yuren Zhang","Zhongnan Pu","Lei Jing"],"url":"https://arxiv.org/abs/2505.03790"}
{"created":"2025-05-08","title":"Practical Boolean Backpropagation","abstract":"Boolean neural networks offer hardware-efficient alternatives to real-valued models. While quantization is common, purely Boolean training remains underexplored. We present a practical method for purely Boolean backpropagation for networks based on a single specific gate we chose, operating directly in Boolean algebra involving no numerics. Initial experiments confirm its feasibility.","authors":["Simon Golbert"],"url":"https://arxiv.org/abs/2505.03791"}
{"created":"2025-05-08","title":"Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft Reinforcement Learning","abstract":"Online fine-tuning vision-language model (VLM) agents with reinforcement learning (RL) has shown promise for equipping agents with multi-step, goal-oriented capabilities in dynamic environments. However, their open-ended textual action space and non-end-to-end nature of action generation present significant challenges to effective online exploration in RL, e.g., explosion of the exploration space. We propose a novel online fine-tuning method, Counterfactual Soft Reinforcement Learning (CoSo), better suited to the textual output space of VLM agents. Compared to prior methods that assign uniform uncertainty to all tokens, CoSo leverages counterfactual reasoning to dynamically assess the causal influence of individual tokens on post-processed actions. By prioritizing the exploration of action-critical tokens while reducing the impact of semantically redundant or low-impact tokens, CoSo enables a more targeted and efficient online rollout process. We provide theoretical analysis proving CoSo's convergence and policy improvement guarantees, and extensive empirical evaluations supporting CoSo's effectiveness. Our results across a diverse set of agent tasks, including Android device control, card gaming, and embodied AI, highlight its remarkable ability to enhance exploration efficiency and deliver consistent performance gains. The code is available at https://github.com/langfengQ/CoSo.","authors":["Lang Feng","Weihao Tan","Zhiyi Lyu","Longtao Zheng","Haiyang Xu","Ming Yan","Fei Huang","Bo An"],"url":"https://arxiv.org/abs/2505.03792"}
{"created":"2025-05-08","title":"LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection","abstract":"The proliferation of open-sourced Large Language Models (LLMs) and diverse downstream tasks necessitates efficient model selection, given the impracticality of fine-tuning all candidates due to computational constraints. Despite the recent advances in LLM selection, a fundamental research question largely remains nascent: how can we model the dynamic behaviors of LLMs during fine-tuning, thereby enhancing our understanding of their generalization performance across diverse downstream tasks? In this work, we propose a novel theoretical framework that provides a proper lens to assess the generalization capabilities of LLMs, thereby enabling accurate and efficient LLM selection for downstream applications. In particular, we first derive a Hessian-based PAC-Bayes generalization bound that unveils fine-tuning dynamics of LLMs and then introduce LENSLLM, a Neural Tangent Kernel(NTK)-based Rectified Scaling Model that enables accurate performance predictions across diverse tasks while maintaining computational efficiency. Extensive empirical results on 3 large-scale benchmarks demonstrate that our model achieves up to 91.1% accuracy and reduces up to 88.5% computational cost in LLM selection, outperforming 5 state-of-the-art methods. We open-source our proposed LENSLLM model and corresponding results at the Github link: https://github.com/Susan571/LENSLLM.git.","authors":["Xinyue Zeng","Haohui Wang","Junhong Lin","Jun Wu","Tyler Cody","Dawei Zhou"],"url":"https://arxiv.org/abs/2505.03793"}
{"created":"2025-05-08","title":"A Double Inertial Forward-Backward Splitting Algorithm With Applications to Regression and Classification Problems","abstract":"This paper presents an improved forward-backward splitting algorithm with two inertial parameters. It aims to find a point in the real Hilbert space at which the sum of a co-coercive operator and a maximal monotone operator vanishes. Under standard assumptions, our proposed algorithm demonstrates weak convergence. We present numerous experimental results to demonstrate the behavior of the developed algorithm by comparing it with existing algorithms in the literature for regression and data classification problems. Furthermore, these implementations suggest our proposed algorithm yields superior outcomes when benchmarked against other relevant algorithms in existing literature.","authors":["\\.Irfan I\\c{s}ik","Ibrahim Karahan","Okan Erkaymaz"],"url":"https://arxiv.org/abs/2505.03794"}
{"created":"2025-05-08","title":"Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics","abstract":"Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare and contrast several methods for learning models of human behavior in a strategic network game called the Junior High Game (JHG). These modeling methods differ with respect to the assumptions they use to parameterize human behavior (behavior vs. community-aware behavior) and the statistical moments they model (mean vs. distribution). Results show that the highest-performing method models the population's distribution rather than the mean and assumes humans use community-aware behavior rather than behavior matching. When applied to small societies (6-11 individuals), this learned model, called hCAB, closely mirrors the population dynamics of human groups (with some differences). Additionally, a user study reveals that human participants were unable to distinguish hCAB agents from other humans, thus illustrating that individual hCAB behavior plausibly mirrors human behavior in this strategic network game.","authors":["Jacob W. Crandall","Jonathan Skaggs"],"url":"https://arxiv.org/abs/2505.03795"}
{"created":"2025-05-08","title":"AI-Driven IRM: Transforming insider risk management with adaptive scoring and LLM-based threat detection","abstract":"Insider threats pose a significant challenge to organizational security, often evading traditional rule-based detection systems due to their subtlety and contextual nature. This paper presents an AI-powered Insider Risk Management (IRM) system that integrates behavioral analytics, dynamic risk scoring, and real-time policy enforcement to detect and mitigate insider threats with high accuracy and adaptability. We introduce a hybrid scoring mechanism - transitioning from the static PRISM model to an adaptive AI-based model utilizing an autoencoder neural network trained on expert-annotated user activity data. Through iterative feedback loops and continuous learning, the system reduces false positives by 59% and improves true positive detection rates by 30%, demonstrating substantial gains in detection precision. Additionally, the platform scales efficiently, processing up to 10 million log events daily with sub-300ms query latency, and supports automated enforcement actions for policy violations, reducing manual intervention. The IRM system's deployment resulted in a 47% reduction in incident response times, highlighting its operational impact. Future enhancements include integrating explainable AI, federated learning, graph-based anomaly detection, and alignment with Zero Trust principles to further elevate its adaptability, transparency, and compliance-readiness. This work establishes a scalable and proactive framework for mitigating emerging insider risks in both on-premises and hybrid environments.","authors":["Lokesh Koli","Shubham Kalra","Rohan Thakur","Anas Saifi","Karanpreet Singh"],"url":"https://arxiv.org/abs/2505.03796"}
{"created":"2025-05-08","title":"Utilising Gradient-Based Proposals Within Sequential Monte Carlo Samplers for Training of Partial Bayesian Neural Networks","abstract":"Partial Bayesian neural networks (pBNNs) have been shown to perform competitively with fully Bayesian neural networks while only having a subset of the parameters be stochastic. Using sequential Monte Carlo (SMC) samplers as the inference method for pBNNs gives a non-parametric probabilistic estimation of the stochastic parameters, and has shown improved performance over parametric methods. In this paper we introduce a new SMC-based training method for pBNNs by utilising a guided proposal and incorporating gradient-based Markov kernels, which gives us better scalability on high dimensional problems. We show that our new method outperforms the state-of-the-art in terms of predictive performance and optimal loss. We also show that pBNNs scale well with larger batch sizes, resulting in significantly reduced training times and often better performance.","authors":["Andrew Millard","Joshua Murphy","Simon Maskell","Zheng Zhao"],"url":"https://arxiv.org/abs/2505.03797"}
{"created":"2025-05-08","title":"Position: Foundation Models Need Digital Twin Representations","abstract":"Current foundation models (FMs) rely on token representations that directly fragment continuous real-world multimodal data into discrete tokens. They limit FMs to learning real-world knowledge and relationships purely through statistical correlation rather than leveraging explicit domain knowledge. Consequently, current FMs struggle with maintaining semantic coherence across modalities, capturing fine-grained spatial-temporal dynamics, and performing causal reasoning. These limitations cannot be overcome by simply scaling up model size or expanding datasets. This position paper argues that the machine learning community should consider digital twin (DT) representations, which are outcome-driven digital representations that serve as building blocks for creating virtual replicas of physical processes, as an alternative to the token representation for building FMs. Finally, we discuss how DT representations can address these challenges by providing physically grounded representations that explicitly encode domain knowledge and preserve the continuous nature of real-world processes.","authors":["Yiqing Shen","Hao Ding","Lalithkumar Seenivasan","Tianmin Shu","Mathias Unberath"],"url":"https://arxiv.org/abs/2505.03798"}
{"created":"2025-05-08","title":"Scalability Matters: Overcoming Challenges in InstructGLM with Similarity-Degree-Based Sampling","abstract":"Large Language Models (LLMs) have demonstrated strong capabilities in various natural language processing tasks; however, their application to graph-related problems remains limited, primarily due to scalability constraints and the absence of dedicated mechanisms for processing graph structures. Existing approaches predominantly integrate LLMs with Graph Neural Networks (GNNs), using GNNs as feature encoders or auxiliary components. However, directly encoding graph structures within LLMs has been underexplored, particularly in the context of large-scale graphs where token limitations hinder effective representation. To address these challenges, we propose SDM-InstructGLM, a novel instruction-tuned Graph Language Model (InstructGLM) framework that enhances scalability and efficiency without relying on GNNs. Our method introduces a similarity-degree-based biased random walk mechanism, which selectively samples and encodes graph information based on node-feature similarity and degree centrality, ensuring an adaptive and structured representation within the LLM. This approach significantly improves token efficiency, mitigates information loss due to random sampling, and enhances performance on graph-based tasks such as node classification and link prediction. Furthermore, our results demonstrate the feasibility of LLM-only graph processing, enabling scalable and interpretable Graph Language Models (GLMs) optimized through instruction-based fine-tuning. This work paves the way for GNN-free approaches to graph learning, leveraging LLMs as standalone graph reasoning models. Our source code is available on GitHub.","authors":["Hyun Lee","Chris Yi","Maminur Islam","B. D. S. Aritra"],"url":"https://arxiv.org/abs/2505.03799"}
{"created":"2025-05-08","title":"Design description of Wisdom Computing Persperctive","abstract":"This course design aims to develop and research a handwriting matrix recognition and step-by-step visual calculation process display system, addressing the issue of abstract formulas and complex calculation steps that students find difficult to understand when learning mathematics. By integrating artificial intelligence with visualization animation technology, the system enhances precise recognition of handwritten matrix content through the introduction of Mamba backbone networks, completes digital extraction and matrix reconstruction using the YOLO model, and simultaneously combines CoordAttention coordinate attention mechanisms to improve the accurate grasp of character spatial positions. The calculation process is demonstrated frame by frame through the Manim animation engine, vividly showcasing each mathematical calculation step, helping students intuitively understand the intrinsic logic of mathematical operations. Through dynamically generating animation processes for different computational tasks, the system exhibits high modularity and flexibility, capable of generating various mathematical operation examples in real-time according to student needs. By innovating human-computer interaction methods, it brings mathematical calculation processes to life, helping students bridge the gap between knowledge and understanding on a deeper level, ultimately achieving a learning experience where \"every step is understood.\" The system's scalability and interactivity make it an intuitive, user-friendly, and efficient auxiliary tool in education.","authors":["TianYi Yu"],"url":"https://arxiv.org/abs/2505.03800"}
{"created":"2025-05-08","title":"Large Language Model Compression with Global Rank and Sparsity Optimization","abstract":"Low-rank and sparse composite approximation is a natural idea to compress Large Language Models (LLMs). However, such an idea faces two primary challenges that adversely affect the performance of existing methods. The first challenge relates to the interaction and cooperation between low-rank and sparse matrices, while the second involves determining weight allocation across different layers, as redundancy varies considerably among them. To address these challenges, we propose a novel two-stage LLM compression method with the capability of global rank and sparsity optimization. It is noteworthy that the overall optimization space is vast, making comprehensive optimization computationally prohibitive. Therefore, to reduce the optimization space, our first stage utilizes robust principal component analysis to decompose the weight matrices of LLMs into low-rank and sparse components, which span the low dimensional and sparse spaces containing the resultant low-rank and sparse matrices, respectively. In the second stage, we propose a probabilistic global optimization technique to jointly identify the low-rank and sparse structures within the above two spaces. The appealing feature of our approach is its ability to automatically detect the redundancy across different layers and to manage the interaction between the sparse and low-rank components. Extensive experimental results indicate that our method significantly surpasses state-of-the-art techniques for sparsification and composite approximation.","authors":["Changhai Zhou","Qian Qiao","Weizhong Zhang","Cheng Jin"],"url":"https://arxiv.org/abs/2505.03801"}
{"created":"2025-05-08","title":"Efficient Fine-Tuning of Quantized Models via Adaptive Rank and Bitwidth","abstract":"QLoRA effectively combines low-bit quantization and LoRA to achieve memory-friendly fine-tuning for large language models (LLM). Recently, methods based on SVD for continuous update iterations to initialize LoRA matrices to accommodate quantization errors have generally failed to consistently improve performance. Dynamic mixed precision is a natural idea for continuously improving the fine-tuning performance of quantized models, but previous methods often optimize low-rank subspaces or quantization components separately, without considering their synergy. To address this, we propose \\textbf{QR-Adaptor}, a unified, gradient-free strategy that uses partial calibration data to jointly search the quantization components and the rank of low-rank spaces for each layer, thereby continuously improving model performance. QR-Adaptor does not minimize quantization error but treats precision and rank allocation as a discrete optimization problem guided by actual downstream performance and memory usage. Compared to state-of-the-art (SOTA) quantized LoRA fine-tuning methods, our approach achieves a 4.89\\% accuracy improvement on GSM8K, and in some cases even outperforms the 16-bit fine-tuned model while maintaining the memory footprint of the 4-bit setting.","authors":["Changhai Zhou","Yuhua Zhou","Qian Qiao","Weizhong Zhang","Cheng Jin"],"url":"https://arxiv.org/abs/2505.03802"}
{"created":"2025-05-08","title":"RWKVQuant: Quantizing the RWKV Family with Proxy Guided Hybrid of Scalar and Vector Quantization","abstract":"RWKV is a modern RNN architecture with comparable performance to Transformer, but still faces challenges when deployed to resource-constrained devices. Post Training Quantization (PTQ), which is a an essential technique to reduce model size and inference latency, has been widely used in Transformer models. However, it suffers significant degradation of performance when applied to RWKV. This paper investigates and identifies two key constraints inherent in the properties of RWKV: (1) Non-linear operators hinder the parameter-fusion of both smooth- and rotation-based quantization, introducing extra computation overhead. (2) The larger amount of uniformly distributed weights poses challenges for cluster-based quantization, leading to reduced accuracy. To this end, we propose RWKVQuant, a PTQ framework tailored for RWKV models, consisting of two novel techniques: (1) a coarse-to-fine proxy capable of adaptively selecting different quantization approaches by assessing the uniformity and identifying outliers in the weights, and (2) a codebook optimization algorithm that enhances the performance of cluster-based quantization methods for element-wise multiplication in RWKV. Experiments show that RWKVQuant can quantize RWKV-6-14B into about 3-bit with less than 1% accuracy loss and 2.14x speed up.","authors":["Chen Xu","Yuxuan Yue","Zukang Xu","Xing Hu","Jiangyong Yu","Zhixuan Chen","Sifan Zhou","Zhihang Yuan","Dawei Yang"],"url":"https://arxiv.org/abs/2505.03803"}
{"created":"2025-05-08","title":"MoEQuant: Enhancing Quantization for Mixture-of-Experts Large Language Models via Expert-Balanced Sampling and Affinity Guidance","abstract":"Mixture-of-Experts (MoE) large language models (LLMs), which leverage dynamic routing and sparse activation to enhance efficiency and scalability, have achieved higher performance while reducing computational costs. However, these models face significant memory overheads, limiting their practical deployment and broader adoption. Post-training quantization (PTQ), a widely used method for compressing LLMs, encounters severe accuracy degradation and diminished generalization performance when applied to MoE models. This paper investigates the impact of MoE's sparse and dynamic characteristics on quantization and identifies two primary challenges: (1) Inter-expert imbalance, referring to the uneven distribution of samples across experts, which leads to insufficient and biased calibration for less frequently utilized experts; (2) Intra-expert imbalance, arising from MoE's unique aggregation mechanism, which leads to varying degrees of correlation between different samples and their assigned experts. To address these challenges, we propose MoEQuant, a novel quantization framework tailored for MoE LLMs. MoE-Quant includes two novel techniques: 1) Expert-Balanced Self-Sampling (EBSS) is an efficient sampling method that efficiently constructs a calibration set with balanced expert distributions by leveraging the cumulative probabilities of tokens and expert balance metrics as guiding factors. 2) Affinity-Guided Quantization (AGQ), which incorporates affinities between experts and samples into the quantization process, thereby accurately assessing the impact of individual samples on different experts within the MoE layer. Experiments demonstrate that MoEQuant achieves substantial performance gains (more than 10 points accuracy gain in the HumanEval for DeepSeekMoE-16B under 4-bit quantization) and boosts efficiency.","authors":["Xing Hu","Zhixuan Chen","Dawei Yang","Zukang Xu","Chen Xu","Zhihang Yuan","Sifan Zhou","Jiangyong Yu"],"url":"https://arxiv.org/abs/2505.03804"}
{"created":"2025-05-08","title":"Feature Optimization for Time Series Forecasting via Novel Randomized Uphill Climbing","abstract":"Randomized Uphill Climbing is a lightweight, stochastic search heuristic that has delivered state of the art equity alpha factors for quantitative hedge funds. I propose to generalize RUC into a model agnostic feature optimization framework for multivariate time series forecasting. The core idea is to synthesize candidate feature programs by randomly composing operators from a domain specific grammar, score candidates rapidly with inexpensive surrogate models on rolling windows, and filter instability via nested cross validation and information theoretic shrinkage. By decoupling feature discovery from GPU heavy deep learning, the method promises faster iteration cycles, lower energy consumption, and greater interpretability. Societal relevance: accurate, transparent forecasting tools empower resource constrained institutions, energy regulators, climate risk NGOs to make data driven decisions without proprietary black box models.","authors":["Nguyen Van Thanh"],"url":"https://arxiv.org/abs/2505.03805"}
{"created":"2025-05-08","title":"Perception-Informed Neural Networks: Beyond Physics-Informed Neural Networks","abstract":"This article introduces Perception-Informed Neural Networks (PrINNs), a framework designed to incorporate perception-based information into neural networks, addressing both systems with known and unknown physics laws or differential equations. Moreover, PrINNs extend the concept of Physics-Informed Neural Networks (PINNs) and their variants, offering a platform for the integration of diverse forms of perception precisiation, including singular, probability distribution, possibility distribution, interval, and fuzzy graph. In fact, PrINNs allow neural networks to model dynamical systems by integrating expert knowledge and perception-based information through loss functions, enabling the creation of modern data-driven models. Some of the key contributions include Mixture of Experts Informed Neural Networks (MOEINNs), which combine heterogeneous expert knowledge into the network, and Transformed-Knowledge Informed Neural Networks (TKINNs), which facilitate the incorporation of meta-information for enhanced model performance. Additionally, Fuzzy-Informed Neural Networks (FINNs) as a modern class of fuzzy deep neural networks leverage fuzzy logic constraints within a deep learning architecture, allowing online training without pre-training and eliminating the need for defuzzification. PrINNs represent a significant step forward in bridging the gap between traditional physics-based modeling and modern data-driven approaches, enabling neural networks to learn from both structured physics laws and flexible perception-based rules. This approach empowers neural networks to operate in uncertain environments, model complex systems, and discover new forms of differential equations, making PrINNs a powerful tool for advancing computational science and engineering.","authors":["Mehran Mazandarani","Marzieh Najariyan"],"url":"https://arxiv.org/abs/2505.03806"}
{"created":"2025-05-08","title":"Facilitating Video Story Interaction with Multi-Agent Collaborative System","abstract":"Video story interaction enables viewers to engage with and explore narrative content for personalized experiences. However, existing methods are limited to user selection, specially designed narratives, and lack customization. To address this, we propose an interactive system based on user intent. Our system uses a Vision Language Model (VLM) to enable machines to understand video stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent System (MAS) to create evolving characters and scene experiences. It includes three stages: 1) Video story processing, utilizing VLM and prior knowledge to simulate human understanding of stories across three modalities. 2) Multi-space chat, creating growth-oriented characters through MAS interactions based on user queries and story stages. 3) Scene customization, expanding and visualizing various story scenes mentioned in dialogue. Applied to the Harry Potter series, our study shows the system effectively portrays emergent character social behavior and growth, enhancing the interactive experience in the video story world.","authors":["Yiwen Zhang","Jianing Hao","Zhan Wang","Hongling Sheng","Wei Zeng"],"url":"https://arxiv.org/abs/2505.03807"}
{"created":"2025-05-08","title":"AI-driven multi-source data fusion for algal bloom severity classification in small inland water bodies: Leveraging Sentinel-2, DEM, and NOAA climate data","abstract":"Harmful algal blooms are a growing threat to inland water quality and public health worldwide, creating an urgent need for efficient, accurate, and cost-effective detection methods. This research introduces a high-performing methodology that integrates multiple open-source remote sensing data with advanced artificial intelligence models. Key data sources include Copernicus Sentinel-2 optical imagery, the Copernicus Digital Elevation Model (DEM), and NOAA's High-Resolution Rapid Refresh (HRRR) climate data, all efficiently retrieved using platforms like Google Earth Engine (GEE) and Microsoft Planetary Computer (MPC). The NIR and two SWIR bands from Sentinel-2, the altitude from the elevation model, the temperature and wind from NOAA as well as the longitude and latitude were the most important features. The approach combines two types of machine learning models, tree-based models and a neural network, into an ensemble for classifying algal bloom severity. While the tree models performed strongly on their own, incorporating a neural network added robustness and demonstrated how deep learning models can effectively use diverse remote sensing inputs. The method leverages high-resolution satellite imagery and AI-driven analysis to monitor algal blooms dynamically, and although initially developed for a NASA competition in the U.S., it shows potential for global application. The complete code is available for further adaptation and practical implementation, illustrating the convergence of remote sensing data and AI to address critical environmental challenges (https://github.com/IoannisNasios/HarmfulAlgalBloomDetection).","authors":["Ioannis Nasios"],"url":"https://arxiv.org/abs/2505.03808"}
{"created":"2025-05-08","title":"When Dynamic Data Selection Meets Data Augmentation","abstract":"Dynamic data selection aims to accelerate training with lossless performance. However, reducing training data inherently limits data diversity, potentially hindering generalization. While data augmentation is widely used to enhance diversity, it is typically not optimized in conjunction with selection. As a result, directly combining these techniques fails to fully exploit their synergies. To tackle the challenge, we propose a novel online data training framework that, for the first time, unifies dynamic data selection and augmentation, achieving both training efficiency and enhanced performance. Our method estimates each sample's joint distribution of local density and multimodal semantic consistency, allowing for the targeted selection of augmentation-suitable samples while suppressing the inclusion of noisy or ambiguous data. This enables a more significant reduction in dataset size without sacrificing model generalization. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches on various benchmark datasets and architectures, e.g., reducing 50\\% training costs on ImageNet-1k with lossless performance. Furthermore, our approach enhances noise resistance and improves model robustness, reinforcing its practical utility in real-world scenarios.","authors":["Suorong Yang","Peng Ye","Furao Shen","Dongzhan Zhou"],"url":"https://arxiv.org/abs/2505.03809"}
{"created":"2025-05-08","title":"Grouped Sequency-arranged Rotation: Optimizing Rotation Transformation for Quantization for Free","abstract":"Large Language Models (LLMs) face deployment challenges due to high computational costs, and while Post-Training Quantization (PTQ) offers a solution, existing rotation-based methods struggle at very low bit-widths like 2-bit. We introduce a novel, training-free approach to construct an improved rotation matrix, addressing the limitations of current methods. The key contributions include leveraging the Walsh-Hadamard transform with sequency ordering, which clusters similar frequency components to reduce quantization error compared to standard Hadamard matrices, significantly improving performance. Furthermore, we propose a Grouped Sequency-arranged Rotation (GSR) using block-diagonal matrices with smaller Walsh blocks, effectively isolating outlier impacts and achieving performance comparable to optimization-based methods without requiring any training. Our method demonstrates robust performance on reasoning tasks and Perplexity (PPL) score on WikiText-2. Our method also enhances results even when applied over existing learned rotation techniques.","authors":["Euntae Choi","Sumin Song","Woosang Lim","Sungjoo Yoo"],"url":"https://arxiv.org/abs/2505.03810"}
{"created":"2025-05-08","title":"ScarceGAN: Discriminative Classification Framework for Rare Class Identification for Longitudinal Data with Weak Prior","abstract":"This paper introduces ScarceGAN which focuses on identification of extremely rare or scarce samples from multi-dimensional longitudinal telemetry data with small and weak label prior. We specifically address: (i) severe scarcity in positive class, stemming from both underlying organic skew in the data, as well as extremely limited labels; (ii) multi-class nature of the negative samples, with uneven density distributions and partially overlapping feature distributions; and (iii) massively unlabelled data leading to tiny and weak prior on both positive and negative classes, and possibility of unseen or unknown behavior in the unlabelled set, especially in the negative class. Although related to PU learning problems, we contend that knowledge (or lack of it) on the negative class can be leveraged to learn the compliment of it (i.e., the positive class) better in a semi-supervised manner. To this effect, ScarceGAN re-formulates semi-supervised GAN by accommodating weakly labelled multi-class negative samples and the available positive samples. It relaxes the supervised discriminator's constraint on exact differentiation between negative samples by introducing a 'leeway' term for samples with noisy prior. We propose modifications to the cost objectives of discriminator, in supervised and unsupervised path as well as that of the generator. For identifying risky players in skill gaming, this formulation in whole gives us a recall of over 85% (~60% jump over vanilla semi-supervised GAN) on our scarce class with very minimal verbosity in the unknown space. Further ScarceGAN outperforms the recall benchmarks established by recent GAN based specialized models for the positive imbalanced class identification and establishes a new benchmark in identifying one of rare attack classes (0.09%) in the intrusion dataset from the KDDCUP99 challenge.","authors":["Surajit Chakrabarty","Rukma Talwadker","Tridib Mukherjee"],"url":"https://arxiv.org/abs/2505.03811"}
{"created":"2025-05-08","title":"Information Filtering Networks: Theoretical Foundations, Generative Methodologies, and Real-World Applications","abstract":"Information Filtering Networks (IFNs) provide a powerful framework for modeling complex systems through globally sparse yet locally dense and interpretable structures that capture multivariate dependencies. This review offers a comprehensive account of IFNs, covering their theoretical foundations, construction methodologies, and diverse applications. Tracing their origins from early network-based models to advanced formulations such as the Triangulated Maximally Filtered Graph (TMFG) and the Maximally Filtered Clique Forest (MFCF), the paper highlights how IFNs address key challenges in high-dimensional data-driven modeling. IFNs and their construction methodologies are intrinsically higher-order networks that generate simplicial complexes-structures that are only now becoming popular in the broader literature. Applications span fields including finance, biology, psychology, and artificial intelligence, where IFNs improve interpretability, computational efficiency, and predictive performance. Special attention is given to their role in graphical modeling, where IFNs enable the estimation of sparse inverse covariance matrices with greater accuracy and scalability than traditional approaches like Graphical LASSO. Finally, the review discusses recent developments that integrate IFNs with machine learning and deep learning, underscoring their potential not only to bridge classical network theory with contemporary data-driven paradigms, but also to shape the architectures of deep learning models themselves.","authors":["Tomaso Aste"],"url":"https://arxiv.org/abs/2505.03812"}
{"created":"2025-05-08","title":"Mapping the Climate Change Landscape on TikTok","abstract":"Social media platforms shape climate action discourse. Mapping these online conversations is essential for effective communication strategies. TikTok's climate discussions are particularly relevant given its young, climate-concerned audience. In this work, we collect the first TikTok dataset on climate topics. We collected 590K videos from 14K creators along with their follower networks. By applying topic modeling to the video descriptions, we map the topics discussed on the platform on a climate taxonomy that we construct by consolidating existing categorizations. Results show TikTok creators primarily approach climate through the angle of lifestyle and dietary choices. By examining semantic connections between topics, we identified non-climate \"gateway\" topics that could draw new audiences into climate discussions.","authors":["Alessia Galdeman","Luca Maria Aiello"],"url":"https://arxiv.org/abs/2505.03813"}
{"created":"2025-05-08","title":"Towards Cognitive Collaborative Robots: Semantic-Level Integration and Explainable Control for Human-Centric Cooperation","abstract":"This is a preprint of a review article that has not yet undergone peer review. The content is intended for early dissemination and academic discussion. The final version may differ upon formal publication. As the Fourth Industrial Revolution reshapes industrial paradigms, human-robot collaboration (HRC) has transitioned from a desirable capability to an operational necessity. In response, collaborative robots (Cobots) are evolving beyond repetitive tasks toward adaptive, semantically informed interaction with humans and environments. This paper surveys five foundational pillars enabling this transformation: semantic-level perception, cognitive action planning, explainable learning and control, safety-aware motion design, and multimodal human intention recognition. We examine the role of semantic mapping in transforming spatial data into meaningful context, and explore cognitive planning frameworks that leverage this context for goal-driven decision-making. Additionally, we analyze explainable reinforcement learning methods, including policy distillation and attention mechanisms, which enhance interpretability and trust. Safety is addressed through force-adaptive control and risk-aware trajectory planning, while seamless human interaction is supported via gaze and gesture-based intent recognition. Despite these advancements, challenges such as perception-action disjunction, real-time explainability limitations, and incomplete human trust persist. To address these, we propose a unified Cognitive Synergy Architecture, integrating all modules into a cohesive framework for truly human-centric cobot collaboration.","authors":["Jaehong Oh"],"url":"https://arxiv.org/abs/2505.03815"}
{"created":"2025-05-08","title":"Geospatial and Temporal Trends in Urban Transportation: A Study of NYC Taxis and Pathao Food Deliveries","abstract":"Urban transportation plays a vital role in modern city life, affecting how efficiently people and goods move around. This study analyzes transportation patterns using two datasets: the NYC Taxi Trip dataset from New York City and the Pathao Food Trip dataset from Dhaka, Bangladesh. Our goal is to identify key trends in demand, peak times, and important geographical hotspots. We start with Exploratory Data Analysis (EDA) to understand the basic characteristics of the datasets. Next, we perform geospatial analysis to map out high-demand and low-demand regions. We use the SARIMAX model for time series analysis to forecast demand patterns, capturing seasonal and weekly variations. Lastly, we apply clustering techniques to identify significant areas of high and low demand. Our findings provide valuable insights for optimizing fleet management and resource allocation in both passenger transport and food delivery services. These insights can help improve service efficiency, better meet customer needs, and enhance urban transportation systems in diverse urban environments.","authors":["Bidyarthi Paul","Fariha Tasnim Chowdhury","Dipta Biswas","Meherin Sultana"],"url":"https://arxiv.org/abs/2505.03816"}
{"created":"2025-05-08","title":"Modeling Behavioral Preferences of Cyber Adversaries Using Inverse Reinforcement Learning","abstract":"This paper presents a holistic approach to attacker preference modeling from system-level audit logs using inverse reinforcement learning (IRL). Adversary modeling is an important capability in cybersecurity that lets defenders characterize behaviors of potential attackers, which enables attribution to known cyber adversary groups. Existing approaches rely on documenting an ever-evolving set of attacker tools and techniques to track known threat actors. Although attacks evolve constantly, attacker behavioral preferences are intrinsic and less volatile. Our approach learns the behavioral preferences of cyber adversaries from forensics data on their tools and techniques. We model the attacker as an expert decision-making agent with unknown behavioral preferences situated in a computer host. We leverage attack provenance graphs of audit logs to derive a state-action trajectory of the attack. We test our approach on open datasets of audit logs containing real attack data. Our results demonstrate for the first time that low-level forensics data can automatically reveal an adversary's subjective preferences, which serves as an additional dimension to modeling and documenting cyber adversaries. Attackers' preferences tend to be invariant despite their different tools and indicate predispositions that are inherent to the attacker. As such, these inferred preferences can potentially serve as unique behavioral signatures of attackers and improve threat attribution.","authors":["Aditya Shinde","Prashant Doshi"],"url":"https://arxiv.org/abs/2505.03817"}
{"created":"2025-05-08","title":"Program Semantic Inequivalence Game with Large Language Models","abstract":"Large Language Models (LLMs) can achieve strong performance on everyday coding tasks, but they can fail on complex tasks that require non-trivial reasoning about program semantics. Finding training examples to teach LLMs to solve these tasks can be challenging.","authors":["Antonio Valerio Miceli-Barone","Vaishak Belle","Ali Payani"],"url":"https://arxiv.org/abs/2505.03818"}
{"created":"2025-05-08","title":"Focus on the Likely: Test-time Instance-based Uncertainty Removal","abstract":"We propose two novel test-time fine-tuning methods to improve uncertain model predictions. Our methods require no auxiliary data and use the given test instance only. Instead of performing a greedy selection of the most likely class to make a prediction, we introduce an additional focus on the likely classes step during inference. By applying a single-step gradient descent, we refine predictions when an initial forward pass indicates high uncertainty. This aligns predictions more closely with the ideal of assigning zero probability to less plausible outcomes. Our theoretical discussion provides a deeper understanding highlighting the impact on shared and non-shared features among (focus) classes. The experimental evaluation highlights accuracy gains on samples exhibiting high decision uncertainty for a diverse set of models from both the text and image domain using the same hyperparameters.","authors":["Johannes Schneider"],"url":"https://arxiv.org/abs/2505.03819"}
{"created":"2025-05-08","title":"Satellite Autonomous Clock Fault Monitoring with Inter-Satellite Ranges Using Euclidean Distance Matrices","abstract":"To address the need for robust positioning, navigation, and timing services in lunar environments, this paper proposes a novel onboard clock phase jump detection framework for satellite constellations using range measurements obtained from dual one-way inter-satellite links. Our approach leverages vertex redundantly rigid graphs to detect faults without relying on prior knowledge of satellite positions or clock biases, providing flexibility for lunar satellite networks with diverse satellite types and operators. We model satellite constellations as graphs, where satellites are vertices and inter-satellite links are edges. The proposed algorithm detects and identifies satellites with clock jumps by monitoring the singular values of the geometric-centered Euclidean distance matrix (GCEDM) of 5-clique sub-graphs. The proposed method is validated through simulations of a GPS constellation and a notional constellation around the Moon, demonstrating its effectiveness in various configurations.","authors":["Keidai Iiyama","Daniel Neamati","Grace Gao"],"url":"https://arxiv.org/abs/2505.03820"}
{"created":"2025-05-08","title":"Beyond Recognition: Evaluating Visual Perspective Taking in Vision Language Models","abstract":"We investigate the ability of Vision Language Models (VLMs) to perform visual perspective taking using a novel set of visual tasks inspired by established human tests. Our approach leverages carefully controlled scenes, in which a single humanoid minifigure is paired with a single object. By systematically varying spatial configurations - such as object position relative to the humanoid minifigure and the humanoid minifigure's orientation - and using both bird's-eye and surface-level views, we created 144 unique visual tasks. Each visual task is paired with a series of 7 diagnostic questions designed to assess three levels of visual cognition: scene understanding, spatial reasoning, and visual perspective taking. Our evaluation of several state-of-the-art models, including GPT-4-Turbo, GPT-4o, Llama-3.2-11B-Vision-Instruct, and variants of Claude Sonnet, reveals that while they excel in scene understanding, the performance declines significantly on spatial reasoning and further deteriorates on perspective-taking. Our analysis suggests a gap between surface-level object recognition and the deeper spatial and perspective reasoning required for complex visual tasks, pointing to the need for integrating explicit geometric representations and tailored training protocols in future VLM development.","authors":["Gracjan G\\'oral","Alicja Ziarko","Piotr Mi{\\l}o\\'s","Micha{\\l} Nauman","Maciej Wo{\\l}czyk","Micha{\\l} Kosi\\'nski"],"url":"https://arxiv.org/abs/2505.03821"}
{"created":"2025-05-08","title":"DRSLF: Double Regularized Second-Order Low-Rank Representation for Web Service QoS Prediction","abstract":"Quality-of-Service (QoS) data plays a crucial role in cloud service selection. Since users cannot access all services, QoS can be represented by a high-dimensional and incomplete (HDI) matrix. Latent factor analysis (LFA) models have been proven effective as low-rank representation techniques for addressing this issue. However, most LFA models rely on first-order optimizers and use L2-norm regularization, which can lead to lower QoS prediction accuracy. To address this issue, this paper proposes a double regularized second-order latent factor (DRSLF) model with two key ideas: a) integrating L1-norm and L2-norm regularization terms to enhance the low-rank representation performance; b) incorporating second-order information by calculating the Hessian-vector product in each conjugate gradient step. Experimental results on two real-world response-time QoS datasets demonstrate that DRSLF has a higher low-rank representation capability than two baselines.","authors":["Hao Wu","Jialiang Wang"],"url":"https://arxiv.org/abs/2505.03822"}
{"created":"2025-05-08","title":"Memory Assisted LLM for Personalized Recommendation System","abstract":"Large language models (LLMs) have demonstrated significant potential in solving recommendation tasks. With proven capabilities in understanding user preferences, LLM personalization has emerged as a critical area for providing tailored responses to individuals. Current studies explore personalization through prompt design and fine-tuning, paving the way for further research in personalized LLMs. However, existing approaches are either costly and inefficient in capturing diverse user preferences or fail to account for timely updates to user history. To address these gaps, we propose the Memory-Assisted Personalized LLM (MAP). Through user interactions, we first create a history profile for each user, capturing their preferences, such as ratings for historical items. During recommendation, we extract relevant memory based on similarity, which is then incorporated into the prompts to enhance personalized recommendations. In our experiments, we evaluate MAP using a sequential rating prediction task under two scenarios: single domain, where memory and tasks are from the same category (e.g., movies), and cross-domain (e.g., memory from movies and recommendation tasks in books). The results show that MAP outperforms regular LLM-based recommenders that integrate user history directly through prompt design. Moreover, as user history grows, MAP's advantage increases in both scenarios, making it more suitable for addressing successive personalized user requests.","authors":["Jiarui Chen"],"url":"https://arxiv.org/abs/2505.03824"}
{"created":"2025-05-08","title":"Intelligently Augmented Contrastive Tensor Factorization: Empowering Multi-dimensional Time Series Classification in Low-Data Environments","abstract":"Classification of multi-dimensional time series from real-world systems require fine-grained learning of complex features such as cross-dimensional dependencies and intra-class variations-all under the practical challenge of low training data availability. However, standard deep learning (DL) struggles to learn generalizable features in low-data environments due to model overfitting. We propose a versatile yet data-efficient framework, Intelligently Augmented Contrastive Tensor Factorization (ITA-CTF), to learn effective representations from multi-dimensional time series. The CTF module learns core explanatory components of the time series (e.g., sensor factors, temporal factors), and importantly, their joint dependencies. Notably, unlike standard tensor factorization (TF), the CTF module incorporates a new contrastive loss optimization to induce similarity learning and class-awareness into the learnt representations for better classification performance. To strengthen this contrastive learning, the preceding ITA module generates targeted but informative augmentations that highlight realistic intra-class patterns in the original data, while preserving class-wise properties. This is achieved by dynamically sampling a \"soft\" class prototype to guide the warping of each query data sample, which results in an augmentation that is intelligently pattern-mixed between the \"soft\" class prototype and the query sample. These augmentations enable the CTF module to recognize complex intra-class variations despite the limited original training data, and seek out invariant class-wise properties for accurate classification performance. The proposed method is comprehensively evaluated on five different classification tasks. Compared to standard TF and several DL benchmarks, notable performance improvements up to 18.7% were achieved.","authors":["Anushiya Arunan","Yan Qin","Xiaoli Li","Yuen Chau"],"url":"https://arxiv.org/abs/2505.03825"}
{"created":"2025-05-08","title":"In-situ and Non-contact Etch Depth Prediction in Plasma Etching via Machine Learning (ANN & BNN) and Digital Image Colorimetry","abstract":"Precise monitoring of etch depth and the thickness of insulating materials, such as Silicon dioxide and silicon nitride, is critical to ensuring device performance and yield in semiconductor manufacturing. While conventional ex-situ analysis methods are accurate, they are constrained by time delays and contamination risks. To address these limitations, this study proposes a non-contact, in-situ etch depth prediction framework based on machine learning (ML) techniques. Two scenarios are explored. In the first scenario, an artificial neural network (ANN) is trained to predict average etch depth from process parameters, achieving a significantly lower mean squared error (MSE) compared to a linear baseline model. The approach is then extended to incorporate variability from repeated measurements using a Bayesian Neural Network (BNN) to capture both aleatoric and epistemic uncertainty. Coverage analysis confirms the BNN's capability to provide reliable uncertainty estimates. In the second scenario, we demonstrate the feasibility of using RGB data from digital image colorimetry (DIC) as input for etch depth prediction, achieving strong performance even in the absence of explicit process parameters. These results suggest that the integration of DIC and ML offers a viable, cost-effective alternative for real-time, in-situ, and non-invasive monitoring in plasma etching processes, contributing to enhanced process stability, and manufacturing efficiency.","authors":["Minji Kang","Seongho Kim","Eunseo Go","Donghyeon Paek","Geon Lim","Muyoung Kim","Soyeun Kim","Sung Kyu Jang","Min Sup Choi","Woo Seok Kang","Jaehyun Kim","Jaekwang Kim","Hyeong-U Kim"],"url":"https://arxiv.org/abs/2505.03826"}
{"created":"2025-05-08","title":"MISE: Meta-knowledge Inheritance for Social Media-Based Stressor Estimation","abstract":"Stress haunts people in modern society, which may cause severe health issues if left unattended. With social media becoming an integral part of daily life, leveraging social media to detect stress has gained increasing attention. While the majority of the work focuses on classifying stress states and stress categories, this study introduce a new task aimed at estimating more specific stressors (like exam, writing paper, etc.) through users' posts on social media. Unfortunately, the diversity of stressors with many different classes but a few examples per class, combined with the consistent arising of new stressors over time, hinders the machine understanding of stressors. To this end, we cast the stressor estimation problem within a practical scenario few-shot learning setting, and propose a novel meta-learning based stressor estimation framework that is enhanced by a meta-knowledge inheritance mechanism. This model can not only learn generic stressor context through meta-learning, but also has a good generalization ability to estimate new stressors with little labeled data. A fundamental breakthrough in our approach lies in the inclusion of the meta-knowledge inheritance mechanism, which equips our model with the ability to prevent catastrophic forgetting when adapting to new stressors. The experimental results show that our model achieves state-of-the-art performance compared with the baselines. Additionally, we construct a social media-based stressor estimation dataset that can help train artificial intelligence models to facilitate human well-being. The dataset is now public at \\href{https://www.kaggle.com/datasets/xinwangcs/stressor-cause-of-mental-health-problem-dataset}{\\underline{Kaggle}} and \\href{https://huggingface.co/datasets/XinWangcs/Stressor}{\\underline{Hugging Face}}.","authors":["Xin Wang","Ling Feng","Huijun Zhang","Lei Cao","Kaisheng Zeng","Qi Li","Yang Ding","Yi Dai","David Clifton"],"url":"https://arxiv.org/abs/2505.03827"}
{"created":"2025-05-08","title":"Sentiment-Aware Recommendation Systems in E-Commerce: A Review from a Natural Language Processing Perspective","abstract":"E-commerce platforms generate vast volumes of user feedback, such as star ratings, written reviews, and comments. However, most recommendation engines rely primarily on numerical scores, often overlooking the nuanced opinions embedded in free text. This paper comprehensively reviews sentiment-aware recommendation systems from a natural language processing perspective, covering advancements from 2023 to early 2025. It highlights the benefits of integrating sentiment analysis into e-commerce recommenders to enhance prediction accuracy and explainability through detailed opinion extraction. Our survey categorizes recent work into four main approaches: deep learning classifiers that combine sentiment embeddings with user item interactions, transformer based methods for nuanced feature extraction, graph neural networks that propagate sentiment signals, and conversational recommenders that adapt in real time to user feedback. We summarize model architectures and demonstrate how sentiment flows through recommendation pipelines, impacting dialogue-based suggestions. Key challenges include handling noisy or sarcastic text, dynamic user preferences, and bias mitigation. Finally, we outline research gaps and provide a roadmap for developing smarter, fairer, and more user-centric recommendation tools.","authors":["Yogesh Gajula"],"url":"https://arxiv.org/abs/2505.03828"}
{"created":"2025-05-08","title":"VideoLLM Benchmarks and Evaluation: A Survey","abstract":"The rapid development of Large Language Models (LLMs) has catalyzed significant advancements in video understanding technologies. This survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for Video Large Language Models (VideoLLMs). We examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. The paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. We highlight the performance trends of state-of-the-art VideoLLMs across these benchmarks and identify key challenges in current evaluation frameworks. Additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. This survey aims to equip researchers with a structured understanding of how to effectively evaluate VideoLLMs and identify promising avenues for advancing the field of video understanding with large language models.","authors":["Yogesh Kumar"],"url":"https://arxiv.org/abs/2505.03829"}
{"created":"2025-05-08","title":"Bridging Model Predictive Control and Deep Learning for Scalable Reachability Analysis","abstract":"Hamilton-Jacobi (HJ) reachability analysis is a widely used method for ensuring the safety of robotic systems. Traditional approaches compute reachable sets by numerically solving an HJ Partial Differential Equation (PDE) over a grid, which is computationally prohibitive due to the curse of dimensionality. Recent learning-based methods have sought to address this challenge by approximating reachability solutions using neural networks trained with PDE residual error. However, these approaches often suffer from unstable training dynamics and suboptimal solutions due to the weak learning signal provided by the residual loss. In this work, we propose a novel approach that leverages model predictive control (MPC) techniques to guide and accelerate the reachability learning process. Observing that HJ reachability is inherently rooted in optimal control, we utilize MPC to generate approximate reachability solutions at key collocation points, which are then used to tactically guide the neural network training by ensuring compliance with these approximations. Moreover, we iteratively refine the MPC generated solutions using the learned reachability solution, mitigating convergence to local optima. Case studies on a 2D vertical drone, a 13D quadrotor, a 7D F1Tenth car, and a 40D publisher-subscriber system demonstrate that bridging MPC with deep learning yields significant improvements in the robustness and accuracy of reachable sets, as well as corresponding safety assurances, compared to existing methods.","authors":["Zeyuan Feng","Le Qiu","Somil Bansal"],"url":"https://arxiv.org/abs/2505.03830"}
{"created":"2025-05-08","title":"A Comprehensive Analysis of Adversarial Attacks against Spam Filters","abstract":"Deep learning has revolutionized email filtering, which is critical to protect users from cyber threats such as spam, malware, and phishing. However, the increasing sophistication of adversarial attacks poses a significant challenge to the effectiveness of these filters. This study investigates the impact of adversarial attacks on deep learning-based spam detection systems using real-world datasets. Six prominent deep learning models are evaluated on these datasets, analyzing attacks at the word, character sentence, and AI-generated paragraph-levels. Novel scoring functions, including spam weights and attention weights, are introduced to improve attack effectiveness. This comprehensive analysis sheds light on the vulnerabilities of spam filters and contributes to efforts to improve their security against evolving adversarial threats.","authors":["Esra Hoto\\u{g}lu","Sevil Sen","Burcu Can"],"url":"https://arxiv.org/abs/2505.03831"}
{"created":"2025-05-08","title":"Video Forgery Detection for Surveillance Cameras: A Review","abstract":"The widespread availability of video recording through smartphones and digital devices has made video-based evidence more accessible than ever. Surveillance footage plays a crucial role in security, law enforcement, and judicial processes. However, with the rise of advanced video editing tools, tampering with digital recordings has become increasingly easy, raising concerns about their authenticity. Ensuring the integrity of surveillance videos is essential, as manipulated footage can lead to misinformation and undermine judicial decisions. This paper provides a comprehensive review of existing forensic techniques used to detect video forgery, focusing on their effectiveness in verifying the authenticity of surveillance recordings. Various methods, including compression-based analysis, frame duplication detection, and machine learning-based approaches, are explored. The findings highlight the growing necessity for more robust forensic techniques to counteract evolving forgery methods. Strengthening video forensic capabilities will ensure that surveillance recordings remain credible and admissible as legal evidence.","authors":["Noor B. Tayfor","Tarik A. Rashid","Shko M. Qader","Bryar A. Hassan","Mohammed H. Abdalla","Jafar Majidpour","Aram M. Ahmed","Hussein M. Ali","Aso M. Aladdin","Abdulhady A. Abdullah","Ahmed S. Shamsaldin","Haval M. Sidqi","Abdulrahman Salih","Zaher M. Yaseen","Azad A. Ameen","Janmenjoy Nayak","Mahmood Yashar Hamza"],"url":"https://arxiv.org/abs/2505.03832"}
{"created":"2025-05-08","title":"PointExplainer: Towards Transparent Parkinson's Disease Diagnosis","abstract":"Deep neural networks have shown potential in analyzing digitized hand-drawn signals for early diagnosis of Parkinson's disease. However, the lack of clear interpretability in existing diagnostic methods presents a challenge to clinical trust. In this paper, we propose PointExplainer, an explainable diagnostic strategy to identify hand-drawn regions that drive model diagnosis. Specifically, PointExplainer assigns discrete attribution values to hand-drawn segments, explicitly quantifying their relative contributions to the model's decision. Its key components include: (i) a diagnosis module, which encodes hand-drawn signals into 3D point clouds to represent hand-drawn trajectories, and (ii) an explanation module, which trains an interpretable surrogate model to approximate the local behavior of the black-box diagnostic model. We also introduce consistency measures to further address the issue of faithfulness in explanations. Extensive experiments on two benchmark datasets and a newly constructed dataset show that PointExplainer can provide intuitive explanations with no diagnostic performance degradation. The source code is available at https://github.com/chaoxuewang/PointExplainer.","authors":["Xuechao Wang","Sven Nomm","Junqing Huang","Kadri Medijainen","Aaro Toomela","Michael Ruzhansky"],"url":"https://arxiv.org/abs/2505.03833"}
{"created":"2025-05-08","title":"The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea","abstract":"The adoption of open science has quickly changed how artificial intelligence (AI) policy research is distributed globally. This study examines the regional trends in the citation of preprints, specifically focusing on the impact of two major disruptive events: the COVID-19 pandemic and the release of ChatGPT, on research dissemination patterns in the United States, Europe, and South Korea from 2015 to 2024. Using bibliometrics data from the Web of Science, this study tracks how global disruptive events influenced the adoption of preprints in AI policy research and how such shifts vary by region. By marking the timing of these disruptive events, the analysis reveals that while all regions experienced growth in preprint citations, the magnitude and trajectory of change varied significantly. The United States exhibited sharp, event-driven increases; Europe demonstrated institutional growth; and South Korea maintained consistent, linear growth in preprint adoption. These findings suggest that global disruptions may have accelerated preprint adoption, but the extent and trajectory are shaped by local research cultures, policy environments, and levels of open science maturity. This paper emphasizes the need for future AI governance strategies to consider regional variability in research dissemination and highlights opportunities for further longitudinal and comparative research to deepen our understanding of open-access adoption in AI policy development.","authors":["Simon Suh","Jihyuk Bang","Ji Woo Han"],"url":"https://arxiv.org/abs/2505.03835"}
{"created":"2025-05-08","title":"OBD-Finder: Explainable Coarse-to-Fine Text-Centric Oracle Bone Duplicates Discovery","abstract":"Oracle Bone Inscription (OBI) is the earliest systematic writing system in China, while the identification of Oracle Bone (OB) duplicates is a fundamental issue in OBI research. In this work, we design a progressive OB duplicate discovery framework that combines unsupervised low-level keypoints matching with high-level text-centric content-based matching to refine and rank the candidate OB duplicates with semantic awareness and interpretability. We compare our approach with state-of-the-art content-based image retrieval and image matching methods, showing that our approach yields comparable recall performance and the highest simplified mean reciprocal rank scores for both Top-5 and Top-15 retrieval results, and with significantly accelerated computation efficiency. We have discovered over 60 pairs of new OB duplicates in real-world deployment, which were missed by OBI researchers for decades. The models, video illustration and demonstration of this work are available at: https://github.com/cszhangLMU/OBD-Finder/.","authors":["Chongsheng Zhang","Shuwen Wu","Yingqi Chen","Matthias A{\\ss}enmacher","Christian Heumann","Yi Men","Gaojuan Fan","Jo\\~ao Gama"],"url":"https://arxiv.org/abs/2505.03836"}
{"created":"2025-05-08","title":"Explainable Face Recognition via Improved Localization","abstract":"Biometric authentication has become one of the most widely used tools in the current technological era to authenticate users and to distinguish between genuine users and imposters. Face is the most common form of biometric modality that has proven effective. Deep learning-based face recognition systems are now commonly used across different domains. However, these systems usually operate like black-box models that do not provide necessary explanations or justifications for their decisions. This is a major disadvantage because users cannot trust such artificial intelligence-based biometric systems and may not feel comfortable using them when clear explanations or justifications are not provided. This paper addresses this problem by applying an efficient method for explainable face recognition systems. We use a Class Activation Mapping (CAM)-based discriminative localization (very narrow/specific localization) technique called Scaled Directed Divergence (SDD) to visually explain the results of deep learning-based face recognition systems. We perform fine localization of the face features relevant to the deep learning model for its prediction/decision. Our experiments show that the SDD Class Activation Map (CAM) highlights the relevant face features very specifically compared to the traditional CAM and very accurately. The provided visual explanations with narrow localization of relevant features can ensure much-needed transparency and trust for deep learning-based face recognition systems.","authors":["Rashik Shadman","Daqing Hou","Faraz Hussain","M G Sarwar Murshed"],"url":"https://arxiv.org/abs/2505.03837"}
{"created":"2025-05-08","title":"An Adaptive Data-Resilient Multi-Modal Framework for Hierarchical Multi-Label Book Genre Identification","abstract":"Identifying the finer details of a book's genres enhances user experience by enabling efficient book discovery and personalized recommendations, ultimately improving reader engagement and satisfaction. It also provides valuable insights into market trends and consumer preferences, allowing publishers and marketers to make data-driven decisions regarding book production and marketing strategies. While traditional book genre classification methods primarily rely on review data or textual analysis, incorporating additional modalities, such as book covers, blurbs, and metadata, can offer richer context and improve prediction accuracy. However, the presence of incomplete or noisy information across these modalities presents a significant challenge. This paper introduces IMAGINE (Intelligent Multi-modal Adaptive Genre Identification NEtwork), a framework designed to address these complexities. IMAGINE extracts robust feature representations from multiple modalities and dynamically selects the most informative sources based on data availability. It employs a hierarchical classification strategy to capture genre relationships and remains adaptable to varying input conditions. Additionally, we curate a hierarchical genre classification dataset that structures genres into a well-defined taxonomy, accommodating the diverse nature of literary works. IMAGINE integrates information from multiple sources and assigns multiple genre labels to each book, ensuring a more comprehensive classification. A key feature of our framework is its resilience to incomplete data, enabling accurate predictions even when certain modalities, such as text, images, or metadata, are missing or incomplete. Experimental results show that IMAGINE outperformed existing baselines in genre classification accuracy, particularly in scenarios with insufficient modality-specific data.","authors":["Utsav Kumar Nareti","Soumi Chattopadhyay","Prolay Mallick","Suraj Kumar","Ayush Vikas Daga","Chandranath Adak","Adarsh Wase","Arjab Roy"],"url":"https://arxiv.org/abs/2505.03839"}
{"created":"2025-05-08","title":"CoCoB: Adaptive Collaborative Combinatorial Bandits for Online Recommendation","abstract":"Clustering bandits have gained significant attention in recommender systems by leveraging collaborative information from neighboring users to better capture target user preferences. However, these methods often lack a clear definition of similar users and face challenges when users with unique preferences lack appropriate neighbors. In such cases, relying on divergent preferences of misidentified neighbors can degrade recommendation quality. To address these limitations, this paper proposes an adaptive Collaborative Combinatorial Bandits algorithm (CoCoB). CoCoB employs an innovative two-sided bandit architecture, applying bandit principles to both the user and item sides. The user-bandit employs an enhanced Bayesian model to explore user similarity, identifying neighbors based on a similarity probability threshold. The item-bandit treats items as arms, generating diverse recommendations informed by the user-bandit's output. CoCoB dynamically adapts, leveraging neighbor preferences when available or focusing solely on the target user otherwise. Regret analysis under a linear contextual bandit setting and experiments on three real-world datasets demonstrate CoCoB's effectiveness, achieving an average 2.4% improvement in F1 score over state-of-the-art methods.","authors":["Cairong Yan","Jinyi Han","Jin Ju","Yanting Zhang","Zijian Wang","Xuan Shao"],"url":"https://arxiv.org/abs/2505.03840"}
{"created":"2025-05-08","title":"Contact-Aware Safety in Soft Robots Using High-Order Control Barrier and Lyapunov Functions","abstract":"Robots operating alongside people, particularly in sensitive scenarios such as aiding the elderly with daily tasks or collaborating with workers in manufacturing, must guarantee safety and cultivate user trust. Continuum soft manipulators promise safety through material compliance, but as designs evolve for greater precision, payload capacity, and speed, and increasingly incorporate rigid elements, their injury risk resurfaces. In this letter, we introduce a comprehensive High-Order Control Barrier Function (HOCBF) + High-Order Control Lyapunov Function (HOCLF) framework that enforces strict contact force limits across the entire soft-robot body during environmental interactions. Our approach combines a differentiable Piecewise Cosserat-Segment (PCS) dynamics model with a convex-polygon distance approximation metric, named Differentiable Conservative Separating Axis Theorem (DCSAT), based on the soft robot geometry to enable real-time, whole-body collision detection, resolution, and enforcement of the safety constraints. By embedding HOCBFs into our optimization routine, we guarantee safety and actively regulate environmental coupling, allowing, for instance, safe object manipulation under HOCLF-driven motion objectives. Extensive planar simulations demonstrate that our method maintains safety-bounded contacts while achieving precise shape and task-space regulation. This work thus lays a foundation for the deployment of soft robots in human-centric environments with provable safety and performance.","authors":["Kiwan Wong","Maximilian St\\\"olzle","Wei Xiao","Cosimo Della Santina","Daniela Rus","Gioele Zardini"],"url":"https://arxiv.org/abs/2505.03841"}
{"created":"2025-05-08","title":"Coverage Biases in High-Resolution Satellite Imagery","abstract":"Satellite imagery is increasingly used to complement traditional data collection approaches such as surveys and censuses across scientific disciplines. However, we ask: Do all places on earth benefit equally from this new wealth of information? In this study, we investigate coverage bias of major satellite constellations that provide optical satellite imagery with a ground sampling distance below 10 meters, evaluating both the future on-demand tasking opportunities as well as the availability of historic images across the globe. Specifically, forward-looking, we estimate how often different places are revisited during a window of 30 days based on the satellites' orbital paths, thus investigating potential coverage biases caused by physical factors. We find that locations farther away from the equator are generally revisited more frequently by the constellations under study. Backward-looking, we show that historic satellite image availability -- based on metadata collected from major satellite imagery providers -- is influenced by socio-economic factors on the ground: less developed, less populated places have less satellite images available. Furthermore, in three small case studies on recent conflict regions in this world, namely Gaza, Sudan and Ukraine, we show that also geopolitical events play an important role in satellite image availability, hinting at underlying business model decisions. These insights lay bare that the digital dividend yielded by satellite imagery is not equally distributed across our planet.","authors":["Vadim Musienko","Axel Jacquet","Ingmar Weber","Till Koebe"],"url":"https://arxiv.org/abs/2505.03842"}
{"created":"2025-05-08","title":"Economic Security of Multiple Shared Security Protocols","abstract":"As restaking protocols gain adoption across blockchain ecosystems, there is a need for Actively Validated Services (AVSs) to span multiple Shared Security Providers (SSPs). This leads to stake fragmentation which introduces new complications where an adversary may compromise an AVS by targeting its weakest SSP. In this paper, we formalize the Multiple SSP Problem and analyze two architectures : an isolated fragmented model called Model $\\mathbb{M}$ and a shared unified model called Model $\\mathbb{S}$, through a convex optimization and game-theoretic lens. We derive utility bounds, attack cost conditions, and market equilibrium that describes protocol security for both models. Our results show that while Model $\\mathbb{M}$ offers deployment flexibility, it inherits lowest-cost attack vulnerabilities, whereas Model $\\mathbb{S}$ achieves tighter security guarantees through single validator sets and aggregated slashing logic. We conclude with future directions of work including an incentive-compatible stake rebalancing allocation in restaking ecosystems.","authors":["Abhimanyu Nag","Dhruv Bodani","Abhishek Kumar"],"url":"https://arxiv.org/abs/2505.03843"}
{"created":"2025-05-08","title":"GAME: Learning Multimodal Interactions via Graph Structures for Personality Trait Estimation","abstract":"Apparent personality analysis from short videos poses significant chal-lenges due to the complex interplay of visual, auditory, and textual cues. In this paper, we propose GAME, a Graph-Augmented Multimodal Encoder designed to robustly model and fuse multi-source features for automatic personality prediction. For the visual stream, we construct a facial graph and introduce a dual-branch Geo Two-Stream Network, which combines Graph Convolutional Networks (GCNs) and Convolutional Neural Net-works (CNNs) with attention mechanisms to capture both structural and appearance-based facial cues. Complementing this, global context and iden-tity features are extracted using pretrained ResNet18 and VGGFace back-bones. To capture temporal dynamics, frame-level features are processed by a BiGRU enhanced with temporal attention modules. Meanwhile, audio representations are derived from the VGGish network, and linguistic se-mantics are captured via the XLM-Roberta transformer. To achieve effective multimodal integration, we propose a Channel Attention-based Fusion module, followed by a Multi-Layer Perceptron (MLP) regression head for predicting personality traits. Extensive experiments show that GAME con-sistently outperforms existing methods across multiple benchmarks, vali-dating its effectiveness and generalizability.","authors":["Kangsheng Wang","Yuhang Li","Chengwei Ye","Yufei Lin","Huanzhen Zhang","Bohan Hu","Linuo Xu","Shuyan Liu"],"url":"https://arxiv.org/abs/2505.03846"}
{"created":"2025-05-08","title":"Event-aware analysis of cross-city visitor flows using large language models and social media data","abstract":"Public events, such as music concerts and fireworks displays, can cause irregular surges in cross-city travel demand, leading to potential overcrowding, travel delays, and public safety concerns. To better anticipate and accommodate such demand surges, it is essential to estimate cross-city visitor flows with awareness of public events. Although prior studies typically focused on the effects of a single mega event or disruptions around a single venue, this study introduces a generalizable framework to analyze visitor flows under diverse and concurrent events. We propose to leverage large language models (LLMs) to extract event features from multi-source online information and massive user-generated content on social media platforms. Specifically, social media popularity metrics are designed to capture the effects of online promotion and word-of-mouth in attracting visitors. An event-aware machine learning model is then adopted to uncover the specific impacts of different event features and ultimately predict visitor flows for upcoming events. Using Hong Kong as a case study, the framework is applied to predict daily flows of mainland Chinese visitors arriving at the city, achieving a testing R-squared of over 85%. We further investigate the heterogeneous event impacts on visitor numbers across different event types and major travel modes. Both promotional popularity and word-of-mouth popularity are found to be associated with increased visitor flows, but the specific effects vary by the event type. This association is more pronounced among visitors arriving by metro and high-speed rail, while it has less effect on air travelers. The findings can facilitate coordinated measures across government agencies and guide specialized transport policies, such as shuttle transit services to event venues, and comprehensive on-site traffic management strategies.","authors":["Xiaohan Wang","Zhan Zhao","Ruiyu Wang","Yang Xu"],"url":"https://arxiv.org/abs/2505.03847"}
{"created":"2025-05-08","title":"Advanced Clustering Framework for Semiconductor Image Analytics Integrating Deep TDA with Self-Supervised and Transfer Learning Techniques","abstract":"Semiconductor manufacturing generates vast amounts of image data, crucial for defect identification and yield optimization, yet often exceeds manual inspection capabilities. Traditional clustering techniques struggle with high-dimensional, unlabeled data, limiting their effectiveness in capturing nuanced patterns. This paper introduces an advanced clustering framework that integrates deep Topological Data Analysis (TDA) with self-supervised and transfer learning techniques, offering a novel approach to unsupervised image clustering. TDA captures intrinsic topological features, while self-supervised learning extracts meaningful representations from unlabeled data, reducing reliance on labeled datasets. Transfer learning enhances the framework's adaptability and scalability, allowing fine-tuning to new datasets without retraining from scratch. Validated on synthetic and open-source semiconductor image datasets, the framework successfully identifies clusters aligned with defect patterns and process variations. This study highlights the transformative potential of combining TDA, self-supervised learning, and transfer learning, providing a scalable solution for proactive process monitoring and quality control in semiconductor manufacturing and other domains with large-scale image datasets.","authors":["Janhavi Giri","Attila Lengyel","Don Kent","Edward Kibardin"],"url":"https://arxiv.org/abs/2505.03848"}
{"created":"2025-05-08","title":"Improved Dimensionality Reduction for Inverse Problems in Nuclear Fusion and High-Energy Astrophysics","abstract":"Many inverse problems in nuclear fusion and high-energy astrophysics research, such as the optimization of tokamak reactor geometries or the inference of black hole parameters from interferometric images, necessitate high-dimensional parameter scans and large ensembles of simulations to be performed. Such inverse problems typically involve large uncertainties, both in the measurement parameters being inverted and in the underlying physics models themselves. Monte Carlo sampling, when combined with modern non-linear dimensionality reduction techniques such as autoencoders and manifold learning, can be used to reduce the size of the parameter spaces considerably. However, there is no guarantee that the resulting combinations of parameters will be physically valid, or even mathematically consistent. In this position paper, we advocate adopting a hybrid approach that leverages our recent advances in the development of formal verification methods for numerical algorithms, with the goal of constructing parameter space restrictions with provable mathematical and physical correctness properties, whilst nevertheless respecting both experimental uncertainties and uncertainties in the underlying physical processes.","authors":["Jonathan Gorard","Ammar Hakim","Hong Qin","Kyle Parfrey","Shantenu Jha"],"url":"https://arxiv.org/abs/2505.03849"}
{"created":"2025-05-08","title":"Impact Analysis of Inference Time Attack of Perception Sensors on Autonomous Vehicles","abstract":"As a safety-critical cyber-physical system, cybersecurity and related safety issues for Autonomous Vehicles (AVs) have been important research topics for a while. Among all the modules on AVs, perception is one of the most accessible attack surfaces, as drivers and AVs have no control over the outside environment. Most current work targeting perception security for AVs focuses on perception correctness. In this work, we propose an impact analysis based on inference time attacks for autonomous vehicles. We demonstrate in a simulation system that such inference time attacks can also threaten the safety of both the ego vehicle and other traffic participants.","authors":["Hanlin Chen","Simin Chen","Wenyu Li","Wei Yang","Yiheng Feng"],"url":"https://arxiv.org/abs/2505.03850"}
{"created":"2025-05-08","title":"The Evaluation of Open Source Software Innovativeness","abstract":"Product innovation assessment in software sector is a timely topic. Nevertheless, research on that subject is particularly scant. As a result, there is a lack of criteria to measure software innovativeness. In a context of theoretical and practical controversy in the open source field, this article assesses open source software innovativeness. Based on almost 500 cases studies and with the collaboration of 125 experts from industry, services and research fields, it suggests an innovation typology supported by the notion of functional added value. It provides also an innovation modelling framework that combines main evaluation methodologies. By showing the shortcomings of widely used innovation metrics, this research supports a new approach of innovativeness assessment specialized in each sector.","authors":["Nordine Benkeltoum"],"url":"https://arxiv.org/abs/2505.03855"}
{"created":"2025-05-08","title":"An Active Inference Model of Covert and Overt Visual Attention","abstract":"The ability to selectively attend to relevant stimuli while filtering out distractions is essential for agents that process complex, high-dimensional sensory input. This paper introduces a model of covert and overt visual attention through the framework of active inference, utilizing dynamic optimization of sensory precisions to minimize free-energy. The model determines visual sensory precisions based on both current environmental beliefs and sensory input, influencing attentional allocation in both covert and overt modalities. To test the effectiveness of the model, we analyze its behavior in the Posner cueing task and a simple target focus task using two-dimensional(2D) visual data. Reaction times are measured to investigate the interplay between exogenous and endogenous attention, as well as valid and invalid cueing. The results show that exogenous and valid cues generally lead to faster reaction times compared to endogenous and invalid cues. Furthermore, the model exhibits behavior similar to inhibition of return, where previously attended locations become suppressed after a specific cue-target onset asynchrony interval. Lastly, we investigate different aspects of overt attention and show that involuntary, reflexive saccades occur faster than intentional ones, but at the expense of adaptability.","authors":["Tin Mi\\v{s}i\\'c","Karlo Koledi\\'c","Fabio Bonsignorio","Ivan Petrovi\\'c","Ivan Markovi\\'c"],"url":"https://arxiv.org/abs/2505.03856"}
{"created":"2025-05-08","title":"Differentially Private Densest-$k$-Subgraph","abstract":"Many graph datasets involve sensitive network data, motivating the need for privacy-preserving graph mining. The Densest-$k$-subgraph (D$k$S) problem is a key primitive in graph mining that aims to extract a subset of $k$ vertices with the maximum internal connectivity. Although non-private algorithms are known for D$k$S, this paper is the first to design algorithms that offer formal differential privacy (DP) guarantees for the problem. We base our general approach on using the principal component (PC) of the graph adjacency matrix to output a subset of $k$ vertices under edge DP. For this task, we first consider output perturbation, which traditionally offer good scalability, but at the expense of utility. Our tight on the local sensitivity indicate a big gap with the global sensitivity, motivating the use of instance specific sensitive methods for private PC. Next, we derive a tight bound on the smooth sensitivity and show that it can be close to the global sensitivity. This leads us to consider the Propose-Test-Release (PTR) framework for private PC. Although computationally expensive in general, we design a novel approach for implementing PTR in the same time as computation of a non-private PC, while offering good utility for \\DkS{}. Additionally, we also consider the iterative private power method (PPM) for private PC, albeit it is significantly slower than PTR on large networks. We run our methods on diverse real-world networks, with the largest having 3 million vertices, and show good privacy-utility trade-offs. Although PTR requires a slightly larger privacy budget, on average, it achieves a 180-fold improvement in runtime over PPM.","authors":["Alireza Khayatian","Anil Vullikanti","Aritra Konar"],"url":"https://arxiv.org/abs/2505.03858"}
{"created":"2025-05-08","title":"Deepfakes on Demand: the rise of accessible non-consensual deepfake image generators","abstract":"Advances in multimodal machine learning have made text-to-image (T2I) models increasingly accessible and popular. However, T2I models introduce risks such as the generation of non-consensual depictions of identifiable individuals, otherwise known as deepfakes. This paper presents an empirical study exploring the accessibility of deepfake model variants online. Through a metadata analysis of thousands of publicly downloadable model variants on two popular repositories, Hugging Face and Civitai, we demonstrate a huge rise in easily accessible deepfake models. Almost 35,000 examples of publicly downloadable deepfake model variants are identified, primarily hosted on Civitai. These deepfake models have been downloaded almost 15 million times since November 2022, with the models targeting a range of individuals from global celebrities to Instagram users with under 10,000 followers. Both Stable Diffusion and Flux models are used for the creation of deepfake models, with 96% of these targeting women and many signalling intent to generate non-consensual intimate imagery (NCII). Deepfake model variants are often created via the parameter-efficient fine-tuning technique known as low rank adaptation (LoRA), requiring as few as 20 images, 24GB VRAM, and 15 minutes of time, making this process widely accessible via consumer-grade computers. Despite these models violating the Terms of Service of hosting platforms, and regulation seeking to prevent dissemination, these results emphasise the pressing need for greater action to be taken against the creation of deepfakes and NCII.","authors":["Will Hawkins","Chris Russell","Brent Mittelstadt"],"url":"https://arxiv.org/abs/2505.03859"}
{"created":"2025-05-08","title":"Machine Learning: a Lecture Note","abstract":"This lecture note is intended to prepare early-year master's and PhD students in data science or a related discipline with foundational ideas in machine learning. It starts with basic ideas in modern machine learning with classification as a main target task. These basic ideas include loss formulation, backpropagation, stochastic gradient descent, generalization, model selection as well as fundamental blocks of artificial neural networks. Based on these basic ideas, the lecture note explores in depth the probablistic approach to unsupervised learning, covering directed latent variable models, product of experts, generative adversarial networks and autoregressive models. Finally, the note ends by covering a diverse set of further topics, such as reinforcement learning, ensemble methods and meta-learning. After reading this lecture note, a student should be ready to embark on studying and researching more advanced topics in machine learning and more broadly artificial intelligence.","authors":["Kyunghyun Cho"],"url":"https://arxiv.org/abs/2505.03861"}
{"created":"2025-05-08","title":"Data-Driven Falsification of Cyber-Physical Systems","abstract":"Cyber-Physical Systems (CPS) are abundant in safety-critical domains such as healthcare, avionics, and autonomous vehicles. Formal verification of their operational safety is, therefore, of utmost importance. In this paper, we address the falsification problem, where the focus is on searching for an unsafe execution in the system instead of proving their absence. The contribution of this paper is a framework that (a) connects the falsification of CPS with the falsification of deep neural networks (DNNs) and (b) leverages the inherent interpretability of Decision Trees for faster falsification of CPS. This is achieved by: (1) building a surrogate model of the CPS under test, either as a DNN model or a Decision Tree, (2) application of various DNN falsification tools to falsify CPS, and (3) a novel falsification algorithm guided by the explanations of safety violations of the CPS model extracted from its Decision Tree surrogate. The proposed framework has the potential to exploit a repertoire of \\emph{adversarial attack} algorithms designed to falsify robustness properties of DNNs, as well as state-of-the-art falsification algorithms for DNNs. Although the presented methodology is applicable to systems that can be executed/simulated in general, we demonstrate its effectiveness, particularly in CPS. We show that our framework, implemented as a tool \\textsc{FlexiFal}, can detect hard-to-find counterexamples in CPS that have linear and non-linear dynamics. Decision tree-guided falsification shows promising results in efficiently finding multiple counterexamples in the ARCH-COMP 2024 falsification benchmarks~\\cite{khandait2024arch}.","authors":["Atanu Kundu","Sauvik Gon","Rajarshi Ray"],"url":"https://arxiv.org/abs/2505.03863"}
{"created":"2025-05-08","title":"From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems","abstract":"Artificial intelligence is rapidly evolving towards multi-agent systems where numerous AI agents collaborate and interact with external tools. Two key open standards, Google's Agent to Agent (A2A) protocol for inter-agent communication and Anthropic's Model Context Protocol (MCP) for standardized tool access, promise to overcome the limitations of fragmented, custom integration approaches. While their potential synergy is significant, this paper argues that effectively integrating A2A and MCP presents unique, emergent challenges at their intersection, particularly concerning semantic interoperability between agent tasks and tool capabilities, the compounded security risks arising from combined discovery and execution, and the practical governance required for the envisioned \"Agent Economy\". This work provides a critical analysis, moving beyond a survey to evaluate the practical implications and inherent difficulties of combining these horizontal and vertical integration standards. We examine the benefits (e.g., specialization, scalability) while critically assessing their dependencies and trade-offs in an integrated context. We identify key challenges increased by the integration, including novel security vulnerabilities, privacy complexities, debugging difficulties across protocols, and the need for robust semantic negotiation mechanisms. In summary, A2A+MCP offers a vital architectural foundation, but fully realizing its potential requires substantial advancements to manage the complexities of their combined operation.","authors":["Qiaomu Li","Ying Xie"],"url":"https://arxiv.org/abs/2505.03864"}
{"created":"2025-05-08","title":"Enhancing Women's Experiences in Software Engineering","abstract":"Context: Women face many challenges in their lives, which affect their daily experiences and influence major life decisions, starting before they enroll in bachelor's programs, setting a difficult path for those aspiring to enter the software development industry. Goal: To explore the challenges that women face across three different life stages, beginning as high school students, continuing as university undergraduates, and extending into their professional lives, as well as potential solutions to address these challenges. Research Method: We conducted a literature review followed by workshops to understand the perspectives of high school women, undergraduates, and practitioners regarding the same set of challenges and solutions identified in the literature. Results: Regardless of the life stage, women feel discouraged in a toxic environment often characterized by a lack of inclusion, harassment, and the exhausting need to prove themselves. We also discovered that some challenges are specific to certain life stages; for example, issues related to maternity were mentioned only by practitioners. Conclusions: Gender-related challenges arise before women enter the software development field when the proportion of men and women is still similar. While the need to prove themselves is mentioned at all three stages, high school women's challenges are more often directed toward convincing their parents that they are mature enough to handle their responsibilities. As they progress, the emphasis shifts to proving their competence in managing responsibilities for which they have received training. Increasing the inclusion of women in the field should, therefore, start earlier, and profound societal changes may be necessary to boost women's participation.","authors":["J\\'ulia Rocha Fortunato","Luana Ribeiro Soares","Gabriela Silva Alves","Edna Dias Canedo","Fabiana Freitas Mendes"],"url":"https://arxiv.org/abs/2505.03866"}
{"created":"2025-05-08","title":"Scratch Copilot: Supporting Youth Creative Coding with AI","abstract":"Creative coding platforms like Scratch have democratized programming for children, yet translating imaginative ideas into functional code remains a significant hurdle for many young learners. While AI copilots assist adult programmers, few tools target children in block-based environments. Building on prior research \\cite{druga_how_2021,druga2023ai, druga2023scratch}, we present Cognimates Scratch Copilot: an AI-powered assistant integrated into a Scratch-like environment, providing real-time support for ideation, code generation, debugging, and asset creation. This paper details the system architecture and findings from an exploratory qualitative evaluation with 18 international children (ages 7--12). Our analysis reveals how the AI Copilot supported key creative coding processes, particularly aiding ideation and debugging. Crucially, it also highlights how children actively negotiated the use of AI, demonstrating strong agency by adapting or rejecting suggestions to maintain creative control. Interactions surfaced design tensions between providing helpful scaffolding and fostering independent problem-solving, as well as learning opportunities arising from navigating AI limitations and errors. Findings indicate Cognimates Scratch Copilot's potential to enhance creative self-efficacy and engagement. Based on these insights, we propose initial design guidelines for AI coding assistants that prioritize youth agency and critical interaction alongside supportive scaffolding.","authors":["Stefania Druga","Amy J. Ko"],"url":"https://arxiv.org/abs/2505.03867"}
{"created":"2025-05-08","title":"Novel Extraction of Discriminative Fine-Grained Feature to Improve Retinal Vessel Segmentation","abstract":"Retinal vessel segmentation is a vital early detection method for several severe ocular diseases. Despite significant progress in retinal vessel segmentation with the advancement of Neural Networks, there are still challenges to overcome. Specifically, retinal vessel segmentation aims to predict the class label for every pixel within a fundus image, with a primary focus on intra-image discrimination, making it vital for models to extract more discriminative features. Nevertheless, existing methods primarily focus on minimizing the difference between the output from the decoder and the label, but ignore fully using feature-level fine-grained representations from the encoder. To address these issues, we propose a novel Attention U-shaped Kolmogorov-Arnold Network named AttUKAN along with a novel Label-guided Pixel-wise Contrastive Loss for retinal vessel segmentation. Specifically, we implement Attention Gates into Kolmogorov-Arnold Networks to enhance model sensitivity by suppressing irrelevant feature activations and model interpretability by non-linear modeling of KAN blocks. Additionally, we also design a novel Label-guided Pixel-wise Contrastive Loss to supervise our proposed AttUKAN to extract more discriminative features by distinguishing between foreground vessel-pixel pairs and background pairs. Experiments are conducted across four public datasets including DRIVE, STARE, CHASE_DB1, HRF and our private dataset. AttUKAN achieves F1 scores of 82.50%, 81.14%, 81.34%, 80.21% and 80.09%, along with MIoU scores of 70.24%, 68.64%, 68.59%, 67.21% and 66.94% in the above datasets, which are the highest compared to 11 networks for retinal vessel segmentation. Quantitative and qualitative results show that our AttUKAN achieves state-of-the-art performance and outperforms existing retinal vessel segmentation methods. Our code will be available at https://github.com/stevezs315/AttUKAN.","authors":["Shuang Zeng","Chee Hong Lee","Micky C Nnamdi","Wenqi Shi","J Ben Tamo","Lei Zhu","Hangzhou He","Xinliang Zhang","Qian Chen","May D. Wang","Yanye Lu","Qiushi Ren"],"url":"https://arxiv.org/abs/2505.03896"}
{"created":"2025-05-08","title":"Unveiling the Role of ChatGPT in Software Development: Insights from Developer-ChatGPT Interactions on GitHub","abstract":"The advent of Large Language Models (LLMs) has introduced a new paradigm in software engineering, with generative AI tools like ChatGPT gaining widespread adoption among developers. While ChatGPT's potential has been extensively discussed, there is limited empirical evidence exploring its real-world usage by developers. This study bridges this gap by conducting a large-scale empirical analysis of ChatGPT-assisted development activities, leveraging a curated dataset, DevChat, comprising 2,547 unique shared ChatGPT links collected from GitHub between May 2023 and June 2024. Our study examines the characteristics of ChatGPT's usage on GitHub (including the tendency, prompt turns distribution, and link descriptions) and identifies five categories of developers' purposes for sharing developer-ChatGPT conversations during software development. Additionally, we analyzed the development-related activities where developers shared ChatGPT links to facilitate their workflows. We then established a mapping framework among data sources, activities, and SE tasks associated with these shared ChatGPT links. Our study offers a comprehensive view of ChatGPT's application in real-world software development scenarios and provides a foundation for its future integration into software development workflows.","authors":["Ruiyin Li","Peng Liang","Yifei Wang","Yangxiao Cai","Weisong Sun","Zengyang Li"],"url":"https://arxiv.org/abs/2505.03901"}
{"created":"2025-05-08","title":"MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models","abstract":"Large language models (LLMs) have transformed software development through code generation capabilities, yet their effectiveness for high-performance computing (HPC) remains limited. HPC code requires specialized optimizations for parallelism, memory efficiency, and architecture-specific considerations that general-purpose LLMs often overlook. We present MARCO (Multi-Agent Reactive Code Optimizer), a novel framework that enhances LLM-generated code for HPC through a specialized multi-agent architecture. MARCO employs separate agents for code generation and performance evaluation, connected by a feedback loop that progressively refines optimizations. A key innovation is MARCO's web-search component that retrieves real-time optimization techniques from recent conference proceedings and research publications, bridging the knowledge gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem set demonstrates that MARCO achieves a 14.6% average runtime reduction compared to Claude 3.5 Sonnet alone, while the integration of the web-search component yields a 30.9% performance improvement over the base MARCO system. These results highlight the potential of multi-agent systems to address the specialized requirements of high-performance code generation, offering a cost-effective alternative to domain-specific model fine-tuning.","authors":["Asif Rahman","Veljko Cvetkovic","Kathleen Reece","Aidan Walters","Yasir Hassan","Aneesh Tummeti","Bryan Torres","Denise Cooney","Margaret Ellis","Dimitrios S. Nikolopoulos"],"url":"https://arxiv.org/abs/2505.03906"}
{"created":"2025-05-08","title":"Minimum Congestion Routing of Unsplittable Flows in Data-Center Networks","abstract":"Millions of flows are routed concurrently through a modern data-center. These networks are often built as Clos topologies, and flow demands are constrained only by the link capacities at the ingress and egress points. The minimum congestion routing problem seeks to route a set of flows through a data center while minimizing the maximum flow demand on any link. This is easily achieved by splitting flow demands along all available paths. However, arbitrary flow splitting is unrealistic. Instead, network operators rely on heuristics for routing unsplittable flows, the best of which results in a worst-case congestion of $2$ (twice the uniform link capacities). But is $2$ the lowest possible congestion? If not, can an efficient routing algorithm attain congestion below $2$?","authors":["Miguel Ferreira","Nirav Atre","Justine Sherry","Michael Dinitz","Jo\\~ao Lu\\'is Sobrinho"],"url":"https://arxiv.org/abs/2505.03908"}
{"created":"2025-05-08","title":"Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty","abstract":"Automating chest radiograph interpretation using Deep Learning (DL) models has the potential to significantly improve clinical workflows, decision-making, and large-scale health screening. However, in medical settings, merely optimising predictive performance is insufficient, as the quantification of uncertainty is equally crucial. This paper investigates the relationship between predictive uncertainty, derived from Bayesian Deep Learning approximations, and human/linguistic uncertainty, as estimated from free-text radiology reports labelled by rule-based labellers. Utilising BERT as the model of choice, this study evaluates different binarisation methods for uncertainty labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in estimating predictive uncertainty. The results demonstrate good model performance, but also a modest correlation between predictive and linguistic uncertainty, highlighting the challenges in aligning machine uncertainty with human interpretation nuances. Our findings suggest that while Bayesian approximations provide valuable uncertainty estimates, further refinement is necessary to fully capture and utilise the subtleties of human uncertainty in clinical applications.","authors":["Gianluca Manzo","Julia Ive"],"url":"https://arxiv.org/abs/2505.03910"}
{"created":"2025-05-08","title":"Explaining Anomalies with Tensor Networks","abstract":"Tensor networks, a class of variational quantum many-body wave functions have attracted considerable research interest across many disciplines, including classical machine learning. Recently, Aizpurua et al. demonstrated explainable anomaly detection with matrix product states on a discrete-valued cyber-security task, using quantum-inspired methods to gain insight into the learned model and detected anomalies. Here, we extend this framework to real-valued data domains. We furthermore introduce tree tensor networks for the task of explainable anomaly detection. We demonstrate these methods with three benchmark problems, show adequate predictive performance compared to several baseline models and both tensor network architectures' ability to explain anomalous samples. We thereby extend the application of tensor networks to a broader class of potential problems and open a pathway for future extensions to more complex tensor network architectures.","authors":["Hans Hohenfeld","Marius Beuerle","Elie Mounzer"],"url":"https://arxiv.org/abs/2505.03911"}
{"created":"2025-05-08","title":"OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation","abstract":"Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from. Project page: https://openhelix-robot.github.io/.","authors":["Can Cui","Pengxiang Ding","Wenxuan Song","Shuanghao Bai","Xinyang Tong","Zirui Ge","Runze Suo","Wanqi Zhou","Yang Liu","Bofang Jia","Han Zhao","Siteng Huang","Donglin Wang"],"url":"https://arxiv.org/abs/2505.03912"}
{"created":"2025-05-08","title":"Hybrid Quantum-Classical Maximum-Likelihood Detection via Grover-based Adaptive Search for RIS-assisted Broadband Wireless Systems","abstract":"The escalating complexity and stringent performance demands of sixth-generation wireless systems necessitate advanced signal processing methods capable of simultaneously achieving high spectral efficiency and low computational complexity, especially under frequency-selective propagation conditions. In this paper, we propose a hybrid quantum-classical detection framework for broadband systems enhanced by reconfigurable intelligent surfaces (RISs). We address the maximum likelihood detection (MLD) problem for RIS-aided broadband wireless communications by formulating it as a quadratic unconstrained binary optimization problem, that is then solved using Grover adaptive search (GAS). To accelerate convergence, we initialize the GAS algorithm with a threshold based on a classical minimum mean-squared error detector. The simulation results show that the proposed hybrid classical-quantum detection scheme achieves near-optimal MLD performance while substantially reducing query complexity. These findings highlight the potential of quantum-enhanced detection strategies combined with RIS technology, offering efficient and near-optimal solutions for broadband wireless communications.","authors":["Maryam Tariq","Raneem Abdelrahim","Omar Alhussein","Sami Muhaidat"],"url":"https://arxiv.org/abs/2505.03914"}
{"created":"2025-05-08","title":"Improving Failure Prediction in Aircraft Fastener Assembly Using Synthetic Data in Imbalanced Datasets","abstract":"Automating aircraft manufacturing still relies heavily on human labor due to the complexity of the assembly processes and customization requirements. One key challenge is achieving precise positioning, especially for large aircraft structures, where errors can lead to substantial maintenance costs or part rejection. Existing solutions often require costly hardware or lack flexibility. Used in aircraft by the thousands, threaded fasteners, e.g., screws, bolts, and collars, are traditionally executed by fixed-base robots and usually have problems in being deployed in the mentioned manufacturing sites. This paper emphasizes the importance of error detection and classification for efficient and safe assembly of threaded fasteners, especially aeronautical collars. Safe assembly of threaded fasteners is paramount since acquiring sufficient data for training deep learning models poses challenges due to the rarity of failure cases and imbalanced datasets. The paper addresses this by proposing techniques like class weighting and data augmentation, specifically tailored for temporal series data, to improve classification performance. Furthermore, the paper introduces a novel problem-modeling approach, emphasizing metrics relevant to collar assembly rather than solely focusing on accuracy. This tailored approach enhances the models' capability to handle the challenges of threaded fastener assembly effectively.","authors":["Gustavo J. G. Lahr","Ricardo V. Godoy","Thiago H. Segreto","Jose O. Savazzi","Arash Ajoudani","Thiago Boaventura","Glauco A. P. Caurin"],"url":"https://arxiv.org/abs/2505.03917"}
{"created":"2025-05-08","title":"Omnidirectional vision sensors based on catadioptric systems with discrete infrared photoreceptors for swarm robotics","abstract":"In this work, we fabricated and studied two designs for omnidirectional vision sensors for swarm robotics, based on catadioptric systems consisting of a mirror with rotational symmetry, eight discrete infrared photodiodes and a single LED, in order to provide localization and navigation abilities for mobile robotic agents. We considered two arrangements for the photodiodes: one in which they point upward into the mirror, and one in which they point outward, perpendicular to the mirror. To determine which design offers a better field of view on the plane, as well as detection of distance and orientation between two agents, we developed a test rail with three degrees of freedom to experimentally and systematically measure the signal registered by the photodiodes of a given sensor (in a single readout) from the light emitted by another as functions of the distance and orientation. Afterwards, we processed and analyzed the experimental data to develop mathematical models for the mean response of a photodiode in each design. Finally, by numerically inverting the models, we compared the two designs in terms of their accuracy. Our results show that the design with the photodiodes pointing upward resolves better the distance, while the other resolves better the orientation of the emitting agent, both providing an omnidirectional field of view.","authors":["Jose Fernando Contreras-Monsalvo","Victor Dossetti","Blanca Susana Soto-Cruz"],"url":"https://arxiv.org/abs/2505.03920"}
{"created":"2025-05-08","title":"SAND: One-Shot Feature Selection with Additive Noise Distortion","abstract":"Feature selection is a critical step in data-driven applications, reducing input dimensionality to enhance learning accuracy, computational efficiency, and interpretability. Existing state-of-the-art methods often require post-selection retraining and extensive hyperparameter tuning, complicating their adoption. We introduce a novel, non-intrusive feature selection layer that, given a target feature count $k$, automatically identifies and selects the $k$ most informative features during neural network training. Our method is uniquely simple, requiring no alterations to the loss function, network architecture, or post-selection retraining. The layer is mathematically elegant and can be fully described by: \\begin{align} \\nonumber \\tilde{x}_i = a_i x_i + (1-a_i)z_i \\end{align} where $x_i$ is the input feature, $\\tilde{x}_i$ the output, $z_i$ a Gaussian noise, and $a_i$ trainable gain such that $\\sum_i{a_i^2}=k$. This formulation induces an automatic clustering effect, driving $k$ of the $a_i$ gains to $1$ (selecting informative features) and the rest to $0$ (discarding redundant ones) via weighted noise distortion and gain normalization. Despite its extreme simplicity, our method delivers state-of-the-art performance on standard benchmark datasets and a novel real-world dataset, outperforming or matching existing approaches without requiring hyperparameter search for $k$ or retraining. Theoretical analysis in the context of linear regression further validates its efficacy. Our work demonstrates that simplicity and performance are not mutually exclusive, offering a powerful yet straightforward tool for feature selection in machine learning.","authors":["Pedram Pad","Hadi Hammoud","Mohamad Dia","Nadim Maamari","L. Andrea Dunbar"],"url":"https://arxiv.org/abs/2505.03923"}
{"created":"2025-05-08","title":"MIHRaGe: A Mixed-Reality Interface for Human-Robot Interaction via Gaze-Oriented Control","abstract":"Individuals with upper limb mobility impairments often require assistive technologies to perform activities of daily living. While gaze-tracking has emerged as a promising method for robotic assistance, existing solutions lack sufficient feedback mechanisms, leading to uncertainty in user intent recognition and reduced adaptability. This paper presents the MIHRAGe interface, an integrated system that combines gaze-tracking, robotic assistance, and a mixed-reality to create an immersive environment for controlling the robot using only eye movements. The system was evaluated through an experimental protocol involving four participants, assessing gaze accuracy, robotic positioning precision, and the overall success of a pick and place task. Results showed an average gaze fixation error of 1.46 cm, with individual variations ranging from 1.28 cm to 2.14 cm. The robotic arm demonstrated an average positioning error of +-1.53 cm, with discrepancies attributed to interface resolution and calibration constraints. In a pick and place task, the system achieved a success rate of 80%, highlighting its potential for improving accessibility in human-robot interaction with visual feedback to the user.","authors":["Rafael R. Baptista","Nina R. Gerszberg","Ricardo V. Godoy","Gustavo J. G. Lahr"],"url":"https://arxiv.org/abs/2505.03929"}
{"created":"2025-05-08","title":"NMPC-Lander: Nonlinear MPC with Barrier Function for UAV Landing on a Mobile Platform","abstract":"Quadcopters are versatile aerial robots gaining popularity in numerous critical applications. However, their operational effectiveness is constrained by limited battery life and restricted flight range. To address these challenges, autonomous drone landing on stationary or mobile charging and battery-swapping stations has become an essential capability. In this study, we present NMPC-Lander, a novel control architecture that integrates Nonlinear Model Predictive Control (NMPC) with Control Barrier Functions (CBF) to achieve precise and safe autonomous landing on both static and dynamic platforms. Our approach employs NMPC for accurate trajectory tracking and landing, while simultaneously incorporating CBF to ensure collision avoidance with static obstacles. Experimental evaluations on the real hardware demonstrate high precision in landing scenarios, with an average final position error of 9.0 cm and 11 cm for stationary and mobile platforms, respectively. Notably, NMPC-Lander outperforms the B-spline combined with the A* planning method by nearly threefold in terms of position tracking, underscoring its superior robustness and practical effectiveness.","authors":["Amber Batool","Faryal Batool","Roohan Ahmed Khan","Muhammad Ahsan Mustafa","Aleksey Fedoseev","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2505.03931"}
{"created":"2025-05-08","title":"GRAML: Dynamic Goal Recognition As Metric Learning","abstract":"Goal Recognition (GR) is the problem of recognizing an agent's objectives based on observed actions. Recent data-driven approaches for GR alleviate the need for costly, manually crafted domain models. However, these approaches can only reason about a pre-defined set of goals, and time-consuming training is needed for new emerging goals. To keep this model-learning automated while enabling quick adaptation to new goals, this paper introduces GRAML: Goal Recognition As Metric Learning. GRAML uses a Siamese network to treat GR as a deep metric learning task, employing an RNN that learns a metric over an embedding space, where the embeddings for observation traces leading to different goals are distant, and embeddings of traces leading to the same goals are close. This metric is especially useful when adapting to new goals, even if given just one example observation trace per goal. Evaluated on a versatile set of environments, GRAML shows speed, flexibility, and runtime improvements over the state-of-the-art GR while maintaining accurate recognition.","authors":["Matan Shamir","Reuth Mirsky"],"url":"https://arxiv.org/abs/2505.03941"}
{"created":"2025-05-08","title":"AI-Driven Security in Cloud Computing: Enhancing Threat Detection, Automated Response, and Cyber Resilience","abstract":"Cloud security concerns have been greatly realized in recent years due to the increase of complicated threats in the computing world. Many traditional solutions do not work well in real-time to detect or prevent more complex threats. Artificial intelligence is today regarded as a revolution in determining a protection plan for cloud data architecture through machine learning, statistical visualization of computing infrastructure, and detection of security breaches followed by counteraction. These AI-enabled systems make work easier as more network activities are scrutinized, and any anomalous behavior that might be a precursor to a more serious breach is prevented. This paper examines ways AI can enhance cloud security by applying predictive analytics, behavior-based security threat detection, and AI-stirring encryption. It also outlines the problems of the previous security models and how AI overcomes them. For a similar reason, issues like data privacy, biases in the AI model, and regulatory compliance are also covered. So, AI improves the protection of cloud computing contexts; however, more efforts are needed in the subsequent phases to extend the technology's reliability, modularity, and ethical aspects. This means that AI can be blended with other new computing technologies, including blockchain, to improve security frameworks further. The paper discusses the current trends in securing cloud data architecture using AI and presents further research and application directions.","authors":["Shamnad Mohamed Shaffi","Sunish Vengathattil","Jezeena Nikarthil Sidhick","Resmi Vijayan"],"url":"https://arxiv.org/abs/2505.03945"}
{"created":"2025-05-08","title":"Decentralized Distributed Proximal Policy Optimization (DD-PPO) for High Performance Computing Scheduling on Multi-User Systems","abstract":"Resource allocation in High Performance Computing (HPC) environments presents a complex and multifaceted challenge for job scheduling algorithms. Beyond the efficient allocation of system resources, schedulers must account for and optimize multiple performance metrics, including job wait time and system utilization. While traditional rule-based scheduling algorithms dominate the current deployments of HPC systems, the increasing heterogeneity and scale of those systems is expected to challenge the efficiency and flexibility of those algorithms in minimizing job wait time and maximizing utilization. Recent research efforts have focused on leveraging advancements in Reinforcement Learning (RL) to develop more adaptable and intelligent scheduling strategies. Recent RL-based scheduling approaches have explored a range of algorithms, from Deep Q-Networks (DQN) to Proximal Policy Optimization (PPO), and more recently, hybrid methods that integrate Graph Neural Networks with RL techniques. However, a common limitation across these methods is their reliance on relatively small datasets, and these methods face scalability issues when using large datasets. This study introduces a novel RL-based scheduler utilizing the Decentralized Distributed Proximal Policy Optimization (DD-PPO) algorithm, which supports large-scale distributed training across multiple workers without requiring parameter synchronization at every step. By eliminating reliance on centralized updates to a shared policy, the DD-PPO scheduler enhances scalability, training efficiency, and sample utilization. The validation dataset leveraged over 11.5 million real HPC job traces for comparing DD-PPO performance between traditional and advanced scheduling approaches, and the experimental results demonstrate improved scheduling performance in comparison to both rule-based schedulers and existing RL-based scheduling algorithms.","authors":["Matthew Sgambati","Aleksandar Vakanski","Matthew Anderson"],"url":"https://arxiv.org/abs/2505.03946"}
{"created":"2025-05-08","title":"Frog Soup: Zero-Shot, In-Context, and Sample-Efficient Frogger Agents","abstract":"One of the primary aspirations in reinforcement learning research is developing general-purpose agents capable of rapidly adapting to and mastering novel tasks. While RL gaming agents have mastered many Atari games, they remain slow and costly to train for each game. In this work, we demonstrate that latest reasoning LLMs with out-of-domain RL post-training can play a challenging Atari game called Frogger under a zero-shot setting. We then investigate the effect of in-context learning and the amount of reasoning effort on LLM performance. Lastly, we demonstrate a way to bootstrap traditional RL method with LLM demonstrations, which significantly improves their performance and sample efficiency. Our implementation is open sourced at https://github.com/AlienKevin/frogger.","authors":["Xiang Li","Yiyang Hao","Doug Fulop"],"url":"https://arxiv.org/abs/2505.03947"}
{"created":"2025-05-08","title":"Deep Q-Network (DQN) multi-agent reinforcement learning (MARL) for Stock Trading","abstract":"This project addresses the challenge of automated stock trading, where traditional methods and direct reinforcement learning (RL) struggle with market noise, complexity, and generalization. Our proposed solution is an integrated deep learning framework combining a Convolutional Neural Network (CNN) to identify patterns in technical indicators formatted as images, a Long Short-Term Memory (LSTM) network to capture temporal dependencies across both price history and technical indicators, and a Deep Q-Network (DQN) agent which learns the optimal trading policy (buy, sell, hold) based on the features extracted by the CNN and LSTM.","authors":["John Christopher Tidwell","John Storm Tidwell"],"url":"https://arxiv.org/abs/2505.03949"}
{"created":"2025-05-08","title":"Sufficient Decision Proxies for Decision-Focused Learning","abstract":"When solving optimization problems under uncertainty with contextual data, utilizing machine learning to predict the uncertain parameters is a popular and effective approach. Decision-focused learning (DFL) aims at learning a predictive model such that decision quality, instead of prediction accuracy, is maximized. Common practice here is to predict a single value for each uncertain parameter, implicitly assuming that there exists a (single-scenario) deterministic problem approximation (proxy) that is sufficient to obtain an optimal decision. Other work assumes the opposite, where the underlying distribution needs to be estimated. However, little is known about when either choice is valid. This paper investigates for the first time problem properties that justify using either assumption. Using this, we present effective decision proxies for DFL, with very limited compromise on the complexity of the learning task. We show the effectiveness of presented approaches in experiments on problems with continuous and discrete variables, as well as uncertainty in the objective function and in the constraints.","authors":["Noah Schutte","Grigorii Veviurko","Krzysztof Postek","Neil Yorke-Smith"],"url":"https://arxiv.org/abs/2505.03953"}
{"created":"2025-05-08","title":"Hierarchical Forecast Reconciliation on Networks: A Network Flow Optimization Formulation","abstract":"Hierarchical forecasting with reconciliation requires forecasting values of a hierarchy (e.g.~customer demand in a state and district), such that forecast values are linked (e.g.~ district forecasts should add up to the state forecast). Basic forecasting provides no guarantee for these desired structural relationships. Reconciliation addresses this problem, which is crucial for organizations requiring coherent predictions across multiple aggregation levels. Current methods like minimum trace (MinT) are mostly limited to tree structures and are computationally expensive. We introduce FlowRec, which reformulates hierarchical forecast reconciliation as a network flow optimization, enabling forecasting on generalized network structures. While reconciliation under the $\\ell_0$ norm is NP-hard, we prove polynomial-time solvability for all $\\ell_{p > 0}$ norms and , for any strictly convex and continuously differentiable loss function. For sparse networks, FlowRec achieves $O(n^2\\log n)$ complexity, significantly improving upon MinT's $O(n^3)$. Furthermore, we prove that FlowRec extends MinT to handle general networks, replacing MinT's error-covariance estimation step with direct network structural information. A key novelty of our approach is its handling of dynamic scenarios: while traditional methods recompute both base forecasts and reconciliation, FlowRec provides efficient localised updates with optimality guarantees. Monotonicity ensures that when forecasts improve incrementally, the initial reconciliation remains optimal. We also establish efficient, error-bounded approximate reconciliation, enabling fast updates in time-critical applications. Experiments on both simulated and real benchmarks demonstrate that FlowRec improves accuracy, runtime by 3-40x and memory usage by 5-7x. These results establish FlowRec as a powerful tool for large-scale hierarchical forecasting applications.","authors":["Charupriya Sharma","I\\~naki Estella Aguerri","Daniel Guimarans"],"url":"https://arxiv.org/abs/2505.03955"}
{"created":"2025-05-08","title":"Bicluster Editing with Overlaps: A Vertex Splitting Approach","abstract":"The BiCluster Editing problem aims at editing a given bipartite graph into a disjoint union of bicliques via a minimum number of edge deletion or addition operations. As a graph-based model for data clustering, the problem aims at a partition of the input dataset, which cannot always obtain meaningful clusters when some data elements are expected to belong to more than one cluster each. To address this limitation, we introduce the Bicluster Editing with Vertex Splitting problem (BCEVS) which consists of finding a minimum sequence of edge editions and vertex splittings such that the result graph is a disjoint union of bicliques. The vertex splitting operation consists of replacing a vertex $v$ with two vertices whose union of neighborhoods is the neighborhood of $v$. We also introduce the problem of Bicluster Editing with One-Sided Vertex Splitting (BCEOVS) where we restrict the splitting operations to the first set of the bipartition (often corresponding to data elements in the input raw data). We prove the two problems are NP-complete even when restricted to bipartite planar graphs of maximum degree three. Moreover, assuming the Exponential Time Hypothesis holds, there is no $2^{o(n)}n^{O(1)}$-time (resp. $2^{o(\\sqrt{n})}n^{O(1)}$-time) algorithm for BCEVS and BCEOVS on bipartite (resp. planar) graphs with maximum degree three where $n$ is the number of vertices of the graph. Furthermore we prove both problems are APX-hard and solvable in polynomial time on trees. On the other hand, we prove that BCEOVS is fixed parameter tractable with respect to solution size and admits a polynomial size kernel.","authors":["Faisal N. Abu-Khzam","Lucas Isenmann","Zeina Merchad"],"url":"https://arxiv.org/abs/2505.03959"}
{"created":"2025-05-08","title":"The Power of Stories: Narrative Priming Shapes How LLM Agents Collaborate and Compete","abstract":"According to Yuval Noah Harari, large-scale human cooperation is driven by shared narratives that encode common beliefs and values. This study explores whether such narratives can similarly nudge LLM agents toward collaboration. We use a finitely repeated public goods game in which LLM agents choose either cooperative or egoistic spending strategies. We prime agents with stories highlighting teamwork to different degrees and test how this influences negotiation outcomes. Our experiments explore four questions:(1) How do narratives influence negotiation behavior? (2) What differs when agents share the same story versus different ones? (3) What happens when the agent numbers grow? (4) Are agents resilient against self-serving negotiators? We find that story-based priming significantly affects negotiation strategies and success rates. Common stories improve collaboration, benefiting each agent. By contrast, priming agents with different stories reverses this effect, and those agents primed toward self-interest prevail. We hypothesize that these results carry implications for multi-agent system design and AI alignment.","authors":["Gerrit Gro{\\ss}mann","Larisa Ivanova","Sai Leela Poduru","Mohaddeseh Tabrizian","Islam Mesabah","David A. Selby","Sebastian J. Vollmer"],"url":"https://arxiv.org/abs/2505.03961"}
{"created":"2025-05-08","title":"A Reasoning-Focused Legal Retrieval Benchmark","abstract":"As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs (\"RAG\" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.","authors":["Lucia Zheng","Neel Guha","Javokhir Arifov","Sarah Zhang","Michal Skreta","Christopher D. Manning","Peter Henderson","Daniel E. Ho"],"url":"https://arxiv.org/abs/2505.03970"}
{"created":"2025-05-08","title":"Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale","abstract":"LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.","authors":["Jiale Liu","Yifan Zeng","Shaokun Zhang","Chi Zhang","Malte H{\\o}jmark-Bertelsen","Marie Normann Gadeberg","Huazheng Wang","Qingyun Wu"],"url":"https://arxiv.org/abs/2505.03973"}
{"created":"2025-05-08","title":"Deep Learning Framework for Infrastructure Maintenance: Crack Detection and High-Resolution Imaging of Infrastructure Surfaces","abstract":"Recently, there has been an impetus for the application of cutting-edge data collection platforms such as drones mounted with camera sensors for infrastructure asset management. However, the sensor characteristics, proximity to the structure, hard-to-reach access, and environmental conditions often limit the resolution of the datasets. A few studies used super-resolution techniques to address the problem of low-resolution images. Nevertheless, these techniques were observed to increase computational cost and false alarms of distress detection due to the consideration of all the infrastructure images i.e., positive and negative distress classes. In order to address the pre-processing of false alarm and achieve efficient super-resolution, this study developed a framework consisting of convolutional neural network (CNN) and efficient sub-pixel convolutional neural network (ESPCNN). CNN accurately classified both the classes. ESPCNN, which is the lightweight super-resolution technique, generated high-resolution infrastructure image of positive distress obtained from CNN. The ESPCNN outperformed bicubic interpolation in all the evaluation metrics for super-resolution. Based on the performance metrics, the combination of CNN and ESPCNN was observed to be effective in preprocessing the infrastructure images with negative distress, reducing the computational cost and false alarms in the next step of super-resolution. The visual inspection showed that EPSCNN is able to capture crack propagation, complex geometry of even minor cracks. The proposed framework is expected to help the highway agencies in accurately performing distress detection and assist in efficient asset management practices.","authors":["Nikhil M. Pawar","Jorge A. Prozzi","Feng Hong","Surya Sarat Chandra Congress"],"url":"https://arxiv.org/abs/2505.03974"}
{"created":"2025-05-08","title":"Call for Action: towards the next generation of symbolic regression benchmark","abstract":"Symbolic Regression (SR) is a powerful technique for discovering interpretable mathematical expressions. However, benchmarking SR methods remains challenging due to the diversity of algorithms, datasets, and evaluation criteria. In this work, we present an updated version of SRBench. Our benchmark expands the previous one by nearly doubling the number of evaluated methods, refining evaluation metrics, and using improved visualizations of the results to understand the performances. Additionally, we analyze trade-offs between model complexity, accuracy, and energy consumption. Our results show that no single algorithm dominates across all datasets. We propose a call for action from SR community in maintaining and evolving SRBench as a living benchmark that reflects the state-of-the-art in symbolic regression, by standardizing hyperparameter tuning, execution constraints, and computational resource allocation. We also propose deprecation criteria to maintain the benchmark's relevance and discuss best practices for improving SR algorithms, such as adaptive hyperparameter tuning and energy-efficient implementations.","authors":["Guilherme S. Imai Aldeia","Hengzhe Zhang","Geoffrey Bomarito","Miles Cranmer","Alcides Fonseca","Bogdan Burlacu","William G. La Cava","Fabr\\'icio Olivetti de Fran\\c{c}a"],"url":"https://arxiv.org/abs/2505.03977"}
{"created":"2025-05-08","title":"Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations","abstract":"Stochastic differential equations such as the Ornstein-Uhlenbeck process have long been used to model realworld probablistic events such as stock prices and temperature fluctuations. While statistical methods such as Maximum Likelihood Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have historically been used to estimate the parameters of stochastic differential equations, the recent explosion of deep learning technology suggests that models such as a Recurrent Neural Network (RNN) could produce more precise estimators. We present a series of experiments that compare the estimation accuracy and computational expensiveness of a statistical method (MLE) with a deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.","authors":["Aroon Sankoh","Victor Wickerhauser"],"url":"https://arxiv.org/abs/2505.03980"}
{"created":"2025-05-08","title":"X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains","abstract":"Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.","authors":["Qianchu Liu","Sheng Zhang","Guanghui Qin","Timothy Ossowski","Yu Gu","Ying Jin","Sid Kiblawi","Sam Preston","Mu Wei","Paul Vozila","Tristan Naumann","Hoifung Poon"],"url":"https://arxiv.org/abs/2505.03981"}
{"created":"2025-05-08","title":"Diffusion Models are Secretly Exchangeable: Parallelizing DDPMs via Autospeculation","abstract":"Denoising Diffusion Probabilistic Models (DDPMs) have emerged as powerful tools for generative modeling. However, their sequential computation requirements lead to significant inference-time bottlenecks. In this work, we utilize the connection between DDPMs and Stochastic Localization to prove that, under an appropriate reparametrization, the increments of DDPM satisfy an exchangeability property. This general insight enables near-black-box adaptation of various performance optimization techniques from autoregressive models to the diffusion setting. To demonstrate this, we introduce \\emph{Autospeculative Decoding} (ASD), an extension of the widely used speculative decoding algorithm to DDPMs that does not require any auxiliary draft models. Our theoretical analysis shows that ASD achieves a $\\tilde{O} (K^{\\frac{1}{3}})$ parallel runtime speedup over the $K$ step sequential DDPM. We also demonstrate that a practical implementation of autospeculative decoding accelerates DDPM inference significantly in various domains.","authors":["Hengyuan Hu","Aniket Das","Dorsa Sadigh","Nima Anari"],"url":"https://arxiv.org/abs/2505.03983"}
{"created":"2025-05-08","title":"LogiDebrief: A Signal-Temporal Logic based Automated Debriefing Approach with Large Language Models Integration","abstract":"Emergency response services are critical to public safety, with 9-1-1 call-takers playing a key role in ensuring timely and effective emergency operations. To ensure call-taking performance consistency, quality assurance is implemented to evaluate and refine call-takers' skillsets. However, traditional human-led evaluations struggle with high call volumes, leading to low coverage and delayed assessments. We introduce LogiDebrief, an AI-driven framework that automates traditional 9-1-1 call debriefing by integrating Signal-Temporal Logic (STL) with Large Language Models (LLMs) for fully-covered rigorous performance evaluation. LogiDebrief formalizes call-taking requirements as logical specifications, enabling systematic assessment of 9-1-1 calls against procedural guidelines. It employs a three-step verification process: (1) contextual understanding to identify responder types, incident classifications, and critical conditions; (2) STL-based runtime checking with LLM integration to ensure compliance; and (3) automated aggregation of results into quality assurance reports. Beyond its technical contributions, LogiDebrief has demonstrated real-world impact. Successfully deployed at Metro Nashville Department of Emergency Communications, it has assisted in debriefing 1,701 real-world calls, saving 311.85 hours of active engagement. Empirical evaluation with real-world data confirms its accuracy, while a case study and extensive user study highlight its effectiveness in enhancing call-taking performance.","authors":["Zirong Chen","Ziyan An","Jennifer Reynolds","Kristin Mullen","Stephen Martini","Meiyi Ma"],"url":"https://arxiv.org/abs/2505.03985"}
{"created":"2025-05-08","title":"Can Large Language Models Predict Parallel Code Performance?","abstract":"Accurate determination of the performance of parallel GPU code typically requires execution-time profiling on target hardware -- an increasingly prohibitive step due to limited access to high-end GPUs. This paper explores whether Large Language Models (LLMs) can offer an alternative approach for GPU performance prediction without relying on hardware. We frame the problem as a roofline classification task: given the source code of a GPU kernel and the hardware specifications of a target GPU, can an LLM predict whether the GPU kernel is compute-bound or bandwidth-bound?","authors":["Gregory Bolet","Giorgis Georgakoudis","Harshitha Menon","Konstantinos Parasyris","Niranjan Hasabnis","Hayden Estes","Kirk W. Cameron","Gal Oren"],"url":"https://arxiv.org/abs/2505.03988"}
{"created":"2025-05-08","title":"An alignment safety case sketch based on debate","abstract":"If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them towards desirable traits. One proposed solution is to leverage another superhuman system to point out flaws in the system's outputs via a debate. This paper outlines the value of debate for AI safety, as well as the assumptions and further research required to make debate work. It does so by sketching an ``alignment safety case'' -- an argument that an AI system will not autonomously take actions which could lead to egregious harm, despite being able to do so. The sketch focuses on the risk of an AI R\\&amp;D agent inside an AI company sabotaging research, for example by producing false results. To prevent this, the agent is trained via debate, subject to exploration guarantees, to teach the system to be honest. Honesty is maintained throughout deployment via online training. The safety case rests on four key claims: (1) the agent has become good at the debate game, (2) good performance in the debate game implies that the system is mostly honest, (3) the system will not become significantly less honest during deployment, and (4) the deployment context is tolerant of some errors. We identify open research problems that, if solved, could render this a compelling argument that an AI system is safe.","authors":["Marie Davidsen Buhl","Jacob Pfau","Benjamin Hilton","Geoffrey Irving"],"url":"https://arxiv.org/abs/2505.03989"}
{"created":"2025-05-08","title":"Action Spotting and Precise Event Detection in Sports: Datasets, Methods, and Challenges","abstract":"Video event detection has become an essential component of sports analytics, enabling automated identification of key moments and enhancing performance analysis, viewer engagement, and broadcast efficiency. Recent advancements in deep learning, particularly Convolutional Neural Networks (CNNs) and Transformers, have significantly improved accuracy and efficiency in Temporal Action Localization (TAL), Action Spotting (AS), and Precise Event Spotting (PES). This survey provides a comprehensive overview of these three key tasks, emphasizing their differences, applications, and the evolution of methodological approaches. We thoroughly review and categorize existing datasets and evaluation metrics specifically tailored for sports contexts, highlighting the strengths and limitations of each. Furthermore, we analyze state-of-the-art techniques, including multi-modal approaches that integrate audio and visual information, methods utilizing self-supervised learning and knowledge distillation, and approaches aimed at generalizing across multiple sports. Finally, we discuss critical open challenges and outline promising research directions toward developing more generalized, efficient, and robust event detection frameworks applicable to diverse sports. This survey serves as a foundation for future research on efficient, generalizable, and multi-modal sports event detection.","authors":["Hao Xu","Arbind Agrahari Baniya","Sam Well","Mohamed Reda Bouadjenek","Richard Dazeley","Sunil Aryal"],"url":"https://arxiv.org/abs/2505.03991"}
{"created":"2025-05-08","title":"Algorithmic Accountability in Small Data: Sample-Size-Induced Bias Within Classification Metrics","abstract":"Evaluating machine learning models is crucial not only for determining their technical accuracy but also for assessing their potential societal implications. While the potential for low-sample-size bias in algorithms is well known, we demonstrate the significance of sample-size bias induced by combinatorics in classification metrics. This revelation challenges the efficacy of these metrics in assessing bias with high resolution, especially when comparing groups of disparate sizes, which frequently arise in social applications. We provide analyses of the bias that appears in several commonly applied metrics and propose a model-agnostic assessment and correction technique. Additionally, we analyze counts of undefined cases in metric calculations, which can lead to misleading evaluations if improperly handled. This work illuminates the previously unrecognized challenge of combinatorics and probability in standard evaluation practices and thereby advances approaches for performing fair and trustworthy classification methods.","authors":["Jarren Briscoe","Garrett Kepler","Daryl Deford","Assefaw Gebremedhin"],"url":"https://arxiv.org/abs/2505.03992"}
{"created":"2025-05-08","title":"Quiet Feature Learning in Algorithmic Tasks","abstract":"We train Transformer-based language models on ten foundational algorithmic tasks and observe pronounced phase transitions in their loss curves that deviate from established power-law scaling trends. Over large ranges of compute, the validation loss barely improves, then abruptly decreases. Probing the models' internal representations reveals the learning of quiet features during the stagnant phase, followed by sudden acquisition of loud features that coincide with the sharp drop in loss. Our ablation experiments show that disrupting a single learned feature can dramatically degrade performance, providing evidence of their causal role in task performance. These findings challenge the prevailing assumption that next-token predictive loss reliably tracks incremental progress; instead, key internal features may be developing below the surface until they coalesce, triggering a rapid performance gain.","authors":["Prudhviraj Naidu","Zixian Wang","Leon Bergen","Ramamohan Paturi"],"url":"https://arxiv.org/abs/2505.03997"}
{"created":"2025-05-08","title":"PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers","abstract":"Humans excel in navigating diverse, complex environments with agile motor skills, exemplified by parkour practitioners performing dynamic maneuvers, such as climbing up walls and jumping across gaps. Reproducing these agile movements with simulated characters remains challenging, in part due to the scarcity of motion capture data for agile terrain traversal behaviors and the high cost of acquiring such data. In this work, we introduce PARC (Physics-based Augmentation with Reinforcement Learning for Character Controllers), a framework that leverages machine learning and physics-based simulation to iteratively augment motion datasets and expand the capabilities of terrain traversal controllers. PARC begins by training a motion generator on a small dataset consisting of core terrain traversal skills. The motion generator is then used to produce synthetic data for traversing new terrains. However, these generated motions often exhibit artifacts, such as incorrect contacts or discontinuities. To correct these artifacts, we train a physics-based tracking controller to imitate the motions in simulation. The corrected motions are then added to the dataset, which is used to continue training the motion generator in the next iteration. PARC's iterative process jointly expands the capabilities of the motion generator and tracker, creating agile and versatile models for interacting with complex environments. PARC provides an effective approach to develop controllers for agile terrain traversal, which bridges the gap between the scarcity of motion data and the need for versatile character controllers.","authors":["Michael Xu","Yi Shi","KangKang Yin","Xue Bin Peng"],"url":"https://arxiv.org/abs/2505.04002"}
{"created":"2025-05-08","title":"Bridging the Gap Between Deterministic and Probabilistic Approaches to State Estimation","abstract":"We consider the problem of state estimation from limited discrete and noisy measurements. In particular, we focus on modal state estimation, which approximates the unknown state of the system within a prescribed basis. We estimate the coefficients of the modal expansion using available observational data. This is usually accomplished through two distinct frameworks. One is deterministic and estimates the expansion coefficients by solving a least-squares (LS) problem. The second is probabilistic and uses a Bayesian approach to derive a distribution for the coefficients, resulting in the maximum-a-posteriori (MAP) estimate. Here, we seek to quantify and compare the accuracy of these two approaches. To this end, we derive a computable expression for the difference in Bayes risk between the deterministic LS and the Bayesian MAP estimates. We prove that this difference is always nonnegative, indicating that the MAP estimate is always more reliable than the LS estimate. We further show that this difference comprises two nonnegative components representing measurement noise and prior uncertainty, and identify regimes where one component dominates the other in magnitude. We also derive a novel prior distribution from the sample covariance matrix of the training data, and examine the greedy Bayesian and column-pivoted QR (CPQR) sensor placement algorithms with this prior as an input. Using numerical examples, we show that the greedy Bayesian algorithm returns nearly optimal sensor locations. We show that, under certain conditions, the greedy Bayesian sensor locations are identical or nearly identical to those of CPQR when applied to a regularized modal basis.","authors":["Lev Kakasenko","Alen Alexanderian","Mohammad Farazmand","Arvind K. Saibaba"],"url":"https://arxiv.org/abs/2505.04004"}
{"created":"2025-05-08","title":"Iterative Orthogonalization Scaling Laws","abstract":"The muon optimizer has picked up much attention as of late as a possible replacement to the seemingly omnipresent Adam optimizer. Recently, care has been taken to document the scaling laws of hyper-parameters under muon such as weight decay and learning rate. However, at much larger scales the iterative orthogonalization procedure present in muon may suffer a possible issue as the singular values of random matrices shrink with scale. This paper shows this scaling behavior theoretically and empirically on random matrices but does not suggest what to do about it.","authors":["Devan Selvaraj"],"url":"https://arxiv.org/abs/2505.04005"}
{"created":"2025-05-08","title":"The Eye as a Window to Systemic Health: A Survey of Retinal Imaging from Classical Techniques to Oculomics","abstract":"The unique vascularized anatomy of the human eye, encased in the retina, provides an opportunity to act as a window for human health. The retinal structure assists in assessing the early detection, monitoring of disease progression and intervention for both ocular and non-ocular diseases. The advancement in imaging technology leveraging Artificial Intelligence has seized this opportunity to bridge the gap between the eye and human health. This track paves the way for unveiling systemic health insight from the ocular system and surrogating non-invasive markers for timely intervention and identification. The new frontiers of oculomics in ophthalmology cover both ocular and systemic diseases, and getting more attention to explore them. In this survey paper, we explore the evolution of retinal imaging techniques, the dire need for the integration of AI-driven analysis, and the shift of retinal imaging from classical techniques to oculomics. We also discuss some hurdles that may be faced in the progression of oculomics, highlighting the research gaps and future directions.","authors":["Inamullah","Imran Razzak","Shoaib Jameel"],"url":"https://arxiv.org/abs/2505.04006"}
{"created":"2025-05-08","title":"SAT-Solving the Poset Cover Problem","abstract":"The poset cover problem seeks a minimum set of partial orders whose linear extensions cover a given set of linear orders. Recognizing its NP-completeness, we devised a non-trivial reduction to the Boolean satisfiability problem using a technique we call swap graphs, which avoids the complexity explosion of the naive method. By leveraging modern SAT solvers, we efficiently solve instances with reasonable universe sizes. Experimental results using the Z3 theorem prover on randomly generated inputs demonstrate the effectiveness of our method.","authors":["Chih-Cheng Rex Yuan","Bow-Yaw Wang"],"url":"https://arxiv.org/abs/2505.04013"}
{"created":"2025-05-08","title":"Rollbaccine : Herd Immunity against Storage Rollback Attacks in TEEs [Technical Report]","abstract":"Today, users can \"lift-and-shift\" unmodified applications into modern, VM-based Trusted Execution Environments (TEEs) in order to gain hardware-based security guarantees. However, TEEs do not protect applications against disk rollback attacks, where persistent storage can be reverted to an earlier state after a crash; existing rollback resistance solutions either only support a subset of applications or require code modification. Our key insight is that restoring disk consistency after a rollback attack guarantees rollback resistance for any application. We present Rollbaccine, a device mapper that provides automatic rollback resistance for all applications by provably preserving disk consistency. Rollbaccine intercepts and replicates writes to disk, restores lost state from backups during recovery, and minimizes overheads by taking advantage of the weak, multi-threaded semantics of disk operations. Across benchmarks over two real applications (PostgreSQL and HDFS) and two file systems (ext4 and xfs), Rollbaccine adds only 19% overhead, except for the fsync-heavy Filebench Varmail. In addition, Rollbaccine outperforms the state-of-the-art, non-automatic rollback resistant solution by $208\\times$.","authors":["David Chu","Aditya Balasubramanian","Dee Bao","Natacha Crooks","Heidi Howard","Lucky E. Katahanas","Soujanya Ponnapalli"],"url":"https://arxiv.org/abs/2505.04014"}
{"created":"2025-05-08","title":"MergeGuard: Efficient Thwarting of Trojan Attacks in Machine Learning Models","abstract":"This paper proposes MergeGuard, a novel methodology for mitigation of AI Trojan attacks. Trojan attacks on AI models cause inputs embedded with triggers to be misclassified to an adversary's target class, posing a significant threat to model usability trained by an untrusted third party. The core of MergeGuard is a new post-training methodology for linearizing and merging fully connected layers which we show simultaneously improves model generalizability and performance. Our Proof of Concept evaluation on Transformer models demonstrates that MergeGuard maintains model accuracy while decreasing trojan attack success rate, outperforming commonly used (post-training) Trojan mitigation by fine-tuning methodologies.","authors":["Soheil Zibakhsh Shabgahi","Yaman Jandali","Farinaz Koushanfar"],"url":"https://arxiv.org/abs/2505.04015"}
{"created":"2025-05-08","title":"SLOT: Structuring the Output of Large Language Models","abstract":"Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.","authors":["Darren Yow-Bang Wang","Zhengyuan Shen","Soumya Smruti Mishra","Zhichao Xu","Yifei Teng","Haibo Ding"],"url":"https://arxiv.org/abs/2505.04016"}
{"created":"2025-05-08","title":"Modal Decomposition and Identification for a Population of Structures Using Physics-Informed Graph Neural Networks and Transformers","abstract":"Modal identification is crucial for structural health monitoring and structural control, providing critical insights into structural dynamics and performance. This study presents a novel deep learning framework that integrates graph neural networks (GNNs), transformers, and a physics-informed loss function to achieve modal decomposition and identification across a population of structures. The transformer module decomposes multi-degrees-of-freedom (MDOF) structural dynamic measurements into single-degree-of-freedom (SDOF) modal responses, facilitating the identification of natural frequencies and damping ratios. Concurrently, the GNN captures the structural configurations and identifies mode shapes corresponding to the decomposed SDOF modal responses. The proposed model is trained in a purely physics-informed and unsupervised manner, leveraging modal decomposition theory and the independence of structural modes to guide learning without the need for labeled data. Validation through numerical simulations and laboratory experiments demonstrates its effectiveness in accurately decomposing dynamic responses and identifying modal properties from sparse structural dynamic measurements, regardless of variations in external loads or structural configurations. Comparative analyses against established modal identification techniques and model variations further underscore its superior performance, positioning it as a favorable approach for population-based structural health monitoring.","authors":["Xudong Jian","Kiran Bacsa","Gregory Duth\\'e","Eleni Chatzi"],"url":"https://arxiv.org/abs/2505.04018"}
{"created":"2025-05-08","title":"Extending Decision Predicate Graphs for Comprehensive Explanation of Isolation Forest","abstract":"The need to explain predictive models is well-established in modern machine learning. However, beyond model interpretability, understanding pre-processing methods is equally essential. Understanding how data modifications impact model performance improvements and potential biases and promoting a reliable pipeline is mandatory for developing robust machine learning solutions. Isolation Forest (iForest) is a widely used technique for outlier detection that performs well. Its effectiveness increases with the number of tree-based learners. However, this also complicates the explanation of outlier selection and the decision boundaries for inliers. This research introduces a novel Explainable AI (XAI) method, tackling the problem of global explainability. In detail, it aims to offer a global explanation for outlier detection to address its opaque nature. Our approach is based on the Decision Predicate Graph (DPG), which clarifies the logic of ensemble methods and provides both insights and a graph-based metric to explain how samples are identified as outliers using the proposed Inlier-Outlier Propagation Score (IOP-Score). Our proposal enhances iForest's explainability and provides a comprehensive view of the decision-making process, detailing which features contribute to outlier identification and how the model utilizes them. This method advances the state-of-the-art by providing insights into decision boundaries and a comprehensive view of holistic feature usage in outlier identification. -- thus promoting a fully explainable machine learning pipeline.","authors":["Matteo Ceschin","Leonardo Arrighi","Luca Longo","Sylvio Barbon Junior"],"url":"https://arxiv.org/abs/2505.04019"}
{"created":"2025-05-08","title":"Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving","abstract":"Serving large language models (LLMs) is expensive, especially for providers hosting many models, making cost reduction essential. The unique workload patterns of serving multiple LLMs (i.e., multi-LLM serving) create new opportunities and challenges for this task. The long-tail popularity of models and their long idle periods present opportunities to improve utilization through GPU sharing. However, existing GPU sharing systems lack the ability to adjust their resource allocation and sharing policies at runtime, making them ineffective at meeting latency service-level objectives (SLOs) under rapidly fluctuating workloads.","authors":["Shan Yu","Jiarong Xing","Yifan Qiao","Mingyuan Ma","Yangmin Li","Yang Wang","Shuo Yang","Zhiqiang Xie","Shiyi Cao","Ke Bao","Ion Stoica","Harry Xu","Ying Sheng"],"url":"https://arxiv.org/abs/2505.04021"}
{"created":"2025-05-08","title":"Racing Against the Clock: Exploring the Impact of Scheduled Deadlines on Technical Debt","abstract":"Background: Technical Debt (TD) describes suboptimal software development practices with long-term consequences, such as defects and vulnerabilities. Deadlines are a leading cause of the emergence of TD in software systems. While multiple aspects of TD have been studied, the empirical research findings on the impact of deadlines are still inconclusive. Aims: This study investigates the impact of scheduled deadlines on TD. It analyzes how scheduled deadlines affect code quality, commit activities, and issues in issue-tracking systems. Method: We analyzed eight Open Source Software (OSS) projects with regular release schedules using SonarQube. We analyzed 12.3k commits and 371 releases across these eight OSS projects. The study combined quantitative metrics with qualitative analyses to comprehensively understand TD accumulation under scheduled deadlines. Results: Our findings indicated that some projects had a clear increase in TD as deadlines approached (with above 50% of releases having increasing TD accumulation as deadlines approached), while others managed to maintain roughly the same amount of TD. Analysis of commit activities and issue tracking revealed that deadline proximity could lead to increased commit frequency and bug-related issue creation. Conclusions: Our study highlights that, in some cases, impending deadlines have a clear impact on TD. The findings pinpoint the need to mitigate last-minute coding rushes and the risks associated with deadline-driven TD accumulation.","authors":["Joshua Aldrich Edbert","Zadia Codabux","Roberto Verdecchia"],"url":"https://arxiv.org/abs/2505.04027"}
{"created":"2025-05-08","title":"Appeal and Scope of Misinformation Spread by AI Agents and Humans","abstract":"This work examines the influence of misinformation and the role of AI agents, called bots, on social network platforms. To quantify the impact of misinformation, it proposes two new metrics based on attributes of tweet engagement and user network position: Appeal, which measures the popularity of the tweet, and Scope, which measures the potential reach of the tweet. In addition, it analyzes 5.8 million misinformation tweets on the COVID-19 vaccine discourse over three time periods: Pre-Vaccine, Vaccine Launch, and Post-Vaccine. Results show that misinformation was more prevalent during the first two periods. Human-generated misinformation tweets tend to have higher appeal and scope compared to bot-generated ones. Tweedie regression analysis reveals that human-generated misinformation tweets were most concerning during Vaccine Launch week, whereas bot-generated misinformation reached its highest appeal and scope during the Pre-Vaccine period.","authors":["Lynnette Hui Xian Ng","Wenqi Zhou","Kathleen M. Carley"],"url":"https://arxiv.org/abs/2505.04028"}
{"created":"2025-05-08","title":"Izhikevich-Inspired Temporal Dynamics for Enhancing Privacy, Efficiency, and Transferability in Spiking Neural Networks","abstract":"Biological neurons exhibit diverse temporal spike patterns, which are believed to support efficient, robust, and adaptive neural information processing. While models such as Izhikevich can replicate a wide range of these firing dynamics, their complexity poses challenges for directly integrating them into scalable spiking neural networks (SNN) training pipelines. In this work, we propose two probabilistically driven, input-level temporal spike transformations: Poisson-Burst and Delayed-Burst that introduce biologically inspired temporal variability directly into standard Leaky Integrate-and-Fire (LIF) neurons. This enables scalable training and systematic evaluation of how spike timing dynamics affect privacy, generalization, and learning performance. Poisson-Burst modulates burst occurrence based on input intensity, while Delayed-Burst encodes input strength through burst onset timing. Through extensive experiments across multiple benchmarks, we demonstrate that Poisson-Burst maintains competitive accuracy and lower resource overhead while exhibiting enhanced privacy robustness against membership inference attacks, whereas Delayed-Burst provides stronger privacy protection at a modest accuracy trade-off. These findings highlight the potential of biologically grounded temporal spike dynamics in improving the privacy, generalization and biological plausibility of neuromorphic learning systems.","authors":["Ayana Moshruba","Hamed Poursiami","Maryam Parsa"],"url":"https://arxiv.org/abs/2505.04034"}
{"created":"2025-05-08","title":"Identities are not Interchangeable: The Problem of Overgeneralization in Fair Machine Learning","abstract":"A key value proposition of machine learning is generalizability: the same methods and model architecture should be able to work across different domains and different contexts. While powerful, this generalization can sometimes go too far, and miss the importance of the specifics. In this work, we look at how fair machine learning has often treated as interchangeable the identity axis along which discrimination occurs. In other words, racism is measured and mitigated the same way as sexism, as ableism, as ageism. Disciplines outside of computer science have pointed out both the similarities and differences between these different forms of oppression, and in this work we draw out the implications for fair machine learning. While certainly not all aspects of fair machine learning need to be tailored to the specific form of oppression, there is a pressing need for greater attention to such specificity than is currently evident. Ultimately, context specificity can deepen our understanding of how to build more fair systems, widen our scope to include currently overlooked harms, and, almost paradoxically, also help to narrow our scope and counter the fear of an infinite number of group-specific methods of analysis.","authors":["Angelina Wang"],"url":"https://arxiv.org/abs/2505.04038"}
{"created":"2025-05-08","title":"Identification and Optimization of Redundant Code Using Large Language Models","abstract":"Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases.","authors":["Shamse Tasnim Cynthia"],"url":"https://arxiv.org/abs/2505.04040"}
{"created":"2025-05-08","title":"Reliable Disentanglement Multi-view Learning Against View Adversarial Attacks","abstract":"Recently, trustworthy multi-view learning has attracted extensive attention because evidence learning can provide reliable uncertainty estimation to enhance the credibility of multi-view predictions. Existing trusted multi-view learning methods implicitly assume that multi-view data is secure. In practice, however, in safety-sensitive applications such as autonomous driving and security monitoring, multi-view data often faces threats from adversarial perturbations, thereby deceiving or disrupting multi-view learning models. This inevitably leads to the adversarial unreliability problem (AUP) in trusted multi-view learning. To overcome this tricky problem, we propose a novel multi-view learning framework, namely Reliable Disentanglement Multi-view Learning (RDML). Specifically, we first propose evidential disentanglement learning to decompose each view into clean and adversarial parts under the guidance of corresponding evidences, which is extracted by a pretrained evidence extractor. Then, we employ the feature recalibration module to mitigate the negative impact of adversarial perturbations and extract potential informative features from them. Finally, to further ignore the irreparable adversarial interferences, a view-level evidential attention mechanism is designed. Extensive experiments on multi-view classification tasks with adversarial attacks show that our RDML outperforms the state-of-the-art multi-view learning methods by a relatively large margin.","authors":["Xuyang Wang","Siyuan Duan","Qizhi Li","Guiduo Duan","Yuan Sun","Dezhong Peng"],"url":"https://arxiv.org/abs/2505.04046"}
{"created":"2025-05-08","title":"The Kinetic Hourglass Data Structure for Computing the Bottleneck Distance of Dynamic Data","abstract":"The kinetic data structure (KDS) framework is a powerful tool for maintaining various geometric configurations of continuously moving objects. In this work, we introduce the kinetic hourglass, a novel KDS implementation designed to compute the bottleneck distance for geometric matching problems. We detail the events and updates required for handling general graphs, accompanied by a complexity analysis. Furthermore, we demonstrate the utility of the kinetic hourglass by applying it to compute the bottleneck distance between two persistent homology transforms (PHTs) derived from shapes in $\\mathbb{R}^2$, which are topological summaries obtained by computing persistent homology from every direction in $\\mathbb{S}^1$.","authors":["Elizabeth Munch","Elena Xinyi Wang","Carola Wenk"],"url":"https://arxiv.org/abs/2505.04048"}
{"created":"2025-05-08","title":"TerraFusion: Joint Generation of Terrain Geometry and Texture Using Latent Diffusion Models","abstract":"3D terrain models are essential in fields such as video game development and film production. Since surface color often correlates with terrain geometry, capturing this relationship is crucial to achieving realism. However, most existing methods generate either a heightmap or a texture, without sufficiently accounting for the inherent correlation. In this paper, we propose a method that jointly generates terrain heightmaps and textures using a latent diffusion model. First, we train the model in an unsupervised manner to randomly generate paired heightmaps and textures. Then, we perform supervised learning of an external adapter to enable user control via hand-drawn sketches. Experiments show that our approach allows intuitive terrain generation while preserving the correlation between heightmaps and textures.","authors":["Kazuki Higo","Toshiki Kanai","Yuki Endo","Yoshihiro Kanamori"],"url":"https://arxiv.org/abs/2505.04050"}
{"created":"2025-05-08","title":"BuildingBlock: A Hybrid Approach for Structured Building Generation","abstract":"Three-dimensional building generation is vital for applications in gaming, virtual reality, and digital twins, yet current methods face challenges in producing diverse, structured, and hierarchically coherent buildings. We propose BuildingBlock, a hybrid approach that integrates generative models, procedural content generation (PCG), and large language models (LLMs) to address these limitations. Specifically, our method introduces a two-phase pipeline: the Layout Generation Phase (LGP) and the Building Construction Phase (BCP).","authors":["Junming Huang","Chi Wang","Letian Li","Changxin Huang","Qiang Dai","Weiwei Xu"],"url":"https://arxiv.org/abs/2505.04051"}
{"created":"2025-05-08","title":"Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control","abstract":"Compositing human figures into scene images has broad applications in areas such as entertainment and advertising. However, existing methods often cannot handle occlusion of the inserted person by foreground objects and unnaturally place the person in the frontmost layer. Moreover, they offer limited control over the inserted person's pose. To address these challenges, we propose two methods. Both allow explicit pose control via a 3D body model and leverage latent diffusion models to synthesize the person at a contextually appropriate depth, naturally handling occlusions without requiring occlusion masks. The first is a two-stage approach: the model first learns a depth map of the scene with the person through supervised learning, and then synthesizes the person accordingly. The second method learns occlusion implicitly and synthesizes the person directly from input data without explicit depth supervision. Quantitative and qualitative evaluations show that both methods outperform existing approaches by better preserving scene consistency while accurately reflecting occlusions and user-specified poses.","authors":["Shun Masuda","Yuki Endo","Yoshihiro Kanamori"],"url":"https://arxiv.org/abs/2505.04052"}
{"created":"2025-05-08","title":"FoodTrack: Estimating Handheld Food Portions with Egocentric Video","abstract":"Accurately tracking food consumption is crucial for nutrition and health monitoring. Traditional approaches typically require specific camera angles, non-occluded images, or rely on gesture recognition to estimate intake, making assumptions about bite size rather than directly measuring food volume. We propose the FoodTrack framework for tracking and measuring the volume of hand-held food items using egocentric video which is robust to hand occlusions and flexible with varying camera and object poses. FoodTrack estimates food volume directly, without relying on intake gestures or fixed assumptions about bite size, offering a more accurate and adaptable solution for tracking food consumption. We achieve absolute percentage loss of approximately 7.01% on a handheld food object, improving upon a previous approach that achieved a 16.40% mean absolute percentage error in its best case, under less flexible conditions.","authors":["Ervin Wang","Yuhao Chen"],"url":"https://arxiv.org/abs/2505.04055"}
{"created":"2025-05-08","title":"AS3D: 2D-Assisted Cross-Modal Understanding with Semantic-Spatial Scene Graphs for 3D Visual Grounding","abstract":"3D visual grounding aims to localize the unique target described by natural languages in 3D scenes. The significant gap between 3D and language modalities makes it a notable challenge to distinguish multiple similar objects through the described spatial relationships. Current methods attempt to achieve cross-modal understanding in complex scenes via a target-centered learning mechanism, ignoring the perception of referred objects. We propose a novel 2D-assisted 3D visual grounding framework that constructs semantic-spatial scene graphs with referred object discrimination for relationship perception. The framework incorporates a dual-branch visual encoder that utilizes 2D pre-trained attributes to guide the multi-modal object encoding. Furthermore, our cross-modal interaction module uses graph attention to facilitate relationship-oriented information fusion. The enhanced object representation and iterative relational learning enable the model to establish effective alignment between 3D vision and referential descriptions. Experimental results on the popular benchmarks demonstrate our superior performance compared to state-of-the-art methods, especially in addressing the challenges of multiple similar distractors.","authors":["Feng Xiao","Hongbin Xu","Guocan Zhao","Wenxiong Kang"],"url":"https://arxiv.org/abs/2505.04058"}
{"created":"2025-05-08","title":"Tensor robust principal component analysis via the tensor nuclear over Frobenius norm","abstract":"We address the problem of tensor robust principal component analysis (TRPCA), which entails decomposing a given tensor into the sum of a low-rank tensor and a sparse tensor. By leveraging the tensor singular value decomposition (t-SVD), we introduce the ratio of the tensor nuclear norm to the tensor Frobenius norm (TNF) as a nonconvex approximation of the tensor's tubal rank in TRPCA. Additionally, we utilize the traditional L1 norm to identify the sparse tensor. For brevity, we refer to the combination of TNF and L1 as simply TNF. Under a series of incoherence conditions, we prove that a pair of tensors serves as a local minimizer of the proposed TNF-based TRPCA model if one tensor is sufficiently low in rank and the other tensor is sufficiently sparse. In addition, we propose replacing the L1 norm with the ratio of the L1 and Frobenius norm for tensors, the latter denoted as the LF norm. We refer to the combination of TNF and L1/LF as the TNF+ model in short. To solve both TNF and TNF+ models, we employ the alternating direction method of multipliers (ADMM) and prove subsequential convergence under certain conditions. Finally, extensive experiments on synthetic data, real color images, and videos are conducted to demonstrate the superior performance of our proposed models in comparison to state-of-the-art methods in TRPCA.","authors":["Huiwen Zheng","Yifei Lou","Guoliang Tian","Chao Wang"],"url":"https://arxiv.org/abs/2505.04063"}
{"created":"2025-05-08","title":"LLAMAPIE: Proactive In-Ear Conversation Assistants","abstract":"We introduce LlamaPIE, the first real-time proactive assistant designed to enhance human conversations through discreet, concise guidance delivered via hearable devices. Unlike traditional language models that require explicit user invocation, this assistant operates in the background, anticipating user needs without interrupting conversations. We address several challenges, including determining when to respond, crafting concise responses that enhance conversations, leveraging knowledge of the user for context-aware assistance, and real-time, on-device processing. To achieve this, we construct a semi-synthetic dialogue dataset and propose a two-model pipeline: a small model that decides when to respond and a larger model that generates the response. We evaluate our approach on real-world datasets, demonstrating its effectiveness in providing helpful, unobtrusive assistance. User studies with our assistant, implemented on Apple Silicon M2 hardware, show a strong preference for the proactive assistant over both a baseline with no assistance and a reactive model, highlighting the potential of LlamaPie to enhance live conversations.","authors":["Tuochao Chen","Nicholas Batchelder","Alisa Liu","Noah Smith","Shyamnath Gollakota"],"url":"https://arxiv.org/abs/2505.04066"}
{"created":"2025-05-08","title":"Shadow Wireless Intelligence: Large Language Model-Driven Reasoning in Covert Communications","abstract":"Covert Communications (CC) can secure sensitive transmissions in industrial, military, and mission-critical applications within 6G wireless networks. However, traditional optimization methods based on Artificial Noise (AN), power control, and channel manipulation might not adapt to dynamic and adversarial environments due to the high dimensionality, nonlinearity, and stringent real-time covertness requirements. To bridge this gap, we introduce Shadow Wireless Intelligence (SWI), which integrates the reasoning capabilities of Large Language Models (LLMs) with retrieval-augmented generation to enable intelligent decision-making in covert wireless systems. Specifically, we utilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning, combined with real-time retrieval of domain-specific knowledge to improve context accuracy and mitigate hallucinations. Our approach develops a structured CC knowledge base, supports context-aware retrieval, and performs semantic optimization, allowing LLMs to generate and adapt CC strategies in real time. In a case study on optimizing AN power in a full-duplex CC scenario, DeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in the generation of simulation code, outperforming baseline models. These results validate SWI as a robust, interpretable, and adaptive foundation for LLM-driven intelligent covert wireless systems in 6G networks.","authors":["Yuanai Xie","Zhaozhi Liu","Xiao Zhang","Shihua Zhang","Rui Hou","Minrui Xu","Ruichen Zhang","Dusit Niyato"],"url":"https://arxiv.org/abs/2505.04068"}
{"created":"2025-05-08","title":"Advancing and Benchmarking Personalized Tool Invocation for LLMs","abstract":"Tool invocation is a crucial mechanism for extending the capabilities of Large Language Models (LLMs) and has recently garnered significant attention. It enables LLMs to solve complex problems through tool calls while accessing up-to-date world knowledge. However, existing work primarily focuses on the fundamental ability of LLMs to invoke tools for problem-solving, without considering personalized constraints in tool invocation. In this work, we introduce the concept of Personalized Tool Invocation and define two key tasks: Tool Preference and Profile-dependent Query. Tool Preference addresses user preferences when selecting among functionally similar tools, while Profile-dependent Query considers cases where a user query lacks certain tool parameters, requiring the model to infer them from the user profile. To tackle these challenges, we propose PTool, a data synthesis framework designed for personalized tool invocation. Additionally, we construct \\textbf{PTBench}, the first benchmark for evaluating personalized tool invocation. We then fine-tune various open-source models, demonstrating the effectiveness of our framework and providing valuable insights. Our benchmark is public at https://github.com/hyfshadow/PTBench.","authors":["Xu Huang","Yuefeng Huang","Weiwen Liu","Xingshan Zeng","Yasheng Wang","Ruiming Tang","Hong Xie","Defu Lian"],"url":"https://arxiv.org/abs/2505.04072"}
{"created":"2025-05-08","title":"Natural Language Generation in Healthcare: A Review of Methods and Applications","abstract":"Natural language generation (NLG) is the key technology to achieve generative artificial intelligence (AI). With the breakthroughs in large language models (LLMs), NLG has been widely used in various medical applications, demonstrating the potential to enhance clinical workflows, support clinical decision-making, and improve clinical documentation. Heterogeneous and diverse medical data modalities, such as medical text, images, and knowledge bases, are utilized in NLG. Researchers have proposed many generative models and applied them in a number of healthcare applications. There is a need for a comprehensive review of NLG methods and applications in the medical domain. In this study, we systematically reviewed 113 scientific publications from a total of 3,988 NLG-related articles identified using a literature search, focusing on data modality, model architecture, clinical applications, and evaluation methods. Following PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines, we categorize key methods, identify clinical applications, and assess their capabilities, limitations, and emerging challenges. This timely review covers the key NLG technologies and medical applications and provides valuable insights for future studies to leverage NLG to transform medical discovery and healthcare.","authors":["Mengxian Lyu","Xiaohan Li","Ziyi Chen","Jinqian Pan","Cheng Peng","Sankalp Talankar","Yonghui Wu"],"url":"https://arxiv.org/abs/2505.04073"}
{"created":"2025-05-08","title":"LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?","abstract":"This paper examines whether large language model (LLM) capabilities can continue to advance without additional compute by analyzing the development and role of algorithms used in state-of-the-art LLMs. Motivated by regulatory efforts that have largely focused on restricting access to high-performance hardware, we ask: Can LLMs progress in a compute-constrained environment, and how do algorithmic innovations perform under such conditions?","authors":["Teddy Foley","Spencer Guo","Henry Josephson","Anqi Qu","Jack Sanderson"],"url":"https://arxiv.org/abs/2505.04075"}
{"created":"2025-05-08","title":"Secret Sharing Schemes from Correlated Random Variables and Rate-Limited Public Communication","abstract":"A dealer aims to share a secret with participants so that only predefined subsets can reconstruct it, while others learn nothing. The dealer and participants access correlated randomness and communicate over a one-way, public, rate-limited channel. For this problem, we propose the first explicit coding scheme able to handle arbitrary access structures and achieve the best known achievable rates, previously obtained non-constructively. Our construction relies on lossy source coding coupled with distribution approximation to handle the reliability constraints, followed by universal hashing to handle the security constraints. We stress that our coding scheme does not require symmetry or degradation assumptions on the correlated random variables, and does not need a pre-shared secret among the participants and dealer. As a by-product, our construction also yields explicit coding schemes for secret-key generation under one-way, rate-limited public communication that, unlike prior work, achieves the capacity for arbitrary source correlations and do not require a pre-shared secret to ensure strong secrecy.","authors":["Rumia Sultana","Remi A. Chou"],"url":"https://arxiv.org/abs/2505.04076"}
{"created":"2025-05-08","title":"MojoFrame: Dataframe Library in Mojo Language","abstract":"Mojo is an emerging programming language built on MLIR (Multi-Level Intermediate Representation) and JIT compilation. It enables transparent optimizations with respect to the underlying hardware (e.g., CPUs, GPUs), while allowing users to express their logic using Python-like user-friendly syntax. Mojo has been shown to offer great performance in tensor operations; however, its performance has not been tested for relational operations (e.g., filtering, join, and group-by), which are common in data science workflows. To date, no dataframe implementation exists in the Mojo ecosystem.","authors":["Shengya Huang","Zhaoheng Li","Derek Werner","Yongjoo Park"],"url":"https://arxiv.org/abs/2505.04080"}
{"created":"2025-05-08","title":"QStore: Quantization-Aware Compressed Model Storage","abstract":"Modern applications commonly leverage large, multi-modal foundation models. These applications often feature complex workflows that demand the storage and usage of similar models in multiple precisions. A straightforward approach is to maintain a separate file for each model precision (e.g., INT8, BF16), which is indeed the approach taken by many model providers such as HuggingFace and Ollama. However, this approach incurs excessive storage costs since a higher precision model (e.g., BF16) is a strict superset of a lower precision model (e.g., INT8) in terms of information. Unfortunately, simply maintaining only the higher-precision model and requiring every user to dynamically convert the model precision is not desirable because every user of lower precision models must pay the cost for model download and precision conversion.","authors":["Raunak Shah","Zhaoheng Li","Yongjoo Park"],"url":"https://arxiv.org/abs/2505.04081"}
{"created":"2025-05-08","title":"Plexus: Taming Billion-edge Graphs with 3D Parallel GNN Training","abstract":"Graph neural networks have emerged as a potent class of neural networks capable of leveraging the connectivity and structure of real-world graphs to learn intricate properties and relationships between nodes. Many real-world graphs exceed the memory capacity of a GPU due to their sheer size, and using GNNs on them requires techniques such as mini-batch sampling to scale. However, this can lead to reduced accuracy in some cases, and sampling and data transfer from the CPU to the GPU can also slow down training. On the other hand, distributed full-graph training suffers from high communication overhead and load imbalance due to the irregular structure of graphs. We propose Plexus, a three-dimensional (3D) parallel approach for full-graph training that tackles these issues and scales to billion-edge graphs. Additionally, we introduce optimizations such as a permutation scheme for load balancing, and a performance model to predict the optimal 3D configuration. We evaluate Plexus on several graph datasets and show scaling results for up to 2048 GPUs on Perlmutter, which is 33% of the machine, and 2048 GCDs on Frontier. Plexus achieves unprecedented speedups of 2.3x-12.5x over existing methods and a reduction in the time to solution by 5.2-8.7x on Perlmutter and 7-54.2x on Frontier.","authors":["Aditya K. Ranjan","Siddharth Singh","Cunyang Wei","Abhinav Bhatele"],"url":"https://arxiv.org/abs/2505.04083"}
{"created":"2025-05-08","title":"An Empirical Study of OpenAI API Discussions on Stack Overflow","abstract":"The rapid advancement of large language models (LLMs), represented by OpenAI's GPT series, has significantly impacted various domains such as natural language processing, software development, education, healthcare, finance, and scientific research. However, OpenAI APIs introduce unique challenges that differ from traditional APIs, such as the complexities of prompt engineering, token-based cost management, non-deterministic outputs, and operation as black boxes. To the best of our knowledge, the challenges developers encounter when using OpenAI APIs have not been explored in previous empirical studies. To fill this gap, we conduct the first comprehensive empirical study by analyzing 2,874 OpenAI API-related discussions from the popular Q&amp;A forum Stack Overflow. We first examine the popularity and difficulty of these posts. After manually categorizing them into nine OpenAI API-related categories, we identify specific challenges associated with each category through topic modeling analysis. Based on our empirical findings, we finally propose actionable implications for developers, LLM vendors, and researchers.","authors":["Xiang Chen","Jibin Wang","Chaoyang Gao","Xiaolin Ju","Zhanqi Cui"],"url":"https://arxiv.org/abs/2505.04084"}
{"created":"2025-05-08","title":"SEVA: Leveraging Single-Step Ensemble of Vicinal Augmentations for Test-Time Adaptation","abstract":"Test-Time adaptation (TTA) aims to enhance model robustness against distribution shifts through rapid model adaptation during inference. While existing TTA methods often rely on entropy-based unsupervised training and achieve promising results, the common practice of a single round of entropy training is typically unable to adequately utilize reliable samples, hindering adaptation efficiency. In this paper, we discover augmentation strategies can effectively unleash the potential of reliable samples, but the rapidly growing computational cost impedes their real-time application. To address this limitation, we propose a novel TTA approach named Single-step Ensemble of Vicinal Augmentations (SEVA), which can take advantage of data augmentations without increasing the computational burden. Specifically, instead of explicitly utilizing the augmentation strategy to generate new data, SEVA develops a theoretical framework to explore the impacts of multiple augmentations on model adaptation and proposes to optimize an upper bound of the entropy loss to integrate the effects of multiple rounds of augmentation training into a single step. Furthermore, we discover and verify that using the upper bound as the loss is more conducive to the selection mechanism, as it can effectively filter out harmful samples that confuse the model. Combining these two key advantages, the proposed efficient loss and a complementary selection strategy can simultaneously boost the potential of reliable samples and meet the stringent time requirements of TTA. The comprehensive experiments on various network architectures across challenging testing scenarios demonstrate impressive performances and the broad adaptability of SEVA. The code will be publicly available.","authors":["Zixuan Hu","Yichun Hu","Ling-Yu Duan"],"url":"https://arxiv.org/abs/2505.04087"}
{"created":"2025-05-08","title":"SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking","abstract":"Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge details using bidirectional modeling and self-attention. We propose a Siamese parameter-sharing strate-gy that allows certain convolutional layers to share weights. This approach reduces computational redundancy while preserving strong feature represen-tation. In addition, we design a motion edge-aware regression loss to improve tracking accuracy, especially for motion-blurred targets. Extensive experi-ments are conducted on four TIR tracking benchmarks, including LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT achieves superior performance in TIR target tracking.","authors":["Shang Zhang","Huanbin Zhang","Dali Feng","Yujie Cui","Ruoyan Xiong","Cen He"],"url":"https://arxiv.org/abs/2505.04088"}
{"created":"2025-05-08","title":"A New Scope and Domain Measure Comparison Method for Global Convergence Analysis in Evolutionary Computation","abstract":"Convergence analysis is a fundamental research topic in evolutionary computation (EC). The commonly used analysis method models the EC algorithm as a homogeneous Markov chain for analysis, which is not always suitable for different EC variants, and also sometimes causes misuse and confusion due to their complex process. In this article, we categorize the existing researches on convergence analysis in EC algorithms into stable convergence and global convergence, and then prove that the conditions for these two convergence properties are somehow mutually exclusive. Inspired by this proof, we propose a new scope and domain measure comparison (SDMC) method for analyzing the global convergence of EC algorithms and provide a rigorous proof of its necessity and sufficiency as an alternative condition. Unlike traditional methods, the SDMC method is straightforward, bypasses Markov chain modeling, and minimizes errors from misapplication as it only focuses on the measure of the algorithm's search scope. We apply SDMC to two algorithm types that are unsuitable for traditional methods, confirming its effectiveness in global convergence analysis. Furthermore, we apply the SDMC method to explore the gene targeting mechanism's impact on the global convergence in large-scale global optimization, deriving insights into how to design EC algorithms that guarantee global convergence and exploring how theoretical analysis can guide EC algorithm design.","authors":["Liu-Yue Luo","Zhi-Hui Zhan","Kay Chen Tan","Jun Zhang"],"url":"https://arxiv.org/abs/2505.04089"}
{"created":"2025-05-08","title":"SolPhishHunter: Towards Detecting and Understanding Phishing on Solana","abstract":"Solana is a rapidly evolving blockchain platform that has attracted an increasing number of users. However, this growth has also drawn the attention of malicious actors, with some phishers extending their reach into the Solana ecosystem. Unlike platforms such as Ethereum, Solana has distinct designs of accounts and transactions, leading to the emergence of new types of phishing transactions that we term SolPhish. We define three types of SolPhish and develop a detection tool called SolPhishHunter. Utilizing SolPhishHunter, we detect a total of 8,058 instances of SolPhish and conduct an empirical analysis of these detected cases. Our analysis explores the distribution and impact of SolPhish, the characteristics of the phishers, and the relationships among phishing gangs. Particularly, the detected SolPhish transactions have resulted in nearly \\$1.1 million in losses for victims. We report our detection results to the community and construct SolPhishDataset, the \\emph{first} Solana phishing-related dataset in academia.","authors":["Ziwei Li","Zigui Jiang","Ming Fang","Jiaxin Chen","Zhiying Wu","Jiajing Wu","Lun Zhang","Zibin Zheng"],"url":"https://arxiv.org/abs/2505.04094"}
{"created":"2025-05-08","title":"Scalable Aerial GNSS Localization for Marine Robots","abstract":"Accurate localization is crucial for water robotics, yet traditional onboard Global Navigation Satellite System (GNSS) approaches are difficult or ineffective due to signal reflection on the water's surface and its high cost of aquatic GNSS receivers. Existing approaches, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic-based methods, face challenges like error accumulation and high computational complexity. Therefore, a more efficient and scalable solution remains necessary. This paper proposes an alternative approach that leverages an aerial drone equipped with GNSS localization to track and localize a marine robot once it is near the surface of the water. Our results show that this novel adaptation enables accurate single and multi-robot marine robot localization.","authors":["Shuo Wen","Edwin Meriaux","Mariana Sosa Guzm\\'an","Charlotte Morissette","Chloe Si","Bobak Baghi","Gregory Dudek"],"url":"https://arxiv.org/abs/2505.04095"}
{"created":"2025-05-08","title":"Satellite-Assisted Low-Altitude Economy Networking: Concepts, Applications, and Opportunities","abstract":"The low-altitude economy (LAE) is a new economic paradigm that leverages low-altitude vehicles (LAVs) to perform diverse missions across diverse areas. To support the operations of LAE, it is essential to establish LAE networks that enable LAV management and communications.Existing studies mainly reuse terrestrial networks to construct LAE networks. However, the limited coverage of terrestrial networks poses challenges for serving LAVs in remote areas. Besides, efficient LAV operations also require support such as localization and navigation, which terrestrial networks designed for communications cannot fully provide. Due to ubiquitous coverage and diverse functions, satellites are a promising technology to support LAVs. Therefore, this article investigates satellite-assisted LAE networking. First, we introduce an overview of LAE and satellites, discussing their features, applications, and architectures. Next, we investigate opportunities for satellites to assist LAE from aspects of communication, control, and computation. As all assistance depends on reliable satellite-LAV communications, we propose a satellite-assisted LAE framework to tackle issues caused by the severe path loss and high dynamics in satellite-assisted LAE networks.The case study demonstrates that the distributed MIMO architecture efficiently reduces the required transmission power and extends service duration, while the two-timescale optimization scheme balances the performance and control signaling overheads. Specifically, the proposed framework comprises distributed satellite MIMO, distributed LAV MIMO, and a two-timescale optimization scheme.","authors":["Shizhao He","Jiacheng Wang","Ying-Chang Liang","Geng Sun","Dusit Niyato"],"url":"https://arxiv.org/abs/2505.04098"}
{"created":"2025-05-08","title":"LLMs' Suitability for Network Security: A Case Study of STRIDE Threat Modeling","abstract":"Artificial Intelligence (AI) is expected to be an integral part of next-generation AI-native 6G networks. With the prevalence of AI, researchers have identified numerous use cases of AI in network security. However, there are almost nonexistent studies that analyze the suitability of Large Language Models (LLMs) in network security. To fill this gap, we examine the suitability of LLMs in network security, particularly with the case study of STRIDE threat modeling. We utilize four prompting techniques with five LLMs to perform STRIDE classification of 5G threats. From our evaluation results, we point out key findings and detailed insights along with the explanation of the possible underlying factors influencing the behavior of LLMs in the modeling of certain threats. The numerical results and the insights support the necessity for adjusting and fine-tuning LLMs for network security use cases.","authors":["AbdulAziz AbdulGhaffar","Ashraf Matrawy"],"url":"https://arxiv.org/abs/2505.04101"}
{"created":"2025-05-08","title":"Position: We need responsible, application-driven (RAD) AI research","abstract":"This position paper argues that achieving meaningful scientific and societal advances with artificial intelligence (AI) requires a responsible, application-driven approach (RAD) to AI research. As AI is increasingly integrated into society, AI researchers must engage with the specific contexts where AI is being applied. This includes being responsive to ethical and legal considerations, technical and societal constraints, and public discourse. We present the case for RAD-AI to drive research through a three-staged approach: (1) building transdisciplinary teams and people-centred studies; (2) addressing context-specific methods, ethical commitments, assumptions, and metrics; and (3) testing and sustaining efficacy through staged testbeds and a community of practice. We present a vision for the future of application-driven AI research to unlock new value through technically feasible methods that are adaptive to the contextual needs and values of the communities they ultimately serve.","authors":["Sarah Hartman","Cheng Soon Ong","Julia Powles","Petra Kuhnert"],"url":"https://arxiv.org/abs/2505.04104"}
{"created":"2025-05-08","title":"MAISY: Motion-Aware Image SYnthesis for MedicalImage Motion Correction","abstract":"Patient motion during medical image acquisition causes blurring, ghosting, and distorts organs, which makes image interpretation challenging.Current state-of-the-art algorithms using Generative Adversarial Network (GAN)-based methods with their ability to learn the mappings between corrupted images and their ground truth via Structural Similarity Index Measure (SSIM) loss effectively generate motion-free images. However, we identified the following limitations: (i) they mainly focus on global structural characteristics and therefore overlook localized features that often carry critical pathological information, and (ii) the SSIM loss function struggles to handle images with varying pixel intensities, luminance factors, and variance. In this study, we propose Motion-Aware Image SYnthesis (MAISY) which initially characterize motion and then uses it for correction by: (a) leveraging the foundation model Segment Anything Model (SAM), to dynamically learn spatial patterns along anatomical boundaries where motion artifacts are most pronounced and, (b) introducing the Variance-Selective SSIM (VS-SSIM) loss which adaptively emphasizes spatial regions with high pixel variance to preserve essential anatomical details during artifact correction. Experiments on chest and head CT datasets demonstrate that our model outperformed the state-of-the-art counterparts, with Peak Signal-to-Noise Ratio (PSNR) increasing by 40%, SSIM by 10%, and Dice by 16%.","authors":["Andrew Zhang","Hao Wang","Shuchang Ye","Michael Fulham","Jinman Kim"],"url":"https://arxiv.org/abs/2505.04105"}
{"created":"2025-05-08","title":"In-Situ Hardware Error Detection Using Specification-Derived Petri Net Models and Behavior-Derived State Sequences","abstract":"In hardware accelerators used in data centers and safety-critical applications, soft errors and resultant silent data corruption significantly compromise reliability, particularly when upsets occur in control-flow operations, leading to severe failures. To address this, we introduce two methods for monitoring control flows: using specification-derived Petri nets and using behavior-derived state transitions. We validated our method across four designs: convolutional layer operation, Gaussian blur, AES encryption, and a router in Network-on-Chip. Our fault injection campaign targeting the control registers and primary control inputs demonstrated high error detection rates in both datapath and control logic. Synthesis results show that a maximum detection rate is achieved with a few to around 10% area overhead in most cases. The proposed detectors quickly detect 48% to 100% of failures resulting from upsets in internal control registers and perturbations in primary control inputs. The two proposed methods were compared in terms of area overhead and error detection rate. By selectively applying these two methods, a wide range of area constraints can be accommodated, enabling practical implementation and effectively enhancing error detection capabilities.","authors":["Tomonari Tanaka","Takumi Uezono","Kohei Suenaga","Masanori Hashimoto"],"url":"https://arxiv.org/abs/2505.04108"}
{"created":"2025-05-08","title":"One2Any: One-Reference 6D Pose Estimation for Any Object","abstract":"6D object pose estimation remains challenging for many applications due to dependencies on complete 3D models, multi-view images, or training limited to specific object categories. These requirements make generalization to novel objects difficult for which neither 3D models nor multi-view images may be available. To address this, we propose a novel method One2Any that estimates the relative 6-degrees of freedom (DOF) object pose using only a single reference-single query RGB-D image, without prior knowledge of its 3D model, multi-view data, or category constraints. We treat object pose estimation as an encoding-decoding process, first, we obtain a comprehensive Reference Object Pose Embedding (ROPE) that encodes an object shape, orientation, and texture from a single reference view. Using this embedding, a U-Net-based pose decoding module produces Reference Object Coordinate (ROC) for new views, enabling fast and accurate pose estimation. This simple encoding-decoding framework allows our model to be trained on any pair-wise pose data, enabling large-scale training and demonstrating great scalability. Experiments on multiple benchmark datasets demonstrate that our model generalizes well to novel objects, achieving state-of-the-art accuracy and robustness even rivaling methods that require multi-view or CAD inputs, at a fraction of compute.","authors":["Mengya Liu","Siyuan Li","Ajad Chhatkuli","Prune Truong","Luc Van Gool","Federico Tombari"],"url":"https://arxiv.org/abs/2505.04109"}
{"created":"2025-05-08","title":"Alpha Excel Benchmark","abstract":"This study presents a novel benchmark for evaluating Large Language Models (LLMs) using challenges derived from the Financial Modeling World Cup (FMWC) Excel competitions. We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats and use this dataset to compare the performance of several leading LLMs. Our findings demonstrate significant variations in performance across different challenge categories, with models showing specific strengths in pattern recognition tasks but struggling with complex numerical reasoning. The benchmark provides a standardized framework for assessing LLM capabilities in realistic business-oriented tasks rather than abstract academic problems. This research contributes to the growing field of AI benchmarking by establishing proficiency among the 1.5 billion people who daily use Microsoft Excel as a meaningful evaluation metric that bridges the gap between academic AI benchmarks and practical business applications.","authors":["David Noever","Forrest McKee"],"url":"https://arxiv.org/abs/2505.04110"}
{"created":"2025-05-08","title":"Advancing Zero-shot Text-to-Speech Intelligibility across Diverse Domains via Preference Alignment","abstract":"Modern zero-shot text-to-speech (TTS) systems, despite using extensive pre-training, often struggle in challenging scenarios such as tongue twisters, repeated words, code-switching, and cross-lingual synthesis, leading to intelligibility issues. To address these limitations, this paper leverages preference alignment techniques, which enable targeted construction of out-of-pretraining-distribution data to enhance performance. We introduce a new dataset, named the Intelligibility Preference Speech Dataset (INTP), and extend the Direct Preference Optimization (DPO) framework to accommodate diverse TTS architectures. After INTP alignment, in addition to intelligibility, we observe overall improvements including naturalness, similarity, and audio quality for multiple TTS models across diverse domains. Based on that, we also verify the weak-to-strong generalization ability of INTP for more intelligible models such as CosyVoice 2 and Ints. Moreover, we showcase the potential for further improvements through iterative alignment based on Ints. Audio samples are available at https://intalign.github.io/.","authors":["Xueyao Zhang","Yuancheng Wang","Chaoren Wang","Ziniu Li","Zhuo Chen","Zhizheng Wu"],"url":"https://arxiv.org/abs/2505.04113"}
{"created":"2025-05-08","title":"Polynomial-Time Relational Probabilistic Inference in Open Universes","abstract":"Reasoning under uncertainty is a fundamental challenge in Artificial Intelligence. As with most of these challenges, there is a harsh dilemma between the expressive power of the language used, and the tractability of the computational problem posed by reasoning. Inspired by human reasoning, we introduce a method of first-order relational probabilistic inference that satisfies both criteria, and can handle hybrid (discrete and continuous) variables. Specifically, we extend sum-of-squares logic of expectation to relational settings, demonstrating that lifted reasoning in the bounded-degree fragment for knowledge bases of bounded quantifier rank can be performed in polynomial time, even with an a priori unknown and/or countably infinite set of objects. Crucially, our notion of tractability is framed in proof-theoretic terms, which extends beyond the syntactic properties of the language or queries. We are able to derive the tightest bounds provable by proofs of a given degree and size and establish completeness in our sum-of-squares refutations for fixed degrees.","authors":["Luise Ge","Brendan Juba","Kris Nilsson"],"url":"https://arxiv.org/abs/2505.04115"}
{"created":"2025-05-08","title":"RFNNS: Robust Fixed Neural Network Steganography with Popular Deep Generative Models","abstract":"Image steganography is a technique that conceals secret information in a cover image to achieve covert communication. Recent research has demonstrated that Fixed Neural Network Steganography (FNNS) exhibits significant practical advantages, as it enables stable and efficient steganographic embedding and extraction without requiring neural network training. However, the stego image generated by existing FNNS methods suffers from considerable distortion and exhibits poor robustness, severely reducing the security and practicality of steganography. To address the aforementioned issues, we propose a Robust Fixed Neural Network Steganography (RFNNS). In RFNNS, we introduce a texture-aware localization technique to add perturbations carrying secret image information to complex texture areas that are less perceptible to the human eye, thereby ensuring the quality of the stego image. To enhance robustness, a robust steganographic perturbation generation (RSPG) strategy is designed, which enables slight perturbations to be accurately decoded even after common image attacks. Subsequently, the generated robust perturbations are combined with the AI-generated cover image to produce the stego image. The receiver only needs to share the secret key and employ the same decoding network structure to accurately extract the secret image from the attacked stego image. Experimental results demonstrate that RFNNS achieves enhanced performance in terms of security, including imperceptibility and anti-steganalysis performance. Furthermore, RFNNS demonstrates superior robustness against common image attacks, such as JPEG compression, Gaussian noise, and contrast adjustment, across diverse embedding capacities, outperforming existing SOTA FNNS methods.","authors":["Yu Cheng","Jiuan Zhou","Jiawei Chen","Zhaoxia Yin","Xinpeng Zhang"],"url":"https://arxiv.org/abs/2505.04116"}
{"created":"2025-05-08","title":"GAPrompt: Geometry-Aware Point Cloud Prompt for 3D Vision Model","abstract":"Pre-trained 3D vision models have gained significant attention for their promising performance on point cloud data. However, fully fine-tuning these models for downstream tasks is computationally expensive and storage-intensive. Existing parameter-efficient fine-tuning (PEFT) approaches, which focus primarily on input token prompting, struggle to achieve competitive performance due to their limited ability to capture the geometric information inherent in point clouds. To address this challenge, we propose a novel Geometry-Aware Point Cloud Prompt (GAPrompt) that leverages geometric cues to enhance the adaptability of 3D vision models. First, we introduce a Point Prompt that serves as an auxiliary input alongside the original point cloud, explicitly guiding the model to capture fine-grained geometric details. Additionally, we present a Point Shift Prompter designed to extract global shape information from the point cloud, enabling instance-specific geometric adjustments at the input level. Moreover, our proposed Prompt Propagation mechanism incorporates the shape information into the model's feature extraction process, further strengthening its ability to capture essential geometric characteristics. Extensive experiments demonstrate that GAPrompt significantly outperforms state-of-the-art PEFT methods and achieves competitive results compared to full fine-tuning on various benchmarks, while utilizing only 2.19% of trainable parameters. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.","authors":["Zixiang Ai","Zichen Liu","Yuanhang Lei","Zhenyu Cui","Xu Zou","Jiahuan Zhou"],"url":"https://arxiv.org/abs/2505.04119"}
{"created":"2025-05-08","title":"On the Crouzeix-Raviart Finite Element Approximation of Phase-Field Dependent Topology Optimization in Stokes Flow","abstract":"In this work, we investigate a nonconforming finite element approximation of phase-field parameterized topology optimization governed by the Stokes flow. The phase field, the velocity field and the pressure field are approximated by conforming linear finite elements, nonconforming linear finite elements (Crouzeix-Raviart elements) and piecewise constants, respectively. When compared with the standard conforming counterpart, the nonconforming FEM can provide an approximation with fewer degrees of freedom, leading to improved computational efficiency. We establish the convergence of the resulting numerical scheme in the sense that the sequences of phase-field functions and discrete velocity fields contain subsequences that converge to a minimizing pair of the continuous problem in the $H^1$-norm and a mesh-dependent norm, respectively. We present extensive numerical results to illustrate the performance of the approach, including a comparison with the popular Taylor-Hood elements.","authors":["Bangti Jin","Jing Li","Yifeng Xu","Shengfeng Zhu"],"url":"https://arxiv.org/abs/2505.04120"}
{"created":"2025-05-08","title":"Vision Graph Prompting via Semantic Low-Rank Decomposition","abstract":"Vision GNN (ViG) demonstrates superior performance by representing images as graph structures, providing a more natural way to capture irregular semantic patterns beyond traditional grid or sequence-based representations. To efficiently adapt ViG to downstream tasks, parameter-efficient fine-tuning techniques like visual prompting become increasingly essential. However, existing prompting methods are primarily designed for Transformer-based models, neglecting the rich topological relationships among nodes and edges in graph-based representations, limiting their capacity to model complex semantics. In this paper, we propose Vision Graph Prompting (VGP), a novel framework tailored for vision graph structures. Our core insight reveals that semantically connected components in the graph exhibit low-rank properties. Building on this observation, we introduce a semantic low-rank prompting method that decomposes low-rank semantic features and integrates them with prompts on vision graph topologies, capturing both global structural patterns and fine-grained semantic dependencies. Extensive experiments demonstrate our method significantly improves ViG's transfer performance on diverse downstream tasks, achieving results comparable to full fine-tuning while maintaining parameter efficiency. Our code is available at https://github.com/zhoujiahuan1991/ICML2025-VGP.","authors":["Zixiang Ai","Zichen Liu","Jiahuan Zhou"],"url":"https://arxiv.org/abs/2505.04121"}
{"created":"2025-05-08","title":"A Framework to Prevent Biometric Data Leakage in the Immersive Technologies Domain","abstract":"Doubtlessly, the immersive technologies have potential to ease people's life and uplift economy, however the obvious data privacy risks cannot be ignored. For example, a participant wears a 3D headset device which detects participant's head motion to track the pose of participant's head to match the orientation of camera with participant's eyes positions in the real-world. In a preliminary study, researchers have proved that the voice command features on such headsets could lead to major privacy leakages. By analyzing the facial dynamics captured with the motion sensors, the headsets suffer security vulnerabilities revealing a user's sensitive speech without user's consent. The psychography data (such as voice command features, facial dynamics, etc.) is sensitive data and it should not be leaked out of the device without users consent else it is a privacy breach. To the best of our literature review, the work done in this particular research problem is very limited. Motivated from this, we develop a simple technical framework to mitigate sensitive data (or biometric data) privacy leaks in immersive technology domain. The performance evaluation is conducted in a robust way using six data sets, to show that the proposed solution is effective and feasible to prevent this issue.","authors":["Keshav Sood","Iynkaran Natgunanathan","Uthayasanker Thayasivam","Vithurabiman Senthuran","Xiaoning Zhang","Shui Yu"],"url":"https://arxiv.org/abs/2505.04123"}
{"created":"2025-05-08","title":"Reinforcement Learning-Aided Design of Efficient Polarization Kernels","abstract":"Polar codes with large kernels achieve optimal error exponents but are difficult to construct when low decoding complexity is also required. We address this challenge under recursive maximum likelihood decoding (RMLD) using a rein- forcement learning approach based on the Gumbel AlphaZero algorithm. The resulting method, PolarZero, consistently matches exhaustive search in identifying low-complexity kernels, and discovers a size-16 kernel with complexity comparable to handcrafted designs. Our results suggest that PolarZero is a scalable tool for large-kernel design, where brute-force search is no longer feasible.","authors":["Yi-Ting Hong","Stefano Rini","Luca Barletta"],"url":"https://arxiv.org/abs/2505.04127"}
{"created":"2025-05-08","title":"Scalable 49-Channel Neural Recorder with an Event-Driven Ramp ADC and PCA Compression in 28 nm CMOS","abstract":"Neural interfaces advance neuroscience research and therapeutic innovations by accurately measuring neuronal activity. However, recording raw data from numerous neurons results in substantial amount of data and poses challenges for wireless transmission. While conventional neural recorders consume energy to digitize and process the full neural signal, only a fraction of this data carries essential spiking information. Leveraging on this signal sparsity, this paper introduces a neural recording integrated circuit in TSMC 28nm CMOS. It features an event-driven ramp analog-to-digital converter, and a spike compression module based on principal component analysis. The circuit consists of 49 channels, each occupying an on-chip area of 50 $\\times$ 60 $\\mu$m$^2$. The circuit measures 1370 $\\times$ 1370 $\\mu$m$^2$ and consumes 534 $\\mu$W. Compression testing on a synthetic dataset demonstrated an 8.8-fold reduction compared to raw spikes and a 328-fold reduction relative to the raw signal. This compression approach maintained a spike sorting accuracy of 74.9%, compared to the 79.5% accuracy obtained with the raw signal. The paper details the architecture and performance outcomes of the neural recording circuit and its compression module.","authors":["William Lemaire","Esmaeil Ranjbar Koleibi","Maher Benhouria","Konin Koua","J\\'er\\'emy M\\'enard","Keven Gagnon","Charles Quesnel","Louis-Philippe Gauthier","Takwa Omrani","Montassar Dridi","Mahdi Majdoub","Marwan Besrour","S\\'ebastien Roy","R\\'ejean Fontaine"],"url":"https://arxiv.org/abs/2505.04128"}
{"created":"2025-05-08","title":"Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes in the Agave Validator","abstract":"In this paper we analyze some of the bottlenecks in the execution pipeline of Solana's Agave validator client, focusing on RAM and program cache usage under mainnet conditions. Through a series of controlled experiments, we measure the validator's throughput and resource efficiency as RAM availability ranges between 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance degrades significantly below 256 GB, with transaction processing falling behind real-time block production. Additionally, we study the program cache behavior, identifying inefficiencies in program eviction and load latency. Our results provide practical guidance for hardware provisioning and suggest improvements to the Solana execution and caching strategy, reducing latency due to the program cache by 90%.","authors":["Turan Vural","Yuki Yuminaga","Alex Petrosyan","Ben Livshits"],"url":"https://arxiv.org/abs/2505.04129"}
{"created":"2025-05-08","title":"Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model","abstract":"Access to legal information is fundamental to access to justice. Yet accessibility refers not only to making legal documents available to the public, but also rendering legal information comprehensible to them. A vexing problem in bringing legal information to the public is how to turn formal legal documents such as legislation and judgments, which are often highly technical, to easily navigable and comprehensible knowledge to those without legal education. In this study, we formulate a three-step approach for bringing legal knowledge to laypersons, tackling the issues of navigability and comprehensibility. First, we translate selected sections of the law into snippets (called CLIC-pages), each being a small piece of article that focuses on explaining certain technical legal concept in layperson's terms. Second, we construct a Legal Question Bank (LQB), which is a collection of legal questions whose answers can be found in the CLIC-pages. Third, we design an interactive CLIC Recommender (CRec). Given a user's verbal description of a legal situation that requires a legal solution, CRec interprets the user's input and shortlists questions from the question bank that are most likely relevant to the given legal situation and recommends their corresponding CLIC pages where relevant legal knowledge can be found. In this paper we focus on the technical aspects of creating an LQB. We show how large-scale pre-trained language models, such as GPT-3, can be used to generate legal questions. We compare machine-generated questions (MGQs) against human-composed questions (HCQs) and find that MGQs are more scalable, cost-effective, and more diversified, while HCQs are more precise. We also show a prototype of CRec and illustrate through an example how our 3-step approach effectively brings relevant legal knowledge to the public.","authors":["Mingruo Yuan","Ben Kao","Tien-Hsuan Wu","Michael M. K. Cheung","Henry W. H. Chan","Anne S. Y. Cheung","Felix W. H. Chan","Yongxi Chen"],"url":"https://arxiv.org/abs/2505.04132"}
{"created":"2025-05-08","title":"Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models","abstract":"We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.","authors":["Vihaan Miriyala","Smrithi Bukkapatnam","Lavanya Prahallad"],"url":"https://arxiv.org/abs/2505.04135"}
{"created":"2025-05-08","title":"Delegation and Participation in Decentralized Governance: An Epistemic View","abstract":"We develop and apply epistemic tests to various decentralized governance methods as well as to study the impact of participation. These tests probe the ability to reach a correct outcome when there is one. We find that partial abstention is a strong governance method from an epistemic standpoint compared to alternatives such as various forms of ``transfer delegation\" in which voters explicitly transfer some or all of their voting rights to others. We make a stronger case for multi-step transfer delegation than is present in previous work but also demonstrate that transfer delegation has inherent epistemic weaknesses. We show that enhanced direct participation, voters exercising their own voting rights, can have a variety of epistemic impacts, some very negative. We identify governance conditions under which additional direct participation is guaranteed to do no epistemic harm and is likely to increase the probability of making correct decisions. In light of the epistemic challenges of voting-based decentralized governance, we consider the possible supplementary use of prediction markets, auctions, and AI agents to improve outcomes. All these results are significant because epistemic performance matters if entities such as DAOs (decentralized autonomous organizations) wish to compete with organizations that are more centralized.","authors":["Jeff Strnad"],"url":"https://arxiv.org/abs/2505.04136"}
{"created":"2025-05-08","title":"optHIM: Hybrid Iterative Methods for Continuous Optimization in PyTorch","abstract":"We introduce optHIM, an open-source library of continuous unconstrained optimization algorithms implemented in PyTorch for both CPU and GPU. By leveraging PyTorch's autograd, optHIM seamlessly integrates function, gradient, and Hessian information into flexible line-search and trust-region methods. We evaluate eleven state-of-the-art variants on benchmark problems spanning convex and non-convex landscapes. Through a suite of quantitative metrics and qualitative analyses, we demonstrate each method's strengths and trade-offs. optHIM aims to democratize advanced optimization by providing a transparent, extensible, and efficient framework for research and education.","authors":["Nikhil Sridhar","Sajiv Shah"],"url":"https://arxiv.org/abs/2505.04137"}
{"created":"2025-05-08","title":"LHT: Statistically-Driven Oblique Decision Trees for Interpretable Classification","abstract":"We introduce the Learning Hyperplane Tree (LHT), a novel oblique decision tree model designed for expressive and interpretable classification. LHT fundamentally distinguishes itself through a non-iterative, statistically-driven approach to constructing splitting hyperplanes. Unlike methods that rely on iterative optimization or heuristics, LHT directly computes the hyperplane parameters, which are derived from feature weights based on the differences in feature expectations between classes within each node. This deterministic mechanism enables a direct and well-defined hyperplane construction process. Predictions leverage a unique piecewise linear membership function within leaf nodes, obtained via local least-squares fitting. We formally analyze the convergence of the LHT splitting process, ensuring that each split yields meaningful, non-empty partitions. Furthermore, we establish that the time complexity for building an LHT up to depth $d$ is $O(mnd)$, demonstrating the practical feasibility of constructing trees with powerful oblique splits using this methodology. The explicit feature weighting at each split provides inherent interpretability. Experimental results on benchmark datasets demonstrate LHT's competitive accuracy, positioning it as a practical, theoretically grounded, and interpretable alternative in the landscape of tree-based models. The implementation of the proposed method is available at https://github.com/Hongyi-Li-sz/LHT_model.","authors":["Hongyi Li","Jun Xu","William Ward Armstrong"],"url":"https://arxiv.org/abs/2505.04139"}
{"created":"2025-05-08","title":"NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large Language Model Guidance","abstract":"Several planners have been proposed to compute robot paths that reach desired goal regions while avoiding obstacles. However, these methods fail when all pathways to the goal are blocked. In such cases, the robot must reason about how to reconfigure the environment to access task-relevant regions - a problem known as Navigation Among Movable Objects (NAMO). While various solutions to this problem have been developed, they often struggle to scale to highly cluttered environments. To address this, we propose NAMO-LLM, a sampling-based planner that searches over robot and obstacle configurations to compute feasible plans specifying which obstacles to move, where, and in what order. Its key novelty is a non-uniform sampling strategy guided by Large Language Models (LLMs) biasing the tree construction toward directions more likely to yield a solution. We show that NAMO-LLM is probabilistically complete and demonstrate through experiments that it efficiently scales to cluttered environments, outperforming related works in both runtime and plan quality.","authors":["Yuqing Zhang","Yiannis Kantaros"],"url":"https://arxiv.org/abs/2505.04141"}
{"created":"2025-05-08","title":"Evaluating Performance Consistency in Competitive Programming: Educational Implications and Contest Design Insights","abstract":"Competitive programming (CP) contests are often treated as interchangeable proxies for algorithmic skill, yet the extent to which results at lower contest tiers anticipate performance at higher tiers, and how closely any tier resembles the ubiquitous online-contest circuit, remains unclear. We analyze ten years (2015--2024) of International Collegiate Programming Contest (ICPC) standings, comprising five long-running superregional championships (Africa \\& Arab, Asia East, Asia West, North America, and Northern Eurasia), associated local regionals of North America and Northern Eurasia, and the World Finals. For 366 World Finalist teams (2021--2024) we augment the dataset with pre-contest Codeforces ratings. Pairwise rank alignment is measured with Kendall's $\\tau$.","authors":["Zhongtang Luo","Ethan Dickey"],"url":"https://arxiv.org/abs/2505.04143"}
{"created":"2025-05-08","title":"Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety","abstract":"Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.","authors":["Variath Madhupal Gautham Nair","Vishal Varma Dantuluri"],"url":"https://arxiv.org/abs/2505.04146"}
{"created":"2025-05-08","title":"R^3-VQA: \"Read the Room\" by Video Social Reasoning","abstract":"\"Read the room\" is a significant social reasoning capability in human daily life. Humans can infer others' mental states from subtle social cues. Previous social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic interactions, incomplete mental state variables, single-step reasoning, etc.) and fall far short of the challenges present in real-life social interactions. In this paper, we contribute a valuable, high-quality, and comprehensive video dataset named R^3-VQA with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social causal chains in complex social scenarios. Moreover, we include human-annotated and model-generated QAs. Our task R^3-VQA includes three aspects: Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. As a benchmark, we comprehensively evaluate the social reasoning capabilities and consistencies of current state-of-the-art large vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs are still far from human-level consistent social reasoning in complex social scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on social reasoning tasks. We provide some of our dataset and codes in supplementary material and will release our full dataset and codes upon acceptance.","authors":["Lixing Niu","Jiapeng Li","Xingping Yu","Shu Wang","Ruining Feng","Bo Wu","Ping Wei","Yisen Wang","Lifeng Fan"],"url":"https://arxiv.org/abs/2505.04147"}
{"created":"2025-05-08","title":"Learning from Similarity Proportion Loss for Classifying Skeletal Muscle Recovery Stages","abstract":"Evaluating the regeneration process of damaged muscle tissue is a fundamental analysis in muscle research to measure experimental effect sizes and uncover mechanisms behind muscle weakness due to aging and disease. The conventional approach to assessing muscle tissue regeneration involves whole-slide imaging and expert visual inspection of the recovery stages based on the morphological information of cells and fibers. There is a need to replace these tasks with automated methods incorporating machine learning techniques to ensure a quantitative and objective analysis. Given the limited availability of fully labeled data, a possible approach is Learning from Label Proportions (LLP), a weakly supervised learning method using class label proportions. However, current LLP methods have two limitations: (1) they cannot adapt the feature extractor for muscle tissues, and (2) they treat the classes representing recovery stages and cell morphological changes as nominal, resulting in the loss of ordinal information. To address these issues, we propose Ordinal Scale Learning from Similarity Proportion (OSLSP), which uses a similarity proportion loss derived from two bag combinations. OSLSP can update the feature extractor by using class proportion attention to the ordinal scale of the class. Our model with OSLSP outperforms large-scale pre-trained and fine-tuning models in classification tasks of skeletal muscle recovery stages.","authors":["Yu Yamaoka or Weng Ian Chan","Shigeto Seno","Soichiro Fukada","Hideo Matsuda"],"url":"https://arxiv.org/abs/2505.04150"}
{"created":"2025-05-08","title":"Can Language Models Understand Social Behavior in Clinical Conversations?","abstract":"Effective communication between providers and their patients influences health and care outcomes. The effectiveness of such conversations has been linked not only to the exchange of clinical information, but also to a range of interpersonal behaviors; commonly referred to as social signals, which are often conveyed through non-verbal cues and shape the quality of the patient-provider relationship. Recent advances in large language models (LLMs) have demonstrated an increasing ability to infer emotional and social behaviors even when analyzing only textual information. As automation increases also in clinical settings, such as for transcription of patient-provider conversations, there is growing potential for LLMs to automatically analyze and extract social behaviors from these interactions. To explore the foundational capabilities of LLMs in tracking social signals in clinical dialogue, we designed task-specific prompts and evaluated model performance across multiple architectures and prompting styles using a highly imbalanced, annotated dataset spanning 20 distinct social signals such as provider dominance, patient warmth, etc. We present the first system capable of tracking all these 20 coded signals, and uncover patterns in LLM behavior. Further analysis of model configurations and clinical context provides insights for enhancing LLM performance on social signal processing tasks in healthcare settings.","authors":["Manas Satish Bedmutha","Feng Chen","Andrea Hartzler","Trevor Cohen","Nadir Weibel"],"url":"https://arxiv.org/abs/2505.04152"}
{"created":"2025-05-08","title":"Global Hash Tables Strike Back! An Analysis of Parallel GROUP BY Aggregation","abstract":"Efficiently computing group aggregations (i.e., GROUP BY) on modern many-core architectures is critical for analytic database systems. Today's engines predominately use a partitioned approach to group aggregation, in which an incoming data stream is partitioned by key values so that every row for a particular key is sent to the same thread. In this paper, we revisit a simpler strategy: a fully concurrent group aggregation technique using a shared global hash table. While approaches using general-purpose concurrent hash tables have generally been found to perform worse than partitioning-based approaches, we argue that the key ingredient is customizing the concurrent hash table for the specific task of group aggregation. Through extensive experiments on synthetic workloads (varying key cardinality, skew, and thread counts), we demonstrate that a purpose-built concurrent hash table can match or surpass partitioning-based techniques. We also analyze the operational characteristics of both techniques, including resizing costs and memory pressure. In the process, we derive practical guidelines for database implementers. Overall, our analysis indicates that fully concurrent group aggregation is a viable alternative to partitioning.","authors":["Daniel Xue","Ryan Marcus"],"url":"https://arxiv.org/abs/2505.04153"}
{"created":"2025-05-08","title":"An Adaptive Mixed Precision and Dynamically Scaled Preconditioned Conjugate Gradient Algorithm","abstract":"We propose an adaptive mixed precision and dynamically scaled preconditioned conjugate gradient algorithm (AMP-PCG). It dynamically adjusts the precision for storing vectors and computing, exploiting low precision when appropriate, while maintaining a convergence rate and accuracy comparable to that of double precision PCG. Our mixed precision strategy consists of three main components: (1) The residual and matrix-vector product are initially computed in double precision, and the algorithm switches these to single precision based on the chosen convergence tolerance and an estimate of the residual gap. (2) Depending on the eigenvalue distribution, the preconditioned residual and search direction are either in half precision throughout the iterations or initially in double precision and then stepwise reduced to single and half precision. (3) A dynamically scaled residual is used at every iteration to mitigate underflow in half precision. We provide theoretical support for our estimates and we demonstrate the effectiveness of AMP-PCG through numerical experiments, highlighting both its robustness and the significant performance gains (1.63x speedup) achieved compared to double precision PCG on a GPU.","authors":["Yichen Guo","Eric de Sturler","Tim Warburton"],"url":"https://arxiv.org/abs/2505.04155"}
{"created":"2025-05-08","title":"FilterTS: Comprehensive Frequency Filtering for Multivariate Time Series Forecasting","abstract":"Multivariate time series forecasting is crucial across various industries, where accurate extraction of complex periodic and trend components can significantly enhance prediction performance. However, existing models often struggle to capture these intricate patterns. To address these challenges, we propose FilterTS, a novel forecasting model that utilizes specialized filtering techniques based on the frequency domain. FilterTS introduces a Dynamic Cross-Variable Filtering Module, a key innovation that dynamically leverages other variables as filters to extract and reinforce shared variable frequency components across variables in multivariate time series. Additionally, a Static Global Filtering Module captures stable frequency components, identified throughout the entire training set. Moreover, the model is built in the frequency domain, converting time-domain convolutions into frequency-domain multiplicative operations to enhance computational efficiency. Extensive experimental results on eight real-world datasets have demonstrated that FilterTS significantly outperforms existing methods in terms of prediction accuracy and computational efficiency.","authors":["Yulong Wang","Yushuo Liu","Xiaoyi Duan","Kai Wang"],"url":"https://arxiv.org/abs/2505.04158"}
{"created":"2025-05-08","title":"Optimization of Infectious Disease Intervention Measures Based on Reinforcement Learning - Empirical analysis based on UK COVID-19 epidemic data","abstract":"Globally, the outbreaks of infectious diseases have exerted an extremely profound and severe influence on health security and the economy. During the critical phases of epidemics, devising effective intervention measures poses a significant challenge to both the academic and practical arenas. There is numerous research based on reinforcement learning to optimize intervention measures of infectious diseases. Nevertheless, most of these efforts have been confined within the differential equation based on infectious disease models. Although a limited number of studies have incorporated reinforcement learning methodologies into individual-based infectious disease models, the models employed therein have entailed simplifications and limitations, rendering it incapable of modeling the complexity and dynamics inherent in infectious disease transmission. We establish a decision-making framework based on an individual agent-based transmission model, utilizing reinforcement learning to continuously explore and develop a strategy function. The framework's validity is verified through both experimental and theoretical approaches. Covasim, a detailed and widely used agent-based disease transmission model, was modified to support reinforcement learning research. We conduct an exhaustive exploration of the application efficacy of multiple algorithms across diverse action spaces. Furthermore, we conduct an innovative preliminary theoretical analysis concerning the issue of \"time coverage\". The results of the experiment robustly validate the effectiveness and feasibility of the methodological framework of this study. The coping strategies gleaned therefrom prove highly efficacious in suppressing the expansion of the epidemic scale and safeguarding the stability of the economic system, thereby providing crucial reference perspectives for the formulation of global public health security strategies.","authors":["Baida Zhang","Yakai Chen","Huichun Li","Zhenghu Zu"],"url":"https://arxiv.org/abs/2505.04161"}
{"created":"2025-05-08","title":"SCU-Hand: Soft Conical Universal Robotic Hand for Scooping Granular Media from Containers of Various Sizes","abstract":"Automating small-scale experiments in materials science presents challenges due to the heterogeneous nature of experimental setups. This study introduces the SCU-Hand (Soft Conical Universal Robot Hand), a novel end-effector designed to automate the task of scooping powdered samples from various container sizes using a robotic arm. The SCU- Hand employs a flexible, conical structure that adapts to dif- ferent container geometries through deformation, maintaining consistent contact without complex force sensing or machine learning-based control methods. Its reconfigurable mechanism allows for size adjustment, enabling efficient scooping from diverse container types. By combining soft robotics principles with a sheet-morphing design, our end-effector achieves high flexibility while retaining the necessary stiffness for effective powder manipulation. We detail the design principles, fabri- cation process, and experimental validation of the SCU-Hand. Experimental validation showed that the scooping capacity is about 20% higher than that of a commercial tool, with a scooping performance of more than 95% for containers of sizes between 67 mm to 110 mm. This research contributes to laboratory automation by offering a cost-effective, easily implementable solution for automating tasks such as materials synthesis and characterization processes.","authors":["Tomoya Takahashi","Cristian C. Beltran-Hernandez","Yuki Kuroda","Kazutoshi Tanaka","Masashi Hamaya","Yoshitaka Ushiku"],"url":"https://arxiv.org/abs/2505.04162"}
{"created":"2025-05-08","title":"Retrieval Augmented Time Series Forecasting","abstract":"Time series forecasting uses historical data to predict future trends, leveraging the relationships between past observations and available features. In this paper, we propose RAFT, a retrieval-augmented time series forecasting method to provide sufficient inductive biases and complement the model's learning capacity. When forecasting the subsequent time frames, we directly retrieve historical data candidates from the training dataset with patterns most similar to the input, and utilize the future values of these candidates alongside the inputs to obtain predictions. This simple approach augments the model's capacity by externally providing information about past patterns via retrieval modules. Our empirical evaluations on ten benchmark datasets show that RAFT consistently outperforms contemporary baselines with an average win ratio of 86%.","authors":["Sungwon Han","Seungeon Lee","Meeyoung Cha","Sercan O Arik","Jinsung Yoon"],"url":"https://arxiv.org/abs/2505.04163"}
{"created":"2025-05-08","title":"TS-SNN: Temporal Shift Module for Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs) are increasingly recognized for their biological plausibility and energy efficiency, positioning them as strong alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing applications. SNNs inherently process temporal information by leveraging the precise timing of spikes, but balancing temporal feature utilization with low energy consumption remains a challenge. In this work, we introduce Temporal Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel Temporal Shift (TS) module to integrate past, present, and future spike features within a single timestep via a simple yet effective shift operation. A residual combination method prevents information loss by integrating shifted and original features. The TS module is lightweight, requiring only one additional learnable parameter, and can be seamlessly integrated into existing architectures with minimal additional computational cost. TS-SNN achieves state-of-the-art performance on benchmarks like CIFAR-10 (96.72\\%), CIFAR-100 (80.28\\%), and ImageNet (70.61\\%) with fewer timesteps, while maintaining low energy consumption. This work marks a significant step forward in developing efficient and accurate SNN architectures.","authors":["Kairong Yu","Tianqing Zhang","Qi Xu","Gang Pan","Hongwei Wang"],"url":"https://arxiv.org/abs/2505.04165"}
{"created":"2025-05-08","title":"STRGCN: Capturing Asynchronous Spatio-Temporal Dependencies for Irregular Multivariate Time Series Forecasting","abstract":"Irregular multivariate time series (IMTS) are prevalent in real-world applications across many fields, where varying sensor frequencies and asynchronous measurements pose significant modeling challenges. Existing solutions often rely on a pre-alignment strategy to normalize data, which can distort intrinsic patterns and escalate computational and memory demands. Addressing these limitations, we introduce STRGCN, a Spatio-Temporal Relational Graph Convolutional Network that avoids pre-alignment and directly captures the complex interdependencies in IMTS by representing them as a fully connected graph. Each observation is represented as a node, allowing the model to effectively handle misaligned timestamps by mapping all inter-node relationships, thus faithfully preserving the asynchronous nature of the data. Moreover, we enhance this model with a hierarchical ``Sandwich'' structure that strategically aggregates nodes to optimize graph embeddings, reducing computational overhead while maintaining detailed local and global context. Extensive experiments on four public datasets demonstrate that STRGCN achieves state-of-the-art accuracy, competitive memory usage and training speed.","authors":["Yulong Wang","Xiaofeng Hu","Xiaojian Cui","Kai Wang"],"url":"https://arxiv.org/abs/2505.04167"}
{"created":"2025-05-08","title":"Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts","abstract":"Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.","authors":["Nouar Aldahoul","Hazem Ibrahim","Matteo Varvello","Aaron Kaufman","Talal Rahwan","Yasir Zaki"],"url":"https://arxiv.org/abs/2505.04171"}
{"created":"2025-05-08","title":"DiffPattern-Flex: Efficient Layout Pattern Generation via Discrete Diffusion","abstract":"Recent advancements in layout pattern generation have been dominated by deep generative models. However, relying solely on neural networks for legality guarantees raises concerns in many practical applications. In this paper, we present \\tool{DiffPattern}-Flex, a novel approach designed to generate reliable layout patterns efficiently. \\tool{DiffPattern}-Flex incorporates a new method for generating diverse topologies using a discrete diffusion model while maintaining a lossless and compute-efficient layout representation. To ensure legal pattern generation, we employ {an} optimization-based, white-box pattern assessment process based on specific design rules. Furthermore, fast sampling and efficient legalization technologies are employed to accelerate the generation process. Experimental results across various benchmarks demonstrate that \\tool{DiffPattern}-Flex significantly outperforms existing methods and excels at producing reliable layout patterns.","authors":["Zixiao Wang","Wenqian Zhao","Yunheng Shen","Yang Bai","Guojin Chen","Farzan Farnia","Bei Yu"],"url":"https://arxiv.org/abs/2505.04173"}
{"created":"2025-05-08","title":"On-Device LLM for Context-Aware Wi-Fi Roaming","abstract":"Wireless roaming is a critical yet challenging task for maintaining seamless connectivity in dynamic mobile environments. Conventional threshold-based or heuristic schemes often fail, leading to either sticky or excessive handovers. We introduce the first cross-layer use of an on-device large language model (LLM): high-level reasoning in the application layer that issues real-time actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i) context-aware AP selection, where structured prompts fuse environmental cues (e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold adjustment, where the model adaptively decides when to roam. To satisfy the tight latency and resource budgets of edge hardware, we apply a suite of optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and quantization. Experiments on indoor and outdoor datasets show that our approach surpasses legacy heuristics and DRL baselines, achieving a strong balance between roaming stability and signal quality. These findings underscore the promise of application-layer LLM reasoning for lower-layer wireless control in future edge systems.","authors":["Ju-Hyung Lee","Yanqing Lu"],"url":"https://arxiv.org/abs/2505.04174"}
{"created":"2025-05-08","title":"DOTA: Deformable Optimized Transformer Architecture for End-to-End Text Recognition with Retrieval-Augmented Generation","abstract":"Text recognition in natural images remains a challenging yet essential task, with broad applications spanning computer vision and natural language processing. This paper introduces a novel end-to-end framework that combines ResNet and Vision Transformer backbones with advanced methodologies, including Deformable Convolutions, Retrieval-Augmented Generation, and Conditional Random Fields (CRF). These innovations collectively enhance feature representation and improve Optical Character Recognition (OCR) performance. Specifically, the framework substitutes standard convolution layers in the third and fourth blocks with Deformable Convolutions, leverages adaptive dropout for regularization, and incorporates CRF for more refined sequence modeling. Extensive experiments conducted on six benchmark datasets IC13, IC15, SVT, IIIT5K, SVTP, and CUTE80 validate the proposed method's efficacy, achieving notable accuracies: 97.32% on IC13, 58.26% on IC15, 88.10% on SVT, 74.13% on IIIT5K, 82.17% on SVTP, and 66.67% on CUTE80, resulting in an average accuracy of 77.77%. These results establish a new state-of-the-art for text recognition, demonstrating the robustness of the approach across diverse and challenging datasets.","authors":["Naphat Nithisopa","Teerapong Panboonyuen"],"url":"https://arxiv.org/abs/2505.04175"}
{"created":"2025-05-08","title":"Impact of Grid-Forming Inverters on Protective Relays: A Perspective for Current Limiting Control Design","abstract":"Grid-forming (GFM) inverters can significantly alter the fault characteristics of power systems, which challenges the proper function of protective relays. This paper gives a holistic analysis of the interaction between GFM inverter-based resources (IBRs) and the supervising elements in protective relays, including directional and phase selection elements. It is revealed that the current limiting control (CLC) that is based on the current reference saturation method, adversely affects the performance of supervising elements that rely on the negative-sequence quantities. In contrast, adopting highly inductive virtual impedance in the CLC enables a reliable operation of such elements. This finding provides insights into the design of CLC for GFM IBRs from a protection perspective. It is further found that even with a highly inductive virtual impedance, the altered virtual impedance dynamics introduced by the CLC can still lead to malfunctions of the incremental quantity-based supervising elements. These theoretical findings are corroborated by simulations and controller hardware-in-the-loop (CHIL) tests.","authors":["Yifei Li","Heng Wu","Xiongfei Wang"],"url":"https://arxiv.org/abs/2505.04177"}
{"created":"2025-05-08","title":"Towards Large-scale Generative Ranking","abstract":"Generative recommendation has recently emerged as a promising paradigm in information retrieval. However, generative ranking systems are still understudied, particularly with respect to their effectiveness and feasibility in large-scale industrial settings. This paper investigates this topic at the ranking stage of Xiaohongshu's Explore Feed, a recommender system that serves hundreds of millions of users. Specifically, we first examine how generative ranking outperforms current industrial recommenders. Through theoretical and empirical analyses, we find that the primary improvement in effectiveness stems from the generative architecture, rather than the training paradigm. To facilitate efficient deployment of generative ranking, we introduce RankGPT, a novel generative architecture for ranking. We validate the effectiveness and efficiency of our solution through online A/B experiments. The results show that RankGPT achieves significant improvements in user satisfaction with nearly equivalent computational resources compared to the existing production system.","authors":["Yanhua Huang","Yuqi Chen","Xiong Cao","Rui Yang","Mingliang Qi","Yinghao Zhu","Qingchang Han","Yaowei Liu","Zhaoyu Liu","Xuefeng Yao","Yuting Jia","Leilei Ma","Yinqi Zhang","Taoyu Zhu","Liujie Zhang","Lei Chen","Weihang Chen","Min Zhu","Ruiwen Xu","Lei Zhang"],"url":"https://arxiv.org/abs/2505.04180"}
{"created":"2025-05-08","title":"Privacy Challenges In Image Processing Applications","abstract":"As image processing systems proliferate, privacy concerns intensify given the sensitive personal information contained in images. This paper examines privacy challenges in image processing and surveys emerging privacy-preserving techniques including differential privacy, secure multiparty computation, homomorphic encryption, and anonymization. Key applications with heightened privacy risks include healthcare, where medical images contain patient health data, and surveillance systems that can enable unwarranted tracking. Differential privacy offers rigorous privacy guarantees by injecting controlled noise, while MPC facilitates collaborative analytics without exposing raw data inputs. Homomorphic encryption enables computations on encrypted data and anonymization directly removes identifying elements. However, balancing privacy protections and utility remains an open challenge. Promising future directions identified include quantum-resilient cryptography, federated learning, dedicated hardware, and conceptual innovations like privacy by design. Ultimately, a holistic effort combining technological innovations, ethical considerations, and policy frameworks is necessary to uphold the fundamental right to privacy as image processing capabilities continue advancing rapidly.","authors":["Maneesha","Bharat Gupta","Rishabh Sethi","Charvi Adita Das"],"url":"https://arxiv.org/abs/2505.04181"}
{"created":"2025-05-08","title":"Beyond Task Performance: Human Experience in Human-Robot Collaboration","abstract":"Human interaction experience plays a crucial role in the effectiveness of human-machine collaboration, especially as interactions in future systems progress towards tighter physical and functional integration. While automation design has been shown to impact task performance, its influence on human experi- ence metrics such as flow, sense of agency (SoA), and embodiment remains underexplored. This study investigates how variations in automation design affect these psychological experience mea- sures and examines correlations between subjective experience and physiological indicators. A user study was conducted in a simulated wood workshop, where participants collaborated with a lightweight robot under four automation levels. The results of the study indicate that medium automation levels enhance flow, SoA and embodiment, striking a balance between support and user autonomy. In contrast, higher automation, despite optimizing task performance, diminishes perceived flow and agency. Furthermore, we observed that grip force might be considered as a real-time proxy of SoA, while correlations with heart rate variability were inconclusive. The findings underscore the necessity for automation strategies that integrate human- centric metrics, aiming to optimize both performance and user experience in collaborative robotic systems","authors":["Sean Kille","Jan Heinrich Robens","Philipp Dahlinger","Alejandra Rodriguez-Velasquez","Simon Rothfu{\\ss}","Balint Varga","Andreas Lindenmann","Gerhard Neumann","Sven Matthiesen","Andrea Kiesel","S\\\"oren Hohmann"],"url":"https://arxiv.org/abs/2505.04182"}
{"created":"2025-05-08","title":"State-of-the-Art HCI for Dementia Care: A Scoping Review of Recent Technological Advances","abstract":"Dementia significantly impacts cognitive, behavioral, and functional abilities, creating challenges for both individuals and caregivers. Recent advancements in HCI have introduced innovative technological solutions to support people with dementia (PwD) and their caregivers. This scoping review systematically examines 32 recent publications from leading digital libraries, categorizing technological interventions into four key domains: Assistive and Smart Technology for Daily Life, Social Interaction and Communication, Well-being and Psychological Support, and Caregiver Support and Training. Our analysis highlights how emerging technologies are transforming dementia care. These technologies enhance quality of life by promoting independence, fostering social engagement, and providing emotional and cognitive support. However, the review also identifies critical gaps, particularly in addressing the needs of individuals with early-stage dementia and the lack of individualized support mechanisms. By emphasizing user-centered design, accessibility, and ethical considerations, this paper offers a structured roadmap for future research and practice in dementia care. It bridges the gap between technological innovation and the real-world needs of PwD and their caregivers, providing valuable insights for researchers, practitioners, and policymakers. This review not only synthesizes current advancements but also sets the stage for future HCI-driven innovations in dementia care, aiming to improve outcomes for an aging global population.","authors":["Yong Ma","Yuchong Zhang","Oda Elise Nordberg","Arvid Rongve","Miroslav Bachinski","Morten Fjeld"],"url":"https://arxiv.org/abs/2505.04184"}
{"created":"2025-05-08","title":"S3D: Sketch-Driven 3D Model Generation","abstract":"Generating high-quality 3D models from 2D sketches is a challenging task due to the inherent ambiguity and sparsity of sketch data. In this paper, we present S3D, a novel framework that converts simple hand-drawn sketches into detailed 3D models. Our method utilizes a U-Net-based encoder-decoder architecture to convert sketches into face segmentation masks, which are then used to generate a 3D representation that can be rendered from novel views. To ensure robust consistency between the sketch domain and the 3D output, we introduce a novel style-alignment loss that aligns the U-Net bottleneck features with the initial encoder outputs of the 3D generation module, significantly enhancing reconstruction fidelity. To further enhance the network's robustness, we apply augmentation techniques to the sketch dataset. This streamlined framework demonstrates the effectiveness of S3D in generating high-quality 3D models from sketch inputs. The source code for this project is publicly available at https://github.com/hailsong/S3D.","authors":["Hail Song","Wonsik Shin","Naeun Lee","Soomin Chung","Nojun Kwak","Woontack Woo"],"url":"https://arxiv.org/abs/2505.04185"}
{"created":"2025-05-08","title":"VideoPath-LLaVA: Pathology Diagnostic Reasoning Through Video Instruction Tuning","abstract":"We present VideoPath-LLaVA, the first large multimodal model (LMM) in computational pathology that integrates three distinct image scenarios, single patch images, automatically keyframe-extracted clips, and manually segmented video pathology images, to mimic the natural diagnostic process of pathologists. By generating detailed histological descriptions and culminating in a definitive sign-out diagnosis, VideoPath-LLaVA bridges visual narratives with diagnostic reasoning.","authors":["Trinh T. L. Vuong","Jin Tae Kwak"],"url":"https://arxiv.org/abs/2505.04192"}
{"created":"2025-05-08","title":"Trajectory Entropy Reinforcement Learning for Predictable and Robust Control","abstract":"Simplicity is a critical inductive bias for designing data-driven controllers, especially when robustness is important. Despite the impressive results of deep reinforcement learning in complex control tasks, it is prone to capturing intricate and spurious correlations between observations and actions, leading to failure under slight perturbations to the environment. To tackle this problem, in this work we introduce a novel inductive bias towards simple policies in reinforcement learning. The simplicity inductive bias is introduced by minimizing the entropy of entire action trajectories, corresponding to the number of bits required to describe information in action trajectories after the agent observes state trajectories. Our reinforcement learning agent, Trajectory Entropy Reinforcement Learning, is optimized to minimize the trajectory entropy while maximizing rewards. We show that the trajectory entropy can be effectively estimated by learning a variational parameterized action prediction model, and use the prediction model to construct an information-regularized reward function. Furthermore, we construct a practical algorithm that enables the joint optimization of models, including the policy and the prediction model. Experimental evaluations on several high-dimensional locomotion tasks show that our learned policies produce more cyclical and consistent action trajectories, and achieve superior performance, and robustness to noise and dynamic changes than the state-of-the-art.","authors":["Bang You","Chenxu Wang","Huaping Liu"],"url":"https://arxiv.org/abs/2505.04193"}
{"created":"2025-05-08","title":"AutoPatch: Multi-Agent Framework for Patching Real-World CVE Vulnerabilities","abstract":"Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.","authors":["Minjae Seo","Wonwoo Choi","Myoungsung You","Seungwon Shin"],"url":"https://arxiv.org/abs/2505.04195"}
{"created":"2025-05-08","title":"A Large Language Model for Feasible and Diverse Population Synthesis","abstract":"Generating a synthetic population that is both feasible and diverse is crucial for ensuring the validity of downstream activity schedule simulation in activity-based models (ABMs). While deep generative models (DGMs), such as variational autoencoders and generative adversarial networks, have been applied to this task, they often struggle to balance the inclusion of rare but plausible combinations (i.e., sampling zeros) with the exclusion of implausible ones (i.e., structural zeros). To improve feasibility while maintaining diversity, we propose a fine-tuning method for large language models (LLMs) that explicitly controls the autoregressive generation process through topological orderings derived from a Bayesian Network (BN). Experimental results show that our hybrid LLM-BN approach outperforms both traditional DGMs and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically, our approach achieves approximately 95% feasibility, significantly higher than the ~80% observed in DGMs, while maintaining comparable diversity, making it well-suited for practical applications. Importantly, the method is based on a lightweight open-source LLM, enabling fine-tuning and inference on standard personal computing environments. This makes the approach cost-effective and scalable for large-scale applications, such as synthesizing populations in megacities, without relying on expensive infrastructure. By initiating the ABM pipeline with high-quality synthetic populations, our method improves overall simulation reliability and reduces downstream error propagation. The source code for these methods is available for research and practical application.","authors":["Sung Yoo Lim","Hyunsoo Yun","Prateek Bansal","Dong-Kyu Kim","Eui-Jin Kim"],"url":"https://arxiv.org/abs/2505.04196"}
{"created":"2025-05-08","title":"Estimating Causal Effects in Networks with Cluster-Based Bandits","abstract":"The gold standard for estimating causal effects is randomized controlled trial (RCT) or A/B testing where a random group of individuals from a population of interest are given treatment and the outcome is compared to a random group of individuals from the same population. However, A/B testing is challenging in the presence of interference, commonly occurring in social networks, where individuals can impact each others outcome. Moreover, A/B testing can incur a high performance loss when one of the treatment arms has a poor performance and the test continues to treat individuals with it. Therefore, it is important to design a strategy that can adapt over time and efficiently learn the total treatment effect in the network. We introduce two cluster-based multi-armed bandit (MAB) algorithms to gradually estimate the total treatment effect in a network while maximizing the expected reward by making a tradeoff between exploration and exploitation. We compare the performance of our MAB algorithms with a vanilla MAB algorithm that ignores clusters and the corresponding RCT methods on semi-synthetic data with simulated interference. The vanilla MAB algorithm shows higher reward-action ratio at the cost of higher treatment effect error due to undesired spillover. The cluster-based MAB algorithms show higher reward-action ratio compared to their corresponding RCT methods without sacrificing much accuracy in treatment effect estimation.","authors":["Ahmed Sayeed Faruk","Jason Sulskis","Elena Zheleva"],"url":"https://arxiv.org/abs/2505.04200"}
{"created":"2025-05-08","title":"SToLa: Self-Adaptive Touch-Language Framework with Tactile Commonsense Reasoning in Open-Ended Scenarios","abstract":"This paper explores the challenges of integrating tactile sensing into intelligent systems for multimodal reasoning, particularly in enabling commonsense reasoning about the open-ended physical world. We identify two key challenges: modality discrepancy, where existing large touch-language models often treat touch as a mere sub-modality of language, and open-ended tactile data scarcity, where current datasets lack the diversity, open-endness and complexity needed for reasoning. To overcome these challenges, we introduce SToLa, a Self-Adaptive Touch-Language framework. SToLa utilizes Mixture of Experts (MoE) to dynamically process, unify, and manage tactile and language modalities, capturing their unique characteristics. Crucially, we also present a comprehensive tactile commonsense reasoning dataset and benchmark featuring free-form questions and responses, 8 physical properties, 4 interactive characteristics, and diverse commonsense knowledge. Experiments show SToLa exhibits competitive performance compared to existing models on the PhysiCLeAR benchmark and self-constructed datasets, proving the effectiveness of the Mixture of Experts architecture in multimodal management and the performance advantages for open-scenario tactile commonsense reasoning tasks.","authors":["Ning Cheng","Jinan Xu","Jialing Chen","Wenjuan Han"],"url":"https://arxiv.org/abs/2505.04201"}
{"created":"2025-05-08","title":"ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition","abstract":"The art of instrument performance stands as a vivid manifestation of human creativity and emotion. Nonetheless, generating instrument performance motions is a highly challenging task, as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performer-instrument interaction. While existing works primarily focus on modeling partial body motions, we propose Expressive ceLlo performance motion Generation for Audio Rendition (ELGAR), a state-of-the-art diffusion-based framework for whole-body fine-grained instrument performance motion generation solely from audio. To emphasize the interactive nature of the instrument performance, we introduce Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL), which effectively guarantee the authenticity of the interplay. Moreover, to better evaluate whether the generated motions align with the semantic context of the music audio, we design novel metrics specifically for string instrument performance motion generation, including finger-contact distance, bow-string distance, and bowing score. Extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods. In addition, we put forward a motion generation dataset SPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated, ELGAR has shown great potential in generating instrument performance motions with complicated and fast interactions, which will promote further development in areas such as animation, music education, interactive art creation, etc.","authors":["Zhiping Qiu","Yitong Jin","Yuan Wang","Yi Shi","Chongwu Wang","Chao Tan","Xiaobing Li","Feng Yu","Tao Yu","Qionghai Dai"],"url":"https://arxiv.org/abs/2505.04203"}
{"created":"2025-05-08","title":"Cyber Security Data Science: Machine Learning Methods and their Performance on Imbalanced Datasets","abstract":"Cybersecurity has become essential worldwide and at all levels, concerning individuals, institutions, and governments. A basic principle in cybersecurity is to be always alert. Therefore, automation is imperative in processes where the volume of daily operations is large. Several cybersecurity applications can be addressed as binary classification problems, including anomaly detection, fraud detection, intrusion detection, spam detection, or malware detection. We present three experiments. In the first experiment, we evaluate single classifiers including Random Forests, Light Gradient Boosting Machine, eXtreme Gradient Boosting, Logistic Regression, Decision Tree, and Gradient Boosting Decision Tree. In the second experiment, we test different sampling techniques including over-sampling, under-sampling, Synthetic Minority Over-sampling Technique, and Self-Paced Ensembling. In the last experiment, we evaluate Self-Paced Ensembling and its number of base classifiers. We found that imbalance learning techniques had positive and negative effects, as reported in related studies. Thus, these techniques should be applied with caution. Besides, we found different best performers for each dataset. Therefore, we recommend testing single classifiers and imbalance learning techniques for each new dataset and application involving imbalanced datasets as is the case in several cyber security applications.","authors":["Mateo Lopez-Ledezma","Gissel Velarde"],"url":"https://arxiv.org/abs/2505.04204"}
{"created":"2025-05-08","title":"An Enhanced YOLOv8 Model for Real-Time and Accurate Pothole Detection and Measurement","abstract":"Potholes cause vehicle damage and traffic accidents, creating serious safety and economic problems. Therefore, early and accurate detection of potholes is crucial. Existing detection methods are usually only based on 2D RGB images and cannot accurately analyze the physical characteristics of potholes. In this paper, a publicly available dataset of RGB-D images (PothRGBD) is created and an improved YOLOv8-based model is proposed for both pothole detection and pothole physical features analysis. The Intel RealSense D415 depth camera was used to collect RGB and depth data from the road surfaces, resulting in a PothRGBD dataset of 1000 images. The data was labeled in YOLO format suitable for segmentation. A novel YOLO model is proposed based on the YOLOv8n-seg architecture, which is structurally improved with Dynamic Snake Convolution (DSConv), Simple Attention Module (SimAM) and Gaussian Error Linear Unit (GELU). The proposed model segmented potholes with irregular edge structure more accurately, and performed perimeter and depth measurements on depth maps with high accuracy. The standard YOLOv8n-seg model achieved 91.9% precision, 85.2% recall and 91.9% mAP@50. With the proposed model, the values increased to 93.7%, 90.4% and 93.8% respectively. Thus, an improvement of 1.96% in precision, 6.13% in recall and 2.07% in mAP was achieved. The proposed model performs pothole detection as well as perimeter and depth measurement with high accuracy and is suitable for real-time applications due to its low model complexity. In this way, a lightweight and effective model that can be used in deep learning-based intelligent transportation solutions has been acquired.","authors":["Mustafa Yurdakul","\\c{S}akir Tasdemir"],"url":"https://arxiv.org/abs/2505.04207"}
{"created":"2025-05-08","title":"To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase Relevance at eBay","abstract":"E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.","authors":["Soumik Dey","Hansi Wu","Binbin Li"],"url":"https://arxiv.org/abs/2505.04209"}
{"created":"2025-05-08","title":"Sick of being driven? - Prevalence and modulating factors of carsickness in the European population in context of automated driving","abstract":"As in automated driving the driver becomes a passenger, carsickness might reduce comfort for susceptible individuals. Insights in the prevalence of carsickness and its modulating factors are considered useful for the development of automated vehicles to mitigate or prevent its occurrence. An online survey was conducted with N = 3999 participants in Spain, Sweden, Poland, and Germany. 30% of participants reported to have already experienced carsickness as adult. The frequency of carsickness was modulated not only by demographic factors (country, gender, age), but also by frequency of being a passenger, type of non-driving related task, road type, and the seating position in car. Furthermore, the efficiency of applied countermeasures, temporal aspects of carsickness development, as well as the relation of carsickness with the acceptability of automated driving and the effect on subjective fitness to drive was investigated. The results are discussed with focus on automated driving.","authors":["Myriam Metzulat","Barbara Metz","Aaron Edelmann","Alexandra Neukum","Wilfried Kunde"],"url":"https://arxiv.org/abs/2505.04210"}
{"created":"2025-05-08","title":"CM1 - A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models","abstract":"The automatic extraction of key-value information from handwritten documents is a key challenge in document analysis. A reliable extraction is a prerequisite for the mass digitization efforts of many archives. Large Vision Language Models (LVLM) are a promising technology to tackle this problem especially in scenarios where little annotated training data is available. In this work, we present a novel dataset specifically designed to evaluate the few-shot capabilities of LVLMs. The CM1 documents are a historic collection of forms with handwritten entries created in Europe to administer the Care and Maintenance program after World War Two. The dataset establishes three benchmarks on extracting name and birthdate information and, furthermore, considers different training set sizes. We provide baseline results for two different LVLMs and compare performances to an established full-page extraction model. While the traditional full-page model achieves highly competitive performances, our experiments show that when only a few training samples are available the considered LVLMs benefit from their size and heavy pretraining and outperform the classical approach.","authors":["Fabian Wolf","Oliver T\\\"uselmann","Arthur Matei","Lukas Hennies","Christoph Rass","Gernot A. Fink"],"url":"https://arxiv.org/abs/2505.04214"}
{"created":"2025-05-08","title":"Random walks with resetting on hypergraph","abstract":"Hypergraph has been selected as a powerful candidate for characterizing higher-order networks and has received","authors":["Fei Ma","Xincheng Hu","Haobin Shi","Wei Pan","Ping Wang"],"url":"https://arxiv.org/abs/2505.04215"}
{"created":"2025-05-08","title":"FRAIN to Train: A Fast-and-Reliable Solution for Decentralized Federated Learning","abstract":"Federated learning (FL) enables collaborative model training across distributed clients while preserving data locality. Although FedAvg pioneered synchronous rounds for global model averaging, slower devices can delay collective progress. Asynchronous FL (e.g., FedAsync) addresses stragglers by continuously integrating client updates, yet naive implementations risk client drift due to non-IID data and stale contributions. Some Blockchain-based FL approaches (e.g., BRAIN) employ robust weighting or scoring of updates to resist malicious or misaligned proposals. However, performance drops can still persist under severe data heterogeneity or high staleness, and synchronization overhead has emerged as a new concern due to its aggregator-free architectures.","authors":["Sanghyeon Park","Soo-Mook Moon"],"url":"https://arxiv.org/abs/2505.04223"}
{"created":"2025-05-08","title":"Modeling of thin plate flexural vibrations by Partition of Unity Finite Element Method","abstract":"This paper presents a conforming thin plate bending element based on the Partition of Unity Finite Element Method (PUFEM), for the simulation of steady-state forced vibration. The issue of ensuring the continuity of displacement and slope between elements is addressed by the use of cubic Hermite-type Partition of Unity (PU) functions. With appropriate PU functions, the PUFEM allows the incorporation of the special enrichment functions into the finite elements to better cope with plate oscillations in a broad frequency band. The enrichment strategies consist of the sum of a power series up to a given order and a combination of progressive flexural wave solutions with polynomials. The applicability and the effectiveness of the PUFEM plate elements is first verified via the structural frequency response. Investigation is then carried out to analyze the role of polynomial enrichment orders and enriched plane wave distributions for achieving good computational performance in terms of accuracy and data reduction. Numerical results show that the PUFEM with high-order polynomials and hybrid wave-polynomial combinations can provide highly accurate prediction results by using reduced degrees of freedom and improved rate of convergence, as compared with the classical FEM.","authors":["Tong Zhou","Jean-Daniel Chazot","Emmanuel Perrey-Debain","Li Cheng"],"url":"https://arxiv.org/abs/2505.04227"}
{"created":"2025-05-08","title":"Low Resolution Next Best View for Robot Packing","abstract":"Automating the packing of objects with robots is a key challenge in industrial automation, where efficient object perception plays a fundamental role. This paper focuses on scenarios where precise 3D reconstruction is not required, prioritizing cost-effective and scalable solutions. The proposed Low-Resolution Next Best View (LR-NBV) algorithm leverages a utility function that balances pose redundancy and acquisition density, ensuring efficient object reconstruction. Experimental validation demonstrates that LR-NBV consistently outperforms standard NBV approaches, achieving comparable accuracy with significantly fewer poses. This method proves highly suitable for applications requiring efficiency, scalability, and adaptability without relying on high-precision sensing.","authors":["Giuseppe Fabio Preziosa","Chiara Castellano","Andrea Maria Zanchettin","Marco Faroni","Paolo Rocco"],"url":"https://arxiv.org/abs/2505.04228"}
{"created":"2025-05-08","title":"A Weak Supervision Learning Approach Towards an Equitable Parking Lot Occupancy Estimation","abstract":"The scarcity and high cost of labeled high-resolution imagery have long challenged remote sensing applications, particularly in low-income regions where high-resolution data are scarce. In this study, we propose a weak supervision framework that estimates parking lot occupancy using 3m resolution satellite imagery. By leveraging coarse temporal labels -- based on the assumption that parking lots of major supermarkets and hardware stores in Germany are typically full on Saturdays and empty on Sundays -- we train a pairwise comparison model that achieves an AUC of 0.92 on large parking lots. The proposed approach minimizes the reliance on expensive high-resolution images and holds promise for scalable urban mobility analysis. Moreover, the method can be adapted to assess transit patterns and resource allocation in vulnerable communities, providing a data-driven basis to improve the well-being of those most in need.","authors":["Theophilus Aidoo","Till Koebe","Akansh Maurya","Hewan Shrestha","Ingmar Weber"],"url":"https://arxiv.org/abs/2505.04229"}
{"created":"2025-05-08","title":"Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving in Smart Intersections","abstract":"Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \\textit{(i)} achieving failure rates below 0.03\\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \\textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps.","authors":["Taoyuan Yu","Kui Wang","Zongdian Li","Tao Yu","Kei Sakaguchi"],"url":"https://arxiv.org/abs/2505.04231"}
{"created":"2025-05-08","title":"Binary Reconstruction Codes for Correcting One Deletion and One Substitution","abstract":"In this paper, we investigate binary reconstruction codes capable of correcting one deletion and one substitution. We define the \\emph{single-deletion single-substitution ball} function $ \\mathcal{B} $ as a mapping from a sequence to the set of sequences that can be derived from it by performing one deletion and one substitution. A binary \\emph{$(n,N;\\mathcal{B})$-reconstruction code} is defined as a collection of binary sequences of length $ n $ such that the intersection size between the single-deletion single-substitution balls of any two distinct codewords is strictly less than $ N $. This property ensures that each codeword can be uniquely reconstructed from $ N $ distinct elements in its single-deletion single-substitution ball. Our main contribution is to demonstrate that when $ N $ is set to $ 4n - 8 $, $ 3n - 4 $, $2n+9$, $ n+21 $, $31$, and $7$, the redundancy of binary $(n,N;\\mathcal{B})$-reconstruction codes can be $0$, $1$, $2$, $ \\log\\log n + 3 $, $\\log n + 1 $, and $ 3\\log n + 4 $, respectively, where the logarithm is on base two.","authors":["Yuling Li","Yubo Sun","Gennian Ge"],"url":"https://arxiv.org/abs/2505.04232"}
{"created":"2025-05-08","title":"Technology prediction of a 3D model using Neural Network","abstract":"Accurate estimation of production times is critical for effective manufacturing scheduling, yet traditional methods relying on expert analysis or historical data often fall short in dynamic or customized production environments. This paper introduces a data-driven approach that predicts manufacturing steps and their durations directly from a product's 3D model. By rendering the model into multiple 2D images and leveraging a neural network inspired by the Generative Query Network, the method learns to map geometric features into time estimates for predefined production steps enabling scalable, adaptive, and precise process planning across varied product types.","authors":["Grzegorz Miebs","Rafa{\\l} A. Bachorz"],"url":"https://arxiv.org/abs/2505.04241"}
{"created":"2025-05-08","title":"Self-Calibrating Position Measurements: Applied to Imperfect Hall Sensors","abstract":"Linear Hall sensors are a cost-effective alternative to optical encoders for measuring the rotor positions of actuators, with the main challenge being that they exhibit position-dependent inaccuracies resulting from manufacturing tolerances. This paper develops a data-driven calibration procedure for linear analog Hall sensors that enables accurate online estimates of the rotor angle without requiring expensive external encoders. The approach combines closed-loop data collection with nonlinear identification to obtain an accurate model of the sensor inaccuracies, which is subsequently used for online compensation. Simulation results show that when the flux density model structure is known, measurement errors are reduced to the sensor noise floor, and experiments on an industrial setup demonstrate a factor of 2.6 reduction in the root-mean-square measurement error. These results confirm that Hall sensor inaccuracies can be calibrated even when no external encoder is available, improving their practical applicability.","authors":["Max van Meer","Marijn van Noije","Koen Tiels","Enzo Evers","Lennart Blanken","Gert Witvoet","Tom Oomen"],"url":"https://arxiv.org/abs/2505.04245"}
{"created":"2025-05-08","title":"Learning-Based Approaches for Job Shop Scheduling Problems: A Review","abstract":"Job Shop Scheduling (JSS) is one of the most studied combinatorial optimization problems. It involves scheduling a set of jobs with predefined processing constraints on a set of machines to achieve a desired objective, such as minimizing makespan, tardiness, or flowtime. Since it introduction, JSS has become an attractive research area. Many approaches have been successfully used to address this problem, including exact methods, heuristics, and meta-heuristics. Furthermore, various learning-based approaches have been proposed to solve the JSS problem. However, these approaches are still limited when compared to the more established methods. This paper summarizes and evaluates the most important works in the literature on machine learning approaches for the JSSP. We present models, analyze their benefits and limitations, and propose future research directions.","authors":["Karima Rihane","Adel Dabah","Abdelhakim AitZai"],"url":"https://arxiv.org/abs/2505.04246"}
{"created":"2025-05-08","title":"A block preconditioner for thermo-poromechanics with frictional deformation of fractures","abstract":"The numerical modeling of fracture contact thermo-poromechanics is crucial for advancing subsurface engineering applications, including CO2 sequestration, production of geo-energy resources, energy storage and wastewater disposal operations. Accurately modeling this problem presents substantial challenges due to the complex physics involved in strongly coupled thermo-poromechanical processes and the frictional contact mechanics of fractures. To resolve process couplings in the resulting mathematical model, it is common to apply fully implicit time stepping. This necessitates the use of an iterative linear solver to run the model. The solver's efficiency primarily depends on a robust preconditioner, which is particularly challenging to develop because it must handle the mutual couplings between linearized contact mechanics and energy, momentum, and mass balance. In this work, we introduce a preconditioner for the problem based on the nested approximations of Schur complements. To decouple the momentum balance, we utilize the fixed-stress approximation, extended to account for both the porous media and fracture subdomains. The singularity of the contact mechanics submatrix is resolved by a linear transformation. Two variations of the algorithm are proposed to address the coupled mass and energy balance submatrix: either the Constrained Pressure Residual or the System-AMG approach. The preconditioner is evaluated through numerical experiments of fluid injection into fractured porous media, which causes thermal contraction and subsequent sliding and opening of fractures. The experiments show that the preconditioner performs robustly for a wide range of simulation regimes governed by various fracture states, friction coefficients and Peclet number. The grid refinement experiments demonstrate that the preconditioner scales well in terms of GMRES iterations, in both two and three dimensions.","authors":["Yury Zabegaev","Inga Berre","Eirik Keilegavlen"],"url":"https://arxiv.org/abs/2505.04247"}
{"created":"2025-05-08","title":"On the Vulnerability of Underwater Magnetic Induction Communication","abstract":"Typical magnetic induction (MI) communication is commonly considered a secure underwater wireless communication (UWC) technology due to its non-audible and non-visible nature compared to acoustic and optical UWC technologies. However, vulnerabilities in communication systems inevitably exist and may lead to different types of attacks. In this paper, we investigate the eavesdropping attack in underwater MI communication to quantitatively measure the system's vulnerability under this attack. We consider different potential eavesdropping configuration setups based on the positions and orientations of the eavesdropper node to investigate how they impact the received voltage and secrecy at the legitimate receiver node. To this end, we develop finite-element-method-based simulation models for each configuration in an underwater environment and evaluate the received voltage and the secrecy capacity against different system parameters such as magnetic flux, magnetic flux density, distance, and orientation sensitivity. Furthermore, we construct an experimental setup within a laboratory environment to replicate the simulation experiments. Both simulation and lab experimental confirm the susceptibility of underwater MI communication to eavesdropping attacks. However, this vulnerability is highly dependent on the position and orientation of the coil between the eavesdropper and the legitimate transmitter. On the positive side, we also observe a unique behavior in the received coil reception that might be used to detect malicious node activities in the vicinity, which might lead to a potential security mechanism against eavesdropping attacks.","authors":["Muhammad Muzzammil","Waqas Aman","Irfan Ullah","Shang Zhigang","Saif Al-Kuwari","Zhou Tian","Marwa Qaraqe"],"url":"https://arxiv.org/abs/2505.04249"}
{"created":"2025-05-08","title":"Facilitating Trustworthy Human-Agent Collaboration in LLM-based Multi-Agent System oriented Software Engineering","abstract":"Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular autonomous agents. This holds true within the field of software engineering (SE) as well. The state-of-the-art research on MAS within SE focuses on integrating LLMs at the core of autonomous agents to create LLM-based multi-agent autonomous (LMA) systems. However, the introduction of LMA systems into SE brings a plethora of challenges. One of the major challenges is the strategic allocation of tasks between humans and the LMA system in a trustworthy manner. To address this challenge, a RACI-based framework is proposed in this work in progress article, along with implementation guidelines and an example implementation of the framework. The proposed framework can facilitate efficient collaboration, ensure accountability, and mitigate potential risks associated with LLM-driven automation while aligning with the Trustworthy AI guidelines. The future steps for this work delineating the planned empirical validation method are also presented.","authors":["Krishna Ronanki"],"url":"https://arxiv.org/abs/2505.04251"}
{"created":"2025-05-08","title":"LLM-Independent Adaptive RAG: Let the Question Speak for Itself","abstract":"Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.","authors":["Maria Marina","Nikolay Ivanov","Sergey Pletenev","Mikhail Salnikov","Daria Galimzianova","Nikita Krayko","Vasily Konovalov","Alexander Panchenko","Viktor Moskvoretskii"],"url":"https://arxiv.org/abs/2505.04253"}
{"created":"2025-05-08","title":"CompileAgent: Automated Real-World Repo-Level Compilation with Tool-Integrated LLM-based Agent System","abstract":"With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects.","authors":["Li Hu","Guoqiang Chen","Xiuwei Shang","Shaoyin Cheng","Benlong Wu","Gangyang Li","Xu Zhu","Weiming Zhang","Nenghai Yu"],"url":"https://arxiv.org/abs/2505.04254"}
{"created":"2025-05-08","title":"Automating Box Folding: Sequence Extraction and Ranking Methodologies","abstract":"Box folding represents a crucial challenge for automated packaging systems. This work bridges the gap between existing methods for folding sequence extraction and approaches focused on the adaptability of automated systems to specific box types. An innovative method is proposed to identify and rank folding sequences, enabling the transformation of a box from an initial state to a desired final configuration. The system evaluates and ranks these sequences based on their feasibility and compatibility with available hardware, providing recommendations for real-world implementations. Finally, an illustrative use case is presented, where a robot performs the folding of a box.","authors":["Giuseppe Fabio Preziosa","Davide Ferloni","Andrea Maria Zanchettin","Marco Faroni","Paolo Rocco"],"url":"https://arxiv.org/abs/2505.04257"}
{"created":"2025-05-08","title":"RGB-Event Fusion with Self-Attention for Collision Prediction","abstract":"Ensuring robust and real-time obstacle avoidance is critical for the safe operation of autonomous robots in dynamic, real-world environments. This paper proposes a neural network framework for predicting the time and collision position of an unmanned aerial vehicle with a dynamic object, using RGB and event-based vision sensors. The proposed architecture consists of two separate encoder branches, one for each modality, followed by fusion by self-attention to improve prediction accuracy. To facilitate benchmarking, we leverage the ABCD [8] dataset collected that enables detailed comparisons of single-modality and fusion-based approaches. At the same prediction throughput of 50Hz, the experimental results show that the fusion-based model offers an improvement in prediction accuracy over single-modality approaches of 1% on average and 10% for distances beyond 0.5m, but comes at the cost of +71% in memory and + 105% in FLOPs. Notably, the event-based model outperforms the RGB model by 4% for position and 26% for time error at a similar computational cost, making it a competitive alternative. Additionally, we evaluate quantized versions of the event-based models, applying 1- to 8-bit quantization to assess the trade-offs between predictive performance and computational efficiency. These findings highlight the trade-offs of multi-modal perception using RGB and event-based cameras in robotic applications.","authors":["Pietro Bonazzi","Christian Vogt","Michael Jost","Haotong Qin","Lyes Khacef","Federico Paredes-Valles","Michele Magno"],"url":"https://arxiv.org/abs/2505.04258"}
{"created":"2025-05-08","title":"Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering","abstract":"As large language models (LLMs) improve in their capacity to serve as personal AI assistants, their ability to output uniquely tailored, personalized responses that align with the soft preferences of their users is essential for enhancing user satisfaction and retention. However, untrained lay users have poor prompt specification abilities and often struggle with conveying their latent preferences to AI assistants. To address this, we leverage activation steering to guide LLMs to align with interpretable preference dimensions during inference. In contrast to memory-based personalization methods that require longer user history, steering is extremely lightweight and can be easily controlled by the user via an linear strength factor. We embed steering into three different interactive chatbot interfaces and conduct a within-subjects user study (n=14) to investigate how end users prefer to personalize their conversations. The results demonstrate the effectiveness of preference-based steering for aligning real-world conversations with hidden user preferences, and highlight further insights on how diverse values around control, usability, and transparency lead users to prefer different interfaces.","authors":["Jessica Y. Bo","Tianyu Xu","Ishan Chatterjee","Katrina Passarella-Ward","Achin Kulshrestha","D Shin"],"url":"https://arxiv.org/abs/2505.04260"}
{"created":"2025-05-08","title":"Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting","abstract":"Score Distillation Sampling (SDS) leverages pretrained 2D diffusion models to advance text-to-3D generation but neglects multi-view correlations, being prone to geometric inconsistencies and multi-face artifacts in the generated 3D content. In this work, we propose Coupled Score Distillation (CSD), a framework that couples multi-view joint distribution priors to ensure geometrically consistent 3D generation while enabling the stable and direct optimization of 3D Gaussian Splatting. Specifically, by reformulating the optimization as a multi-view joint optimization problem, we derive an effective optimization rule that effectively couples multi-view priors to guide optimization across different viewpoints while preserving the diversity of generated 3D assets. Additionally, we propose a framework that directly optimizes 3D Gaussian Splatting (3D-GS) with random initialization to generate geometrically consistent 3D content. We further employ a deformable tetrahedral grid, initialized from 3D-GS and refined through CSD, to produce high-quality, refined meshes. Quantitative and qualitative experimental results demonstrate the efficiency and competitive quality of our approach.","authors":["Feng Yang","Wenliang Qian","Wangmeng Zuo","Hui Li"],"url":"https://arxiv.org/abs/2505.04262"}
{"created":"2025-05-08","title":"Physics-Informed DeepONets for drift-diffusion on metric graphs: simulation and parameter identification","abstract":"We develop a novel physics informed deep learning approach for solving nonlinear drift-diffusion equations on metric graphs. These models represent an important model class with a large number of applications in areas ranging from transport in biological cells to the motion of human crowds. While traditional numerical schemes require a large amount of tailoring, especially in the case of model design or parameter identification problems, physics informed deep operator networks (DeepONet) have emerged as a versatile tool for the solution of partial differential equations with the particular advantage that they easily incorporate parameter identification questions. We here present an approach where we first learn three DeepONet models for representative inflow, inner and outflow edges, resp., and then subsequently couple these models for the solution of the drift-diffusion metric graph problem by relying on an edge-based domain decomposition approach. We illustrate that our framework is applicable for the accurate evaluation of graph-coupled physics models and is well suited for solving optimization or inverse problems on these coupled networks.","authors":["Jan Blechschmidt","Tom-Christian Riemer","Max Winkler","Martin Stoll","Jan-F. Pietschmann"],"url":"https://arxiv.org/abs/2505.04263"}
{"created":"2025-05-08","title":"Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper","abstract":"This, with the ever-increasing sophistication of cyberwar, calls for novel solutions. In this regard, Large Language Models (LLMs) have emerged as a highly promising tool for defensive and offensive cybersecurity-related strategies. While existing literature has focused much on the defensive use of LLMs, when it comes to their offensive utilization, very little has been reported-namely, concerning Vulnerability Assessment (VA) report validation. Consequentially, this paper tries to fill that gap by investigating the capabilities of LLMs in automating and improving the validation process of the report of the VA. From the critical review of the related literature, this paper hereby proposes a new approach to using the LLMs in the automation of the analysis and within the validation process of the report of the VA that could potentially reduce the number of false positives and generally enhance efficiency. These results are promising for LLM automatization for improving validation on reports coming from VA in order to improve accuracy while reducing human effort and security postures. The contribution of this paper provides further evidence about the offensive and defensive LLM capabilities and therefor helps in devising more appropriate cybersecurity strategies and tools accordingly.","authors":["Abdulrahman S Almuhaidib","Azlan Mohd Zain","Zalmiyah Zakaria","Izyan Izzati Kamsani","Abdulaziz S Almuhaidib"],"url":"https://arxiv.org/abs/2505.04265"}
{"created":"2025-05-08","title":"Accelerating Triangle Counting with Real Processing-in-Memory Systems","abstract":"Triangle Counting (TC) is a procedure that involves enumerating the number of triangles within a graph. It has important applications in numerous fields, such as social or biological network analysis and network security. TC is a memory-bound workload that does not scale efficiently in conventional processor-centric systems due to several memory accesses across large memory regions and low data reuse. However, recent Processing-in-Memory (PIM) architectures present a promising solution to alleviate these bottlenecks. Our work presents the first TC algorithm that leverages the capabilities of the UPMEM system, the first commercially available PIM architecture, while at the same time addressing its limitations. We use a vertex coloring technique to avoid expensive communication between PIM cores and employ reservoir sampling to address the limited amount of memory available in the PIM cores' DRAM banks. In addition, our work makes use of the Misra-Gries summary to speed up counting triangles on graphs with high-degree nodes and uniform sampling of the graph edges for quicker approximate results. Our PIM implementation surpasses state-of-the-art CPU-based TC implementations when processing dynamic graphs in Coordinate List format, showcasing the effectiveness of the UPMEM architecture in addressing TC's memory-bound challenges.","authors":["Lorenzo Asquini","Manos Frouzakis","Juan G\\'omez-Luna","Mohammad Sadrosadati","Onur Mutlu","Francesco Silvestri"],"url":"https://arxiv.org/abs/2505.04269"}
{"created":"2025-05-08","title":"Object-Shot Enhanced Grounding Network for Egocentric Video","abstract":"Egocentric video grounding is a crucial task for embodied intelligence applications, distinct from exocentric video moment localization. Existing methods primarily focus on the distributional differences between egocentric and exocentric videos but often neglect key characteristics of egocentric videos and the fine-grained information emphasized by question-type queries. To address these limitations, we propose OSGNet, an Object-Shot enhanced Grounding Network for egocentric video. Specifically, we extract object information from videos to enrich video representation, particularly for objects highlighted in the textual query but not directly captured in the video features. Additionally, we analyze the frequent shot movements inherent to egocentric videos, leveraging these features to extract the wearer's attention information, which enhances the model's ability to perform modality alignment. Experiments conducted on three datasets demonstrate that OSGNet achieves state-of-the-art performance, validating the effectiveness of our approach. Our code can be found at https://github.com/Yisen-Feng/OSGNet.","authors":["Yisen Feng","Haoyu Zhang","Meng Liu","Weili Guan","Liqiang Nie"],"url":"https://arxiv.org/abs/2505.04270"}
{"created":"2025-05-08","title":"Joint Task Offloading and Channel Allocation in Spatial-Temporal Dynamic for MEC Networks","abstract":"Computation offloading and resource allocation are critical in mobile edge computing (MEC) systems to handle the massive and complex requirements of applications restricted by limited resources. In a multi-user multi-server MEC network, the mobility of terminals causes computing requests to be dynamically distributed in space. At the same time, the non-negligible dependencies among tasks in some specific applications impose temporal correlation constraints on the solution as well, leading the time-adjacent tasks to experience varying resource availability and competition from parallel counterparts. To address such dynamic spatial-temporal characteristics as a challenge in the allocation of communication and computation resources, we formulate a long-term delay-energy trade-off cost minimization problem in the view of jointly optimizing task offloading and resource allocation. We begin by designing a priority evaluation scheme to decouple task dependencies and then develop a grouped Knapsack problem for channel allocation considering the current data load and channel status. Afterward, in order to meet the rapid response needs of MEC systems, we exploit the double duel deep Q network (D3QN) to make offloading decisions and integrate channel allocation results into the reward as part of the dynamic environment feedback in D3QN, constituting the joint optimization of task offloading and channel allocation. Finally, comprehensive simulations demonstrate the performance of the proposed algorithm in the delay-energy trade-off cost and its adaptability for various applications.","authors":["Tianyi Shi","Tiankui Zhang","Jonathan Loo","Rong Huang","Yapeng Wang"],"url":"https://arxiv.org/abs/2505.04272"}
{"created":"2025-05-08","title":"With Friends Like These, Who Needs Explanations? Evaluating User Understanding of Group Recommendations","abstract":"Group Recommender Systems (GRS) employing social choice-based aggregation strategies have previously been explored in terms of perceived consensus, fairness, and satisfaction. At the same time, the impact of textual explanations has been examined, but the results suggest a low effectiveness of these explanations. However, user understanding remains fairly unexplored, even if it can contribute positively to transparent GRS. This is particularly interesting to study in more complex or potentially unfair scenarios when user preferences diverge, such as in a minority scenario (where group members have similar preferences, except for a single member in a minority position). In this paper, we analyzed the impact of different types of explanations on user understanding of group recommendations. We present a randomized controlled trial (n = 271) using two between-subject factors: (i) the aggregation strategy (additive, least misery, and approval voting), and (ii) the modality of explanation (no explanation, textual explanation, or multimodal explanation). We measured both subjective (self-perceived by the user) and objective understanding (performance on model simulation, counterfactuals and error detection). In line with recent findings on explanations for machine learning models, our results indicate that more detailed explanations, whether textual or multimodal, did not increase subjective or objective understanding. However, we did find a significant effect of aggregation strategies on both subjective and objective understanding. These results imply that when constructing GRS, practitioners need to consider that the choice of aggregation strategy can influence the understanding of users. Post-hoc analysis also suggests that there is value in analyzing performance on different tasks, rather than through a single aggregated metric of understanding.","authors":["Cedric Waterschoot","Raciel Yera Toledo","Nava Tintarev","Francesco Barile"],"url":"https://arxiv.org/abs/2505.04273"}
{"created":"2025-05-08","title":"HDiffTG: A Lightweight Hybrid Diffusion-Transformer-GCN Architecture for 3D Human Pose Estimation","abstract":"We propose HDiffTG, a novel 3D Human Pose Estimation (3DHPE) method that integrates Transformer, Graph Convolutional Network (GCN), and diffusion model into a unified framework. HDiffTG leverages the strengths of these techniques to significantly improve pose estimation accuracy and robustness while maintaining a lightweight design. The Transformer captures global spatiotemporal dependencies, the GCN models local skeletal structures, and the diffusion model provides step-by-step optimization for fine-tuning, achieving a complementary balance between global and local features. This integration enhances the model's ability to handle pose estimation under occlusions and in complex scenarios. Furthermore, we introduce lightweight optimizations to the integrated model and refine the objective function design to reduce computational overhead without compromising performance. Evaluation results on the Human3.6M and MPI-INF-3DHP datasets demonstrate that HDiffTG achieves state-of-the-art (SOTA) performance on the MPI-INF-3DHP dataset while excelling in both accuracy and computational efficiency. Additionally, the model exhibits exceptional robustness in noisy and occluded environments. Source codes and models are available at https://github.com/CirceJie/HDiffTG","authors":["Yajie Fu","Chaorui Huang","Junwei Li","Hui Kong","Yibin Tian","Huakang Li","Zhiyuan Zhang"],"url":"https://arxiv.org/abs/2505.04276"}
{"created":"2025-05-08","title":"Revolutionizing Newcomers' Onboarding Process in OSS Communities: The Future AI Mentor","abstract":"Onboarding newcomers is vital for the sustainability of open-source software (OSS) projects. To lower barriers and increase engagement, OSS projects have dedicated experts who provide guidance for newcomers. However, timely responses are often hindered by experts' busy schedules. The recent rapid advancements of AI in software engineering have brought opportunities to leverage AI as a substitute for expert mentoring. However, the potential role of AI as a comprehensive mentor throughout the entire onboarding process remains unexplored. To identify design strategies of this ``AI mentor'', we applied Design Fiction as a participatory method with 19 OSS newcomers. We investigated their current onboarding experience and elicited 32 design strategies for future AI mentor. Participants envisioned AI mentor being integrated into OSS platforms like GitHub, where it could offer assistance to newcomers, such as ``recommending projects based on personalized requirements'' and ``assessing and categorizing project issues by difficulty''. We also collected participants' perceptions of a prototype, named ``OSSerCopilot'', that implemented the envisioned strategies. They found the interface useful and user-friendly, showing a willingness to use it in the future, which suggests the design strategies are effective. Finally, in order to identify the gaps between our design strategies and current research, we conducted a comprehensive literature review, evaluating the extent of existing research support for this concept. We find that research is relatively scarce in certain areas where newcomers highly anticipate AI mentor assistance, such as ``discovering an interested project''. Our study has the potential to revolutionize the current newcomer-expert mentorship and provides valuable insights for researchers and tool designers aiming to develop and enhance AI mentor systems.","authors":["Xin Tan","Xiao Long","Yinghao Zhu","Lin Shi","Xiaoli Lian","Li Zhang"],"url":"https://arxiv.org/abs/2505.04277"}
{"created":"2025-05-08","title":"Non-stationary Diffusion For Probabilistic Time Series Forecasting","abstract":"Due to the dynamics of underlying physics and external influences, the uncertainty of time series often varies over time. However, existing Denoising Diffusion Probabilistic Models (DDPMs) often fail to capture this non-stationary nature, constrained by their constant variance assumption from the additive noise model (ANM). In this paper, we innovatively utilize the Location-Scale Noise Model (LSNM) to relax the fixed uncertainty assumption of ANM. A diffusion-based probabilistic forecasting framework, termed Non-stationary Diffusion (NsDiff), is designed based on LSNM that is capable of modeling the changing pattern of uncertainty. Specifically, NsDiff combines a denoising diffusion-based conditional generative model with a pre-trained conditional mean and variance estimator, enabling adaptive endpoint distribution modeling. Furthermore, we propose an uncertainty-aware noise schedule, which dynamically adjusts the noise levels to accurately reflect the data uncertainty at each step and integrates the time-varying variances into the diffusion process. Extensive experiments conducted on nine real-world and synthetic datasets demonstrate the superior performance of NsDiff compared to existing approaches. Code is available at https://github.com/wwy155/NsDiff.","authors":["Weiwei Ye","Zhuopeng Xu","Ning Gui"],"url":"https://arxiv.org/abs/2505.04278"}
{"created":"2025-05-08","title":"TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement","abstract":"This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras. A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment. To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications. Source codes and models are available at https://github.com/CircccleK/TS-Diff","authors":["Yi Li","Zhiyuan Zhang","Jiangnan Xia","Jianghan Cheng","Qilong Wu","Junwei Li","Yibin Tian","Hui Kong"],"url":"https://arxiv.org/abs/2505.04281"}
{"created":"2025-05-08","title":"GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance","abstract":"In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available.","authors":["Sofia Jamil","Aryan Dabad","Bollampalli Areen Reddy","Sriparna Saha","Rajiv Misra","Adil A. Shakur"],"url":"https://arxiv.org/abs/2505.04284"}
{"created":"2025-05-08","title":"A hybridizable discontinuous Galerkin method with transmission variables for time-harmonic electromagnetic problems","abstract":"The CHDG method is a hybridizable discontinuous Galerkin (HDG) finite element method suitable for the iterative solution of time-harmonic wave propagation problems. Hybrid unknowns corresponding to transmission variables are introduced at the element interfaces and the physical unknowns inside the elements are eliminated, resulting in a hybridized system with favorable properties for fast iterative solution. In this paper, we extend the CHDG method, initially studied for the Helmholtz equation, to the time-harmonic Maxwell equations. We prove that the local problems stemming from hybridization are well-posed and that the fixed-point iteration naturally associated to the hybridized system is contractive. We propose a 3D implementation with a discrete scheme based on nodal basis functions. The resulting solver and different iterative strategies are studied with several numerical examples using a high-performance parallel C++ code.","authors":["Ari E. Rappaport","Th\\'eophile Chaumont-Frelet","Axel Modave"],"url":"https://arxiv.org/abs/2505.04288"}
{"created":"2025-05-08","title":"From Incidents to Insights: Patterns of Responsibility following AI Harms","abstract":"The AI Incident Database was inspired by aviation safety databases, which enable collective learning from failures to prevent future incidents. The database documents hundreds of AI failures, collected from the news and media. However, criticism highlights that the AIID's reliance on media reporting limits its utility for learning about implementation failures. In this paper, we accept that the AIID falls short in its original mission, but argue that by looking beyond technically-focused learning, the dataset can provide new, highly valuable insights: specifically, opportunities to learn about patterns between developers, deployers, victims, wider society, and law-makers that emerge after AI failures. Through a three-tier mixed-methods analysis of 962 incidents and 4,743 related reports from the AIID, we examine patterns across incidents, focusing on cases with public responses tagged in the database. We identify 'typical' incidents found in the AIID, from Tesla crashes to deepfake scams.","authors":["Isabel Richards","Claire Benn","Miri Zilka"],"url":"https://arxiv.org/abs/2505.04291"}
{"created":"2025-05-08","title":"Massive MIMO: Instantaneous versus Statistical CSI-Based Power Allocation","abstract":"The deployment of instantaneous CSI-based power control schemes necessitates computationally intensive signal processing operations, requiring substantial resources to handle real-time CSI updates and the associated overhead. Conversely, statistical CSIbased schemes enable efficient implementation of advanced power allocation algorithms within large-scale massive MIMO (mMIMO) systems, where the algorithms are updated much less frequently. Nevertheless, these schemes may deviate from optimal results in certain practical mMIMO configurations, necessitating the adoption of instantaneous CSI-based schemes. In addition, they may be limited in practical implementation where instantaneous CSI-based resource allocation and management schemes are widely adopted. This lecture provides a comprehensive comparison between the statistical CSI-based power allocation and instantaneous CSI-based power allocation designs for mMIMO systems from performance, complexity, and practical implementation aspects.","authors":["Zahra Mobini","Hien Quoc Ngo"],"url":"https://arxiv.org/abs/2505.04294"}
{"created":"2025-05-08","title":"PPO-ACT: Proximal Policy Optimization with Adversarial Curriculum Transfer for Spatial Public Goods Games","abstract":"This study investigates cooperation evolution mechanisms in the spatial public goods game. A novel deep reinforcement learning framework, Proximal Policy Optimization with Adversarial Curriculum Transfer (PPO-ACT), is proposed to model agent strategy optimization in dynamic environments. Traditional evolutionary game models frequently exhibit limitations in modeling long-term decision-making processes. Deep reinforcement learning effectively addresses this limitation by bridging policy gradient methods with evolutionary game theory. Our study pioneers the application of proximal policy optimization's continuous strategy optimization capability to public goods games through a two-stage adversarial curriculum transfer training paradigm. The experimental results show that PPO-ACT performs better in critical enhancement factor regimes. Compared to conventional standard proximal policy optimization methods, Q-learning and Fermi update rules, achieve earlier cooperation phase transitions and maintain stable cooperative equilibria. This framework exhibits better robustness when handling challenging scenarios like all-defector initial conditions. Systematic comparisons reveal the unique advantage of policy gradient methods in population-scale cooperation, i.e., achieving spatiotemporal payoff coordination through value function propagation. Our work provides a new computational framework for studying cooperation emergence in complex systems, algorithmically validating the punishment promotes cooperation hypothesis while offering methodological insights for multi-agent system strategy design.","authors":["Zhaoqilin Yang","Chanchan Li","Xin Wang","Youliang Tian"],"url":"https://arxiv.org/abs/2505.04302"}
{"created":"2025-05-08","title":"MoDE: Mixture of Diffusion Experts for Any Occluded Face Recognition","abstract":"With the continuous impact of epidemics, people have become accustomed to wearing masks. However, most current occluded face recognition (OFR) algorithms lack prior knowledge of occlusions, resulting in poor performance when dealing with occluded faces of varying types and severity in reality. Recognizing occluded faces is still a significant challenge, which greatly affects the convenience of people's daily lives. In this paper, we propose an identity-gated mixture of diffusion experts (MoDE) for OFR. Each diffusion-based generative expert estimates one possible complete image for occluded faces. Considering the random sampling process of the diffusion model, which introduces inevitable differences and variations between the inpainted faces and the real ones. To ensemble effective information from multi-reconstructed faces, we introduce an identity-gating network to evaluate the contribution of each reconstructed face to the identity and adaptively integrate the predictions in the decision space. Moreover, our MoDE is a plug-and-play module for most existing face recognition models. Extensive experiments on three public face datasets and two datasets in the wild validate our advanced performance for various occlusions in comparison with the competing methods.","authors":["Qiannan Fan","Zhuoyang Li","Jitong Li","Chenyang Cao"],"url":"https://arxiv.org/abs/2505.04306"}
{"created":"2025-05-08","title":"Tracing Vulnerability Propagation Across Open Source Software Ecosystems","abstract":"The paper presents a traceability analysis of how over 84 thousand vulnerabilities have propagated across 28 open source software ecosystems. According to the results, the propagation sequences have been complex in general, although GitHub, Debian, and Ubuntu stand out. Furthermore, the associated propagation delays have been lengthy, and these do not correlate well with the number of ecosystems involved in the associated sequences. Nor does the presence or absence of particularly ecosystems in the sequences yield clear, interpretable patterns. With these results, the paper contributes to the overlapping knowledge bases about software ecosystems, traceability, and vulnerabilities.","authors":["Jukka Ruohonen","Qusai Ramadan"],"url":"https://arxiv.org/abs/2505.04307"}
{"created":"2025-05-08","title":"Guardians of the Web: The Evolution and Future of Website Information Security","abstract":"Website information security has become a critical concern in the digital age. This article explores the evolution of website information security, examining its historical development, current practices, and future directions. The early beginnings from the 1960s to the 1980s laid the groundwork for modern cybersecurity, with the development of ARPANET, TCP/IP, public-key cryptography, and the first antivirus programs. The 1990s marked a transformative era, driven by the commercialization of the Internet and the emergence of web-based services. As the Internet grew, so did the range and sophistication of cyber threats, leading to advancements in security technologies such as the Secure Sockets Layer (SSL) protocol, password protection, and firewalls. Current practices in website information security involve a multi-layered approach, including encryption, secure coding practices, regular security audits, and user education. The future of website information security is expected to be shaped by emerging technologies such as artificial intelligence, blockchain, and quantum computing, as well as the increasing importance of international cooperation and standardization efforts. As cyber threats continue to evolve, ongoing research and innovation in website information security will be essential to protect sensitive information and maintain trust in the digital world.","authors":["Md Saiful Islam","Li Xiangdong"],"url":"https://arxiv.org/abs/2505.04308"}
{"created":"2025-05-08","title":"Integrating Large Citation Datasets","abstract":"This paper explores methods for building a comprehensive citation graph using big data techniques to evaluate scientific impact more accurately. Traditional citation metrics have limitations, and this work investigates merging large citation datasets to create a more accurate picture. Challenges of big data, like inconsistent data formats and lack of unique identifiers, are addressed through deduplication efforts, resulting in a streamlined and reliable merged dataset with over 119 million records and 1.4 billion citations. We demonstrate that merging large citation datasets builds a more accurate citation graph facilitating a more robust evaluation of scientific impact.","authors":["Inci Yueksel-Erguen","Ida Litzel","Hanqiu Peng"],"url":"https://arxiv.org/abs/2505.04309"}
{"created":"2025-05-08","title":"Flow Models for Unbounded and Geometry-Aware Distributional Reinforcement Learning","abstract":"We introduce a new architecture for Distributional Reinforcement Learning (DistRL) that models return distributions using normalizing flows. This approach enables flexible, unbounded support for return distributions, in contrast to categorical approaches like C51 that rely on fixed or bounded representations. It also offers richer modeling capacity to capture multi-modality, skewness, and tail behavior than quantile based approaches. Our method is significantly more parameter-efficient than categorical approaches. Standard metrics used to train existing models like KL divergence or Wasserstein distance either are scale insensitive or have biased sample gradients, especially when return supports do not overlap. To address this, we propose a novel surrogate for the Cram\\`er distance, that is geometry-aware and computable directly from the return distribution's PDF, avoiding the costly CDF computation. We test our model on the ATARI-5 sub-benchmark and show that our approach outperforms PDF based models while remaining competitive with quantile based methods.","authors":["Simo Alami C.","Rim Kaddah","Jesse Read","Marie-Paule Cani"],"url":"https://arxiv.org/abs/2505.04310"}
{"created":"2025-05-08","title":"How the Misuse of a Dataset Harmed Semantic Clone Detection","abstract":"BigCloneBench is a well-known and widely used large-scale dataset for the evaluation of recall of clone detection tools. It has been beneficial for research on clone detection and has become a standard in evaluating the performance of clone detection tools. More recently, it has also been widely used as a dataset to evaluate machine learning approaches to semantic clone detection or code similarity detection for functional or semantic similarity.","authors":["Jens Krinke","Chaiyong Ragkhitwetsagul"],"url":"https://arxiv.org/abs/2505.04311"}
{"created":"2025-05-08","title":"KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge Representation and Reasoning","abstract":"In this paper, we introduce KERAIA, a novel framework and software platform for symbolic knowledge engineering designed to address the persistent challenges of representing, reasoning with, and executing knowledge in dynamic, complex, and context-sensitive environments. The central research question that motivates this work is: How can unstructured, often tacit, human expertise be effectively transformed into computationally tractable algorithms that AI systems can efficiently utilise? KERAIA seeks to bridge this gap by building on foundational concepts such as Minsky's frame-based reasoning and K-lines, while introducing significant innovations. These include Clouds of Knowledge for dynamic aggregation, Dynamic Relations (DRels) for context-sensitive inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and Cloud Elaboration for adaptive knowledge transformation. This approach moves beyond the limitations of traditional, often static, knowledge representation paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle, ensuring transparency and interpretability, particularly through the use of LoTs. The paper details the framework's architecture, the KSYNTH representation language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse inference methods within a unified structure. We validate KERAIA's versatility, expressiveness, and practical applicability through detailed analysis of multiple case studies spanning naval warfare simulation, industrial diagnostics in water treatment plants, and strategic decision-making in the game of RISK. Furthermore, we provide a comparative analysis against established knowledge representation paradigms (including ontologies, rule-based systems, and knowledge graphs) and discuss the implementation aspects and computational considerations of the KERAIA platform.","authors":["Stephen Richard Varey","Alessandro Di Stefano","The Anh Han"],"url":"https://arxiv.org/abs/2505.04313"}
{"created":"2025-05-08","title":"The minimum distance of the antiprimitive BCH code with designed distance 3","abstract":"Let $\\mathcal{C}_{(q,q^m+1,3,h)}$ denote the antiprimitive BCH code with designed distance 3. In this paper, we demonstrate that the minimum distance $d$ of $\\mathcal{C}_{(q,q^m+1,3,h)}$ equals 3 if and only if $\\gcd(2h+1,q+1,q^m+1)\\ne1$. When both $q$ and $m$ are odd, we determine the sufficient and necessary condition for $d=4$ and fully characterize the minimum distance in this case. Based on these conditions, we investigate the parameters of $\\mathcal{C}_{(q,q^m+1,3,h)}$ for certain $h$. Additionally, two infinite families of distance-optimal codes and several linear codes with the best known parameters are presented.","authors":["Haojie Xu","Xia Wu","Wei Lu","Xiwang Cao"],"url":"https://arxiv.org/abs/2505.04315"}
{"created":"2025-05-08","title":"Mastering Multi-Drone Volleyball through Hierarchical Co-Self-Play Reinforcement Learning","abstract":"In this paper, we tackle the problem of learning to play 3v3 multi-drone volleyball, a new embodied competitive task that requires both high-level strategic coordination and low-level agile control. The task is turn-based, multi-agent, and physically grounded, posing significant challenges due to its long-horizon dependencies, tight inter-agent coupling, and the underactuated dynamics of quadrotors. To address this, we propose Hierarchical Co-Self-Play (HCSP), a hierarchical reinforcement learning framework that separates centralized high-level strategic decision-making from decentralized low-level motion control. We design a three-stage population-based training pipeline to enable both strategy and skill to emerge from scratch without expert demonstrations: (I) training diverse low-level skills, (II) learning high-level strategy via self-play with fixed low-level controllers, and (III) joint fine-tuning through co-self-play. Experiments show that HCSP achieves superior performance, outperforming non-hierarchical self-play and rule-based hierarchical baselines with an average 82.9\\% win rate and a 71.5\\% win rate against the two-stage variant. Moreover, co-self-play leads to emergent team behaviors such as role switching and coordinated formations, demonstrating the effectiveness of our hierarchical design and training scheme.","authors":["Ruize Zhang","Sirui Xiang","Zelai Xu","Feng Gao","Shilong Ji","Wenhao Tang","Wenbo Ding","Chao Yu","Yu Wang"],"url":"https://arxiv.org/abs/2505.04317"}
{"created":"2025-05-08","title":"Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of Fit Testing","abstract":"As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference. Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data. Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios. In this paper, we introduce an application of the $\\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference. To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs. Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions.","authors":["Jacob Glenn Ayers","Buvaneswari A. Ramanan","Manzoor A. Khan"],"url":"https://arxiv.org/abs/2505.04318"}
{"created":"2025-05-08","title":"Multi-turn Consistent Image Editing","abstract":"Many real-world applications, such as interactive photo retouching, artistic content creation, and product design, require flexible and iterative image editing. However, existing image editing methods primarily focus on achieving the desired modifications in a single step, which often struggles with ambiguous user intent, complex transformations, or the need for progressive refinements. As a result, these methods frequently produce inconsistent outcomes or fail to meet user expectations. To address these challenges, we propose a multi-turn image editing framework that enables users to iteratively refine their edits, progressively achieving more satisfactory results. Our approach leverages flow matching for accurate image inversion and a dual-objective Linear Quadratic Regulators (LQR) for stable sampling, effectively mitigating error accumulation. Additionally, by analyzing the layer-wise roles of transformers, we introduce a adaptive attention highlighting method that enhances editability while preserving multi-turn coherence. Extensive experiments demonstrate that our framework significantly improves edit success rates and visual fidelity compared to existing methods.","authors":["Zijun Zhou","Yingying Deng","Xiangyu He","Weiming Dong","Fan Tang"],"url":"https://arxiv.org/abs/2505.04320"}
{"created":"2025-05-08","title":"Verification of Digital Twins using Classical and Statistical Model Checking","abstract":"With the increasing adoption of digital techniques, the concept of digital twin (DT) has received a widespread attention in both industry and academia. While several definitions exist for a DT, most definitions focus on the existence of a virtual entity (VE) of a real-world object or process, often comprising interconnected models which interact with each other, undergoing changes continuously owing to the synchronization with the real-world object. These interactions might lead to inconsistencies at execution time, due to their highly stochastic and/or time-critical nature, which may lead to undesirable behavior. In addition, the continuously varying nature of VE owing to its synchronization with the real-world object further contributes to the complexity arising from these interactions and corresponding model execution times, which could possibly affect its overall functioning at runtime. This creates a need to perform (continuous) verification of the VE, to ensure that it behaves consistently at runtime by adhering to desired properties such as deadlock freeness, functional correctness, liveness and timeliness. Some critical properties such as deadlock freeness can only be verified using classical model checking; on the other hand, statistical model checking provides the possibility to model actual stochastic temporal behavior. We therefore propose to use both these techniques to verify the correctness and the fulfillment of desirable properties of VE. We present our observations and findings from applying these techniques on the DT of an autonomously driving truck. Results from these verification techniques suggest that this DT adheres to properties of deadlock freeness and functional correctness, but not adhering to timeliness properties.","authors":["Raghavendran Gunasekaran (Tilburg University)","Boudewijn Haverkort (University of Twente)"],"url":"https://arxiv.org/abs/2505.04322"}
{"created":"2025-05-08","title":"A Case Study on the Application of Digital Twins for Enhancing CPS Operations","abstract":"To ensure the availability and reduce the downtime of complex cyber-physical systems across different domains, e.g., agriculture and manufacturing, fault tolerance mechanisms are implemented which are complex in both their development and operation. In addition, cyber-physical systems are often confronted with limited hardware resources or are legacy systems, both often hindering the addition of new functionalities directly on the onboard hardware. Digital Twins can be adopted to offload expensive computations, as well as providing support through fault tolerance mechanisms, thus decreasing costs and operational downtime of cyber-physical systems. In this paper, we show the feasibility of a Digital Twin used for enhancing cyber-physical system operations, specifically through functional augmentation and increased fault tolerance, in an industry-oriented use case.","authors":["Irina Muntean (TYTAN Technologies","Munich","Germany)","Mirgita Frasheri (Aarhus University","Denmark)","Tiziano Munaro (fortiss GmbH","Munich","Germany)"],"url":"https://arxiv.org/abs/2505.04323"}
{"created":"2025-05-08","title":"Towards Federated Digital Twin Platforms","abstract":"Digital Twin (DT) technology has become rather popular in recent years, promising to optimize production processes, manage the operation of cyber-physical systems, with an impact spanning across multiple application domains (e.g., manufacturing, robotics, space etc.).  DTs can include different kinds of assets, e.g., models, data, which could potentially be reused across DT projects by multiple users, directly affecting development costs, as well as enabling collaboration and further development of these assets. To provide user support for these purposes, dedicated DT frameworks and platforms are required, that take into account user needs, providing the infrastructure and building blocks for DT development and management.  In this demo paper, we show how the DT as a Service (DTaaS) platform has been extended to enable a federated approach to DT development and management, that allows multiple users across multiple instances of DTaaS to discover, reuse, reconfigure, and modify existing DT assets.","authors":["Mirgita Frasheri (Aarhus University)","Prasad Talasila (Aarhus University)","Vanessa Scherma (Politecnico Di Torino)"],"url":"https://arxiv.org/abs/2505.04324"}
{"created":"2025-05-08","title":"Design and Evaluation of an NDN-Based Network for Distributed Digital Twins","abstract":"Digital twins (DT) have received significant attention due to their numerous benefits, such as real-time data analytics and cost reduction in production. DT serves as a fundamental component of many applications, encompassing smart manufacturing, intelligent vehicles, and smart cities. By using Machine Learning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently facilitate decision-making and productivity by simulating the status and changes of a physical entity. To handle the massive amount of data brought by DTs, it is challenging to achieve low response latency for data fetching over existing IP-based networks. IP-based networks use host addresses for end-to-end communication, making data distribution between DTs inefficient. Thus, we propose to use DTs in a distributed manner over Named Data Networking (NDN) networks. NDN is data-centric where data is routed based on content names, dynamically adjusting paths to optimize latency. Popular data is cached in network nodes, reducing data transmission and network congestion. Since data is fetched by content names, users and mobile devices can move freely without IP address reassignment. By using in-network caching and adaptive routing, we reckon NDN is an ideal fit for Future G Networks in the context of Digital Twins. We compared DTs in edge scenarios with cloud scenarios over NDN and IP-based networks to validate our insights. Extensive simulation results show that using DT in the edge reduces response latency by 10.2x. This position paper represents an initial investigation into the gap in distributed DTs over NDN, serving as an early-stage study.","authors":["Chen Chen","Zihan Jia","Ze Wang","Lin Cui","Fung Po Tso"],"url":"https://arxiv.org/abs/2505.04326"}
{"created":"2025-05-08","title":"Applied Post Quantum Cryptography: A Practical Approach for Generating Certificates in Industrial Environments","abstract":"The transition to post-quantum cryptography (PQC) presents significant challenges for certificate-based identity management in industrial environments, where secure onboarding of devices relies on long-lived and interoperable credentials. This work analyzes the integration of PQC into X.509 certificate structures and compares existing tool support for classical, hybrid, composite, and chameleon certificates. A gap is identified in available open-source solutions, particularly for the generation and validation of hybrid and composite certificates via command-line interfaces. To address this, a proof-of-concept implementation based on the Bouncy Castle library is developed. The tool supports the creation of classical, hybrid (Catalyst), composite, and partially chameleon certificates using PQC algorithms such as ML-DSA and SLH-DSA. It demonstrates compatibility with standard X.509 workflows and aims to support headless operation and constrained platforms typical of industrial systems. The implementation is modular, publicly available, and intended to facilitate further research and testing of PQC migration strategies in practice. A comparison with OpenSSL-based solutions highlights current limitations in standardization, toolchain support, and algorithm coverage.","authors":["Nino Ricchizzi","Christian Schwinne","Jan Pelzl"],"url":"https://arxiv.org/abs/2505.04333"}
{"created":"2025-05-08","title":"Hyperbolic Fuzzy $C$-Means with Adaptive Weight-based Filtering for Clustering in Non-Euclidean Spaces","abstract":"Clustering algorithms play a pivotal role in unsupervised learning by identifying and grouping similar objects based on shared characteristics. While traditional clustering techniques, such as hard and fuzzy center-based clustering, have been widely used, they struggle with complex, high-dimensional, and non-Euclidean datasets. In particular, the Fuzzy $C$-Means (FCM) algorithm, despite its efficiency and popularity, exhibits notable limitations in non-Euclidean spaces. Euclidean spaces assume linear separability and uniform distance scaling, limiting their effectiveness in capturing complex, hierarchical, or non-Euclidean structures in fuzzy clustering. To overcome these challenges, we introduce Filtration-based Hyperbolic Fuzzy $C$-Means (HypeFCM), a novel clustering algorithm tailored for better representation of data relationships in non-Euclidean spaces. HypeFCM integrates the principles of fuzzy clustering with hyperbolic geometry and employs a weight-based filtering mechanism to improve performance. The algorithm initializes weights using a Dirichlet distribution and iteratively refines cluster centroids and membership assignments based on a hyperbolic metric in the Poincar\\'e Disc model. Extensive experimental evaluations demonstrate that HypeFCM significantly outperforms conventional fuzzy clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.","authors":["Swagato Das","Arghya Pratihar","Swagatam Das"],"url":"https://arxiv.org/abs/2505.04335"}
{"created":"2025-05-08","title":"Riemannian Denoising Diffusion Probabilistic Models","abstract":"We propose Riemannian Denoising Diffusion Probabilistic Models (RDDPMs) for learning distributions on submanifolds of Euclidean space that are level sets of functions, including most of the manifolds relevant to applications. Existing methods for generative modeling on manifolds rely on substantial geometric information such as geodesic curves or eigenfunctions of the Laplace-Beltrami operator and, as a result, they are limited to manifolds where such information is available. In contrast, our method, built on a projection scheme, can be applied to more general manifolds, as it only requires being able to evaluate the value and the first order derivatives of the function that defines the submanifold. We provide a theoretical analysis of our method in the continuous-time limit, which elucidates the connection between our RDDPMs and score-based generative models on manifolds. The capability of our method is demonstrated on datasets from previous studies and on new datasets sampled from two high-dimensional manifolds, i.e. $\\mathrm{SO}(10)$ and the configuration space of molecular system alanine dipeptide with fixed dihedral angle.","authors":["Zichen Liu","Wei Zhang","Christof Sch\\\"utte","Tiejun Li"],"url":"https://arxiv.org/abs/2505.04338"}
{"created":"2025-05-08","title":"Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning","abstract":"DBSCAN, a well-known density-based clustering algorithm, has gained widespread popularity and usage due to its effectiveness in identifying clusters of arbitrary shapes and handling noisy data. However, it encounters challenges in producing satisfactory cluster results when confronted with datasets of varying density scales, a common scenario in real-world applications. In this paper, we propose a novel Adaptive and Robust DBSCAN with Multi-agent Reinforcement Learning cluster framework, namely AR-DBSCAN. First, we model the initial dataset as a two-level encoding tree and categorize the data vertices into distinct density partitions according to the information uncertainty determined in the encoding tree. Each partition is then assigned to an agent to find the best clustering parameters without manual assistance. The allocation is density-adaptive, enabling AR-DBSCAN to effectively handle diverse density distributions within the dataset by utilizing distinct agents for different partitions. Second, a multi-agent deep reinforcement learning guided automatic parameter searching process is designed. The process of adjusting the parameter search direction by perceiving the clustering environment is modeled as a Markov decision process. Using a weakly-supervised reward training policy network, each agent adaptively learns the optimal clustering parameters by interacting with the clusters. Third, a recursive search mechanism adaptable to the data's scale is presented, enabling efficient and controlled exploration of large parameter spaces. Extensive experiments are conducted on nine artificial datasets and a real-world dataset. The results of offline and online tasks show that AR-DBSCAN not only improves clustering accuracy by up to 144.1% and 175.3% in the NMI and ARI metrics, respectively, but also is capable of robustly finding dominant parameters.","authors":["Hao Peng","Xiang Huang","Shuo Sun","Ruitong Zhang","Philip S. Yu"],"url":"https://arxiv.org/abs/2505.04339"}
{"created":"2025-05-08","title":"Multi-Granular Attention based Heterogeneous Hypergraph Neural Network","abstract":"Heterogeneous graph neural networks (HeteGNNs) have demonstrated strong abilities to learn node representations by effectively extracting complex structural and semantic information in heterogeneous graphs. Most of the prevailing HeteGNNs follow the neighborhood aggregation paradigm, leveraging meta-path based message passing to learn latent node representations. However, due to the pairwise nature of meta-paths, these models fail to capture high-order relations among nodes, resulting in suboptimal performance. Additionally, the challenge of ``over-squashing'', where long-range message passing in HeteGNNs leads to severe information distortion, further limits the efficacy of these models. To address these limitations, this paper proposes MGA-HHN, a Multi-Granular Attention based Heterogeneous Hypergraph Neural Network for heterogeneous graph representation learning. MGA-HHN introduces two key innovations: (1) a novel approach for constructing meta-path based heterogeneous hypergraphs that explicitly models higher-order semantic information in heterogeneous graphs through multiple views, and (2) a multi-granular attention mechanism that operates at both the node and hyperedge levels. This mechanism enables the model to capture fine-grained interactions among nodes sharing the same semantic context within a hyperedge type, while preserving the diversity of semantics across different hyperedge types. As such, MGA-HHN effectively mitigates long-range message distortion and generates more expressive node representations. Extensive experiments on real-world benchmark datasets demonstrate that MGA-HHN outperforms state-of-the-art models, showcasing its effectiveness in node classification, node clustering and visualization tasks.","authors":["Hong Jin","Kaicheng Zhou","Jie Yin","Lan You","Zhifeng Zhou"],"url":"https://arxiv.org/abs/2505.04340"}
{"created":"2025-05-08","title":"Resist Platform-Controlled AI Agents and Champion User-Centric Agent Advocates","abstract":"Language model agents could reshape how users navigate and act in digital environments. If controlled by platform companies -- either those that already dominate online search, communication, and commerce, or those vying to replace them -- platform agents could intensify surveillance, exacerbate user lock-in, and further entrench the incumbent digital giants. This position paper argues that to resist the undesirable effects of platform agents, we should champion agent advocates -- agents that are controlled by users, serve the interests of users, and preserve user autonomy and choice. We identify key interventions to enable agent advocates: ensuring public access to compute, developing interoperability protocols and safety standards, and implementing appropriate market regulations.","authors":["Sayash Kapoor","Noam Kolt","Seth Lazar"],"url":"https://arxiv.org/abs/2505.04345"}
{"created":"2025-05-08","title":"Topology-Driven Clustering: Enhancing Performance with Betti Number Filtration","abstract":"Clustering aims to form groups of similar data points in an unsupervised regime. Yet, clustering complex datasets containing critically intertwined shapes poses significant challenges. The prevailing clustering algorithms widely depend on evaluating similarity measures based on Euclidean metrics. Exploring topological characteristics to perform clustering of complex datasets inevitably presents a better scope. The topological clustering algorithms predominantly perceive the point set through the lens of Simplicial complexes and Persistent homology. Despite these approaches, the existing topological clustering algorithms cannot somehow fully exploit topological structures and show inconsistent performances on some highly complicated datasets. This work aims to mitigate the limitations by identifying topologically similar neighbors through the Vietoris-Rips complex and Betti number filtration. In addition, we introduce the concept of the Betti sequences to capture flexibly essential features from the topological structures. Our proposed algorithm is adept at clustering complex, intertwined shapes contained in the datasets. We carried out experiments on several synthetic and real-world datasets. Our algorithm demonstrated commendable performances across the datasets compared to some of the well-known topology-based clustering algorithms.","authors":["Arghya Pratihar","Kushal Bose","Swagatam Das"],"url":"https://arxiv.org/abs/2505.04346"}
{"created":"2025-05-08","title":"CountDiffusion: Text-to-Image Synthesis with Training-Free Counting-Guidance Diffusion","abstract":"Stable Diffusion has advanced text-to-image synthesis, but training models to generate images with accurate object quantity is still difficult due to the high computational cost and the challenge of teaching models the abstract concept of quantity. In this paper, we propose CountDiffusion, a training-free framework aiming at generating images with correct object quantity from textual descriptions. CountDiffusion consists of two stages. In the first stage, an intermediate denoising result is generated by the diffusion model to predict the final synthesized image with one-step denoising, and a counting model is used to count the number of objects in this image. In the second stage, a correction module is used to correct the object quantity by changing the attention map of the object with universal guidance. The proposed CountDiffusion can be plugged into any diffusion-based text-to-image (T2I) generation models without further training. Experiment results demonstrate the superiority of our proposed CountDiffusion, which improves the accurate object quantity generation ability of T2I models by a large margin.","authors":["Yanyu Li","Pencheng Wan","Liang Han","Yaowei Wang","Liqiang Nie","Min Zhang"],"url":"https://arxiv.org/abs/2505.04347"}
{"created":"2025-05-08","title":"On the one-dimensional SPH approximation of fractional-order operators","abstract":"This work presents a theoretical formalism and the corresponding numerical techniques to obtain the approximation of fractional-order operators over a 1D domain via the smoothed particle hydrodynamics (SPH) method. The method is presented for both constant- and variable-order operators, in either integral or differential forms. Several numerical examples are presented in order to validate the theory against analytical results and to evaluate the performance of the methodology. This formalism paves the way for the solution of fractional-order continuum mechanics models via the SPH method.","authors":["Khashayar Ghorbani","Fabio Semperlotti"],"url":"https://arxiv.org/abs/2505.04350"}
{"created":"2025-05-08","title":"Uncertain Machine Ethics Planning","abstract":"Machine Ethics decisions should consider the implications of uncertainty over decisions. Decisions should be made over sequences of actions to reach preferable outcomes long term. The evaluation of outcomes, however, may invoke one or more moral theories, which might have conflicting judgements. Each theory will require differing representations of the ethical situation. For example, Utilitarianism measures numerical values, Deontology analyses duties, and Virtue Ethics emphasises moral character. While balancing potentially conflicting moral considerations, decisions may need to be made, for example, to achieve morally neutral goals with minimal costs. In this paper, we formalise the problem as a Multi-Moral Markov Decision Process and a Multi-Moral Stochastic Shortest Path Problem. We develop a heuristic algorithm based on Multi-Objective AO*, utilising Sven-Ove Hansson's Hypothetical Retrospection procedure for ethical reasoning under uncertainty. Our approach is validated by a case study from Machine Ethics literature: the problem of whether to steal insulin for someone who needs it.","authors":["Simon Kolker","Louise A. Dennis","Ramon Fraga Pereira","Mengwei Xu"],"url":"https://arxiv.org/abs/2505.04352"}
{"created":"2025-05-08","title":"Yield and Buckling Stress Limits in Topology Optimization of Multiscale Structures","abstract":"This study presents an extension of multiscale topology optimization by integrating both yield stress and local/global buckling considerations into the design process. Building upon established multiscale methodologies, we develop a new framework incorporating yield stress limits either as constraints or objectives alongside previously established local and global buckling constraints. This approach significantly refines the optimization process, ensuring that the resulting designs meet mechanical performance criteria and adhere to critical material yield constraints. First, we establish local density-dependent von Mises yield surfaces based on local yield estimates from homogenization-based analysis to predict the local yield limits of the homogenized materials. Then, these local Yield-based Load Factors (YLFs) are combined with local and global buckling criteria to obtain topology optimized designs that consider yield and buckling failure on all levels. This integration is crucial for the practical application of optimized structures in real-world scenarios, where material yield and stability behavior critically influence structural integrity and durability. Numerical examples demonstrate how optimized designs depend on the stiffness to yield ratio of the considered building material. Despite the foundational assumption of separation of scales, the de-homogenized structures, even at relatively coarse length scales, exhibit a high degree of agreement with the corresponding homogenized predictions.","authors":["Christoffer Fyllgraf Christensen","Fengwen Wang","Ole Sigmund"],"url":"https://arxiv.org/abs/2505.04353"}
{"created":"2025-05-08","title":"RDPP-TD: Reputation and Data Privacy-Preserving based Truth Discovery Scheme in Mobile Crowdsensing","abstract":"Truth discovery (TD) plays an important role in Mobile Crowdsensing (MCS). However, existing TD methods, including privacy-preserving TD approaches, estimate the truth by weighting only the data submitted in the current round, which often results in low data quality. Moreover, there is a lack of effective TD methods that preserve both reputation and data privacy. To address these issues, a Reputation and Data Privacy-Preserving based Truth Discovery (RDPP-TD) scheme is proposed to obtain high-quality data for MCS. The RDPP-TD scheme consists of two key approaches: a Reputation-based Truth Discovery (RTD) approach, which integrates the weight of current-round data with workers' reputation values to estimate the truth, thereby achieving more accurate results, and a Reputation and Data Privacy-Preserving (RDPP) approach, which ensures privacy preservation for sensing data and reputation values. First, the RDPP approach, when seamlessly integrated with RTD, can effectively evaluate the reliability of workers and their sensing data in a privacy-preserving manner. Second, the RDPP scheme supports reputation-based worker recruitment and rewards, ensuring high-quality data collection while incentivizing workers to provide accurate information. Comprehensive theoretical analysis and extensive experiments based on real-world datasets demonstrate that the proposed RDPP-TD scheme provides strong privacy protection and improves data quality by up to 33.3%.","authors":["Lijian Wu","Weikun Xie","Wei Tan","Tian Wang","Houbing Herbert Song","Anfeng Liu"],"url":"https://arxiv.org/abs/2505.04361"}
{"created":"2025-05-08","title":"Benchmarking LLMs' Swarm intelligence","abstract":"Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.","authors":["Kai Ruan","Mowen Huang","Ji-Rong Wen","Hao Sun"],"url":"https://arxiv.org/abs/2505.04364"}
{"created":"2025-05-08","title":"CDE-Mapper: Using Retrieval-Augmented Language Models for Linking Clinical Data Elements to Controlled Vocabularies","abstract":"The standardization of clinical data elements (CDEs) aims to ensure consistent and comprehensive patient information across various healthcare systems. Existing methods often falter when standardizing CDEs of varying representation and complex structure, impeding data integration and interoperability in clinical research. We introduce CDE-Mapper, an innovative framework that leverages Retrieval-Augmented Generation approach combined with Large Language Models to automate the linking of CDEs to controlled vocabularies. Our modular approach features query decomposition to manage varying levels of CDEs complexity, integrates expert-defined rules within prompt engineering, and employs in-context learning alongside multiple retriever components to resolve terminological ambiguities. In addition, we propose a knowledge reservoir validated by a human-in-loop approach, achieving accurate concept linking for future applications while minimizing computational costs. For four diverse datasets, CDE-Mapper achieved an average of 7.2\\% higher accuracy improvement compared to baseline methods. This work highlights the potential of advanced language models in improving data harmonization and significantly advancing capabilities in clinical decision support systems and research.","authors":["Komal Gilani","Marlo Verket","Christof Peters","Michel Dumontier","Hans-Peter Brunner-La Rocca","Visara Urovi"],"url":"https://arxiv.org/abs/2505.04365"}
{"created":"2025-05-08","title":"Deep Learning Innovations for Energy Efficiency: Advances in Non-Intrusive Load Monitoring and EV Charging Optimization for a Sustainable Grid","abstract":"The global energy landscape is undergoing a profound transformation, often referred to as the energy transition, driven by the urgent need to mitigate climate change, reduce greenhouse gas emissions, and ensure sustainable energy supplies. However, the undoubted complexity of new investments in renewables, as well as the phase out of high CO2-emission energy sources, hampers the pace of the energy transition and raises doubts as to whether new renewable energy sources are capable of solely meeting the climate target goals. This highlights the need to investigate alternative pathways to accelerate the energy transition, by identifying human activity domains with higher/excessive energy demands. Two notable examples where there is room for improvement, in the sense of reducing energy consumption and consequently CO2 emissions, are residential energy consumption and road transport. This dissertation investigates the development of novel Deep Learning techniques to create tools which solve limitations in these two key energy domains. Reduction of residential energy consumption can be achieved by empowering end-users with the user of Non-Intrusive Load Monitoring, whereas optimization of EV charging with Deep Reinforcement Learning can tackle road transport decarbonization.","authors":["Stavros Sykiotis"],"url":"https://arxiv.org/abs/2505.04367"}
{"created":"2025-05-08","title":"Pipelining Split Learning in Multi-hop Edge Networks","abstract":"To support large-scale model training, split learning (SL) enables multiple edge devices/servers to share the intensive training workload. However, most existing works on SL focus solely on two-tier model splitting. Moreover, while some recent works have investigated the model splitting and placement problems for multi-hop SL, these solutions fail to overcome the resource idleness issue, resulting in significant network idle time. In this work, we propose a pipelined SL scheme by addressing the joint optimization problem of model splitting and placement (MSP) in multi-hop edge networks. By applying pipeline parallelism to SL, we identify that the MSP problem can be mapped to a problem of minimizing the weighted sum of a bottleneck cost function (min-max) and a linear cost function (min-sum). Based on graph theory, we devise a bottleneck-aware shortest-path algorithm to obtain the optimal solution. Besides, given the MSP outcomes, we also derive the closed-form solution to the micro-batch size in the pipeline. Finally, we develop an alternating optimization algorithm of MSP and micro-batch size to solve the joint optimization problem to minimize the end-to-end training latency. Extensive simulations have demonstrated the significant advantages of our algorithm compared to existing benchmarks without pipeline parallelism.","authors":["Wei Wei","Zheng Lin","Tao Li","Xuanheng Li","Xianhao Chen"],"url":"https://arxiv.org/abs/2505.04368"}
{"created":"2025-05-08","title":"WDMamba: When Wavelet Degradation Prior Meets Vision Mamba for Image Dehazing","abstract":"In this paper, we reveal a novel haze-specific wavelet degradation prior observed through wavelet transform analysis, which shows that haze-related information predominantly resides in low-frequency components. Exploiting this insight, we propose a novel dehazing framework, WDMamba, which decomposes the image dehazing task into two sequential stages: low-frequency restoration followed by detail enhancement. This coarse-to-fine strategy enables WDMamba to effectively capture features specific to each stage of the dehazing process, resulting in high-quality restored images. Specifically, in the low-frequency restoration stage, we integrate Mamba blocks to reconstruct global structures with linear complexity, efficiently removing overall haze and producing a coarse restored image. Thereafter, the detail enhancement stage reinstates fine-grained information that may have been overlooked during the previous phase, culminating in the final dehazed output. Furthermore, to enhance detail retention and achieve more natural dehazing, we introduce a self-guided contrastive regularization during network training. By utilizing the coarse restored output as a hard negative example, our model learns more discriminative representations, substantially boosting the overall dehazing performance. Extensive evaluations on public dehazing benchmarks demonstrate that our method surpasses state-of-the-art approaches both qualitatively and quantitatively. Code is available at https://github.com/SunJ000/WDMamba.","authors":["Jie Sun","Heng Liu","Yongzhen Wang","Xiao-Ping Zhang","Mingqiang Wei"],"url":"https://arxiv.org/abs/2505.04369"}
{"created":"2025-05-08","title":"Fast Bellman algorithm for real Monge-Amp\\`ere equation","abstract":"In this paper, we introduce a new numerical algorithm for solving the Dirichlet","authors":["Aleksandra Le","Frank Wikstr\\\"om"],"url":"https://arxiv.org/abs/2505.04370"}
{"created":"2025-05-08","title":"Extending a Quantum Reinforcement Learning Exploration Policy with Flags to Connect Four","abstract":"Action selection based on flags is a Reinforcement Learning (RL) exploration policy that improves the exploration of the state space through the use of flags, which can identify the most promising actions to take in each state. The quantum counterpart of this exploration policy further improves upon this by taking advantage of a quadratic speedup for sampling flagged actions. This approach has already been successfully employed for the game of Checkers. In this work, we describe the application of this method to the context of Connect Four, in order to study its performance in a different setting, which can lead to a better generalization of the technique. We also kept track of a metric that wasn't taken into account in previous work: the average number of iterations to obtain a flagged action. Since going second is a significant disadvantage in Connect Four, we also had the intent of exploring how this more complex scenario would impact the performance of our approach. The experiments involved training and testing classical and quantum RL agents that played either going first or going second against a Randomized Negamax opponent. The results showed that both flagged exploration policies were clearly superior to a simple epsilon-greedy policy. Furthermore, the quantum agents did in fact sample flagged actions in less iterations. Despite obtaining tagged actions more consistently, the win rates between the classical and quantum versions of the approach were identical, which could be due to the simplicity of the training scenario chosen.","authors":["Filipe Santos (CISUC","DEI","University of Coimbra)","Jo\\~ao Paulo Fernandes (LIACC","New York University Abu Dhabi)","Lu\\'is Macedo (CISUC","DEI","University of Coimbra)"],"url":"https://arxiv.org/abs/2505.04371"}
{"created":"2025-05-08","title":"NN-Based Joint Mitigation of IQ Imbalance and PA Nonlinearity With Multiple States","abstract":"Joint mitigation of IQ imbalance and PA nonlinearity is important for improving the performance of radio frequency (RF) transmitters. In this paper, we propose a new neural network (NN) model, which can be used for joint digital pre-distortion (DPD) of non-ideal IQ modulators and PAs in a transmitter with multiple operating states. The model is based on the methodology of multi-task learning (MTL). In this model, the hidden layers of the main NN are shared by all signal states, and the output layer's weights and biases are dynamically generated by another NN. The experimental results show that the proposed model can effectively perform joint DPD for IQ-PA systems, and it achieves better overall performance within multiple signal states than the existing methods.","authors":["Yundi Zhang","Wendong Cheng","Li Chen"],"url":"https://arxiv.org/abs/2505.04373"}
{"created":"2025-05-08","title":"Balancing Accuracy, Calibration, and Efficiency in Active Learning with Vision Transformers Under Label Noise","abstract":"Fine-tuning pre-trained convolutional neural networks on ImageNet for downstream tasks is well-established. Still, the impact of model size on the performance of vision transformers in similar scenarios, particularly under label noise, remains largely unexplored. Given the utility and versatility of transformer architectures, this study investigates their practicality under low-budget constraints and noisy labels. We explore how classification accuracy and calibration are affected by symmetric label noise in active learning settings, evaluating four vision transformer configurations (Base and Large with 16x16 and 32x32 patch sizes) and three Swin Transformer configurations (Tiny, Small, and Base) on CIFAR10 and CIFAR100 datasets, under varying label noise rates. Our findings show that larger ViT models (ViTl32 in particular) consistently outperform their smaller counterparts in both accuracy and calibration, even under moderate to high label noise, while Swin Transformers exhibit weaker robustness across all noise levels. We find that smaller patch sizes do not always lead to better performance, as ViTl16 performs consistently worse than ViTl32 while incurring a higher computational cost. We also find that information-based Active Learning strategies only provide meaningful accuracy improvements at moderate label noise rates, but they result in poorer calibration compared to models trained on randomly acquired labels, especially at high label noise rates. We hope these insights provide actionable guidance for practitioners looking to deploy vision transformers in resource-constrained environments, where balancing model complexity, label noise, and compute efficiency is critical in model fine-tuning or distillation.","authors":["Moseli Mots'oehli","Hope Mogale","Kyungim Baek"],"url":"https://arxiv.org/abs/2505.04375"}
{"created":"2025-05-08","title":"Label-efficient Single Photon Images Classification via Active Learning","abstract":"Single-photon LiDAR achieves high-precision 3D imaging in extreme environments through quantum-level photon detection technology. Current research primarily focuses on reconstructing 3D scenes from sparse photon events, whereas the semantic interpretation of single-photon images remains underexplored, due to high annotation costs and inefficient labeling strategies. This paper presents the first active learning framework for single-photon image classification. The core contribution is an imaging condition-aware sampling strategy that integrates synthetic augmentation to model variability across imaging conditions. By identifying samples where the model is both uncertain and sensitive to these conditions, the proposed method selectively annotates only the most informative examples. Experiments on both synthetic and real-world datasets show that our approach outperforms all baselines and achieves high classification accuracy with significantly fewer labeled samples. Specifically, our approach achieves 97% accuracy on synthetic single-photon data using only 1.5% labeled samples. On real-world data, we maintain 90.63% accuracy with just 8% labeled samples, which is 4.51% higher than the best-performing baseline. This illustrates that active learning enables the same level of classification performance on single-photon images as on classical images, opening doors to large-scale integration of single-photon data in real-world applications.","authors":["Zili Zhang","Ziting Wen","Yiheng Qiang","Hongzhou Dong","Wenle Dong","Xinyang Li","Xiaofan Wang","Xiaoqiang Ren"],"url":"https://arxiv.org/abs/2505.04376"}
{"created":"2025-05-08","title":"Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and Performance in Mixed Urban Traffic","abstract":"Transportation systems have long been shaped by complexity and heterogeneity, driven by the interdependency of agent actions and traffic outcomes. The deployment of automated vehicles (AVs) in such systems introduces a new challenge: achieving consensus across safety, interaction quality, and traffic performance. In this work, we position consensus as a fundamental property of the traffic system and aim to quantify it. We use high-resolution trajectory data from the Third Generation Simulation (TGSIM) dataset to empirically analyze AV and human-driven vehicle (HDV) behavior at a signalized urban intersection and around vulnerable road users (VRUs). Key metrics, including Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns, headways, and string stability, are evaluated across the three performance dimensions. Results show that full consensus across safety, interaction, and performance is rare, with only 1.63% of AV-VRU interaction frames meeting all three conditions. These findings highlight the need for AV models that explicitly balance multi-dimensional performance in mixed-traffic environments. Full reproducibility is supported via our open-source codebase on https://github.com/wissamkontar/Consensus-AV-Analysis.","authors":["Mohammad Elayan","Wissam Kontar"],"url":"https://arxiv.org/abs/2505.04379"}
{"created":"2025-05-08","title":"Tetrahedron-Net for Medical Image Registration","abstract":"Medical image registration plays a vital role in medical image processing. Extracting expressive representations for medical images is crucial for improving the registration quality. One common practice for this end is constructing a convolutional backbone to enable interactions with skip connections among feature extraction layers. The de facto structure, U-Net-like networks, has attempted to design skip connections such as nested or full-scale ones to connect one single encoder and one single decoder to improve its representation capacity. Despite being effective, it still does not fully explore interactions with a single encoder and decoder architectures. In this paper, we embrace this observation and introduce a simple yet effective alternative strategy to enhance the representations for registrations by appending one additional decoder. The new decoder is designed to interact with both the original encoder and decoder. In this way, it not only reuses feature presentation from corresponding layers in the encoder but also interacts with the original decoder to corporately give more accurate registration results. The new architecture is concise yet generalized, with only one encoder and two decoders forming a ``Tetrahedron'' structure, thereby dubbed Tetrahedron-Net. Three instantiations of Tetrahedron-Net are further constructed regarding the different structures of the appended decoder. Our extensive experiments prove that superior performance can be obtained on several representative benchmarks of medical image registration. Finally, such a ``Tetrahedron'' design can also be easily integrated into popular U-Net-like architectures including VoxelMorph, ViT-V-Net, and TransMorph, leading to consistent performance gains.","authors":["Jinhai Xiang","Shuai Guo","Qianru Han","Dantong Shi","Xinwei He","Xiang Bai"],"url":"https://arxiv.org/abs/2505.04380"}
{"created":"2025-05-08","title":"DATA: Multi-Disentanglement based Contrastive Learning for Open-World Semi-Supervised Deepfake Attribution","abstract":"Deepfake attribution (DFA) aims to perform multiclassification on different facial manipulation techniques, thereby mitigating the detrimental effects of forgery content on the social order and personal reputations. However, previous methods focus only on method-specific clues, which easily lead to overfitting, while overlooking the crucial role of common forgery features. Additionally, they struggle to distinguish between uncertain novel classes in more practical open-world scenarios. To address these issues, in this paper we propose an innovative multi-DisentAnglement based conTrastive leArning framework, DATA, to enhance the generalization ability on novel classes for the open-world semi-supervised deepfake attribution (OSS-DFA) task. Specifically, since all generation techniques can be abstracted into a similar architecture, DATA defines the concept of 'Orthonormal Deepfake Basis' for the first time and utilizes it to disentangle method-specific features, thereby reducing the overfitting on forgery-irrelevant information. Furthermore, an augmented-memory mechanism is designed to assist in novel class discovery and contrastive learning, which aims to obtain clear class boundaries for the novel classes through instance-level disentanglements. Additionally, to enhance the standardization and discrimination of features, DATA uses bases contrastive loss and center contrastive loss as auxiliaries for the aforementioned modules. Extensive experimental evaluations show that DATA achieves state-of-the-art performance on the OSS-DFA benchmark, e.g., there are notable accuracy improvements in 2.55% / 5.7% under different settings, compared with the existing methods.","authors":["Ming-Hui Liu","Xiao-Qian Liu","Xin Luo","Xin-Shun Xu"],"url":"https://arxiv.org/abs/2505.04384"}
{"created":"2025-05-08","title":"Geometry-Aware Texture Generation for 3D Head Modeling with Artist-driven Control","abstract":"Creating realistic 3D head assets for virtual characters that match a precise artistic vision remains labor-intensive. We present a novel framework that streamlines this process by providing artists with intuitive control over generated 3D heads. Our approach uses a geometry-aware texture synthesis pipeline that learns correlations between head geometry and skin texture maps across different demographics. The framework offers three levels of artistic control: manipulation of overall head geometry, adjustment of skin tone while preserving facial characteristics, and fine-grained editing of details such as wrinkles or facial hair. Our pipeline allows artists to make edits to a single texture map using familiar tools, with our system automatically propagating these changes coherently across the remaining texture maps needed for realistic rendering. Experiments demonstrate that our method produces diverse results with clean geometries. We showcase practical applications focusing on intuitive control for artists, including skin tone adjustments and simplified editing workflows for adding age-related details or removing unwanted features from scanned models. This integrated approach aims to streamline the artistic workflow in virtual character creation.","authors":["Amin Fadaeinejad","Abdallah Dib","Luiz Gustavo Hafemann","Emeline Got","Trevor Anderson","Amaury Depierre","Nikolaus F. Troje","Marcus A. Brubaker","Marc-Andr\\'e Carbonneau"],"url":"https://arxiv.org/abs/2505.04387"}
{"created":"2025-05-08","title":"The Aloe Family Recipe for Open and Specialized Healthcare LLMs","abstract":"Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.","authors":["Dario Garcia-Gasulla","Jordi Bayarri-Planas","Ashwin Kumar Gururajan","Enrique Lopez-Cuena","Adrian Tormos","Daniel Hinjos","Pablo Bernabeu-Perez","Anna Arias-Duart","Pablo Agustin Martin-Torres","Marta Gonzalez-Mallo","Sergio Alvarez-Napagao","Eduard Ayguad\\'e-Parra","Ulises Cort\\'es"],"url":"https://arxiv.org/abs/2505.04388"}
{"created":"2025-05-08","title":"Clust-Splitter $-$ an Efficient Nonsmooth Optimization-Based Algorithm for Clustering Large Datasets","abstract":"Clustering is a fundamental task in data mining and machine learning, particularly for analyzing large-scale data. In this paper, we introduce Clust-Splitter, an efficient algorithm based on nonsmooth optimization, designed to solve the minimum sum-of-squares clustering problem in very large datasets. The clustering task is approached through a sequence of three nonsmooth optimization problems: two auxiliary problems used to generate suitable starting points, followed by a main clustering formulation. To solve these problems effectively, the limited memory bundle method is combined with an incremental approach to develop the Clust-Splitter algorithm. We evaluate Clust-Splitter on real-world datasets characterized by both a large number of attributes and a large number of data points and compare its performance with several state-of-the-art large-scale clustering algorithms. Experimental results demonstrate the efficiency of the proposed method for clustering very large datasets, as well as the high quality of its solutions, which are on par with those of the best existing methods.","authors":["Jenni Lampainen","Kaisa Joki","Napsu Karmitsa","Marko M. M\\\"akel\\\"a"],"url":"https://arxiv.org/abs/2505.04389"}
{"created":"2025-05-08","title":"Predicting Road Surface Anomalies by Visual Tracking of a Preceding Vehicle","abstract":"A novel approach to detect road surface anomalies by visual tracking of a preceding vehicle is proposed. The method is versatile, predicting any kind of road anomalies, such as potholes, bumps, debris, etc., unlike direct observation methods that rely on training visual detectors of those cases. The method operates in low visibility conditions or in dense traffic where the anomaly is occluded by a preceding vehicle. Anomalies are detected predictively, i.e., before a vehicle encounters them, which allows to pre-configure low-level vehicle systems (such as chassis) or to plan an avoidance maneuver in case of autonomous driving. A challenge is that the signal coming from camera-based tracking of a preceding vehicle may be weak and disturbed by camera ego motion due to vibrations affecting the ego vehicle. Therefore, we propose an efficient method to compensate camera pitch rotation by an iterative robust estimator. Our experiments on both controlled setup and normal traffic conditions show that road anomalies can be detected reliably at a distance even in challenging cases where the ego vehicle traverses imperfect road surfaces. The method is effective and performs in real time on standard consumer hardware.","authors":["Petr Jahoda","Jan Cech"],"url":"https://arxiv.org/abs/2505.04392"}
{"created":"2025-05-08","title":"Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters","abstract":"With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users. Large language models (LLMs) are predominantly used by many as a primary source of information for various topics. LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions. Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification. We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat. This metric measures the alignment between an individual's political views and the positions of German political parties. We compare the models' alignment scores to identify factors influencing their political preferences. Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs. Also, we find that the language we use to communicate with the models affects their political views. Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag. Our results imply that LLMs are prone to exhibiting political bias. Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale.","authors":["David Exler","Mark Schutera","Markus Reischl","Luca Rettenberger"],"url":"https://arxiv.org/abs/2505.04393"}
{"created":"2025-05-08","title":"SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin Transformer","abstract":"This paper presents an efficient visual speech encoder for lip reading. While most recent lip reading studies have been based on the ResNet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. Additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). To overcome the limitations of Convolutional Neural Network (CNN)-based models, we apply the hierarchical structure and window self-attention of the Swin Transformer to lip reading. We configure a new lightweight scale of the Swin Transformer suitable for processing lip reading data and present the SwinLip visual speech encoder, which efficiently reduces computational load by integrating modified Convolution-augmented Transformer (Conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. Through extensive experiments, we have validated that our SwinLip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. In particular, our SwinLip demonstrated robust performance in both English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art performance on the Mandarin LRW-1000 dataset with less computation compared to the existing state-of-the-art model.","authors":["Young-Hu Park","Rae-Hong Park","Hyung-Min Park"],"url":"https://arxiv.org/abs/2505.04394"}
{"created":"2025-05-08","title":"Supporting renewable energy planning and operation with data-driven high-resolution ensemble weather forecast","abstract":"The planning and operation of renewable energy, especially wind power, depend crucially on accurate, timely, and high-resolution weather information. Coarse-grid global numerical weather forecasts are typically downscaled to meet these requirements, introducing challenges of scale inconsistency, process representation error, computation cost, and entanglement of distinct uncertainty sources from chaoticity, model bias, and large-scale forcing. We address these challenges by learning the climatological distribution of a target wind farm using its high-resolution numerical weather simulations. An optimal combination of this learned high-resolution climatological prior with coarse-grid large scale forecasts yields highly accurate, fine-grained, full-variable, large ensemble of weather pattern forecasts. Using observed meteorological records and wind turbine power outputs as references, the proposed methodology verifies advantageously compared to existing numerical/statistical forecasting-downscaling pipelines, regarding either deterministic/probabilistic skills or economic gains. Moreover, a 100-member, 10-day forecast with spatial resolution of 1 km and output frequency of 15 min takes < 1 hour on a moderate-end GPU, as contrast to $\\mathcal{O}(10^3)$ CPU hours for conventional numerical simulation. By drastically reducing computational costs while maintaining accuracy, our method paves the way for more efficient and reliable renewable energy planning and operation.","authors":["Jingnan Wang","Jie Chao","Shangshang Yang","Congyi Nai","Kaijun Ren","Kefeng Deng","Xi Chen","Yaxin Liu","Hanqiuzi Wen","Ziniu Xiao","Lifeng Zhang","Xiaodong Wang","Jiping Guan","Baoxiang Pan"],"url":"https://arxiv.org/abs/2505.04396"}
{"created":"2025-05-08","title":"Deep residual learning with product units","abstract":"We propose a deep product-unit residual neural network (PURe) that integrates product units into residual blocks to improve the expressiveness and parameter efficiency of deep convolutional networks. Unlike standard summation neurons, product units enable multiplicative feature interactions, potentially offering a more powerful representation of complex patterns. PURe replaces conventional convolutional layers with 2D product units in the second layer of each residual block, eliminating nonlinear activation functions to preserve structural information. We validate PURe on three benchmark datasets. On Galaxy10 DECaLS, PURe34 achieves the highest test accuracy of 84.89%, surpassing the much deeper ResNet152, while converging nearly five times faster and demonstrating strong robustness to Poisson noise. On ImageNet, PURe architectures outperform standard ResNet models at similar depths, with PURe34 achieving a top-1 accuracy of 80.27% and top-5 accuracy of 95.78%, surpassing deeper ResNet variants (ResNet50, ResNet101) while utilizing significantly fewer parameters and computational resources. On CIFAR-10, PURe consistently outperforms ResNet variants across varying depths, with PURe272 reaching 95.01% test accuracy, comparable to ResNet1001 but at less than half the model size. These results demonstrate that PURe achieves a favorable balance between accuracy, efficiency, and robustness. Compared to traditional residual networks, PURe not only achieves competitive classification performance with faster convergence and fewer parameters, but also demonstrates greater robustness to noise. Its effectiveness across diverse datasets highlights the potential of product-unit-based architectures for scalable and reliable deep learning in computer vision.","authors":["Ziyuan Li","Uwe Jaekel","Babette Dellen"],"url":"https://arxiv.org/abs/2505.04397"}
{"created":"2025-05-08","title":"Blockchain Data Analytics: A Scoping Literature Review and Directions for Future Research","abstract":"Blockchain technology has rapidly expanded beyond its original use in cryptocurrencies to a broad range of applications, creating vast amounts of immutable, decentralized data. As blockchain adoption grows, so does the need for advanced data analytics techniques to extract insights for business intelligence, fraud detection, financial analysis and many more. While previous research has examined specific aspects of blockchain data analytics, such as transaction patterns, illegal activity detection, and data management, there remains a lack of comprehensive reviews that explore the full scope of blockchain data analytics. This study addresses this gap through a scoping literature review, systematically mapping the existing research landscape, identifying key topics, and highlighting emerging trends. Using established methodologies for literature reviews, we analyze 466 publications, clustering them into six major research themes: illegal activity detection, data management, financial analysis, user analysis, community detection, and mining analysis. Our findings reveal a strong focus on detecting illicit activities and financial applications, while holistic business intelligence use cases remain underexplored. This review provides a structured overview of blockchain data analytics, identifying research gaps and proposing future directions to enhance the fields impact.","authors":["Marcel B\\\"uhlmann","Hans-Georg Fill","Simon Curty"],"url":"https://arxiv.org/abs/2505.04403"}
{"created":"2025-05-08","title":"In-Context Adaptation to Concept Drift for Learned Database Operations","abstract":"Machine learning has demonstrated transformative potential for database operations, such as query optimization and in-database data analytics. However, dynamic database environments, characterized by frequent updates and evolving data distributions, introduce concept drift, which leads to performance degradation for learned models and limits their practical applicability. Addressing this challenge requires efficient frameworks capable of adapting to shifting concepts while minimizing the overhead of retraining or fine-tuning.","authors":["Jiaqi Zhu","Shaofeng Cai","Yanyan Shen","Gang Chen","Fang Deng","Beng Chin Ooi"],"url":"https://arxiv.org/abs/2505.04404"}
{"created":"2025-05-08","title":"YABLoCo: Yet Another Benchmark for Long Context Code Generation","abstract":"Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.","authors":["Aidar Valeev (Research Center of the Artificial Intelligence Institute","Innopolis University","Russia)","Roman Garaev (Research Center of the Artificial Intelligence Institute","Innopolis University","Russia)","Vadim Lomshakov (St. Petersburg Department of the Steklov Institute of Mathematics","Russia)","Irina Piontkovskaya (Huawei Noah's Ark Lab)","Vladimir Ivanov (Research Center of the Artificial Intelligence Institute","Innopolis University","Russia)","Israel Adewuyi (Research Center of the Artificial Intelligence Institute","Innopolis University","Russia)"],"url":"https://arxiv.org/abs/2505.04406"}
{"created":"2025-05-08","title":"MFSeg: Efficient Multi-frame 3D Semantic Segmentation","abstract":"We propose MFSeg, an efficient multi-frame 3D semantic segmentation framework. By aggregating point cloud sequences at the feature level and regularizing the feature extraction and aggregation process, MFSeg reduces computational overhead while maintaining high accuracy. Moreover, by employing a lightweight MLP-based point decoder, our method eliminates the need to upsample redundant points from past frames. Experiments on the nuScenes and Waymo datasets show that MFSeg outperforms existing methods, demonstrating its effectiveness and efficiency.","authors":["Chengjie Huang","Krzysztof Czarnecki"],"url":"https://arxiv.org/abs/2505.04408"}
{"created":"2025-05-08","title":"DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception","abstract":"Dense visual prediction tasks have been constrained by their reliance on predefined categories, limiting their applicability in real-world scenarios where visual concepts are unbounded. While Vision-Language Models (VLMs) like CLIP have shown promise in open-vocabulary tasks, their direct application to dense prediction often leads to suboptimal performance due to limitations in local feature representation. In this work, we present our observation that CLIP's image tokens struggle to effectively aggregate information from spatially or semantically related regions, resulting in features that lack local discriminability and spatial consistency. To address this issue, we propose DeCLIP, a novel framework that enhances CLIP by decoupling the self-attention module to obtain ``content'' and ``context'' features respectively. The ``content'' features are aligned with image crop representations to improve local discriminability, while ``context'' features learn to retain the spatial correlations under the guidance of vision foundation models, such as DINO. Extensive experiments demonstrate that DeCLIP significantly outperforms existing methods across multiple open-vocabulary dense prediction tasks, including object detection and semantic segmentation. Code is available at \\textcolor{magenta}{https://github.com/xiaomoguhz/DeCLIP}.","authors":["Junjie Wang","Bin Chen","Yulin Li","Bin Kang","Yichi Chen","Zhuotao Tian"],"url":"https://arxiv.org/abs/2505.04410"}
{"created":"2025-05-08","title":"Latent Manifold Reconstruction and Representation with Topological and Geometrical Regularization","abstract":"Manifold learning aims to discover and represent low-dimensional structures underlying high-dimensional data while preserving critical topological and geometric properties. Existing methods often fail to capture local details with global topological integrity from noisy data or construct a balanced dimensionality reduction, resulting in distorted or fractured embeddings. We present an AutoEncoder-based method that integrates a manifold reconstruction layer, which uncovers latent manifold structures from noisy point clouds, and further provides regularizations on topological and geometric properties during dimensionality reduction, whereas the two components promote each other during training. Experiments on point cloud datasets demonstrate that our method outperforms baselines like t-SNE, UMAP, and Topological AutoEncoders in discovering manifold structures from noisy data and preserving them through dimensionality reduction, as validated by visualization and quantitative metrics. This work demonstrates the significance of combining manifold reconstruction with manifold learning to achieve reliable representation of the latent manifold, particularly when dealing with noisy real-world data. Code repository: https://github.com/Thanatorika/mrtg.","authors":["Ren Wang","Pengcheng Zhou"],"url":"https://arxiv.org/abs/2505.04412"}
{"created":"2025-05-08","title":"OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models","abstract":"Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.","authors":["Xiaoyu Xu","Minxin Du","Qingqing Ye","Haibo Hu"],"url":"https://arxiv.org/abs/2505.04416"}
{"created":"2025-05-08","title":"Localized Diffusion Models for High Dimensional Distributions Generation","abstract":"Diffusion models are the state-of-the-art tools for various generative tasks. However, estimating high-dimensional score functions makes them potentially suffer from the curse of dimensionality (CoD). This underscores the importance of better understanding and exploiting low-dimensional structure in the target distribution. In this work, we consider locality structure, which describes sparse dependencies between model components. Under locality structure, the score function is effectively low-dimensional, so that it can be estimated by a localized neural network with significantly reduced sample complexity. This motivates the localized diffusion model, where a localized score matching loss is used to train the score function within a localized hypothesis space. We prove that such localization enables diffusion models to circumvent CoD, at the price of additional localization error. Under realistic sample size scaling, we show both theoretically and numerically that a moderate localization radius can balance the statistical and localization error, leading to a better overall performance. The localized structure also facilitates parallel training of diffusion models, making it potentially more efficient for large-scale applications.","authors":["Georg A. Gottwald","Shuigen Liu","Youssef Marzouk","Sebastian Reich","Xin T. Tong"],"url":"https://arxiv.org/abs/2505.04417"}
{"created":"2025-05-08","title":"LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders","abstract":"Modeling ultra-long user behavior sequences is critical for capturing both long- and short-term preferences in industrial recommender systems. Existing solutions typically rely on two-stage retrieval or indirect modeling paradigms, incuring upstream-downstream inconsistency and computational inefficiency. In this paper, we present LONGER, a Long-sequence Optimized traNsformer for GPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism for stabilizing attention over long contexts, (ii) a token merge module with lightweight InnerTransformers and hybrid attention strategy to reduce quadratic complexity, and (iii) a series of engineering optimizations, including training with mixed-precision and activation recomputation, KV cache serving, and the fully synchronous model training and serving framework for unified GPU-based dense and sparse parameter updates. LONGER consistently outperforms strong baselines in both offline metrics and online A/B testing in both advertising and e-commerce services at ByteDance, validating its consistent effectiveness and industrial-level scaling laws. Currently, LONGER has been fully deployed at more than 10 influential scenarios at ByteDance, serving billion users.","authors":["Zheng Chai","Qin Ren","Xijun Xiao","Huizhi Yang","Bo Han","Sijun Zhang","Di Chen","Hui Lu","Wenlin Zhao","Lele Yu","Xionghang Xie","Shiru Ren","Xiang Sun","Yaocheng Tan","Peng Xu","Yuchao Zheng","Di Wu"],"url":"https://arxiv.org/abs/2505.04421"}
{"created":"2025-05-08","title":"Pool Formation in Oceanic Games: Shapley Value and Proportional Sharing","abstract":"We study a game-theoretic model for pool formation in Proof of Stake blockchain protocols. In such systems, stakeholders can form pools as a means of obtaining regular rewards from participation in ledger maintenance, with the power of each pool being dependent on its collective stake. The question we are interested in is the design of mechanisms that suitably split rewards among pool members and achieve favorable properties in the resulting pool configuration. With this in mind, we initiate a non-cooperative game-theoretic analysis of the well known Shapley value scheme from cooperative game theory into the context of blockchains. In particular, we focus on the oceanic model of games, proposed by Milnor and Shapley (1978), which is suitable for populations where a small set of large players coexists with a big mass of rather small, negligible players. This provides an appropriate level of abstraction for pool formation processes among the stakeholders. We provide comparisons between the Shapley mechanism and the more standard proportional scheme, in terms of attained decentralization, via a Price of Stability analysis and in terms of susceptibility to Sybil attacks, i.e., the strategic splitting of a players' stake with the intention of participating in multiple pools for increased profit. Interestingly, while the widely deployed proportional scheme appears to have certain advantages, the Shapley value scheme, which rewards higher the most pivotal players, emerges as a competitive alternative, by being able to bypass some of the downsides of proportional sharing, while also not being far from optimal guarantees w.r.t. decentralization. Finally, we complement our study with some variations of proportional sharing, where the profit is split in proportion to a superadditive or a subadditive function of the stake, showing that the Shapley value scheme still maintains the same advantages.","authors":["Aggelos Kiayias","Elias Koutsoupias","Evangelos Markakis","Panagiotis Tsamopoulos"],"url":"https://arxiv.org/abs/2505.04422"}
{"created":"2025-05-08","title":"RLMiniStyler: Light-weight RL Style Agent for Arbitrary Sequential Neural Style Generation","abstract":"Arbitrary style transfer aims to apply the style of any given artistic image to another content image. Still, existing deep learning-based methods often require significant computational costs to generate diverse stylized results. Motivated by this, we propose a novel reinforcement learning-based framework for arbitrary style transfer RLMiniStyler. This framework leverages a unified reinforcement learning policy to iteratively guide the style transfer process by exploring and exploiting stylization feedback, generating smooth sequences of stylized results while achieving model lightweight. Furthermore, we introduce an uncertainty-aware multi-task learning strategy that automatically adjusts loss weights to adapt to the content and style balance requirements at different training stages, thereby accelerating model convergence. Through a series of experiments across image various resolutions, we have validated the advantages of RLMiniStyler over other state-of-the-art methods in generating high-quality, diverse artistic image sequences at a lower cost. Codes are available at https://github.com/fengxiaoming520/RLMiniStyler.","authors":["Jing Hu","Chengming Feng","Shu Hu","Ming-Ching Chang","Xin Li","Xi Wu","Xin Wang"],"url":"https://arxiv.org/abs/2505.04424"}
{"created":"2025-05-08","title":"An Asynchronous Distributed-Memory Parallel Algorithm for k-mer Counting","abstract":"This paper describes a new asynchronous algorithm and implementation for the problem of k-mer counting (KC), which concerns quantifying the frequency of length k substrings in a DNA sequence. This operation is common to many computational biology workloads and can take up to 77% of the total runtime of de novo genome assembly. The performance and scalability of the current state-of-the-art distributed-memory KC algorithm are hampered by multiple rounds of Many-To-Many collectives. Therefore, we develop an asynchronous algorithm (DAKC) that uses fine-grained, asynchronous messages to obviate most of this global communication while utilizing network bandwidth efficiently via custom message aggregation protocols. DAKC can perform strong scaling up to 256 nodes (512 sockets / 6K cores) and can count k-mers up to 9x faster than the state-of-the-art distributed-memory algorithm, and up to 100x faster than the shared-memory alternative. We also provide an analytical model to understand the hardware resource utilization of our asynchronous KC algorithm and provide insights on the performance.","authors":["Souvadra Hati","Akihiro Hayashi","Richard Vuduc"],"url":"https://arxiv.org/abs/2505.04431"}
{"created":"2025-05-08","title":"Improving Inclusivity for Emotion Recognition Based on Face Tracking","abstract":"The limited expressiveness of virtual user representations in Mixed Reality and Virtual Reality can inhibit an integral part of communication: emotional expression. Emotion recognition based on face tracking is often used to compensate for this. However, emotional facial expressions are highly individual, which is why many approaches have difficulties recognizing unique variations of emotional expressions. We propose several strategies to improve face tracking systems for emotion recognition with and without user intervention for the Affective Interaction Workshop at CHI '25.","authors":["Mats Ole Ellenberg","Katja Krug"],"url":"https://arxiv.org/abs/2505.04433"}
{"created":"2025-05-08","title":"Theoretical Guarantees for LT-TTD: A Unified Transformer-based Architecture for Two-Level Ranking Systems","abstract":"Modern recommendation and search systems typically employ multi-stage ranking architectures to efficiently handle billions of candidates. The conventional approach uses distinct L1 (candidate retrieval) and L2 (re-ranking) models with different optimization objectives, introducing critical limitations including irreversible error propagation and suboptimal ranking. This paper identifies and analyzes the fundamental limitations of this decoupled paradigm and proposes LT-TTD (Listwise Transformer with Two-Tower Distillation), a novel unified architecture that bridges retrieval and ranking phases. Our approach combines the computational efficiency of two-tower models with the expressivity of transformers in a unified listwise learning framework. We provide a comprehensive theoretical analysis of our architecture and establish formal guarantees regarding error propagation mitigation, ranking quality improvements, and optimization convergence. We derive theoretical bounds showing that LT-TTD reduces the upper limit on irretrievable relevant items by a factor that depends on the knowledge distillation strength, and prove that our multi-objective optimization framework achieves a provably better global optimum than disjoint training. Additionally, we analyze the computational complexity of our approach, demonstrating that the asymptotic complexity remains within practical bounds for real-world applications. We also introduce UPQE, a novel evaluation metric specifically designed for unified ranking architectures that holistically captures retrieval quality, ranking performance, and computational efficiency.","authors":["Ayoub Abraich"],"url":"https://arxiv.org/abs/2505.04434"}
{"created":"2025-05-08","title":"FedBWO: Enhancing Communication Efficiency in Federated Learning","abstract":"Federated Learning (FL) is a distributed Machine Learning (ML) setup, where a shared model is collaboratively trained by various clients using their local datasets while keeping the data private. Considering resource-constrained devices, FL clients often suffer from restricted transmission capacity. Aiming to enhance the system performance, the communication between clients and server needs to be diminished. Current FL strategies transmit a tremendous amount of data (model weights) within the FL process, which needs a high communication bandwidth. Considering resource constraints, increasing the number of clients and, consequently, the amount of data (model weights) can lead to a bottleneck. In this paper, we introduce the Federated Black Widow Optimization (FedBWO) technique to decrease the amount of transmitted data by transmitting only a performance score rather than the local model weights from clients. FedBWO employs the BWO algorithm to improve local model updates. The conducted experiments prove that FedBWO remarkably improves the performance of the global model and the communication efficiency of the overall system. According to the experimental outcomes, FedBWO enhances the global model accuracy by an average of 21% over FedAvg, and 12% over FedGWO. Furthermore, FedBWO dramatically decreases the communication cost compared to other methods.","authors":["Vahideh Hayyolalam","\\\"Oznur \\\"Ozkasap"],"url":"https://arxiv.org/abs/2505.04435"}
{"created":"2025-05-08","title":"Do We Still Need to Work on Odometry for Autonomous Driving?","abstract":"Over the past decades, a tremendous amount of work has addressed the topic of ego-motion estimation of moving platforms based on various proprioceptive and exteroceptive sensors. At the cost of ever-increasing computational load and sensor complexity, odometry algorithms have reached impressive levels of accuracy with minimal drift in various conditions. In this paper, we question the need for more research on odometry for autonomous driving by assessing the accuracy of one of the simplest algorithms: the direct integration of wheel encoder data and yaw rate measurements from a gyroscope. We denote this algorithm as Odometer-Gyroscope (OG) odometry. This work shows that OG odometry can outperform current state-of-the-art radar-inertial SE(2) odometry for a fraction of the computational cost in most scenarios. For example, the OG odometry is on top of the Boreas leaderboard with a relative translation error of 0.20%, while the second-best method displays an error of 0.26%. Lidar-inertial approaches can provide more accurate estimates, but the computational load is three orders of magnitude higher than the OG odometry. To further the analysis, we have pushed the limits of the OG odometry by purposely violating its fundamental no-slip assumption using data collected during a heavy snowstorm with different driving behaviours. Our conclusion shows that a significant amount of slippage is required to result in non-satisfactory pose estimates from the OG odometry.","authors":["Cedric Le Gentil","Daniil Lisus","Timothy D. Barfoot"],"url":"https://arxiv.org/abs/2505.04438"}
{"created":"2025-05-08","title":"Towards Initialization-Agnostic Clustering with Iterative Adaptive Resonance Theory","abstract":"The clustering performance of Fuzzy Adaptive Resonance Theory (Fuzzy ART) is highly dependent on the preset vigilance parameter, where deviations in its value can lead to significant fluctuations in clustering results, severely limiting its practicality for non-expert users. Existing approaches generally enhance vigilance parameter robustness through adaptive mechanisms such as particle swarm optimization and fuzzy logic rules. However, they often introduce additional hyperparameters or complex frameworks that contradict the original simplicity of the algorithm. To address this, we propose Iterative Refinement Adaptive Resonance Theory (IR-ART), which integrates three key phases into a unified iterative framework: (1) Cluster Stability Detection: A dynamic stability detection module that identifies unstable clusters by analyzing the change of sample size (number of samples in the cluster) in iteration. (2) Unstable Cluster Deletion: An evolutionary pruning module that eliminates low-quality clusters. (3) Vigilance Region Expansion: A vigilance region expansion mechanism that adaptively adjusts similarity thresholds. Independent of the specific execution of clustering, these three phases sequentially focus on analyzing the implicit knowledge within the iterative process, adjusting weights and vigilance parameters, thereby laying a foundation for the next iteration. Experimental evaluation on 15 datasets demonstrates that IR-ART improves tolerance to suboptimal vigilance parameter values while preserving the parameter simplicity of Fuzzy ART. Case studies visually confirm the algorithm's self-optimization capability through iterative refinement, making it particularly suitable for non-expert users in resource-constrained scenarios.","authors":["Xiaozheng Qu","Zhaochuan Li","Zhuang Qi","Xiang Li","Haibei Huang","Lei Meng","Xiangxu Meng"],"url":"https://arxiv.org/abs/2505.04440"}
{"created":"2025-05-08","title":"Towards Effectively Leveraging Execution Traces for Program Repair with Code LLMs","abstract":"Large Language Models (LLMs) show promising performance on various programming tasks, including Automatic Program Repair (APR). However, most approaches to LLM-based APR are limited to the static analysis of the programs, while disregarding their runtime behavior. Inspired by knowledge-augmented NLP, in this work, we aim to remedy this potential blind spot by augmenting standard APR prompts with program execution traces. We evaluate our approach using the GPT family of models on three popular APR datasets. Our findings suggest that simply incorporating execution traces into the prompt provides a limited performance improvement over trace-free baselines, in only 2 out of 6 tested dataset / model configurations. We further find that the effectiveness of execution traces for APR diminishes as their complexity increases. We explore several strategies for leveraging traces in prompts and demonstrate that LLM-optimized prompts help outperform trace-free prompts more consistently. Additionally, we show trace-based prompting to be superior to finetuning a smaller LLM on a small-scale dataset; and conduct probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs.","authors":["Mirazul Haque","Petr Babkin","Farima Farmahinifarahani","Manuela Veloso"],"url":"https://arxiv.org/abs/2505.04441"}
{"created":"2025-05-08","title":"M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation","abstract":"Sequential recommendation systems aim to predict users' next preferences based on their interaction histories, but existing approaches face critical limitations in efficiency and multi-scale pattern recognition. While Transformer-based methods struggle with quadratic computational complexity, recent Mamba-based models improve efficiency but fail to capture periodic user behaviors, leverage rich semantic information, or effectively fuse multimodal features. To address these challenges, we propose \\model, a novel sequential recommendation framework that integrates multi-scale Mamba with Fourier analysis, Large Language Models (LLMs), and adaptive gating. First, we enhance Mamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns in the frequency domain, separating meaningful trends from noise. Second, we incorporate LLM-based text embeddings to enrich sparse interaction data with semantic context from item descriptions. Finally, we introduce a learnable gate mechanism to dynamically balance temporal (Mamba), frequency (FFT), and semantic (LLM) features, ensuring harmonious multimodal fusion. Extensive experiments demonstrate that \\model\\ achieves state-of-the-art performance, improving Hit Rate@10 by 3.2\\% over existing Mamba-based models while maintaining 20\\% faster inference than Transformer baselines. Our results highlight the effectiveness of combining frequency analysis, semantic understanding, and adaptive fusion for sequential recommendation. Code and datasets are available at: https://anonymous.4open.science/r/M2Rec.","authors":["Qianru Zhang","Liang Qu","Honggang Wen","Dong Huang","Siu-Ming Yiu","Nguyen Quoc Viet Hung","Hongzhi Yin"],"url":"https://arxiv.org/abs/2505.04445"}
{"created":"2025-05-08","title":"Practice Support for Violin Bowing by Measuring Bow Pressure and Position","abstract":"The violin is one of the most popular musical instruments. Various parameters of bowing motion, such as pressure, position, and speed, are crucial for producing a beautiful tone. However, mastering them is challenging and requires extensive practice. In this study, we aimed to support practice of bowing, focusing on bow pressure. First, we compared the bowing movements, specifically bow pressure, bow position, and bow speed, of eight experienced players with those of eight beginners. Next, we developed and evaluated a visual feedback system that displays bow pressure to support practice. We taught the identified differences to 14 beginners, dividing them into two groups: one practiced with an explanation, and the other with both an explanation and a feedback system. These two experiments found that clarifying the characteristics unique to experienced players can support practice.","authors":["Yurina Mizuho","Yuta Sugiura"],"url":"https://arxiv.org/abs/2505.04446"}
{"created":"2025-05-08","title":"Automatic Music Transcription using Convolutional Neural Networks and Constant-Q transform","abstract":"Automatic music transcription (AMT) is the problem of analyzing an audio recording of a musical piece and detecting notes that are being played. AMT is a challenging problem, particularly when it comes to polyphonic music. The goal of AMT is to produce a score representation of a music piece, by analyzing a sound signal containing multiple notes played simultaneously. In this work, we design a processing pipeline that can transform classical piano audio files in .wav format into a music score representation. The features from the audio signals are extracted using the constant-Q transform, and the resulting coefficients are used as an input to the convolutional neural network (CNN) model.","authors":["Yohannis Telila","Tommaso Cucinotta","Davide Bacciu"],"url":"https://arxiv.org/abs/2505.04451"}
{"created":"2025-05-08","title":"Miipher-2: A Universal Speech Restoration Model for Million-Hour Scale Data Restoration","abstract":"Training data cleaning is a new application for generative model-based speech restoration (SR). This paper introduces Miipher-2, an SR model designed for million-hour scale data, for training data cleaning for large-scale generative models like large language models. Key challenges addressed include generalization to unseen languages, operation without explicit conditioning (e.g., text, speaker ID), and computational efficiency. Miipher-2 utilizes a frozen, pre-trained Universal Speech Model (USM), supporting over 300 languages, as a robust, conditioning-free feature extractor. To optimize efficiency and minimize memory, Miipher-2 incorporates parallel adapters for predicting clean USM features from noisy inputs and employs the WaneFit neural vocoder for waveform synthesis. These components were trained on 3,000 hours of multi-lingual, studio-quality recordings with augmented degradations, while USM parameters remained fixed. Experimental results demonstrate Miipher-2's superior or comparable performance to conventional SR models in word-error-rate, speaker similarity, and both objective and subjective sound quality scores across all tested languages. Miipher-2 operates efficiently on consumer-grade accelerators, achieving a real-time factor of 0.0078, enabling the processing of a million-hour speech dataset in approximately three days using only 100 such accelerators.","authors":["Shigeki Karita","Yuma Koizumi","Heiga Zen","Haruko Ishikawa","Robin Scheibler","Michiel Bacchiani"],"url":"https://arxiv.org/abs/2505.04457"}
{"created":"2025-05-08","title":"Learning Real Facial Concepts for Independent Deepfake Detection","abstract":"Deepfake detection models often struggle with generalization to unseen datasets, manifesting as misclassifying real instances as fake in target domains. This is primarily due to an overreliance on forgery artifacts and a limited understanding of real faces. To address this challenge, we propose a novel approach RealID to enhance generalization by learning a comprehensive concept of real faces while assessing the probabilities of belonging to the real and fake classes independently. RealID comprises two key modules: the Real Concept Capture Module (RealC2) and the Independent Dual-Decision Classifier (IDC). With the assistance of a MultiReal Memory, RealC2 maintains various prototypes for real faces, allowing the model to capture a comprehensive concept of real class. Meanwhile, IDC redefines the classification strategy by making independent decisions based on the concept of the real class and the presence of forgery artifacts. Through the combined effect of the above modules, the influence of forgery-irrelevant patterns is alleviated, and extensive experiments on five widely used datasets demonstrate that RealID significantly outperforms existing state-of-the-art methods, achieving a 1.74% improvement in average accuracy.","authors":["Ming-Hui Liu","Harry Cheng","Tianyi Wang","Xin Luo","Xin-Shun Xu"],"url":"https://arxiv.org/abs/2505.04460"}
{"created":"2025-05-08","title":"A Survey on Temporal Interaction Graph Representation Learning: Progress, Challenges, and Opportunities","abstract":"Temporal interaction graphs (TIGs), defined by sequences of timestamped interaction events, have become ubiquitous in real-world applications due to their capability to model complex dynamic system behaviors. As a result, temporal interaction graph representation learning (TIGRL) has garnered significant attention in recent years. TIGRL aims to embed nodes in TIGs into low-dimensional representations that effectively preserve both structural and temporal information, thereby enhancing the performance of downstream tasks such as classification, prediction, and clustering within constantly evolving data environments. In this paper, we begin by introducing the foundational concepts of TIGs and emphasize the critical role of temporal dependencies. We then propose a comprehensive taxonomy of state-of-the-art TIGRL methods, systematically categorizing them based on the types of information utilized during the learning process to address the unique challenges inherent to TIGs. To facilitate further research and practical applications, we curate the source of datasets and benchmarks, providing valuable resources for empirical investigations. Finally, we examine key open challenges and explore promising research directions in TIGRL, laying the groundwork for future advancements that have the potential to shape the evolution of this field.","authors":["Pengfei Jiao","Hongjiang Chen","Xuan Guo","Zhidong Zhao","Dongxiao He","Di Jin"],"url":"https://arxiv.org/abs/2505.04461"}
{"created":"2025-05-08","title":"Discriminative Ordering Through Ensemble Consensus","abstract":"Evaluating the performance of clustering models is a challenging task where the outcome depends on the definition of what constitutes a cluster. Due to this design, current existing metrics rarely handle multiple clustering models with diverse cluster definitions, nor do they comply with the integration of constraints when available. In this work, we take inspiration from consensus clustering and assume that a set of clustering models is able to uncover hidden structures in the data. We propose to construct a discriminative ordering through ensemble clustering based on the distance between the connectivity of a clustering model and the consensus matrix. We first validate the proposed method with synthetic scenarios, highlighting that the proposed score ranks the models that best match the consensus first. We then show that this simple ranking score significantly outperforms other scoring methods when comparing sets of different clustering algorithms that are not restricted to a fixed number of clusters and is compatible with clustering constraints.","authors":["Louis Ohl","Fredrik Lindsten"],"url":"https://arxiv.org/abs/2505.04464"}
{"created":"2025-05-08","title":"Securing Immersive 360 Video Streams through Attribute-Based Selective Encryption","abstract":"Delivering high-quality, secure 360{\\deg} video content introduces unique challenges, primarily due to the high bitrates and interactive demands of immersive media. Traditional HTTPS-based methods, although widely used, face limitations in computational efficiency and scalability when securing these high-resolution streams. To address these issues, this paper proposes a novel framework integrating Attribute-Based Encryption (ABE) with selective encryption techniques tailored specifically for tiled 360{\\deg} video streaming. Our approach employs selective encryption of frames at varying levels to reduce computational overhead while ensuring robust protection against unauthorized access.","authors":["Mohammad Waquas Usmani","Susmit Shannigrahi","Michael Zink"],"url":"https://arxiv.org/abs/2505.04466"}
{"created":"2025-05-08","title":"Spectral and Temporal Denoising for Differentially Private Optimization","abstract":"This paper introduces the FFT-Enhanced Kalman Filter (FFTKF), a differentially private optimization method that addresses the challenge of preserving performance in DP-SGD, where added noise typically degrades model utility. FFTKF integrates frequency-domain noise shaping with Kalman filtering to enhance gradient quality while preserving $(\\varepsilon, \\delta)$-DP guarantees. It employs a high-frequency shaping mask in the Fourier domain to concentrate differential privacy noise in less informative spectral components, preserving low-frequency gradient signals. A scalar-gain Kalman filter with finite-difference Hessian approximation further refines the denoised gradients. With a per-iteration complexity of $\\mathcal{O}(d \\log d)$, FFTKF demonstrates improved test accuracy over DP-SGD and DiSK across MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets using CNNs, Wide ResNets, and Vision Transformers. Theoretical analysis confirms that FFTKF maintains equivalent privacy guarantees while achieving a tighter privacy-utility trade-off through reduced noise and controlled bias.","authors":["Hyeju Shin","Kyudan Jung","Seongwon Yun","Juyoung Yun"],"url":"https://arxiv.org/abs/2505.04468"}
{"created":"2025-05-08","title":"Hamiltonian Normalizing Flows as kinetic PDE solvers: application to the 1D Vlasov-Poisson Equations","abstract":"Many conservative physical systems can be described using the Hamiltonian formalism. A notable example is the Vlasov-Poisson equations, a set of partial differential equations that govern the time evolution of a phase-space density function representing collisionless particles under a self-consistent potential. These equations play a central role in both plasma physics and cosmology. Due to the complexity of the potential involved, analytical solutions are rarely available, necessitating the use of numerical methods such as Particle-In-Cell. In this work, we introduce a novel approach based on Hamiltonian-informed Normalizing Flows, specifically a variant of Fixed-Kinetic Neural Hamiltonian Flows. Our method transforms an initial Gaussian distribution in phase space into the final distribution using a sequence of invertible, volume-preserving transformations derived from Hamiltonian dynamics. The model is trained on a dataset comprising initial and final states at a fixed time T, generated via numerical simulations. After training, the model enables fast sampling of the final distribution from any given initial state. Moreover, by automatically learning an interpretable physical potential, it can generalize to intermediate states not seen during training, offering insights into the system's evolution across time.","authors":["Vincent Souveton","S\\'ebastien Terrana"],"url":"https://arxiv.org/abs/2505.04471"}
{"created":"2025-05-08","title":"Opinion Dynamics on Signed Graphs and Graphons","abstract":"In this paper, we make use of graphon theory to study opinion dynamics on large undirected networks. The opinion dynamics models that we take into consideration allow for negative interactions between the individuals, whose opinions can thus grow apart. We consider both the repelling and the opposing models of negative interactions, which have been studied in the literature. We define the repelling and the opposing dynamics on signed graphons and we show that their initial value problem solutions exist and are unique. We then show that, in a suitable sense, the graphon dynamics is a good approximation of the dynamics on large graphs that converge to a graphon. This result applies to large random graphs that are sampled according to a graphon (W-random graphs), for which we provide a new convergence result under very general assumptions.","authors":["Raoul Prisant","Federica Garin","Paolo Frasca"],"url":"https://arxiv.org/abs/2505.04472"}
{"created":"2025-05-08","title":"TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution","abstract":"Trajectory prediction is a crucial task in modeling human behavior, especially in fields as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy, while recently proposed deep learning approaches suffer from computational cost, lack of explainability, and generalization issues that limit their practical adoption. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We introduce a Cross-Generation Elite Sampling to promote population diversity and a Statistics Feedback Loop allowing the LLM to analyze alternative predictions. Our evaluations show TrajEvo outperforms previous heuristic methods on the ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning methods when generalizing to the unseen SDD dataset. TrajEvo represents a first step toward automated design of fast, explainable, and generalizable trajectory prediction heuristics. We make our source code publicly available to foster future research at https://github.com/ai4co/trajevo.","authors":["Zhikai Zhao","Chuanbo Hua","Federico Berto","Kanghoon Lee","Zihan Ma","Jiachen Li","Jinkyoo Park"],"url":"https://arxiv.org/abs/2505.04480"}
{"created":"2025-05-08","title":"CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation","abstract":"Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines.","authors":["Jiahao Li","Weijian Ma","Xueyang Li","Yunzhong Lou","Guichun Zhou","Xiangdong Zhou"],"url":"https://arxiv.org/abs/2505.04481"}
{"created":"2025-05-08","title":"FA-KPConv: Introducing Euclidean Symmetries to KPConv via Frame Averaging","abstract":"We present Frame-Averaging Kernel-Point Convolution (FA-KPConv), a neural network architecture built on top of the well-known KPConv, a widely adopted backbone for 3D point cloud analysis. Even though invariance and/or equivariance to Euclidean transformations are required for many common tasks, KPConv-based networks can only approximately achieve such properties when training on large datasets or with significant data augmentations. Using Frame Averaging, we allow to flexibly customize point cloud neural networks built with KPConv layers, by making them exactly invariant and/or equivariant to translations, rotations and/or reflections of the input point clouds. By simply wrapping around an existing KPConv-based network, FA-KPConv embeds geometrical prior knowledge into it while preserving the number of learnable parameters and not compromising any input information. We showcase the benefit of such an introduced bias for point cloud classification and point cloud registration, especially in challenging cases such as scarce training data or randomly rotated test data.","authors":["Ali Alawieh (Robert Bosch GmbH - ADAS Systems","Software and Services","University of L\\\"ubeck - Institute for Signal Processing)","Alexandru P. Condurache (Robert Bosch GmbH - ADAS Systems","Software and Services","University of L\\\"ubeck - Institute for Signal Processing)"],"url":"https://arxiv.org/abs/2505.04485"}
{"created":"2025-05-08","title":"Efficient Flow Matching using Latent Variables","abstract":"Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \\texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \\texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\\sim 50\\%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features.","authors":["Anirban Samaddar","Yixuan Sun","Viktor Nilsson","Sandeep Madireddy"],"url":"https://arxiv.org/abs/2505.04486"}
{"created":"2025-05-08","title":"A Design Space for the Critical Validation of LLM-Generated Tabular Data","abstract":"LLM-generated tabular data is creating new opportunities for data-driven applications in academia, business, and society. To leverage benefits like missing value imputation, labeling, and enrichment with context-aware attributes, LLM-generated data needs a critical validation process. The number of pioneering approaches is increasing fast, opening a promising validation space that, so far, remains unstructured. We present a design space for the critical validation of LLM-generated tabular data with two dimensions: First, the Analysis Granularity dimension- from within-attribute (single-item and multi-item) to across- attribute perspectives (1 x 1, 1 x m, and n x n). Second, the Data Source dimension- differentiating between LLM-generated values, ground truth values, explanations, and their combinations. We discuss analysis tasks for each dimension cross-cut, map 19 existing validation approaches, and discuss the characteristics of two approaches in detail, demonstrating descriptive power.","authors":["Madhav Sachdeva","Christopher Narayanan","Marvin Wiedenkeller","Jana Sedlakova","J\\\"urgen Bernard"],"url":"https://arxiv.org/abs/2505.04487"}
{"created":"2025-05-08","title":"\"I Can See Forever!\": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments","abstract":"The visually impaired population, especially the severely visually impaired, is currently large in scale, and daily activities pose significant challenges for them. Although many studies use large language and vision-language models to assist the blind, most focus on static content and fail to meet real-time perception needs in dynamic and complex environments, such as daily activities. To provide them with more effective intelligent assistance, it is imperative to incorporate advanced visual understanding technologies. Although real-time vision and speech interaction VideoLLMs demonstrate strong real-time visual understanding, no prior work has systematically evaluated their effectiveness in assisting visually impaired individuals. In this work, we conduct the first such evaluation. First, we construct a benchmark dataset (VisAssistDaily), covering three categories of assistive tasks for visually impaired individuals: Basic Skills, Home Life Tasks, and Social Life Tasks. The results show that GPT-4o achieves the highest task success rate. Next, we conduct a user study to evaluate the models in both closed-world and open-world scenarios, further exploring the practical challenges of applying VideoLLMs in assistive contexts. One key issue we identify is the difficulty current models face in perceiving potential hazards in dynamic environments. To address this, we build an environment-awareness dataset named SafeVid and introduce a polling mechanism that enables the model to proactively detect environmental risks. We hope this work provides valuable insights and inspiration for future research in this field.","authors":["Ziyi Zhang","Zhen Sun","Zongmin Zhang","Zifan Peng","Yuemeng Zhao","Zichun Wang","Zeren Luo","Ruiting Zuo","Xinlei He"],"url":"https://arxiv.org/abs/2505.04488"}
{"created":"2025-05-08","title":"Estimating Dynamic Soft Continuum Robot States From Boundaries","abstract":"Accurate state estimation is essential for effective control of robots. For soft robots, this task is particularly challenging because their states are inherently infinite-dimensional functions due to the robots' continuous deformability. Traditional sensing techniques, however, can only provide discrete measurements. Recently, a dynamic state estimation method known as a boundary observer was introduced, which leverages Cosserat rod theory to recover all infinite-dimensional states by measuring only the velocity twist at the robot's tip. In this work, we present a novel boundary observer that can also recover infinite-dimensional dynamic states, but instead relies on measuring the internal wrench at the robot's base. This design exploits the duality between the velocity twist at the tip and the internal wrench at the base, with both types of boundary observers being inspired by principles of energy dissipation. Despite the mathematical duality, the proposed approach offers a distinct advantage: it requires only a 6-axis force/torque sensor embedded at the base, eliminating the need for external sensing systems such as motion capture cameras. Moreover, combining both tip- and base-based techniques enhances energy dissipation, accelerates convergence, and improves estimation accuracy. We validate the proposed algorithms through both simulation studies and experiments based on tendon-driven continuum robots. Our results demonstrate that all boundary observers converge to the ground truth within 3 seconds, even with significantly deviated initial conditions. Furthermore, they recover from unknown perturbations and effectively track high-frequency vibrations. We also show that combining the dual techniques further improves convergence speed and accuracy. Finally, the computational efficiency of these algorithms indicates their feasibility for real-time state estimation.","authors":["Tongjia Zheng","Jessica Burgner-Kahrs"],"url":"https://arxiv.org/abs/2505.04491"}
{"created":"2025-05-08","title":"Model-Based AI planning and Execution Systems for Robotics","abstract":"Model-based planning and execution systems offer a principled approach to building flexible autonomous robots that can perform diverse tasks by automatically combining a host of basic skills. This idea is almost as old as modern robotics. Yet, while diverse general-purpose reasoning architectures have been proposed since, general-purpose systems that are integrated with modern robotic platforms have emerged only recently, starting with the influential ROSPlan system. Since then, a growing number of model-based systems for robot task-level control have emerged. In this paper, we consider the diverse design choices and issues existing systems attempt to address, the different solutions proposed so far, and suggest avenues for future development.","authors":["Or Wertheim","Ronen I. Brafman"],"url":"https://arxiv.org/abs/2505.04493"}
{"created":"2025-05-08","title":"Defining and Quantifying Creative Behavior in Popular Image Generators","abstract":"Creativity of generative AI models has been a subject of scientific debate in the last years, without a conclusive answer. In this paper, we study creativity from a practical perspective and introduce quantitative measures that help the user to choose a suitable AI model for a given task. We evaluated our measures on a number of popular image-to-image generation models, and the results of this suggest that our measures conform to human intuition.","authors":["Aditi Ramaswamy"],"url":"https://arxiv.org/abs/2505.04497"}
{"created":"2025-05-08","title":"Uncovering Key Features for Model-Driven Engineering of Complex Performance Indicators: A Scoping Review","abstract":"This paper addresses challenges of designing and managing Complex Performance Indicators (CPI), which amalgamate individual indicators to measure latent, yet crucial business factors like customer satisfaction or sustainability indices. Despite their significant value, designing and managing CPI is intricate; they evolve with rapidly changing business contexts and present comprehension and explanation challenges for end-users. Model-Driven Engineering (MDE) emerges as a potent solution to overcome these hurdles and ensure CPI adoption, though its application to CPI remains an understudied research area. While prior efforts targeted specific CPI modeling objectives, a comprehensive overview of literature advancements is lacking. This study addresses this gap by conducting a scoping review yielding dual outcomes: (1) a comprehensive mapping of modeling features in the literature and (2) a comparative analysis of the coverage offered by the modeling frameworks. These outcomes enhance CPI understanding in academic and practitioner circles and offer insights for future MDE CPI advancements.","authors":["Benito Giunta","Corentin Burnay"],"url":"https://arxiv.org/abs/2505.04498"}
{"created":"2025-05-08","title":"VeriFast's separation logic: a higher-order(ish) logic without laters for modular verification of fine-grained concurrent programs","abstract":"VeriFast is one of the leading tools for semi-automated modular formal program verification. A central feature of VeriFast is its support for higher-order ghost code, which enables its support for expressively specifying fine-grained concurrent modules, without the need for a later modality. We present the first formalization and soundness proof for this aspect of VeriFast's logic.","authors":["Bart Jacobs"],"url":"https://arxiv.org/abs/2505.04500"}
{"created":"2025-05-08","title":"Leveraging Simultaneous Usage of Edge GPU Hardware Engines for Video Face Detection and Recognition","abstract":"Video face detection and recognition in public places at the edge is required in several applications, such as security reinforcement and contactless access to authorized venues. This paper aims to maximize the simultaneous usage of hardware engines available in edge GPUs nowadays by leveraging the concurrency and pipelining of tasks required for face detection and recognition. This also includes the video decoding task, which is required in most face monitoring applications as the video streams are usually carried via Gbps Ethernet network. This constitutes an improvement over previous works where the tasks are usually allocated to a single engine due to the lack of a unified and automated framework that simultaneously explores all hardware engines. In addition, previously, the input faces were usually embedded in still images or within raw video streams that overlook the burst delay caused by the decoding stage. The results on real-life video streams suggest that simultaneously using all the hardware engines available in the recent NVIDIA edge Orin GPU, higher throughput, and a slight saving of power consumption of around 300 mW, accounting for around 5%, have been achieved while satisfying the real-time performance constraint. The performance gets even higher by considering several video streams simultaneously. Further performance improvement could have been obtained if the number of shuffle layers that were created by the tensor RT framework for the face recognition task was lower. Thus, the paper suggests some hardware improvements to the existing edge GPU processors to enhance their performance even higher.","authors":["Asma Baobaid","Mahmoud Meribout"],"url":"https://arxiv.org/abs/2505.04502"}
{"created":"2025-05-08","title":"Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts","abstract":"The quality of natural language texts in fine-tuning datasets plays a critical role in the performance of generative models, particularly in computational creativity tasks such as poem or song lyric generation. Fluency defects in generated poems significantly reduce their value. However, training texts are often sourced from internet-based platforms without stringent quality control, posing a challenge for data engineers to manage defect levels effectively.","authors":["Ilya Koziev"],"url":"https://arxiv.org/abs/2505.04507"}
{"created":"2025-05-08","title":"HunyuanCustom: A Multimodal-Driven Architecture for Customized Video Generation","abstract":"Customized video generation aims to produce videos featuring specific subjects under flexible user-defined conditions, yet existing methods often struggle with identity consistency and limited input modalities. In this paper, we propose HunyuanCustom, a multi-modal customized video generation framework that emphasizes subject consistency while supporting image, audio, video, and text conditions. Built upon HunyuanVideo, our model first addresses the image-text conditioned generation task by introducing a text-image fusion module based on LLaVA for enhanced multi-modal understanding, along with an image ID enhancement module that leverages temporal concatenation to reinforce identity features across frames. To enable audio- and video-conditioned generation, we further propose modality-specific condition injection mechanisms: an AudioNet module that achieves hierarchical alignment via spatial cross-attention, and a video-driven injection module that integrates latent-compressed conditional video through a patchify-based feature-alignment network. Extensive experiments on single- and multi-subject scenarios demonstrate that HunyuanCustom significantly outperforms state-of-the-art open- and closed-source methods in terms of ID consistency, realism, and text-video alignment. Moreover, we validate its robustness across downstream tasks, including audio and video-driven customized video generation. Our results highlight the effectiveness of multi-modal conditioning and identity-preserving strategies in advancing controllable video generation. All the code and models are available at https://hunyuancustom.github.io.","authors":["Teng Hu","Zhentao Yu","Zhengguang Zhou","Sen Liang","Yuan Zhou","Qin Lin","Qinglin Lu"],"url":"https://arxiv.org/abs/2505.04512"}
{"created":"2025-05-08","title":"User and Recommender Behavior Over Time: Contextualizing Activity, Effectiveness, Diversity, and Fairness in Book Recommendation","abstract":"Data is an essential resource for studying recommender systems. While there has been significant work on improving and evaluating state-of-the-art models and measuring various properties of recommender system outputs, less attention has been given to the data itself, particularly how data has changed over time. Such documentation and analysis provide guidance and context for designing and evaluating recommender systems, particularly for evaluation designs making use of time (e.g., temporal splitting). In this paper, we present a temporal explanatory analysis of the UCSD Book Graph dataset scraped from Goodreads, a social reading and recommendation platform active since 2006. We measure the book interaction data using a set of activity, diversity, and fairness metrics; we then train a set of collaborative filtering algorithms on rolling training windows to observe how the same measures evolve over time in the recommendations. Additionally, we explore whether the introduction of algorithmic recommendations in 2011 was followed by observable changes in user or recommender system behavior.","authors":["Samira Vaez Barenji","Sushobhan Parajuli","Michael D. Ekstrand"],"url":"https://arxiv.org/abs/2505.04518"}
{"created":"2025-05-08","title":"Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs","abstract":"Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.","authors":["Yehui Tang","Yichun Yin","Yaoyuan Wang","Hang Zhou","Yu Pan","Wei Guo","Ziyang Zhang","Miao Rang","Fangcheng Liu","Naifu Zhang","Binghan Li","Yonghan Dong","Xiaojun Meng","Yasheng Wang","Dong Li","Yin Li","Dandan Tu","Can Chen","Youliang Yan","Fisher Yu","Ruiming Tang","Yunhe Wang","Botian Huang","Bo Wang","Boxiao Liu","Changzheng Zhang","Da Kuang","Fei Liu","Gang Huang","Jiansheng Wei","Jiarui Qin","Jie Ran","Jinpeng Li","Jun Zhao","Liang Dai","Lin Li","Liqun Deng","Peifeng Qin","Pengyuan Zeng","Qiang Gu","Shaohua Tang","Shengjun Cheng","Tao Gao","Tao Yu","Tianshu Li","Tianyu Bi","Wei He","Weikai Mao","Wenyong Huang","Wulong Liu","Xiabing Li","Xianzhi Yu","Xueyu Wu","Xu He","Yangkai Du","Yan Xu","Ye Tian","Yimeng Wu","Yongbing Huang","Yong Tian","Yong Zhu","Yue Li","Yufei Wang","Yuhang Gai","Yujun Li","Yu Luo","Yunsheng Ni","Yusen Sun","Zelin Chen","Zhe Liu","Zhicheng Liu","Zhipeng Tu","Zilin Ding","Zongyuan Zhan"],"url":"https://arxiv.org/abs/2505.04519"}
{"created":"2025-05-08","title":"Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code Development","abstract":"Large Language Models (LLM) have significantly transformed various domains, including software development. These models assist programmers in generating code, potentially increasing productivity and efficiency. However, the environmental impact of utilising these AI models is substantial, given their high energy consumption during both training and inference stages. This research aims to compare the energy consumption of manual software development versus an LLM-assisted approach, using Codeforces as a simulation platform for software development. The goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using LLM in software development. Our results show that the LLM-assisted code generation leads on average to 32.72 higher carbon footprint than the manual one. Moreover, there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches.","authors":["Kuen Sum Cheung","Mayuri Kaul","Gunel Jahangirova","Mohammad Reza Mousavi","Eric Zie"],"url":"https://arxiv.org/abs/2505.04521"}
{"created":"2025-05-08","title":"Text2CT: Towards 3D CT Volume Generation from Free-text Descriptions Using Diffusion Model","abstract":"Generating 3D CT volumes from descriptive free-text inputs presents a transformative opportunity in diagnostics and research. In this paper, we introduce Text2CT, a novel approach for synthesizing 3D CT volumes from textual descriptions using the diffusion model. Unlike previous methods that rely on fixed-format text input, Text2CT employs a novel prompt formulation that enables generation from diverse, free-text descriptions. The proposed framework encodes medical text into latent representations and decodes them into high-resolution 3D CT scans, effectively bridging the gap between semantic text inputs and detailed volumetric representations in a unified 3D framework. Our method demonstrates superior performance in preserving anatomical fidelity and capturing intricate structures as described in the input text. Extensive evaluations show that our approach achieves state-of-the-art results, offering promising potential applications in diagnostics, and data augmentation.","authors":["Pengfei Guo","Can Zhao","Dong Yang","Yufan He","Vishwesh Nath","Ziyue Xu","Pedro R. A. S. Bassi","Zongwei Zhou","Benjamin D. Simon","Stephanie Anne Harmon","Baris Turkbey","Daguang Xu"],"url":"https://arxiv.org/abs/2505.04522"}
{"created":"2025-05-08","title":"Edge-GPU Based Face Tracking for Face Detection and Recognition Acceleration","abstract":"Cost-effective machine vision systems dedicated to real-time and accurate face detection and recognition in public places are crucial for many modern applications. However, despite their high performance, which could be reached using specialized edge or cloud AI hardware accelerators, there is still room for improvement in throughput and power consumption. This paper aims to suggest a combined hardware-software approach that optimizes face detection and recognition systems on one of the latest edge GPUs, namely NVIDIA Jetson AGX Orin. First, it leverages the simultaneous usage of all its hardware engines to improve processing time. This offers an improvement over previous works where these tasks were mainly allocated automatically and exclusively to the CPU or, to a higher extent, to the GPU core. Additionally, the paper suggests integrating a face tracker module to avoid redundantly running the face recognition algorithm for every frame but only when a new face appears in the scene. The results of extended experiments suggest that simultaneous usage of all the hardware engines that are available in the Orin GPU and tracker integration into the pipeline yield an impressive throughput of 290 FPS (frames per second) on 1920 x 1080 input size frames containing in average of 6 faces/frame. Additionally, a substantial saving of power consumption of around 800 mW was achieved when compared to running the task on the CPU/GPU engines only and without integrating a tracker into the Orin GPU\\'92s pipeline. This hardware-codesign approach can pave the way to design high-performance machine vision systems at the edge, critically needed in video monitoring in public places where several nearby cameras are usually deployed for a same scene.","authors":["Asma Baobaid","Mahmoud Meribout"],"url":"https://arxiv.org/abs/2505.04524"}
{"created":"2025-05-08","title":"On some improvements to Unbounded Minimax","abstract":"This paper presents the first experimental evaluation of four previously untested modifications of Unbounded Best-First Minimax algorithm. This algorithm explores the game tree by iteratively expanding the most promising sequences of actions based on the current partial game tree. We first evaluate the use of transposition tables, which convert the game tree into a directed acyclic graph by merging duplicate states. Second, we compare the original algorithm by Korf & Chickering with the variant proposed by Cohen-Solal, which differs in its backpropagation strategy: instead of stopping when a stable value is encountered, it updates values up to the root. This change slightly improves performance when value ties or transposition tables are involved. Third, we assess replacing the exact terminal evaluation function with the learned heuristic function. While beneficial when exact evaluations are costly, this modification reduces performance in inexpensive settings. Finally, we examine the impact of the completion technique that prioritizes resolved winning states and avoids resolved losing states. This technique also improves performance. Overall, our findings highlight how targeted modifications can enhance the efficiency of Unbounded Best-First Minimax.","authors":["Quentin Cohen-Solal","Tristan Cazenave"],"url":"https://arxiv.org/abs/2505.04525"}
{"created":"2025-05-08","title":"DFVO: Learning Darkness-free Visible and Infrared Image Disentanglement and Fusion All at Once","abstract":"Visible and infrared image fusion is one of the most crucial tasks in the field of image fusion, aiming to generate fused images with clear structural information and high-quality texture features for high-level vision tasks. However, when faced with severe illumination degradation in visible images, the fusion results of existing image fusion methods often exhibit blurry and dim visual effects, posing major challenges for autonomous driving. To this end, a Darkness-Free network is proposed to handle Visible and infrared image disentanglement and fusion all at Once (DFVO), which employs a cascaded multi-task approach to replace the traditional two-stage cascaded training (enhancement and fusion), addressing the issue of information entropy loss caused by hierarchical data transmission. Specifically, we construct a latent-common feature extractor (LCFE) to obtain latent features for the cascaded tasks strategy. Firstly, a details-extraction module (DEM) is devised to acquire high-frequency semantic information. Secondly, we design a hyper cross-attention module (HCAM) to extract low-frequency information and preserve texture features from source images. Finally, a relevant loss function is designed to guide the holistic network learning, thereby achieving better image fusion. Extensive experiments demonstrate that our proposed approach outperforms state-of-the-art alternatives in terms of qualitative and quantitative evaluations. Particularly, DFVO can generate clearer, more informative, and more evenly illuminated fusion results in the dark environments, achieving best performance on the LLVIP dataset with 63.258 dB PSNR and 0.724 CC, providing more effective information for high-level vision tasks. Our code is publicly accessible at https://github.com/DaVin-Qi530/DFVO.","authors":["Qi Zhou","Yukai Shi","Xiaojun Yang","Xiaoyu Xian","Lunjia Liao","Ruimao Zhang","Liang Lin"],"url":"https://arxiv.org/abs/2505.04526"}
{"created":"2025-05-08","title":"Beyond Theorem Proving: Formulation, Framework and Benchmark for Formal Problem-Solving","abstract":"As a seemingly self-explanatory task, problem-solving has been a significant component of science and engineering. However, a general yet concrete formulation of problem-solving itself is missing. With the recent development of AI-based problem-solving agents, the demand for process-level verifiability is rapidly increasing yet underexplored. To fill these gaps, we present a principled formulation of problem-solving as a deterministic Markov decision process; a novel framework, FPS (Formal Problem-Solving), which utilizes existing FTP (formal theorem proving) environments to perform process-verified problem-solving; and D-FPS (Deductive FPS), decoupling solving and answer verification for better human-alignment. The expressiveness, soundness and completeness of the frameworks are proven. We construct three benchmarks on problem-solving: FormalMath500, a formalization of a subset of the MATH500 benchmark; MiniF2F-Solving and PutnamBench-Solving, adaptations of FTP benchmarks MiniF2F and PutnamBench. For faithful, interpretable, and human-aligned evaluation, we propose RPE (Restricted Propositional Equivalence), a symbolic approach to determine the correctness of answers by formal verification. We evaluate four prevalent FTP models and two prompting methods as baselines, solving at most 23.77% of FormalMath500, 27.47% of MiniF2F-Solving, and 0.31% of PutnamBench-Solving.","authors":["Qi Liu","Xinhao Zheng","Renqiu Xia","Xingzhi Qi","Qinxiang Cao","Junchi Yan"],"url":"https://arxiv.org/abs/2505.04528"}
{"created":"2025-05-08","title":"RAFT: Robust Augmentation of FeaTures for Image Segmentation","abstract":"Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.","authors":["Edward Humes","Xiaomin Lin","Uttej Kallakuri","Tinoosh Mohsenin"],"url":"https://arxiv.org/abs/2505.04529"}
{"created":"2025-05-08","title":"Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review","abstract":"Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini. While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English. This has amplified concerns over linguistic inequality in natural language processing (NLP). This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL). Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks. We also analyse trends in architecture choices, language family representation, and evaluation methods. Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies. We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems. Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies.","authors":["Josh McGiff","Nikola S. Nikolov"],"url":"https://arxiv.org/abs/2505.04531"}
{"created":"2025-05-08","title":"Integrated equilibrium model for electrified logistics and power systems","abstract":"This paper proposes an integrated equilibrium model to characterize the complex interactions between electrified logistics systems and electric power delivery systems. The model consists of two major players: an electrified logistics operator (ELO) and a power system operator (PSO). The ELO aims to maximize its profit by strategically scheduling and routing its electric delivery vehicles (e-trucks) for deliveries and charging, in response to the locational marginal price (LMP) set by the PSO. The routing, delivery, and charging behaviors of e-trucks are modeled by a perturbed utility Markov decision process (PU-MDP) while their collective operations are optimized to achieve the ELO's objective by designing rewards in the PU-MDP. On the other hand, PSO optimizes the energy price by considering both the spatiotemporal e-truck charging demand and the base electricity load. The equilibrium of the integrated system is formulated as a fixed point, proved to exist under mild assumptions, and solved for a case study on the Hawaii network via Anderson's fixed-point acceleration algorithm. Along with these numerical results, this paper provides both theoretical insights and practical guidelines to achieve sustainable and efficient operations in modern electrified logistics and power systems.","authors":["Rui Yao","Xuhang Liu","Anna Scaglione","Shlomo Bekhor","Kenan Zhang"],"url":"https://arxiv.org/abs/2505.04532"}
{"created":"2025-05-08","title":"Communication-Efficient Federated Fine-Tuning of Language Models via Dynamic Update Schedules","abstract":"Federated learning (FL) makes it possible to train models on data that would otherwise remain untapped and inaccessible. Simultaneously, pre-trained language models (LMs) have emerged as indispensable tools in modern workflows. These models exhibit extraordinary capabilities and are easily adapted to downstream tasks. This opens one of the most exciting frontiers in FL: fine-tuning LMs. However, a persistent challenge in FL is the frequent, rigid communication of parameters, a problem which is magnified by the sheer size of these modern models. Currently, the FedOpt family of algorithms is the prevailing approach in FL, though it relies on fixed, heuristic intervals for model synchronization. Recently, the FDA algorithm introduced a dynamic alternative by monitoring training progress, but it came with its own drawbacks; namely, a hard-to-tune threshold parameter and a rigid synchronization scheme. In this work, we introduce the FDA-Opt family of algorithms -- a unified generalization that extends the principles behind both FDA and FedOpt, while resolving their core limitations. We evaluate our approach on fine-tuning LMs across a range of downstream NLP tasks, and demonstrate that it consistently outperforms FedOpt -- even when FDA-Opt operates under hyper-parameter settings originally optimized for its competitors. In other words, we show that FDA-Opt is a practical, drop-in replacement for FedOpt in modern FL libraries and systems: it requires no additional configuration and delivers superior performance out of the box.","authors":["Michail Theologitis","Vasilis Samoladas","Antonios Deligiannakis"],"url":"https://arxiv.org/abs/2505.04535"}
{"created":"2025-05-08","title":"Light Spanners with Small Hop-Diameter","abstract":"Lightness, sparsity, and hop-diameter are the fundamental parameters of geometric spanners. Arya et al. [STOC'95] showed in their seminal work that there exists a construction of Euclidean $(1+\\varepsilon)$-spanners with hop-diameter $O(\\log n)$ and lightness $O(\\log n)$. They also gave a general tradeoff of hop-diameter $k$ and sparsity $O(\\alpha_k(n))$, where $\\alpha_k$ is a very slowly growing inverse of an Ackermann-style function. The former combination of logarithmic hop-diameter and lightness is optimal due to the lower bound by Dinitz et al. [FOCS'08]. Later, Elkin and Solomon [STOC'13] generalized the light spanner construction to doubling metrics and extended the tradeoff for more values of hop-diameter $k$. In a recent line of work [SoCG'22, SoCG'23], Le et al. proved that the aforementioned tradeoff between the hop-diameter and sparsity is tight for every choice of hop-diameter $k$. A fundamental question remains: What is the optimal tradeoff between the hop-diameter and lightness for every value of $k$?","authors":["Sujoy Bhore","Lazar Milenkovic"],"url":"https://arxiv.org/abs/2505.04536"}
{"created":"2025-05-08","title":"Qualitative Analysis of $\\omega$-Regular Objectives on Robust MDPs","abstract":"Robust Markov Decision Processes (RMDPs) generalize classical MDPs that consider uncertainties in transition probabilities by defining a set of possible transition functions. An objective is a set of runs (or infinite trajectories) of the RMDP, and the value for an objective is the maximal probability that the agent can guarantee against the adversarial environment. We consider (a) reachability objectives, where given a target set of states, the goal is to eventually arrive at one of them; and (b) parity objectives, which are a canonical representation for $\\omega$-regular objectives. The qualitative analysis problem asks whether the objective can be ensured with probability 1.","authors":["Ali Asadi","Krishnendu Chatterjee","Ehsan Kafshdar Goharshady","Mehrdad Karrabi","Ali Shafiee"],"url":"https://arxiv.org/abs/2505.04539"}
{"created":"2025-05-08","title":"Registration of 3D Point Sets Using Exponential-based Similarity Matrix","abstract":"Point cloud registration is a fundamental problem in computer vision and robotics, involving the alignment of 3D point sets captured from varying viewpoints using depth sensors such as LiDAR or structured light. In modern robotic systems, especially those focused on mapping, it is essential to merge multiple views of the same environment accurately. However, state-of-the-art registration techniques often struggle when large rotational differences exist between point sets or when the data is significantly corrupted by sensor noise. These challenges can lead to misalignments and, consequently, to inaccurate or distorted 3D reconstructions. In this work, we address both these limitations by proposing a robust modification to the classic Iterative Closest Point (ICP) algorithm. Our method, termed Exponential Similarity Matrix ICP (ESM-ICP), integrates a Gaussian-inspired exponential weighting scheme to construct a similarity matrix that dynamically adapts across iterations. This matrix facilitates improved estimation of both rotational and translational components during alignment. We demonstrate the robustness of ESM-ICP in two challenging scenarios: (i) large rotational discrepancies between the source and target point clouds, and (ii) data corrupted by non-Gaussian noise. Our results show that ESM-ICP outperforms traditional geometric registration techniques as well as several recent learning-based methods. To encourage reproducibility and community engagement, our full implementation is made publicly available on GitHub. https://github.com/aralab-unr/ESM_ICP","authors":["Ashutosh Singandhupe","Sanket Lokhande","Hung Manh La"],"url":"https://arxiv.org/abs/2505.04540"}
{"created":"2025-05-08","title":"Fast Pattern Matching with Epsilon Transitions","abstract":"In the String Matching in Labeled Graphs (SMLG) problem, we need to determine whether a pattern string appears on a given labeled graph or a given automaton. Under the Orthogonal Vectors hypothesis, the SMLG problem cannot be solved in subquadratic time [ICALP 2019]. In typical bioinformatics applications, pattern matching algorithms should be both fast and space-efficient, so we need to determine useful classes of graphs on which the SLMG problem can be solved efficiently.","authors":["Nicola Cotumaccio"],"url":"https://arxiv.org/abs/2505.04549"}
{"created":"2025-05-08","title":"Runtime Advocates: A Persona-Driven Framework for Requirements@Runtime Decision Support","abstract":"Complex systems, such as small Uncrewed Aerial Systems (sUAS) swarms dispatched for emergency response, often require dynamic reconfiguration at runtime under the supervision of human operators. This introduces human-on-the-loop requirements, where evolving needs shape ongoing system functionality and behaviors. While traditional personas support upfront, static requirements elicitation, we propose a persona-based advocate framework for runtime requirements engineering to provide ethically informed, safety-driven, and regulatory-aware decision support. Our approach extends standard personas into event-driven personas. When triggered by events such as adverse environmental conditions, evolving mission state, or operational constraints, the framework updates the sUAS operator's view of the personas, ensuring relevance to current conditions. We create three key advocate personas, namely Safety Controller, Ethical Governor, and Regulatory Auditor, to manage trade-offs among risk, ethical considerations, and regulatory compliance. We perform a proof-of-concept validation in an emergency response scenario using sUAS, showing how our advocate personas provide context-aware guidance grounded in safety, regulatory, and ethical constraints. By evolving static, design-time personas into adaptive, event-driven advocates, the framework surfaces mission-critical runtime requirements in response to changing conditions. These requirements shape operator decisions in real time, aligning actions with the operational demands of the moment.","authors":["Demetrius Hernandez","Jane Cleland-Huang"],"url":"https://arxiv.org/abs/2505.04551"}
{"created":"2025-05-08","title":"Comparing CPU and GPU compute of PERMANOVA on MI300A","abstract":"Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is often challenging, due to the drastically different memory subsystems on host CPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both CPU and GPU cores in a single package, all backed by the same type of HBM memory. In this paper we analyze the performance of Permutational Multivariate Analysis of Variance (PERMANOVA), a non-parametric method that tests whether two or more groups of objects are significantly different based on a categorical factor. This method is memory-bound and has been recently optimized for CPU cache locality. Our tests show that GPU cores on the MI300A prefer the brute force approach instead, significantly outperforming the CPU-based implementation. The significant benefit of Simultaneous Multithreading (SMT) was also a pleasant surprise.","authors":["Igor Sfiligoi"],"url":"https://arxiv.org/abs/2505.04556"}
{"created":"2025-05-08","title":"Purity Law for Generalizable Neural TSP Solvers","abstract":"Achieving generalization in neural approaches across different scales and distributions remains a significant challenge for the Traveling Salesman Problem~(TSP). A key obstacle is that neural networks often fail to learn robust principles for identifying universal patterns and deriving optimal solutions from diverse instances. In this paper, we first uncover Purity Law (PuLa), a fundamental structural principle for optimal TSP solutions, defining that edge prevalence grows exponentially with the sparsity of surrounding vertices. Statistically validated across diverse instances, PuLa reveals a consistent bias toward local sparsity in global optima. Building on this insight, we propose Purity Policy Optimization~(PUPO), a novel training paradigm that explicitly aligns characteristics of neural solutions with PuLa during the solution construction process to enhance generalization. Extensive experiments demonstrate that PUPO can be seamlessly integrated with popular neural solvers, significantly enhancing their generalization performance without incurring additional computational overhead during inference.","authors":["Wenzhao Liu","Haoran Li","Congying Han","Zicheng Zhang","Anqi Li","Tiande Guo"],"url":"https://arxiv.org/abs/2505.04558"}
{"created":"2025-05-08","title":"ABKD: Pursuing a Proper Allocation of the Probability Mass in Knowledge Distillation via $\\alpha$-$\\beta$-Divergence","abstract":"Knowledge Distillation (KD) transfers knowledge from a large teacher model to a smaller student model by minimizing the divergence between their output distributions, typically using forward Kullback-Leibler divergence (FKLD) or reverse KLD (RKLD). It has become an effective training paradigm due to the broader supervision information provided by the teacher distribution compared to one-hot labels. We identify that the core challenge in KD lies in balancing two mode-concentration effects: the \\textbf{\\textit{Hardness-Concentration}} effect, which refers to focusing on modes with large errors, and the \\textbf{\\textit{Confidence-Concentration}} effect, which refers to focusing on modes with high student confidence. Through an analysis of how probabilities are reassigned during gradient updates, we observe that these two effects are entangled in FKLD and RKLD, but in extreme forms. Specifically, both are too weak in FKLD, causing the student to fail to concentrate on the target class. In contrast, both are too strong in RKLD, causing the student to overly emphasize the target class while ignoring the broader distributional information from the teacher. To address this imbalance, we propose ABKD, a generic framework with $\\alpha$-$\\beta$-divergence. Our theoretical results show that ABKD offers a smooth interpolation between FKLD and RKLD, achieving an effective trade-off between these effects. Extensive experiments on 17 language/vision datasets with 12 teacher-student settings confirm its efficacy. The code is available at https://github.com/ghwang-s/abkd.","authors":["Guanghui Wang","Zhiyong Yang","Zitai Wang","Shi Wang","Qianqian Xu","Qingming Huang"],"url":"https://arxiv.org/abs/2505.04560"}
{"created":"2025-05-08","title":"From Flowers to Fascism? The Cottagecore to Tradwife Pipeline on Tumblr","abstract":"In this work we collected and analyzed social media posts to investigate aesthetic-based radicalization where users searching for Cottagecore content may find Tradwife content co-opted by white supremacists, white nationalists, or other far-right extremist groups. Through quantitative analysis of over 200,000 Tumblr posts and qualitative coding of about 2,500 Tumblr posts, we did not find evidence of a explicit radicalization. We found that problematic Tradwife posts found in the literature may be confined to Tradwife-only spaces, while content in the Cottagecore tag generally did not warrant extra moderation. However, we did find evidence of a mainstreaming effect in the overlap between the Tradwife and Cottagecore communities. In our qualitative analysis there was more interaction between queer and Tradwife identities than expected based on the literature, and some Tradwives even explicitly included queer people and disavowed racism in the Tradwife community on Tumblr. This could be genuine, but more likely it was an example of extremists re-branding their content and following platform norms to spread ideologies that would otherwise be rejected by Tumblr users. Additionally, through temporal analysis we observed a change in the central tags used by Tradwives in the Cottagecore tag pre- and post- 2021. Initially these posts focused on aesthetics and hobbies like baking and gardening, but post-2021 the central tags focused more on religion, traditional gender roles, and homesteading, all markers of reactionary ideals.","authors":["Oliver Mel Allen","Yi Zu","Milo Z. Trujillo","Brooke Foucault Welles"],"url":"https://arxiv.org/abs/2505.04561"}
{"created":"2025-05-08","title":"Optimal Deterministic Rendezvous in Labeled Lines","abstract":"In a rendezvous task, a set of mobile agents dispersed in a network have to gather at an arbitrary common site. We consider the rendezvous problem on the infinite labeled line, with $2$ initially asleep agents, without communication, and a synchronous notion of time. Nodes are labeled with unique positive integers. The initial distance between the two agents is denoted by $D$. Time is divided into rounds. We count time from when an agent first wakes up, and denote by $\\tau$ the delay between the agents' wake up times. If awake in a given round $T$, an agent has three options: stay at its current node $v$, take port $0$, or take port $1$. If it decides to stay, the agent is still at node $v$ in round $T+1$. Otherwise, it is at one of the two neighbors of $v$ on the line, based on the port it chose. The agents achieve rendezvous in $T$ rounds if they are at the same node in round $T$. We aim for a deterministic algorithm for this task.","authors":["Yann Bourreau","Ananth Narayanan","Alexandre Nolin"],"url":"https://arxiv.org/abs/2505.04564"}
{"created":"2025-05-08","title":"Hierarchical Task Decomposition for Execution Monitoring and Error Recovery: Understanding the Rationale Behind Task Demonstrations","abstract":"Multi-step manipulation tasks where robots interact with their environment and must apply process forces based on the perceived situation remain challenging to learn and prone to execution errors. Accurately simulating these tasks is also difficult. Hence, it is crucial for robust task performance to learn how to coordinate end-effector pose and applied force, monitor execution, and react to deviations. To address these challenges, we propose a learning approach that directly infers both low- and high-level task representations from user demonstrations on the real system. We developed an unsupervised task segmentation algorithm that combines intention recognition and feature clustering to infer the skills of a task. We leverage the inferred characteristic features of each skill in a novel unsupervised anomaly detection approach to identify deviations from the intended task execution. Together, these components form a comprehensive framework capable of incrementally learning task decisions and new behaviors as new situations arise. Compared to state-of-the-art learning techniques, our approach significantly reduces the required amount of training data and computational complexity while efficiently learning complex in-contact behaviors and recovery strategies. Our proposed task segmentation and anomaly detection approaches outperform state-of-the-art methods on force-based tasks evaluated on two different robotic systems.","authors":["Christoph Willibald","Dongheui Lee"],"url":"https://arxiv.org/abs/2505.04565"}
{"created":"2025-05-08","title":"Multitask LSTM for Arboviral Outbreak Prediction Using Public Health Data","abstract":"This paper presents a multitask learning approach based on long-short-term memory (LSTM) networks for the joint prediction of arboviral outbreaks and case counts of dengue, chikungunya, and Zika in Recife, Brazil. Leveraging historical public health data from DataSUS (2017-2023), the proposed model concurrently performs binary classification (outbreak detection) and regression (case forecasting) tasks. A sliding window strategy was adopted to construct temporal features using varying input lengths (60, 90, and 120 days), with hyperparameter optimization carried out using Keras Tuner. Model evaluation used time series cross-validation for robustness and a held-out test from 2023 for generalization assessment. The results show that longer windows improve dengue regression accuracy, while classification performance peaked at intermediate windows, suggesting an optimal trade-off between sequence length and generalization. The multitask architecture delivers competitive performance across diseases and tasks, demonstrating the feasibility and advantages of unified modeling strategies for scalable epidemic forecasting in data-limited public health scenarios.","authors":["Lucas R. C. Farias","Talita P. Silva","Pedro H. M. Araujo"],"url":"https://arxiv.org/abs/2505.04566"}
{"created":"2025-05-08","title":"Flexing RISC-V Instruction Subset Processors (RISPs) to Extreme Edge","abstract":"This paper presents a methodology for automatically generating processors that support a subset of the RISC-V instruction set for a new class of applications at Extreme Edge. The electronics used in extreme edge applications must be power-efficient, but also provide additional qualities, such as low cost, conformability, comfort and sustainability. Flexible electronics, rather than silicon-based electronics, will be capable of meeting these qualities. For this purpose, we propose a methodology to generate RISPs (RISC-V instruction subset processors) customised to extreme edge applications and to implement them as flexible integrated circuits (FlexICs). The methodology is unique in the sense that verification is an integral part of design. The RISP methodology treats each instruction in the ISA as a discrete, fully functional, pre-verified hardware block. It automatically builds a custom processor by stitching together the hardware blocks of the instructions required by an application or a set of applications in a specific domain. This approach significantly reduces the processor verification and its time-to-market. We generate RISPs using this methodology for three extreme edge applications, and embedded applications from the Embench benchmark suite, synthesize them as FlexICs, and compare their power, performance and area to the baselines. Our results show that RISPs generated using this methodology achieve, on average, 30\\% reductions in power and area compared to a RISC-V processor supporting the full instruction set when synthesized, and are nearly 30 times more energy efficient with respect to Serv - the world's smallest 32-bit RISC-V processor. In addition, the full physical implementation of RISPs show up to 21% and 26% less area and power than Serv.","authors":["Alireza Raisiardali","Konstantinos Iordanou","Jedrzej Kufel","Kowshik Gudimetla","Kris Myny","Emre Ozer"],"url":"https://arxiv.org/abs/2505.04567"}
{"created":"2025-05-08","title":"Duality-Based Algorithm and Numerical Analysis for Optimal Insulation Problems on Non-Smooth Domains","abstract":"This article develops a numerical approximation of a convex non-local and non-smooth minimization problem. The physical problem involves determining the optimal distribution, given by $h\\colon \\Gamma_I\\to [0,+\\infty)$, of a given amount $m\\in \\mathbb{N}$ of insulating material attached to a boundary part $\\Gamma_I\\subseteq \\partial\\Omega$ of a thermally conducting body $\\Omega \\subseteq \\mathbb{R}^d$, $d \\in \\mathbb{N}$, subject to conductive heat transfer. To tackle the non-local and non-smooth character of the problem, the article introduces a (Fenchel) duality framework: (a) At the continuous level, using (Fenchel) duality relations, we derive an a posteriori error identity that can handle arbitrary admissible approximations of the primal and dual formulations of the convex non-local and non-smooth minimization problem; (b) At the discrete level, using discrete (Fenchel) duality relations, we derive an a priori error identity that applies to a Crouzeix--Raviart discretization of the primal formulation and a Raviart--Thomas discretization of the dual formulation. The proposed framework leads to error decay rates that are optimal with respect to the specific regularity of a minimizer. In addition, we prove convergence of the numerical approximation under minimal regularity assumptions. Since the discrete dual formulation can be written as a quadratic program, it is solved using a primal-dual active set strategy interpreted as semismooth Newton method. A solution of the discrete primal formulation is reconstructed from the solution of the discrete dual formulation by means of an inverse generalized Marini formula. This is the first such formula for this class of convex non-local and non-smooth minimization problems.","authors":["Harbir Antil","Alex Kaltenbach","Keegan L. A. Kirk"],"url":"https://arxiv.org/abs/2505.04571"}
{"created":"2025-05-08","title":"Stow: Robotic Packing of Items into Fabric Pods","abstract":"This paper presents a compliant manipulation system capable of placing items onto densely packed shelves. The wide diversity of items and strict business requirements for high producing rates and low defect generation have prohibited warehouse robotics from performing this task. Our innovations in hardware, perception, decision-making, motion planning, and control have enabled this system to perform over 500,000 stows in a large e-commerce fulfillment center. The system achieves human levels of packing density and speed while prioritizing work on overhead shelves to enhance the safety of humans working alongside the robots.","authors":["Nicolas Hudson","Josh Hooks","Rahul Warrier","Curt Salisbury","Ross Hartley","Kislay Kumar","Bhavana Chandrashekhar","Paul Birkmeyer","Bosch Tang","Matt Frost","Shantanu Thakar","Tony Piaskowy","Petter Nilsson","Josh Petersen","Neel Doshi","Alan Slatter","Ankit Bhatia","Cassie Meeker","Yuechuan Xue","Dylan Cox","Alex Kyriazis","Bai Lou","Nadeem Hasan","Asif Rana","Nikhil Chacko","Ruinian Xu","Siamak Faal","Esi Seraj","Mudit Agrawal","Kevin Jamieson","Alessio Bisagni","Valerie Samzun","Christine Fuller","Alex Keklak","Alex Frenkel","Lillian Ratliff","Aaron Parness"],"url":"https://arxiv.org/abs/2505.04572"}
{"created":"2025-05-08","title":"Componential Prompt-Knowledge Alignment for Domain Incremental Learning","abstract":"Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt. Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt","authors":["Kunlun Xu","Xu Zou","Gang Hua","Jiahuan Zhou"],"url":"https://arxiv.org/abs/2505.04575"}
{"created":"2025-05-08","title":"Fight Fire with Fire: Defending Against Malicious RL Fine-Tuning via Reward Neutralization","abstract":"Reinforcement learning (RL) fine-tuning transforms large language models while creating a vulnerability we experimentally verify: Our experiment shows that malicious RL fine-tuning dismantles safety guardrails with remarkable efficiency, requiring only 50 steps and minimal adversarial prompts, with harmful escalating from 0-2 to 7-9. This attack vector particularly threatens open-source models with parameter-level access. Existing defenses targeting supervised fine-tuning prove ineffective against RL's dynamic feedback mechanisms. We introduce Reward Neutralization, the first defense framework specifically designed against RL fine-tuning attacks, establishing concise rejection patterns that render malicious reward signals ineffective. Our approach trains models to produce minimal-information rejections that attackers cannot exploit, systematically neutralizing attempts to optimize toward harmful outputs. Experiments validate that our approach maintains low harmful scores (no greater than 2) after 200 attack steps, while standard models rapidly deteriorate. This work provides the first constructive proof that robust defense against increasingly accessible RL attacks is achievable, addressing a critical security gap for open-weight models.","authors":["Wenjun Cao"],"url":"https://arxiv.org/abs/2505.04578"}
{"created":"2025-05-08","title":"Implicitly Aligning Humans and Autonomous Agents through Shared Task Abstractions","abstract":"In collaborative tasks, autonomous agents fall short of humans in their capability to quickly adapt to new and unfamiliar teammates. We posit that a limiting factor for zero-shot coordination is the lack of shared task abstractions, a mechanism humans rely on to implicitly align with teammates. To address this gap, we introduce HA$^2$: Hierarchical Ad Hoc Agents, a framework leveraging hierarchical reinforcement learning to mimic the structured approach humans use in collaboration. We evaluate HA$^2$ in the Overcooked environment, demonstrating statistically significant improvement over existing baselines when paired with both unseen agents and humans, providing better resilience to environmental shifts, and outperforming all state-of-the-art methods.","authors":["St\\'ephane Aroca-Ouellette","Miguel Aroca-Ouellette","Katharina von der Wense","Alessandro Roncone"],"url":"https://arxiv.org/abs/2505.04579"}
{"created":"2025-05-08","title":"Consensus Seminorms and their Applications","abstract":"Consensus is a well-studied problem in distributed sensing, computation and control, yet deriving useful and easily computable bounds on the rate of convergence to consensus remains a challenge. We study the applications of seminorms for this goal. We revisit a previously suggested family of seminorms and correct an error made in their original presentation where it was claimed that the a certain seminorm is equal to the well-known coefficient of ergodicity. We then propose a wider family of seminorms which guarantee convergence at an exponential rate of infinite products of matrices which generalizes known results on stochastic matrices to the class of matrices whose row sums are all equal one. Finally, we show that such seminorms cannot be used to bound the rate of convergence of classes larger than the well-known class of scrambling matrices, and pose several open questions for future research.","authors":["Ron Ofir","Ji Liu","A. Stephen Morse","Brian D. O. Anderson"],"url":"https://arxiv.org/abs/2505.04580"}
{"created":"2025-05-08","title":"Modeling Personalized Difficulty of Rehabilitation Exercises Using Causal Trees","abstract":"Rehabilitation robots are often used in game-like interactions for rehabilitation to increase a person's motivation to complete rehabilitation exercises. By adjusting exercise difficulty for a specific user throughout the exercise interaction, robots can maximize both the user's rehabilitation outcomes and the their motivation throughout the exercise. Previous approaches have assumed exercises have generic difficulty values that apply to all users equally, however, we identified that stroke survivors have varied and unique perceptions of exercise difficulty. For example, some stroke survivors found reaching vertically more difficult than reaching farther but lower while others found reaching farther more challenging than reaching vertically. In this paper, we formulate a causal tree-based method to calculate exercise difficulty based on the user's performance. We find that this approach accurately models exercise difficulty and provides a readily interpretable model of why that exercise is difficult for both users and caretakers.","authors":["Nathaniel Dennler","Zhonghao Shi","Uksang Yoo","Stefanos Nikolaidis","Maja Matari\\'c"],"url":"https://arxiv.org/abs/2505.04583"}
{"created":"2025-05-08","title":"SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended Questions","abstract":"Feedback is important in supporting student learning. While various automated feedback systems have been implemented to make the feedback scalable, many existing solutions only focus on generating text-based feedback. As is indicated in the multimedia learning principle, learning with more modalities could help utilize more separate channels, reduce the cognitive load and facilitate students' learning. Hence, it is important to explore the potential of Artificial Intelligence (AI) in feedback generation from and to different modalities. Our study leverages Large Language Models (LLMs) for textual feedback with the supplementary guidance from other modality - relevant lecture slide retrieved from the slides hub. Through an online crowdsourcing study (N=91), this study investigates learning gains and student perceptions using a 2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant slide), evaluating the clarity, engagement, perceived effectiveness, and reliability) of AI-facilitated multimodal feedback. We observed significant pre-to-post learning gains across all conditions. However, the differences in these gains were not statistically significant between conditions. The post-survey revealed that students found the slide feedback helpful in their learning process, though they reported difficulty in understanding it. Regarding the AI-generated open-ended feedback, students considered it personalized and relevant to their responses, but they expressed lower trust in the AI feedback compared to human-generated feedback.","authors":["Chloe Qianhui Zhao","Jie Cao","Eason Chen","Kenneth R. Koedinger","Jionghao Lin"],"url":"https://arxiv.org/abs/2505.04584"}
{"created":"2025-05-08","title":"Active Sampling for MRI-based Sequential Decision Making","abstract":"Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling","authors":["Yuning Du","Jingshuai Liu","Rohan Dharmakumar","Sotirios A. Tsaftaris"],"url":"https://arxiv.org/abs/2505.04586"}
{"created":"2025-05-08","title":"ZeroSearch: Incentivize the Search Capability of LLMs without Searching","abstract":"Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.","authors":["Hao Sun","Zile Qiao","Jiayan Guo","Xuanbo Fan","Yingyan Hou","Yong Jiang","Pengjun Xie","Fei Huang","Yan Zhang"],"url":"https://arxiv.org/abs/2505.04588"}
{"created":"2025-05-08","title":"TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization","abstract":"We introduce TetWeave, a novel isosurface representation for gradient-based mesh optimization that jointly optimizes the placement of a tetrahedral grid used for Marching Tetrahedra and a novel directional signed distance at each point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay triangulation, enabling increased flexibility compared to predefined grids. The extracted meshes are guaranteed to be watertight, two-manifold and intersection-free. The flexibility of TetWeave enables a resampling strategy that places new points where reconstruction error is high and allows to encourage mesh fairness without compromising on reconstruction error. This leads to high-quality, adaptive meshes that require minimal memory usage and few parameters to optimize. Consequently, TetWeave exhibits near-linear memory scaling relative to the vertex count of the output mesh - a substantial improvement over predefined grids. We demonstrate the applicability of TetWeave to a broad range of challenging tasks in computer graphics and vision, such as multi-view 3D reconstruction, mesh compression and geometric texture generation.","authors":["Alexandre Binninger","Ruben Wiersma","Philipp Herholz","Olga Sorkine-Hornung"],"url":"https://arxiv.org/abs/2505.04590"}
{"created":"2025-05-08","title":"AI Governance to Avoid Extinction: The Strategic Landscape and Actionable Research Questions","abstract":"Humanity appears to be on course to soon develop AI systems that substantially outperform human experts in all cognitive domains and activities. We believe the default trajectory has a high likelihood of catastrophe, including human extinction. Risks come from failure to control powerful AI systems, misuse of AI by malicious rogue actors, war between great powers, and authoritarian lock-in. This research agenda has two aims: to describe the strategic landscape of AI development and to catalog important governance research questions. These questions, if answered, would provide important insight on how to successfully reduce catastrophic risks.","authors":["Peter Barnett","Aaron Scher"],"url":"https://arxiv.org/abs/2505.04592"}
{"created":"2025-05-08","title":"MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection","abstract":"Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.","authors":["Zhihao Zhang","Abhinav Kumar","Girish Chandar Ganesan","Xiaoming Liu"],"url":"https://arxiv.org/abs/2505.04594"}
{"created":"2025-05-08","title":"Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness","abstract":"Recent results in non-convex stochastic optimization demonstrate the convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0, L_1)$-smoothness condition, but the rate of convergence is a higher-order polynomial in terms of problem parameters like the smoothness constants. The complexity guaranteed by such algorithms to find an $\\epsilon$-stationary point may be significantly larger than the optimal complexity of $\\Theta \\left( \\Delta L \\sigma^2 \\epsilon^{-4} \\right)$ achieved by SGD in the $L$-smooth setting, where $\\Delta$ is the initial optimality gap, $\\sigma^2$ is the variance of stochastic gradient. However, it is currently not known whether these higher-order dependencies can be tightened. To answer this question, we investigate complexity lower bounds for several adaptive optimization algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence in terms of problem parameters $\\Delta, L_0, L_1$. We provide complexity bounds for three variations of AdaGrad, which show at least a quadratic dependence on problem parameters $\\Delta, L_0, L_1$. Notably, we show that the decorrelated variant of AdaGrad-Norm requires at least $\\Omega \\left( \\Delta^2 L_1^2 \\sigma^2 \\epsilon^{-4} \\right)$ stochastic gradient queries to find an $\\epsilon$-stationary point. We also provide a lower bound for SGD with a broad class of adaptive stepsizes. Our results show that, for certain adaptive algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult than the standard smooth setting, in terms of the initial optimality gap and the smoothness constants.","authors":["Michael Crawshaw","Mingrui Liu"],"url":"https://arxiv.org/abs/2505.04599"}
{"created":"2025-05-08","title":"Perpetuating Misogyny with Generative AI: How Model Personalization Normalizes Gendered Harm","abstract":"Open-source text-to-image (TTI) pipelines have become dominant in the landscape of AI-generated visual content, driven by technological advances that enable users to personalize models through adapters tailored to specific tasks. While personalization methods such as LoRA offer unprecedented creative opportunities, they also facilitate harmful practices, including the generation of non-consensual deepfakes and the amplification of misogynistic or hypersexualized content. This study presents an exploratory sociotechnical analysis of CivitAI, the most active platform for sharing and developing open-source TTI models. Drawing on a dataset of more than 40 million user-generated images and over 230,000 models, we find a disproportionate rise in not-safe-for-work (NSFW) content and a significant number of models intended to mimic real individuals. We also observe a strong influence of internet subcultures on the tools and practices shaping model personalizations and resulting visual media. In response to these findings, we contextualize the emergence of exploitative visual media through feminist and constructivist perspectives on technology, emphasizing how design choices and community dynamics shape platform outcomes. Building on this analysis, we propose interventions aimed at mitigating downstream harm, including improved content moderation, rethinking tool design, and establishing clearer platform policies to promote accountability and consent.","authors":["Laura Wagner","Eva Cetinic"],"url":"https://arxiv.org/abs/2505.04600"}
{"created":"2025-05-08","title":"OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision Encoders for Multimodal Learning","abstract":"OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.","authors":["Xianhang Li","Yanqing Liu","Haoqin Tu","Hongru Zhu","Cihang Xie"],"url":"https://arxiv.org/abs/2505.04601"}
{"created":"2025-05-08","title":"Testing Juntas Optimally with Samples","abstract":"We prove tight upper and lower bounds of $\\Theta\\left(\\tfrac{1}{\\epsilon}\\left( \\sqrt{2^k \\log\\binom{n}{k} } + \\log\\binom{n}{k} \\right)\\right)$ on the number of samples required for distribution-free $k$-junta testing. This is the first tight bound for testing a natural class of Boolean functions in the distribution-free sample-based model. Our bounds also hold for the feature selection problem, showing that a junta tester must learn the set of relevant variables. For tolerant junta testing, we prove a sample lower bound of $\\Omega(2^{(1-o(1)) k} + \\log\\binom{n}{k})$ showing that, unlike standard testing, there is no large gap between tolerant testing and learning.","authors":["Lorenzo Beretta","Nathaniel Harms","Caleb Koch"],"url":"https://arxiv.org/abs/2505.04604"}
{"created":"2025-05-08","title":"OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution","abstract":"The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.","authors":["Lianghong Guo","Wei Tao","Runhan Jiang","Yanlin Wang","Jiachi Chen","Xilin Liu","Yuchi Ma","Mingzhi Mao","Hongyu Zhang","Zibin Zheng"],"url":"https://arxiv.org/abs/2505.04606"}
{"created":"2025-05-08","title":"WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via Weighted-Conformal Martingales","abstract":"Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria,'' such as data shifts that violate certain exchangeability assumptions, or do not allow for online adaptation in response to shifts. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that accommodate online adaptation to mild covariate shifts (in the marginal input distribution) while raising alarms in response to more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.","authors":["Drew Prinster","Xing Han","Anqi Liu","Suchi Saria"],"url":"https://arxiv.org/abs/2505.04608"}
{"created":"2025-05-08","title":"FastMap: Revisiting Dense and Scalable Structure from Motion","abstract":"We propose FastMap, a new global structure from motion method focused on speed and simplicity. Previous methods like COLMAP and GLOMAP are able to estimate high-precision camera poses, but suffer from poor scalability when the number of matched keypoint pairs becomes large. We identify two key factors leading to this problem: poor parallelization and computationally expensive optimization steps. To overcome these issues, we design an SfM framework that relies entirely on GPU-friendly operations, making it easily parallelizable. Moreover, each optimization step runs in time linear to the number of image pairs, independent of keypoint pairs or 3D points. Through extensive experiments, we show that FastMap is one to two orders of magnitude faster than COLMAP and GLOMAP on large-scale scenes with comparable pose accuracy.","authors":["Jiahao Li","Haochen Wang","Muhammad Zubair Irshad","Igor Vasiljevic","Matthew R. Walter","Vitor Campagnolo Guizilini","Greg Shakhnarovich"],"url":"https://arxiv.org/abs/2505.04612"}
{"created":"2025-05-08","title":"Person Recognition at Altitude and Range: Fusion of Face, Body Shape and Gait","abstract":"We address the problem of whole-body person recognition in unconstrained environments. This problem arises in surveillance scenarios such as those in the IARPA Biometric Recognition and Identification at Altitude and Range (BRIAR) program, where biometric data is captured at long standoff distances, elevated viewing angles, and under adverse atmospheric conditions (e.g., turbulence and high wind velocity). To this end, we propose FarSight, a unified end-to-end system for person recognition that integrates complementary biometric cues across face, gait, and body shape modalities. FarSight incorporates novel algorithms across four core modules: multi-subject detection and tracking, recognition-aware video restoration, modality-specific biometric feature encoding, and quality-guided multi-modal fusion. These components are designed to work cohesively under degraded image conditions, large pose and scale variations, and cross-domain gaps. Extensive experiments on the BRIAR dataset, one of the most comprehensive benchmarks for long-range, multi-modal biometric recognition, demonstrate the effectiveness of FarSight. Compared to our preliminary system, this system achieves a 34.1% absolute gain in 1:1 verification accuracy (TAR@0.1% FAR), a 17.8% increase in closed-set identification (Rank-20), and a 34.3% reduction in open-set identification errors (FNIR@1% FPIR). Furthermore, FarSight was evaluated in the 2025 NIST RTE Face in Video Evaluation (FIVE), which conducts standardized face recognition testing on the BRIAR dataset. These results establish FarSight as a state-of-the-art solution for operational biometric recognition in challenging real-world conditions.","authors":["Feng Liu","Nicholas Chimitt","Lanqing Guo","Jitesh Jain","Aditya Kane","Minchul Kim","Wes Robbins","Yiyang Su","Dingqiang Ye","Xingguang Zhang","Jie Zhu","Siddharth Satyakam","Christopher Perry","Stanley H. Chan","Arun Ross","Humphrey Shi","Zhangyang Wang","Anil Jain","Xiaoming Liu"],"url":"https://arxiv.org/abs/2505.04616"}
{"created":"2025-05-08","title":"Report on Nearest Dominating Point Queries","abstract":"Given two points $p, q \\in \\mathbb R^d$, we say that $p$ dominates $q$ and write $p \\succ q$ if each coordinate of $p$ is larger than the corresponding coordinate of $q$. That is, if $p = (p^{(1)}, p^{(2)}, \\ldots, p^{(d)})$ and $q = (q^{(1)}, q^{(2)}, \\ldots, q^{(d)})$, $p \\succ q$ if and only if $p^{(i)} > q^{(i)}$ for all $1 \\le i \\le d$.","authors":["Naman Mishra (Indian Institute of Science)","K S Sreeramji (Indian Institute of Science)"],"url":"https://arxiv.org/abs/2505.04617"}
{"created":"2025-05-08","title":"Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation","abstract":"Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad","authors":["Abdulaziz Almuzairee","Rohan Patil","Dwait Bhatt","Henrik I. Christensen"],"url":"https://arxiv.org/abs/2505.04619"}
{"created":"2025-05-08","title":"On Path to Multimodal Generalist: General-Level and General-Bench","abstract":"The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/","authors":["Hao Fei","Yuan Zhou","Juncheng Li","Xiangtai Li","Qingshan Xu","Bobo Li","Shengqiong Wu","Yaoting Wang","Junbao Zhou","Jiahao Meng","Qingyu Shi","Zhiyuan Zhou","Liangtao Shi","Minghe Gao","Daoan Zhang","Zhiqi Ge","Weiming Wu","Siliang Tang","Kaihang Pan","Yaobo Ye","Haobo Yuan","Tao Zhang","Tianjie Ju","Zixiang Meng","Shilin Xu","Liyu Jia","Wentao Hu","Meng Luo","Jiebo Luo","Tat-Seng Chua","Shuicheng Yan","Hanwang Zhang"],"url":"https://arxiv.org/abs/2505.04620"}
{"created":"2025-05-08","title":"Score Distillation Sampling for Audio: Source Separation, Synthesis, and Beyond","abstract":"We introduce Audio-SDS, a generalization of Score Distillation Sampling (SDS) to text-conditioned audio diffusion models. While SDS was initially designed for text-to-3D generation using image diffusion, its core idea of distilling a powerful generative prior into a separate parametric representation extends to the audio domain. Leveraging a single pretrained model, Audio-SDS enables a broad range of tasks without requiring specialized datasets. In particular, we demonstrate how Audio-SDS can guide physically informed impact sound simulations, calibrate FM-synthesis parameters, and perform prompt-specified source separation. Our findings illustrate the versatility of distillation-based methods across modalities and establish a robust foundation for future work using generative priors in audio tasks.","authors":["Jessie Richter-Powell","Antonio Torralba","Jonathan Lorraine"],"url":"https://arxiv.org/abs/2505.04621"}
{"created":"2025-05-08","title":"PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer","abstract":"Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io","authors":["Jingwen Ye","Yuze He","Yanning Zhou","Yiqin Zhu","Kaiwen Xiao","Yong-Jin Liu","Wei Yang","Xiao Han"],"url":"https://arxiv.org/abs/2505.04622"}
{"created":"2025-05-08","title":"Beyond Misinformation: A Conceptual Framework for Studying AI Hallucinations in (Science) Communication","abstract":"This paper proposes a conceptual framework for understanding AI hallucinations as a distinct form of misinformation. While misinformation scholarship has traditionally focused on human intent, generative AI systems now produce false yet plausible outputs absent of such intent. I argue that these AI hallucinations should not be treated merely as technical failures but as communication phenomena with social consequences. Drawing on a supply-and-demand model and the concept of distributed agency, the framework outlines how hallucinations differ from human-generated misinformation in production, perception, and institutional response. I conclude by outlining a research agenda for communication scholars to investigate the emergence, dissemination, and audience reception of hallucinated content, with attention to macro (institutional), meso (group), and micro (individual) levels. This work urges communication researchers to rethink the boundaries of misinformation theory in light of probabilistic, non-human actors increasingly embedded in knowledge production.","authors":["Anqi Shao"],"url":"https://arxiv.org/abs/2504.13777"}
{"created":"2025-05-08","title":"Fundamental Limits Of Quickest Change-point Detection With Continuous-Variable Quantum States","abstract":"We generalize the quantum CUSUM (QUSUM) algorithm for quickest change-point detection, analyzed in finite dimensions by Fanizza, Hirche, and Calsamiglia (Phys. Rev. Lett. 131, 020602, 2023), to infinite-dimensional quantum systems. Our analysis relies on a novel generalization of a result by Hayashi (Hayashi, J. Phys. A: Math. Gen. 34, 3413, 2001) concerning the asymptotics of quantum relative entropy, which we establish for the infinite-dimensional setting. This enables us to prove that the QUSUM strategy retains its asymptotic optimality, characterized by the relationship between the expected detection delay and the average false alarm time for any pair of states with finite relative entropy. Consequently, our findings apply broadly, including continuous-variable systems (e.g., Gaussian states), facilitating the development of optimal change-point detection schemes in quantum optics and other physical platforms, and rendering experimental verification feasible.","authors":["Tiju Cherian John","Christos N. Gagatsos","Boulat A. Bash"],"url":"https://arxiv.org/abs/2504.16259"}
{"created":"2025-05-08","title":"IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery","abstract":"The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System","authors":["Aniketh Garikaparthi","Manasi Patwardhan","Lovekesh Vig","Arman Cohan"],"url":"https://arxiv.org/abs/2504.16728"}
{"created":"2025-05-08","title":"Generative AI Literacy: A Comprehensive Framework for Literacy and Responsible Use","abstract":"After the release of several AI literacy guidelines, the rapid rise and widespread adoption of generative AI, such as ChatGPT, Dall E, and Deepseek, have transformed our lives. Unlike traditional AI algorithms (e.g., convolutional neural networks, semantic networks, classifiers) captured in existing AI literacy frameworks, generative AI exhibits distinct and more nuanced characteristics. However, a lack of robust generative AI literacy is hindering individuals ability to evaluate critically and use these models effectively and responsibly. To address this gap, we propose a set of guidelines with 12 items for generative AI literacy, organized into four key aspects: (1) Guidelines for Generative AI Tool Selection and Prompting, (2) Guidelines for Understanding Interaction with Generative AI, (3) Guidelines for Understanding Interaction with Generative AI, and (4) Guidelines for High Level Understanding of Generative AI. These guidelines aim to support schools, companies, educators, and organizations in developing frameworks that empower their members, such as students, employees, and stakeholders, to use generative AI in an efficient, ethical, and informed way.","authors":["Chengzhi Zhang","Brian Magerko"],"url":"https://arxiv.org/abs/2504.19038"}
{"created":"2025-05-08","title":"What is a Gaussian channel, and when is it physically implementable using a multiport interferometer?","abstract":"Quantum Gaussian channels are fundamental models for communication and information processing in continuous-variable quantum systems. This work addresses both foundational aspects and physical implementation pathways for these channels. Firstly, we provide a rigorous, unified framework by formally proving the equivalence of three principal definitions of quantum Gaussian channels prevalent in the literature, consolidating theoretical understanding. Secondly, we investigate the physical realization of these channels using multiport interferometers, a key platform in quantum optics. The central research contribution is a precise characterization of the channel parameters that correspond to Gaussian channels physically implementable via linear optical multiport interferometers. This characterization bridges the abstract mathematical description with concrete physical architectures. Along the way, we also resolve some questions posed by Parthasarathy (Indian J. Pure Appl. Math. 46, (2015)).","authors":["Repana Devendra","Tiju Cherian John","K. Sumesh"],"url":"https://arxiv.org/abs/2505.02834"}
{"created":"2025-05-08","title":"MathPartner is a breakthrough technology for natural sciences education, scientic and engineering applications","abstract":"The article provides a brief description of the MathPartner service. This freely available cloud-based Mathematics is a universal system for symbolic-numeric calculations. Its Mathpar language is a subset of the LaTeX language, but allows you to create mathematical texts that contain \"computable\" mathematical operators. This opens up completely new opportunities for improving the educational process for all natural science disciplines, for the use of mathematics in scientific and engineering calculations. To save and freely exchange educational and other texts in the Mathpar language, a GitHub repository has been created. It is concluded that cloud mathematics MathPartner is a new breakthrough technology for school and university natural science education, for scientific and engineering applications.","authors":["Gennadi Malaschonok","Roman Sakh"],"url":"https://arxiv.org/abs/2505.03740"}
{"created":"2025-05-08","title":"The Evolution of Rough Sets 1970s-1981","abstract":"In this note research and publications by Zdzis{\\l}aw Pawlak and his collaborators from 1970s and 1981 are recalled. Focus is placed on the sources of inspiration which one can identify on the basis of those publications. Finally, developments from 1981 related to rough sets and information systems are outlined.","authors":["Viktor Marek","Ewa Or{\\l}owska","Ivo D\\\"untsch"],"url":"https://arxiv.org/abs/2505.03747"}
{"created":"2025-05-08","title":"On the Residual-based Neural Network for Unmodeled Distortions in Coordinate Transformation","abstract":"Coordinate transformation models often fail to account for nonlinear and spatially dependent distortions, leading to significant residual errors in geospatial applications. Here we propose a residual-based neural correction strategy, in which a neural network learns to model only the systematic distortions left by an initial geometric transformation. By focusing solely on residual patterns, the proposed method reduces model complexity and improves performance, particularly in scenarios with sparse or structured control point configurations. We evaluate the method using both simulated datasets with varying distortion intensities and sampling strategies, as well as under the real-world image georeferencing tasks. Compared with direct neural network coordinate converter and classical transformation models, the residual-based neural correction delivers more accurate and stable results under challenging conditions, while maintaining comparable performance in ideal cases. These findings demonstrate the effectiveness of residual modelling as a lightweight and robust alternative for improving coordinate transformation accuracy.","authors":["Vinicius Francisco Rofatto","Luiz Felipe Rodrigues de Almeida","Marcelo Tomio Matsuoka","Ivandro Klein","Mauricio Roberto Veronez","Luiz Gonzaga Da Silveira Junior"],"url":"https://arxiv.org/abs/2505.03757"}
{"created":"2025-05-08","title":"Deep Reinforcement Learning for Investor-Specific Portfolio Optimization: A Volatility-Guided Asset Selection Approach","abstract":"Portfolio optimization requires dynamic allocation of funds by balancing the risk and return tradeoff under dynamic market conditions. With the recent advancements in AI, Deep Reinforcement Learning (DRL) has gained prominence in providing adaptive and scalable strategies for portfolio optimization. However, the success of these strategies depends not only on their ability to adapt to market dynamics but also on the careful pre-selection of assets that influence overall portfolio performance. Incorporating the investor's preference in pre-selecting assets for a portfolio is essential in refining their investment strategies. This study proposes a volatility-guided DRL-based portfolio optimization framework that dynamically constructs portfolios based on investors' risk profiles. The Generalized Autoregressive Conditional Heteroscedasticity (GARCH) model is utilized for volatility forecasting of stocks and categorizes them based on their volatility as aggressive, moderate, and conservative. The DRL agent is then employed to learn an optimal investment policy by interacting with the historical market data. The efficacy of the proposed methodology is established using stocks from the Dow $30$ index. The proposed investor-specific DRL-based portfolios outperformed the baseline strategies by generating consistent risk-adjusted returns.","authors":["Arishi Orra","Aryan Bhambu","Himanshu Choudhary","Manoj Thakur","Selvaraju Natarajan"],"url":"https://arxiv.org/abs/2505.03760"}
{"created":"2025-05-08","title":"Cer-Eval: Certifiable and Cost-Efficient Evaluation Framework for LLMs","abstract":"As foundation models continue to scale, the size of trained models grows exponentially, presenting significant challenges for their evaluation. Current evaluation practices involve curating increasingly large datasets to assess the performance of large language models (LLMs). However, there is a lack of systematic analysis and guidance on determining the sufficiency of test data or selecting informative samples for evaluation. This paper introduces a certifiable and cost-efficient evaluation framework for LLMs. Our framework adapts to different evaluation objectives and outputs confidence intervals that contain true values with high probability. We use ``test sample complexity'' to quantify the number of test points needed for a certifiable evaluation and derive tight bounds on test sample complexity. Based on the developed theory, we develop a partition-based algorithm, named Cer-Eval, that adaptively selects test points to minimize the cost of LLM evaluation. Real-world experiments demonstrate that Cer-Eval can save 20% to 40% test points across various benchmarks, while maintaining an estimation error level comparable to the current evaluation process and providing a 95% confidence guarantee.","authors":["Ganghua Wang","Zhaorun Chen","Bo Li","Haifeng Xu"],"url":"https://arxiv.org/abs/2505.03814"}
{"created":"2025-05-08","title":"IntelliCardiac: An Intelligent Platform for Cardiac Image Segmentation and Classification","abstract":"Precise and effective processing of cardiac imaging data is critical for the identification and management of the cardiovascular diseases. We introduce IntelliCardiac, a comprehensive, web-based medical image processing platform for the automatic segmentation of 4D cardiac images and disease classification, utilizing an AI model trained on the publicly accessible ACDC dataset. The system, intended for patients, cardiologists, and healthcare professionals, offers an intuitive interface and uses deep learning models to identify essential heart structures and categorize cardiac diseases. The system supports analysis of both the right and left ventricles as well as myocardium, and then classifies patient's cardiac images into five diagnostic categories: dilated cardiomyopathy, myocardial infarction, hypertrophic cardiomyopathy, right ventricular abnormality, and no disease. IntelliCardiac combines a deep learning-based segmentation model with a two-step classification pipeline. The segmentation module gains an overall accuracy of 92.6\\%. The classification module, trained on characteristics taken from segmented heart structures, achieves 98\\% accuracy in five categories. These results exceed the performance of the existing state-of-the-art methods that integrate both segmentation and classification models. IntelliCardiac, which supports real-time visualization, workflow integration, and AI-assisted diagnostics, has great potential as a scalable, accurate tool for clinical decision assistance in cardiac imaging and diagnosis.","authors":["Ting Yu Tsai","An Yu","Meghana Spurthi Maadugundu","Ishrat Jahan Mohima","Umme Habiba Barsha","Mei-Hwa F. Chen","Balakrishnan Prabhakaran","Ming-Ching Chang"],"url":"https://arxiv.org/abs/2505.03838"}
{"created":"2025-05-08","title":"From Spaceborn to Airborn: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation","abstract":"The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.","authors":["Sol\\`ene Debuys\\`ere","Nicolas Trouv\\'e","Nathan Letheule","Olivier L\\'ev\\^eque","Elise Colin"],"url":"https://arxiv.org/abs/2505.03844"}
{"created":"2025-05-08","title":"A Deep Learning approach for Depressive Symptoms assessment in Parkinson's disease patients using facial videos","abstract":"Parkinson's disease (PD) is a neurodegenerative disorder, manifesting with motor and non-motor symptoms. Depressive symptoms are prevalent in PD, affecting up to 45% of patients. They are often underdiagnosed due to overlapping motor features, such as hypomimia. This study explores deep learning (DL) models-ViViT, Video Swin Tiny, and 3D CNN-LSTM with attention layers-to assess the presence and severity of depressive symptoms, as detected by the Geriatric Depression Scale (GDS), in PD patients through facial video analysis. The same parameters were assessed in a secondary analysis taking into account whether patients were one hour after (ON-medication state) or 12 hours without (OFF-medication state) dopaminergic medication. Using a dataset of 1,875 videos from 178 patients, the Video Swin Tiny model achieved the highest performance, with up to 94% accuracy and 93.7% F1-score in binary classification (presence of absence of depressive symptoms), and 87.1% accuracy with an 85.4% F1-score in multiclass tasks (absence or mild or severe depressive symptoms).","authors":["Ioannis Kyprakis","Vasileios Skaramagkas","Iro Boura","Georgios Karamanis","Dimitrios I. Fotiadis","Zinovia Kefalopoulou","Cleanthe Spanaki","Manolis Tsiknakis"],"url":"https://arxiv.org/abs/2505.03845"}
{"created":"2025-05-08","title":"GRAPE: Heterogeneous Graph Representation Learning for Genetic Perturbation with Coding and Non-Coding Biotype","abstract":"Predicting genetic perturbations enables the identification of potentially crucial genes prior to wet-lab experiments, significantly improving overall experimental efficiency. Since genes are the foundation of cellular life, building gene regulatory networks (GRN) is essential to understand and predict the effects of genetic perturbations. However, current methods fail to fully leverage gene-related information, and solely rely on simple evaluation metrics to construct coarse-grained GRN. More importantly, they ignore functional differences between biotypes, limiting the ability to capture potential gene interactions. In this work, we leverage pre-trained large language model and DNA sequence model to extract features from gene descriptions and DNA sequence data, respectively, which serve as the initialization for gene representations. Additionally, we introduce gene biotype information for the first time in genetic perturbation, simulating the distinct roles of genes with different biotypes in regulating cellular processes, while capturing implicit gene relationships through graph structure learning (GSL). We propose GRAPE, a heterogeneous graph neural network (HGNN) that leverages gene representations initialized with features from descriptions and sequences, models the distinct roles of genes with different biotypes, and dynamically refines the GRN through GSL. The results on publicly available datasets show that our method achieves state-of-the-art performance.","authors":["Changxi Chi","Jun Xia","Jingbo Zhou","Jiabei Cheng","Chang Yu","Stan Z. Li"],"url":"https://arxiv.org/abs/2505.03853"}
{"created":"2025-05-08","title":"Categorical and geometric methods in statistical, manifold, and machine learning","abstract":"We present and discuss applications of the category of probabilistic morphisms, initially developed in \\cite{Le2023}, as well as some geometric methods to several classes of problems in statistical, machine and manifold learning which shall be, along with many other topics, considered in depth in the forthcoming book \\cite{LMPT2024}.","authors":["H\\^ong V\\^an L\\^e","H\\`a Quang Minh","Frederic Protin","Wilderich Tuschmann"],"url":"https://arxiv.org/abs/2505.03862"}
{"created":"2025-05-08","title":"A Graphical Global Optimization Framework for Parameter Estimation of Statistical Models with Nonconvex Regularization Functions","abstract":"Optimization problems with norm-bounding constraints arise in a variety of applications, including portfolio optimization, machine learning, and feature selection. A common approach to these problems involves relaxing the norm constraint via Lagrangian relaxation, transforming it into a regularization term in the objective function. A particularly challenging class includes the zero-norm function, which promotes sparsity in statistical parameter estimation. Most existing exact methods for solving these problems introduce binary variables and artificial bounds to reformulate them as higher-dimensional mixed-integer programs, solvable by standard solvers. Other exact approaches exploit specific structural properties of the objective, making them difficult to generalize across different problem types. Alternative methods employ nonconvex penalties with favorable statistical characteristics, but these are typically addressed using heuristic or local optimization techniques due to their structural complexity. In this paper, we propose a novel graph-based method to globally solve optimization problems involving generalized norm-bounding constraints. Our approach encompasses standard $\\ell_p$-norms for $p \\in [0, \\infty)$ and nonconvex penalties such as SCAD and MCP. We leverage decision diagrams to construct strong convex relaxations directly in the original variable space, eliminating the need for auxiliary variables or artificial bounds. Integrated into a spatial branch-and-cut framework, our method guarantees convergence to the global optimum. We demonstrate its effectiveness through preliminary computational experiments on benchmark sparse linear regression problems involving complex nonconvex penalties, which are not tractable using existing global optimization techniques.","authors":["Danial Davarnia","Mohammadreza Kiaghadi"],"url":"https://arxiv.org/abs/2505.03899"}
{"created":"2025-05-08","title":"Alternating projections between two inconsistent affine subspaces with varying relaxation","abstract":"In a Hilbert space, we study the convergence in norm of alternating projections between two inconsistent affine subspaces with varying relaxation on one side.","authors":["Nguyen T. Thao"],"url":"https://arxiv.org/abs/2505.03982"}
{"created":"2025-05-08","title":"Prototype-Based Information Compensation Network for Multi-Source Remote Sensing Data Classification","abstract":"Multi-source remote sensing data joint classification aims to provide accuracy and reliability of land cover classification by leveraging the complementary information from multiple data sources. Existing methods confront two challenges: inter-frequency multi-source feature coupling and inconsistency of complementary information exploration. To solve these issues, we present a Prototype-based Information Compensation Network (PICNet) for land cover classification based on HSI and SAR/LiDAR data. Specifically, we first design a frequency interaction module to enhance the inter-frequency coupling in multi-source feature extraction. The multi-source features are first decoupled into high- and low-frequency components. Then, these features are recoupled to achieve efficient inter-frequency communication. Afterward, we design a prototype-based information compensation module to model the global multi-source complementary information. Two sets of learnable modality prototypes are introduced to represent the global modality information of multi-source data. Subsequently, cross-modal feature integration and alignment are achieved through cross-attention computation between the modality-specific prototype vectors and the raw feature representations. Extensive experiments on three public datasets demonstrate the significant superiority of our PICNet over state-of-the-art methods. The codes are available at https://github.com/oucailab/PICNet.","authors":["Feng Gao","Sheng Liu","Chuanzheng Gong","Xiaowei Zhou","Jiayi Wang","Junyu Dong","Qian Du"],"url":"https://arxiv.org/abs/2505.04003"}
{"created":"2025-05-08","title":"Variational Formulation of the Particle Flow Particle Filter","abstract":"This paper provides a formulation of the particle flow particle filter from the perspective of variational inference. We show that the transient density used to derive the particle flow particle filter follows a time-scaled trajectory of the Fisher-Rao gradient flow in the space of probability densities. The Fisher-Rao gradient flow is obtained as a continuous-time algorithm for variational inference, minimizing the Kullback-Leibler divergence between a variational density and the true posterior density.","authors":["Yinzhuang Yi","Jorge Cort\\'es","Nikolay Atanasov"],"url":"https://arxiv.org/abs/2505.04007"}
{"created":"2025-05-08","title":"Recovery of the matrix potential of the one-dimensional Dirac equation from spectral data","abstract":"A method for solving an inverse spectral problem for the one-dimensional Dirac equation is developed. The method is based on the Gelfand-Levitan equation and the Fourier-Legendre series expansion of the transmutation kernel. A linear algebraic system of equations is obtained, which can be solved numerically. To the best of our knowledge, this is the first practical method for the solution of the inverse problem for the one-dimensional Dirac equation on a finite interval.","authors":["Emmanuel Roque","Sergii M. Torba"],"url":"https://arxiv.org/abs/2505.04010"}
{"created":"2025-05-08","title":"Learning based convex approximation for constrained parametric optimization","abstract":"We propose an input convex neural network (ICNN)-based self-supervised learning framework to solve continuous constrained optimization problems. By integrating the augmented Lagrangian method (ALM) with the constraint correction mechanism, our framework ensures \\emph{non-strict constraint feasibility}, \\emph{better optimality gap}, and \\emph{best convergence rate} with respect to the state-of-the-art learning-based methods. We provide a rigorous convergence analysis, showing that the algorithm converges to a Karush-Kuhn-Tucker (KKT) point of the original problem even when the internal solver is a neural network, and the approximation error is bounded. We test our approach on a range of benchmark tasks including quadratic programming (QP), nonconvex programming, and large-scale AC optimal power flow problems. The results demonstrate that compared to existing solvers (e.g., \\texttt{OSQP}, \\texttt{IPOPT}) and the latest learning-based methods (e.g., DC3, PDL), our approach achieves a superior balance among accuracy, feasibility, and computational efficiency.","authors":["Kang Liu","Wei Peng","Jianchen Hu"],"url":"https://arxiv.org/abs/2505.04037"}
{"created":"2025-05-08","title":"Multilevel Sampling in Algebraic Statistics","abstract":"This paper proposes a multilevel sampling algorithm for fiber sampling problems in algebraic statistics, inspired by Henry Wynn's suggestion to adapt multilevel Monte Carlo (MLMC) ideas to discrete models. Focusing on log-linear models, we sample from high-dimensional lattice fibers defined by algebraic constraints. Building on Markov basis methods and results from Diaconis and Sturmfels, our algorithm uses variable step sizes to accelerate exploration and reduce the need for long burn-in. We introduce a novel Fiber Coverage Score (FCS) based on Voronoi partitioning to assess sample quality, and highlight the utility of the Maximum Mean Discrepancy (MMD) quality metric. Simulations on benchmark fibers show that multilevel sampling outperforms naive MCMC approaches. Our results demonstrate that multilevel methods, when properly applied, provide practical benefits for discrete sampling in algebraic statistics.","authors":["Nathan Kirk","Ivan Gvozdanovi\\'c","Sonja Petrovi\\'c"],"url":"https://arxiv.org/abs/2505.04062"}
{"created":"2025-05-08","title":"Aliasing Reduction in Neural Amp Modeling by Smoothing Activations","abstract":"The increasing demand for high-quality digital emulations of analog audio hardware such as vintage guitar amplifiers has led to numerous works in neural-network-based black-box modeling, with deep learning architectures like WaveNet showing promising results. However, a key limitation in all of these models is the aliasing artifacts that arise from the use of nonlinear activation functions in neural networks. In this paper, we investigate novel and modified activation functions aimed at mitigating aliasing within neural amplifier models. Supporting this, we introduce a novel metric, the Aliasing-to-Signal Ratio (ASR), which quantitatively assesses the level of aliasing with high accuracy. Measuring also the conventional Error-to-Signal Ratio (ESR), we conducted studies on a range of preexisting and modern activation functions with varying stretch factors. Our findings confirmed that activation functions with smoother curves tend to achieve lower ASR values, indicating a noticeable reduction in aliasing. Notably, this improvement in aliasing reduction was achievable without a substantial increase in ESR, demonstrating the potential for high modeling accuracy with reduced aliasing in neural amp models.","authors":["Ryota Sato","Julius O. Smith III"],"url":"https://arxiv.org/abs/2505.04082"}
{"created":"2025-05-08","title":"3D Brain MRI Classification for Alzheimer Diagnosis Using CNN with Data Augmentation","abstract":"A three-dimensional convolutional neural network was developed to classify T1-weighted brain MRI scans as healthy or Alzheimer. The network comprises 3D convolution, pooling, batch normalization, dense ReLU layers, and a sigmoid output. Using stochastic noise injection and five-fold cross-validation, the model achieved test set accuracy of 0.912 and area under the ROC curve of 0.961, an improvement of approximately 0.027 over resizing alone. Sensitivity and specificity both exceeded 0.90. These results align with prior work reporting up to 0.10 gain via synthetic augmentation. The findings demonstrate the effectiveness of simple augmentation for 3D MRI classification and motivate future exploration of advanced augmentation methods and architectures such as 3D U-Net and vision transformers.","authors":["Thien Nhan Vo","Bac Nam Ho","Thanh Xuan Truong"],"url":"https://arxiv.org/abs/2505.04097"}
{"created":"2025-05-08","title":"UX-aware Rate Allocation for Real-Time Media","abstract":"Immersive communications is a key use case for 6G where applications require reliable latency-bound media traffic at a certain data rate to deliver an acceptable User Experience (UX) or Quality-of-Experience (QoE). The Quality-of-Service (QoS) framework of current cellular systems (4G and 5G) and prevalent network congestion control algorithms for latency-bound traffic like L4S typically target network-related Key Performance Indicators (KPIs) such as data rates and latencies. Network capacity is based on the number of users that attain these KPIs. However, the UX of an immersive application for a given data rate and latency is not the same across users, since it depends on other factors such as the complexity of the media being transmitted and the encoder format. This implies that guarantees on network KPIs do not necessarily translate to guarantees on the UX.","authors":["Belal Korany","Peerapol Tinnakornsrisuphap","Saadallah Kassir","Prashanth Hande","Hyun Yong Lee","Thomas Stockhammer"],"url":"https://arxiv.org/abs/2505.04114"}
{"created":"2025-05-08","title":"Energy Efficient RSMA-Based LEO Satellite Communications Assisted by UAV-Mounted BD-Active RIS: A DRL Approach","abstract":"This paper proposes an advanced non-terrestrial communication architecture that integrates Rate-Splitting Multiple Access (RSMA) with a Beyond-Diagonal Active Reconfigurable Intelligent Surface (BD-ARIS) mounted on a UAV under the coverage of a Low Earth Orbit (LEO) satellite. The BD-ARIS adopts a group-connected structure to enhance signal amplification and adaptability, while RSMA enables efficient multi-user access by dividing messages into common and private components. The system jointly optimizes satellite beamforming, UAV positioning, power allocation, and rate-splitting ratios to maximize the overall energy efficiency (EE). To solve the resulting non-convex and high-dimensional problem, we employ three state-of-the-art deep reinforcement learning (DRL) algorithms: Trust Region Policy Optimization (TRPO), Twin Delayed Deep Deterministic Policy Gradient (TD3), and Asynchronous Advantage Actor-Critic (A3C). Moreover, realistic models for the power consumption of both the UAV and the BD-ARIS are considered. Simulation results reveal that TRPO consistently achieves the best performance in terms of EE and sum rate, especially under high transmit powers and challenging deployment scenarios. TD3 converges faster and performs competitively in moderate settings, while A3C suffers from instability due to its high variance. Additionally, the robustness of each algorithm under channel state information (CSI) uncertainty is evaluated, confirming TRPO resilience to imperfect observations. Overall, the proposed RSMA-BD-ARIS framework significantly outperforms conventional RIS-assisted designs and provides a scalable, energy-efficient solution for 6G and massive IoT applications in non-terrestrial networks.","authors":["Rahman Saadat Yeganeh","Hamid Behroozi"],"url":"https://arxiv.org/abs/2505.04148"}
{"created":"2025-05-08","title":"A Dataset and Toolkit for Multiparameter Cardiovascular Physiology Sensing on Rings","abstract":"Smart rings offer a convenient way to continuously and unobtrusively monitor cardiovascular physiological signals. However, a gap remains between the ring hardware and reliable methods for estimating cardiovascular parameters, partly due to the lack of publicly available datasets and standardized analysis tools. In this work, we present $\\tau$-Ring, the first open-source ring-based dataset designed for cardiovascular physiological sensing. The dataset comprises photoplethysmography signals (infrared and red channels) and 3-axis accelerometer data collected from two rings (reflective and transmissive optical paths), with 28.21 hours of raw data from 34 subjects across seven activities. $\\tau$-Ring encompasses both stationary and motion scenarios, as well as stimulus-evoked abnormal physiological states, annotated with four ground-truth labels: heart rate, respiratory rate, oxygen saturation, and blood pressure. Using our proposed RingTool toolkit, we evaluated three widely-used physics-based methods and four cutting-edge deep learning approaches. Our results show superior performance compared to commercial rings, achieving best MAE values of 5.18 BPM for heart rate, 2.98 BPM for respiratory rate, 3.22\\% for oxygen saturation, and 13.33/7.56 mmHg for systolic/diastolic blood pressure estimation. The open-sourced dataset and toolkit aim to foster further research and community-driven advances in ring-based cardiovascular health sensing.","authors":["Iankai Tang","Kegang Wang","Yingke Ding","Jiatong Ji","Zeyu Wang","Xiyuxing Zhang","Ping Chen","Yuanchun Shi","Yuntao Wang"],"url":"https://arxiv.org/abs/2505.04172"}
{"created":"2025-05-08","title":"The stability of generalized phase retrieval problem over compact groups","abstract":"The generalized phase retrieval problem over compact groups aims to recover a set of matrices, representing an unknown signal, from their associated Gram matrices, leveraging prior structural knowledge about the signal. This framework generalizes the classical phase retrieval problem, which reconstructs a signal from the magnitudes of its Fourier transform, to a richer setting involving non-abelian compact groups. In this broader context, the unknown phases in Fourier space are replaced by unknown orthogonal matrices that arise from the action of a compact group on a finite-dimensional vector space. This problem is primarily motivated by advances in electron microscopy to determining the 3D structure of biological macromolecules from highly noisy observations. To capture realistic assumptions from machine learning and signal processing, we model the signal as belonging to one of several broad structural families: a generic linear subspace, a sparse representation in a generic basis, the output of a generic ReLU neural network, or a generic low-dimensional manifold. Our main result shows that, under mild conditions, the generalized phase retrieval problem not only admits a unique solution (up to inherent group symmetries), but also satisfies a bi-Lipschitz property. This implies robustness to both noise and model mismatch, an essential requirement for practical use, especially when measurements are severely corrupted by noise. These findings provide theoretical support for a wide class of scientific problems under modern structural assumptions, and they offer strong foundations for developing robust algorithms in high-noise regimes.","authors":["Tal Amir","Tamir Bendory","Nadav Dym","Dan Edidin"],"url":"https://arxiv.org/abs/2505.04190"}
{"created":"2025-05-08","title":"On the mutiplicities of interpoint distances","abstract":"Given a set $X\\subseteq\\mathbb{R}^2$ of $n$ points and a distance $d>0$, the multiplicity of $d$ is the number of times the distance $d$ appears between points in $X$. Let $a_1(X) \\geq a_2(X) \\geq \\cdots \\geq a_m(X)$ denote the multiplicities of the $m$ distances determined by $X$ and let $a(X)=\\left(a_1(X),\\dots,a_m(X)\\right)$. In this paper, we study several questions from Erd\\H{o}s's time regarding distance multiplicities. Among other results, we show that:","authors":["Felix Christian Clemen","Adrian Dumitrescu","Dingyuan Liu"],"url":"https://arxiv.org/abs/2505.04283"}
{"created":"2025-05-08","title":"Sparsity is All You Need: Rethinking Biological Pathway-Informed Approaches in Deep Learning","abstract":"Biologically-informed neural networks typically leverage pathway annotations to enhance performance in biomedical applications. We hypothesized that the benefits of pathway integration does not arise from its biological relevance, but rather from the sparsity it introduces. We conducted a comprehensive analysis of all relevant pathway-based neural network models for predictive tasks, critically evaluating each study's contributions. From this review, we curated a subset of methods for which the source code was publicly available. The comparison of the biologically informed state-of-the-art deep learning models and their randomized counterparts showed that models based on randomized information performed equally well as biologically informed ones across different metrics and datasets. Notably, in 3 out of the 15 analyzed models, the randomized versions even outperformed their biologically informed counterparts. Moreover, pathway-informed models did not show any clear advantage in interpretability, as randomized models were still able to identify relevant disease biomarkers despite lacking explicit pathway information. Our findings suggest that pathway annotations may be too noisy or inadequately explored by current methods. Therefore, we propose a methodology that can be applied to different domains and can serve as a robust benchmark for systematically comparing novel pathway-informed models against their randomized counterparts. This approach enables researchers to rigorously determine whether observed performance improvements can be attributed to biological insights.","authors":["Isabella Caranzano","Corrado Pancotti","Cesare Rollo","Flavio Sartori","Pietro Li\\`o","Piero Fariselli","Tiziana Sanavia"],"url":"https://arxiv.org/abs/2505.04300"}
{"created":"2025-05-08","title":"Quantum Circuits for the Black-Scholes equations via Schr\\\"{o}dingerisation","abstract":"In this paper, we construct quantum circuits for the Black-Scholes equations, a cornerstone of financial modeling, based on a quantum algorithm that overcome the cure of high dimensionality. Our approach leverages the Schr\\\"odingerisation technique, which converts linear partial and ordinary differential equations with non-unitary dynamics into a system evolved by unitary dynamics. This is achieved through a warped phase transformation that lifts the problem into a higher-dimensional space, enabling the simulation of the Black-Scholes equation on a quantum computer. We will conduct a thorough complexity analysis to highlight the quantum advantages of our approach compared to existing algorithms. The effectiveness of our quantum circuit is substantiated through extensive numerical experiments.","authors":["Shi Jin","Zihao Tang","Xu Yin","Lei Zhang"],"url":"https://arxiv.org/abs/2505.04304"}
{"created":"2025-05-08","title":"Optimization Problem Solving Can Transition to Evolutionary Agentic Workflows","abstract":"This position paper argues that optimization problem solving can transition from expert-dependent to evolutionary agentic workflows. Traditional optimization practices rely on human specialists for problem formulation, algorithm selection, and hyperparameter tuning, creating bottlenecks that impede industrial adoption of cutting-edge methods. We contend that an evolutionary agentic workflow, powered by foundation models and evolutionary search, can autonomously navigate the optimization space, comprising problem, formulation, algorithm, and hyperparameter spaces. Through case studies in cloud resource scheduling and ADMM parameter adaptation, we demonstrate how this approach can bridge the gap between academic innovation and industrial implementation. Our position challenges the status quo of human-centric optimization workflows and advocates for a more scalable, adaptive approach to solving real-world optimization problems.","authors":["Wenhao Li","Bo Jin","Mingyi Hong","Changhong Lu","Xiangfeng Wang"],"url":"https://arxiv.org/abs/2505.04354"}
{"created":"2025-05-08","title":"Improved bounds on the zeros of the chromatic polynomial of graphs and claw-free graphs","abstract":"We prove that for any graph $G$ the (complex) zeros of its chromatic polynomial, $\\chi_G(x)$, lie inside the disk centered at $0$ of radius $4.25 \\Delta(G)$, where $\\Delta(G)$ denote the maximum degree of $G$. This improves on a recent result of Jenssen, Patel and the second author, who proved a bound of $5.94\\Delta(G)$. We moreover show that for graphs of sufficiently large girth we can replace $4.25$ by $3.60$ and for claw-free graphs we can replace $4.25$ by $3.81$.","authors":["Ferenc Bencs","Guus Regts"],"url":"https://arxiv.org/abs/2505.04366"}
{"created":"2025-05-08","title":"Discrete Optimal Transport and Voice Conversion","abstract":"In this work, we address the voice conversion (VC) task using a vector-based interface. To align audio embeddings between speakers, we employ discrete optimal transport mapping. Our evaluation results demonstrate the high quality and effectiveness of this method. Additionally, we show that applying discrete optimal transport as a post-processing step in audio generation can lead to the incorrect classification of synthetic audio as real.","authors":["Anton Selitskiy","Maitreya Kocharekar"],"url":"https://arxiv.org/abs/2505.04382"}
{"created":"2025-05-08","title":"A Heuristic-Integrated DRL Approach for Phase Optimization in Large-Scale RISs","abstract":"Optimizing discrete phase shifts in large-scale reconfigurable intelligent surfaces (RISs) is challenging due to their non-convex and non-linear nature. In this letter, we propose a heuristic-integrated deep reinforcement learning (DRL) framework that (1) leverages accumulated actions over multiple steps in the double deep Q-network (DDQN) for RIS column-wise control and (2) integrates a greedy algorithm (GA) into each DRL step to refine the state via fine-grained, element-wise optimization of RIS configurations. By learning from GA-included states, the proposed approach effectively addresses RIS optimization within a small DRL action space, demonstrating its capability to optimize phase-shift configurations of large-scale RISs.","authors":["Wei Wang","Peizheng Li","Angela Doufexi","Mark A. Beach"],"url":"https://arxiv.org/abs/2505.04401"}
{"created":"2025-05-08","title":"High-speed multiwavelength photonic temporal integration using silicon photonics","abstract":"Optical systems have been pivotal for energy-efficient computing, performing high-speed, parallel operations in low-loss carriers. While these predominantly analog optical accelerators bypass digitization to perform parallel floating-point computations, scaling optical hardware to map large-vector sizes for AI tasks remains challenging. Here, we overcome this limitation by unfolding scalar operations in time and introducing a photonic-heater-in-lightpath (PHIL) unit for all-optical temporal integration. Counterintuitively, we exploit a slow heat dissipation process to integrate optical signals modulated at 50 GHz bridging the speed gap between the widely applied thermo-optic effects and ultrafast photonics. This architecture supports optical end-to-end signal processing, eliminates inefficient electro-optical conversions, and enables both linear and nonlinear operations within a unified framework. Our results demonstrate a scalable path towards high-speed photonic computing through thermally driven integration.","authors":["Yi Zhang","Nikolaos Farmakidis","Ioannis Roumpos","Miltiadis Moralis-Pegios","Apostolos Tsakyridis","June Sang Lee","Bowei Dong","Yuhan He","Samarth Aggarwal","Nikolaos Pleros","Harish Bhaskaran"],"url":"https://arxiv.org/abs/2505.04405"}
{"created":"2025-05-08","title":"Recognizing Ornaments in Vocal Indian Art Music with Active Annotation","abstract":"Ornamentations, embellishments, or microtonal inflections are essential to melodic expression across many musical traditions, adding depth, nuance, and emotional impact to performances. Recognizing ornamentations in singing voices is key to MIR, with potential applications in music pedagogy, singer identification, genre classification, and controlled singing voice generation. However, the lack of annotated datasets and specialized modeling approaches remains a major obstacle for progress in this research area. In this work, we introduce R\\=aga Ornamentation Detection (ROD), a novel dataset comprising Indian classical music recordings curated by expert musicians. The dataset is annotated using a custom Human-in-the-Loop tool for six vocal ornaments marked as event-based labels. Using this dataset, we develop an ornamentation detection model based on deep time-series analysis, preserving ornament boundaries during the chunking of long audio recordings. We conduct experiments using different train-test configurations within the ROD dataset and also evaluate our approach on a separate, manually annotated dataset of Indian classical concert recordings. Our experimental results support the superior performance of our proposed approach over the baseline CRNN.","authors":["Sumit Kumar","Parampreet Singh","Vipul Arora"],"url":"https://arxiv.org/abs/2505.04419"}
{"created":"2025-05-08","title":"Adaptive finite element method for an unregularized semilinear optimal control problem","abstract":"We devise an a posteriori error estimator for an affine optimal control problem subject to a semilinear elliptic PDE and control constraints. To approximate the problem, we consider a semidiscrete scheme based on the variational discretization approach. For this solution technique, we design an a posteriori error estimator that accounts for the discretization of the state and adjoint equations, and prove, under suitable local growth conditions of optimal controls, reliability and efficiency properties of such error estimator. A simple adaptive strategy based on the devised estimator is designed and its performance is illustrated with numerical examples.","authors":["Francisco Fuica","Nicolai Jork"],"url":"https://arxiv.org/abs/2505.04439"}
{"created":"2025-05-08","title":"A Tutorial on Discriminative Clustering and Mutual Information","abstract":"To cluster data is to separate samples into distinctive groups that should ideally have some cohesive properties. Today, numerous clustering algorithms exist, and their differences lie essentially in what can be perceived as ``cohesive properties''. Therefore, hypotheses on the nature of clusters must be set: they can be either generative or discriminative. As the last decade witnessed the impressive growth of deep clustering methods that involve neural networks to handle high-dimensional data often in a discriminative manner; we concentrate mainly on the discriminative hypotheses. In this paper, our aim is to provide an accessible historical perspective on the evolution of discriminative clustering methods and notably how the nature of assumptions of the discriminative models changed over time: from decision boundaries to invariance critics. We notably highlight how mutual information has been a historical cornerstone of the progress of (deep) discriminative clustering methods. We also show some known limitations of mutual information and how discriminative clustering methods tried to circumvent those. We then discuss the challenges that discriminative clustering faces with respect to the selection of the number of clusters. Finally, we showcase these techniques using the dedicated Python package, GemClus, that we have developed for discriminative clustering.","authors":["Louis Ohl","Pierre-Alexandre Mattei","Fr\\'ed\\'eric Precioso"],"url":"https://arxiv.org/abs/2505.04484"}
{"created":"2025-05-08","title":"A Two-Timescale Primal-Dual Framework for Reinforcement Learning via Online Dual Variable Guidance","abstract":"We study reinforcement learning by combining recent advances in regularized linear programming formulations with the classical theory of stochastic approximation. Motivated by the challenge of designing algorithms that leverage off-policy data while maintaining on-policy exploration, we propose PGDA-RL, a novel primal-dual Projected Gradient Descent-Ascent algorithm for solving regularized Markov Decision Processes (MDPs). PGDA-RL integrates experience replay-based gradient estimation with a two-timescale decomposition of the underlying nested optimization problem. The algorithm operates asynchronously, interacts with the environment through a single trajectory of correlated data, and updates its policy online in response to the dual variable associated with the occupation measure of the underlying MDP. We prove that PGDA-RL converges almost surely to the optimal value function and policy of the regularized MDP. Our convergence analysis relies on tools from stochastic approximation theory and holds under weaker assumptions than those required by existing primal-dual RL approaches, notably removing the need for a simulator or a fixed behavioral policy.","authors":["Axel Friedrich Wolter","Tobias Sutter"],"url":"https://arxiv.org/abs/2505.04494"}
{"created":"2025-05-08","title":"New bounds for proper $h$-conflict-free colourings","abstract":"A proper $k$-colouring of a graph $G$ is called $h$-conflict-free if every vertex $v$ has at least $\\min\\, \\{h, {\\rm deg}(v)\\}$ colours appearing exactly once in its neighbourhood. Let $\\chi_{\\rm pcf}^h(G)$ denote the minimum $k$ such that such a colouring exists. We show that for every fixed $h\\ge 1$, every graph $G$ of maximum degree $\\Delta$ satisfies $\\chi_{\\rm pcf}^h(G) \\le h\\Delta + \\mathcal{O}(\\log \\Delta)$. This expands on the work of Cho et al., and improves a recent result of Liu and Reed in the case $h=1$. We conjecture that for every $h\\ge 1$ and every graph $G$ of maximum degree $\\Delta$ sufficiently large, the bound $\\chi_{\\rm pcf}^h(G) \\le h\\Delta + 1$ should hold, which would be tight. When the minimum degree $\\delta$ of $G$ is sufficiently large, namely $\\delta \\ge \\max\\{100h, 3000\\log \\Delta\\}$, we show that this upper bound can be further reduced to $\\chi_{\\rm{pcf}}^h(G) \\le \\Delta + \\mathcal{O}(\\sqrt{h\\Delta})$. This improves a recent bound from Kamyczura and Przyby{\\l}o when $\\delta \\le \\sqrt{h\\Delta}$.","authors":["Quentin Chuet","Tianjiao Dai","Qiancheng Ouyang","Fran\\c{c}ois Pirot"],"url":"https://arxiv.org/abs/2505.04543"}
{"created":"2025-05-08","title":"Accelerating Audio Research with Robotic Dummy Heads","abstract":"This work introduces a robotic dummy head that fuses the acoustic realism of conventional audiological mannequins with the mobility of robots. The proposed device is capable of moving, talking, and listening as people do, and can be used to automate spatially-stationary audio experiments, thus accelerating the pace of audio research. Critically, the device may also be used as a moving sound source in dynamic experiments, due to its quiet motor. This feature differentiates our work from previous robotic acoustic research platforms. Validation that the robot enables high quality audio data collection is provided through various experiments and acoustic measurements. These experiments also demonstrate how the robot might be used to study adaptive binaural beamforming. Design files are provided as open-source to stimulate novel audio research.","authors":["Austin Lu","Kanad Sarkar","Yongjie Zhuang","Leo Lin","Ryan M Corey","Andrew C Singer"],"url":"https://arxiv.org/abs/2505.04548"}
{"created":"2025-05-08","title":"Risk-sensitive Reinforcement Learning Based on Convex Scoring Functions","abstract":"We propose a reinforcement learning (RL) framework under a broad class of risk objectives, characterized by convex scoring functions. This class covers many common risk measures, such as variance, Expected Shortfall, entropic Value-at-Risk, and mean-risk utility. To resolve the time-inconsistency issue, we consider an augmented state space and an auxiliary variable and recast the problem as a two-state optimization problem. We propose a customized Actor-Critic algorithm and establish some theoretical approximation guarantees. A key theoretical contribution is that our results do not require the Markov decision process to be continuous. Additionally, we propose an auxiliary variable sampling method inspired by the alternating minimization algorithm, which is convergent under certain conditions. We validate our approach in simulation experiments with a financial application in statistical arbitrage trading, demonstrating the effectiveness of the algorithm.","authors":["Shanyu Han","Yang Liu","Xiang Yu"],"url":"https://arxiv.org/abs/2505.04553"}
{"created":"2025-05-08","title":"Dynamic Network Flow Optimization for Task Scheduling in PTZ Camera Surveillance Systems","abstract":"This paper presents a novel approach for optimizing the scheduling and control of Pan-Tilt-Zoom (PTZ) cameras in dynamic surveillance environments. The proposed method integrates Kalman filters for motion prediction with a dynamic network flow model to enhance real-time video capture efficiency. By assigning Kalman filters to tracked objects, the system predicts future locations, enabling precise scheduling of camera tasks. This prediction-driven approach is formulated as a network flow optimization, ensuring scalability and adaptability to various surveillance scenarios. To further reduce redundant monitoring, we also incorporate group-tracking nodes, allowing multiple objects to be captured within a single camera focus when appropriate. In addition, a value-based system is introduced to prioritize camera actions, focusing on the timely capture of critical events. By adjusting the decay rates of these values over time, the system ensures prompt responses to tasks with imminent deadlines. Extensive simulations demonstrate that this approach improves coverage, reduces average wait times, and minimizes missed events compared to traditional master-slave camera systems. Overall, our method significantly enhances the efficiency, scalability, and effectiveness of surveillance systems, particularly in dynamic and crowded environments.","authors":["Mohammad Merati","David Casta\\~n\\'on"],"url":"https://arxiv.org/abs/2505.04596"}
{"created":"2025-05-08","title":"Likelihood-Free Adaptive Bayesian Inference via Nonparametric Distribution Matching","abstract":"When the likelihood is analytically unavailable and computationally intractable, approximate Bayesian computation (ABC) has emerged as a widely used methodology for approximate posterior inference; however, it suffers from severe computational inefficiency in high-dimensional settings or under diffuse priors. To overcome these limitations, we propose Adaptive Bayesian Inference (ABI), a framework that bypasses traditional data-space discrepancies and instead compares distributions directly in posterior space through nonparametric distribution matching. By leveraging a novel Marginally-augmented Sliced Wasserstein (MSW) distance on posterior measures and exploiting its quantile representation, ABI transforms the challenging problem of measuring divergence between posterior distributions into a tractable sequence of one-dimensional conditional quantile regression tasks. Moreover, we introduce a new adaptive rejection sampling scheme that iteratively refines the posterior approximation by updating the proposal distribution via generative density estimation. Theoretically, we establish parametric convergence rates for the trimmed MSW distance and prove that the ABI posterior converges to the true posterior as the tolerance threshold vanishes. Through extensive empirical evaluation, we demonstrate that ABI significantly outperforms data-based Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free simulators, especially in high-dimensional or dependent observation regimes.","authors":["Wenhui Sophia Lu","Wing Hung Wong"],"url":"https://arxiv.org/abs/2505.04603"}
{"created":"2025-05-08","title":"From Two Sample Testing to Singular Gaussian Discrimination","abstract":"We establish that testing for the equality of two probability measures on a general separable and compact metric space is equivalent to testing for the singularity between two corresponding Gaussian measures on a suitable Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via the notion of kernel mean and covariance embedding of a probability measure. Discerning two singular Gaussians is fundamentally simpler from an information-theoretic perspective than non-parametric two-sample testing, particularly in high-dimensional settings. Our proof leverages the Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert spaces, and shows that discrepancies between distributions are heavily magnified through their corresponding Gaussian embeddings: at a population level, distinct probability measures lead to essentially separated Gaussian embeddings. This appears to be a new instance of the blessing of dimensionality that can be harnessed for the design of efficient inference tools in great generality.","authors":["Leonardo V. Santoro","Kartik G. Waghmare","Victor M. Panaretos"],"url":"https://arxiv.org/abs/2505.04613"}
{"created":"2025-05-08","title":"EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via Reinforcement Learning","abstract":"Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.","authors":["Zhenghao Xing","Xiaowei Hu","Chi-Wing Fu","Wenhai Wang","Jifeng Dai","Pheng-Ann Heng"],"url":"https://arxiv.org/abs/2505.04623"}
{"created":"2025-05-08","title":"Is the end of Insight in Sight ?","abstract":"It is shown that the weight matrices of a Physics-informed neural network (PINN)-based deep learning application to a rarefied gas dynamics problem described by the Boltzmann equation bear no evident link to the mathematical structure of the physical problem. Instead, the weights appear close to Gaussian distributed random matrices. Although significantly more work is needed to support a robust assessment in this direction, these results suggest that deep-learning and the numerical solution of the Boltzmann equation represent two equivalent, but largely distinct paths to the same physical knowledge. If so, Explainable AI might be an unrealistic target and possibly even an ill-posed one.","authors":["Jean-Michel Tucny","Mihir Durve","Sauro Succi"],"url":"https://arxiv.org/abs/2505.04627"}
{"created":"2025-05-08","title":"Machine Learning Cryptanalysis of a Quantum Random Number Generator","abstract":"Random number generators (RNGs) that are crucial for cryptographic applications have been the subject of adversarial attacks. These attacks exploit environmental information to predict generated random numbers that are supposed to be truly random and unpredictable. Though quantum random number generators (QRNGs) are based on the intrinsic indeterministic nature of quantum properties, the presence of classical noise in the measurement process compromises the integrity of a QRNG. In this paper, we develop a predictive machine learning (ML) analysis to investigate the impact of deterministic classical noise in different stages of an optical continuous variable QRNG. Our ML model successfully detects inherent correlations when the deterministic noise sources are prominent. After appropriate filtering and randomness extraction processes are introduced, our QRNG system, in turn, demonstrates its robustness against ML. We further demonstrate the robustness of our ML approach by applying it to uniformly distributed random numbers from the QRNG and a congruential RNG. Hence, our result shows that ML has potentials in benchmarking the quality of RNG devices.","authors":["Nhan Duy Truong","Jing Yan Haw","Syed Muhamad Assad","Ping Koy Lam","Omid Kavehei"],"url":"https://arxiv.org/abs/1905.02342"}
{"created":"2025-05-08","title":"KRW Composition Theorems via Lifting","abstract":"One of the major open problems in complexity theory is proving super-logarithmic lower bounds on the depth of circuits (i.e., $\\mathbf{P}\\not\\subseteq\\mathbf{NC}^1$). Karchmer, Raz, and Wigderson (Computational Complexity 5(3/4), 1995) suggested to approach this problem by proving that depth complexity behaves \"as expected\" with respect to the composition of functions $f\\diamond g$. They showed that the validity of this conjecture would imply that $\\mathbf{P}\\not\\subseteq\\mathbf{NC}^1$.","authors":["Susanna F. de Rezende","Or Meir","Jakob Nordstr\\\"om","Toniann Pitassi","Robert Robere"],"url":"https://arxiv.org/abs/2007.02740"}
{"created":"2025-05-08","title":"Learning to Play Two-Player Perfect-Information Games without Knowledge","abstract":"In this paper, several techniques for learning game state evaluation functions by reinforcement are proposed. The first is a generalization of tree bootstrapping (tree learning): it is adapted to the context of reinforcement learning without knowledge based on non-linear functions. With this technique, no information is lost during the reinforcement learning process. The second is a modification of minimax with unbounded depth extending the best sequences of actions to the terminal states. This modified search is intended to be used during the learning process. The third is to replace the classic gain of a game (+1 / -1) with a reinforcement heuristic. We study particular reinforcement heuristics such as: quick wins and slow defeats ; scoring ; mobility or presence. The four is a new action selection distribution. The conducted experiments suggest that these techniques improve the level of play. Finally, we apply these different techniques to design program-players to the game of Hex (size 11 and 13) surpassing the level of Mohex 3HNN with reinforcement learning from self-play without knowledge.","authors":["Quentin Cohen-Solal"],"url":"https://arxiv.org/abs/2008.01188"}
{"created":"2025-05-08","title":"Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence","abstract":"With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept direction and their impact on CAV applications, including concept sensitivity testing and model correction for shortcut behavior caused by data artifacts. We demonstrate the benefits of pattern-based CAVs using the Pediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet, EfficientNet, and Vision Transformer as model architectures.","authors":["Frederik Pahde","Maximilian Dreyer","Leander Weber","Moritz Weckbecker","Christopher J. Anders","Thomas Wiegand","Wojciech Samek","Sebastian Lapuschkin"],"url":"https://arxiv.org/abs/2202.03482"}
{"created":"2025-05-08","title":"Deep Reinforcement Learning for Traffic Light Control in Intelligent Transportation Systems","abstract":"Smart traffic lights in intelligent transportation systems (ITSs) are envisioned to greatly increase traffic efficiency and reduce congestion. Deep reinforcement learning (DRL) is a promising approach to adaptively control traffic lights based on the real-time traffic situation in a road network. However, conventional methods may suffer from poor scalability. In this paper, we investigate deep reinforcement learning to control traffic lights, and both theoretical analysis and numerical experiments show that the intelligent behavior ``greenwave\" (i.e., a vehicle will see a progressive cascade of green lights, and not have to brake at any intersection) emerges naturally a grid road network, which is proved to be the optimal policy in an avenue with multiple cross streets. As a first step, we use two DRL algorithms for the traffic light control problems in two scenarios. In a single road intersection, we verify that the deep Q-network (DQN) algorithm delivers a thresholding policy; and in a grid road network, we adopt the deep deterministic policy gradient (DDPG) algorithm. Secondly, numerical experiments show that the DQN algorithm delivers the optimal control, and the DDPG algorithm with passive observations has the capability to produce on its own a high-level intelligent behavior in a grid road network, namely, the ``greenwave\" policy emerges. We also verify the ``greenwave\" patterns in a $5 \\times 10$ grid road network. Thirdly, the ``greenwave\" patterns demonstrate that DRL algorithms produce favorable solutions since the ``greenwave\" policy shown in experiment results is proved to be optimal in a specified traffic model (an avenue with multiple cross streets). The delivered policies both in a single road intersection and a grid road network demonstrate the scalability of DRL algorithms.","authors":["Ming Zhu","Xiao-Yang Liu","Sem Borst","Anwar Walid"],"url":"https://arxiv.org/abs/2302.03669"}
{"created":"2025-05-08","title":"A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods","abstract":"The beekeeping sector has experienced significant production fluctuations in recent years, largely due to increasingly frequent adverse weather events linked to climate change. These events can severely affect the environment, reducing its suitability for bee activity. We conduct a forecasting analysis of honey production across Italy using a range of machine learning models, with a particular focus on weather-related variables as key predictors. Our analysis relies on a dataset collected in 2022, which combines hive-level observations with detailed weather data. We train and compare several linear and nonlinear models, evaluating both their predictive accuracy and interpretability. By examining model explanations, we identify the main drivers of honey production. We also ensemble models from different families to assess whether combining predictions improves forecast accuracy. These insights support beekeepers in managing production risks and may inform the development of insurance products against unexpected losses due to poor harvests.","authors":["Alessio Brini","Elisa Giovannini","Elia Smaniotto"],"url":"https://arxiv.org/abs/2304.01215"}
{"created":"2025-05-08","title":"Disjunctive Branch-And-Bound for Certifiably Optimal Low-Rank Matrix Completion","abstract":"Low-rank matrix completion consists of computing a matrix of minimal complexity that recovers a given set of observations as accurately as possible. Unfortunately, existing methods for matrix completion are heuristics that, while highly scalable and often identifying high-quality solutions, do not possess any optimality guarantees. We reexamine matrix completion with an optimality-oriented eye. We reformulate low-rank matrix completion problems as convex problems over the non-convex set of projection matrices and implement a disjunctive branch-and-bound scheme that solves them to certifiable optimality. Further, we derive a novel and often near-exact class of convex relaxations by decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing that two-by-two minors in each rank-one matrix have determinant zero. In numerical experiments, our new convex relaxations decrease the optimality gap by two orders of magnitude compared to existing attempts, and our disjunctive branch-and-bound scheme solves $n \\times m$ rank-$r$ matrix completion problems to certifiable optimality or near optimality in hours for $\\max \\{m, n\\} \\leq 2500$ and $r \\leq 5$. Moreover, this improvement in the training error translates into an average $2\\%$--$50\\%$ improvement in the test set error.","authors":["Dimitris Bertsimas","Ryan Cory-Wright","Sean Lo","Jean Pauphilet"],"url":"https://arxiv.org/abs/2305.12292"}
{"created":"2025-05-08","title":"Playing repeated games with Large Language Models","abstract":"LLMs are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour. We let different LLMs play finitely repeated $2\\times2$ games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination, like the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We additionally show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a \"social chain-of-thought\" (SCoT) strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.","authors":["Elif Akata","Lion Schulz","Julian Coda-Forno","Seong Joon Oh","Matthias Bethge","Eric Schulz"],"url":"https://arxiv.org/abs/2305.16867"}
{"created":"2025-05-08","title":"XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models","abstract":"The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/XrayGPT.","authors":["Omkar Thawakar","Abdelrahman Shaker","Sahal Shaji Mullappilly","Hisham Cholakkal","Rao Muhammad Anwer","Salman Khan","Jorma Laaksonen","Fahad Shahbaz Khan"],"url":"https://arxiv.org/abs/2306.07971"}
{"created":"2025-05-08","title":"A Fully Abstract Model of PCF Based on Extended Addressing Machines","abstract":"Extended addressing machines (EAMs) have been introduced to represent higher-order sequential computations. Previously, we have shown that they are capable of simulating -- via an easy encoding -- the operational semantics of PCF, extended with explicit substitutions. In this paper we prove that the simulation is actually an equivalence: a PCF program terminates in a numeral exactly when the corresponding EAM terminates in the same numeral. It follows that the model of PCF obtained by quotienting typable EAMs by a suitable logical relation is adequate. From a definability result stating that every EAM in the model can be transformed into a PCF program with the same observational behavior, we conclude that the model is fully abstract for PCF.","authors":["Benedetto Intrigila","Giulio Manzonetto","Nicolas Munnich"],"url":"https://arxiv.org/abs/2306.13756"}
{"created":"2025-05-08","title":"3D-IDS: Doubly Disentangled Dynamic Intrusion Detection","abstract":"Network-based intrusion detection system (NIDS) monitors network traffic for malicious activities, forming the frontline defense against increasing attacks over information infrastructures. Although promising, our quantitative analysis shows that existing methods perform inconsistently in declaring various unknown attacks (e.g., 9% and 35% F1 respectively for two distinct unknown threats for an SVM-based method) or detecting diverse known attacks (e.g., 31% F1 for the Backdoor and 93% F1 for DDoS by a GCN-based state-of-the-art method), and reveals that the underlying cause is entangled distributions of flow features. This motivates us to propose 3D-IDS, a novel method that aims to tackle the above issues through two-step feature disentanglements and a dynamic graph diffusion scheme. Specifically, we first disentangle traffic features by a non-parameterized optimization based on mutual information, automatically differentiating tens and hundreds of complex features of various attacks. Such differentiated features will be fed into a memory model to generate representations, which are further disentangled to highlight the attack-specific features. Finally, we use a novel graph diffusion method that dynamically fuses the network topology for spatial-temporal aggregation in evolving data streams. By doing so, we can effectively identify various attacks in encrypted traffics, including unknown threats and known ones that are not easily detected. Experiments show the superiority of our 3D-IDS. We also demonstrate that our two-step feature disentanglements benefit the explainability of NIDS.","authors":["Chenyang Qiu","Yingsheng Geng","Junrui Lu","Kaida Chen","Shitong Zhu","Ya Su","Guoshun Nan","Can Zhang","Junsong Fu","Qimei Cui","Xiaofeng Tao"],"url":"https://arxiv.org/abs/2307.11079"}
{"created":"2025-05-08","title":"JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models","abstract":"Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at https://jenmusic.ai/audio-demos","authors":["Peike Li","Boyu Chen","Yao Yao","Yikai Wang","Allen Wang","Alex Wang"],"url":"https://arxiv.org/abs/2308.04729"}
{"created":"2025-05-08","title":"Illumination and Shadows in Head Rotation: experiments with Denoising Diffusion Models","abstract":"Accurately modeling the effects of illumination and shadows during head rotation is critical in computer vision for enhancing image realism and reducing artifacts. This study delves into the latent space of denoising diffusion models to identify compelling trajectories that can express continuous head rotation under varying lighting conditions. A key contribution of our work is the generation of additional labels from the CelebA dataset,categorizing images into three groups based on prevalent illumination direction: left, center, and right. These labels play a crucial role in our approach, enabling more precise manipulations and improved handling of lighting variations. Leveraging a recent embedding technique for Denoising Diffusion Implicit Models (DDIM), our method achieves noteworthy manipulations, encompassing a wide rotation angle of $\\pm 30$ degrees, while preserving individual distinct characteristics even under challenging illumination conditions. Our methodology involves computing trajectories that approximate clouds of latent representations of dataset samples with different yaw rotations through linear regression. Specific trajectories are obtained by analyzing subsets of data that share significant attributes with the source image, including light direction. Notably, our approach does not require any specific training of the generative model for the task of rotation; we merely compute and follow specific trajectories in the latent space of a pre-trained face generation model. This article showcases the potential of our approach and its current limitations through a qualitative discussion of notable examples. This study contributes to the ongoing advancements in representation learning and the semantic investigation of the latent space of generative models.","authors":["Andrea Asperti","Gabriele Colasuonno","Antonio Guerra"],"url":"https://arxiv.org/abs/2308.06057"}
{"created":"2025-05-08","title":"Federated Semi-Supervised and Semi-Asynchronous Learning for Anomaly Detection in IoT Networks","abstract":"Existing FL-based approaches are based on the unrealistic assumption that the data on the client-side is fully annotated with ground truths. Furthermore, it is a great challenge how to improve the training efficiency while ensuring the detection accuracy in the highly heterogeneous and resource-constrained IoT networks. Meanwhile, the communication cost between clients and the server is also a problem that can not be ignored. Therefore, in this paper, we propose a Federated Semi-Supervised and Semi-Asynchronous (FedS3A) learning for anomaly detection in IoT networks. First, we consider a more realistic assumption that labeled data is only available at the server, and pseudo-labeling is utilized to implement federated semi-supervised learning, in which a dynamic weight of supervised learning is exploited to balance the supervised learning at the server and unsupervised learning at clients. Then, we propose a semi-asynchronous model update and staleness tolerant distribution scheme to achieve a trade-off between the round efficiency and detection accuracy. Meanwhile, the staleness of local models and the participation frequency of clients are considered to adjust their contributions to the global model. In addition, a group-based aggregation function is proposed to deal with the non-IID distribution of the data. Finally, the difference transmission based on the sparse matrix is adopted to reduce the communication cost. Extensive experimental results show that FedS3A can achieve greater than 98% accuracy even when the data is non-IID and is superior to the classic FL-based algorithms in terms of both detection performance and round efficiency, achieving a win-win situation. Meanwhile, FedS3A successfully reduces the communication cost by higher than 50%.","authors":["Liang Liu","Wenbin Zhai","Feng Wang","Youwei Ding","Wanying Lu","Weizhi Meng"],"url":"https://arxiv.org/abs/2308.11981"}
{"created":"2025-05-08","title":"Contaminated Multivariate Time-Series Anomaly Detection with Spatio-Temporal Graph Conditional Diffusion Models","abstract":"Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data is contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three core modules: a Decontaminator to rectify anomalies (aka noise) present during training, a Long-range Variable Dependency Modeling module to capture long-term intra- and inter-variable dependencies within the decontaminated data that is considered as a surrogate of the pure normal data, and an Anomaly Scoring module to detect anomalies from all types. Our extensive experiments conducted on four reliable and diverse datasets conclusively demonstrate that TSAD-C surpasses existing methodologies, thus establishing a new state-of-the-art in the TSAD field.","authors":["Thi Kieu Khanh Ho","Narges Armanfard"],"url":"https://arxiv.org/abs/2308.12563"}
{"created":"2025-05-08","title":"Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models","abstract":"Recently, large language models (LLMs), such as GPT-4, stand out remarkable conversational abilities, enabling them to engage in dynamic and contextually relevant dialogues across a wide range of topics. However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses. To address this, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the chatbot can easily generate a highly consistent response with the help of the latest memory. We evaluate our method on both open and closed LLMs, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Also, we show that our strategy could nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced LLMs, bringing further long-term dialogue performance. Notably, our method is a potential solution to enable the LLM to model the extremely long context. The code and scripts will be released later.","authors":["Qingyue Wang","Yanhe Fu","Yanan Cao","Shuai Wang","Zhiliang Tian","Liang Ding"],"url":"https://arxiv.org/abs/2308.15022"}
{"created":"2025-05-08","title":"Diverse Audio Embeddings-- Bringing Features Back Outperforms CLAP !","abstract":"With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training end-to-end models.","authors":["Prateek Verma"],"url":"https://arxiv.org/abs/2309.08751"}
{"created":"2025-05-08","title":"Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder","abstract":"The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields an approximate closure of the dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the accuracy and stability of the Koopman operator approximation. Demonstrations showcase the technique's improved predictive capability for flow around a cylinder. It also provides a low dimensional approximation for Kuramoto-Sivashinsky (KS) with promising short-term predictability and robust long-term statistical performance. By bridging the gap between data-driven techniques and the mathematical foundations of Koopman theory, MZ-AE offers a promising avenue for improved understanding and prediction of complex nonlinear dynamics.","authors":["Priyam Gupta","Peter J. Schmid","Denis Sipp","Taraneh Sayadi","Georgios Rigas"],"url":"https://arxiv.org/abs/2310.10745"}
{"created":"2025-05-08","title":"Hard-Negative Sampling for Contrastive Learning: Optimal Representation Geometry and Neural- vs Dimensional-Collapse","abstract":"For a widely-studied data model and general loss and sample-hardening functions we prove that the losses of Supervised Contrastive Learning (SCL), Hard-SCL (HSCL), and Unsupervised Contrastive Learning (UCL) are minimized by representations that exhibit Neural-Collapse (NC), i.e., the class means form an Equiangular Tight Frame (ETF) and data from the same class are mapped to the same representation. We also prove that for any representation mapping, the HSCL and Hard-UCL (HUCL) losses are lower bounded by the corresponding SCL and UCL losses. In contrast to existing literature, our theoretical results for SCL do not require class-conditional independence of augmented views and work for a general loss function class that includes the widely used InfoNCE loss function. Moreover, our proofs are simpler, compact, and transparent. Similar to existing literature, our theoretical claims also hold for the practical scenario where batching is used for optimization. We empirically demonstrate, for the first time, that Adam optimization (with batching) of HSCL and HUCL losses with random initialization and suitable hardness levels can indeed converge to the NC-geometry if we incorporate unit-ball or unit-sphere feature normalization. Without incorporating hard-negatives or feature normalization, however, the representations learned via Adam suffer from Dimensional-Collapse (DC) and fail to attain the NC-geometry. These results exemplify the role of hard-negative sampling in contrastive representation learning and we conclude with several open theoretical problems for future work. The code can be found at https://github.com/rjiang03/HCL/tree/main","authors":["Ruijie Jiang","Thuan Nguyen","Shuchin Aeron","Prakash Ishwar"],"url":"https://arxiv.org/abs/2311.05139"}
{"created":"2025-05-08","title":"Universal Optimality of Dijkstra via Beyond-Worst-Case Heaps","abstract":"In this paper we prove that Dijkstra's shortest-path algorithm, if implemented with a sufficiently efficient heap, is universally optimal in its running time, and with suitable small additions is also universally optimal in its number of comparisons.","authors":["Bernhard Haeupler","Richard Hlad\\'ik","V\\'aclav Rozho\\v{n}","Robert E. Tarjan","Jakub T\\v{e}tek"],"url":"https://arxiv.org/abs/2311.11793"}
{"created":"2025-05-08","title":"Anti-Gauss cubature rules with applications to Fredholm integral equations on the square","abstract":"The purpose of this paper is to develop the anti-Gauss cubature rule for approximating integrals defined on the square whose integrand function may have algebraic singularities at the boundaries. An application of such a rule to the numerical solution of Fredholm integral equations of the second-kind is also explored. The stability, convergence, and conditioning of the proposed Nystr\\\"om-type method are studied. The numerical solution of the resulting dense linear system is also investigated and several numerical tests are presented.","authors":["Patricia Diaz de Alba","Luisa Fermo","Giuseppe Rodriguez"],"url":"https://arxiv.org/abs/2311.15967"}
{"created":"2025-05-08","title":"RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance","abstract":"Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog.","authors":["Chantal Pellegrini","Ege \\\"Ozsoy","Benjamin Busam","Nassir Navab","Matthias Keicher"],"url":"https://arxiv.org/abs/2311.18681"}
{"created":"2025-05-08","title":"Introducing Modelling, Analysis and Control of Three-Phase Electrical Systems Using Geometric Algebra","abstract":"State-of-the-art techniques for modeling, analysis and control of three-phase electrical systems belong to the real-valued multi-input/multi-output (MIMO) domain, or to the complex-valued nonlinear single-input/single-output (SISO) domain. In order to complement both domains while simplifying complexity and offering new analysis and design perspectives, this paper introduces the application of geometric algebra (GA) principles to the modeling, analysis and control of three-phase electrical systems. The key contribution for the modeling part is the identification of the transformation that allows transferring real-valued linear MIMO systems into GA-valued linear SISO representations (with independence of having a balanced or unbalanced system). Closed-loop stability analysis in the new space is addressed by using intrinsic properties of GA. In addition, a recipe for designing stabilizing and decoupling GA-valued controllers is provided. Numerical examples illustrate key developments and experiments corroborate the main findings.","authors":["Manel Velasco","Isiah Zaplana","Arnau D\\`oria-Cerezo","Josu\\'e Duarte","Pau Mart\\'i"],"url":"https://arxiv.org/abs/2312.01345"}
{"created":"2025-05-08","title":"Internal and External Calculi: Ordering the Jungle without Being Lost in Translations","abstract":"This paper gives a broad account of the various sequent-based proof formalisms in the proof-theoretic literature. We consider formalisms for various modal and tense logics, intuitionistic logic, conditional logics, and bunched logics. After providing an overview of the logics and proof formalisms under consideration, we show how these sequent-based formalisms can be placed in a hierarchy in terms of the underlying data structure of the sequents. We then discuss how this hierarchy can be traversed using translations. Translating proofs up this hierarchy is found to be relatively straightforward while translating proofs down the hierarchy is substantially more difficult. Finally, we inspect the prevalent distinction in structural proof theory between 'internal calculi' and 'external calculi.' We discuss the ambiguities involved in the informal definitions of these categories, and we critically assess the properties that (calculi from) these classes are purported to possess.","authors":["Tim S. Lyon","Agata Ciabattoni","Didier Galmiche","Marianna Girlando","Dominique Larchey-Wendling","Daniel M\\'ery","Nicola Olivetti","Revantha Ramanayake"],"url":"https://arxiv.org/abs/2312.03426"}
{"created":"2025-05-08","title":"Towards Time Sensitive Networking on Smart Cities: Techniques, Challenges, and Solutions","abstract":"Smart cities transform urban landscapes with interconnected nodes and sensors. The search for seamless communication in time-critical scenarios has become evident during this evolution. With the escalating complexity of urban environments, envisioning a future with a blend of autonomous and conventional systems, each demanding distinct quality-of-service considerations, services in smart cities vary in criticality levels and necessitate differentiated traffic handling, prioritizing critical flows without compromising the network's reliability or failing on hard real-time requirements. To tackle these challenges, in this article, we discuss a time-sensitive networking approach, which presents multi-faceted challenges, notably interoperability among diverse technologies and standards at the scale of a smart city network. TSN emerges as a promising toolkit, encompassing synchronization, latency management, redundancy, and configuration functionalities crucial for addressing smart city challenges. Moreover, the article scrutinizes how TSN, predominantly utilized in domains like automotive and industry, can be tailored to suit the intricate needs of smart cities, emphasizing the necessity for adaptability and scalability in network design. This survey consolidates current research on TSN, outlining its potential in fortifying critical machine-to-machine communications within smart cities while highlighting future challenges, potential solutions, and a roadmap for integrating TSN effectively into the fabric of urban connectivity.","authors":["Rui Lopes","Duarte Raposo","Susana Sargento"],"url":"https://arxiv.org/abs/2312.03635"}
{"created":"2025-05-08","title":"Robust Construction of Polycube Segmentations via Dual Loops","abstract":"Polycube segmentations for 3D models effectively support a wide variety of applications such as seamless texture mapping, spline fitting, structured multi-block grid generation, and hexahedral mesh construction. However, the automated construction of valid polycube segmentations suffers from robustness issues: state-of-the-art methods are not guaranteed to find a valid solution. In this paper we present an iterative algorithm which is guaranteed to return a valid polycube segmentation for 3D models of any genus. Our algorithm is based on a dual representation of polycubes. Starting from an initial simple polycube of the correct genus, together with the corresponding dual loop structure and polycube segmentation, we iteratively refine the polycube, loop structure, and segmentation, while maintaining the correctness of the solution. Our algorithm is robust by construction: at any point during the iterative process the current segmentation is valid. Furthermore, the iterative nature of our algorithm facilitates a seamless trade-off between quality and complexity of the solution. Our algorithm can be implemented using comparatively simple algorithmic building blocks; our experimental evaluation establishes that the quality of our polycube segmentations is on par with, or exceeding, the state-of-the-art.","authors":["Maxim Snoep","Bettina Speckmann","Kevin Verbeek"],"url":"https://arxiv.org/abs/2402.00652"}
{"created":"2025-05-08","title":"TransAxx: Efficient Transformers with Approximate Computing","abstract":"Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs). However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy. Furthermore, we propose a methodology to generate approximate accelerators for ViT models. Our approach uses a Monte Carlo Tree Search (MCTS) algorithm to efficiently search the space of possible configurations using a hardware-driven hand-crafted policy. Our evaluation demonstrates the efficacy of our methodology in achieving significant trade-offs between accuracy and power, resulting in substantial gains without compromising on performance.","authors":["Dimitrios Danopoulos","Georgios Zervakis","Dimitrios Soudris","J\\\"org Henkel"],"url":"https://arxiv.org/abs/2402.07545"}
{"created":"2025-05-08","title":"Constrained Boundary Labeling","abstract":"Boundary labeling is a technique in computational geometry used to label sets of features in an illustration. It involves placing labels along an axis-parallel bounding box and connecting each label with its corresponding feature using non-crossing leader lines. Although boundary labeling is well-studied, semantic constraints on the labels have not been investigated thoroughly. In this paper, we introduce grouping and ordering constraints in boundary labeling: Grouping constraints enforce that all labels in a group are placed consecutively on the boundary, and ordering constraints enforce a partial order over the labels. We show that it is NP-hard to find a labeling for arbitrarily sized labels with unrestricted positions along one side of the boundary. However, we obtain polynomial-time algorithms if we restrict this problem either to uniform-height labels or to a finite set of candidate positions. Furthermore, we show that finding a labeling on two opposite sides of the boundary is NP-complete, even for uniform-height labels and finite label positions. Finally, we experimentally confirm that our approach has also practical relevance.","authors":["Thomas Depian","Martin N\\\"ollenburg","Soeren Terziadis","Markus Wallinger"],"url":"https://arxiv.org/abs/2402.12245"}
{"created":"2025-05-08","title":"Opening Articulated Structures in the Real World","abstract":"What does it take to build mobile manipulation systems that can competently operate on previously unseen objects in previously unseen environments? This work answers this question using opening of articulated structures as a mobile manipulation testbed. Specifically, our focus is on the end-to-end performance on this task without any privileged information, i.e. the robot starts at a location with the novel target articulated object in view, and has to approach the object and successfully open it. We first develop a system for this task, and then conduct 100+ end-to-end system tests across 13 real world test sites. Our large-scale study reveals a number of surprising findings: a) modular systems outperform end-to-end learned systems for this task, even when the end-to-end learned systems are trained on 1000+ demonstrations, b) perception, and not precise end-effector control, is the primary bottleneck to task success, and c) state-of-the-art articulation parameter estimation models developed in isolation struggle when faced with robot-centric viewpoints. Overall, our findings highlight the limitations of developing components of the pipeline in isolation and underscore the need for system-level research, providing a pragmatic roadmap for building generalizable mobile manipulation systems. Videos, code, and models are available on the project website: https://arjung128.github.io/opening-articulated-structures/","authors":["Arjun Gupta","Michelle Zhang","Rishik Sathua","Saurabh Gupta"],"url":"https://arxiv.org/abs/2402.17767"}
{"created":"2025-05-08","title":"Residue Domination in Bounded-Treewidth Graphs","abstract":"For the vertex selection problem $(\\sigma,\\rho)$-DomSet one is given two fixed sets $\\sigma$ and $\\rho$ of integers and the task is to decide whether we can select vertices of the input graph such that, for every selected vertex, the number of selected neighbors is in $\\sigma$ and, for every unselected vertex, the number of selected neighbors is in $\\rho$ [Telle, Nord. J. Comp. 1994]. This framework covers many fundamental graph problems such as Independent Set and Dominating Set.","authors":["Jakob Greilhuber","Philipp Schepper","Philip Wellnitz"],"url":"https://arxiv.org/abs/2403.07524"}
{"created":"2025-05-08","title":"Context-aware LLM-based Safe Control Against Latent Risks","abstract":"Autonomous control systems face significant challenges in performing complex tasks in the presence of latent risks. To address this, we propose an integrated framework that combines Large Language Models (LLMs), numerical optimization, and optimization-based control to facilitate efficient subtask learning while ensuring safety against latent risks. The framework decomposes complex tasks into a sequence of context-aware subtasks that account for latent risks. These subtasks and their parameters are then refined through a multi-time-scale process: high-layer multi-turn in-context learning, mid-layer LLM Chain-of-Thought reasoning and numerical optimization, and low-layer model predictive control. The framework iteratively improves decisions by leveraging qualitative feedback and optimized trajectory data from lower-layer optimization processes and a physics simulator. We validate the proposed framework through simulated case studies involving robot arm and autonomous vehicle scenarios. The experiments demonstrate that the proposed framework can mediate actions based on the context and latent risks and learn complex behaviors efficiently.","authors":["Xiyu Deng","Quan Khanh Luu","Anh Van Ho","Yorie Nakahira"],"url":"https://arxiv.org/abs/2403.11863"}
{"created":"2025-05-08","title":"Human-Robot Interaction and Perceived Irrationality: A Study of Trust Dynamics and Error Acknowledgment","abstract":"As robots become increasingly integrated into various industries, understanding how humans respond to robotic failures is critical. This study systematically examines trust dynamics and system design by analyzing human reactions to robot failures. We conducted a four-stage survey to explore how trust evolves throughout human-robot interactions. The first stage collected demographic data and initial trust levels. The second stage focused on preliminary expectations and perceptions of robotic capabilities. The third stage examined interaction details, including robot precision and error acknowledgment. Finally, the fourth stage assessed post-interaction perceptions, evaluating trust dynamics, forgiveness, and willingness to recommend robotic technologies. Results indicate that trust in robotic systems significantly increased when robots acknowledged their errors or limitations. Additionally, participants showed greater willingness to suggest robots for future tasks, highlighting the importance of direct engagement in shaping trust dynamics. These findings provide valuable insights for designing more transparent, responsive, and trustworthy robotic systems. By enhancing our understanding of human-robot interaction (HRI), this study contributes to the development of robotic technologies that foster greater public acceptance and adoption.","authors":["Ponkoj Chandra Shill","Md. Azizul Hakim"],"url":"https://arxiv.org/abs/2403.14293"}
{"created":"2025-05-08","title":"Large Language Models Are Struggle to Cope with Unreasonability in Math Problems","abstract":"Recent research have demonstrated LLMs' impressive performance in math and reasoning. However, the capacity of LLMs to address math problems under unconventional conditions, such as internal inconsistencies and flawed assumptions, remains largely unexplored. In this paper, we propose a novel benchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to recognize and respond to unreasonability in math problem. The benchmark consists of a carefully curated collection of unreasonable math questions across diverse types. Based on extensive experiments covering 19 LLMs, we observe that even state-of-the-art models such as GPT-4o achieve only limited performance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone to overthinking and unstable. We further explore strategies for improving the recognition of unreasonable inputs, shedding light on both the possibility and limitations of LLMs in this challenging setting.","authors":["Jingyuan Ma","Damai Dai","Zihang Yuan","Rui li","Weilin Luo","Bin Wang","Qun Liu","Lei Sha","Zhifang Sui"],"url":"https://arxiv.org/abs/2403.19346"}
{"created":"2025-05-08","title":"Derandomization with Pseudorandomness","abstract":"Derandomization techniques are often used within advanced randomized algorithms. In particular, pseudorandom objects, such as hash families and expander graphs, are key components of such algorithms, but their verification presents a challenge. This work shows how such algorithms can be expressed and verified in Isabelle and presents a pseudorandom objects library that abstracts away the deep algebraic/analytic results involved. Moreover, it presents examples that show how the library eases and enables the verification of advanced randomized algorithms. Highlighting the value of this framework is that it was recently used to verify the space-optimal distinct elements algorithm by Blasiok from 2018, which relies on the combination of many derandomization techniques to achieve its optimality.","authors":["Emin Karayel"],"url":"https://arxiv.org/abs/2404.16614"}
{"created":"2025-05-08","title":"Enhancing User Interest based on Stream Clustering and Memory Networks in Large-Scale Recommender Systems","abstract":"Recommender Systems (RSs) provide personalized recommendation service based on user interest, which are widely used in various platforms. However, there are lots of users with sparse interest due to lacking consumption behaviors, which leads to poor recommendation results for them. This problem is widespread in large-scale RSs and is particularly difficult to address. To solve this challenging problem, we propose an innovative solution called User Interest Enhancement (UIE). UIE enhances user interest including user profile and user history behavior sequences by leveraging the enhancement vectors and personalized enhancement vectors generated based on dynamic streaming clustering of similar users and items from multiple perspectives, which are stored and updated in memory networks. UIE not only remarkably improves model performance for users with sparse interest, but also delivers notable gains for other users. As an end-to-end solution, UIE is easy to implement on top of existing ranking models. Furthermore, we extend our approach to long-tail items using similar methods, which also yields excellent improvements. We conduct extensive offline and online experiments in a large-scale industrial RS. The results demonstrate that our model substantially outperforms other existing approaches, especially for users with sparse interest. UIE has been deployed in several large-scale RSs at Tencent since 2022, which was made public on 21 May 2024. In addition, UIE-based methods have also been successfully applied in candidate generation, pre-ranking, and context-DNN stages. Multiple teams have developed solutions based on UIE, focusing primarily on updating clustering algorithms and attention mechanisms. As far as we know, UIE has been deployed by many companies. The thoughts of UIE, dynamic streaming clustering and similarity enhancement, have inspired subsequent relevant works.","authors":["Peng Liu","Nian Wang","Cong Xu","Ming Zhao","Bin Wang","Yi Ren"],"url":"https://arxiv.org/abs/2405.13238"}
{"created":"2025-05-08","title":"Spectral-Refiner: Accurate Fine-Tuning of Spatiotemporal Fourier Neural Operator for Turbulent Flows","abstract":"Recent advancements in operator-type neural networks have shown promising results in approximating the solutions of spatiotemporal Partial Differential Equations (PDEs). However, these neural networks often entail considerable training expenses, and may not always achieve the desired accuracy required in many scientific and engineering disciplines. In this paper, we propose a new learning framework to address these issues. A new spatiotemporal adaptation is proposed to generalize any Fourier Neural Operator (FNO) variant to learn maps between Bochner spaces, which can perform an arbitrary-length temporal super-resolution for the first time. To better exploit this capacity, a new paradigm is proposed to refine the commonly adopted end-to-end neural operator training and evaluations with the help from the wisdom from traditional numerical PDE theory and techniques. Specifically, in the learning problems for the turbulent flow modeled by the Navier-Stokes Equations (NSE), the proposed paradigm trains an FNO only for a few epochs. Then, only the newly proposed spatiotemporal spectral convolution layer is fine-tuned without the frequency truncation. The spectral fine-tuning loss function uses a negative Sobolev norm for the first time in operator learning, defined through a reliable functional-type a posteriori error estimator whose evaluation is exact thanks to the Parseval identity. Moreover, unlike the difficult nonconvex optimization problems in the end-to-end training, this fine-tuning loss is convex. Numerical experiments on commonly used NSE benchmarks demonstrate significant improvements in both computational efficiency and accuracy, compared to end-to-end evaluation and traditional numerical PDE solvers under certain conditions. The source code is publicly available at https://github.com/scaomath/torch-cfd.","authors":["Shuhao Cao","Francesco Brarda","Ruipeng Li","Yuanzhe Xi"],"url":"https://arxiv.org/abs/2405.17211"}
{"created":"2025-05-08","title":"Re-ReST: Reflection-Reinforced Self-Training for Language Agents","abstract":"Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a \\textit{reflector} to refine low-quality generated samples during self-training. The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6\\% on HotpotQA and 28.4\\% on AlfWorld, and Re-ReST further boosting performance by 2.0\\% and 14.1\\%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work. Our code is released at https://github.com/PlusLabNLP/Re-ReST.","authors":["Zi-Yi Dou","Cheng-Fu Yang","Xueqing Wu","Kai-Wei Chang","Nanyun Peng"],"url":"https://arxiv.org/abs/2406.01495"}
{"created":"2025-05-08","title":"From Latent to Lucid: Transforming Knowledge Graph Embeddings into Interpretable Structures with KGEPrisma","abstract":"In this paper, we introduce a post-hoc and local explainable AI method tailored for Knowledge Graph Embedding (KGE) models. These models are essential to Knowledge Graph Completion yet criticized for their opaque, black-box nature. Despite their significant success in capturing the semantics of knowledge graphs through high-dimensional latent representations, their inherent complexity poses substantial challenges to explainability. While existing methods like Kelpie use resource-intensive perturbation to explain KGE models, our approach directly decodes the latent representations encoded by KGE models, leveraging the smoothness of the embeddings, which follows the principle that similar embeddings reflect similar behaviours within the Knowledge Graph, meaning that nodes are similarly embedded because their graph neighbourhood looks similar. This principle is commonly referred to as smoothness. By identifying symbolic structures, in the form of triples, within the subgraph neighborhoods of similarly embedded entities, our method identifies the statistical regularities on which the models rely and translates these insights into human-understandable symbolic rules and facts. This bridges the gap between the abstract representations of KGE models and their predictive outputs, offering clear, interpretable insights. Key contributions include a novel post-hoc and local explainable AI method for KGE models that provides immediate, faithful explanations without retraining, facilitating real-time application on large-scale knowledge graphs. The method's flexibility enables the generation of rule-based, instance-based, and analogy-based explanations, meeting diverse user needs. Extensive evaluations show the effectiveness of our approach in delivering faithful and well-localized explanations, enhancing the transparency and trustworthiness of KGE models.","authors":["Christoph Wehner","Chrysa Iliopoulou","Ute Schmid","Tarek R. Besold"],"url":"https://arxiv.org/abs/2406.01759"}
{"created":"2025-05-08","title":"Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA","abstract":"The rapid adoption of Large Language Models (LLMs) has exposed critical security and ethical vulnerabilities, particularly their susceptibility to adversarial manipulations. This paper introduces QROA, a novel black-box jailbreak method designed to identify adversarial suffixes that can bypass LLM alignment safeguards when appended to a malicious instruction. Unlike existing suffix-based jailbreak approaches, QROA does not require access to the model's logit or any other internal information. It also eliminates reliance on human-crafted templates, operating solely through the standard query-response interface of LLMs. By framing the attack as an optimization bandit problem, QROA employs a surrogate model and token level optimization to efficiently explore suffix variations. Furthermore, we propose QROA-UNV, an extension that identifies universal adversarial suffixes for individual models, enabling one-query jailbreaks across a wide range of instructions. Testing on multiple models demonstrates Attack Success Rate (ASR) greater than 80\\%. These findings highlight critical vulnerabilities, emphasize the need for advanced defenses, and contribute to the development of more robust safety evaluations for secure AI deployment. The code is made public on the following link: https://github.com/qroa/QROA","authors":["Hussein Jawad","Yassine Chenik","Nicolas J. -B. Brunel"],"url":"https://arxiv.org/abs/2406.02044"}
{"created":"2025-05-08","title":"VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling","abstract":"In this work, we systematically study music generation conditioned solely on the video. First, we present a large-scale dataset comprising 360K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs. VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. By incorporating local and global visual cues, VidMuse enables the creation of musically coherent audio tracks that consistently match the video content through Long-Short-Term modeling. Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment. The code and datasets are available at https://vidmuse.github.io/.","authors":["Zeyue Tian","Zhaoyang Liu","Ruibin Yuan","Jiahao Pan","Qifeng Liu","Xu Tan","Qifeng Chen","Wei Xue","Yike Guo"],"url":"https://arxiv.org/abs/2406.04321"}
{"created":"2025-05-08","title":"Mixed-Curvature Decision Trees and Random Forests","abstract":"We extend decision tree and random forest algorithms to product space manifolds: Cartesian products of Euclidean, hyperspherical, and hyperbolic manifolds. Such spaces have extremely expressive geometries capable of representing many arrangements of distances with low metric distortion. To date, all classifiers for product spaces fit a single linear decision boundary, and no regressor has been described. Our method enables a simple, expressive method for classification and regression in product manifolds. We demonstrate the superior accuracy of our tool compared to Euclidean methods operating in the ambient space or the tangent plane of the manifold across a range of constant-curvature and product manifolds. Code for our implementation and experiments is available at https://github.com/pchlenski/embedders.","authors":["Philippe Chlenski","Quentin Chu","Itsik Pe'er"],"url":"https://arxiv.org/abs/2406.05227"}
{"created":"2025-05-08","title":"Meta-Learning Loss Functions for Deep Neural Networks","abstract":"Humans can often quickly and efficiently solve complex new learning tasks given only a small set of examples. In contrast, modern artificially intelligent systems often require thousands or millions of observations in order to solve even the most basic tasks. Meta-learning aims to resolve this issue by leveraging past experiences from similar learning tasks to embed the appropriate inductive biases into the learning system. Historically methods for meta-learning components such as optimizers, parameter initializations, and more have led to significant performance increases. This thesis aims to explore the concept of meta-learning to improve performance, through the often-overlooked component of the loss function. The loss function is a vital component of a learning system, as it represents the primary learning objective, where success is determined and quantified by the system's ability to optimize for that objective successfully.","authors":["Christian Raymond"],"url":"https://arxiv.org/abs/2406.09713"}
{"created":"2025-05-08","title":"Uncertainty for SVBRDF Acquisition using Frequency Analysis","abstract":"This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view captures. Under uncontrolled illumination and unstructured viewpoints, there is no guarantee that the observations contain enough information to reconstruct the appearance properties of a captured object. We study this ambiguity, or uncertainty, using entropy and accelerate the analysis by using the frequency domain, rather than the domain of incoming and outgoing viewing angles. The result is a method that computes a map of uncertainty over an entire object within a millisecond. We find that the frequency model allows us to recover SVBRDF parameters with competitive performance, that the accelerated entropy computation matches results with a physically-based path tracer, and that there is a positive correlation between error and uncertainty. We then show that the uncertainty map can be applied to improve SVBRDF acquisition using capture guidance, sharing information on the surface, and using a diffusion model to inpaint uncertain regions. Our code is available at https://github.com/rubenwiersma/svbrdf_uncertainty.","authors":["Ruben Wiersma","Julien Philip","Milo\\v{s} Ha\\v{s}an","Krishna Mullia","Fujun Luan","Elmar Eisemann","Valentin Deschaintre"],"url":"https://arxiv.org/abs/2406.17774"}
{"created":"2025-05-08","title":"XLD: A Cross-Lane Dataset for Benchmarking Novel Driving View Synthesis","abstract":"Comprehensive testing of autonomous systems through simulation is essential to ensure the safety of autonomous driving vehicles. This requires the generation of safety-critical scenarios that extend beyond the limitations of real-world data collection, as many of these scenarios are rare or rarely encountered on public roads. However, evaluating most existing novel view synthesis (NVS) methods relies on sporadic sampling of image frames from the training data, comparing the rendered images with ground-truth images. Unfortunately, this evaluation protocol falls short of meeting the actual requirements in closed-loop simulations. Specifically, the true application demands the capability to render novel views that extend beyond the original trajectory (such as cross-lane views), which are challenging to capture in the real world. To address this, this paper presents a synthetic dataset for novel driving view synthesis evaluation, which is specifically designed for autonomous driving simulations. This unique dataset includes testing images captured by deviating from the training trajectory by $1-4$ meters. It comprises six sequences that cover various times and weather conditions. Each sequence contains $450$ training images, $120$ testing images, and their corresponding camera poses and intrinsic parameters. Leveraging this novel dataset, we establish the first realistic benchmark for evaluating existing NVS approaches under front-only and multicamera settings. The experimental findings underscore the significant gap in current approaches, revealing their inadequate ability to fulfill the demanding prerequisites of cross-lane or closed-loop simulation.","authors":["Hao Li","Chenming Wu","Ming Yuan","Yan Zhang","Chen Zhao","Chunyu Song","Haocheng Feng","Errui Ding","Dingwen Zhang","Jingdong Wang"],"url":"https://arxiv.org/abs/2406.18360"}
{"created":"2025-05-08","title":"Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming","abstract":"Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has claimed that ICL is functionally equivalent to gradient descent, a type of error-driven learning mechanism. In this paper, we introduce a new way of diagnosing whether ICL is functionally performing error-driven learning. Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an agent's behavior is influenced to a greater degree when presented with improbable examples as compared to more likely ones. The IFE has previously been identified in psycholinguistics where humans exhibit the IFE in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently). In that context, the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming with ICL and found that LLMs indeed display the IFE, with the effect being stronger in larger models. We conclude that at least in the case we studied, ICL is indeed a type of error-driven learning, supporting the hypothesis that an error signal is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of error-driven processing mechanisms in on-line processing.","authors":["Zhenghao Zhou","Robert Frank","R. Thomas McCoy"],"url":"https://arxiv.org/abs/2406.18501"}
{"created":"2025-05-08","title":"Image-GS: Content-Adaptive Image Representation via 2D Gaussians","abstract":"Neural image representations have emerged as a promising approach for encoding and rendering visual data. Combined with learning-based workflows, they demonstrate impressive trade-offs between visual fidelity and memory footprint. Existing methods in this domain, however, often rely on fixed data structures that suboptimally allocate memory or compute-intensive implicit models, hindering their practicality for real-time graphics applications.","authors":["Yunxiang Zhang","Bingxuan Li","Alexandr Kuznetsov","Akshay Jindal","Stavros Diolatzis","Kenneth Chen","Anton Sochenov","Anton Kaplanyan","Qi Sun"],"url":"https://arxiv.org/abs/2407.01866"}
{"created":"2025-05-08","title":"The Latency Price of Threshold Cryptosystem in Blockchains","abstract":"Threshold cryptography is essential for many blockchain protocols. For example, many protocols rely on threshold common coin to implement asynchronous consensus, leader elections, and provide support for randomized applications. Similarly, threshold decryption and threshold time-lock puzzles are often necessary for privacy.","authors":["Zhuolun Xiang","Sourav Das","Zekun Li","Zhoujun Ma","Alexander Spiegelman"],"url":"https://arxiv.org/abs/2407.12172"}
{"created":"2025-05-08","title":"Randomized Transport Plans via Hierarchical Fully Probabilistic Design","abstract":"An optimal randomized strategy for design of balanced, normalized mass transport plans is developed. It replaces -- but specializes to -- the deterministic, regularized optimal transport (OT) strategy, which yields only a certainty-equivalent plan. The incompletely specified -- and therefore uncertain -- transport plan is acknowledged to be a random process. Therefore, hierarchical fully probabilistic design (HFPD) is adopted, yielding an optimal hyperprior supported on the set of possible transport plans, and consistent with prior mean constraints on the marginals of the uncertain plan. This Bayesian resetting of the design problem for transport plans -- which we call HFPD-OT -- confers new opportunities. These include (i) a strategy for the generation of a random sample of joint transport plans; (ii) randomized marginal contracts for individual source-target pairs; and (iii) consistent measures of uncertainty in the plan and its contracts. An application in fair market matching is outlined, in which HFPD-OT enables the recruitment of a more diverse subset of contracts -- than is possible in classical OT -- into the delivery of an expected plan.","authors":["Sarah Boufelja Y.","Anthony Quinn","Robert Shorten"],"url":"https://arxiv.org/abs/2408.02701"}
{"created":"2025-05-08","title":"Modeling Domain and Feedback Transitions for Cross-Domain Sequential Recommendation","abstract":"Nowadays, many recommender systems encompass various domains to cater to users' diverse needs, leading to user behaviors transitioning across different domains. In fact, user behaviors across different domains reveal changes in preference toward recommended items. For instance, a shift from negative feedback to positive feedback indicates improved user satisfaction. However, existing cross-domain sequential recommendation methods typically model user interests by focusing solely on information about domain transitions, often overlooking the valuable insights provided by users' feedback transitions. In this paper, we propose $\\text{Transition}^2$, a novel method to model transitions across both domains and types of user feedback. Specifically, $\\text{Transition}^2$ introduces a transition-aware graph encoder based on user history, assigning different weights to edges according to the feedback type. This enables the graph encoder to extract historical embeddings that capture the transition information between different domains and feedback types. Subsequently, we encode the user history using a cross-transition multi-head self-attention, incorporating various masks to distinguish different types of transitions. To further enhance representation learning, we employ contrastive losses to align transitions across domains and feedback types. Finally, we integrate these modules to make predictions across different domains. Experimental results on two public datasets demonstrate the effectiveness of $\\text{Transition}^2$.","authors":["Changshuo Zhang","Teng Shi","Xiao Zhang","Qi Liu","Ruobing Xie","Jun Xu","Ji-Rong Wen"],"url":"https://arxiv.org/abs/2408.08209"}
{"created":"2025-05-08","title":"End-to-end Surface Optimization for Light Control","abstract":"Designing a freeform surface to reflect or refract light to achieve a target distribution is a challenging inverse problem. In this paper, we propose an end-to-end optimization strategy for an optical surface mesh. Our formulation leverages a novel differentiable rendering model, and is directly driven by the difference between the resulting light distribution and the target distribution. We also enforce geometric constraints related to fabrication requirements, to facilitate CNC milling and polishing of the designed surface. To address the issue of local minima, we formulate a face-based optimal transport problem between the current mesh and the target distribution, which makes effective large changes to the surface shape. The combination of our optimal transport update and rendering-guided optimization produces an optical surface design with a resulting image closely resembling the target, while the geometric constraints in our optimization help to ensure consistency between the rendering model and the final physical results. The effectiveness of our algorithm is demonstrated on a variety of target images using both simulated rendering and physical prototypes.","authors":["Yuou Sun","Bailin Deng","Juyong Zhang"],"url":"https://arxiv.org/abs/2408.13117"}
{"created":"2025-05-08","title":"Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning","abstract":"Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied intelligence. However, even in simple maze environments, LLMs still encounter challenges in long-term path-planning, primarily influenced by their spatial hallucination and context inconsistency hallucination by long-term reasoning. To address this challenge, this study proposes an innovative model, Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs, we propose the Spatial-to-Relational approach, which transforms spatial prompts into entity relations and paths representing entity relation chains. This approach fully taps the potential of LLMs in terms of sequential thinking. As a result, we design a path-planning algorithm based on Q-learning to mitigate the context inconsistency hallucination, which enhances the reasoning ability of LLMs. Using the Q-value of state-action as auxiliary information for prompts, we correct the hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally, we propose a reverse curriculum learning technique based on LLMs to further mitigate the context inconsistency hallucination. LLMs can rapidly accumulate successful experiences by reducing task difficulty and leveraging them to tackle more complex tasks. We performed comprehensive experiments based on Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our S2RCQL achieved a 23%--40% improvement in both success and optimality rates compared with advanced prompt engineering.","authors":["Hourui Deng","Hongjie Zhang","Jie Ou","Chaosheng Feng"],"url":"https://arxiv.org/abs/2408.13184"}
{"created":"2025-05-08","title":"Shared-PIM: Enabling Concurrent Computation and Data Flow for Faster Processing-in-DRAM","abstract":"Processing-in-Memory (PIM) enhances memory with computational capabilities, potentially solving energy and latency issues associated with data transfer between memory and processors. However, managing concurrent computation and data flow within the PIM architecture incurs significant latency and energy penalty for applications. This paper introduces Shared-PIM, an architecture for in-DRAM PIM that strategically allocates rows in memory banks, bolstered by memory peripherals, for concurrent processing and data movement. Shared-PIM enables simultaneous computation and data transfer within a memory bank. When compared to LISA, a state-of-the-art architecture that facilitates data transfers for in-DRAM PIM, Shared-PIM reduces data movement latency and energy by 5x and 1.2x respectively. Furthermore, when integrated to a state-of-the-art (SOTA) in-DRAM PIM architecture (pLUTo), Shared-PIM achieves 1.4x faster addition and multiplication, and thereby improves the performance of matrix multiplication (MM) tasks by 40%, polynomial multiplication (PMM) by 44%, and numeric number transfer (NTT) tasks by 31%. Moreover, for graph processing tasks like Breadth-First Search (BFS) and Depth-First Search (DFS), Shared-PIM achieves a 29% improvement in speed, all with an area overhead of just 7.16% compared to the baseline pLUTo.","authors":["Ahmed Mamdouh","Haoran Geng","Michael Niemier","Xiaobo Sharon Hu","Dayane Reis"],"url":"https://arxiv.org/abs/2408.15489"}
{"created":"2025-05-08","title":"Evolutionary Algorithms Are Significantly More Robust to Noise When They Ignore It","abstract":"Randomized search heuristics (RSHs) are known to have a certain robustness to noise. Mathematical analyses trying to quantify rigorously how robust RSHs are to a noisy access to the objective function typically assume that each solution is re-evaluated whenever it is compared to others. This aims at preventing that a single noisy evaluation has a lasting negative effect, but is computationally expensive and requires the user to foresee that noise is present (as in a noise-free setting, one would never re-evaluate solutions).","authors":["Denis Antipov","Benjamin Doerr"],"url":"https://arxiv.org/abs/2409.00306"}
{"created":"2025-05-08","title":"Training-Free Sketch-Guided Diffusion with Latent Optimization","abstract":"Based on recent advanced diffusion models, Text-to-image (T2I) generation models have demonstrated their capabilities to generate diverse and high-quality images. However, leveraging their potential for real-world content creation, particularly in providing users with precise control over the image generation result, poses a significant challenge. In this paper, we propose an innovative training-free pipeline that extends existing text-to-image generation models to incorporate a sketch as an additional condition. To generate new images with a layout and structure closely resembling the input sketch, we find that these core features of a sketch can be tracked with the cross-attention maps of diffusion models. We introduce latent optimization, a method that refines the noisy latent at each intermediate step of the generation process using cross-attention maps to ensure that the generated images adhere closely to the desired structure outlined in the reference sketch. Through latent optimization, our method enhances the accuracy of image generation, offering users greater control and customization options in content creation.","authors":["Sandra Zhang Ding","Jiafeng Mao","Kiyoharu Aizawa"],"url":"https://arxiv.org/abs/2409.00313"}
{"created":"2025-05-08","title":"Enhancing Test Time Adaptation with Few-shot Guidance","abstract":"Deep neural networks often encounter significant performance drops while facing with domain shifts between training (source) and test (target) data. To address this issue, Test Time Adaptation (TTA) methods have been proposed to adapt pre-trained source model to handle out-of-distribution streaming target data. Although these methods offer some relief, they lack a reliable mechanism for domain shift correction, which can often be erratic in real-world applications. In response, we develop Few-Shot Test Time Adaptation (FS-TTA), a novel and practical setting that utilizes a few-shot support set on top of TTA. Adhering to the principle of few inputs, big gains, FS-TTA reduces blind exploration in unseen target domains. Furthermore, we propose a two-stage framework to tackle FS-TTA, including (i) fine-tuning the pre-trained source model with few-shot support set, along with using feature diversity augmentation module to avoid overfitting, (ii) implementing test time adaptation based on prototype memory bank guidance to produce high quality pseudo-label for model adaptation. Through extensive experiments on three cross-domain classification benchmarks, we demonstrate the superior performance and reliability of our FS-TTA and framework.","authors":["Siqi Luo","Yi Xin","Yuntao Du","Zhongwei Wan","Tao Tan","Guangtao Zhai","Xiaohong Liu"],"url":"https://arxiv.org/abs/2409.01341"}
{"created":"2025-05-08","title":"Question-Answering Dense Video Events","abstract":"This paper presents question-answering on dense video events, a novel task that answers and grounds dense-event questions in long videos, thus challenging MLLMs to faithfully comprehend and reason about multiple events over extended periods of time. To facilitate the study, we construct DeVE-QA -- a dataset featuring 78K questions about 26K events on 10.6K long videos. Our benchmarking shows that state-of-the-art MLLMs struggle on DeVE-QA. For improvement, we propose DeVi, a novel training-free MLLM approach that highlights a hierarchical captioning module, a temporal event memory module, and a self-consistency checking module to respectively detect, contextualize and memorize, and ground dense-events in long videos for question answering. Extensive experiments show that DeVi is superior at answering dense-event questions and grounding relevant video moments. Compared with existing MLLMs, it achieves a remarkable increase of 4.8% and 2.1% for G(round)QA accuracy on DeVE-QA~and NExT-GQA, respectively. Our data and code will be released upon acceptance.","authors":["Hangyu Qin","Junbin Xiao","Angela Yao"],"url":"https://arxiv.org/abs/2409.04388"}
{"created":"2025-05-08","title":"Asynchronous Fractional Multi-Agent Deep Reinforcement Learning for Age-Minimal Mobile Edge Computing","abstract":"In the realm of emerging real-time networked applications like cyber-physical systems (CPS), the Age of Information (AoI) has merged as a pivotal metric for evaluating the timeliness. To meet the high computational demands, such as those in intelligent manufacturing within CPS, mobile edge computing (MEC) presents a promising solution for optimizing computing and reducing AoI. In this work, we study the timeliness of computational-intensive updates and explores jointly optimize the task updating and offloading policies to minimize AoI. Specifically, we consider edge load dynamics and formulate a task scheduling problem to minimize the expected time-average AoI. The fractional objective introduced by AoI and the semi-Markov game nature of the problem render this challenge particularly difficult, with existing approaches not directly applicable. To this end, we present a comprehensive framework to fractional reinforcement learning (RL). We first introduce a fractional single-agent RL framework and prove its linear convergence. We then extend this to a fractional multi-agent RL framework with a convergence analysis. To tackle the challenge of asynchronous control in semi-Markov game, we further design an asynchronous model-free fractional multi-agent RL algorithm, where each device makes scheduling decisions with the hybrid action space without knowing the system dynamics and decisions of other devices. Experimental results show that our proposed algorithms reduce the average AoI by up to 52.6% compared with the best baseline algorithm in our experiments.","authors":["Lyudong Jin","Ming Tang","Jiayu Pan","Meng Zhang","Hao Wang"],"url":"https://arxiv.org/abs/2409.16832"}
{"created":"2025-05-08","title":"LiRA: Light-Robust Adversary for Model-based Reinforcement Learning in Real World","abstract":"Model-based reinforcement learning has attracted much attention due to its high sample efficiency and is expected to be applied to real-world robotic applications. In the real world, as unobservable disturbances can lead to unexpected situations, robot policies should be taken to improve not only control performance but also robustness. Adversarial learning is an effective way to improve robustness, but excessive adversary would increase the risk of malfunction, and make the control performance too conservative. Therefore, this study addresses a new adversarial learning framework to make reinforcement learning robust moderately and not conservative too much. To this end, the adversarial learning is first rederived with variational inference. In addition, \\textit{light robustness}, which allows for maximizing robustness within an acceptable performance degradation, is utilized as a constraint. As a result, the proposed framework, so-called LiRA, can automatically adjust adversary level, balancing robustness and conservativeness. The expected behaviors of LiRA are confirmed in numerical simulations. In addition, LiRA succeeds in learning a force-reactive gait control of a quadrupedal robot only with real-world data collected less than two hours.","authors":["Taisuke Kobayashi"],"url":"https://arxiv.org/abs/2409.19617"}
{"created":"2025-05-08","title":"Replace Anyone in Videos","abstract":"The field of controllable human-centric video generation has witnessed remarkable progress, particularly with the advent of diffusion models. However, achieving precise and localized control over human motion in videos, such as replacing or inserting individuals while preserving desired motion patterns, still remains a formidable challenge. In this work, we present the ReplaceAnyone framework, which focuses on localized human replacement and insertion featuring intricate backgrounds. Specifically, we formulate this task as an image-conditioned video inpainting paradigm with pose guidance, utilizing a unified end-to-end video diffusion architecture that facilitates image-conditioned video inpainting within masked regions. To prevent shape leakage and enable granular local control, we introduce diverse mask forms involving both regular and irregular shapes. Furthermore, we implement an enriched visual guidance mechanism to enhance appearance alignment, a hybrid inpainting encoder to further preserve the detailed background information in the masked video, and a two-phase optimization methodology to simplify the training difficulty. ReplaceAnyone enables seamless replacement or insertion of characters while maintaining the desired pose motion and reference appearance within a single framework. Extensive experimental results demonstrate the effectiveness of our method in generating realistic and coherent video content. The proposed ReplaceAnyone can be seamlessly applied not only to traditional 3D-UNet base models but also to DiT-based video models such as Wan2.1. The code will be available at https://github.com/ali-vilab/UniAnimate-DiT.","authors":["Xiang Wang","Shiwei Zhang","Haonan Qiu","Ruihang Chu","Zekun Li","Yingya Zhang","Changxin Gao","Yuehuan Wang","Chunhua Shen","Nong Sang"],"url":"https://arxiv.org/abs/2409.19911"}
{"created":"2025-05-08","title":"OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition","abstract":"Multimodal Emotion Recognition (MER) is a critical research area that seeks to decode human emotions from diverse data modalities. However, existing machine learning methods predominantly rely on predefined emotion taxonomies, which fail to capture the inherent complexity, subtlety, and multi-appraisal nature of human emotional experiences, as demonstrated by studies in psychology and cognitive science. To overcome this limitation, we advocate for introducing the concept of open vocabulary into MER. This paradigm shift aims to enable models to predict emotions beyond a fixed label space, accommodating a flexible set of categories to better reflect the nuanced spectrum of human emotions. To achieve this, we propose a novel paradigm: Open-Vocabulary MER (OV-MER), which enables emotion prediction without being confined to predefined spaces. However, constructing a dataset that encompasses the full range of emotions for OV-MER is practically infeasible; hence, we present a comprehensive solution including a newly curated database, novel evaluation metrics, and a preliminary benchmark. By advancing MER from basic emotions to more nuanced and diverse emotional states, we hope this work can inspire the next generation of MER, enhancing its generalizability and applicability in real-world scenarios. Code and dataset are available at: https://github.com/zeroQiaoba/AffectGPT.","authors":["Zheng Lian","Haiyang Sun","Licai Sun","Haoyu Chen","Lan Chen","Hao Gu","Zhuofan Wen","Shun Chen","Siyuan Zhang","Hailiang Yao","Bin Liu","Rui Liu","Shan Liang","Ya Li","Jiangyan Yi","Jianhua Tao"],"url":"https://arxiv.org/abs/2410.01495"}
{"created":"2025-05-08","title":"Boosting Masked ECG-Text Auto-Encoders as Discriminative Learners","abstract":"The accurate interpretation of Electrocardiogram (ECG) signals is pivotal for diagnosing cardiovascular diseases. Integrating ECG signals with accompanying textual reports further holds immense potential to enhance clinical diagnostics by combining physiological data and qualitative insights. However, this integration faces significant challenges due to inherent modality disparities and the scarcity of labeled data for robust cross-modal learning. To address these obstacles, we propose D-BETA, a novel framework that pre-trains ECG and text data using a contrastive masked auto-encoder architecture. D-BETA uniquely combines the strengths of generative with boosted discriminative capabilities to achieve robust cross-modal representations. This is accomplished through masked modality modeling, specialized loss functions, and an improved negative sampling strategy tailored for cross-modal alignment. Extensive experiments on five public datasets across diverse downstream tasks demonstrate that D-BETA significantly outperforms existing methods, achieving an average AUC improvement of 15% in linear probing with only one percent of training data and 2% in zero-shot performance without requiring training data over state-of-the-art models. These results highlight the effectiveness of D-BETA, underscoring its potential to advance automated clinical diagnostics through multi-modal representations. Our sample code and checkpoint are made available at https://github.com/manhph2211/D-BETA.","authors":["Hung Manh Pham","Aaqib Saeed","Dong Ma"],"url":"https://arxiv.org/abs/2410.02131"}
{"created":"2025-05-08","title":"Multi-Robot Motion Planning with Diffusion Models","abstract":"Diffusion models have recently been successfully applied to a wide range of robotics applications for learning complex multi-modal behaviors from data. However, prior works have mostly been confined to single-robot and small-scale environments due to the high sample complexity of learning multi-robot diffusion models. In this paper, we propose a method for generating collision-free multi-robot trajectories that conform to underlying data distributions while using only single-robot data. Our algorithm, Multi-robot Multi-model planning Diffusion (MMD), does so by combining learned diffusion models with classical search-based techniques -- generating data-driven motions under collision constraints. Scaling further, we show how to compose multiple diffusion models to plan in large environments where a single diffusion model fails to generalize well. We demonstrate the effectiveness of our approach in planning for dozens of robots in a variety of simulated scenarios motivated by logistics environments. View video demonstrations and code at: https://multi-robot-diffusion.github.io/.","authors":["Yorai Shaoul","Itamar Mishani","Shivam Vats","Jiaoyang Li","Maxim Likhachev"],"url":"https://arxiv.org/abs/2410.03072"}
{"created":"2025-05-08","title":"Is What You Ask For What You Get? Investigating Concept Associations in Text-to-Image Models","abstract":"Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose Concept2Concept, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user-defined distributions or empirical, real-world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool to facilitate use by non-technical end-users. A demo is available at https://tinyurl.com/Concept2ConceptDemo.","authors":["Salma S. Abdel Magid","Weiwei Pan","Simon Warchol","Grace Guo","Junsik Kim","Mahia Rahman","Hanspeter Pfister"],"url":"https://arxiv.org/abs/2410.04634"}
{"created":"2025-05-08","title":"Batched Bayesian optimization by maximizing the probability of including the optimum","abstract":"Batched Bayesian optimization (BO) can accelerate molecular design by efficiently identifying top-performing compounds from a large chemical library. Existing acquisition strategies for batch design in BO aim to balance exploration and exploitation. This often involves optimizing non-additive batch acquisition functions, necessitating approximation via myopic construction and/or diversity heuristics. In this work, we propose an acquisition strategy for discrete optimization that is motivated by pure exploitation, qPO (multipoint Probability of Optimality). qPO maximizes the probability that the batch includes the true optimum, which is expressible as the sum over individual acquisition scores and thereby circumvents the combinatorial challenge of optimizing a batch acquisition function. We differentiate the proposed strategy from parallel Thompson sampling and discuss how it implicitly captures diversity. Finally, we apply our method to the model-guided exploration of large chemical libraries and provide empirical evidence that it is competitive with and complements other state-of-the-art methods in batched Bayesian optimization.","authors":["Jenna Fromer","Runzhong Wang","Mrunali Manjrekar","Austin Tripp","Jos\\'e Miguel Hern\\'andez-Lobato","Connor W. Coley"],"url":"https://arxiv.org/abs/2410.06333"}
{"created":"2025-05-08","title":"Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling","abstract":"Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference.","authors":["Matthew X. Burns","Qingyuan Hou","Michael C. Huang"],"url":"https://arxiv.org/abs/2410.06397"}
{"created":"2025-05-08","title":"HM-DF SNN: Transcending Conventional Online Learning with Advanced Training and Deployment","abstract":"Spiking Neural Networks (SNNs) are considered to have enormous potential in the future development of Artificial Intelligence due to their brain-inspired and energy-efficient properties. Compared to vanilla Spatial-Temporal Back-propagation (STBP) training methods, online training can effectively overcome the risk of GPU memory explosion. However, current online learning framework cannot tackle the inseparability problem of temporal dependent gradients and merely aim to optimize the training memory, resulting in no performance advantages compared to the STBP training models in the inference phase. To address the aforementioned challenges, we propose Hybrid Mechanism-Driven Firing (HM-DF) model, which is a family of advanced models that respectively adopt different spiking calculation schemes in the upper-region and lower-region of the firing threshold. We point out that HM-DF model can effectively separate temporal gradients and tackle the mismatch problem of surrogate gradients, as well as achieving full-stage optimization towards computation speed and memory footprint. Experimental results have demonstrated that HM-DF model can be flexibly combined with various techniques to achieve state-of-the-art performance in the field of online learning, without triggering further power consumption.","authors":["Zecheng Hao","Yifan Huang","Zijie Xu","Wenxuan Liu","Yuanhong Tang","Zhaofei Yu","Tiejun Huang"],"url":"https://arxiv.org/abs/2410.07547"}
{"created":"2025-05-08","title":"Conditional Lagrangian Wasserstein Flow for Time Series Imputation","abstract":"Time series imputation is important for numerous real-world applications. To overcome the limitations of diffusion model-based imputation methods, e.g., slow convergence in inference, we propose a novel method for time series imputation in this work, called Conditional Lagrangian Wasserstein Flow (CLWF). Following the principle of least action in Lagrangian mechanics, we learn the velocity by minimizing the corresponding kinetic energy. Moreover, to enhance the model's performance, we estimate the gradient of a task-specific potential function using a time-dependent denoising autoencoder and integrate it into the base estimator to reduce the sampling variance. Finally, the proposed method demonstrates competitive performance compared to other state-of-the-art imputation approaches.","authors":["Weizhu Qian","Dalin Zhang","Yan Zhao","Yunyao Cheng"],"url":"https://arxiv.org/abs/2410.07550"}
{"created":"2025-05-08","title":"Unique continuation principles for finite-element discretizations of the Laplacian","abstract":"Unique continuation principles are fundamental properties of elliptic partial differential equations, giving conditions that guarantee that the solution to an elliptic equation must be uniformly zero. Since finite-element discretizations are a natural tool to help gain understanding into elliptic equations, it is natural to ask if such principles also hold at the discrete level. In this work, we prove a version of the unique continuation principle for piecewise-linear and -bilinear finite-element discretizations of the Laplacian eigenvalue problem on polygonal domains in $\\mathbb{R}^2$. Namely, we show that any solution to the discretized equation $-\\Delta u = \\lambda u$ with vanishing Dirichlet and Neumann traces must be identically zero under certain geometric and topological assumptions on the resulting triangulation. We also provide a counterexample, showing that a nonzero \\emph{inner solution} exists when the topological assumptions are not satisfied. Finally, we give an application to an eigenvalue interlacing problem, where the space of inner solutions makes an explicit appearance.","authors":["Graham Cox","Scott MacLachlan","Luke Steeves"],"url":"https://arxiv.org/abs/2410.08963"}
{"created":"2025-05-08","title":"Harnessing Causality in Reinforcement Learning With Bagged Decision Times","abstract":"We consider reinforcement learning (RL) for a class of problems with bagged decision times. A bag contains a finite sequence of consecutive decision times. The transition dynamics are non-Markovian and non-stationary within a bag. All actions within a bag jointly impact a single reward, observed at the end of the bag. For example, in mobile health, multiple activity suggestions in a day collectively affect a user's daily commitment to being active. Our goal is to develop an online RL algorithm to maximize the discounted sum of the bag-specific rewards. To handle non-Markovian transitions within a bag, we utilize an expert-provided causal directed acyclic graph (DAG). Based on the DAG, we construct states as a dynamical Bayesian sufficient statistic of the observed history, which results in Markov state transitions within and across bags. We then formulate this problem as a periodic Markov decision process (MDP) that allows non-stationarity within a period. An online RL algorithm based on Bellman equations for stationary MDPs is generalized to handle periodic MDPs. We show that our constructed state achieves the maximal optimal value function among all state constructions for a periodic MDP. Finally, we evaluate the proposed method on testbed variants built from real data in a mobile health clinical trial.","authors":["Daiqi Gao","Hsin-Yu Lai","Predrag Klasnja","Susan A. Murphy"],"url":"https://arxiv.org/abs/2410.14659"}
{"created":"2025-05-08","title":"DeMuVGN: Effective Software Defect Prediction Model by Learning Multi-view Software Dependency via Graph Neural Networks","abstract":"Software defect prediction (SDP) aims to identify high-risk defect modules in software development, optimizing resource allocation. While previous studies show that dependency network metrics improve defect prediction, most methods focus on code-based dependency graphs, overlooking developer factors. Current metrics, based on handcrafted features like ego and global network metrics, fail to fully capture defect-related information. To address this, we propose DeMuVGN, a defect prediction model that learns multi-view software dependency via graph neural networks. We introduce a Multi-view Software Dependency Graph (MSDG) that integrates data, call, and developer dependencies. DeMuVGN also leverages the Synthetic Minority Oversampling Technique (SMOTE) to address class imbalance and enhance defect module identification. In a case study of eight open-source projects across 20 versions, DeMuVGN demonstrates significant improvements: i) models based on multi-view graphs improve F1 scores by 11.1% to 12.1% over single-view models; ii) DeMuVGN improves F1 scores by 17.4% to 45.8% in within-project contexts and by 17.9% to 41.0% in cross-project contexts. Additionally, DeMuVGN excels in software evolution, showing more improvement in later-stage software versions. Its strong performance across different projects highlights its generalizability. We recommend future research focus on multi-view dependency graphs for defect prediction in both mature and newly developed projects.","authors":["Yu Qiao","Lina Gong","Yu Zhao","Yongwei Wang","Mingqiang Wei"],"url":"https://arxiv.org/abs/2410.19550"}
{"created":"2025-05-08","title":"Vision-Language Models Create Cross-Modal Task Representations","abstract":"Autoregressive vision-language models (VLMs) can handle many tasks within a single model, yet the representations that enable this capability remain opaque. We find that VLMs align conceptually equivalent inputs into a shared task vector, which is invariant to modality (text, image) and format (examples, instruction), and may simplify VLM processing. We measure this alignment via cross-modal transfer -- the ability of a task vector derived in one modality to trigger the correct generation in another -- on a range of tasks and model architectures. Although the task vector is highly compressed, we find that this single vector outperforms prompting the model with the full task information, unique to this cross-modal case. Furthermore, we show that task vectors can be transferred from a base language model to its fine-tuned vision-language counterpart, and that they can be derived solely from instructions without the need for examples. Taken together, our findings shed light on how VLMs internally process task information, and how they map different modalities into common semantic representations. Project page: https://vlm-cross-modal-reps.github.io.","authors":["Grace Luo","Trevor Darrell","Amir Bar"],"url":"https://arxiv.org/abs/2410.22330"}
{"created":"2025-05-08","title":"A Systematic Literature Review of Spatio-Temporal Graph Neural Network Models for Time Series Forecasting and Classification","abstract":"In recent years, spatio-temporal graph neural networks (GNNs) have attracted considerable interest in the field of time series analysis, due to their ability to capture dependencies among variables and across time points. The objective of the presented systematic literature review is hence to provide a comprehensive overview of the various modeling approaches and application domains of GNNs for time series classification and forecasting. A database search was conducted, and over 150 journal papers were selected for a detailed examination of the current state-of-the-art in the field. This examination is intended to offer to the reader a comprehensive collection of proposed models, links to related source code, available datasets, benchmark models, and fitting results. All this information is hoped to assist researchers in future studies. To the best of our knowledge, this is the first systematic literature review presenting a detailed comparison of the results of current spatio-temporal GNN models in different domains. In addition, in its final part this review discusses current limitations and challenges in the application of spatio-temporal GNNs, such as comparability, reproducibility, explainability, poor information capacity, and scalability.","authors":["Flavio Corradini","Flavio Gerosa","Marco Gori","Carlo Lucheroni","Marco Piangerelli","Martina Zannotti"],"url":"https://arxiv.org/abs/2410.22377"}
{"created":"2025-05-08","title":"VecCity: A Taxonomy-guided Library for Map Entity Representation Learning","abstract":"Electronic maps consist of diverse entities, such as points of interest (POIs), road networks, and land parcels, playing a vital role in applications like ITS and LBS. Map entity representation learning (MapRL) generates versatile and reusable data representations, providing essential tools for efficiently managing and utilizing map entity data. Despite the progress in MapRL, two key challenges constrain further development. First, existing research is fragmented, with models classified by the type of map entity, limiting the reusability of techniques across different tasks. Second, the lack of unified benchmarks makes systematic evaluation and comparison of models difficult. To address these challenges, we propose a novel taxonomy for MapRL that organizes models based on functional module-such as encoders, pre-training tasks, and downstream tasks-rather than by entity type. Building on this taxonomy, we present a taxonomy-driven library, VecCity, which offers easy-to-use interfaces for encoding, pre-training, fine-tuning, and evaluation. The library integrates datasets from nine cities and reproduces 21 mainstream MapRL models, establishing the first standardized benchmarks for the field. VecCity also allows users to modify and extend models through modular components, facilitating seamless experimentation. Our comprehensive experiments cover multiple types of map entities and evaluate 21 VecCity pre-built models across various downstream tasks. Experimental results demonstrate the effectiveness of VecCity in streamlining model development and provide insights into the impact of various components on performance. By promoting modular design and reusability, VecCity offers a unified framework to advance research and innovation in MapRL. The code is available at https://github.com/Bigscity-VecCity/VecCity.","authors":["Wentao Zhang","Jingyuan Wang","Yifan Yang","Leong Hou U"],"url":"https://arxiv.org/abs/2411.00874"}
{"created":"2025-05-08","title":"Advancements and limitations of LLMs in replicating human color-word associations","abstract":"Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT-4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and 80 words (10 word from eight categories) in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category. However, the highest median performance was approximately 50% even for GPT-4o with visual inputs (chance level of 10%). Moreover, we found performance variations across word categories and colors: while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions. Interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. Thus, despite reasonable alignment in basic color discrimination, humans and LLMs still diverge systematically in the words they assign to those colors. Our study highlights both the advancements in LLM capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and LLMs in representing color-word associations.","authors":["Makoto Fukushima","Shusuke Eshita","Hiroshige Fukuhara"],"url":"https://arxiv.org/abs/2411.02116"}
{"created":"2025-05-08","title":"LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation","abstract":"CLIP is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. Its effectiveness primarily stems from the use of natural language as rich supervision. Motivated by the remarkable advancements in large language models (LLMs), this work explores how LLMs' superior text understanding and extensive open-world knowledge can enhance CLIP's capability, especially for processing longer and more complex image captions. We propose an efficient post-training strategy that integrates LLMs into pretrained CLIP. To address the challenge posed by the autoregressive nature of LLMs, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of LLM outputs. Extensive experiments demonstrate that our approach outperforms LoRA-based methods, achieving nearly fourfold faster training with superior performance. Furthermore, we validate substantial improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining.","authors":["Weiquan Huang","Aoqi Wu","Yifan Yang","Xufang Luo","Yuqing Yang","Liang Hu","Qi Dai","Chunyu Wang","Xiyang Dai","Dongdong Chen","Chong Luo","Lili Qiu"],"url":"https://arxiv.org/abs/2411.04997"}
{"created":"2025-05-08","title":"Cyclic Vision-Language Manipulator: Towards Reliable and Fine-Grained Image Interpretation for Automated Report Generation","abstract":"Despite significant advancements in automated report generation, the opaqueness of text interpretability continues to cast doubt on the reliability of the content produced. This paper introduces a novel approach to identify specific image features in X-ray images that influence the outputs of report generation models. Specifically, we propose Cyclic Vision-Language Manipulator CVLM, a module to generate a manipulated X-ray from an original X-ray and its report from a designated report generator. The essence of CVLM is that cycling manipulated X-rays to the report generator produces altered reports aligned with the alterations pre-injected into the reports for X-ray generation, achieving the term \"cyclic manipulation\". This process allows direct comparison between original and manipulated X-rays, clarifying the critical image features driving changes in reports and enabling model users to assess the reliability of the generated texts. Empirical evaluations demonstrate that CVLM can identify more precise and reliable features compared to existing explanation methods, significantly enhancing the transparency and applicability of AI-generated reports.","authors":["Yingying Fang","Zihao Jin","Shaojie Guo","Jinda Liu","Zhiling Yue","Yijian Gao","Junzhi Ning","Zhi Li","Simon Walsh","Guang Yang"],"url":"https://arxiv.org/abs/2411.05261"}
{"created":"2025-05-08","title":"Error Analysis of a Fully Discrete Scheme for The Cahn--Hilliard Cross-Diffusion Model in Lymphangiogenesis","abstract":"This paper introduces a stabilized finite element scheme for the Cahn--Hilliard cross-diffusion model, which is characterized by strongly coupled mobilities, nonlinear diffusion, and complex cross-diffusion terms. These features pose significant analytical and computational challenges, particularly due to the destabilizing effects of cross-diffusion and the absence of standard structural properties. To address these issues, we establish discrete energy stability and prove the existence of a finite element solution for the proposed scheme. A key contribution of this work is the derivation of rigorous error estimates, utilizing the novel $L^{\\frac{4}{3}}(0,T; L^{\\frac{6}{5}}(\\Omega))$ norm for the chemical potential. This enables a comprehensive convergence analysis, where we derive error estimates in the $L^{\\infty}(H^1(\\Omega))$ and $L^{\\infty}(L^2(\\Omega))$ norms, and establish convergence of the numerical solution in the $L^{\\frac{4}{3}}(0,T; W^{1,\\frac{6}{5}}(\\Omega))$ norm. Furthermore, the convergence analysis relies on a uniform bound of the form $\\sum_{k=0}^n\\tau\\|\\nabla(\\cdot)\\|_{L^{\\frac{6}{5}}}^{\\frac{4}{3}}$ to control the chemical potentials, marking a clear departure from the classical $\\sum_{k=0}^n\\tau\\|\\nabla(\\cdot)\\|_{L^{2}}^{2}$ estimate commonly used in Cahn--Hilliard-type models. Our approach builds upon and extends existing frameworks, effectively addressing challenges posed by cross-diffusion effects and the lack of uniform estimates. Numerical experiments validate the theoretical results and demonstrate the scheme's ability to capture phase separation dynamics consistent with the Cahn--Hilliard equation.","authors":["Boyi Wang","Naresh Kumar","Jinyun Yuan"],"url":"https://arxiv.org/abs/2411.06488"}
{"created":"2025-05-08","title":"LLM-based Bi-level Multi-interest Learning Framework for Sequential Recommendation","abstract":"Sequential recommendation (SR) leverages users' dynamic preferences, with recent advances incorporating multi-interest learning to model diverse user interests. However, most multi-interest SR models rely on noisy, sparse implicit feedback, limiting recommendation accuracy. Large language models (LLMs) offer robust reasoning on low-quality data but face high computational costs and latency challenges for SR integration. We propose a novel LLM-based multi-interest SR framework combining implicit behavioral and explicit semantic perspectives. It includes two modules: the Implicit Behavioral Interest Module (IBIM), which learns from user behavior using a traditional SR model, and the Explicit Semantic Interest Module (ESIM), which uses clustering and prompt-engineered LLMs to extract semantic multi-interest representations from informative samples. Semantic insights from ESIM enhance IBIM's behavioral representations via modality alignment and semantic prediction tasks. During inference, only IBIM is used, ensuring efficient, LLM-free recommendations. Experiments on four real-world datasets validate the framework's effectiveness and practicality.","authors":["Shutong Qiao","Chen Gao","Wei Yuan","Yong Li","Hongzhi Yin"],"url":"https://arxiv.org/abs/2411.09410"}
{"created":"2025-05-08","title":"Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT","abstract":"Collaborative problem solving (CPS) is widely recognized as a critical 21st-century skill. Assessing CPS depends heavily on coding the communication data using a construct-relevant framework, and this process has long been a major bottleneck to scaling up such assessments. Based on five datasets and two coding frameworks, we demonstrate that ChatGPT can code communication data to a satisfactory level, though performance varies across ChatGPT models, and depends on the coding framework and task characteristics. Interestingly, newer reasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily yield better coding results. Additionally, we show that refining prompts based on feedback from miscoded cases can improve coding accuracy in some instances, though the effectiveness of this approach is not consistent across all tasks. These findings offer practical guidance for researchers and practitioners in developing scalable, efficient methods to analyze communication data in support of 21st-century skill assessment.","authors":["Jiangang Hao","Wenju Cui","Patrick Kyllonen","Emily Kerzabi","Lei Liu","Michael Flor"],"url":"https://arxiv.org/abs/2411.10246"}
{"created":"2025-05-08","title":"Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting Content Creators From AI Crawlers","abstract":"The success of generative AI relies heavily on training on data scraped through extensive crawling of the Internet, a practice that has raised significant copyright, privacy, and ethical concerns. While few measures are designed to resist a resource-rich adversary determined to scrape a site, crawlers can be impacted by a range of existing tools such as robots.txt, NoAI meta tags, and active crawler blocking by reverse proxies.","authors":["Enze Liu","Elisa Luo","Shawn Shan","Geoffrey M. Voelker","Ben Y. Zhao","Stefan Savage"],"url":"https://arxiv.org/abs/2411.15091"}
{"created":"2025-05-08","title":"Adaptive Gen-AI Guidance in Virtual Reality: A Multimodal Exploration of Engagement in Neapolitan Pizza-Making","abstract":"Virtual reality (VR) offers promising opportunities for procedural learning, particularly in preserving intangible cultural heritage. Advances in generative artificial intelligence (Gen-AI) further enrich these experiences by enabling adaptive learning pathways. However, evaluating such adaptive systems using traditional temporal metrics remains challenging due to the inherent variability in Gen-AI response times. To address this, our study employs multimodal behavioural metrics, including visual attention, physical exploratory behaviour, and verbal interaction, to assess user engagement in an adaptive VR environment. In a controlled experiment with 54 participants, we compared three levels of adaptivity (high, moderate, and non-adaptive baseline) within a Neapolitan pizza-making VR experience. Results show that moderate adaptivity optimally enhances user engagement, significantly reducing unnecessary exploratory behaviour and increasing focused visual attention on the AI avatar. Our findings suggest that a balanced level of adaptive AI provides the most effective user support, offering practical design recommendations for future adaptive educational technologies.","authors":["Ka Hei Carrie Lau","Sema Sen","Philipp Stark","Efe Bozkir","Enkelejda Kasneci"],"url":"https://arxiv.org/abs/2411.18438"}
{"created":"2025-05-08","title":"Garden city: A synthetic dataset and sandbox environment for analysis of pre-processing algorithms for GPS human mobility data","abstract":"Human mobility datasets have seen increasing adoption in the past decade, enabling diverse applications that leverage the high precision of measured trajectories relative to other human mobility datasets. However, there are concerns about whether the high sparsity in some commercial datasets can introduce errors due to lack of robustness in processing algorithms, which could compromise the validity of downstream results. The scarcity of \"ground-truth\" data makes it particularly challenging to evaluate and calibrate these algorithms. To overcome these limitations and allow for an intermediate form of validation of common processing algorithms, we propose a synthetic trajectory simulator and sandbox environment meant to replicate the features of commercial datasets that could cause errors in such algorithms, and which can be used to compare algorithm outputs with \"ground-truth\" synthetic trajectories and mobility diaries. Our code is open-source and is publicly available alongside tutorial notebooks and sample datasets generated with it.","authors":["Thomas H. Li","Francisco Barreras"],"url":"https://arxiv.org/abs/2412.00913"}
{"created":"2025-05-08","title":"SUICA: Learning Super-high Dimensional Sparse Implicit Neural Representations for Spatial Transcriptomics","abstract":"Spatial Transcriptomics (ST) is a method that captures gene expression profiles aligned with spatial coordinates. The discrete spatial distribution and the super-high dimensional sequencing results make ST data challenging to be modeled effectively. In this paper, we manage to model ST in a continuous and compact manner by the proposed tool, SUICA, empowered by the great approximation capability of Implicit Neural Representations (INRs) that can enhance both the spatial density and the gene expression. Concretely within the proposed SUICA, we incorporate a graph-augmented Autoencoder to effectively model the context information of the unstructured spots and provide informative embeddings that are structure-aware for spatial mapping. We also tackle the extremely skewed distribution in a regression-by-classification fashion and enforce classification-based loss functions for the optimization of SUICA. By extensive experiments of a wide range of common ST platforms under varying degradations, SUICA outperforms both conventional INR variants and SOTA methods regarding numerical fidelity, statistical correlation, and bio-conservation. The prediction by SUICA also showcases amplified gene signatures that enriches the bio-conservation of the raw data and benefits subsequent analysis. The code is available at https://github.com/Szym29/SUICA.","authors":["Qingtian Zhu","Yumin Zheng","Yuling Sang","Yifan Zhan","Ziyan Zhu","Jun Ding","Yinqiang Zheng"],"url":"https://arxiv.org/abs/2412.01124"}
{"created":"2025-05-08","title":"Enhanced Photovoltaic Power Forecasting: An iTransformer and LSTM-Based Model Integrating Temporal and Covariate Interactions","abstract":"Accurate photovoltaic (PV) power forecasting is critical for integrating renewable energy sources into the grid, optimizing real-time energy management, and ensuring energy reliability amidst increasing demand. However, existing models often struggle with effectively capturing the complex relationships between target variables and covariates, as well as the interactions between temporal dynamics and multivariate data, leading to suboptimal forecasting accuracy. To address these challenges, we propose a novel model architecture that leverages the iTransformer for feature extraction from target variables and employs long short-term memory (LSTM) to extract features from covariates. A cross-attention mechanism is integrated to fuse the outputs of both models, followed by a Kolmogorov-Arnold network (KAN) mapping for enhanced representation. The effectiveness of the proposed model is validated using publicly available datasets from Australia, with experiments conducted across four seasons. Results demonstrate that the proposed model effectively capture seasonal variations in PV power generation and improve forecasting accuracy.","authors":["Guang Wu","Yun Wang","Qian Zhou","Ziyang Zhang"],"url":"https://arxiv.org/abs/2412.02302"}
{"created":"2025-05-08","title":"DynamicControl: Adaptive Condition Selection for Improved Text-to-Image Generation","abstract":"To enhance the controllability of text-to-image diffusion models, current ControlNet-like models have explored various control signals to dictate image attributes. However, existing methods either handle conditions inefficiently or use a fixed number of conditions, which does not fully address the complexity of multiple conditions and their potential conflicts. This underscores the need for innovative approaches to manage multiple conditions effectively for more reliable and detailed image synthesis. To address this issue, we propose a novel framework, DynamicControl, which supports dynamic combinations of diverse control signals, allowing adaptive selection of different numbers and types of conditions. Our approach begins with a double-cycle controller that generates an initial real score sorting for all input conditions by leveraging pre-trained conditional generation models and discriminative models. This controller evaluates the similarity between extracted conditions and input conditions, as well as the pixel-level similarity with the source image. Then, we integrate a Multimodal Large Language Model (MLLM) to build an efficient condition evaluator. This evaluator optimizes the ordering of conditions based on the double-cycle controller's score ranking. Our method jointly optimizes MLLMs and diffusion models, utilizing MLLMs' reasoning capabilities to facilitate multi-condition text-to-image (T2I) tasks. The final sorted conditions are fed into a parallel multi-control adapter, which learns feature maps from dynamic visual conditions and integrates them to modulate ControlNet, thereby enhancing control over generated images. Through both quantitative and qualitative comparisons, DynamicControl demonstrates its superiority over existing methods in terms of controllability, generation quality and composability under various conditional controls.","authors":["Qingdong He","Jinlong Peng","Pengcheng Xu","Boyuan Jiang","Xiaobin Hu","Donghao Luo","Yong Liu","Yabiao Wang","Chengjie Wang","Xiangtai Li","Jiangning Zhang"],"url":"https://arxiv.org/abs/2412.03255"}
{"created":"2025-05-08","title":"Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion","abstract":"Sea Surface Temperature (SST) reconstructions from satellite images affected by cloud gaps have been extensively documented in the past three decades. Here we describe several Machine Learning models to fill the cloud-occluded areas starting from MODIS Aqua nighttime L3 images. To tackle this challenge, we employed a type of Convolutional Neural Network model (U-net) to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas. We demonstrate the outstanding precision of U-net with respect to available products done using OI interpolation algorithms. Our best-performing architecture show 50% lower root mean square errors over established gap-filling methods.","authors":["Andrea Asperti","Ali Aydogdu","Angelo Greco","Fabio Merizzi","Pietro Miraglio","Beniamino Tartufoli","Alessandro Testa","Nadia Pinardi","Paolo Oddo"],"url":"https://arxiv.org/abs/2412.03413"}
{"created":"2025-05-08","title":"Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail","abstract":"We introduce Stereo Anywhere, a novel stereo-matching framework that combines geometric constraints with robust priors from monocular depth Vision Foundation Models (VFMs). By elegantly coupling these complementary worlds through a dual-branch architecture, we seamlessly integrate stereo matching with learned contextual cues. Following this design, our framework introduces novel cost volume fusion mechanisms that effectively handle critical challenges such as textureless regions, occlusions, and non-Lambertian surfaces. Through our novel optical illusion dataset, MonoTrap, and extensive evaluation across multiple benchmarks, we demonstrate that our synthetic-only trained model achieves state-of-the-art results in zero-shot generalization, significantly outperforming existing solutions while showing remarkable robustness to challenging cases such as mirrors and transparencies.","authors":["Luca Bartolomei","Fabio Tosi","Matteo Poggi","Stefano Mattoccia"],"url":"https://arxiv.org/abs/2412.04472"}
{"created":"2025-05-08","title":"Budgeted Spatial Data Acquisition: When Coverage and Connectivity Matter","abstract":"Data is undoubtedly becoming a commodity like oil, land, and labor in the 21st century. Although there have been many successful marketplaces for data trading, the existing data marketplaces lack consideration of the case where buyers want to acquire a collection of datasets (instead of one), and the overall spatial coverage and connectivity matter. In this paper, we take the first attempt to formulate this problem as Budgeted Maximum Coverage with Connectivity Constraint (BMCC), which aims to acquire a dataset collection with the maximum spatial coverage under a limited budget while maintaining spatial connectivity. To solve the problem, we propose two approximate algorithms with detailed theoretical guarantees and time complexity analysis, followed by two acceleration strategies to further improve the efficiency of the algorithm. Experiments are conducted on five real-world spatial dataset collections to verify the efficiency and effectiveness of our algorithms.","authors":["Wenzhe Yang","Shixun Huang","Sheng Wang","Zhiyong Peng"],"url":"https://arxiv.org/abs/2412.04853"}
{"created":"2025-05-08","title":"Efficiency Meets Fidelity: A Novel Quantization Framework for Stable Diffusion","abstract":"Text-to-image generation via Stable Diffusion models (SDM) have demonstrated remarkable capabilities. However, their computational intensity, particularly in the iterative denoising process, hinders real-time deployment in latency-sensitive applications. While Recent studies have explored post-training quantization (PTQ) and quantization-aware training (QAT) methods to compress Diffusion models, existing methods often overlook the consistency between results generated by quantized models and those from floating-point models. This consistency is paramount for professional applications where both efficiency and output reliability are essential. To ensure that quantized SDM generates high-quality and consistent images, we propose an efficient quantization framework for SDM. Our framework introduces a Serial-to-Parallel pipeline that simultaneously maintains training-inference consistency and ensures optimization stability. Building upon this foundation, we further develop several techniques including multi-timestep activation quantization, time information precalculation, inter-layer distillation, and selective freezing, to achieve high-fidelity generation in comparison to floating-point models while maintaining quantization efficiency.","authors":["Shuaiting Li","Juncan Deng","Zeyu Wang","Kedong Xu","Rongtao Deng","Hong Gu","Haibin Shen","Kejie Huang"],"url":"https://arxiv.org/abs/2412.06661"}
{"created":"2025-05-08","title":"SecureNT: Smart Topology Obfuscation for Privacy-Aware Network Monitoring","abstract":"Network tomography plays a crucial role in network monitoring and management, where network topology serves as the fundamental basis for various tomography tasks including traffic matrix estimation and link performance inference. The topology information, however, can be inferred through end-to-end measurements using various inference algorithms, posing significant security risks to network infrastructure. While existing protection methods attempt to secure topology information by modifying end-to-end measurements, they often require complex computation and sophisticated modification strategies, making real-time protection challenging. Moreover, these modifications typically render the measurements unusable for network monitoring, even by trusted users. This paper presents a novel privacy-preserving framework that addresses these limitations. Our approach provides efficient topology protection while maintaining the utility of measurements for authorized network monitoring. Through extensive evaluation on both simulated and real-world networks, we demonstrate that our framework achieves superior privacy protection compared to existing methods while enabling trusted users to effectively monitor network performance. Our solution offers a practical approach for organizations to protect sensitive topology information without sacrificing their network monitoring capabilities.","authors":["Chengze Du","Jibin Shi","Hui Xu","Guangzhen Yao"],"url":"https://arxiv.org/abs/2412.08177"}
{"created":"2025-05-08","title":"SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph Generation","abstract":"Dynamic scenes contain intricate spatio-temporal information, crucial for mobile robots, UAVs, and autonomous driving systems to make informed decisions. Parsing these scenes into semantic triplets  for accurate Scene Graph Generation (SGG) is highly challenging due to the fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities of Large Language Models (LLMs), we propose SceneLLM, a novel framework that leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework introduces a Video-to-Language (V2L) mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for LLMs. To better encode spatial information, we devise a Spatial Information Aggregation (SIA) scheme, inspired by the structure of Chinese characters, which encodes spatial data into tokens. Using Optimal Transport (OT), we generate an implicit language signal from the frame-level token sequence that captures the video's spatio-temporal information. To further improve the LLM's ability to process this implicit linguistic input, we apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a transformer-based SGG predictor to decode the LLM's reasoning and predict semantic triplets. Our method achieves state-of-the-art results on the Action Genome (AG) benchmark, and extensive experiments show the effectiveness of SceneLLM in understanding and generating accurate dynamic scene graphs.","authors":["Hang Zhang","Zhuoling Li","Jun Liu"],"url":"https://arxiv.org/abs/2412.11026"}
{"created":"2025-05-08","title":"Information-Geometric Barycenters for Bayesian Federated Learning","abstract":"Federated learning (FL) is a widely used and impactful distributed optimization framework that achieves consensus through averaging locally trained models. While effective, this approach may not align well with Bayesian inference, where the model space has the structure of a distribution space. Taking an information-geometric perspective, we reinterpret FL aggregation as the problem of finding the barycenter of local posteriors using a prespecified divergence metric, minimizing the average discrepancy across clients. This perspective provides a unifying framework that generalizes many existing methods and offers crisp insights into their theoretical underpinnings. We then propose BA-BFL, an algorithm that retains the convergence properties of Federated Averaging in non-convex settings. In non-independent and identically distributed scenarios, we conduct extensive comparisons with statistical aggregation techniques, showing that BA-BFL achieves performance comparable to state-of-the-art methods while offering a geometric interpretation of the aggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep Learning, exploring the impact of Bayesian layers on uncertainty quantification and model calibration.","authors":["Nour Jamoussi","Giuseppe Serra","Photios A. Stavrou","Marios Kountouris"],"url":"https://arxiv.org/abs/2412.11646"}
{"created":"2025-05-08","title":"Rehearsal-Free Continual Federated Learning with Synergistic Synaptic Intelligence","abstract":"Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding knowledge forgetting of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.","authors":["Yichen Li","Yuying Wang","Haozhao Wang","Yining Qi","Tianzhe Xiao","Ruixuan Li"],"url":"https://arxiv.org/abs/2412.13779"}
{"created":"2025-05-08","title":"ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language Modeling Exploitation","abstract":"As large language models (LLMs) increasingly depend on web-scraped datasets, concerns arise over their potential to generate verbatim training content with copyrighted or private information. However, current protections against web crawling or sample-specific memorization are inherently limited, as they require compliance from crawlers (e.g., respecting robots.txt) or model trainers (e.g., applying differential privacy). To empower data owners with direct control, we propose ExpShiled, a proactive self-defense mechanism that mitigates sample-specific memorization via imperceptible text perturbations. This approach requires no external collaboration while maintaining original readability. To evaluate individual-level defense efficacy, we first propose the metric of instance exploitation: a zero value indicates perfect defense, achieved when a protected text's log-perplexity ranking aligns with its counterfactual untrained ranking. We then reveal and validate the memorization trigger hypothesis, demonstrating that a model's memorization of a specific text sample stems primarily from its outlier tokens. Leveraging this insight, we design targeted perturbations that (1) prioritize inherent trigger tokens and (2) introduce artificial trigger tokens as pitfalls to disrupt memorization on the protected sample. Experiments validate our defense across model scales, languages, vision-to-language tasks, and fine-tuning methods. Even with privacy backdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55, and instance exploitation approaches zero. This suggests that compared to the ideal no-misuse scenario, the risk of exposing a text instance remains nearly unchanged despite its inclusion in training data.","authors":["Ruixuan Liu","Toan Tran","Tianhao Wang","Hongsheng Hu","Shuo Wang","Li Xiong"],"url":"https://arxiv.org/abs/2412.21123"}
{"created":"2025-05-08","title":"Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling","abstract":"Given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. Prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures. Our work explores ways to tackle these issues through both synthetic data as well as model refinement. We introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. The synthetic pairs can then be used to augment the training of virtual try-on. We also propose an Error-Aware Refinement-based Schr\\\"odinger Bridge (EARSB) that surgically targets localized generation errors for correcting the output of a base virtual try-on model. To identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the Schr\\\"odinger Bridge's noise schedule with its confidence heatmap. Experiments on VITON-HD and DressCode-Upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while EARSB improves the overall image quality. In user studies, our model is preferred by the users in an average of 59% of cases.","authors":["Nannan Li","Kevin J. Shih","Bryan A. Plummer"],"url":"https://arxiv.org/abs/2501.04666"}
{"created":"2025-05-08","title":"SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution","abstract":"Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving competitive performance among open-source models with scores of 22.0% and 30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.","authors":["Chengxing Xie","Bowen Li","Chang Gao","He Du","Wai Lam","Difan Zou","Kai Chen"],"url":"https://arxiv.org/abs/2501.05040"}
{"created":"2025-05-08","title":"Automated Analysis of Logically Constrained Rewrite Systems using crest","abstract":"We present crest, a tool for automatically proving (non-)confluence and termination of logically constrained rewrite systems. We compare crest to other tools for logically constrained rewriting. Extensive experiments demonstrate the promise of crest.","authors":["Jonas Sch\\\"opf","Aart Middeldorp"],"url":"https://arxiv.org/abs/2501.05240"}
{"created":"2025-05-08","title":"Integrating adaptive optimization into least squares progressive iterative approximation","abstract":"This paper introduces the Adaptive Gradient Least Squares Progressive iterative Approximation (AdagradLSPIA), an accelerated version of the Least Squares Progressive Iterative Approximation (LSPIA) method, enhanced with adaptive optimization techniques inspired by the adaptive gradient (Adagrad) algorithm. By using historical (accumulated) gradient information to dynamically adjust weights, AdagradLSPIA achieves faster convergence compared to the standard LSPIA method. The effectiveness of AdagradLSPIA is demonstrated through its application to tensor product B-spline surface fitting, where this method consistently outperforms LSPIA in terms of accuracy, computational efficiency, and robustness to variations in global weight selection.","authors":["Svaj\\=unas Sajavi\\v{c}ius"],"url":"https://arxiv.org/abs/2501.10170"}
{"created":"2025-05-08","title":"Uncertainty-Aware Digital Twins: Robust Model Predictive Control using Time-Series Deep Quantile Learning","abstract":"Digital Twins, virtual replicas of physical systems that enable real-time monitoring, model updates, predictions, and decision-making, present novel avenues for proactive control strategies for autonomous systems. However, achieving real-time decision-making in Digital Twins considering uncertainty necessitates an efficient uncertainty quantification (UQ) approach and optimization driven by accurate predictions of system behaviors, which remains a challenge for learning-based methods. This paper presents a simultaneous multi-step robust model predictive control (MPC) framework that incorporates real-time decision-making with uncertainty awareness for Digital Twin systems. Leveraging a multistep ahead predictor named Time-Series Dense Encoder (TiDE) as the surrogate model, this framework differs from conventional MPC models that provide only one-step ahead predictions. In contrast, TiDE can predict future states within the prediction horizon in a one-shot, significantly accelerating MPC. Furthermore, quantile regression is employed with the training of TiDE to perform flexible while computationally efficient UQ on data uncertainty. Consequently, with the deep learning quantiles, the robust MPC problem is formulated into a deterministic optimization problem and provides a safety buffer that accommodates disturbances to enhance constraint satisfaction rate. As a result, the proposed method outperforms existing robust MPC methods by providing less-conservative UQ and has demonstrated efficacy in an engineering case study involving Directed Energy Deposition (DED) additive manufacturing. This proactive while uncertainty-aware control capability positions the proposed method as a potent tool for future Digital Twin applications and real-time process control in engineering systems.","authors":["Yi-Ping Chen","Ying-Kuan Tsai","Vispi Karkaria","Wei Chen"],"url":"https://arxiv.org/abs/2501.10337"}
{"created":"2025-05-08","title":"Checkification: A Practical Approach for Testing Static Analysis Truths","abstract":"Static analysis is an essential component of many modern software development tools. Unfortunately, the ever-increasing complexity of static analyzers makes their coding error-prone. Even analysis tools based on rigorous mathematical techniques, such as abstract interpretation, are not immune to bugs. Ensuring the correctness and reliability of software analyzers is critical if they are to be inserted in production compilers and development environments. While compiler validation has seen notable success, formal validation of static analysis tools remains relatively unexplored. In this paper, we propose a method for testing abstract interpretation-based static analyzers. Broadly, it consists in checking, over a suite of benchmarks, that the properties inferred statically are satisfied dynamically. The main advantage of our approach lies in its simplicity, which stems directly from framing it within the Ciao assertion-based validation framework, and its blended static/dynamic assertion checking approach. We demonstrate that in this setting, the analysis can be tested with little effort by combining the following components already present in the framework: 1) the static analyzer, which outputs its results as the original program source with assertions interspersed; 2) the assertion run-time checking mechanism, which instruments a program to ensure that no assertion is violated at run time; 3) the random test case generator, which generates random test cases satisfying the properties present in assertion preconditions; and 4) the unit-test framework, which executes those test cases. We have applied our approach to the CiaoPP static analyzer, resulting in the identification of many bugs with reasonable overhead. Most of these bugs have been either fixed or confirmed, helping us detect a range of errors not only related to analysis soundness but also within other aspects of the framework.","authors":["Daniela Ferreiro","Ignacio Casso","Jose F. Morales","Pedro L\\'opez-Garc\\'ia","Manuel V. Hermenegildo"],"url":"https://arxiv.org/abs/2501.12093"}
{"created":"2025-05-08","title":"A quantitative comparison of high-order asymptotic-preserving and asymptotically-accurate IMEX methods for the Euler equations with non-ideal gases","abstract":"We present a quantitative comparison between two different Implicit-Explicit Runge-Kutta (IMEX-RK) approaches for the Euler equations of gas dynamics, specifically tailored for the low Mach limit. In this regime, a classical IMEX-RK approach involves an implicit coupling between the momentum and energy balance so as to avoid the acoustic CFL restriction, while the density can be treated in a fully explicit fashion. This approach leads to a mildly nonlinear equation for the pressure, which can be solved according to a fixed point procedure. An alternative strategy consists of employing a semi-implicit temporal integrator based on IMEX-RK methods (SI-IMEX-RK). The stiff dependence is carefully analyzed, so as to avoid the solution of a nonlinear equation for the pressure also for equations of state (EOS) of non-ideal gases. The spatial discretization is based on a Discontinuous Galerkin (DG) method, which naturally allows high-order accuracy. The asymptotic-preserving (AP) and the asymptotically-accurate (AA) properties of the two approaches are assessed on a number of classical benchmarks for ideal gases and on their extension to non-ideal gases.","authors":["Giuseppe Orlando","Sebastiano Boscarino","Giovanni Russo"],"url":"https://arxiv.org/abs/2501.12733"}
{"created":"2025-05-08","title":"Towards Robust Incremental Learning under Ambiguous Supervision","abstract":"Traditional Incremental Learning (IL) targets to handle sequential fully-supervised learning problems where novel classes emerge from time to time. However, due to inherent annotation uncertainty and ambiguity, collecting high-quality annotated data in a dynamic learning system can be extremely expensive. To mitigate this problem, we propose a novel weakly-supervised learning paradigm called Incremental Partial Label Learning (IPLL), where the sequentially arrived data relate to a set of candidate labels rather than the ground truth. Technically, we develop the Prototype-Guided Disambiguation and Replay Algorithm (PGDR) which leverages the class prototypes as a proxy to mitigate two intertwined challenges in IPLL, i.e., label ambiguity and catastrophic forgetting. To handle the former, PGDR encapsulates a momentum-based pseudo-labeling algorithm along with prototype-guided initialization, resulting in a balanced perception of classes. To alleviate forgetting, we develop a memory replay technique that collects well-disambiguated samples while maintaining representativeness and diversity. By jointly distilling knowledge from curated memory data, our framework exhibits a great disambiguation ability for samples of new tasks and achieves less forgetting of knowledge. Extensive experiments demonstrate that PGDR achieves superior","authors":["Rui Wang","Mingxuan Xia","Chang Yao","Lei Feng","Junbo Zhao","Gang Chen","Haobo Wang"],"url":"https://arxiv.org/abs/2501.13584"}
{"created":"2025-05-08","title":"Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression","abstract":"We analyze two variants of Local Gradient Descent applied to distributed logistic regression with heterogeneous, separable data and show convergence at the rate $O(1/KR)$ for $K$ local steps and sufficiently large $R$ communication rounds. In contrast, all existing convergence guarantees for Local GD applied to any problem are at least $\\Omega(1/R)$, meaning they fail to show the benefit of local updates. The key to our improved guarantee is showing progress on the logistic regression objective when using a large stepsize $\\eta \\gg 1/K$, whereas prior analysis depends on $\\eta \\leq 1/K$.","authors":["Michael Crawshaw","Blake Woodworth","Mingrui Liu"],"url":"https://arxiv.org/abs/2501.13790"}
{"created":"2025-05-08","title":"Emergence of Abstract Rules in Recurrent Spiking Neural Networks","abstract":"The emergence of abstract rules from exemplars is a cornerstone of genuine intelligence in both biological and artificial systems. However, the internal organizational principles underlying different abstract rules remain poorly understood. We propose a hierarchically modulated recurrent spiking neural network (HM-RSNN), inspired by astrocyte signaling. The model globally configures and locally fine-tunes intrinsic neuronal properties via a two-stage neuromodulatory mechanism. This design enhances neuronal adaptability and diversity, thus enabling fine-grained analysis of internal organizational principles. We evaluate abstract rule emergence across four cognitive task sets. To probe internal organization, we examine network-level connectivity via structural modularity analysis and neuron-level functional biases via bin-wise lesion studies based on intrinsic properties. Our experiments show that HM-RSNN successfully achieves abstract rule emergence, with rule-contingent organizational principles evident at both network and neuron levels. These findings highlight the critical role of dynamic internal organization in supporting flexible cognition.","authors":["Yingchao Yu","Yaochu Jin","Kuangrong Hao","Yuchen Xiao","Yuping Yan","Hengjie Yu"],"url":"https://arxiv.org/abs/2501.14539"}
{"created":"2025-05-08","title":"Piecewise Ruled Approximation for Freeform Mesh Surfaces","abstract":"A ruled surface is a shape swept out by moving a line in 3D space. Due to their simple geometric forms, ruled surfaces have applications in various domains such as architecture and engineering. In the past, various approaches have been proposed to approximate a target shape using developable surfaces, which are special ruled surfaces with zero Gaussian curvature. However, methods for shape approximation using general ruled surfaces remain limited and often require the target shape to be either represented as parametric surfaces or have non-positive Gaussian curvature. In this paper, we propose a method to compute a piecewise ruled surface that approximates an arbitrary freeform mesh surface. We first use a group-sparsity formulation to optimize the given mesh shape into an approximately piecewise ruled form, in conjunction with a tangent vector field that indicates the ruling directions. Afterward, we utilize the optimization result to extract seams that separate smooth families of rulings, and use the seams to construct the initial rulings. Finally, we further optimize the positions and orientations of the rulings to improve the alignment with the input target shape. We apply our method to a variety of freeform shapes with different topologies and complexity, demonstrating its effectiveness in approximating arbitrary shapes.","authors":["Yiling Pan","Zhixin Xu","Bin Wang","Bailin Deng"],"url":"https://arxiv.org/abs/2501.15258"}
{"created":"2025-05-08","title":"Does Functional Package Management Enable Reproducible Builds at Scale? Yes","abstract":"Reproducible Builds (R-B) guarantee that rebuilding a software package from source leads to bitwise identical artifacts. R-B is a promising approach to increase the integrity of the software supply chain, when installing open source software built by third parties. Unfortunately, despite success stories like high build reproducibility levels in Debian packages, uncertainty remains among field experts on the scalability of R-B to very large package repositories. In this work, we perform the first large-scale study of bitwise reproducibility, in the context of the Nix functional package manager, rebuilding 709 816 packages from historical snapshots of the nixpkgs repository, the largest cross-ecosystem open source software distribution, sampled in the period 2017-2023. We obtain very high bitwise reproducibility rates, between 69 and 91% with an upward trend, and even higher rebuildability rates, over 99%. We investigate unreproducibility causes, showing that about 15% of failures are due to embedded build dates. We release a novel dataset with all build statuses, logs, as well as full ''diffoscopes'': recursive diffs of where unreproducible build artifacts differ.","authors":["Julien Malka (ACES","INFRES)","Stefano Zacchiroli (ACES","INFRES)","Th\\'eo Zimmermann (ACES","INFRES)"],"url":"https://arxiv.org/abs/2501.15919"}
{"created":"2025-05-08","title":"AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models","abstract":"The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level, from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption) and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results show AffectGPT's robust performance across various MER tasks. We have released both the code and the dataset to advance research and development in emotion understanding: https://github.com/zeroQiaoba/AffectGPT.","authors":["Zheng Lian","Haoyu Chen","Lan Chen","Haiyang Sun","Licai Sun","Yong Ren","Zebang Cheng","Bin Liu","Rui Liu","Xiaojiang Peng","Jiangyan Yi","Jianhua Tao"],"url":"https://arxiv.org/abs/2501.16566"}
{"created":"2025-05-08","title":"TikTok's recommendations skewed towards Republican content during the 2024 U.S. presidential race","abstract":"TikTok is a major force among social media platforms with over a billion monthly active users worldwide and 170 million in the United States. The platform's status as a key news source, particularly among younger demographics, raises concerns about its potential influence on politics in the U.S. and globally. Despite these concerns, there is scant research investigating TikTok's recommendation algorithm for political biases. We fill this gap by conducting 323 independent algorithmic audit experiments testing partisan content recommendations in the lead-up to the 2024 U.S. presidential elections. Specifically, we create hundreds of \"sock puppet\" TikTok accounts in Texas, New York, and Georgia, seeding them with varying partisan content and collecting algorithmic content recommendations for each of them. Collectively, these accounts viewed ~394,000 videos from April 30th to November 11th, 2024, which we label for political and partisan content. Our analysis reveals significant asymmetries in content distribution: Republican-seeded accounts received ~11.8% more party-aligned recommendations compared to their Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to ~7.5% more opposite-party recommendations on average. These asymmetries exist across all three states and persist when accounting for video- and channel-level engagement metrics such as likes, views, shares, comments, and followers, and are driven primarily by negative partisanship content. Our findings provide insights into the inner workings of TikTok's recommendation algorithm during a critical election period, raising fundamental questions about platform neutrality.","authors":["Hazem Ibrahim","HyunSeok Daniel Jang","Nouar Aldahoul","Aaron R. Kaufman","Talal Rahwan","Yasir Zaki"],"url":"https://arxiv.org/abs/2501.17831"}
{"created":"2025-05-08","title":"Vision-Language Model Selection and Reuse for Downstream Adaptation","abstract":"Pre-trained Vision-Language Models (VLMs) are becoming increasingly popular across various visual tasks, and several open-sourced VLM variants have been released. However, selecting the best-performing pre-trained VLM for a specific downstream task is challenging since no single VLM can achieve promising performance on all downstream tasks, and evaluating all available VLMs is impossible due to time and data limitations. To address this problem, this paper proposes a novel paradigm to select and reuse VLM for downstream tasks, called Model Label Learning (MLL). The proposal contains three key modules: \\emph{model labeling}, which assigns labels to each VLM to describe their specialty and utility; \\emph{model selection}, which matches the requirements of the target task with model labels; and \\emph{model reuse}, which applies selected VLMs to the target task in an ensemble manner. The proposal is highly computationally efficient and growable since the model labeling process is completed target task independent and the ability could grow with the number of candidate VLMs. We also introduce a new benchmark for evaluating VLM selection methods, including 49 VLMs and 17 target task datasets. Experimental results clearly demonstrate the effectiveness of the proposed method for selecting and reusing VLMs.","authors":["Hao-Zhe Tan","Zhi Zhou","Yu-Feng Li","Lan-Zhe Guo"],"url":"https://arxiv.org/abs/2501.18271"}
{"created":"2025-05-08","title":"CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to Sustainability Data Extraction","abstract":"Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.","authors":["Peter J. Bentley","Soo Ling Lim","Fuyuki Ishikawa"],"url":"https://arxiv.org/abs/2501.18504"}
{"created":"2025-05-08","title":"EcoWeedNet: A Lightweight and Automated Weed Detection Method for Sustainable Next-Generation Agricultural Consumer Electronics","abstract":"Sustainable agriculture plays a crucial role in ensuring world food security for consumers. A critical challenge faced by sustainable precision agriculture is weed growth, as weeds compete for essential resources with crops, such as water, soil nutrients, and sunlight, which notably affect crop yields. The adoption of automated computer vision technologies and ground agricultural consumer electronic vehicles in precision agriculture offers sustainable, low-carbon solutions. However, prior works suffer from issues such as low accuracy and precision, as well as high computational expense. This work proposes EcoWeedNet, a novel model that enhances weed detection performance without introducing significant computational complexity, aligning with the goals of low-carbon agricultural practices. The effectiveness of the proposed model is demonstrated through comprehensive experiments on the CottonWeedDet12 benchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves performance comparable to that of large models (mAP@0.5 = 95.2%), yet with significantly fewer parameters (approximately 4.21% of the parameters of YOLOv4), lower computational complexity and better computational efficiency 6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's deployability on low-power consumer hardware, lower energy consumption, and hence reduced carbon footprint, thereby emphasizing the application prospects of EcoWeedNet in next-generation sustainable agriculture. These findings provide the way forward for increased application of environmentally-friendly agricultural consumer technologies.","authors":["Omar H. Khater","Abdul Jabbar Siddiqui","M. Shamim Hossain","Aiman El-Maleh"],"url":"https://arxiv.org/abs/2502.00205"}
{"created":"2025-05-08","title":"Estimating LLM Uncertainty with Logits","abstract":"Over the past few years, Large Language Models (LLMs) have developed rapidly and are widely applied in various domains. However, LLMs face the issue of hallucinations, generating responses that may be unreliable when the models lack relevant knowledge. To be aware of potential hallucinations, uncertainty estimation methods have been introduced, and most of them have confirmed that reliability lies in critical tokens. However, probability-based methods perform poorly in identifying token reliability, limiting their practical utility. In this paper, we reveal that the probability-based method fails to estimate token reliability due to the loss of evidence strength information which is accumulated in the training stage. Therefore, we present Logits-induced token uncertainty (LogTokU), a framework for estimating decoupled token uncertainty in LLMs, enabling real-time uncertainty estimation without requiring multiple sampling processes. We employ evidence modeling to implement LogTokU and use the estimated uncertainty to guide downstream tasks. The experimental results demonstrate that LogTokU has significant effectiveness and promise.","authors":["Huan Ma","Jingdong Chen","Joey Tianyi Zhou","Guangyu Wang","Changqing Zhang"],"url":"https://arxiv.org/abs/2502.00290"}
{"created":"2025-05-08","title":"Federated Generalised Variational Inference: A Robust Probabilistic Federated Learning Framework","abstract":"We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.","authors":["Terje Mildner","Oliver Hamelijnck","Paris Giampouras","Theodoros Damoulas"],"url":"https://arxiv.org/abs/2502.00846"}
{"created":"2025-05-08","title":"InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM with Optical Circuit Switching Transceivers","abstract":"Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism (TP) and Expert Parallelism (EP). However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such as TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches, but the fault explosion radius remains large at the cube level (e.g., 64 TPUs).","authors":["Chenchen Shou","Guyue Liu","Hao Nie","Huaiyu Meng","Yu Zhou","Yimin Jiang","Wenqing Lv","Yelong Xu","Yuanwei Lu","Zhang Chen","Yanbo Yu","Yichen Shen","Yibo Zhu","Daxin Jiang"],"url":"https://arxiv.org/abs/2502.03885"}
{"created":"2025-05-08","title":"Online Controller Synthesis for Robot Collision Avoidance: A Case Study","abstract":"The inherent uncertainty of dynamic environments poses significant challenges for modeling robot behavior, particularly in tasks such as collision avoidance. This paper presents an online controller synthesis framework tailored for robots equipped with deep learning-based perception components, with a focus on addressing distribution shifts. Our approach integrates periodic monitoring and repair mechanisms for the deep neural network perception component, followed by uncertainty reassessment. These uncertainty evaluations are injected into a parametric discrete-time markov chain, enabling the synthesis of robust controllers via probabilistic model checking. To ensure high system availability during the repair process, we propose a dual-component configuration that seamlessly transitions between operational states. Through a case study on robot collision avoidance, we demonstrate the efficacy of our method, showcasing substantial performance improvements over baseline approaches. This work provides a comprehensive and scalable solution for enhancing the safety and reliability of autonomous systems operating in uncertain environments.","authors":["Yuheng Fan","Wang Lin"],"url":"https://arxiv.org/abs/2502.05667"}
{"created":"2025-05-08","title":"Approximation Algorithms for Optimal Hopsets","abstract":"For a given graph $G$, a \"hopset\" $H$ with hopbound $\\beta$ and stretch $\\alpha$ is a set of edges such that between every pair of vertices $u$ and $v$, there is a path with at most $\\beta$ hops in $G \\cup H$ that approximates the distance between $u$ and $v$ up to a multiplicative stretch of $\\alpha$. Hopsets have found a wide range of applications for distance-based problems in various computational models since the 90s. More recently, there has been significant interest in understanding these fundamental objects from an existential and structural perspective. But all of this work takes a worst-case (or existential) point of view: How many edges do we need to add to satisfy a given hopbound and stretch requirement for any input graph?","authors":["Michael Dinitz","Ama Koranteng","Yasamin Nazari"],"url":"https://arxiv.org/abs/2502.06522"}
{"created":"2025-05-08","title":"Semantics-Aware Updates from Remote Energy Harvesting Devices to Interconnected LEO Satellites","abstract":"Providing timely and informative data in integrated terrestrial and non-terrestrial networks is critical as data volume grows while the resources available on devices remain limited. To address this, we adopt a semantics-aware approach to optimize the Version Age of Information (VAoI) in a status update system in which a remote Energy Harvesting (EH) Internet of Things (IoT) device samples data and transmits it to a network of interconnected Low Earth Orbit (LEO) satellites for dissemination and utilization. The optimal update policy is derived through stochastic modeling and optimization of the VAoI across the network. The results indicate that this policy reduces the frequency of updates by skipping stale or irrelevant data, significantly improving energy efficiency.","authors":["Erfan Delfani","Nikolaos Pappas"],"url":"https://arxiv.org/abs/2502.07069"}
{"created":"2025-05-08","title":"MonoForce: Learnable Image-conditioned Physics Engine","abstract":"We propose a novel model for the prediction of robot trajectories on rough offroad terrain from the onboard camera images. This model enforces the laws of classical mechanics through a physics-aware neural symbolic layer while preserving the ability to learn from large-scale data as it is end-to-end differentiable. The proposed hybrid model integrates a black-box component that predicts robot-terrain interaction forces with a neural-symbolic layer. This layer includes a differentiable physics engine that computes the robot's trajectory by querying these forces at the points of contact with the terrain. As the proposed architecture comprises substantial geometrical and physics priors, the resulting model can also be seen as a learnable physics engine conditioned on real images that delivers $10^4$ trajectories per second. We argue and empirically demonstrate that this architecture reduces the sim-to-real gap and mitigates out-of-distribution sensitivity. The differentiability, in conjunction with the rapid simulation speed, makes the model well-suited for various applications including model predictive control, trajectory shooting, supervised and reinforcement learning or SLAM. The codes and data are publicly available.","authors":["Ruslan Agishev","Karel Zimmermann"],"url":"https://arxiv.org/abs/2502.10156"}
{"created":"2025-05-08","title":"DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage Object Detection","abstract":"Recent 2D CNN-based domain adaptation approaches struggle with long-range dependencies due to limited receptive fields, making it difficult to adapt to target domains with significant spatial distribution changes. While transformer-based domain adaptation methods better capture distant relationships through self-attention mechanisms that facilitate more effective cross-domain feature alignment, their quadratic computational complexity makes practical deployment challenging for object detection tasks across diverse domains. Inspired by the global modeling and linear computation complexity of the Mamba architecture, we present the first domain-adaptive Mamba-based one-stage object detection model, termed DA-Mamba. Specifically, we combine Mamba's efficient state-space modeling with attention mechanisms to address domain-specific spatial and channel-wise variations. Our design leverages domain-adaptive spatial and channel-wise scanning within the Mamba block to extract highly transferable representations for efficient sequential processing, while cross-attention modules generate long-range, mixed-domain spatial features to enable robust soft alignment across domains. Besides, motivated by the observation that hybrid architectures introduce feature noise in domain adaptation tasks, we propose an entropy-based knowledge distillation framework with margin ReLU, which adaptively refines multi-level representations by suppressing irrelevant activations and aligning uncertainty across source and target domains. Finally, to prevent overfitting caused by the mixed-up features generated through cross-attention mechanisms, we propose entropy-driven gating attention with random perturbations that simultaneously refine target features and enhance model generalization.","authors":["A. Enes Doruk","Hasan F. Ates"],"url":"https://arxiv.org/abs/2502.11178"}
{"created":"2025-05-08","title":"Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer Gate","abstract":"Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.","authors":["Zhiyuan Fang","Zicong Hong","Yuegui Huang","Yufeng Lyu","Wuhui Chen","Yue Yu","Fan Yu","Zibin Zheng"],"url":"https://arxiv.org/abs/2502.12224"}
{"created":"2025-05-08","title":"Generative Detail Enhancement for Physically Based Materials","abstract":"We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to enhance the visual fidelity of materials with detail that is often tedious to author, by adding signs of wear, aging, weathering, etc. As these appearance details are often rooted in real-world processes, we leverage a generative image model trained on a large dataset of natural images with corresponding visuals in context. Starting with a given geometry, UV mapping, and basic appearance, we render multiple views of the object. We use these views, together with an appearance-defining text prompt, to condition a diffusion model. The details it generates are then backpropagated from the enhanced images to the material parameters via inverse differentiable rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the initial noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce geometric consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic of the material model used, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. This project is available at https://generative-detail.github.io.","authors":["Saeed Hadadan","Benedikt Bitterli","Tizian Zeltner","Jan Nov\\'ak","Fabrice Rousselle","Jacob Munkberg","Jon Hasselgren","Bartlomiej Wronski","Matthias Zwicker"],"url":"https://arxiv.org/abs/2502.13994"}
{"created":"2025-05-08","title":"$\\texttt{SPIN}$: distilling $\\texttt{Skill-RRT}$ for long-horizon prehensile and non-prehensile manipulation","abstract":"Current robots struggle with long-horizon manipulation tasks requiring sequences of prehensile and non-prehensile skills, contact-rich interactions, and long-term reasoning. We present $\\texttt{SPIN}$ ($\\textbf{S}$kill $\\textbf{P}$lanning to $\\textbf{IN}$ference), a framework that distills a computationally intensive planning algorithm into a policy via imitation learning. We propose $\\texttt{Skill-RRT}$, an extension of RRT that incorporates skill applicability checks and intermediate object pose sampling for solving such long-horizon problems. To chain independently trained skills, we introduce $\\textit{connectors}$, goal-conditioned policies trained to minimize object disturbance during transitions. High-quality demonstrations are generated with $\\texttt{Skill-RRT}$ and distilled through noise-based replay in order to reduce online computation time. The resulting policy, trained entirely in simulation, transfers zero-shot to the real world and achieves over 80% success across three challenging long-horizon manipulation tasks and outperforms state-of-the-art hierarchical RL and planning methods.","authors":["Haewon Jung","Donguk Lee","Haecheol Park","JunHyeop Kim","Beomjoon Kim"],"url":"https://arxiv.org/abs/2502.18015"}
{"created":"2025-05-08","title":"IID-Based QPP-RNG: A Random Number Generator Utilizing Random Permutation Sorting Driven by System Jitter","abstract":"We propose a groundbreaking random number generator that achieves truly uniform, independent, and identically distributed (IID) randomness by integrating Quantum Permutation Pads (QPP) with system jitter--derived entropy, herein called IID-based QPP-RNG. Unlike conventional RNGs that use raw timing variations, our design uses system jitter solely to generate ephemeral QPP pads and derives 8-bit outputs directly from permutation counts, eliminating the need for post-processing. This approach leverages the factorial complexity of permutation sorting to systematically accumulate entropy from dynamic hardware interactions, ensuring non-deterministic outputs even from fixed seeds. Notably, IID-based QPP-RNG achieves a min-entropy of 7.85-7.95 bits per byte from IID min-entropy estimate, surpassing ID Quantique's QRNG (7.157042 bits per byte), which marks a breakthrough in randomness quality. Our implementation employs a dynamic seed evolution protocol that continuously refreshes the internal state with unpredictable system jitter, effectively decoupling the QPP sequence from the initial seed. Cross-platform validation on macOS (x86 and ARM) and Windows (x86) confirms uniformly distributed outputs, while evaluations compliant with NIST SP 800-90B show a Shannon entropy of 7.9999 bits per byte. Overall, IID-based QPP-RNG represents a significant advancement in random number generation, offering a scalable, system-based, software-only, post-quantum secure solution for a wide range of cryptographic applications.","authors":["Randy Kuang","Dafu Lou"],"url":"https://arxiv.org/abs/2502.18609"}
{"created":"2025-05-08","title":"SYN-LUNGS: Towards Simulating Lung Nodules with Anatomy-Informed Digital Twins for AI Training","abstract":"AI models for lung cancer screening are limited by data scarcity, impacting generalizability and clinical applicability. Generative models address this issue but are constrained by training data variability. We introduce SYN-LUNGS, a framework for generating high-quality 3D CT images with detailed annotations. SYN-LUNGS integrates XCAT3 phantoms for digital twin generation, X-Lesions for nodule simulation (varying size, location, and appearance), and DukeSim for CT image formation with vendor and parameter variability. The dataset includes 3,072 nodule images from 1,044 simulated CT scans, with 512 lesions and 174 digital twins. Models trained on clinical + simulated data outperform clinical only models, achieving 10% improvement in detection, 2-9% in segmentation and classification, and enhanced synthesis. By incorporating anatomy-informed simulations, SYN-LUNGS provides a scalable approach for AI model development, particularly in rare disease representation and improving model reliability.","authors":["Fakrul Islam Tushar","Lavsen Dahal","Cindy McCabe","Fong Chi Ho","Paul Segars","Ehsan Abadi","Kyle J. Lafata","Ehsan Samei","Joseph Y. Lo"],"url":"https://arxiv.org/abs/2502.21187"}
{"created":"2025-05-08","title":"Execution Welfare Across Solver-based DEXes","abstract":"Decentralized exchanges (DEXes) have evolved dramatically since the introduction of Automated Market Makers (AMMs). In recent years, solver-based protocols have emerged as an alternative venue aiming to introduce competition for routing, access to offchain liquidity, and thereby improve end-user execution. Currently, these solver auctions are hosted on opaque backends, and the extent of price improvement they provide to end users remains unclear.","authors":["Yuki Yuminaga","Dex Chen","Danning Sui"],"url":"https://arxiv.org/abs/2503.00738"}
{"created":"2025-05-08","title":"Scalable Connectivity for Ising Machines: Dense to Sparse","abstract":"In recent years, hardware implementations of Ising machines have emerged as a viable alternative to quantum computing for solving hard optimization problems among other applications. Unlike quantum hardware, dense connectivity can be achieved in classical systems. However, we show that dense connectivity leads to severe frequency slowdowns and interconnect congestion scaling unfavorably with system sizes. As a scalable solution, we propose a systematic sparsification method for dense graphs by introducing copy nodes to limit the number of neighbors per graph node. In addition to solving interconnect congestion, this approach enables constant frequency scaling where all spins in a network can be updated in constant time. On the other hand, sparsification introduces new difficulties, such as constraint-breaking between copied spins and increased convergence times to solve optimization problems, especially if exact ground states are sought. Relaxing the exact solution requirements, we find that the overheads in convergence times are milder. We demonstrate these ideas by designing probabilistic bit Ising machines using ASAP7 (a predictive 7nm FinFET technology model) process design kits as well as Field Programmable Gate Array (FPGA)-based implementations. Finally, we show how formulating problems in naturally sparse networks (e.g., by invertible logic) sidesteps challenges introduced by sparsification methods. Our results are applicable to a broad family of Ising machines using different hardware implementations.","authors":["M Mahmudul Hasan Sajeeb","Navid Anjum Aadit","Shuvro Chowdhury","Tong Wu","Cesely Smith","Dhruv Chinmay","Atharva Raut","Kerem Y. Camsari","Corentin Delacour","Tathagata Srimani"],"url":"https://arxiv.org/abs/2503.01177"}
{"created":"2025-05-08","title":"Liger: Linearizing Large Language Models to Gated Recurrent Structures","abstract":"Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\\% of the Transformer-based LLM at 0.02\\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.","authors":["Disen Lan","Weigao Sun","Jiaxi Hu","Jusen Du","Yu Cheng"],"url":"https://arxiv.org/abs/2503.01496"}
{"created":"2025-05-08","title":"Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation","abstract":"In Australia, post-contact language varieties, including creoles and local varieties of international languages, emerged as a result of forced contact between Indigenous communities and English speakers. These contact varieties are widely used, yet are poorly supported by language technologies. This gap presents barriers to participation in civil and economic society for Indigenous communities using these varieties, and reproduces minoritisation of contemporary Indigenous sociolinguistic identities. This paper concerns three questions regarding this context. First, can speech technologies support speakers of Australian Aboriginal English, a local indigenised variety of English? Second, what risks are inherent in such a project? Third, what technology development practices are appropriate for this context, and how can researchers integrate meaningful community participation in order to mitigate risks? We argue that opportunities do exist -- as well as risks -- and demonstrate this through a case study exploring design practices in a real-world project aiming to improve speech technologies for Australian Aboriginal English. We discuss how we integrated culturally appropriate and participatory processes throughout the project. We call for increased support for languages used by Indigenous communities, including contact varieties, which provide practical economic and socio-cultural benefits, provided that participatory and culturally safe practices are enacted.","authors":["Ben Hutchinson","Celeste Rodr\\'iguez Louro","Glenys Collard","Ned Cooper"],"url":"https://arxiv.org/abs/2503.03186"}
{"created":"2025-05-08","title":"Generating Building-Level Heat Demand Time Series by Combining Occupancy Simulations and Thermal Modeling","abstract":"Despite various efforts, decarbonizing the heating sector remains a significant challenge. To tackle it by smart planning, the availability of highly resolved heating demand data is key. Several existing models provide heating demand only for specific applications. Typically, they either offer time series for a larger area or annual demand data on a building level, but not both simultaneously. Additionally, the diversity in heating demand across different buildings is often not considered. To address these limitations, this paper presents a novel method for generating temporally resolved heat demand time series at the building level using publicly available data. The approach integrates a thermal building model with stochastic occupancy simulations that account for variability in user behavior. As a result, the tool serves as a cost-effective resource for cross-sectoral energy system planning and policy development, particularly with a focus on the heating sector. The obtained data can be used to assess the impact of renovation and retrofitting strategies, or to analyze district heating expansion. To illustrate the potential applications of this approach, we conducted a case study in Puertollano (Spain), where we prepared a dataset of heating demand with hourly resolution for each of 9,298 residential buildings. This data was then used to compare two different pathways for the thermal renovation of these buildings. By relying on publicly available data, this method can be adapted and applied to various European regions, offering broad usability in energy system optimization and analysis of decarbonization strategies.","authors":["Simon Malacek","Jos\\'e Portela","Yannick Marcus Werner","Sonja Wogrin"],"url":"https://arxiv.org/abs/2503.05427"}
{"created":"2025-05-08","title":"Evaluation Framework for Sensor Configuration Impact on Deep Learning-Based Perception","abstract":"Current research on automotive perception systems predominantly focusses on either improving the performance of sensor technology or enhancing the perception functions in isolation. High-level perception functions are increasingly based on deep learning (DL) models due to their improved performance and generalisability compared to traditional algorithms. Despite the vital need to evaluate the performance of DL-based perception functions under real-world conditions using onboard sensor inputs, there is a lack of frameworks to implement such systematic evaluations. This paper presents a versatile framework to evaluate the impact of perception sensor modalities and parameter settings on DL-based perception functions. Using a simulation environment, the framework facilitates sensor modality selection and parameter tuning under different operational design domain conditions. Its effectiveness is demonstrated through a case study involving a state-of-the-art surround trajectory prediction model, highlighting performance differences across the sensor modalities radar and camera. Different settings for the parameter, horizontal field of view (HFOV) were evaluated to identify the optimal configuration. The results indicate that a radar sensor with a narrow HFOV is the most suitable configuration for the evaluated perception algorithm. The proposed framework offers a holistic approach to the design of the perception sensor suite, significantly contributing to the development of robust perception systems for automated driving systems.","authors":["A Gamage","V Donzella"],"url":"https://arxiv.org/abs/2503.05939"}
{"created":"2025-05-08","title":"Generative AI in Transportation Planning: A Survey","abstract":"The integration of generative artificial intelligence (GenAI) into transportation planning has the potential to revolutionize tasks such as demand forecasting, infrastructure design, policy evaluation, and traffic simulation. However, there is a critical need for a systematic framework to guide the adoption of GenAI in this interdisciplinary domain. In this survey, we, a multidisciplinary team of researchers spanning computer science and transportation engineering, present the first comprehensive framework for leveraging GenAI in transportation planning. Specifically, we introduce a new taxonomy that categorizes existing applications and methodologies into two perspectives: transportation planning tasks and computational techniques. From the transportation planning perspective, we examine the role of GenAI in automating descriptive, predictive, generative, simulation, and explainable tasks to enhance mobility systems. From the computational perspective, we detail advancements in data preparation, domain-specific fine-tuning, and inference strategies, such as retrieval-augmented generation and zero-shot learning tailored to transportation applications. Additionally, we address critical challenges, including data scarcity, explainability, bias mitigation, and the development of domain-specific evaluation frameworks that align with transportation goals like sustainability, equity, and system efficiency. This survey aims to bridge the gap between traditional transportation planning methodologies and modern AI techniques, fostering collaboration and innovation. By addressing these challenges and opportunities, we seek to inspire future research that ensures ethical, equitable, and impactful use of generative AI in transportation planning.","authors":["Longchao Da","Tiejin Chen","Zhuoheng Li","Shreyas Bachiraju","Huaiyuan Yao","Li Li","Yushun Dong","Xiyang Hu","Zhengzhong Tu","Dongjie Wang","Yue Zhao","Ben Zhou","Ram Pendyala","Benjamin Stabler","Yezhou Yang","Xuesong Zhou","Hua Wei"],"url":"https://arxiv.org/abs/2503.07158"}
{"created":"2025-05-08","title":"High-Dimensional Interlingual Representations of Large Language Models","abstract":"Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.","authors":["Bryan Wilie","Samuel Cahyawijaya","Junxian He","Pascale Fung"],"url":"https://arxiv.org/abs/2503.11280"}
{"created":"2025-05-08","title":"Set-based and Dynamical Feedback-augmented Hands-off Control","abstract":"A novel set-theoretical approach to hands-off control is proposed, which focuses on spatial arguments for command limitation, rather than temporal ones. By employing dynamical feedback alongside invariant set-based constraints, actuation is employed only to drive the system's state inside a \"hands-off region\" of its state-space, where the plant may freely evolve in open-loop configuration. A computationally-efficient procedure with strong theoretical guarantees is devised, and its effectiveness is showcased via an intuitive practical example.","authors":["Andrei Speril\\u{a}","Sorin Olaru","St\\'ephane Drobot"],"url":"https://arxiv.org/abs/2503.11795"}
{"created":"2025-05-08","title":"A Unified Approach to Enforce Non-Negativity Constraint in Neural Network Approximation for Optimal Voltage Regulation (preprint)","abstract":"Power system voltage regulation is crucial to maintain power quality while integrating intermittent renewable resources in distribution grids. However, the system model on the grid edge is often unknown, making it difficult to model physical equations for optimal control. Therefore, previous work proposes structured data-driven methods like input convex neural networks (ICNN) for \"optimal\" control without relying on a physical model. While ICNNs offer theoretical guarantees based on restrictive assumptions of non-negative neural network parameters, can one improve the approximation power with an extra step on negative duplication of inputs? We show that such added mirroring step fails to improve accuracy, as a linear combination of the original input and duplicated input is equivalent to a linear operation of ICNN's input without duplication. While this design can not improve performance, we propose a unified approach to embed the non-negativity constraint as a regularized optimization of the neural network, contrary to the existing methods, which added a loosely integrated second step for post-processing on parameter negation. Our integration directly ties back-propagation to simultaneously minimizing the approximation error while enforcing the convexity constraints. Numerical experiments validate the issues of the mirroring method and show that our integrated objective can avoid problems such as unstable training and non-convergence existing in other methods for optimal control.","authors":["Jiaqi Wu","Jingyi Yuan","Yang Weng","Guangwen Wang"],"url":"https://arxiv.org/abs/2503.12380"}
{"created":"2025-05-08","title":"Stable Volume Dissipation for High-Order Finite-Difference and Spectral-Element Methods with the Summation-by-Parts Property","abstract":"The construction of stable, conservative, and accurate volume dissipation is extended to discretizations that possess a generalized summation-by-parts (SBP) property within a tensor-product framework. The dissipation operators can be applied to any finite-difference or spectral-element scheme that uses the SBP framework, including high-order entropy-stable schemes. Additionally, we clarify the incorporation of a variable coefficient within the operator structure and analyze the impact of a boundary correction matrix on operator structure and accuracy. Following the theoretical development and construction of novel dissipation operators, we relate the presented volume dissipation to the use of upwind SBP operators. When applied to spectral-element methods, the presented approach yields unique dissipation operators that can also be derived through alternative approaches involving orthogonal polynomials. Numerical examples featuring the linear convection, Burgers, and Euler equations verify the properties of the constructed dissipation operators and assess their performance compared to existing upwind SBP schemes, including linear stability behaviour. When applied to entropy-stable schemes, the presented approach results in accurate and robust methods that can solve a broader range of problems where comparable existing methods fail.","authors":["Alex Bercik","David A. Craig Penner","David W. Zingg"],"url":"https://arxiv.org/abs/2503.12670"}
{"created":"2025-05-08","title":"Conversation-Based Multimodal Abuse Detection Through Text and Graph Embeddings","abstract":"Abusive behavior is common on online social networks, and forces the hosts of such platforms to find new solutions to address this problem. Various methods have been proposed to automate this task in the past decade. Most of them rely on the exchanged content, but ignore the structure and dynamics of the conversation, which could provide some relevant information. In this article, we propose to use representation learning methods to automatically produce embeddings of this textual content and of the conversational graphs depicting message exchanges. While the latter could be enhanced by including additional information on top of the raw conversational structure, no method currently exists to learn whole-graph representations using simultaneously edge directions, weights, signs, and vertex attributes. We propose two such methods to fill this gap in the literature. We experiment with 5 textual and 13 graph embedding methods, and apply them to a dataset of online messages annotated for abuse detection. Our best results achieve an F -measure of 81.02 using text alone and 80.61 using graphs alone. We also combine both modalities of information (text and graphs) through three fusion strategies, and show that this strongly improves abuse detection performance, increasing the F -measure to 87.06. Finally, we identify which specific engineered features are captured by the embedding methods under consideration. These features have clear interpretations and help explain what information the representation learning methods deem discriminative.","authors":["No\\'e Cecillon (LIA)","Vincent Labatut (LIA)","Richard Dufour (LS2N - \\'equipe TALN)"],"url":"https://arxiv.org/abs/2503.12994"}
{"created":"2025-05-08","title":"A Space-Efficient Algorithm for Longest Common Almost Increasing Subsequence of Two Sequences","abstract":"Let $A$ and $B$ be two number sequences of length $n$ and $m$, respectively, where $m\\le n$. Given a positive number $\\delta$, a common almost increasing sequence $s_1\\ldots s_k$ is a common subsequence for both $A$ and $B$ such that for all $2\\le i\\le k$, $s_i+\\delta > \\max_{1\\le j < i} s_j$. The LCaIS problem seeks to find the longest common almost increasing subsequence (LCaIS) of $A$ and $B$. An LCaIS can be computed in $O(nm\\ell)$ time and $O(nm)$ space [Ta, Shieh, Lu (TCS 2021)], where $\\ell$ is the length of the LCaIS of $A$ and $B$. In this paper we first give an $O(nm\\ell)$-time and $O(n+m\\ell)$-space algorithm to find LCaIS, which improves the space complexity. We then design an $O((n+m)\\log n +\\mathcal{M}\\log \\mathcal{M} + \\mathcal{C}\\ell)$-time and $O(\\mathcal{M}(\\ell+\\log \\mathcal{M}))$-space algorithm, which is faster when the number of matching pairs $\\mathcal{M}$ and the number of compatible matching pairs $\\mathcal{C}$ are in $o(nm/\\log m)$.","authors":["Md Tanzeem Rahat","Md. Manzurul Hasan","Debajyoti Mondal"],"url":"https://arxiv.org/abs/2503.15442"}
{"created":"2025-05-08","title":"Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters","abstract":"Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\\sim$44% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in modern Transformer architectures.","authors":["Roberto Garcia","Jerry Liu","Daniel Sorvisto","Sabri Eyuboglu"],"url":"https://arxiv.org/abs/2503.18216"}
{"created":"2025-05-08","title":"LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty","abstract":"We present LoTUS, a novel Machine Unlearning (MU) method that eliminates the influence of training samples from pre-trained models, avoiding retraining from scratch. LoTUS smooths the prediction probabilities of the model up to an information-theoretic bound, mitigating its over-confidence stemming from data memorization. We evaluate LoTUS on Transformer and ResNet18 models against eight baselines across five public datasets. Beyond established MU benchmarks, we evaluate unlearning on ImageNet1k, a large-scale dataset, where retraining is impractical, simulating real-world conditions. Moreover, we introduce the novel Retrain-Free Jensen-Shannon Divergence (RF-JSD) metric to enable evaluation under real-world conditions. The experimental results show that LoTUS outperforms state-of-the-art methods in terms of both efficiency and effectiveness. Code: https://github.com/cspartalis/LoTUS.","authors":["Christoforos N. Spartalis","Theodoros Semertzidis","Efstratios Gavves","Petros Daras"],"url":"https://arxiv.org/abs/2503.18314"}
{"created":"2025-05-08","title":"SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild","abstract":"DeepSeek-R1 has shown that long chain-of-thought (CoT) reasoning can naturally emerge through a simple reinforcement learning (RL) framework with rule-based rewards, where the training may directly start from the base models-a paradigm referred to as zero RL training. Most recent efforts to reproduce zero RL training have primarily focused on the Qwen2.5 model series, which may not be representative as we find the base models already exhibit strong instruction-following and self-reflection abilities. In this work, we investigate zero RL training across 10 diverse base models, spanning different families and sizes including LLama3-8B, Mistral-7B/24B, DeepSeek-Math-7B, Qwen2.5-math-7B, and all Qwen2.5 models from 0.5B to 32B. Leveraging several key design strategies-such as adjusting format reward and controlling query difficulty-we achieve substantial improvements in both reasoning accuracy and response length across most settings. However, by carefully monitoring the training dynamics, we observe that different base models exhibit distinct patterns during training. For instance, the increased response length does not always correlate with the emergence of certain cognitive behaviors such as verification (i.e., the \"aha moment\"). Notably, we observe the \"aha moment\" for the first time in small models not from the Qwen family. We share the key designs that enable successful zero RL training, along with our findings and practices. To facilitate further research, we open-source the code, models, and analysis tools.","authors":["Weihao Zeng","Yuzhen Huang","Qian Liu","Wei Liu","Keqing He","Zejun Ma","Junxian He"],"url":"https://arxiv.org/abs/2503.18892"}
{"created":"2025-05-08","title":"OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching","abstract":"Hallucinations are often inevitable in downstream tasks using large language models (LLMs). To tackle the substantial challenge of addressing hallucinations for LLM-based ontology matching (OM) systems, we introduce a new benchmark dataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching) datasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of different LLMs performing OM tasks. These OM-specific hallucinations are carefully classified into two primary categories and six sub-categories. We showcase the usefulness of the dataset in constructing the LLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.","authors":["Zhangcheng Qiang"],"url":"https://arxiv.org/abs/2503.21813"}
{"created":"2025-05-08","title":"Bimanual Regrasp Planning and Control for Active Reduction of Object Pose Uncertainty","abstract":"Precisely grasping an object is a challenging task due to pose uncertainties. Conventional methods have used cameras and fixtures to reduce object uncertainty. They are effective but require intensive preparation, such as designing jigs based on the object geometry and calibrating cameras with high-precision tools fabricated using lasers. In this study, we propose a method to reduce the uncertainty of the position and orientation of a grasped object without using a fixture or a camera. Our method is based on the concept that the flat finger pads of a parallel gripper can reduce uncertainty along its opening/closing direction through flat surface contact. Three orthogonal grasps by parallel grippers with flat finger pads collectively constrain an object's position and orientation to a unique state. Guided by the concepts, we develop a regrasp planning and admittance control approach that sequentially finds and leverages three orthogonal grasps of two robotic arms to actively reduce uncertainties in the object pose. We evaluated the proposed method on different initial object uncertainties and verified that it had good repeatability. The deviation levels of the experimental trials were on the same order of magnitude as those of an optical tracking system, demonstrating strong relative inference performance.","authors":["Ryuta Nagahama","Weiwei Wan","Zhengtao Hu","Kensuke Harada"],"url":"https://arxiv.org/abs/2503.22240"}
{"created":"2025-05-08","title":"TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting","abstract":"We present TranSplat, a 3D scene rendering algorithm that enables realistic cross-scene object transfer (from a source to a target scene) based on the Gaussian Splatting framework. Our approach addresses two critical challenges: (1) precise 3D object extraction from the source scene, and (2) faithful relighting of the transferred object in the target scene without explicit material property estimation. TranSplat fits a splatting model to the source scene, using 2D object masks to drive fine-grained 3D segmentation. Following user-guided insertion of the object into the target scene, along with automatic refinement of position and orientation, TranSplat derives per-Gaussian radiance transfer functions via spherical harmonic analysis to adapt the object's appearance to match the target scene's lighting environment. This relighting strategy does not require explicitly estimating physical scene properties such as BRDFs. Evaluated on several synthetic and real-world scenes and objects, TranSplat yields excellent 3D object extractions and relighting performance compared to recent baseline methods and visually convincing cross-scene object transfers. We conclude by discussing the limitations of the approach.","authors":["Tony Yu","Yanlin Jin","Ashok Veeraraghavan","Akshat Dave","Guha Balakrishnan"],"url":"https://arxiv.org/abs/2503.22676"}
{"created":"2025-05-08","title":"Disturbance-adaptive Model Predictive Control for Bounded Average Constraint Violations","abstract":"This paper considers stochastic linear time-invariant systems subject to constraints on the average number of state-constraint violations over time without knowing the disturbance distribution. We present a novel disturbance-adaptive model predictive control (DAD-MPC) framework, which adjusts the disturbance model based on measured constraint violations. Using a robust invariance method, DAD-MPC ensures recursive feasibility and guarantees asymptotic or robust bounds on average constraint violations. Additionally, the bounds hold even with an inaccurate disturbance model, which allows for data-driven disturbance quantification methods to be used, such as conformal prediction. Simulation results demonstrate that the proposed approach outperforms state-of-the-art methods while satisfying average violation constraints.","authors":["Jicheng Shi","Colin N. Jones"],"url":"https://arxiv.org/abs/2503.24169"}
{"created":"2025-05-08","title":"MultiSensor-Home: A Wide-area Multi-modal Multi-view Dataset for Action Recognition and Transformer-based Sensor Fusion","abstract":"Multi-modal multi-view action recognition is a rapidly growing field in computer vision, offering significant potential for applications in surveillance. However, current datasets often fail to address real-world challenges such as wide-area distributed settings, asynchronous data streams, and the lack of frame-level annotations. Furthermore, existing methods face difficulties in effectively modeling inter-view relationships and enhancing spatial feature learning. In this paper, we introduce the MultiSensor-Home dataset, a novel benchmark designed for comprehensive action recognition in home environments, and also propose the Multi-modal Multi-view Transformer-based Sensor Fusion (MultiTSF) method. The proposed MultiSensor-Home dataset features untrimmed videos captured by distributed sensors, providing high-resolution RGB and audio data along with detailed multi-view frame-level action labels. The proposed MultiTSF method leverages a Transformer-based fusion mechanism to dynamically model inter-view relationships. Furthermore, the proposed method integrates a human detection module to enhance spatial feature learning, guiding the model to prioritize frames with human activity to enhance action the recognition accuracy. Experiments on the proposed MultiSensor-Home and the existing MM-Office datasets demonstrate the superiority of MultiTSF over the state-of-the-art methods. Quantitative and qualitative results highlight the effectiveness of the proposed method in advancing real-world multi-modal multi-view action recognition. The source code is available at https://github.com/thanhhff/MultiTSF.","authors":["Trung Thanh Nguyen","Yasutomo Kawanishi","Vijay John","Takahiro Komamizu","Ichiro Ide"],"url":"https://arxiv.org/abs/2504.02287"}
{"created":"2025-05-08","title":"FAST: Federated Active Learning with Foundation Models for Communication-efficient Sampling and Training","abstract":"Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget.","authors":["Haoyuan Li","Mathias Funk","Jindong Wang","Aaqib Saeed"],"url":"https://arxiv.org/abs/2504.03783"}
{"created":"2025-05-08","title":"Investigating Popularity Bias Amplification in Recommender Systems Employed in the Entertainment Domain","abstract":"Recommender systems have become an integral part of our daily online experience by analyzing past user behavior to suggest relevant content in entertainment domains such as music, movies, and books. Today, they are among the most widely used applications of AI and machine learning. Consequently, regulations and guidelines for trustworthy AI, such as the European AI Act, which addresses issues like bias and fairness, are highly relevant to the design, development, and evaluation of recommender systems. One particularly important type of bias in this context is popularity bias, which results in the unfair underrepresentation of less popular content in recommendation lists. This work summarizes our research on investigating the amplification of popularity bias in recommender systems within the entertainment sector. Analyzing datasets from three entertainment domains, music, movies, and anime, we demonstrate that an item's recommendation frequency is positively correlated with its popularity. As a result, user groups with little interest in popular content receive less accurate recommendations compared to those who prefer widely popular items. Furthermore, this work contributes to a better understanding of the connection between recommendation accuracy, calibration quality of algorithms, and popularity bias amplification.","authors":["Dominik Kowald"],"url":"https://arxiv.org/abs/2504.04752"}
{"created":"2025-05-08","title":"Design of a compact low loss 2-way millimetre wave power divider for future communication","abstract":"In this paper, a rectangular-shaped power divider has been presented operating at 27.9 GHz. The power divider has achieved acceptable results for important parameters such as S11, S12, S21, and S22. The substrate employed for the power divider is Roger 3003 which has a thickness of 1.6 mm. This power divider provides a reflection coefficient of -12.2 dB and an insertion loss of 3.1 dB at 28 GHz. This ka-band T-junction power divider covers 68% of the bandwidth. Dimensions of the ka-band T-junction power divider are 50x80 mm. Due to its dimensions and bandwidth this power divider is more suitable for millimetre wave applications like RADAR, beamforming, and 5G applications.","authors":["Muhammad Asfar Saeed","Augustine O. Nwajana","Muneeb Ahmad"],"url":"https://arxiv.org/abs/2504.04901"}
{"created":"2025-05-08","title":"MSA-UNet3+: Multi-Scale Attention UNet3+ with New Supervised Prototypical Contrastive Loss for Coronary DSA Image Segmentation","abstract":"Accurate segmentation of coronary Digital Subtraction Angiography images is essential to diagnose and treat coronary artery diseases. Despite advances in deep learning, challenges such as high intra-class variance and class imbalance limit precise vessel delineation. Most existing approaches for coronary DSA segmentation cannot address these issues. Also, existing segmentation network's encoders do not directly generate semantic embeddings, which could enable the decoder to reconstruct segmentation masks effectively from these well-defined features. We propose a Supervised Prototypical Contrastive Loss that fuses supervised and prototypical contrastive learning to enhance coronary DSA image segmentation. The supervised contrastive loss enforces semantic embeddings in the encoder, improving feature differentiation. The prototypical contrastive loss allows the model to focus on the foreground class while alleviating the high intra-class variance and class imbalance problems by concentrating only on the hard-to-classify background samples. We implement the proposed SPCL loss within an MSA-UNet3+: a Multi-Scale Attention-Enhanced UNet3+ architecture. The architecture integrates key components: a Multi-Scale Attention Encoder and a Multi-Scale Dilated Bottleneck designed to enhance multi-scale feature extraction and a Contextual Attention Fusion Module built to keep fine-grained details while improving contextual understanding. Experiments on a private coronary DSA dataset show that MSA-UNet3+ outperforms state-of-the-art methods, achieving the highest Dice coefficient and F1-score and significantly reducing ASD and ACD. The developed framework provides clinicians with precise vessel segmentation, enabling accurate identification of coronary stenosis and supporting informed diagnostic and therapeutic decisions. The code will be released at https://github.com/rayanmerghani/MSA-UNet3plus.","authors":["Rayan Merghani Ahmed","Adnan Iltaf","Mohamed Elmanna","Gang Zhao","Hongliang Li","Yue Du","Bin Li","Shoujun Zhou"],"url":"https://arxiv.org/abs/2504.05184"}
{"created":"2025-05-08","title":"Probability Density Geodesics in Image Diffusion Latent Space","abstract":"Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.","authors":["Qingtao Yu","Jaskirat Singh","Zhaoyuan Yang","Peter Henry Tu","Jing Zhang","Hongdong Li","Richard Hartley","Dylan Campbell"],"url":"https://arxiv.org/abs/2504.06675"}
{"created":"2025-05-08","title":"Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and Counterfactuals","abstract":"Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that significantly increases the risk of cardiovascular diseases and type 2 diabetes. Despite its global prevalence, accurate prediction of MetS remains challenging due to issues such as class imbalance, data scarcity, and methodological inconsistencies in existing studies. In this paper, we address these challenges by systematically evaluating and optimizing machine learning (ML) models for MetS prediction, leveraging advanced data balancing techniques and counterfactual analysis. Multiple ML models, including XGBoost, Random Forest, TabNet, etc., were trained and compared under various data balancing techniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN. Additionally, we introduce MetaBoost, a novel hybrid framework that integrates SMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted averaging and iterative weight tuning to enhance the model's performance (achieving up to a 1.87% accuracy improvement over individual balancing techniques). A comprehensive counterfactual analysis is conducted to quantify the feature-level changes required to shift individuals from high-risk to low-risk categories. The results indicate that blood glucose (50.3%) and triglycerides (46.7%) were the most frequently modified features, highlighting their clinical significance in MetS risk reduction. Additionally, probabilistic analysis shows elevated blood glucose (85.5% likelihood) and triglycerides (74.9% posterior probability) as the strongest predictors. This study not only advances the methodological rigor of MetS prediction but also provides actionable insights for clinicians and researchers, highlighting the potential of ML in mitigating the public health burden of metabolic syndrome.","authors":["Sanyam Paresh Shah","Abdullah Mamun","Shovito Barua Soumma","Hassan Ghasemzadeh"],"url":"https://arxiv.org/abs/2504.06987"}
{"created":"2025-05-08","title":"PNE-SGAN: Probabilistic NDT-Enhanced Semantic Graph Attention Network for LiDAR Loop Closure Detection","abstract":"LiDAR loop closure detection (LCD) is crucial for consistent Simultaneous Localization and Mapping (SLAM) but faces challenges in robustness and accuracy. Existing methods, including semantic graph approaches, often suffer from coarse geometric representations and lack temporal robustness against noise, dynamics, and viewpoint changes. We introduce PNE-SGAN, a Probabilistic NDT-Enhanced Semantic Graph Attention Network, to overcome these limitations. PNE-SGAN enhances semantic graphs by using Normal Distributions Transform (NDT) covariance matrices as rich, discriminative geometric node features, processed via a Graph Attention Network (GAT). Crucially, it integrates graph similarity scores into a probabilistic temporal filtering framework (modeled as an HMM/Bayes filter), incorporating uncertain odometry for motion modeling and utilizing forward-backward smoothing to effectively handle ambiguities. Evaluations on challenging KITTI sequences (00 and 08) demonstrate state-of-the-art performance, achieving Average Precision of 96.2\\% and 95.1\\%, respectively. PNE-SGAN significantly outperforms existing methods, particularly in difficult bidirectional loop scenarios where others falter. By synergizing detailed NDT geometry with principled probabilistic temporal reasoning, PNE-SGAN offers a highly accurate and robust solution for LiDAR LCD, enhancing SLAM reliability in complex, large-scale environments.","authors":["Xiong Li","Shulei Liu","Xingning Chen","Yisong Wu","Dong Zhu"],"url":"https://arxiv.org/abs/2504.08280"}
{"created":"2025-05-08","title":"Breaking the Lens of the Telescope: Online Relevance Estimation over Large Retrieval Sets","abstract":"Advanced relevance models, such as those that use large language models (LLMs), provide highly accurate relevance estimations. However, their computational costs make them infeasible for processing large document corpora. To address this, retrieval systems often employ a telescoping approach, where computationally efficient but less precise lexical and semantic retrievers filter potential candidates for further ranking. However, this approach heavily depends on the quality of early-stage retrieval, which can potentially exclude relevant documents early in the process. In this work, we propose a novel paradigm for re-ranking called online relevance estimation that continuously updates relevance estimates for a query throughout the ranking process. Instead of re-ranking a fixed set of top-k documents in a single step, online relevance estimation iteratively re-scores smaller subsets of the most promising documents while adjusting relevance scores for the remaining pool based on the estimations from the final model using an online bandit-based algorithm. This dynamic process mitigates the recall limitations of telescoping systems by re-prioritizing documents initially deemed less relevant by earlier stages -- including those completely excluded by earlier-stage retrievers. We validate our approach on TREC benchmarks under two scenarios: hybrid retrieval and adaptive retrieval. Experimental results demonstrate that our method is sample-efficient and significantly improves recall, highlighting the effectiveness of our online relevance estimation framework for modern search systems.","authors":["Mandeep Rathee","V Venktesh","Sean MacAvaney","Avishek Anand"],"url":"https://arxiv.org/abs/2504.09353"}
{"created":"2025-05-08","title":"Mitigating Many-Shot Jailbreaking","abstract":"Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a \"fake\" assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the \"fake\" assistant. In this work, we probe the effectiveness of different fine-tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training.","authors":["Christopher M. Ackerman","Nina Panickssery"],"url":"https://arxiv.org/abs/2504.09604"}
{"created":"2025-05-08","title":"Graph-based Path Planning with Dynamic Obstacle Avoidance for Autonomous Parking","abstract":"Safe and efficient path planning in parking scenarios presents a significant challenge due to the presence of cluttered environments filled with static and dynamic obstacles. To address this, we propose a novel and computationally efficient planning strategy that seamlessly integrates the predictions of dynamic obstacles into the planning process, ensuring the generation of collision-free paths. Our approach builds upon the conventional Hybrid A star algorithm by introducing a time-indexed variant that explicitly accounts for the predictions of dynamic obstacles during node exploration in the graph, thus enabling dynamic obstacle avoidance. We integrate the time-indexed Hybrid A star algorithm within an online planning framework to compute local paths at each planning step, guided by an adaptively chosen intermediate goal. The proposed method is validated in diverse parking scenarios, including perpendicular, angled, and parallel parking. Through simulations, we showcase our approach's potential in greatly improving the efficiency and safety when compared to the state of the art spline-based planning method for parking situations.","authors":["Farhad Nawaz","Minjun Sung","Darshan Gadginmath","Jovin D'sa","Sangjae Bae","David Isele","Nadia Figueroa","Nikolai Matni","Faizan M. Tariq"],"url":"https://arxiv.org/abs/2504.12616"}
{"created":"2025-05-08","title":"A Numerical Gradient Inversion Attack in Variational Quantum Neural-Networks","abstract":"The loss landscape of Variational Quantum Neural Networks (VQNNs) is characterized by local minima that grow exponentially with increasing qubits. Because of this, it is more challenging to recover information from model gradients during training compared to classical Neural Networks (NNs). In this paper we present a numerical scheme that successfully reconstructs input training, real-world, practical data from trainable VQNNs' gradients. Our scheme is based on gradient inversion that works by combining gradients estimation with the finite difference method and adaptive low-pass filtering. The scheme is further optimized with Kalman filter to obtain efficient convergence. Our experiments show that our algorithm can invert even batch-trained data, given the VQNN model is sufficiently over-parameterized.","authors":["Georgios Papadopoulos","Shaltiel Eloul","Yash Satsangi","Jamie Heredge","Niraj Kumar","Chun-Fu Chen","Marco Pistoia"],"url":"https://arxiv.org/abs/2504.12806"}
{"created":"2025-05-08","title":"Publicly Verifiable Secret Sharing: Generic Constructions and Lattice-Based Instantiations in the Standard Model","abstract":"Publicly verifiable secret sharing (PVSS) allows a dealer to share a secret among a set of shareholders so that the secret can be reconstructed later from any set of qualified participants. In addition, any public verifier should be able to check the correctness of the sharing and reconstruction process. PVSS has been demonstrated to yield various applications, such as e-voting, distributed key generation, decentralized random number generation protocols, and multi-party computation. Although many concrete PVSS protocols have been proposed, their security is either proven in the random oracle model or relies on quantum-vulnerable assumptions such as factoring or discrete logarithm. In this work, we put forward a generic construction for PVSS that can be instantiated in the standard model under the Learning With Errors (LWE) assumption. Our instantiation provides the first post-quantum PVSS in the standard model, with a reasonable level of asymptotic efficiency.","authors":["Pham Nhat Minh","Khoa Nguyen","Willy Susilo","Khuong Nguyen-An"],"url":"https://arxiv.org/abs/2504.14381"}
{"created":"2025-05-08","title":"Adaptive continuity-preserving simplification of street networks","abstract":"Street network data is widely used to study human-based activities and urban structure. Often, these data are geared towards transportation applications, which require highly granular, directed graphs that capture the complex relationships of potential traffic patterns. While this level of network detail is critical for certain fine-grained mobility models, it represents a hindrance for studies concerned with the morphology of the street network. For the latter case, street network simplification - the process of converting a highly granular input network into its most simple morphological form - is a necessary, but highly tedious preprocessing step, especially when conducted manually. In this manuscript, we develop and present a novel adaptive algorithm for simplifying street networks that is both fully automated and able to mimic results obtained through a manual simplification routine. The algorithm - available in the neatnet Python package - outperforms current state-of-the-art procedures when comparing those methods to manually, human-simplified data, while preserving network continuity.","authors":["Martin Fleischmann","Anastassia Vybornova","James D. Gaboardi","Anna Br\\'azdov\\'a","Daniela Dan\\v{c}ejov\\'a"],"url":"https://arxiv.org/abs/2504.16198"}
{"created":"2025-05-08","title":"EEmo-Bench: A Benchmark for Multi-modal Large Language Models on Image Evoked Emotion Assessment","abstract":"The furnishing of multi-modal large language models (MLLMs) has led to the emergence of numerous benchmark studies, particularly those evaluating their perception and understanding capabilities. Among these, understanding image-evoked emotions aims to enhance MLLMs' empathy, with significant applications such as human-machine interaction and advertising recommendations. However, current evaluations of this MLLM capability remain coarse-grained, and a systematic and comprehensive assessment is still lacking. To this end, we introduce EEmo-Bench, a novel benchmark dedicated to the analysis of the evoked emotions in images across diverse content categories. Our core contributions include: 1) Regarding the diversity of the evoked emotions, we adopt an emotion ranking strategy and employ the Valence-Arousal-Dominance (VAD) as emotional attributes for emotional assessment. In line with this methodology, 1,960 images are collected and manually annotated. 2) We design four tasks to evaluate MLLMs' ability to capture the evoked emotions by single images and their associated attributes: Perception, Ranking, Description, and Assessment. Additionally, image-pairwise analysis is introduced to investigate the model's proficiency in performing joint and comparative analysis. In total, we collect 6,773 question-answer pairs and perform a thorough assessment on 19 commonly-used MLLMs. The results indicate that while some proprietary and large-scale open-source MLLMs achieve promising overall performance, the analytical capabilities in certain evaluation dimensions remain suboptimal. Our EEmo-Bench paves the path for further research aimed at enhancing the comprehensive perceiving and understanding capabilities of MLLMs concerning image-evoked emotions, which is crucial for machine-centric emotion perception and understanding.","authors":["Lancheng Gao","Ziheng Jia","Yunhao Zeng","Wei Sun","Yiming Zhang","Wei Zhou","Guangtao Zhai","Xiongkuo Min"],"url":"https://arxiv.org/abs/2504.16405"}
{"created":"2025-05-08","title":"Towards User-Centred Design of AI-Assisted Decision-Making in Law Enforcement","abstract":"Artificial Intelligence (AI) has become an important part of our everyday lives, yet user requirements for designing AI-assisted systems in law enforcement remain unclear. To address this gap, we conducted qualitative research on decision-making within a law enforcement agency. Our study aimed to identify limitations of existing practices, explore user requirements and understand the responsibilities that humans expect to undertake in these systems.","authors":["Vesna Nowack","Dalal Alrajeh","Carolina Gutierrez Mu\\~noz","Katie Thomas","William Hobson","Patrick Benjamin","Catherine Hamilton-Giachritsis","Tim Grant","Juliane A. Kloess","Jessica Woodhams"],"url":"https://arxiv.org/abs/2504.17393"}
{"created":"2025-05-08","title":"Towards a HIPAA Compliant Agentic AI System in Healthcare","abstract":"Agentic AI systems powered by Large Language Models (LLMs) as their foundational reasoning engine, are transforming clinical workflows such as medical report generation and clinical summarization by autonomously analyzing sensitive healthcare data and executing decisions with minimal human oversight. However, their adoption demands strict compliance with regulatory frameworks such as Health Insurance Portability and Accountability Act (HIPAA), particularly when handling Protected Health Information (PHI). This work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that enforces regulatory compliance through dynamic, context-aware policy enforcement. Our framework integrates three core mechanisms: (1) Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid PHI sanitization pipeline combining regex patterns and BERT-based model to minimize leakage, and (3) immutable audit trails for compliance verification.","authors":["Subash Neupane","Sudip Mittal","Shahram Rahimi"],"url":"https://arxiv.org/abs/2504.17669"}
{"created":"2025-05-08","title":"Maximum Coverage in Turnstile Streams with Applications to Fingerprinting Measures","abstract":"In the maximum coverage problem we are given $d$ subsets from a universe $[n]$, and the goal is to output $k$ subsets such that their union covers the largest possible number of distinct items. We present the first algorithm for maximum coverage in the turnstile streaming model, where updates which insert or delete an item from a subset come one-by-one. Notably our algorithm only uses $poly\\log n$ update time. We also present turnstile streaming algorithms for targeted and general fingerprinting for risk management where the goal is to determine which features pose the greatest re-identification risk in a dataset. As part of our work, we give a result of independent interest: an algorithm to estimate the complement of the $p^{\\text{th}}$ frequency moment of a vector for $p \\geq 2$. Empirical evaluation confirms the practicality of our fingerprinting algorithms demonstrating a speedup of up to $210$x over prior work.","authors":["Alina Ene","Alessandro Epasto","Vahab Mirrokni","Hoai-An Nguyen","Huy L. Nguyen","David P. Woodruff","Peilin Zhong"],"url":"https://arxiv.org/abs/2504.18394"}
{"created":"2025-05-08","title":"Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency","abstract":"Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements.The project can be found at https://alibaba-damo-academy.github.io/VCBench/.","authors":["Zhikai Wang","Jiashuo Sun","Wenqi Zhang","Zhiqiang Hu","Xin Li","Fan Wang","Deli Zhao"],"url":"https://arxiv.org/abs/2504.18589"}
{"created":"2025-05-08","title":"Test It Before You Trust It: Applying Software Testing for Trustworthy In-context Learning","abstract":"In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.","authors":["Teeradaj Racharak","Chaiyong Ragkhitwetsagul","Chommakorn Sontesadisai","Thanwadee Sunetnanta"],"url":"https://arxiv.org/abs/2504.18827"}
{"created":"2025-05-08","title":"Sentiment and Social Signals in the Climate Crisis: A Survey on Analyzing Social Media Responses to Extreme Weather Events","abstract":"Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.","authors":["Pouya Shaeri","Yasaman Mohammadpour","Alimohammad Beigi","Ariane Middel","Huan Liu"],"url":"https://arxiv.org/abs/2504.18837"}
{"created":"2025-05-08","title":"A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification","abstract":"With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%.","authors":["Junichiro Niimi"],"url":"https://arxiv.org/abs/2504.18884"}
{"created":"2025-05-08","title":"LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition","abstract":"In autonomous driving, place recognition is critical for global localization in GPS-denied environments. LiDAR and radar-based place recognition methods have garnered increasing attention, as LiDAR provides precise ranging, whereas radar excels in adverse weather resilience. However, effectively leveraging LiDAR-radar fusion for place recognition remains challenging. The noisy and sparse nature of radar data limits its potential to further improve recognition accuracy. In addition, heterogeneous radar configurations complicate the development of unified cross-modality fusion frameworks. In this paper, we propose LRFusionPR, which improves recognition accuracy and robustness by fusing LiDAR with either single-chip or scanning radar. Technically, a dual-branch network is proposed to fuse different modalities within the unified polar coordinate bird's eye view (BEV) representation. In the fusion branch, cross-attention is utilized to perform cross-modality feature interactions. The knowledge from the fusion branch is simultaneously transferred to the distillation branch, which takes radar as its only input to further improve the robustness. Ultimately, the descriptors from both branches are concatenated, producing the multimodal global descriptor for place retrieval. Extensive evaluations on multiple datasets demonstrate that our LRFusionPR achieves accurate place recognition, while maintaining robustness under varying weather conditions. Our open-source code will be released at https://github.com/QiZS-BIT/LRFusionPR.","authors":["Zhangshuo Qi","Luqi Cheng","Zijie Zhou","Guangming Xiong"],"url":"https://arxiv.org/abs/2504.19186"}
{"created":"2025-05-08","title":"Stochastic Subspace via Probabilistic Principal Component Analysis for Characterizing Model Error","abstract":"This paper proposes a probabilistic model of subspaces based on the probabilistic principal component analysis (PCA). Given a sample of vectors in the embedding space -- commonly known as a snapshot matrix -- this method uses quantities derived from the probabilistic PCA to construct distributions of the sample matrix, as well as the principal subspaces. It is applicable to projection-based reduced-order modeling methods, such as proper orthogonal decomposition and related model reduction methods. The stochastic subspace thus constructed can be used, for example, to characterize model-form uncertainty in computational mechanics. The proposed method has multiple desirable properties: (1) it is naturally justified by the probabilistic PCA and has analytic forms for the induced random matrix models; (2) it satisfies linear constraints, such as boundary conditions of all kinds, by default; (3) it has only one hyperparameter, which significantly simplifies training; and (4) its algorithm is very easy to implement. We demonstrate the performance of the proposed method via several numerical examples in computational mechanics and structural dynamics.","authors":["Akash Yadav","Ruda Zhang"],"url":"https://arxiv.org/abs/2504.19963"}
{"created":"2025-05-08","title":"A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts","abstract":"We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works.","authors":["Robert Becker","Aron Culotta"],"url":"https://arxiv.org/abs/2504.20065"}
{"created":"2025-05-08","title":"Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception","abstract":"Large Vision-Language Models (LVLMs) have achieved impressive results across various cross-modal tasks. However, hallucinations, i.e., the models generating counterfactual responses, remain a challenge. Though recent studies have attempted to alleviate object perception hallucinations, they focus on the models' response generation, and overlooking the task question itself. This paper discusses the vulnerability of LVLMs in solving counterfactual presupposition questions (CPQs), where the models are prone to accept the presuppositions of counterfactual objects and produce severe hallucinatory responses. To this end, we introduce \"Antidote\", a unified, synthetic data-driven post-training framework for mitigating both types of hallucination above. It leverages synthetic data to incorporate factual priors into questions to achieve self-correction, and decouple the mitigation process into a preference optimization problem. Furthermore, we construct \"CP-Bench\", a novel benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce factual responses. Applied to the LLaVA series, Antidote can simultaneously enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR by 30-50%, all without relying on external supervision from stronger LVLMs or human feedback and introducing noticeable catastrophic forgetting issues.","authors":["Yuanchen Wu","Lu Zhang","Hang Yao","Junlong Du","Ke Yan","Shouhong Ding","Yunsheng Wu","Xiaoqiang Li"],"url":"https://arxiv.org/abs/2504.20468"}
{"created":"2025-05-08","title":"Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers","abstract":"Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.","authors":["Roman Abramov","Felix Steinbauer","Gjergji Kasneci"],"url":"https://arxiv.org/abs/2504.20752"}
{"created":"2025-05-08","title":"ACE: A Security Architecture for LLM-Integrated App Systems","abstract":"LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.","authors":["Evan Li","Tushin Mallick","Evan Rose","William Robertson","Alina Oprea","Cristina Nita-Rotaru"],"url":"https://arxiv.org/abs/2504.20984"}
{"created":"2025-05-08","title":"CodeBC: A More Secure Large Language Model for Smart Contract Code Generation in Blockchain","abstract":"Large language models (LLMs) excel at generating code from natural language instructions, yet they often lack an understanding of security vulnerabilities. This limitation makes it difficult for LLMs to avoid security risks in generated code, particularly in high-security programming tasks such as smart contract development for blockchain. Researchers have attempted to enhance the vulnerability awareness of these models by training them to differentiate between vulnerable and fixed code snippets. However, this approach relies heavily on manually labeled vulnerability data, which is only available for popular languages like Python and C++. For low-resource languages like Solidity, used in smart contracts, large-scale annotated datasets are scarce and difficult to obtain. To address this challenge, we introduce CodeBC, a code generation model specifically designed for generating secure smart contracts in blockchain. CodeBC employs a three-stage fine-tuning approach based on CodeLlama, distinguishing itself from previous methods by not relying on pairwise vulnerability location annotations. Instead, it leverages vulnerability and security tags to teach the model the differences between vulnerable and secure code. During the inference phase, the model leverages security tags to generate secure and robust code. Experimental results demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU, and compilation pass rates, while significantly reducing vulnerability rates. These findings validate the effectiveness and cost-efficiency of our three-stage fine-tuning strategy, making CodeBC a promising solution for generating secure smart contract code.","authors":["Lingxiang Wang","Hainan Zhang","Qinnan Zhang","Ziwei Wang","Hongwei Zheng","Jin Dong","Zhiming Zheng"],"url":"https://arxiv.org/abs/2504.21043"}
{"created":"2025-05-08","title":"MemeBLIP2: A novel lightweight multimodal system to detect harmful memes","abstract":"Memes often merge visuals with brief text to share humor or opinions, yet some memes contain harmful messages such as hate speech. In this paper, we introduces MemeBLIP2, a light weight multimodal system that detects harmful memes by combining image and text features effectively. We build on previous studies by adding modules that align image and text representations into a shared space and fuse them for better classification. Using BLIP-2 as the core vision-language model, our system is evaluated on the PrideMM datasets. The results show that MemeBLIP2 can capture subtle cues in both modalities, even in cases with ironic or culturally specific content, thereby improving the detection of harmful material.","authors":["Jiaqi Liu","Ran Tong","Aowei Shen","Shuzheng Li","Changlin Yang","Lisha Xu"],"url":"https://arxiv.org/abs/2504.21226"}
{"created":"2025-05-08","title":"Implementation and Security Analysis of Cryptocurrencies Based on Ethereum","abstract":"Blockchain technology has set off a wave of decentralization in the world since its birth. The trust system constructed by blockchain technology based on cryptography algorithm and computing power provides a practical and powerful solution to solve the trust problem in human society. In order to make more convenient use of the characteristics of blockchain and build applications on it, smart contracts appear. By defining some trigger automatic execution contracts, the application space of blockchain is expanded and the foundation for the rapid development of blockchain is laid. This is blockchain 2.0. However, the programmability of smart contracts also introduces vulnerabilities. In order to cope with the insufficient security guarantee of high-value application networks running on blockchain 2.0 and smart contracts, this article will be represented by Ethereum to introduce the technical details of understanding blockchain 2.0 and the operation principle of contract virtual machines, and explain how cryptocurrencies based on blockchain 2.0 are constructed and operated. The common security problems and solutions are also discussed. Based on relevant research and on-chain practice, this paper provides a complete and comprehensive perspective to understanding cryptocurrency technology based on blockchain 2.0 and provides a reference for building more secure cryptocurrency contracts.","authors":["Pengfei Gao","Dechao Kong","Xiaoqi Li"],"url":"https://arxiv.org/abs/2504.21367"}
{"created":"2025-05-08","title":"A Comprehensive Survey of Electrical Stimulation Haptic Feedback in Human-Computer Interaction","abstract":"Haptic perception and feedback play a pivotal role in interactive experiences, forming an essential component of human-computer interaction (HCI). In recent years, the field of haptic interaction has witnessed significant advancements, particularly in the area of electrical haptic feedback, driving innovation across various domains. To gain a comprehensive understanding of the current state of research and the latest developments in electrical haptic interaction, this study systematically reviews the literature in this area. Our investigation covers key aspects including haptic devices, haptic perception mechanisms, the comparison and integration of electrical haptic feedback with other feedback modalities, and their diverse applications. Specifically, we conduct a systematic analysis of 110 research papers to explore the forefront of electrical haptic feedback, providing insights into its latest trends, challenges, and future directions.","authors":["Simin Yang","Xian Wang","Yang Li","Lik-Hang Lee","Tristan Camille Braud","Pan Hui"],"url":"https://arxiv.org/abs/2504.21477"}
{"created":"2025-05-08","title":"Measurement-Based Line-Impedance Estimation in the Absence of Phasor Measurement Units","abstract":"This paper proposes and compares experimentally several methods to estimate the series resistance and reactance (i.e., the transversal components of the $\\pi$-model of a line) of low-voltage lines in distribution grids. It first shows that if phasor measurements are available and the grid nodal voltages and power injections are known, the problem can be formulated and solved as a conventional load flow with properly adjusted unknowns. To solve this problem, we propose an analytical derivation of the Jacobian matrix. If only RMS values are available, such as from smart meters, integrating information from multiple intervals becomes necessary, ultimately opening to least-squares estimations, widely adopted in the literature. In this context, applying the proposed Jacobian contributes to accelerating the problem resolution of existing algorithms. The methods are compared in terms of estimation performance and convergence by using measurements from an experimental distribution grid interfacing real-world components and with realistic size implemented at the Gridlab at HES-SO Valais.","authors":["Plouton Grammatikos","Ali Mohamed Ali","Fabrizio Sossan"],"url":"https://arxiv.org/abs/2504.21606"}
{"created":"2025-05-08","title":"Fast Sign Retrieval via Sub-band Convolution: An Elementary Extension of Binary Classification","abstract":"To efficiently compress the sign information of images, we address a sign retrieval problem for the block-wise discrete cosine transformation (DCT): reconstruction of the signs of DCT coefficients from their amplitudes. To this end, we propose a fast sign retrieval method on the basis of binary classification machine learning. We first introduce 3D representations of the amplitudes and signs, where we pack amplitudes/signs belonging to the same frequency band into a 2D slice, referred to as the sub-band block. We then retrieve the signs from the 3D amplitudes via binary classification, where each sign is regarded as a binary label. We implement a binary classification algorithm using convolutional neural networks, which are advantageous for efficiently extracting features in the 3D amplitudes. Experimental results demonstrate that our method achieves accurate sign retrieval with an overwhelmingly low computation cost.","authors":["Fuma Ito","Chihiro Tsutake","Keita Takahashi","Toshiaki Fujii"],"url":"https://arxiv.org/abs/2504.21632"}
{"created":"2025-05-08","title":"VividListener: Expressive and Controllable Listener Dynamics Modeling for Multi-Modal Responsive Interaction","abstract":"Generating responsive listener head dynamics with nuanced emotions and expressive reactions is crucial for practical dialogue modeling in various virtual avatar animations. Previous studies mainly focus on the direct short-term production of listener behavior. They overlook the fine-grained control over motion variations and emotional intensity, especially in long-sequence modeling. Moreover, the lack of long-term and large-scale paired speaker-listener corpora including head dynamics and fine-grained multi-modality annotations (e.g., text-based expression descriptions, emotional intensity) also limits the application of dialogue modeling.Therefore, we first newly collect a large-scale multi-turn dataset of 3D dyadic conversation containing more than 1.4M valid frames for multi-modal responsive interaction, dubbed ListenerX. Additionally, we propose VividListener, a novel framework enabling fine-grained, expressive and controllable listener dynamics modeling. This framework leverages multi-modal conditions as guiding principles for fostering coherent interactions between speakers and listeners.Specifically, we design the Responsive Interaction Module (RIM) to adaptively represent the multi-modal interactive embeddings. RIM ensures the listener dynamics achieve fine-grained semantic coordination with textual descriptions and adjustments, while preserving expressive reaction with speaker behavior. Meanwhile, we design the Emotional Intensity Tags (EIT) for emotion intensity editing with multi-modal information integration, applying to both text descriptions and listener motion amplitude.Extensive experiments conducted on our newly collected ListenerX dataset demonstrate that VividListener achieves state-of-the-art performance, realizing expressive and controllable listener dynamics.","authors":["Shiying Li","Xingqun Qi","Bingkun Yang","Chen Weile","Zezhao Tian","Muyi Sun","Qifeng Liu","Man Zhang","Zhenan Sun"],"url":"https://arxiv.org/abs/2504.21718"}
{"created":"2025-05-08","title":"Deterministic Scheduling over Wi-Fi 6 using Target Wake Time: An Experimental Approach","abstract":"Wi-Fi networks traditionally use Distributed Coordination Function (DCF) that employs CSMA/CA along with the binary backoff mechanism for channel access. This causes unavoidable contention overheads and does not provide performance guarantees. In this work, we outline some issues that occur with the probabilistic channel access in highly congested scenarios and how those can be mitigated using deterministic scheduling. Towards this, we propose to use Target Wake Time (TWT) - a feature introduced in Wi-Fi 6 as a power-saving mechanism, to improve the performance of Wi-Fi. To gain insights into the workings of the TWT over commercially available off-the-shelf components and to analyze the factors that affect its performance, we carry out various experiments with it over our Wi-Fi 6 testbed. Using these insights and analysis, we formulate and solve an optimization problem to synthesize deterministic schedules and obtain the optimal values of various system parameters. Lastly, we configure our testbed with these optimal parameter values and show that the TWT based deterministic scheduling consistently results in better performance of the TWT-capable clients and overall system performance compared to traditional CSMA/CA based scheduling.","authors":["Govind Rajendran","Samar Agnihotri"],"url":"https://arxiv.org/abs/2505.00447"}
{"created":"2025-05-08","title":"On the generalization of language models from in-context learning and finetuning: a controlled study","abstract":"Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning. E.g. they can fail to generalize to simple reversals of relations they are trained on, or fail to make simple logical deductions based on trained information. These failures to generalize from fine-tuning can hinder practical application of these models. On the other hand, language models' in-context learning shows different inductive biases, and can generalize better in some cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' abilities to generalize from finetuning data. The datasets are designed to create clean tests of generalization, by isolating the knowledge in the dataset from that in pretraining. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.","authors":["Andrew K. Lampinen","Arslan Chaudhry","Stephanie C. Y. Chan","Cody Wild","Diane Wan","Alex Ku","J\\\"org Bornschein","Razvan Pascanu","Murray Shanahan","James L. McClelland"],"url":"https://arxiv.org/abs/2505.00661"}
{"created":"2025-05-08","title":"SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation","abstract":"Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan -- a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics.","authors":["Quang P. M. Pham","Khoi T. N. Nguyen","Nhi H. Doan","Cuong A. Pham","Kentaro Inui","Dezhen Song"],"url":"https://arxiv.org/abs/2505.00831"}
{"created":"2025-05-08","title":"A Character-based Diffusion Embedding Algorithm for Enhancing the Generation Quality of Generative Linguistic Steganographic Texts","abstract":"Generating high-quality steganographic text is a fundamental challenge in the field of generative linguistic steganography. This challenge arises primarily from two aspects: firstly, the capabilities of existing models in text generation are limited; secondly, embedding algorithms fail to effectively mitigate the negative impacts of sensitive information's properties, such as semantic content or randomness. Specifically, to ensure that the recipient can accurately extract hidden information, embedding algorithms often have to consider selecting candidate words with relatively low probabilities. This phenomenon leads to a decrease in the number of high-probability candidate words and an increase in low-probability candidate words, thereby compromising the semantic coherence and logical fluency of the steganographic text and diminishing the overall quality of the generated steganographic material. To address this issue, this paper proposes a novel embedding algorithm, character-based diffusion embedding algorithm (CDEA). Unlike existing embedding algorithms that strive to eliminate the impact of sensitive information's properties on the generation process, CDEA leverages sensitive information's properties. It enhances the selection frequency of high-probability candidate words in the candidate pool based on general statistical properties at the character level and grouping methods based on power-law distributions, while reducing the selection frequency of low-probability candidate words in the candidate pool. Furthermore, to ensure the effective transformation of sensitive information in long sequences, we also introduce the XLNet model. Experimental results demonstrate that the combination of CDEA and XLNet significantly improves the quality of generated steganographic text, particularly in terms of perceptual-imperceptibility.","authors":["Yingquan Chen","Qianmu Li","Xiaocong Wu","Huifeng Li","Qing Chang"],"url":"https://arxiv.org/abs/2505.00977"}
{"created":"2025-05-08","title":"AdSight: Scalable and Accurate Quantification of User Attention in Multi-Slot Sponsored Search","abstract":"Modern Search Engine Results Pages (SERPs) present complex layouts where multiple elements compete for visibility. Attention modelling is crucial for optimising web design and computational advertising, whereas attention metrics can inform ad placement and revenue strategies. We introduce AdSight, a method leveraging mouse cursor trajectories to quantify in a scalable and accurate manner user attention in multi-slot environments like SERPs. AdSight uses a novel Transformer-based sequence-to-sequence architecture where the encoder processes cursor trajectory embeddings, and the decoder incorporates slot-specific features, enabling robust attention prediction across various SERP layouts. We evaluate our approach on two Machine Learning tasks: (1) regression, to predict fixation times and counts; and (2) classification, to determine some slot types were noticed. Our findings demonstrate the model's ability to predict attention with unprecedented precision, offering actionable insights for researchers and practitioners.","authors":["Mario Villaiz\\'an-Vallelado","Matteo Salvatori","Kayhan Latifzadeh","Antonio Penta","Luis A. Leiva","Ioannis Arapakis"],"url":"https://arxiv.org/abs/2505.01451"}
{"created":"2025-05-08","title":"A Defect Taxonomy for Infrastructure as Code: A Replication Study","abstract":"Background: As Infrastructure as Code (IaC) becomes standard practice, ensuring the reliability of IaC scripts is essential. Defect taxonomies are valuable tools for this, offering a common language for issues and enabling systematic tracking. A significant prior study developed such a taxonomy, but based it exclusively on the declarative language Puppet. It remained unknown whether this taxonomy applies to programming language-based IaC (PL-IaC) tools like Pulumi, Terraform CDK, and AWS CDK. Aim: We replicated this foundational work to assess the generalizability of the taxonomy across a broader and more diverse landscape. Method: We performed qualitative analysis on 3,364 defect-related commits from 285 open-source PL-IaC repositories (PIPr dataset) to derive a PL-IaC-specific defect taxonomy. We then enhanced the ACID tool, originally developed for the prior study, to automatically classify and analyze defect distributions across an expanded dataset-447 open-source repositories and 94 proprietary projects from VTEX (e-commerce) and Nubank (financial). Results: Our research confirmed the same eight defect categories identified in the original study, with idempotency and security defects appearing infrequently but persistently across projects. Configuration Data defects maintain high frequency in both open-source and proprietary codebases. Conclusions: Our replication supports the generalizability of the original taxonomy, suggesting IaC development challenges surpass organizational boundaries. Configuration Data defects emerge as a persistent high-frequency problem, while idempotency and security defects remain important concerns despite lower frequency. These patterns appear consistent across open-source and proprietary projects, indicating they are fundamental to the IaC paradigm itself, transcending specific tools or project types.","authors":["Filipe Paiva","Jo\\~ao Brunet","Thiago Emmanuel Pereira","Wendell Oliveira"],"url":"https://arxiv.org/abs/2505.01568"}
{"created":"2025-05-08","title":"RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation","abstract":"Operating robots in open-ended scenarios with diverse tasks is a crucial research and application direction in robotics. While recent progress in natural language processing and large multimodal models has enhanced robots' ability to understand complex instructions, robot manipulation still faces the procedural skill dilemma and the declarative skill dilemma in open environments. Existing methods often compromise cognitive and executive capabilities. To address these challenges, in this paper, we propose RoBridge, a hierarchical intelligent architecture for general robotic manipulation. It consists of a high-level cognitive planner (HCP) based on a large-scale pre-trained vision-language model (VLM), an invariant operable representation (IOR) serving as a symbolic bridge, and a generalist embodied agent (GEA). RoBridge maintains the declarative skill of VLM and unleashes the procedural skill of reinforcement learning, effectively bridging the gap between cognition and execution. RoBridge demonstrates significant performance improvements over existing baselines, achieving a 75% success rate on new tasks and an 83% average success rate in sim-to-real generalization using only five real-world data samples per task. This work represents a significant step towards integrating cognitive reasoning with physical execution in robotic systems, offering a new paradigm for general robotic manipulation.","authors":["Kaidong Zhang","Rongtao Xu","Pengzhen Ren","Junfan Lin","Hefeng Wu","Liang Lin","Xiaodan Liang"],"url":"https://arxiv.org/abs/2505.01709"}
{"created":"2025-05-08","title":"Weakly-supervised Audio Temporal Forgery Localization via Progressive Audio-language Co-learning Network","abstract":"Audio temporal forgery localization (ATFL) aims to find the precise forgery regions of the partial spoof audio that is purposefully modified. Existing ATFL methods rely on training efficient networks using fine-grained annotations, which are obtained costly and challenging in real-world scenarios. To meet this challenge, in this paper, we propose a progressive audio-language co-learning network (LOCO) that adopts co-learning and self-supervision manners to prompt localization performance under weak supervision scenarios. Specifically, an audio-language co-learning module is first designed to capture forgery consensus features by aligning semantics from temporal and global perspectives. In this module, forgery-aware prompts are constructed by using utterance-level annotations together with learnable prompts, which can incorporate semantic priors into temporal content features dynamically. In addition, a forgery localization module is applied to produce forgery proposals based on fused forgery-class activation sequences. Finally, a progressive refinement strategy is introduced to generate pseudo frame-level labels and leverage supervised semantic contrastive learning to amplify the semantic distinction between real and fake content, thereby continuously optimizing forgery-aware features. Extensive experiments show that the proposed LOCO achieves SOTA performance on three public benchmarks.","authors":["Junyan Wu","Wenbo Xu","Wei Lu","Xiangyang Luo","Rui Yang","Shize Guo"],"url":"https://arxiv.org/abs/2505.01880"}
{"created":"2025-05-08","title":"Interactive authoring of outcome-oriented lesson plans for immersive Virtual Reality training","abstract":"Immersive Virtual Reality (iVR) applications have shown immense potential for skill training and learning in manufacturing. However, authoring of such applications requires technical expertise, which makes it difficult for educators to author instructions targeted at desired learning outcomes. We present FlowTrainer, an LLM-assisted interactive system to allow educators to author lesson plans for their iVR instruction based on desired goals. The authoring workflow is supported by Backward design to align the planned lesson based on the desired outcomes. We implemented a welding use case and conducted a user study with welding experts to test the effectiveness of the system in authoring outcome-oriented lesson plans. The study results showed that the system allowed users to plan lesson plans based on desired outcomes while reducing the time and technical expertise required for the authoring process. We believe that such efforts can allow widespread adoption of iVR solutions in manufacturing training to meet the workforce demands in the industry.","authors":["Ananya Ipsita","Ramesh Kaki","Mayank Patel","Asim Unmesh","Kylie A. Peppler","Karthik Ramani"],"url":"https://arxiv.org/abs/2505.01886"}
{"created":"2025-05-08","title":"Requirements-Based Test Generation: A Comprehensive Survey","abstract":"As an important way of assuring software quality, software testing generates and executes test cases to identify software failures. Many strategies have been proposed to guide test-case generation, such as source-code-based approaches and methods based on bug reports. Requirements-based test generation (RBTG) constructs test cases based on specified requirements, aligning with user needs and expectations, without requiring access to the source code. Since its introduction in 1994, there have been many contributions to the development of RBTG, including various approaches, implementations, tools, assessment and evaluation methods, and applications. This paper provides a comprehensive survey on RBTG, categorizing requirement types, classifying approaches, investigating types of test cases, summarizing available tools, and analyzing experimental evaluations. This paper also summarizes the domains and industrial applications of RBTG, and discusses some open research challenges and potential future work.","authors":["Zhenzhen Yang","Rubing Huang","Chenhui Cui","Nan Niu","Dave Towey"],"url":"https://arxiv.org/abs/2505.02015"}
{"created":"2025-05-08","title":"Grassroots Democratic Federation: Fair Governance of Large-Scale, Decentralized, Sovereign Digital Communities","abstract":"Grassroots Democratic Federation aims to address the egalitarian formation and the fair democratic governance of large-scale, decentralized, sovereign digital communities, the size of the EU, the US, existing social networks, and even humanity at large. A grassroots democratic federation evolves via the grassroots formation of digital communities and their consensual federation. Such digital communities may form according to geography, jurisdiction, affiliations, relations, interests, or causes. Small communities (say up to 100 members) govern themselves; larger communities -- no matter how large -- are governed by a small assembly elected by sortition among its members. Earlier work on Grassroots Democratic Federation explored the fair sortition of the assemblies of a federation in a static setting: Given a federation, populate its assemblies with members satisfying ex ante and ex post fairness conditions on the participation of members of a community in its assembly, and on the representation of child communities in the assembly of their parent community.","authors":["Ehud Shapiro","Nimrod Talmon"],"url":"https://arxiv.org/abs/2505.02208"}
{"created":"2025-05-08","title":"Practical Efficiency of Muon for Pretraining","abstract":"We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.","authors":["Essential AI",":","Ishaan Shah","Anthony M. Polloreno","Karl Stratos","Philip Monk","Adarsh Chaluvaraju","Andrew Hojel","Andrew Ma","Anil Thomas","Ashish Tanwer","Darsh J Shah","Khoi Nguyen","Kurt Smith","Michael Callahan","Michael Pust","Mohit Parmar","Peter Rushton","Platon Mazarakis","Ritvik Kapila","Saurabh Srivastava","Somanshu Singla","Tim Romanski","Yash Vanjani","Ashish Vaswani"],"url":"https://arxiv.org/abs/2505.02222"}
{"created":"2025-05-08","title":"JTCSE: Joint Tensor-Modulus Constraints and Cross-Attention for Unsupervised Contrastive Learning of Sentence Embeddings","abstract":"Unsupervised contrastive learning has become a hot research topic in natural language processing. Existing works usually aim at constraining the orientation distribution of the representations of positive and negative samples in the high-dimensional semantic space in contrastive learning, but the semantic representation tensor possesses both modulus and orientation features, and the existing works ignore the modulus feature of the representations and cause insufficient contrastive learning. % Therefore, we firstly propose a training objective that aims at modulus constraints on the semantic representation tensor, to strengthen the alignment between the positive samples in contrastive learning. Therefore, we first propose a training objective that is designed to impose modulus constraints on the semantic representation tensor, to strengthen the alignment between positive samples in contrastive learning. Then, the BERT-like model suffers from the phenomenon of sinking attention, leading to a lack of attention to CLS tokens that aggregate semantic information. In response, we propose a cross-attention structure among the twin-tower ensemble models to enhance the model's attention to CLS token and optimize the quality of CLS Pooling. Combining the above two motivations, we propose a new \\textbf{J}oint \\textbf{T}ensor representation modulus constraint and \\textbf{C}ross-attention unsupervised contrastive learning \\textbf{S}entence \\textbf{E}mbedding representation framework JTCSE, which we evaluate in seven semantic text similarity computation tasks, and the experimental results show that JTCSE's twin-tower ensemble model and single-tower distillation model outperform the other baselines and become the current SOTA. In addition, we have conducted an extensive zero-shot downstream task evaluation, which shows that JTCSE outperforms other baselines overall on more than 130 tasks.","authors":["Tianyu Zong","Hongzhu Yi","Bingkang Shi","Yuanxiang Wang","Jungang Xu"],"url":"https://arxiv.org/abs/2505.02366"}
{"created":"2025-05-08","title":"Sharpness-Aware Minimization with Z-Score Gradient Filtering for Neural Networks","abstract":"Sharpness-Aware Minimization (SAM) improves neural network generalization by optimizing the worst-case loss within a neighborhood of parameters, yet it perturbs parameters using the entire gradient vector, including components with low statistical significance. We introduce ZSharp, a refined sharpness-aware optimization method that incorporates layer-wise Z-score normalization followed by percentile-based filtering. This process selects only the most statistically significant gradient components-those with large standardized magnitudes-for constructing the perturbation direction. ZSharp retains the standard two-phase SAM structure of ascent and descent while modifying the ascent step to focus on sharper, curvature-relevant directions. We evaluate ZSharp on CIFAR-10, CIFAR-100, and Tiny-ImageNet using a range of models including ResNet, VGG, and Vision Transformers. Across all architectures and datasets, ZSharp consistently achieves higher test accuracy compared to SAM, ASAM, and Friendly-SAM. These results indicate that Z-score-based gradient filtering can enhance the sharpness sensitivity of the update direction, leading to improved generalization in deep neural network training.","authors":["Juyoung Yun"],"url":"https://arxiv.org/abs/2505.02369"}
{"created":"2025-05-08","title":"Token Coordinated Prompt Attention is Needed for Visual Prompting","abstract":"Visual prompting techniques are widely used to efficiently fine-tune pretrained Vision Transformers (ViT) by learning a small set of shared prompts for all tokens. However, existing methods overlook the unique roles of different tokens in conveying discriminative information and interact with all tokens using the same prompts, thereby limiting the representational capacity of ViT. This often leads to indistinguishable and biased prompt-extracted features, hindering performance. To address this issue, we propose a plug-and-play Token Coordinated Prompt Attention (TCPA) module, which assigns specific coordinated prompts to different tokens for attention-based interactions. Firstly, recognizing the distinct functions of CLS and image tokens-global information aggregation and local feature extraction, we disentangle the prompts into CLS Prompts and Image Prompts, which interact exclusively with CLS tokens and image tokens through attention mechanisms. This enhances their respective discriminative abilities. Furthermore, as different image tokens correspond to distinct image patches and contain diverse information, we employ a matching function to automatically assign coordinated prompts to individual tokens. This enables more precise attention interactions, improving the diversity and representational capacity of the extracted features. Extensive experiments across various benchmarks demonstrate that TCPA significantly enhances the diversity and discriminative power of the extracted features. The code is available at https://github.com/zhoujiahuan1991/ICML2025-TCPA.","authors":["Zichen Liu","Xu Zou","Gang Hua","Jiahuan Zhou"],"url":"https://arxiv.org/abs/2505.02406"}
{"created":"2025-05-08","title":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction","abstract":"We introduce Ming-Lite-Uni, an open-source multimodal framework featuring a newly designed unified visual generator and a native multimodal autoregressive model tailored for unifying vision and language. Specifically, this project provides an open-source implementation of the integrated MetaQueries and M2-omni framework, while introducing the novel multi-scale learnable tokens and multi-scale representation alignment strategy. By leveraging a fixed MLLM and a learnable diffusion model, Ming-Lite-Uni enables native multimodal AR models to perform both text-to-image generation and instruction based image editing tasks, expanding their capabilities beyond pure visual understanding. Our experimental results demonstrate the strong performance of Ming-Lite-Uni and illustrate the impressive fluid nature of its interactive process. All code and model weights are open-sourced to foster further exploration within the community. Notably, this work aligns with concurrent multimodal AI milestones - such as ChatGPT-4o with native image generation updated in March 25, 2025 - underscoring the broader significance of unified models like Ming-Lite-Uni on the path toward AGI. Ming-Lite-Uni is in alpha stage and will soon be further refined.","authors":["Inclusion AI","Biao Gong","Cheng Zou","Dandan Zheng","Hu Yu","Jingdong Chen","Jianxin Sun","Junbo Zhao","Jun Zhou","Kaixiang Ji","Lixiang Ru","Libin Wang","Qingpei Guo","Rui Liu","Weilong Chai","Xinyu Xiao","Ziyuan Huang"],"url":"https://arxiv.org/abs/2505.02471"}
{"created":"2025-05-08","title":"Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities","abstract":"Recent years have seen remarkable progress in both multimodal understanding models and image generation models. Despite their respective successes, these two domains have evolved independently, leading to distinct architectural paradigms: While autoregressive-based architectures have dominated multimodal understanding, diffusion-based models have become the cornerstone of image generation. Recently, there has been growing interest in developing unified frameworks that integrate these tasks. The emergence of GPT-4o's new capabilities exemplifies this trend, highlighting the potential for unification. However, the architectural differences between the two domains pose significant challenges. To provide a clear overview of current efforts toward unification, we present a comprehensive survey aimed at guiding future research. First, we introduce the foundational concepts and recent advancements in multimodal understanding and text-to-image generation models. Next, we review existing unified models, categorizing them into three main architectural paradigms: diffusion-based, autoregressive-based, and hybrid approaches that fuse autoregressive and diffusion mechanisms. For each category, we analyze the structural designs and innovations introduced by related works. Additionally, we compile datasets and benchmarks tailored for unified models, offering resources for future exploration. Finally, we discuss the key challenges facing this nascent field, including tokenization strategy, cross-modal attention, and data. As this area is still in its early stages, we anticipate rapid advancements and will regularly update this survey. Our goal is to inspire further research and provide a valuable reference for the community. The references associated with this survey are available on GitHub (https://github.com/AIDC-AI/Awesome-Unified-Multimodal-Models).","authors":["Xinjie Zhang","Jintao Guo","Shanshan Zhao","Minghao Fu","Lunhao Duan","Guo-Hua Wang","Qing-Guo Chen","Zhao Xu","Weihua Luo","Kaifu Zhang"],"url":"https://arxiv.org/abs/2505.02567"}
{"created":"2025-05-08","title":"Advances in Automated Fetal Brain MRI Segmentation and Biometry: Insights from the FeTA 2024 Challenge","abstract":"Accurate fetal brain tissue segmentation and biometric analysis are essential for studying brain development in utero. The FeTA Challenge 2024 advanced automated fetal brain MRI analysis by introducing biometry prediction as a new task alongside tissue segmentation. For the first time, our diverse multi-centric test set included data from a new low-field (0.55T) MRI dataset. Evaluation metrics were also expanded to include the topology-specific Euler characteristic difference (ED). Sixteen teams submitted segmentation methods, most of which performed consistently across both high- and low-field scans. However, longitudinal trends indicate that segmentation accuracy may be reaching a plateau, with results now approaching inter-rater variability. The ED metric uncovered topological differences that were missed by conventional metrics, while the low-field dataset achieved the highest segmentation scores, highlighting the potential of affordable imaging systems when paired with high-quality reconstruction. Seven teams participated in the biometry task, but most methods failed to outperform a simple baseline that predicted measurements based solely on gestational age, underscoring the challenge of extracting reliable biometric estimates from image data alone. Domain shift analysis identified image quality as the most significant factor affecting model generalization, with super-resolution pipelines also playing a substantial role. Other factors, such as gestational age, pathology, and acquisition site, had smaller, though still measurable, effects. Overall, FeTA 2024 offers a comprehensive benchmark for multi-class segmentation and biometry estimation in fetal brain MRI, underscoring the need for data-centric approaches, improved topological evaluation, and greater dataset diversity to enable clinically robust and generalizable AI tools.","authors":["Vladyslav Zalevskyi","Thomas Sanchez","Misha Kaandorp","Margaux Roulet","Diego Fajardo-Rojas","Liu Li","Jana Hutter","Hongwei Bran Li","Matthew Barkovich","Hui Ji","Luca Wilhelmi","Aline D\\\"andliker","C\\'eline Steger","M\\'eriam Koob","Yvan Gomez","Anton Jakov\\v{c}i\\'c","Melita Klai\\'c","Ana Ad\\v{z}i\\'c","Pavel Markovi\\'c","Gracia Grabari\\'c","Milan Rados","Jordina Aviles Verdera","Gregor Kasprian","Gregor Dovjak","Raphael Gaubert-Rachm\\\"uhl","Maurice Aschwanden","Qi Zeng","Davood Karimi","Denis Peruzzo","Tommaso Ciceri","Giorgio Longari","Rachika E. Hamadache","Amina Bouzid","Xavier Llad\\'o","Simone Chiarella","Gerard Mart\\'i-Juan","Miguel \\'Angel Gonz\\'alez Ballester","Marco Castellaro","Marco Pinamonti","Valentina Visani","Robin Cremese","Ke\\\"in Sam","Fleur Gaudfernau","Param Ahir","Mehul Parikh","Maximilian Zenk","Michael Baumgartner","Klaus Maier-Hein","Li Tianhong","Yang Hong","Zhao Longfei","Domen Preloznik","\\v{Z}iga \\v{S}piclin","Jae Won Choi","Muyang Li","Jia Fu","Guotai Wang","Jingwen Jiang","Lyuyang Tong","Bo Du","Milton O. Candela-Leal","Andrea Gondova","Sungmin You","Abdul Qayyum","Moona Mazher","Steven A Niederer","Andras Jakab","Roxane Licandro","Kelly Payette","Meritxell Bach Cuadra"],"url":"https://arxiv.org/abs/2505.02784"}
{"created":"2025-05-08","title":"Cell-Free Massive MIMO-Assisted SWIPT for IoT Networks","abstract":"This paper studies cell-free massive multiple-input multiple-output (CF-mMIMO) systems that underpin simultaneous wireless information and power transfer (SWIPT) for separate information users (IUs) and energy users (EUs) in Internet of Things (IoT) networks. We propose a joint access point (AP) operation mode selection and power control design, wherein certain APs are designated for energy transmission to EUs, while others are dedicated to information transmission to IUs. The performance of the system, from both a spectral efficiency (SE) and energy efficiency (EE) perspective, is comprehensively analyzed. Specifically, we formulate two mixed-integer nonconvex optimization problems for maximizing the average sum-SE and EE, under realistic power consumption models and constraints on the minimum individual SE requirements for individual IUs, minimum HE for individual EUs, and maximum transmit power at each AP. The challenging optimization problems are solved using successive convex approximation (SCA) techniques. The proposed framework design is further applied to the average sum-HE maximization and energy harvesting fairness problems. Our numerical results demonstrate that the proposed joint AP operation mode selection and power control algorithm can achieve EE performance gains of up to $4$-fold and $5$-fold over random AP operation mode selection, with and without power control respectively.","authors":["Mohammadali Mohammadi","Le-Nam Tran","Zahra Mobini","Hien Quoc Ngo","Michail Matthaiou"],"url":"https://arxiv.org/abs/2505.02806"}
{"created":"2025-05-08","title":"No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves","abstract":"Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance generation quality of the diffusion transformers. However, existing approaches necessitate to either introduce an additional and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation Alignment (SRA), a simple yet straightforward method that obtain representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in earlier layer with higher noise to that in later layer with lower noise to progressively enhance the overall representation learning during only generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that heavily dependent on powerful external representation priors.","authors":["Dengyang Jiang","Mengmeng Wang","Liuzhuozheng Li","Lei Zhang","Haoyu Wang","Wei Wei","Guang Dai","Yanning Zhang","Jingdong Wang"],"url":"https://arxiv.org/abs/2505.02831"}
{"created":"2025-05-08","title":"Toward a Harmonized Approach - Requirement-based Structuring of a Safety Assurance Argumentation for Automated Vehicles","abstract":"Despite increasing testing operation on public roads, media reports on incidents show that safety issues remain to this day. One major cause factoring into this circumstance is high development uncertainty that manufacturers face when deploying these systems in an open context. In particular, one challenge is establishing a valid argument at design time that the vehicle will exhibit reasonable residual risk when operating in its intended operational design domain. Regulations, such as the European Implementing Regulation 2022/1426, require manufacturers to provide a safety assurance argumentation for SAE-Level-4 automated vehicles. While there is extensive literature on assurance cases for safety-critical systems, the domain of automated driving lacks explicit requirements regarding the creation of safety assurance argumentations. In this paper, we aim to narrow this gap by elaborating a requirement-based approach. We derive structural requirements for an argumentation from literature and supplement these with requirements derived from stakeholder concerns. We implement the requirements, yielding a proposal for an overall argumentation structure. The resulting \"safety arguments\" argue over four topic complexes: The developed product, the underlying process including its conformance/compliance to standards/laws, as well as the argumentations' context and soundness. Finally, we instantiate this structure with respect to domain-specific needs and principles.","authors":["Marvin Loba","Nayel Fabian Salem","Marcus Nolte","Andreas Dotzler","Dieter Ludwig","Markus Maurer"],"url":"https://arxiv.org/abs/2505.03709"}
{"created":"2025-05-08","title":"A characterization of testable hypergraph properties","abstract":"We provide a combinatorial characterization of all testable properties of $k$-uniform hypergraphs ($k$-graphs for short). Here, a $k$-graph property $P$ is testable if there is a randomized algorithm which makes a bounded number of edge queries and distinguishes with probability $2/3$ between $k$-graphs that satisfy $P$ and those that are far from satisfying $P$. For the $2$-graph case, such a combinatorial characterization was obtained by Alon, Fischer, Newman and Shapira. Our results for the $k$-graph setting are in contrast to those of Austin and Tao, who showed that for the somewhat stronger concept of local repairability, the testability results for graphs do not extend to the $3$-graph setting. Our proof relies on a random subhypergraph sampling result proved in a companion paper.","authors":["Felix Joos","Jaehoon Kim","Daniela K\\\"uhn","Deryk Osthus"],"url":"https://arxiv.org/abs/1707.03303"}
{"created":"2025-05-08","title":"Tight Regret Bounds for Bayesian Optimization in One Dimension","abstract":"We consider the problem of Bayesian optimization (BO) in one dimension, under a Gaussian process prior and Gaussian sampling noise. We provide a theoretical analysis showing that, under fairly mild technical assumptions on the kernel, the best possible cumulative regret up to time $T$ behaves as $\\Omega(\\sqrt{T})$ and $O(\\sqrt{T\\log T})$. This gives a tight characterization up to a $\\sqrt{\\log T}$ factor, and includes the first non-trivial lower bound for noisy BO. Our assumptions are satisfied, for example, by the squared exponential and Mat\\'ern-$\\nu$ kernels, with the latter requiring $\\nu > 2$. Our results certify the near-optimality of existing bounds (Srinivas {\\em et al.}, 2009) for the SE kernel, while proving them to be strictly suboptimal for the Mat\\'ern kernel with $\\nu > 2$.","authors":["Jonathan Scarlett"],"url":"https://arxiv.org/abs/1805.11792"}
{"created":"2025-05-08","title":"Welfare Analysis in Dynamic Models","abstract":"This paper introduces metrics for welfare analysis in dynamic models. We develop estimation and inference for these parameters even in the presence of a high-dimensional state space. Examples of welfare metrics include average welfare, average marginal welfare effects, and welfare decompositions into direct and indirect effects similar to Oaxaca (1973) and Blinder (1973). We derive dual and doubly robust representations of welfare metrics that facilitate debiased inference. For average welfare, the value function does not have to be estimated. In general, debiasing can be applied to any estimator of the value function, including neural nets, random forests, Lasso, boosting, and other high-dimensional methods. In particular, we derive Lasso and Neural Network estimators of the value function and associated dynamic dual representation and establish associated mean square convergence rates for these functions. Debiasing is automatic in the sense that it only requires knowledge of the welfare metric of interest, not the form of bias correction. The proposed methods are applied to estimate a dynamic behavioral model of teacher absenteeism in \\cite{DHR} and associated average teacher welfare.","authors":["Victor Chernozhukov","Whitney Newey","Vira Semenova"],"url":"https://arxiv.org/abs/1908.09173"}
{"created":"2025-05-08","title":"Auto.gov: Learning-based Governance for Decentralized Finance (DeFi)","abstract":"Decentralized finance (DeFi) is an integral component of the blockchain ecosystem, enabling a range of financial activities through smart-contract-based protocols. Traditional DeFi governance typically involves manual parameter adjustments by protocol teams or token holder votes, and is thus prone to human bias and financial risks, undermining the system's integrity and security. While existing efforts aim to establish more adaptive parameter adjustment schemes, there remains a need for a governance model that is both more efficient and resilient to significant market manipulations. In this paper, we introduce \"Auto$.$gov\", a learning-based governance framework that employs a deep Qnetwork (DQN) reinforcement learning (RL) strategy to perform semi-automated, data-driven parameter adjustments. We create a DeFi environment with an encoded action-state space akin to the Aave lending protocol for simulation and testing purposes, where Auto$.$gov has demonstrated the capability to retain funds that would have otherwise been lost to price oracle attacks. In tests with real-world data, Auto$.$gov outperforms the benchmark approaches by at least 14% and the static baseline model by tenfold, in terms of the preset performance metric--protocol profitability. Overall, the comprehensive evaluations confirm that Auto$.$gov is more efficient and effective than traditional governance methods, thereby enhancing the security, profitability, and ultimately, the sustainability of DeFi protocols.","authors":["Jiahua Xu","Yebo Feng","Daniel Perez","Benjamin Livshits"],"url":"https://arxiv.org/abs/2302.09551"}
{"created":"2025-05-08","title":"QbC: Quantum Correctness by Construction","abstract":"Thanks to the rapid progress and growing complexity of quantum algorithms, correctness of quantum programs has become a major concern. Pioneering research over the past years has proposed various approaches to formally verify quantum programs using proof systems such as quantum Hoare logic. All these prior approaches are post-hoc: one first implements a program and only then verifies its correctness. Here we propose Quantum Correctness by Construction (QbC): an approach to constructing quantum programs from their specification in a way that ensures correctness. We use pre- and postconditions to specify program properties, and propose sound and complete refinement rules for constructing programs in a quantum while language from their specification. We validate QbC by constructing quantum programs for idiomatic problems and patterns. We find that the approach naturally suggests how to derive program details, highlighting key design choices along the way. As such, we believe that QbC can play a role in supporting the design and taxonomization of quantum algorithms and software.","authors":["Anurudh Peduri","Ina Schaefer","Michael Walter"],"url":"https://arxiv.org/abs/2307.15641"}
{"created":"2025-05-08","title":"Computable and Faithful Lower Bound on Entanglement Cost","abstract":"Quantifying the minimum entanglement needed to prepare quantum states and implement quantum processes is a key challenge in quantum information theory. In this work, we develop computable and faithful lower bounds on the entanglement cost under quantum operations that completely preserve the positivity of partial transpose (PPT operations), by introducing the generalized divergence of $k$-negativity, a generalization of logarithmic negativity. Our bounds are efficiently computable via semidefinite programming and provide non-trivial values for all states that are non-PPT (NPT), establishing their faithfulness for the resource theory of NPT entanglement. Notably, we find and affirm the irreversibility of asymptotic entanglement manipulation under PPT operations for full-rank entangled states. Furthermore, we extend our methodology to derive lower bounds on the entanglement cost of both point-to-point and bipartite quantum channels. Our bound demonstrates improvements over previously known computable bounds for a wide range of quantum states and channels. These findings push the boundaries of understanding the structure of entanglement and the fundamental limits of entanglement manipulation.","authors":["Xin Wang","Mingrui Jing","Chengkai Zhu"],"url":"https://arxiv.org/abs/2311.10649"}
{"created":"2025-05-08","title":"Simulating Nighttime Visible Satellite Imagery of Tropical Cyclones Using Conditional Generative Adversarial Networks","abstract":"Visible (VIS) imagery is important for monitoring Tropical Cyclones (TCs) but is unavailable at night. This study presents a Conditional Generative Adversarial Networks (CGAN) model to generate nighttime VIS imagery with significantly enhanced accuracy and spatial resolution. Our method offers three key improvements compared to existing models. First, we replaced the L1 loss in the pix2pix framework with the Structural Similarity Index Measure (SSIM) loss, which significantly reduced image blurriness. Second, we selected multispectral infrared (IR) bands as input based on a thorough examination of their spectral properties, providing essential physical information for accurate simulation. Third, we incorporated the direction parameters of the sun and the satellite, which addressed the dependence of VIS images on sunlight directions and enabled a much larger training set from continuous daytime data. The model was trained and validated using data from the Advanced Himawari Imager (AHI) in the daytime, achieving statistical results of SSIM = 0.923 and Root Mean Square Error (RMSE) = 0.0299, which significantly surpasses existing models. We also performed a cross-satellite nighttime model validation using the Day/Night Band (DNB) of the Visible/Infrared Imager Radiometer Suite (VIIRS), which yields outstanding results compared to existing models. Our model is operationally applied to generate accurate VIS imagery with arbitrary virtual sunlight directions, significantly contributing to the nighttime monitoring of various meteorological phenomena.","authors":["Jinghuai Yao","Puyuan Du","Yucheng Zhao","Yubo Wang"],"url":"https://arxiv.org/abs/2401.11679"}
{"created":"2025-05-08","title":"On Understanding Attention-Based In-Context Learning for Categorical Data","abstract":"In-context learning based on attention models is examined for data with categorical outcomes, with inference in such models viewed from the perspective of functional gradient descent (GD). We develop a network composed of attention blocks, with each block employing a self-attention layer followed by a cross-attention layer, with associated skip connections. This model can exactly perform multi-step functional GD inference for in-context inference with categorical observations. We perform a theoretical analysis of this setup, generalizing many prior assumptions in this line of work, including the class of attention mechanisms for which it is appropriate. We demonstrate the framework empirically on synthetic data, image classification and language generation.","authors":["Aaron T. Wang","William Convertino","Xiang Cheng","Ricardo Henao","Lawrence Carin"],"url":"https://arxiv.org/abs/2405.17248"}
{"created":"2025-05-08","title":"Optimal Rates of Convergence for Entropy Regularization in Discounted Markov Decision Processes","abstract":"We study the error introduced by entropy regularization in infinite-horizon, discrete, discounted Markov decision processes. We show that this error decreases exponentially in the inverse regularization strength both in a weighted KL-divergence and in value with a problem-specific exponent. This is in contrast to previously known estimates, of the order $O(\\tau)$, where $\\tau$ is the regularization strength. We provide a lower bound matching our upper bound up to a polynomial term, thereby characterizing the exponential convergence rate for entropy regularization. Our proof relies on the observation that the solutions of entropy-regularized Markov decision processes solve a gradient flow of the unregularized reward with respect to a Riemannian metric common in natural policy gradient methods. This correspondence allows us to identify the limit of this gradient flow as the generalized maximum entropy optimal policy, thereby characterizing the implicit bias of this gradient flow, which corresponds to a time-continuous version of the natural policy gradient method. We use our improved error estimates to show that for entropy-regularized natural policy gradient methods, the overall error decays exponentially in the square root of the number of iterations, improving over existing sublinear guarantees. Finally, we extend our analysis to settings beyond the entropy. In particular, we characterize the implicit bias regarding general convex potentials and their resulting generalized natural policy gradients.","authors":["Johannes M\\\"uller","Semih Cayci"],"url":"https://arxiv.org/abs/2406.04163"}
{"created":"2025-05-08","title":"SelectTTS: Synthesizing Anyone's Voice via Discrete Unit-Based Frame Selection","abstract":"Synthesizing the voices of unseen speakers remains a persisting challenge in multi-speaker text-to-speech (TTS). Existing methods model speaker characteristics through speaker conditioning during training, leading to increased model complexity and limiting reproducibility and accessibility. A lower-complexity method would enable speech synthesis research with limited computational and data resources to reach to a wider use. To this end, we propose SelectTTS, a simple and effective alternative. SelectTTS selects appropriate frames from the target speaker and decodes them using frame-level self-supervised learning (SSL) features. We demonstrate that this approach can effectively capture speaker characteristics for unseen speakers and achieves performance comparable to state-of-the-art multi-speaker TTS frameworks on both objective and subjective metrics. By directly selecting frames from the target speaker's speech, SelectTTS enables generalization to unseen speakers with significantly lower model complexity. Compared to baselines such as XTTS-v2 and VALL-E, SelectTTS achieves better speaker similarity while reducing model parameters by over 8x and training data requirements by 270x.","authors":["Ismail Rasim Ulgen","Shreeram Suresh Chandra","Junchen Lu","Berrak Sisman"],"url":"https://arxiv.org/abs/2408.17432"}
{"created":"2025-05-08","title":"Random sampling of permutations through quantum circuits","abstract":"In this paper, we introduce a classical algorithm for random sampling of permutations, drawing inspiration from the Steinhaus-Johnson-Trotter algorithm. Our approach takes a comprehensive view of permutation sampling by expressing them as products of adjacent transpositions. Building on this, we develop a quantum analogue of the classical algorithm using a quantum circuit model for random sampling of permutations. As an application, we present a quantum algorithm for the two-sample randomization test to assess the difference of means in classical data. Finally, we propose a nested corona product graph generative model for symmetric groups, which facilitates random sampling of permutations from specific sets of permutations through a quantum circuit model.","authors":["Bibhas Adhikari"],"url":"https://arxiv.org/abs/2409.03018"}
{"created":"2025-05-08","title":"Bayesian computation with generative diffusion models by Multilevel Monte Carlo","abstract":"Generative diffusion models have recently emerged as a powerful strategy to perform stochastic sampling in Bayesian inverse problems, delivering remarkably accurate solutions for a wide range of challenging applications. However, diffusion models often require a large number of neural function evaluations per sample in order to deliver accurate posterior samples. As a result, using diffusion models as stochastic samplers for Monte Carlo integration in Bayesian computation can be highly computationally expensive, particularly in applications that require a substantial number of Monte Carlo samples for conducting uncertainty quantification analyses. This cost is especially high in large-scale inverse problems such as computational imaging, which rely on large neural networks that are expensive to evaluate. With quantitative imaging applications in mind, this paper presents a Multilevel Monte Carlo strategy that significantly reduces the cost of Bayesian computation with diffusion models. This is achieved by exploiting cost-accuracy trade-offs inherent to diffusion models to carefully couple models of different levels of accuracy in a manner that significantly reduces the overall cost of the calculation, without reducing the final accuracy. The proposed approach achieves a $4\\times$-to-$8\\times$ reduction in computational cost w.r.t. standard techniques across three benchmark imaging problems.","authors":["Abdul-Lateef Haji-Ali","Marcelo Pereyra","Luke Shaw","Konstantinos Zygalakis"],"url":"https://arxiv.org/abs/2409.15511"}
{"created":"2025-05-08","title":"Phase Diagram from Nonlinear Interaction between Superconducting Order and Density: Toward Data-Based Holographic Superconductor","abstract":"We address an inverse problem in modeling holographic superconductors. We focus our research on the critical temperature behavior depicted by experiments. We use a physics-informed neural network method to find a mass function $M(F^2)$, which is necessary to understand phase transition behavior. This mass function describes a nonlinear interaction between superconducting order and charge carrier density. We introduce positional embedding layers to improve the learning process in our algorithm, and the Adam optimization is used to predict the critical temperature data via holographic calculation with appropriate accuracy. Consideration of the positional embedding layers is motivated by the transformer model of natural-language processing in the artificial intelligence (AI) field. We obtain holographic models that reproduce borderlines of the normal and superconducting phases provided by actual data. Our work is the first holographic attempt to match phase transition data quantitatively obtained from experiments. Also, the present work offers a new methodology for data-based holographic models.","authors":["Sejin Kim","Kyung Kiu Kim","Yunseok Seo"],"url":"https://arxiv.org/abs/2410.06523"}
{"created":"2025-05-08","title":"Stein's method for marginals on large graphical models","abstract":"Many spatial models exhibit locality structures that effectively reduce their intrinsic dimensionality, enabling efficient approximation and sampling of high-dimensional distributions. However, existing approximation techniques mainly focus on joint distributions, and do not guarantee accuracy for low-dimensional marginals. By leveraging the locality structures, we establish a dimension independent uniform error bound for the marginals of approximate distributions. Inspired by the Stein's method, we introduce a novel $\\delta$-locality condition that quantifies the locality in distributions, and link it to the structural assumptions such as the sparse graphical models. The theoretical guarantee motivates the localization of existing sampling methods, as we illustrate through the localized likelihood-informed subspace method and localized score matching. We show that by leveraging the locality structure, these methods greatly reduce the sample complexity and computational cost via localized and parallel implementations.","authors":["Tiangang Cui","Shuigen Liu","Xin T. Tong"],"url":"https://arxiv.org/abs/2410.11771"}
{"created":"2025-05-08","title":"Adaptive Aggregation Weights for Federated Segmentation of Pancreas MRI","abstract":"Federated learning (FL) enables collaborative model training across institutions without sharing sensitive data, making it an attractive solution for medical imaging tasks. However, traditional FL methods, such as Federated Averaging (FedAvg), face difficulties in generalizing across domains due to variations in imaging protocols and patient demographics across institutions. This challenge is particularly evident in pancreas MRI segmentation, where anatomical variability and imaging artifacts significantly impact performance. In this paper, we conduct a comprehensive evaluation of FL algorithms for pancreas MRI segmentation and introduce a novel approach that incorporates adaptive aggregation weights. By dynamically adjusting the contribution of each client during model aggregation, our method accounts for domain-specific differences and improves generalization across heterogeneous datasets. Experimental results demonstrate that our approach enhances segmentation accuracy and reduces the impact of domain shift compared to conventional FL methods while maintaining privacy-preserving capabilities. Significant performance improvements are observed across multiple hospitals (centers).","authors":["Hongyi Pan","Gorkem Durak","Zheyuan Zhang","Yavuz Taktak","Elif Keles","Halil Ertugrul Aktas","Alpay Medetalibeyoglu","Yury Velichko","Concetto Spampinato","Ivo Schoots","Marco J. Bruno","Rajesh N. Keswani","Pallavi Tiwari","Candice Bolan","Tamas Gonda","Michael G. Goggins","Michael B. Wallace","Ziyue Xu","Ulas Bagci"],"url":"https://arxiv.org/abs/2410.22530"}
{"created":"2025-05-08","title":"ASURA-FDPS-ML: Star-by-star Galaxy Simulations Accelerated by Surrogate Modeling for Supernova Feedback","abstract":"We introduce new high-resolution galaxy simulations accelerated by a surrogate model that reduces the computation cost by approximately 75 percent. Massive stars with a Zero Age Main Sequence mass of more than about 10 $\\mathrm{M_\\odot}$ explode as core-collapse supernovae (CCSNe), which play a critical role in galaxy formation. The energy released by CCSNe is essential for regulating star formation and driving feedback processes in the interstellar medium (ISM). However, the short integration timesteps required for SNe feedback have presented significant bottlenecks in astrophysical simulations across various scales. Overcoming this challenge is crucial for enabling star-by-star galaxy simulations, which aim to capture the dynamics of individual stars and the inhomogeneous shell's expansion within the turbulent ISM. To address this, our new framework combines direct numerical simulations and surrogate modeling, including machine learning and Gibbs sampling. The star formation history and the time evolution of outflow rates in the galaxy match those obtained from resolved direct numerical simulations. Our new approach achieves high-resolution fidelity while reducing computational costs, effectively bridging the physical scale gap and enabling multi-scale simulations.","authors":["Keiya Hirashima","Kana Moriwaki","Michiko S. Fujii","Yutaka Hirai","Takayuki R. Saitoh","Junnichiro Makino","Ulrich P. Steinwandel","Shirley Ho"],"url":"https://arxiv.org/abs/2410.23346"}
{"created":"2025-05-08","title":"Non-Reciprocal Beyond Diagonal RIS: Multiport Network Models and Performance Benefits in Full-Duplex Systems","abstract":"Beyond diagonal reconfigurable intelligent surfaces (BD-RIS) is a new advance in RIS techniques that introduces reconfigurable inter-element connections to generate scattering matrices not limited to being diagonal. BD-RIS has been recently proposed and proven to have benefits in enhancing channel gain and enlarging coverage in wireless communications. Uniquely, BD-RIS enables reciprocal and non-reciprocal architectures characterized by symmetric and non-symmetric scattering matrices. However, the performance benefits and new use cases enabled by non-reciprocal BD-RIS for wireless systems remain unexplored. This work takes a first step toward closing this knowledge gap and studies the non-reciprocal BD-RIS in full-duplex systems and its performance benefits over reciprocal counterparts. We start by deriving a general RIS aided full-duplex system model using a multiport circuit theory, followed by a simplified channel model based on physically consistent assumptions. With the considered channel model, we investigate the effect of BD-RIS non-reciprocity and identify the theoretical conditions for reciprocal and non-reciprocal BD-RISs to simultaneously achieve the maximum received power of the signal of interest in the uplink and the downlink. Simulation results validate the theories and highlight the significant benefits offered by non-reciprocal BD-RIS in full-duplex systems. The significant gains are achieved because of the non-reciprocity principle which implies that if a wave hits the non-reciprocal BD-RIS from one direction, the surface behaves differently than if it hits from the opposite direction. This enables an uplink user and a downlink user at different locations to optimally communicate with the same full-duplex base station via a non-reciprocal BD-RIS, which would not be possible with reciprocal surfaces.","authors":["Hongyu Li","Bruno Clerckx"],"url":"https://arxiv.org/abs/2411.04370"}
{"created":"2025-05-08","title":"Core-periphery detection in multilayer networks","abstract":"Multilayer networks provide a powerful framework for modeling complex systems that capture different types of interactions between the same set of entities across multiple layers. Core-periphery detection involves partitioning the nodes of a network into core nodes, which are highly connected across the network, and peripheral nodes, which are densely connected to the core but sparsely connected among themselves. In this paper, we propose a new model of core-periphery in multilayer network and a nonlinear spectral method that simultaneously detects the corresponding core and periphery structures of both nodes and layers in weighted and directed multilayer networks. Our method reveals novel structural insights in three empirical multilayer networks from distinct application areas: the citation network of complex network scientists, the European airlines transport network, and the world trade network.","authors":["Kai Bergermann","Francesco Tudisco"],"url":"https://arxiv.org/abs/2412.04179"}
{"created":"2025-05-08","title":"Data Efficient Prediction of excited-state properties using Quantum Neural Networks","abstract":"Understanding the properties of excited states of complex molecules is crucial for many chemical and physical processes. Calculating these properties is often significantly more resource-intensive than calculating their ground state counterparts. We present a quantum machine learning model that predicts excited-state properties from the molecular ground state for different geometric configurations. The model comprises a symmetry-invariant quantum neural network and a conventional neural network and is able to provide accurate predictions with only a few training data points. The proposed procedure is fully NISQ compatible. This is achieved by using a quantum circuit that requires a number of parameters linearly proportional to the number of molecular orbitals, along with a parameterized measurement observable, thereby reducing the number of necessary measurements. We benchmark the algorithm on three different molecules with three different system sizes: $H_2$ with four orbitals, LiH with five orbitals, and $H_4$ with six orbitals. For these molecules, we predict the excited state transition energies and transition dipole moments. We show that, in many cases, the procedure is able to outperform various classical models (support vector machines, Gaussian processes, and neural networks) that rely solely on classical features, by up to two orders of magnitude in the test mean squared error.","authors":["Manuel Hagel\\\"uken","Marco F. Huber","Marco Roth"],"url":"https://arxiv.org/abs/2412.09423"}
{"created":"2025-05-08","title":"Spectrum Sharing in Satellite-Terrestrial Integrated Networks: Frameworks, Approaches, and Opportunities","abstract":"To accommodate the increasing communication needs in non-terrestrial networks (NTNs), wireless users in remote areas may require access to more spectrum than is currently allocated. Terrestrial networks (TNs), such as cellular networks, are deployed in specific areas, but many underused licensed spectrum bands remain in remote areas. Therefore, bringing NTNs to a shared spectrum with TNs can improve network capacity under reasonable interference management. However, in satellite-terrestrial integrated networks (STINs), the comprehensive coverage of a satellite and the unbalanced communication resources of STINs make it challenging to effectively manage mutual interference between NTN and TN. This article presents the fundamentals and prospects of spectrum sharing (SS) in STINs by introducing four SS frameworks, their potential application scenarios, and technical challenges. Furthermore, advanced SS approaches related to interference management in STINs and performance metrics of SS in STINs are introduced. Moreover, a preliminary performance evaluation showcases the potential for sharing the spectrum between NTN and TN. Finally, future research opportunities for SS in STINs are discussed.","authors":["Bodong Shang","Zheng Wang","Xiangyu Li","Chungang Yang","Chao Ren","Haijun Zhang"],"url":"https://arxiv.org/abs/2501.02750"}
{"created":"2025-05-08","title":"Improved subsample-and-aggregate via the private modified winsorized mean","abstract":"We develop a univariate, differentially private mean estimator, called the private modified winsorized mean, designed to be used as the aggregator in subsample-and-aggregate. We demonstrate, via real data analysis, that common differentially private multivariate mean estimators may not perform well as the aggregator, even in large datasets, motivating our developments.We show that the modified winsorized mean is minimax optimal for several, large classes of distributions, even under adversarial contamination. We also demonstrate that, empirically, the private modified winsorized mean performs well compared to other private mean estimates. We consider the modified winsorized mean as the aggregator in subsample-and-aggregate, deriving a finite sample deviations bound for a subsample-and-aggregate estimate generated with the new aggregator. This result yields two important insights: (i) the optimal choice of subsamples depends on the bias of the estimator computed on the subsamples, and (ii) the rate of convergence of the subsample-and-aggregate estimator depends on the robustness of the estimator computed on the subsamples.","authors":["Kelly Ramsay","Dylan Spicker"],"url":"https://arxiv.org/abs/2501.14095"}
{"created":"2025-05-08","title":"Separating complexity classes of LCL problems on grids","abstract":"We study the complexity of locally checkable labeling (LCL) problems on $\\mathbb{Z}^n$ from the point of view of descriptive set theory, computability theory, and factors of i.i.d. Our results separate various complexity classes that were not previously known to be distinct and serve as counterexamples to a number of natural conjectures in the field.","authors":["Katalin Berlow","Anton Bernshteyn","Clark Lyons","Felix Weilacher"],"url":"https://arxiv.org/abs/2501.17445"}
{"created":"2025-05-08","title":"mWhisper-Flamingo for Multilingual Audio-Visual Noise-Robust Speech Recognition","abstract":"Audio-Visual Speech Recognition (AVSR) combines lip-based video with audio and can improve performance in noise, but most methods are trained only on English data. One limitation is the lack of large-scale multilingual video data, which makes it hard to train models from scratch. In this work, we propose mWhisper-Flamingo for multilingual AVSR which combines the strengths of a pre-trained audio model (Whisper) and video model (AV-HuBERT). To enable better multi-modal integration and improve the noisy multilingual performance, we introduce decoder modality dropout where the model is trained both on paired audio-visual inputs and separate audio/visual inputs. mWhisper-Flamingo achieves state-of-the-art WER on MuAViC, an AVSR dataset of 9 languages. Audio-visual mWhisper-Flamingo consistently outperforms audio-only Whisper on all languages in noisy conditions.","authors":["Andrew Rouditchenko","Samuel Thomas","Hilde Kuehne","Rogerio Feris","James Glass"],"url":"https://arxiv.org/abs/2502.01547"}
{"created":"2025-05-08","title":"Proxy Prompt: Endowing SAM and SAM 2 with Auto-Interactive-Prompt for Medical Segmentation","abstract":"In this paper, we aim to address the unmet demand for automated prompting and enhanced human-model interactions of SAM and SAM2 for the sake of promoting their widespread clinical adoption. Specifically, we propose Proxy Prompt (PP), auto-generated by leveraging non-target data with a pre-annotated mask. We devise a novel 3-step context-selection strategy for adaptively selecting the most representative contextual information from non-target data via vision mamba and selective maps, empowering the guiding capability of non-target image-mask pairs for segmentation on target image/video data. To reinforce human-model interactions in PP, we further propose a contextual colorization module via a dual-reverse cross-attention to enhance interactions between target features and contextual-embedding with amplifying distinctive features of user-defined object(s). Via extensive evaluations, our method achieves state-of-the-art performance on four public datasets and yields comparable results with fully-trained models, even when trained with only 16 image masks.","authors":["Wang Xinyi","Kang Hongyu","Wei Peishan","Shuai Li","Yu Sun","Sai Kit Lam","Yongping Zheng"],"url":"https://arxiv.org/abs/2502.03501"}
{"created":"2025-05-08","title":"Information geometry of tempered stable processes","abstract":"We find the information geometry of tempered stable processes. Beginning with the derivation of $\\alpha$-divergence between two tempered stable processes, we obtain the corresponding Fisher information matrices and the $\\alpha$-connections on their statistical manifolds. Furthermore, we explore statistical applications of this geometric framework. Various tempered stable processes such as generalized tempered stable processes, classical tempered stable processes, and rapidly-decreasing tempered stable processes are presented as illustrative examples.","authors":["Jaehyung Choi"],"url":"https://arxiv.org/abs/2502.12037"}
{"created":"2025-05-08","title":"An Approximate-Master-Equation Formulation of the Watts Threshold Model on Hypergraphs","abstract":"In traditional models of behavioral or opinion dynamics on social networks, researchers suppose that all interactions occur between pairs of individuals. However, in reality, social interactions also occur in groups of three or more individuals. A common way to incorporate such polyadic interactions is to study dynamical processes on hypergraphs. In a hypergraph, interactions can occur between any number of the individuals in a network. The Watts threshold model (WTM) is a well-known model of a simplistic social spreading process. Very recently, Chen et al. extended the WTM from dyadic networks (i.e., graphs) to polyadic networks (i.e., hypergraphs). In the present paper, we extend their discrete-time model to continuous time using approximate master equations (AMEs). By using AMEs, we are able to model the system with very high accuracy. We then reduce the high-dimensional AME system to a system of three coupled differential equations without any detectable loss of accuracy. This much lower-dimensional system is more computationally efficient to solve numerically and is also easier to interpret. We linearize the reduced AME system and calculate a cascade condition, which allows us to determine when a large spreading event occurs. We then apply our model to a social contact network of a French primary school and to a hypergraph of computer-science coauthorships. We find that the AME system is accurate in modelling the polyadic WTM on these empirical networks; however, we expect that future work that incorporates structural correlations between nearby nodes and groups into the model for the dynamics will lead to more accurate theory for real-world networks.","authors":["Leah A. Keating","Kwang-Il Goh","Mason A. Porter"],"url":"https://arxiv.org/abs/2503.04020"}
{"created":"2025-05-08","title":"Vertex-Based Localization of Erd\\H{o}s-Gallai Theorems for Paths and Cycles","abstract":"For a simple graph $G$, let $n$ and $m$ denote the number of vertices and edges in $G$, respectively. The Erd\\H{o}s-Gallai theorem for paths states that in a simple $P_k$-free graph, $m \\leq \\frac{n(k-1)}{2}$, where $P_k$ denotes a path with length $k$ (that is, with $k$ edges). In this paper, we generalize this result as follows: For each $v \\in V(G)$, let $p(v)$ be the length of the longest path that contains $v$. We show that \\[m \\leq \\sum_{v \\in V(G)} \\frac{p(v)}{2}\\] The Erd\\H{o}s-Gallai theorem for cycles states that in a simple graph $G$ with circumference (that is, the length of the longest cycle) at most $k$, we have $m \\leq \\frac{k(n-1)}{2}$. We strengthen this result as follows: For each $v \\in V(G)$, let $c(v)$ be the length of the longest cycle that contains $v$, or $2$ if $v$ is not part of any cycle. We prove that \\[m \\leq \\left( \\sum_{v \\in V(G)} \\frac{c(v)}{2} \\right) - \\frac{c(u)}{2}\\] where $c(u)$ denotes the circumference of $G$. \\newline Furthermore, we characterize the class of extremal graphs that attain equality in these bounds.","authors":["Rajat Adak (Indian Institute of Science","Bangalore)","L. Sunil Chandran (Indian Institute of Science","Bangalore)"],"url":"https://arxiv.org/abs/2504.01501"}
{"created":"2025-05-08","title":"Steiner Traveling Salesman Problem with Quantum Annealing","abstract":"The Steiner Traveling Salesman Problem (STSP) is a variant of the classical Traveling Salesman Problem. The STSP involves incorporating steiner nodes, which are extra nodes not originally part of the required visit set but that can be added to the route to enhance the overall solution and minimize the total travel cost. Given the NP-hard nature of the STSP, we propose a quantum approach to address it. Specifically, we employ quantum annealing using D-Wave's hardware to explore its potential for solving this problem. To enhance computational feasibility, we develop a preprocessing method that effectively reduces the network size. Our experimental results demonstrate that this reduction technique significantly decreases the problem complexity, making the Quadratic Unconstrained Binary Optimization formulation, the standard input for quantum annealers, better suited for existing quantum hardware. Furthermore, the results highlight the potential of quantum annealing as a promising and innovative approach for solving the STSP.","authors":["Alessia Ciacco","Francesca Guerriero","Eneko Osaba"],"url":"https://arxiv.org/abs/2504.02388"}
{"created":"2025-05-08","title":"Ranked differences Pearson correlation dissimilarity with an application to electricity users time series clustering","abstract":"Time series clustering is an unsupervised learning method for classifying time series data into groups with similar behavior. It is used in applications such as healthcare, finance, economics, energy, and climate science. Several time series clustering methods have been introduced and used for over four decades. Most of them focus on measuring either Euclidean distances or association dissimilarities between time series. In this work, we propose a new dissimilarity measure called ranked Pearson correlation dissimilarity (RDPC), which combines a weighted average of a specified fraction of the largest element-wise differences with the well-known Pearson correlation dissimilarity. It is incorporated into hierarchical clustering. The performance is evaluated and compared with existing clustering algorithms. The results show that the RDPC algorithm outperforms others in complicated cases involving different seasonal patterns, trends, and peaks. Finally, we demonstrate our method by clustering a random sample of customers from a Thai electricity consumption time series dataset into seven groups with unique characteristics.","authors":["Chutiphan Charoensuk","Nathakhun Wiroonsri"],"url":"https://arxiv.org/abs/2505.02173"}
